[
    {
        "filename": "HIVE-10992.json",
        "creation_time": "2015-06-12T19:52:35.000+0000",
        "bug_report": {
            "Title": "WebHCat should not create delegation tokens when Kerberos is not enabled",
            "Description": "The issue arises in the `TempletonControllerJob.run()` method where a delegation token is created without checking if Kerberos security is enabled. This leads to problems for long-running jobs submitted via WebHCat, as the token may be automatically canceled after 24 hours, resulting in errors when the child job attempts to access it. The relevant code snippet is:\n\n```java\nToken<DelegationTokenIdentifier> mrdt = jc.getDelegationToken(new Text(\"mr token\"));\njob.getCredentials().addToken(new Text(\"mr token\"), mrdt);\n```\n\nThe error logs indicate that the token cannot be found in the cache, leading to a `RuntimeException` when trying to find child jobs.",
            "StackTrace": [
                "2015-05-25 20:49:38,026 WARN [main] org.apache.hadoop.ipc.Client: Exception encountered while connecting to the server : org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.token.SecretManager$InvalidToken): token (owner=btbig3, renewer=mr token, realUser=hdp, issueDate=1432399326562, maxDate=1433004126562, sequenceNumber=3, masterKeyId=4) can't be found in cache",
                "2015-05-25 20:49:38,058 WARN [main] org.apache.hadoop.mapred.YarnChild: Exception running child : java.lang.RuntimeException: Exception occurred while finding child jobs",
                "at org.apache.hadoop.mapred.WebHCatJTShim23.getYarnChildJobs(WebHCatJTShim23.java:204)",
                "at org.apache.hadoop.mapred.WebHCatJTShim23.killJobs(WebHCatJTShim23.java:158)",
                "at org.apache.hive.hcatalog.templeton.tool.LaunchMapper.killLauncherChildJobs(LaunchMapper.java:156)",
                "at org.apache.hive.hcatalog.templeton.tool.LaunchMapper.startJob(LaunchMapper.java:124)",
                "at org.apache.hive.hcatalog.templeton.tool.LaunchMapper.run(LaunchMapper.java:261)",
                "at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:784)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)",
                "at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)"
            ],
            "RootCause": "The root cause of the issue is that the `TempletonControllerJob.run()` method creates a delegation token without verifying if Kerberos security is enabled. This results in an `InvalidToken` exception when the token is not found in the cache, particularly for long-running jobs where the token may expire.",
            "StepsToReproduce": [
                "Submit a long-running job via WebHCat without Kerberos enabled.",
                "Wait for the job to run for more than 24 hours.",
                "Observe the logs for warnings related to token expiration and invalid token errors."
            ],
            "ExpectedBehavior": "The system should not create a delegation token if Kerberos is not enabled, preventing any issues related to token expiration and invalid tokens.",
            "ObservedBehavior": "The system creates a delegation token even when Kerberos is not enabled, leading to `InvalidToken` exceptions and failures in finding child jobs.",
            "Suggestions": "To resolve this issue, modify the `TempletonControllerJob.run()` method to check if `UserGroupInformation.isSecurityEnabled()` returns true before creating a delegation token. This will prevent the creation of unnecessary tokens when security is not enabled.",
            "problem_location": {
                "files": [
                    "TempletonControllerJob.java"
                ],
                "classes": [
                    "org.apache.hive.hcatalog.templeton.tool.TempletonControllerJob"
                ],
                "methods": [
                    "TempletonControllerJob.run"
                ]
            },
            "possible_fix": "Modify the `run()` method in `TempletonControllerJob` to include a check for Kerberos security:\n\n```java\nif (UserGroupInformation.isSecurityEnabled()) {\n    Token<DelegationTokenIdentifier> mrdt = jc.getDelegationToken(new Text(\"mr token\"));\n    job.getCredentials().addToken(new Text(\"mr token\"), mrdt);\n}\n```"
        },
        "possible_fix_code": {
            "TempletonControllerJob.run": "public void run() {\n    if (UserGroupInformation.isSecurityEnabled()) {\n        Token<DelegationTokenIdentifier> mrdt = jc.getDelegationToken(new Text(\"mr token\"));\n        job.getCredentials().addToken(new Text(\"mr token\"), mrdt);\n    }\n    // Additional logic for the run method goes here\n}"
        }
    },
    {
        "filename": "HIVE-16450.json",
        "creation_time": "2017-04-14T13:59:12.000+0000",
        "bug_report": {
            "Title": "Some metastore operations are not retried even with desired underlying exceptions",
            "Description": "The issue arises in the `RetryingHMSHandler` class where operations are expected to retry when a `MetaException` is caused by either a `JDOException` or a `NucleusException`. However, in the `ObjectStore` class, many instances throw a new `MetaException` without the underlying cause, leading to missed retries for certain exceptions. For example, a `JDOException` encountered during a query execution is not retried as expected.",
            "StackTrace": [
                "2017-04-04 17:28:21,602 ERROR metastore.ObjectStore (ObjectStore.java:getMTableColumnStatistics(6555)) - Error retrieving statistics via jdo",
                "javax.jdo.JDOException: Exception thrown when executing query",
                "at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:596)",
                "at org.datanucleus.api.jdo.JDOQuery.executeWithArray(JDOQuery.java:321)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.getMTableColumnStatistics(ObjectStore.java:6546)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.access$1200(ObjectStore.java:171)",
                "at org.apache.hadoop.hive.metastore.ObjectStore$9.getJdoResult(ObjectStore.java:6606)",
                "at org.apache.hadoop.hive.metastore.ObjectStore$9.getJdoResult(ObjectStore.java:6595)",
                "at org.apache.hadoop.hive.metastore.ObjectStore$GetHelper.run(ObjectStore.java:2633)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.getTableColumnStatisticsInternal(ObjectStore.java:6594)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.getTableColumnStatistics(ObjectStore.java:6588)",
                "at sun.reflect.GeneratedMethodAccessor23.invoke(Unknown Source)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:103)",
                "at com.sun.proxy.$Proxy0.getTableColumnStatistics(Unknown Source)",
                "at org.apache.hadoop.hive.metastore.HiveAlterHandler.alterTableUpdateTableColumnStats(HiveAlterHandler.java:787)",
                "at org.apache.hadoop.hive.metastore.HiveAlterHandler.alterTable(HiveAlterHandler.java:247)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.alter_table_core(HiveMetaStore.java:3809)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.alter_table_with_environment_context(HiveMetaStore.java:3779)",
                "at sun.reflect.GeneratedMethodAccessor67.invoke(Unknown Source)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:140)",
                "at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:99)",
                "at com.sun.proxy.$Proxy3.alter_table_with_environment_context(Unknown Source)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$alter_table_with_environment_context.getResult(ThriftHiveMetastore.java:9617)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$alter_table_with_environment_context.getResult(ThriftHiveMetastore.java:9601)",
                "at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)",
                "at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:110)",
                "at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:106)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1920)",
                "at org.apache.hadoop.hive.metastore.TUGIBasedProcessor.process(TUGIBasedProcessor.java:118)",
                "at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "RootCause": "The root cause of the issue is that the `ObjectStore` class is throwing a new `MetaException` without the underlying cause, which prevents the retry mechanism from recognizing the `JDOException` or `NucleusException` that should trigger a retry.",
            "StepsToReproduce": [
                "Trigger a metastore operation that results in a `JDOException`.",
                "Observe that the operation does not retry despite the exception being one that should be retried."
            ],
            "ExpectedBehavior": "The system should retry metastore operations when a `MetaException` is caused by a `JDOException` or `NucleusException`.",
            "ObservedBehavior": "Metastore operations do not retry when a `MetaException` is thrown without the underlying cause, leading to failures that should have been retried.",
            "Suggestions": "Modify the `ObjectStore` class to ensure that when throwing a new `MetaException`, the underlying cause is preserved. This will allow the retry mechanism to function correctly.",
            "problem_location": {
                "files": [
                    "ObjectStore.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.metastore.ObjectStore"
                ],
                "methods": [
                    "getMTableColumnStatistics",
                    "getTableColumnStatisticsInternal"
                ]
            },
            "possible_fix": "In the `ObjectStore` class, update the exception handling to include the cause when throwing a new `MetaException`. For example:\n\n```java\nthrow new MetaException(msg, cause);\n```"
        },
        "possible_fix_code": {
            "getMTableColumnStatistics": "  private List<MTableColumnStatistics> getMTableColumnStatistics(Table table, List<String> colNames, QueryWrapper queryWrapper)\n      throws MetaException {\n    if (colNames == null || colNames.isEmpty()) {\n      return null;\n    }\n\n    boolean committed = false;\n\n    try {\n      openTransaction();\n\n      List<MTableColumnStatistics> result = null;\n      validateTableCols(table, colNames);\n      Query query = queryWrapper.query = pm.newQuery(MTableColumnStatistics.class);\n      String filter = \"tableName == t1 && dbName == t2 && (\";\n      String paramStr = \"java.lang.String t1, java.lang.String t2\";\n      Object[] params = new Object[colNames.size() + 2];\n      params[0] = table.getTableName();\n      params[1] = table.getDbName();\n      for (int i = 0; i < colNames.size(); ++i) {\n        filter += ((i == 0) ? \"\" : \" || \") + \"colName == c\" + i;\n        paramStr += \", java.lang.String c\" + i;\n        params[i + 2] = colNames.get(i);\n      }\n      filter += \")\";\n      query.setFilter(filter);\n      query.declareParameters(paramStr);\n      result = (List<MTableColumnStatistics>) query.executeWithArray(params);\n      pm.retrieveAll(result);\n      if (result.size() > colNames.size()) {\n        throw new MetaException(\"Unexpected \" + result.size() + \" statistics for \"\n            + colNames.size() + \" columns\");\n      }\n      committed = commitTransaction();\n      return result;\n    } catch (Exception ex) {\n      LOG.error(\"Error retrieving statistics via jdo\", ex);\n      if (ex instanceof MetaException) {\n        throw (MetaException) ex;\n      }\n      // Include the cause when throwing a new MetaException\n      throw new MetaException(ex.getMessage(), ex);\n    } finally {\n      if (!committed) {\n        rollbackTransaction();\n      }\n    }\n  }"
        }
    },
    {
        "filename": "HIVE-6389.json",
        "creation_time": "2014-02-07T01:33:32.000+0000",
        "bug_report": {
            "Title": "LazyBinaryColumnarSerDe-based RCFile tables break when looking up elements in null-maps.",
            "Description": "RCFile tables utilizing the LazyBinaryColumnarSerDe fail to correctly handle look-ups into map-columns when the column value is null. This issue manifests when executing a query such as `select mymap['1024'] from mytable;` on a table where the `mymap` column contains null values. The resulting error is a `ClassCastException`, indicating a type mismatch during the serialization process, specifically when attempting to cast an `Integer` to a `Text` type. This problem arises from the `LazyBinaryMapOI`, which does not return nulls for either the map or the lookup key when they are null, leading to runtime exceptions.",
            "StackTrace": [
                "2014-02-05 21:50:25,050 FATAL mr.ExecMapper (ExecMapper.java:map(194)) - org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {\"id\":null,\"mymap\":null,\"isnull\":null}",
                "at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:534)",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:177)",
                "at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:429)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)",
                "at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:235)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:262)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:744)",
                "Caused by: java.lang.ClassCastException: java.lang.Integer cannot be cast to org.apache.hadoop.io.Text",
                "at org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableStringObjectInspector.getPrimitiveWritableObject(WritableStringObjectInspector.java:41)",
                "at org.apache.hadoop.hive.serde2.lazy.LazyUtils.writePrimitiveUTF8(LazyUtils.java:226)",
                "at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serialize(LazySimpleSerDe.java:486)",
                "at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serializeField(LazySimpleSerDe.java:439)",
                "at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serialize(LazySimpleSerDe.java:423)",
                "at org.apache.hadoop.hive.ql.exec.FileSinkOperator.processOp(FileSinkOperator.java:560)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:790)",
                "at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:87)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:790)",
                "at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:92)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:790)",
                "at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:524)"
            ],
            "RootCause": "The root cause of the issue is a `ClassCastException` triggered by the `LazyBinaryMapOI` not handling null values correctly. Specifically, when the map or the lookup key is null, the system attempts to cast an `Integer` to a `Text`, leading to a runtime error.",
            "StepsToReproduce": [
                "Create an RCFile table using LazyBinaryColumnarSerDe.",
                "Insert a row into the table where the map column contains null values.",
                "Execute the query `select mymap['1024'] from mytable;`."
            ],
            "ExpectedBehavior": "The system should return null or handle the null values gracefully without throwing a `ClassCastException`.",
            "ObservedBehavior": "The system throws a `ClassCastException` indicating that an `Integer` cannot be cast to a `Text` type when attempting to access null values in the map column.",
            "Suggestions": "Modify the `LazyBinaryMapOI` to ensure that it returns nulls when either the map or the lookup key is null. This adjustment should prevent the `ClassCastException` from occurring during serialization.",
            "problem_location": {
                "files": [
                    "ExecMapper.java",
                    "MapOperator.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.ql.exec.ExecMapper",
                    "org.apache.hadoop.hive.ql.exec.MapOperator"
                ],
                "methods": [
                    "MapOperator.process"
                ]
            },
            "possible_fix": "Update the `LazyBinaryMapOI` implementation to check for null values in both the map and the lookup key, returning null appropriately to avoid type mismatches during serialization."
        },
        "possible_fix_code": {
            "org.apache.hadoop.hive.ql.exec.MapOperator.process": "public void process(Writable value) throws HiveException {\n    // A mapper can span multiple files/partitions.\n    // The serializers need to be reset if the input file changed\n    ExecMapperContext context = getExecContext();\n    if (context != null && context.inputFileChanged()) {\n        // The child operators cleanup if input file has changed\n        cleanUpInputFileChanged();\n    }\n    Object row;\n    try {\n        row = current.readRow(value);\n        if (row == null) {\n            // Handle null row case gracefully\n            forward(null, current.rowObjectInspector);\n            return;\n        }\n        if (current.hasVC()) {\n            current.rowWithPartAndVC[0] = row;\n            if (context != null) {\n                populateVirtualColumnValues(context, current.vcs, current.vcValues, current.deserializer);\n            }\n            int vcPos = current.isPartitioned() ? 2 : 1;\n            current.rowWithPartAndVC[vcPos] = current.vcValues;\n            row = current.rowWithPartAndVC;\n        } else if (current.isPartitioned()) {\n            current.rowWithPart[0] = row;\n            row = current.rowWithPart;\n        }\n    } catch (Exception e) {\n        // Serialize the row and output.\n        String rawRowString;\n        try {\n            rawRowString = value.toString();\n        } catch (Exception e2) {\n            rawRowString = \"[Error getting row data with exception \" + StringUtils.stringifyException(e2) + \" ]\";\n        }\n\n        // TODO: policy on deserialization errors\n        deserialize_error_count.set(deserialize_error_count.get() + 1);\n        throw new HiveException(\"Hive Runtime Error while processing writable \" + rawRowString, e);\n    }\n\n    // The row has been converted to comply with table schema, irrespective of partition schema.\n    // So, use tblOI (and not partOI) for forwarding\n    try {\n        forward(row, current.rowObjectInspector);\n    } catch (Exception e) {\n        // Serialize the row and output the error message.\n        String rowString;\n        try {\n            rowString = SerDeUtils.getJSONString(row, current.rowObjectInspector);\n        } catch (Exception e2) {\n            rowString = \"[Error getting row data with exception \" + StringUtils.stringifyException(e2) + \" ]\";\n        }\n        throw new HiveException(\"Hive Runtime Error while processing row \" + rowString, e);\n    }\n}"
        }
    },
    {
        "filename": "HIVE-2372.json",
        "creation_time": "2011-08-12T09:07:34.000+0000",
        "bug_report": {
            "Title": "java.io.IOException: error=7, Argument list too long",
            "Description": "The issue arises when executing a large query on a table with extensive 2-level partitions, which involves a Perl reducer. While the map tasks execute successfully, all reducer tasks fail with an IOException indicating that the argument list is too long. The stack trace reveals that the error occurs during the execution of the ScriptOperator, specifically when initializing the script with a large number of environment variables, particularly the mapred.input.dir variable, which exceeds the system's limit for environment variable sizes.",
            "StackTrace": [
                "2011-08-11 04:58:29,865 INFO org.apache.hadoop.hive.ql.exec.ScriptOperator: Executing [/usr/bin/perl, <reducer.pl>, <my_argument>]",
                "2011-08-11 04:58:29,935 FATAL ExecReducer: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0)",
                "Caused by: java.io.IOException: Cannot run program \"/usr/bin/perl\": java.io.IOException: error=7, Argument list too long"
            ],
            "RootCause": "The root cause of the issue is that the ScriptOperator is attempting to pass a large number of environment variables to the child process, exceeding the maximum allowed size for command-line arguments in the operating system. This is particularly problematic in Linux systems where the limit is typically around 132KB.",
            "StepsToReproduce": [
                "Execute a query on a table with a significant number of 2-level partitions using a Perl reducer.",
                "Ensure that the map tasks complete successfully.",
                "Observe the reducer tasks failing with the IOException."
            ],
            "ExpectedBehavior": "The reducer tasks should execute successfully without exceeding the argument list limit, allowing the script to run and process the data as intended.",
            "ObservedBehavior": "All reducer tasks fail with an IOException indicating that the argument list is too long, preventing the script from executing.",
            "Suggestions": "Consider reducing the size of the arguments passed to the script, using temporary files to handle large data instead of passing it directly as command-line arguments, and reviewing Hive's configuration settings to manage script execution more effectively.",
            "problem_location": {
                "files": [
                    "ScriptOperator.java",
                    "ExecReducer.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.ql.exec.ScriptOperator",
                    "org.apache.hadoop.hive.ql.exec.ExecReducer"
                ],
                "methods": [
                    "ScriptOperator.processOp",
                    "ExecReducer.reduce"
                ]
            },
            "possible_fix": "Modify the ScriptOperator to handle large data more efficiently by either reducing the number of environment variables or by using temporary files to pass data to the script. Additionally, review and adjust Hive configuration settings related to script execution."
        },
        "possible_fix_code": {
            "org.apache.hadoop.hive.ql.exec.ScriptOperator.processOp": "  public void processOp(Object row, int tag) throws HiveException {\n    // initialize the user's process only when you receive the first row\n    if (firstRow) {\n      firstRow = false;\n      try {\n        String[] cmdArgs = splitArgs(conf.getScriptCmd());\n\n        String prog = cmdArgs[0];\n        File currentDir = new File(\".\").getAbsoluteFile();\n\n        if (!new File(prog).isAbsolute()) {\n          PathFinder finder = new PathFinder(\"PATH\");\n          finder.prependPathComponent(currentDir.toString());\n          File f = finder.getAbsolutePath(prog);\n          if (f != null) {\n            cmdArgs[0] = f.getAbsolutePath();\n          }\n          f = null;\n        }\n\n        String[] wrappedCmdArgs = addWrapper(cmdArgs);\n        LOG.info(\"Executing \" + Arrays.asList(wrappedCmdArgs));\n        LOG.info(\"tablename=\" + hconf.get(HiveConf.ConfVars.HIVETABLENAME.varname));\n        LOG.info(\"partname=\" + hconf.get(HiveConf.ConfVars.HIVEPARTITIONNAME.varname));\n        LOG.info(\"alias=\" + alias);\n\n        ProcessBuilder pb = new ProcessBuilder(wrappedCmdArgs);\n        Map<String, String> env = pb.environment();\n        addJobConfToEnvironment(hconf, env);\n        env.put(safeEnvVarName(HiveConf.ConfVars.HIVEALIAS.varname), String.valueOf(alias));\n\n        // Create an environment variable that uniquely identifies this script operator\n        String idEnvVarName = HiveConf.getVar(hconf, HiveConf.ConfVars.HIVESCRIPTIDENVVAR);\n        String idEnvVarVal = getOperatorId();\n        env.put(safeEnvVarName(idEnvVarName), idEnvVarVal);\n\n        scriptPid = pb.start(); // Runtime.getRuntime().exec(wrappedCmdArgs);\n\n        DataOutputStream scriptOut = new DataOutputStream(new BufferedOutputStream(scriptPid.getOutputStream()));\n        DataInputStream scriptIn = new DataInputStream(new BufferedInputStream(scriptPid.getInputStream()));\n        DataInputStream scriptErr = new DataInputStream(new BufferedInputStream(scriptPid.getErrorStream()));\n\n        scriptOutWriter = conf.getInRecordWriterClass().newInstance();\n        scriptOutWriter.initialize(scriptOut, hconf);\n\n        RecordReader scriptOutputReader = conf.getOutRecordReaderClass().newInstance();\n        scriptOutputReader.initialize(scriptIn, hconf, conf.getScriptOutputInfo().getProperties());\n\n        outThread = new StreamThread(scriptOutputReader, new OutputStreamProcessor(scriptOutputDeserializer.getObjectInspector()), \"OutputProcessor\");\n\n        RecordReader scriptErrReader = conf.getErrRecordReaderClass().newInstance();\n        scriptErrReader.initialize(scriptErr, hconf, conf.getScriptErrInfo().getProperties());\n\n        errThread = new StreamThread(scriptErrReader, new ErrorStreamProcessor(HiveConf.getIntVar(hconf, HiveConf.ConfVars.SCRIPTERRORLIMIT)), \"ErrorProcessor\");\n\n        if (HiveConf.getBoolVar(hconf, HiveConf.ConfVars.HIVESCRIPTAUTOPROGRESS)) {\n          autoProgressor = new AutoProgressor(this.getClass().getName(), reporter, Utilities.getDefaultNotificationInterval(hconf), HiveConf.getIntVar(hconf, HiveConf.ConfVars.HIVES_AUTO_PROGRESS_TIMEOUT) * 1000);\n          autoProgressor.go();\n        }\n\n        outThread.start();\n        errThread.start();\n      } catch (Exception e) {\n        throw new HiveException(\"Cannot initialize ScriptOperator\", e);\n      }\n    }\n\n    if (scriptError != null) {\n      throw new HiveException(scriptError);\n    }\n\n    try {\n      Writable res = scriptInputSerializer.serialize(row, inputObjInspectors[tag]);\n      scriptOutWriter.write(res);\n    } catch (SerDeException e) {\n      LOG.error(\"Error in serializing the row: \" + e.getMessage());\n      scriptError = e;\n      serialize_error_count.set(serialize_error_count.get() + 1);\n      throw new HiveException(e);\n    } catch (IOException e) {\n      if (isBrokenPipeException(e) && allowPartialConsumption()) {\n        setDone(true);\n        LOG.warn(\"Got broken pipe during write: ignoring exception and setting operator to done\");\n      } else {\n        LOG.error(\"Error in writing to script: \" + e.getMessage());\n        if (isBrokenPipeException(e)) {\n          displayBrokenPipeInfo();\n        }\n        scriptError = e;\n        throw new HiveException(e);\n      }\n    }\n  }"
        }
    },
    {
        "filename": "HIVE-2958.json",
        "creation_time": "2012-04-17T15:02:38.000+0000",
        "bug_report": {
            "Title": "GROUP BY causing ClassCastException [LazyDioInteger cannot be cast LazyInteger]",
            "Description": "This bug report addresses a ClassCastException encountered during a GROUP BY operation in Hive. The issue arises when attempting to cast a LazyDioInteger to a LazyInteger, indicating a type mismatch in the data being processed. This problem is likely related to serialization/deserialization processes or schema inconsistencies in the Hive table. The error occurs specifically when executing the query: 'SELECT data_resource_id, count(*) FROM tim_hbase_occurrence GROUP BY data_resource_id;'.",
            "StackTrace": [
                "org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {\"id\":1444,\"scientific_name\":null,\"data_resource_id\":1081}",
                "Caused by: java.lang.ClassCastException: org.apache.hadoop.hive.serde2.lazydio.LazyDioInteger cannot be cast to org.apache.hadoop.hive.serde2.lazy.LazyInteger",
                "at org.apache.hadoop.hive.ql.exec.GroupByOperator.processOp(GroupByOperator.java:737)",
                "at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:471)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:762)",
                "at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:84)",
                "at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:471)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:762)",
                "at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:83)",
                "at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:471)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:762)",
                "at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:529)"
            ],
            "RootCause": "The root cause of the issue is a ClassCastException due to a type mismatch between LazyDioInteger and LazyInteger during the GROUP BY operation. This suggests that the data being processed does not match the expected types defined in the Hive table schema, likely due to serialization issues or schema evolution.",
            "StepsToReproduce": [
                "Create the external table 'tim_hbase_occurrence' with the specified schema.",
                "Insert data into the table ensuring that the data types match the schema.",
                "Execute the query: 'SELECT data_resource_id, count(*) FROM tim_hbase_occurrence GROUP BY data_resource_id;'."
            ],
            "ExpectedBehavior": "The query should return a count of occurrences grouped by data_resource_id without any errors.",
            "ObservedBehavior": "The query fails with a ClassCastException indicating a type mismatch between LazyDioInteger and LazyInteger.",
            "Suggestions": "To resolve this issue, check the Hive table schema for discrepancies with the input data types. Validate the input data for any inconsistencies. Review serialization settings to ensure they align with Hive's expectations. Consult Hive documentation for any known issues related to the specific versions in use.",
            "problem_location": {
                "files": [
                    "MapOperator.java",
                    "GroupByOperator.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.ql.exec.MapOperator",
                    "org.apache.hadoop.hive.ql.exec.GroupByOperator"
                ],
                "methods": [
                    "MapOperator.process",
                    "GroupByOperator.processOp"
                ]
            },
            "possible_fix": "Ensure that the data types in the Hive table schema match the data being processed. If necessary, modify the schema or the input data to resolve the type mismatch. Additionally, review and correct any custom serialization implementations."
        },
        "possible_fix_code": {
            "MapOperator.process": "  public void process(Writable value) throws HiveException {\n    // A mapper can span multiple files/partitions.\n    // The serializers need to be reset if the input file changed\n    if ((this.getExecContext() != null) &&\n        this.getExecContext().inputFileChanged()) {\n      // The child operators cleanup if input file has changed\n      cleanUpInputFileChanged();\n    }\n\n    Object row = null;\n    try {\n      if (this.hasVC) {\n        this.rowWithPartAndVC[0] = deserializer.deserialize(value);\n        int vcPos = isPartitioned ? 2 : 1;\n        populateVirtualColumnValues();\n        this.rowWithPartAndVC[vcPos] = this.vcValues;\n      } else if (!isPartitioned) {\n        row = deserializer.deserialize((Writable) value);\n      } else {\n        rowWithPart[0] = deserializer.deserialize((Writable) value);\n      }\n    } catch (ClassCastException e) {\n      // Handle ClassCastException specifically for LazyDioInteger and LazyInteger\n      throw new HiveException(\"Type mismatch: LazyDioInteger cannot be cast to LazyInteger. Please check the data types in the Hive table schema.\", e);\n    } catch (Exception e) {\n      // Serialize the row and output.\n      String rawRowString;\n      try {\n        rawRowString = value.toString();\n      } catch (Exception e2) {\n        rawRowString = \"[Error getting row data with exception \" +\n            StringUtils.stringifyException(e2) + \" ]\";\n      }\n\n      // TODO: policy on deserialization errors\n      deserialize_error_count.set(deserialize_error_count.get() + 1);\n      throw new HiveException(\"Hive Runtime Error while processing writable \" + rawRowString, e);\n    }\n\n    try {\n      if (this.hasVC) {\n        forward(this.rowWithPartAndVC, this.rowObjectInspector);\n      } else if (!isPartitioned) {\n        forward(row, rowObjectInspector);\n      } else {\n        forward(rowWithPart, rowObjectInspector);\n      }\n    } catch (Exception e) {\n      // Serialize the row and output the error message.\n      String rowString;\n      try {\n        if (this.hasVC) {\n          rowString = SerDeUtils.getJSONString(rowWithPartAndVC, rowObjectInspector);\n        } else if (!isPartitioned) {\n          rowString = SerDeUtils.getJSONString(row, rowObjectInspector);\n        } else {\n          rowString = SerDeUtils.getJSONString(rowWithPart, rowObjectInspector);\n        }\n      } catch (Exception e2) {\n        rowString = \"[Error getting row data with exception \" +\n            StringUtils.stringifyException(e2) + \" ]\";\n      }\n      throw new HiveException(\"Hive Runtime Error while processing row \" + rowString, e);\n    }\n  }"
        }
    },
    {
        "filename": "HIVE-13392.json",
        "creation_time": "2016-03-30T22:32:50.000+0000",
        "bug_report": {
            "Title": "Disable Speculative Execution for ACID Compactor",
            "Description": "The current implementation of the ACID Compactor in Hadoop is not configured to handle speculative execution, which can lead to file creation conflicts in HDFS. Speculative execution is enabled by default, and when multiple tasks attempt to create the same file, it results in a lease conflict, as indicated by the error message in the stack trace. This issue arises when a file lease is owned by another client, preventing the current task from creating the file. To mitigate this, speculative execution should be disabled for the mappers and reducers involved in the compaction process.",
            "StackTrace": [
                "2016-02-08 22:56:38,256 WARN [main] org.apache.hadoop.mapred.YarnChild: Exception running child : org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.protocol.AlreadyBeingCreatedException): Failed to CREATE_FILE /apps/hive/warehouse/service_logs_v2/ds=2016-01-20/_tmp_6cf08b9f-c2e2-4182-bc81-e032801b147f/base_13858600/bucket_00004 for DFSClient_attempt_1454628390210_27756_m_000001_1_131224698_1 on 172.18.129.12 because this file lease is currently owned by DFSClient_attempt_1454628390210_27756_m_000001_0_-2027182532_1 on 172.18.129.18",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.recoverLeaseInternal(FSNamesystem.java:2937)",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:2562)",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:2335)",
                "at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create(NameNodeRpcServer.java:688)"
            ],
            "RootCause": "The root cause of the issue is a lease conflict in HDFS, where a file is already being created by another client, leading to an `AlreadyBeingCreatedException`. This occurs because speculative execution allows multiple tasks to attempt file creation simultaneously.",
            "StepsToReproduce": [
                "Set up a Hadoop job with speculative execution enabled.",
                "Run the ACID Compactor on a dataset that triggers file creation.",
                "Observe the logs for lease conflict warnings."
            ],
            "ExpectedBehavior": "When speculative execution is disabled, each task should be able to create files without encountering lease conflicts, allowing the compaction process to complete successfully.",
            "ObservedBehavior": "The compaction process fails with a lease conflict error, preventing the creation of necessary files in HDFS.",
            "Suggestions": "Disable speculative execution for the ACID Compactor job by setting the JobConf options `mapred.map.tasks.speculative.execution` and `mapred.reduce.tasks.speculative.execution` to false. Consider implementing a mechanism to handle lease recovery or using unique directories for each task to avoid conflicts.",
            "problem_location": {
                "files": [
                    "CompactorMR.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.server.namenode.FSNamesystem"
                ],
                "methods": [
                    "FSNamesystem.startFile",
                    "FSNamesystem.recoverLease"
                ]
            },
            "possible_fix": "Modify the job configuration to disable speculative execution by adding the following lines to the JobConf: \n\n`job.getConfiguration().setBoolean(\"mapred.map.tasks.speculative.execution\", false);`\n`job.getConfiguration().setBoolean(\"mapred.reduce.tasks.speculative.execution\", false);`"
        },
        "possible_fix_code": {
            "disableSpeculativeExecution": "public void disableSpeculativeExecution(JobConf job) {\n    job.getConfiguration().setBoolean(\"mapred.map.tasks.speculative.execution\", false);\n    job.getConfiguration().setBoolean(\"mapred.reduce.tasks.speculative.execution\", false);\n}"
        }
    },
    {
        "filename": "HIVE-11301.json",
        "creation_time": "2015-07-18T00:41:40.000+0000",
        "bug_report": {
            "Title": "Thrift Metastore Issue When Getting Stats Results in Disconnect",
            "Description": "The issue arises when attempting to retrieve aggregate statistics from the Hive Metastore, resulting in a Thrift error due to an unset required field 'colStats' in the AggrStats struct. This leads to a TProtocolException during the validation process, causing the client to lose connection and attempt to reconnect. The problem is compounded by SocketException and TTransportException errors, indicating potential network issues affecting communication with the Hive Metastore.",
            "StackTrace": [
                "2015-07-17 20:32:27,795 ERROR [pool-3-thread-150]: server.TThreadPoolServer (TThreadPoolServer.java:run(294)) - Thrift error occurred during processing of message.",
                "org.apache.thrift.protocol.TProtocolException: Required field 'colStats' is unset! Struct:AggrStats(colStats:null, partsFound:0)",
                "at org.apache.hadoop.hive.metastore.api.AggrStats.validate(AggrStats.java:389)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_aggr_stats_for_result.validate(ThriftHiveMetastore.java)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_aggr_stats_for_result$get_aggr_stats_for_resultStandardScheme.write(ThriftHiveMetastore.java)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_aggr_stats_for_result$get_aggr_stats_for_resultStandardScheme.write(ThriftHiveMetastore.java)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_aggr_stats_for_result.write(ThriftHiveMetastore.java)",
                "at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:53)",
                "at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:110)",
                "at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:106)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)",
                "at org.apache.hadoop.hive.metastore.TUGIBasedProcessor.process(TUGIBasedProcessor.java:118)",
                "at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:285)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "at java.lang.Thread.run(Thread.java:745)",
                "2015-07-17 20:32:27,796 WARN  [pool-3-thread-150]: transport.TIOStreamTransport (TIOStreamTransport.java:close(112)) - Error closing output stream.",
                "java.net.SocketException: Socket closed",
                "at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:116)",
                "at java.net.SocketOutputStream.write(SocketOutputStream.java:153)",
                "at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)",
                "at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)",
                "at java.io.FilterOutputStream.close(FilterOutputStream.java:158)",
                "at org.apache.thrift.transport.TIOStreamTransport.close(TIOStreamTransport.java:110)",
                "at org.apache.thrift.transport.TSocket.close(TSocket.java:196)",
                "at org.apache.hadoop.hive.thrift.TFilterTransport.close(TFilterTransport.java:52)",
                "at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:304)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "at java.lang.Thread.run(Thread.java:745)",
                "2015-07-17 20:32:27,796 WARN  [main()]: metastore.RetryingMetaStoreClient (RetryingMetaStoreClient.java:invoke(187)) - MetaStoreClient lost connection. Attempting to reconnect.",
                "org.apache.thrift.transport.TTransportException",
                "at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)",
                "at org.apache.thrift.transport.TTransport.readAll(TTransport.java:86)",
                "at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:429)",
                "at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:318)",
                "at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:219)",
                "at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:69)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_get_aggr_stats_for(ThriftHiveMetastore.java:3029)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.get_aggr_stats_for(ThriftHiveMetastore.java:3016)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getAggrColStatsFor(HiveMetaStoreClient.java:2067)",
                "at sun.reflect.GeneratedMethodAccessor11.invoke(Unknown Source)",
                "at java.lang.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:497)",
                "at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:156)",
                "at com.sun.proxy.$Proxy12.getAggrColStatsFor(Unknown Source)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)",
                "at java.lang.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:497)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStoreClient$SynchronizedHandler.invoke(HiveMetaStoreClient.java:1991)",
                "at com.sun.proxy.$Proxy12.getAggrColStatsFor(Unknown Source)",
                "at org.apache.hadoop.hive.ql.metadata.Hive.getAggrColStatsFor(Hive.java:3124)",
                "at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:252)",
                "at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:137)",
                "at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:125)",
                "at org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory$TableScanStatsRule.process(StatsRulesProcFactory.java:111)"
            ],
            "RootCause": "The root cause of the issue is the unset 'colStats' field in the AggrStats struct, which leads to a TProtocolException during the validation process. This indicates that the data is not being populated correctly when attempting to retrieve aggregate statistics from the Hive Metastore.",
            "StepsToReproduce": [
                "Attempt to retrieve aggregate column statistics using the HiveMetaStoreClient.getAggrColStatsFor method.",
                "Ensure that the parameters (dbName, tblName, colNames, partNames) are correctly specified.",
                "Observe the logs for any Thrift-related errors or exceptions."
            ],
            "ExpectedBehavior": "The system should successfully retrieve aggregate column statistics without any exceptions, and the 'colStats' field in the AggrStats struct should be populated correctly.",
            "ObservedBehavior": "The system throws a TProtocolException due to the 'colStats' field being unset, leading to a loss of connection and retries by the client.",
            "Suggestions": "1. Check network connectivity to ensure the Hive Metastore service is accessible.\n2. Validate the input parameters passed to the getAggrColStatsFor method to ensure they are correct.\n3. Review the Hive Metastore configuration to ensure it is set up correctly for statistics collection.\n4. Inspect the Hive Metastore service logs for any errors or warnings that could provide more context.",
            "problem_location": {
                "files": [
                    "HiveMetaStoreClient.java",
                    "StatsUtils.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.metastore.HiveMetaStoreClient",
                    "org.apache.hadoop.hive.ql.stats.StatsUtils"
                ],
                "methods": [
                    "HiveMetaStoreClient.getAggrColStatsFor",
                    "StatsUtils.collectStatistics"
                ]
            },
            "possible_fix": "Ensure that the get_aggr_stats_for method in the ThriftHiveMetastore class correctly populates the 'colStats' field in the AggrStats struct. Additionally, verify that the parameters passed to the method are valid and that the Hive Metastore service is properly configured."
        },
        "possible_fix_code": {
            "getAggrColStatsFor": "public AggrStats getAggrColStatsFor(String dbName, String tblName,\n    List<String> colNames, List<String> partNames) throws NoSuchObjectException, MetaException, TException {\n    if (colNames.isEmpty()) return null; // Nothing to aggregate.\n    PartitionsStatsRequest req = new PartitionsStatsRequest(dbName, tblName, colNames, partNames);\n    AggrStats stats = client.get_aggr_stats_for(req);\n    if (stats == null) {\n        // Handle the case where stats are not returned\n        stats = new AggrStats();\n        stats.setColStats(new ColumnStatistics()); // Ensure colStats is initialized\n    }\n    return stats;\n  }"
        }
    },
    {
        "filename": "HIVE-11028.json",
        "creation_time": "2015-06-16T23:03:38.000+0000",
        "bug_report": {
            "Title": "Tez: table self join and join with another table fails with IndexOutOfBoundsException",
            "Description": "The issue arises when executing a SQL query that involves a self join on the table `tez_self_join1` and a join with `tez_self_join2`. The query fails with an `IndexOutOfBoundsException`, indicating that the code is attempting to access an index in a list that does not exist. This typically occurs when the lists used for join values or filters are empty due to improper initialization or configuration.",
            "StackTrace": [
                "2015-06-16 15:41:55,759 ERROR [main]: ql.Driver (SessionState.java:printError(979)) - FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask. Vertex failed, vertexName=Reducer 3, vertexId=vertex_1434494327112_0002_4_04, diagnostics=[Task failed, taskId=task_1434494327112_0002_4_04_000000, diagnostics=[TaskAttempt 0 failed, info=[Error: Failure while running task:java.lang.RuntimeException: java.lang.IndexOutOfBoundsException: Index: 0, Size: 0",
                "at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:171)",
                "at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:137)",
                "at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:324)",
                "at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:176)",
                "at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:168)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)",
                "at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:168)",
                "at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.call(TezTaskRunner.java:163)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:262)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:744)",
                "Caused by: java.lang.IndexOutOfBoundsException: Index: 0, Size: 0",
                "at java.util.ArrayList.rangeCheck(ArrayList.java:635)",
                "at java.util.ArrayList.get(ArrayList.java:411)",
                "at org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector.init(StandardStructObjectInspector.java:118)",
                "at org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector.<init>(StandardStructObjectInspector.java:109)",
                "at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getStandardStructObjectInspector(ObjectInspectorFactory.java:290)",
                "at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getStandardStructObjectInspector(ObjectInspectorFactory.java:275)",
                "at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.getJoinOutputObjectInspector(CommonJoinOperator.java:175)",
                "at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.initializeOp(CommonJoinOperator.java:313)",
                "at org.apache.hadoop.hive.ql.exec.AbstractMapJoinOperator.initializeOp(AbstractMapJoinOperator.java:71)",
                "at org.apache.hadoop.hive.ql.exec.CommonMergeJoinOperator.initializeOp(CommonMergeJoinOperator.java:99)",
                "at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:362)",
                "at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.init(ReduceRecordProcessor.java:146)",
                "at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:147)",
                "... 13 more"
            ],
            "RootCause": "The `IndexOutOfBoundsException` is caused by the `CommonJoinOperator` not being properly initialized, leading to empty lists for `joinValues` and `joinFilters`. This can occur if the input map used to populate these lists does not contain the expected data or if the configuration does not provide the necessary expressions.",
            "StepsToReproduce": [
                "Create table `tez_self_join1` with columns `id1`, `id2`, and `id3`.",
                "Insert values into `tez_self_join1`.",
                "Create table `tez_self_join2` with column `id1`.",
                "Insert values into `tez_self_join2`.",
                "Execute the provided SQL query that performs a self join on `tez_self_join1` and a join with `tez_self_join2`."
            ],
            "ExpectedBehavior": "The query should execute successfully and return the expected results without any exceptions.",
            "ObservedBehavior": "The query fails with an `IndexOutOfBoundsException`, indicating that the code is trying to access an index in a list that does not exist.",
            "Suggestions": "Check the configuration of the `CommonJoinOperator` to ensure that the input map is properly populated with the expected data. Verify that the join conditions and expressions are correctly defined.",
            "problem_location": {
                "files": [
                    "TezProcessor.java",
                    "CommonJoinOperator.java",
                    "JoinUtil.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.ql.exec.tez.TezProcessor",
                    "org.apache.hadoop.hive.ql.exec.CommonJoinOperator",
                    "org.apache.hadoop.hive.ql.exec.JoinUtil"
                ],
                "methods": [
                    "TezProcessor.initializeAndRunProcessor",
                    "CommonJoinOperator.initializeOp",
                    "JoinUtil.populateJoinKeyValue",
                    "CommonJoinOperator.getJoinOutputObjectInspector"
                ]
            },
            "possible_fix": "Ensure that the input map used in `CommonJoinOperator.initializeOp` is correctly populated before the join operation is executed. This may involve checking the configuration settings and ensuring that the expected expressions are provided."
        },
        "possible_fix_code": {
            "CommonJoinOperator.initializeOp": "  protected Collection<Future<?>> initializeOp(Configuration hconf) throws HiveException {\n    Collection<Future<?>> result = super.initializeOp(hconf);\n    this.handleSkewJoin = conf.getHandleSkewJoin();\n    this.hconf = hconf;\n\n    heartbeatInterval = HiveConf.getIntVar(hconf,\n        HiveConf.ConfVars.HIVESENDHEARTBEAT);\n    countAfterReport = 0;\n\n    totalSz = 0;\n\n    int tagLen = conf.getTagLength();\n    // Map that contains the rows for each alias\n    storage = new AbstractRowContainer[tagLen];\n\n    numAliases = conf.getExprs().size();\n\n    joinValues = new List[tagLen];\n    joinFilters = new List[tagLen];\n\n    order = conf.getTagOrder();\n    condn = conf.getConds();\n    nullsafes = conf.getNullSafes();\n    noOuterJoin = conf.isNoOuterJoin();\n\n    // Check if the expressions are properly defined before populating joinValues\n    if (conf.getExprs() == null || conf.getExprs().isEmpty()) {\n        throw new HiveException(\"Join expressions are not defined.\");\n    }\n\n    totalSz = JoinUtil.populateJoinKeyValue(joinValues, conf.getExprs(),\n        order,NOTSKIPBIGTABLE);\n\n    //process join filters\n    joinFilters = new List[tagLen];\n    JoinUtil.populateJoinKeyValue(joinFilters, conf.getFilters(),order,NOTSKIPBIGTABLE);\n\n    joinValuesObjectInspectors = JoinUtil.getObjectInspectorsFromEvaluators(joinValues,\n        inputObjInspectors,NOTSKIPBIGTABLE, tagLen);\n    joinFilterObjectInspectors = JoinUtil.getObjectInspectorsFromEvaluators(joinFilters,\n        inputObjInspectors,NOTSKIPBIGTABLE, tagLen);\n    joinValuesStandardObjectInspectors = JoinUtil.getStandardObjectInspectors(\n        joinValuesObjectInspectors,NOTSKIPBIGTABLE, tagLen);\n\n    filterMaps = conf.getFilterMap();\n\n    if (noOuterJoin) {\n      rowContainerStandardObjectInspectors = joinValuesStandardObjectInspectors;\n    } else {\n      List<ObjectInspector>[] rowContainerObjectInspectors = new List[tagLen];\n      for (Byte alias : order) {\n        ArrayList<ObjectInspector> rcOIs = new ArrayList<ObjectInspector>();\n        rcOIs.addAll(joinValuesObjectInspectors[alias]);\n        // for each alias, add object inspector for short as the last element\n        rcOIs.add(\n            PrimitiveObjectInspectorFactory.writableShortObjectInspector);\n        rowContainerObjectInspectors[alias] = rcOIs;\n      }\n      rowContainerStandardObjectInspectors =\n        JoinUtil.getStandardObjectInspectors(rowContainerObjectInspectors,NOTSKIPBIGTABLE, tagLen);\n    }\n\n    dummyObj = new ArrayList[numAliases];\n    dummyObjVectors = new RowContainer[numAliases];\n\n    joinEmitInterval = HiveConf.getIntVar(hconf,\n        HiveConf.ConfVars.HIVEJOINEMITINTERVAL);\n    joinCacheSize = HiveConf.getIntVar(hconf,\n        HiveConf.ConfVars.HIVEJOINCACHESIZE);\n\n    // construct dummy null row (indicating empty table) and\n    // construct spill table serde which is used if input is too\n    // large to fit into main memory.\n    byte pos = 0;\n    for (Byte alias : order) {\n      int sz = conf.getExprs().get(alias).size();\n      ArrayList<Object> nr = new ArrayList<Object>(sz);\n\n      for (int j = 0; j < sz; j++) {\n        nr.add(null);\n      }\n\n      if (!noOuterJoin) {\n        // add whether the row is filtered or not\n        // this value does not matter for the dummyObj\n        // because the join values are already null\n        nr.add(new ShortWritable());\n      }\n      dummyObj[pos] = nr;\n      // there should be only 1 dummy object in the RowContainer\n      RowContainer<List<Object>> values = JoinUtil.getRowContainer(hconf,\n          rowContainerStandardObjectInspectors[pos],\n          alias, 1, spillTableDesc, conf, !hasFilter(pos), reporter);\n\n      values.addRow(dummyObj[pos]);\n      dummyObjVectors[pos] = values;\n\n      // if serde is null, the input doesn\\'t need to be spilled out\n      // e.g., the output columns does not contains the input table\n      RowContainer<List<Object>> rc = JoinUtil.getRowContainer(hconf,\n          rowContainerStandardObjectInspectors[pos],\n          alias, joinCacheSize, spillTableDesc, conf, !hasFilter(pos), reporter);\n      storage[pos] = rc;\n\n      pos++;\n    }\n\n    forwardCache = new Object[totalSz];\n    aliasFilterTags = new short[numAliases];\n    Arrays.fill(aliasFilterTags, (byte)0xff);\n\n    filterTags = new short[numAliases];\n    skipVectors = new boolean[numAliases][];\n    for(int i = 0; i < skipVectors.length; i++) {\n      skipVectors[i] = new boolean[i + 1];\n    }\n    intermediate = new List[numAliases];\n\n    offsets = new int[numAliases + 1];\n    int sum = 0;\n    for (int i = 0; i < numAliases; i++) {\n      offsets[i] = sum;\n      sum += joinValues[order[i]].size();\n    }\n    offsets[numAliases] = sum;\n\n    outputObjInspector = getJoinOutputObjectInspector(order,\n        joinValuesStandardObjectInspectors, conf);\n\n    for( int i = 0; i < condn.length; i++ ) {\n      if(condn[i].getType() == JoinDesc.LEFT_SEMI_JOIN) {\n        hasLeftSemiJoin = true;\n      }\n    }\n\n    if (isLogInfoEnabled) {\n      LOG.info(\"JOIN \" + outputObjInspector.getTypeName() + \" totalsz = \" + totalSz);\n    }\n    return result;\n  }"
        }
    },
    {
        "filename": "HIVE-14380.json",
        "creation_time": "2016-07-29T00:14:58.000+0000",
        "bug_report": {
            "Title": "Queries on tables with remote HDFS paths fail in 'encryption' checks.",
            "Description": "When querying tables with remote HDFS paths, an IAException is thrown indicating a failure to determine if the specified HDFS path is encrypted. The error message highlights a mismatch between the expected filesystem and the actual filesystem being used. This issue arises from the way the `SessionState` class retrieves the `FileSystem` instance, which is based on the current session configuration rather than the specific path being queried. This leads to incorrect assumptions about the encryption status of the path.",
            "StackTrace": [
                "2016-07-26 01:16:27,471 ERROR parse.CalcitePlanner (SemanticAnalyzer.java:getMetaData(1867)) - org.apache.hadoop.hive.ql.metadata.HiveException: Unable to determine if hdfs://foo.ygrid.yahoo.com:8020/projects/my_db/my_table is encrypted: java.lang.IllegalArgumentException: Wrong FS: hdfs://foo.ygrid.yahoo.com:8020/projects/my_db/my_table, expected: hdfs://bar.ygrid.yahoo.com:8020",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.isPathEncrypted(SemanticAnalyzer.java:2204)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getStrongestEncryptedTablePath(SemanticAnalyzer.java:2274)"
            ],
            "RootCause": "The root cause of the issue is a configuration mismatch where the Hive instance is attempting to access an HDFS path that does not match the expected filesystem configuration. This is due to the `SessionState` class using the current session configuration to create the `FileSystem` instance instead of the specific path's configuration.",
            "StepsToReproduce": [
                "1. Set up a Hive table with a remote HDFS path pointing to a different HDFS cluster.",
                "2. Attempt to query the table.",
                "3. Observe the IAException indicating a mismatch in the expected and actual HDFS filesystem."
            ],
            "ExpectedBehavior": "The system should correctly determine if the specified HDFS path is encrypted and return the appropriate results without throwing an exception.",
            "ObservedBehavior": "An IAException is thrown indicating that the system is unable to determine if the HDFS path is encrypted due to a mismatch in the expected filesystem.",
            "Suggestions": "Check the Hive configuration files (`hive-site.xml`, `core-site.xml`, `hdfs-site.xml`) to ensure they point to the correct HDFS URI. Verify network connectivity and permissions for the user running the Hive queries.",
            "problem_location": {
                "files": [
                    "SessionState.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.ql.parse.SemanticAnalyzer",
                    "org.apache.hadoop.hive.ql.metadata.HiveException"
                ],
                "methods": [
                    "SessionState.getHdfsEncryptionShim",
                    "SemanticAnalyzer.isPathEncrypted"
                ]
            },
            "possible_fix": "Modify the `getHdfsEncryptionShim` method in `SessionState` to retrieve the `FileSystem` instance based on the specific path being checked, rather than the current session configuration."
        },
        "possible_fix_code": {
            "SessionState.getHdfsEncryptionShim": "  public HadoopShims.HdfsEncryptionShim getHdfsEncryptionShim(Path path) throws HiveException {\n    FileSystem fs = null;\n    try {\n      fs = path.getFileSystem(SessionState.get().getConf());\n    } catch (IOException e) {\n      throw new HiveException(\"Unable to get FileSystem for path: \" + path, e);\n    }\n\n    // Retrieve the HDFS encryption shim based on the specific path's FileSystem\n    HadoopShims.HdfsEncryptionShim hdfsEncryptionShim = HadoopShims.getHdfsEncryptionShim(fs);\n    return hdfsEncryptionShim;\n  }"
        }
    },
    {
        "filename": "HIVE-7799.json",
        "creation_time": "2014-08-20T09:45:21.000+0000",
        "bug_report": {
            "Title": "TRANSFORM failed in transform_ppr1.q[Spark Branch]",
            "Description": "The bug report details a `NullPointerException` encountered during the execution of a Spark job that utilizes Hive. The exception occurs in the `HiveKVResultCache.next` method, indicating that an attempt was made to access a null object. The issue may stem from improper handling of the `RowContainer`, which is not designed to allow writing after a read operation. This suggests a potential misuse of the data structure within the Hive Spark integration.",
            "StackTrace": [
                "2014-08-20 01:14:36,594 ERROR executor.Executor (Logging.scala:logError(96)) - Exception in task 0.0 in stage 1.0 (TID 0)",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.hive.ql.exec.spark.HiveKVResultCache.next(HiveKVResultCache.java:113)",
                "at org.apache.hadoop.hive.ql.exec.spark.HiveBaseFunctionResultList$ResultIterator.next(HiveBaseFunctionResultList.java:124)",
                "at org.apache.hadoop.hive.ql.exec.spark.HiveBaseFunctionResultList$ResultIterator.next(HiveBaseFunctionResultList.java:82)",
                "at scala.collection.convert.Wrappers$JIteratorWrapper.next(Wrappers.scala:42)",
                "at scala.collection.Iterator$class.foreach(Iterator.scala:727)",
                "at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)",
                "at org.apache.spark.shuffle.hash.HashShuffleWriter.write(HashShuffleWriter.java:65)",
                "at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)",
                "at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)",
                "at org.apache.spark.scheduler.Task.run(Task.java:54)",
                "at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:199)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:722)"
            ],
            "RootCause": "The `NullPointerException` is likely caused by an attempt to access an uninitialized object within the `HiveKVResultCache.next` method, possibly due to improper usage of the `RowContainer` which does not allow writing after reading.",
            "StepsToReproduce": [
                "Execute the Spark job using the Hive context with the specified query `transform_ppr1.q`.",
                "Ensure that the data being processed involves a `RowContainer` that is accessed after being read."
            ],
            "ExpectedBehavior": "The Spark job should execute without errors, processing the data correctly and returning the expected results.",
            "ObservedBehavior": "The job fails with a `NullPointerException`, indicating that an object was accessed while it was null, disrupting the execution flow.",
            "Suggestions": "Review the initialization and usage of objects in the `HiveKVResultCache` and ensure that the `RowContainer` is not misused. Consider adding null checks or revising the logic to prevent accessing null references.",
            "problem_location": {
                "files": [
                    "HiveKVResultCache.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.ql.exec.spark.HiveKVResultCache"
                ],
                "methods": [
                    "HiveKVResultCache.next"
                ]
            },
            "possible_fix": "Check the implementation of the `next` method in `HiveKVResultCache` to ensure that all objects are properly initialized before being accessed. Implement null checks where necessary to prevent `NullPointerException`."
        },
        "possible_fix_code": {
            "HiveKVResultCache.next": "public RowContainer next() {\n    if (rowContainer == null) {\n        throw new IllegalStateException(\"RowContainer is not initialized.\");\n    }\n    // Additional logic to handle the next row\n    // Ensure that we are not trying to write after reading\n    if (isReadCompleted) {\n        throw new IllegalStateException(\"Cannot read after write operation.\");\n    }\n    // Logic to retrieve the next row from the RowContainer\n    return rowContainer.getNextRow();\n}"
        }
    },
    {
        "filename": "HIVE-6537.json",
        "creation_time": "2014-03-03T18:57:44.000+0000",
        "bug_report": {
            "Title": "NullPointerException when loading hashtable for MapJoin directly",
            "Description": "A NullPointerException is encountered in the Hive framework during the loading of a hashtable for a MapJoin operation. The stack trace indicates that the exception occurs in the `HashTableLoader.load` method, specifically when attempting to fill an array in the `loadDirectly` method. The issue seems to stem from uninitialized or null values in the `mapJoinTables` or `mapJoinTableSerdes`, which are critical for the loading process. The `initializeOp` method in the `MapJoinOperator` class is responsible for setting these values, and if the cache does not contain the expected values, it may lead to this exception.",
            "StackTrace": [
                "2014-02-20 23:33:15,743 FATAL [main] org.apache.hadoop.hive.ql.exec.mr.ExecMapper: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.NullPointerException",
                "at org.apache.hadoop.hive.ql.exec.mr.HashTableLoader.load(HashTableLoader.java:103)",
                "at org.apache.hadoop.hive.ql.exec.MapJoinOperator.loadHashTable(MapJoinOperator.java:149)",
                "at org.apache.hadoop.hive.ql.exec.MapJoinOperator.cleanUpInputFileChangedOp(MapJoinOperator.java:164)",
                "at org.apache.hadoop.hive.ql.exec.Operator.cleanUpInputFileChanged(Operator.java:1026)",
                "at org.apache.hadoop.hive.ql.exec.Operator.cleanUpInputFileChanged(Operator.java:1030)",
                "at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:489)",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:177)",
                "at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:430)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:342)",
                "at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:396)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)",
                "Caused by: java.lang.NullPointerException",
                "at java.util.Arrays.fill(Arrays.java:2685)",
                "at org.apache.hadoop.hive.ql.exec.mr.HashTableLoader.loadDirectly(HashTableLoader.java:155)",
                "at org.apache.hadoop.hive.ql.exec.mr.HashTableLoader.load(HashTableLoader.java:81)"
            ],
            "RootCause": "The NullPointerException is likely caused by uninitialized or null values in the `mapJoinTables` or `mapJoinTableSerdes` arrays, which are not being properly set in the `initializeOp` method of the `MapJoinOperator` class. If the cache does not contain the expected values, these variables remain null, leading to the exception when the `load` method is called.",
            "StepsToReproduce": [
                "1. Set up a Hive environment with a MapJoin operation.",
                "2. Attempt to load a hashtable for the MapJoin without ensuring that the necessary tables and serializers are initialized.",
                "3. Observe the NullPointerException in the logs."
            ],
            "ExpectedBehavior": "The hashtable should load successfully without any exceptions, and the MapJoin operation should proceed as expected.",
            "ObservedBehavior": "A NullPointerException is thrown, indicating that the hashtable could not be loaded due to null values in the required parameters.",
            "Suggestions": "1. Ensure that the cache is populated with the expected values before calling the `loadHashTable` method. \n2. Add null checks and logging in the `initializeOp` method to capture the state of `mapJoinTables` and `mapJoinTableSerdes` before they are used. \n3. Investigate the input data to ensure it does not contain unexpected null values.",
            "problem_location": {
                "files": [
                    "HashTableLoader.java",
                    "MapJoinOperator.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.ql.exec.mr.HashTableLoader",
                    "org.apache.hadoop.hive.ql.exec.MapJoinOperator"
                ],
                "methods": [
                    "HashTableLoader.load",
                    "MapJoinOperator.loadHashTable",
                    "MapJoinOperator.initializeOp",
                    "HashTableLoader.loadDirectly"
                ]
            },
            "possible_fix": "In the `initializeOp` method of the `MapJoinOperator`, ensure that the `mapJoinTables` and `mapJoinTableSerdes` are properly initialized from the cache. If they are null, initialize them with new instances. Additionally, implement null checks before using these variables in the `loadHashTable` method."
        },
        "possible_fix_code": {
            "MapJoinOperator.initializeOp": "  protected void initializeOp(Configuration hconf) throws HiveException {\n    super.initializeOp(hconf);\n\n    int tagLen = conf.getTagLength();\n\n    // On Tez only: The hash map might already be cached in the container we run\n    // the task in. On MR: The cache is a no-op.\n    tableKey = \"__HASH_MAP_\"+this.getOperatorId()+\"_container\";\n    serdeKey = \"__HASH_MAP_\"+this.getOperatorId()+\"_serde\";\n\n    cache = ObjectCacheFactory.getCache(hconf);\n    loader = HashTableLoaderFactory.getLoader(hconf);\n\n    mapJoinTables = (MapJoinTableContainer[]) cache.retrieve(tableKey);\n    mapJoinTableSerdes = (MapJoinTableContainerSerDe[]) cache.retrieve(serdeKey);\n    hashTblInitedOnce = true;\n\n    // Ensure mapJoinTables and mapJoinTableSerdes are initialized properly\n    if (mapJoinTables == null || mapJoinTableSerdes == null) {\n      mapJoinTables = new MapJoinTableContainer[tagLen];\n      mapJoinTableSerdes = new MapJoinTableContainerSerDe[tagLen];\n      hashTblInitedOnce = false;\n    }\n  }"
        }
    },
    {
        "filename": "HIVE-13691.json",
        "creation_time": "2016-05-04T23:40:03.000+0000",
        "bug_report": {
            "Title": "No record with CQ_ID=0 found in COMPACTION_QUEUE",
            "Description": "The error occurs during the compaction process in Hive's metastore, specifically when the `Initiator` attempts to compact a table. The stack trace indicates a `MetaException` due to a timeout while executing the `getTable` method, which is likely caused by performance issues or resource constraints. Following this, an `IllegalStateException` is thrown because the system cannot find a record with `CQ_ID=0` in the `COMPACTION_QUEUE`, indicating a failure in the compaction process. This situation arises even when the compaction queue entry was never created, leading to the need for a record in the `completed_compaction_queue` to represent the failed compaction.",
            "StackTrace": [
                "2016-04-29 18:49:31,594 ERROR [Thread-11]: compactor.Initiator (Initiator.java:run(141)) - Caught exception while trying to determine if we should compact id:0,dbname:default,tableName:service_logs_v2,...",
                "Caused by: org.apache.hadoop.hive.metastore.DeadlineException: Timeout when executing method: getTable",
                "2016-04-29 18:49:31,595 ERROR [Thread-11]: compactor.Initiator (Initiator.java:run(154)) - Initiator loop caught unexpected exception this time through the loop: java.lang.IllegalStateException: No record with CQ_ID=0 found in COMPACTION_QUEUE"
            ],
            "RootCause": "The root cause of the issue is a timeout in the `getTable` method of the `ObjectStore` class, which prevents the retrieval of the necessary table information. This timeout leads to an inability to create a compaction entry in the `COMPACTION_QUEUE`, resulting in the `IllegalStateException` when the system attempts to reference a non-existent entry.",
            "StepsToReproduce": [
                "Trigger a compaction process for a table in Hive.",
                "Ensure that the compaction process encounters a timeout while executing the `getTable` method.",
                "Observe the logs for the `IllegalStateException` indicating that no record with `CQ_ID=0` was found."
            ],
            "ExpectedBehavior": "The system should successfully retrieve the table information and create an entry in the `COMPACTION_QUEUE` for the compaction process. If the compaction fails, a corresponding entry should be recorded in the `completed_compaction_queue` to reflect the failure.",
            "ObservedBehavior": "The system fails to retrieve the table information due to a timeout, leading to an `IllegalStateException` because no record with `CQ_ID=0` exists in the `COMPACTION_QUEUE`.",
            "Suggestions": "Investigate the performance of the `getTable` method to identify potential bottlenecks or resource constraints. Consider implementing timeout handling to ensure that a record is created in the `completed_compaction_queue` even if the compaction fails due to a missing entry in the `COMPACTION_QUEUE`.",
            "problem_location": {
                "files": [
                    "metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java",
                    "metastore/src/java/org/apache/hadoop/hive/metastore/txn/CompactionTxnHandler.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.metastore.ObjectStore",
                    "org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler"
                ],
                "methods": [
                    "ObjectStore.getTable",
                    "CompactionTxnHandler.markFailed"
                ]
            },
            "possible_fix": "Review the `getTable` method implementation to optimize performance and reduce the likelihood of timeouts. Additionally, modify the `markFailed` method in `CompactionTxnHandler` to ensure that it can handle cases where the compaction queue entry does not exist, allowing for proper logging and handling of failed compactions."
        },
        "possible_fix_code": {
            "CompactionTxnHandler.markFailed": "  public void markFailed(CompactionInfo ci) throws MetaException {//todo: this should not throw\n    //todo: this should take \"comment\" as parameter to set in CC_META_INFO to provide some context for the failure\n    try {\n      Connection dbConn = null;\n      Statement stmt = null;\n      PreparedStatement pStmt = null;\n      ResultSet rs = null;\n      try {\n        dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);\n        stmt = dbConn.createStatement();\n        rs = stmt.executeQuery(\"select CQ_ID, CQ_DATABASE, CQ_TABLE, CQ_PARTITION, CQ_STATE, CQ_TYPE, CQ_WORKER_ID, CQ_START, CQ_RUN_AS, CQ_HIGHEST_TXN_ID, CQ_META_INFO, CQ_HADOOP_JOB_ID from COMPACTION_QUEUE WHERE CQ_ID = \" + ci.id);\n        if(rs.next()) {\n          ci = CompactionInfo.loadFullFromCompactionQueue(rs);\n          String s = \"delete from COMPACTION_QUEUE where cq_id = \" + ci.id;\n          LOG.debug(\"Going to execute update <\" + s + \">\");\n          int updCnt = stmt.executeUpdate(s);\n        } else {\n          // Create a record in the completed compaction queue even if the entry does not exist\n          LOG.warn(\"No record with CQ_ID=\" + ci.id + \" found in COMPACTION_QUEUE. Marking as failed.\");\n          ci.state = FAILED_STATE;\n          pStmt = dbConn.prepareStatement(\"insert into COMPLETED_COMPACTIONS(CC_ID, CC_DATABASE, CC_TABLE, CC_PARTITION, CC_STATE, CC_TYPE, CC_WORKER_ID, CC_START, CC_END, CC_RUN_AS, CC_HIGHEST_TXN_ID, CC_META_INFO, CC_HADOOP_JOB_ID) VALUES(?,?,?,?,?, ?,?,?,?,?, ?,?,?)\");\n          CompactionInfo.insertIntoCompletedCompactions(pStmt, ci, getDbTime(dbConn));\n          int updCount = pStmt.executeUpdate();\n        }\n        LOG.debug(\"Going to commit\");\n        closeStmt(pStmt);\n        dbConn.commit();\n      } catch (SQLException e) {\n        LOG.error(\"Unable to delete from compaction queue \" + e.getMessage());\n        LOG.debug(\"Going to rollback\");\n        rollbackDBConn(dbConn);\n        try {\n          checkRetryable(dbConn, e, \"markFailed(\" + ci + \")\");\n        } catch(MetaException ex) {\n          LOG.error(\"Unable to connect to transaction database \" + StringUtils.stringifyException(ex));\n        }\n        LOG.error(\"Unable to connect to transaction database \" + StringUtils.stringifyException(e));\n      } finally {\n        close(rs, stmt, null);\n        close(null, pStmt, dbConn);\n      }\n    } catch (RetryException e) {\n      markFailed(ci);\n    }\n  }"
        }
    },
    {
        "filename": "HIVE-17758.json",
        "creation_time": "2017-10-10T12:33:52.000+0000",
        "bug_report": {
            "Title": "NOTIFICATION_SEQUENCE_LOCK_RETRY_SLEEP_INTERVAL.defaultLongVal is -1",
            "Description": "The issue arises from the introduction of retry logic in HIVE-16886, which includes a configurable retry interval. The problem is that the default value for the configuration parameter `NOTIFICATION_SEQUENCE_LOCK_RETRY_SLEEP_INTERVAL` is not being loaded correctly, resulting in a value of -1 being used in the `ObjectStore` class. This leads to an `IllegalArgumentException` when the negative timeout value is passed to the `Thread.sleep()` method during the execution of the `addNotificationEvent` method.",
            "StackTrace": [
                "2017-10-10 11:22:37,638 ERROR [load-dynamic-partitions-12]: metastore.ObjectStore (ObjectStore.java:addNotificationEvent(7444)) - could not get lock for update",
                "java.lang.IllegalArgumentException: timeout value is negative",
                "at java.lang.Thread.sleep(Native Method)",
                "at org.apache.hadoop.hive.metastore.ObjectStore$RetryingExecutor.run(ObjectStore.java:7407)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.lockForUpdate(ObjectStore.java:7361)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.addNotificationEvent(ObjectStore.java:7424)",
                "at sun.reflect.GeneratedMethodAccessor71.invoke(Unknown Source)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:498)"
            ],
            "RootCause": "The root cause of the issue is that the configuration parameter `NOTIFICATION_SEQUENCE_LOCK_RETRY_SLEEP_INTERVAL` is not being initialized properly, leading to a default value of -1. This negative value is then passed to the `Thread.sleep()` method, which throws an `IllegalArgumentException` as negative timeout values are invalid.",
            "StepsToReproduce": [
                "1. Ensure that the Hive configuration is set up with the default value for `NOTIFICATION_SEQUENCE_LOCK_RETRY_SLEEP_INTERVAL`.",
                "2. Trigger the `addNotificationEvent` method in the `ObjectStore` class.",
                "3. Observe the logs for the `IllegalArgumentException` indicating a negative timeout value."
            ],
            "ExpectedBehavior": "The system should correctly load the default value for `NOTIFICATION_SEQUENCE_LOCK_RETRY_SLEEP_INTERVAL`, which should be a non-negative integer, allowing the `Thread.sleep()` method to execute without throwing an exception.",
            "ObservedBehavior": "The system throws an `IllegalArgumentException` with the message 'timeout value is negative' when attempting to execute the `Thread.sleep()` method due to the incorrect loading of the configuration parameter.",
            "Suggestions": "Check the configuration files to ensure that the `NOTIFICATION_SEQUENCE_LOCK_RETRY_SLEEP_INTERVAL` parameter is set correctly. If it is not set, ensure that the default value is properly initialized in the `HiveConf` class. Consider adding validation to prevent negative values from being set.",
            "problem_location": {
                "files": [
                    "ObjectStore.java",
                    "HiveConf.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.metastore.ObjectStore",
                    "org.apache.hadoop.hive.conf.HiveConf"
                ],
                "methods": [
                    "ObjectStore.addNotificationEvent",
                    "ObjectStore.lockForUpdate",
                    "HiveConf.getLongVar"
                ]
            },
            "possible_fix": "In the `HiveConf` class, ensure that the `NOTIFICATION_SEQUENCE_LOCK_RETRY_SLEEP_INTERVAL` is initialized with a valid default value (e.g., 500 milliseconds) instead of -1. This can be done by checking the configuration loading logic and ensuring that the default value is set correctly."
        },
        "possible_fix_code": {
            "HiveConf": "public class HiveConf extends Configuration {\n  // Other existing code...\n\n  public static final ConfVars NOTIFICATION_SEQUENCE_LOCK_RETRY_SLEEP_INTERVAL = new ConfVars(\n      \"hive.notification.sequence.lock.retry.sleep.interval\", 500,\n      new TimeValidator(TimeUnit.MILLISECONDS),\n      \"Sleep interval between retries to acquire a notification lock as described part of property \" +\n      NOTIFICATION_SEQUENCE_LOCK_MAX_RETRIES.name());\n\n  // Other existing code...\n\n  public static long getLongVar(Configuration conf, ConfVars var) {\n    assert (var.valClass == Long.class) : var.varname;\n    if (var.altName != null) {\n      return conf.getLong(var.varname, conf.getLong(var.altName, var.defaultLongVal));\n    }\n    return conf.getLong(var.varname, var.defaultLongVal);\n  }\n\n  public long getLongVar(ConfVars var) {\n    return getLongVar(this, var);\n  }\n\n  // Other existing code...\n}"
        }
    },
    {
        "filename": "HIVE-14898.json",
        "creation_time": "2016-10-06T00:02:36.000+0000",
        "bug_report": {
            "Title": "HS2 shouldn't log callstack for an empty auth header error",
            "Description": "The HiveServer2 (HS2) service currently logs an error stack trace when the client fails to send an authorization header, which is an expected behavior when the client (Knox) does not include the header until it receives a 401 response. This results in unnecessary logging of stack traces that do not provide useful information for debugging. The error is specifically related to Kerberos authentication, where an empty authorization header leads to an `HttpAuthenticationException` being thrown.",
            "StackTrace": [
                "2016-10-05 15:32:02,408 ERROR [HiveServer2-HttpHandler-Pool: Thread-199]: thrift.ThriftHttpServlet (ThriftHttpServlet.java:doKerberosAuth(169)) - Failed to authenticate with hive/_HOST kerberos principal",
                "2016-10-05 15:32:02,408 ERROR [HiveServer2-HttpHandler-Pool: Thread-199]: thrift.ThriftHttpServlet (ThriftHttpServlet.java:doPost(104)) - Error: ",
                "org.apache.hive.service.auth.HttpAuthenticationException: java.lang.reflect.UndeclaredThrowableException",
                "at org.apache.hive.service.cli.thrift.ThriftHttpServlet.doKerberosAuth(ThriftHttpServlet.java:170)",
                "at org.apache.hive.service.cli.thrift.ThriftHttpServlet.doPost(ThriftHttpServlet.java:83)",
                "at javax.servlet.http.HttpServlet.service(HttpServlet.java:727)",
                "at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)",
                "at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:565)",
                "at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:479)",
                "at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:225)",
                "at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1031)",
                "at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:406)",
                "at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:186)",
                "at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:965)",
                "at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:117)",
                "at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:111)",
                "at org.eclipse.jetty.server.Server.handle(Server.java:349)",
                "at org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:449)",
                "at org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.content(AbstractHttpConnection.java:925)",
                "at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:952)",
                "at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235)",
                "at org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:76)",
                "at org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:609)",
                "at org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:45)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:745)",
                "Caused by: java.lang.reflect.UndeclaredThrowableException",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1686)",
                "at org.apache.hive.service.cli.thrift.ThriftHttpServlet.doKerberosAuth(ThriftHttpServlet.java:167)",
                "... 23 more",
                "Caused by: org.apache.hive.service.auth.HttpAuthenticationException: Authorization header received from the client is empty.",
                "at org.apache.hive.service.cli.thrift.ThriftHttpServlet.getAuthHeader(ThriftHttpServlet.java:311)",
                "at org.apache.hive.service.cli.thrift.ThriftHttpServlet.access$100(ThriftHttpServlet.java:59)",
                "at org.apache.hive.service.cli.thrift.ThriftHttpServlet$HttpKerberosServerAction.run(ThriftHttpServlet.java:212)",
                "at org.apache.hive.service.cli.thrift.ThriftHttpServlet$HttpKerberosServerAction.run(ThriftHttpServlet.java:175)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)",
                "... 24 more"
            ],
            "RootCause": "The root cause of the issue is that the client does not send an authorization header, which is expected behavior when using Knox. This results in an empty authorization header being processed, leading to an `HttpAuthenticationException` being thrown in the `ThriftHttpServlet` class.",
            "StepsToReproduce": [
                "Configure the client (Knox) to connect to the HiveServer2 service without sending an authorization header.",
                "Attempt to authenticate and observe the logs generated by HiveServer2."
            ],
            "ExpectedBehavior": "The HiveServer2 service should not log an error stack trace for expected conditions such as an empty authorization header. Instead, it should handle the situation gracefully and return a 401 response without unnecessary logging.",
            "ObservedBehavior": "The HiveServer2 service logs an error stack trace indicating a failure to authenticate due to an empty authorization header, which is not useful for debugging since this is an expected condition.",
            "Suggestions": "To resolve this issue, consider implementing a check in the `ThriftHttpServlet` class to suppress logging for expected authentication failures, such as when the authorization header is empty. Additionally, ensure that the client is properly configured to send the authorization header when necessary.",
            "problem_location": {
                "files": [
                    "ThriftHttpServlet.java"
                ],
                "classes": [
                    "org.apache.hive.service.cli.thrift.ThriftHttpServlet"
                ],
                "methods": [
                    "doKerberosAuth",
                    "doPost",
                    "getAuthHeader"
                ]
            },
            "possible_fix": "Modify the `doKerberosAuth` method in `ThriftHttpServlet` to check if the authorization header is empty and suppress logging for this specific case. This could involve adding a conditional statement to avoid logging the stack trace when the header is not present."
        },
        "possible_fix_code": {
            "doKerberosAuth": "  private String doKerberosAuth(HttpServletRequest request)\n      throws HttpAuthenticationException {\n    // Check if the authorization header is empty and suppress logging\n    String authHeader = request.getHeader(HttpAuthUtils.AUTHORIZATION);\n    if (authHeader == null || authHeader.isEmpty()) {\n      // Suppress logging for expected empty authorization header\n      return null; // or handle as needed\n    }\n    // Try authenticating with the http/_HOST principal\n    if (httpUGI != null) {\n      try {\n        return httpUGI.doAs(new HttpKerberosServerAction(request, httpUGI));\n      } catch (Exception e) {\n        LOG.info(\"Failed to authenticate with http/_HOST kerberos principal, \" +\n            \"trying with hive/_HOST kerberos principal\");\n      }\n    }\n    // Now try with hive/_HOST principal\n    try {\n      return serviceUGI.doAs(new HttpKerberosServerAction(request, serviceUGI));\n    } catch (Exception e) {\n      LOG.error(\"Failed to authenticate with hive/_HOST kerberos principal\");\n      throw new HttpAuthenticationException(e);\n    }\n  }"
        }
    },
    {
        "filename": "HIVE-5546.json",
        "creation_time": "2013-10-15T15:06:59.000+0000",
        "bug_report": {
            "Title": "OutOfMemoryError in ORCInputFormat due to Incorrect Column Handling",
            "Description": "The issue arises from a change in the ORCInputFormat class, specifically related to how included columns are handled. When the includedColumnIds list is empty, the code should not attempt to read any columns. However, the current implementation in OrcInputFormat.findIncludedColumns incorrectly assumes that an empty includedStr means all columns should be read, leading to excessive memory usage and ultimately an OutOfMemoryError. This was highlighted in the logs where the error occurred during the execution of a MapTask.",
            "StackTrace": [
                "java.lang.OutOfMemoryError: Java heap space",
                "at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.<init>(MapTask.java:949)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:428)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:372)",
                "at org.apache.hadoop.mapred.Child$4.run(Child.java:255)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1136)",
                "at org.apache.hadoop.mapred.Child.main(Child.java:249)"
            ],
            "RootCause": "The root cause of the OutOfMemoryError is the incorrect handling of the includedStr parameter in the OrcInputFormat.findIncludedColumns method. When includedStr is an empty string, the code incorrectly assumes that all columns need to be read, which leads to excessive memory consumption.",
            "StepsToReproduce": [
                "Set up a Hadoop job that processes ORC files with an empty includedColumnIds list.",
                "Run the job and monitor the memory usage.",
                "Observe the OutOfMemoryError in the logs."
            ],
            "ExpectedBehavior": "When includedColumnIds is empty, the system should not attempt to read any columns, thus preventing excessive memory usage and avoiding OutOfMemoryError.",
            "ObservedBehavior": "The system attempts to read all columns when includedStr is empty, leading to an OutOfMemoryError during the execution of the MapTask.",
            "Suggestions": "To resolve this issue, the code in OrcInputFormat.findIncludedColumns should be modified to correctly handle the case when includedStr is an empty string, ensuring that no columns are read in this scenario. Additionally, consider increasing the Java heap space for the Hadoop job to accommodate larger datasets.",
            "problem_location": {
                "files": [
                    "OrcInputFormat.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.ql.io.orc.OrcInputFormat"
                ],
                "methods": [
                    "findIncludedColumns"
                ]
            },
            "possible_fix": "Modify the OrcInputFormat.findIncludedColumns method to return null or an empty list when includedStr is empty, preventing the reading of all columns. Example code change:\n\nif (includedStr == null || includedStr.trim().isEmpty()) {\n    return new ArrayList<>(); // Return an empty list instead of null\n}"
        },
        "possible_fix_code": {
            "findIncludedColumns": "public List<String> findIncludedColumns(String includedStr) {\n    if (includedStr == null || includedStr.trim().isEmpty()) {\n        return new ArrayList<>(); // Return an empty list instead of null\n    }\n    // Existing logic to parse includedStr and return the list of included columns\n    List<String> includedColumns = new ArrayList<>();\n    String[] columns = includedStr.split(\",\");\n    for (String column : columns) {\n        includedColumns.add(column.trim());\n    }\n    return includedColumns;\n}"
        }
    },
    {
        "filename": "HIVE-7557.json",
        "creation_time": "2014-07-30T19:25:12.000+0000",
        "bug_report": {
            "Title": "When reduce is vectorized, dynpart_sort_opt_vectorization.q under Tez fails",
            "Description": "The issue arises when the `dynpart_sort_opt_vectorization.q` query is executed under Tez with vectorization enabled. The stack trace indicates a `ClassCastException` occurring during the processing of vector batches, specifically when attempting to cast a `DoubleColumnVector` to a `LongColumnVector`. This suggests a data type mismatch in the vectorized processing of Hive, which is further complicated by an `ArrayIndexOutOfBoundsException` that indicates an attempt to access an invalid index in an array.",
            "StackTrace": [
                "Container released by application, AttemptID:attempt_1406747677386_0003_2_00_000000_2 Info:Error: java.lang.RuntimeException: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) [Error getting row data with exception java.lang.ClassCastException: org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector cannot be cast to org.apache.hadoop.hive.ql.exec.vector.LongColumnVector",
                "at org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriterFactory$VectorExpressionWriterLong.writeValue(VectorExpressionWriterFactory.java:168)",
                "at org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch.toString(VectorizedRowBatch.java:159)",
                "at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.processVectors(ReduceRecordProcessor.java:481)",
                "at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.processRows(ReduceRecordProcessor.java:371)",
                "at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:291)",
                "at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:165)",
                "at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:307)",
                "at org.apache.hadoop.mapred.YarnTezDagChild$5.run(YarnTezDagChild.java:562)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:394)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)",
                "at org.apache.hadoop.mapred.YarnTezDagChild.main(YarnTezDagChild.java:551)",
                "Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) [Error getting row data with exception java.lang.ClassCastException: org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector cannot be cast to org.apache.hadoop.hive.ql.exec.vector.LongColumnVector",
                "at org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriterFactory$VectorExpressionWriterLong.writeValue(VectorExpressionWriterFactory.java:168)",
                "at org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch.toString(VectorizedRowBatch.java:159)",
                "at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.processVectors(ReduceRecordProcessor.java:481)",
                "at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.processRows(ReduceRecordProcessor.java:371)",
                "at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:291)",
                "at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:165)",
                "at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:307)",
                "at org.apache.hadoop.mapred.YarnTezDagChild$5.run(YarnTezDagChild.java:562)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:394)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)",
                "at org.apache.hadoop.mapred.YarnTezDagChild.main(YarnTezDagChild.java:551)",
                "Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing vector batch (tag=0) [Error getting row data with exception java.lang.ClassCastException: org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector cannot be cast to org.apache.hadoop.hive.ql.exec.vector.LongColumnVector",
                "at org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpressionWriterFactory$VectorExpressionWriterLong.writeValue(VectorExpressionWriterFactory.java:168)",
                "at org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch.toString(VectorizedRowBatch.java:159)",
                "at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.processVectors(ReduceRecordProcessor.java:481)",
                "at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.processRows(ReduceRecordProcessor.java:371)",
                "at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:291)",
                "at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:165)",
                "at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:307)",
                "at org.apache.hadoop.mapred.YarnTezDagChild$5.run(YarnTezDagChild.java:562)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:394)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)",
                "at org.apache.hadoop.mapred.YarnTezDagChild.main(YarnTezDagChild.java:551)",
                "Caused by: java.lang.ArrayIndexOutOfBoundsException: 4",
                "at org.apache.hadoop.hive.ql.io.orc.VectorizedOrcSerde.serialize(VectorizedOrcSerde.java:75)",
                "at org.apache.hadoop.hive.ql.io.orc.OrcSerde.serializeVector(OrcSerde.java:148)",
                "at org.apache.hadoop.hive.ql.exec.vector.VectorFileSinkOperator.processOp(VectorFileSinkOperator.java:79)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:800)",
                "at org.apache.hadoop.hive.ql.exec.vector.VectorExtractOperator.processOp(VectorExtractOperator.java:99)",
                "at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.processVectors(ReduceRecordProcessor.java:470)"
            ],
            "RootCause": "The root cause of the issue is a mismatch in data types during vectorized processing in Hive, specifically a ClassCastException when attempting to cast a DoubleColumnVector to a LongColumnVector. This is compounded by an ArrayIndexOutOfBoundsException, indicating that the code is trying to access an invalid index in an array.",
            "StepsToReproduce": [
                "Enable vectorization in Hive.",
                "Run the query `dynpart_sort_opt_vectorization.q` under Tez."
            ],
            "ExpectedBehavior": "The query should execute successfully without any runtime exceptions, processing the vector batches correctly.",
            "ObservedBehavior": "The query fails with a ClassCastException and an ArrayIndexOutOfBoundsException during the vectorized processing of data.",
            "Suggestions": "Verify the schema of the data being processed to ensure it matches the expected types in Hive. Review the query to ensure compatibility with the data types. If using ORC files, ensure that the data written to the ORC files matches the expected schema.",
            "problem_location": {
                "files": [
                    "VectorExpressionWriterFactory.java",
                    "VectorizedRowBatch.java",
                    "VectorizedOrcSerde.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.ql.exec.vector.VectorExpressionWriterFactory$VectorExpressionWriterLong",
                    "org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch",
                    "org.apache.hadoop.hive.ql.io.orc.VectorizedOrcSerde"
                ],
                "methods": [
                    "writeValue",
                    "toString",
                    "serialize"
                ]
            },
            "possible_fix": "Check the data types being processed in the vectorized operations and ensure they align with the expected types in Hive. Modify the query or data schema as necessary to prevent ClassCastException."
        },
        "possible_fix_code": {
            "ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch.toString": "  public String toString() {\n    if (size == 0) {\n      return \"\";\n    }\n    StringBuilder b = new StringBuilder();\n    try {\n      if (this.selectedInUse) {\n        for (int j = 0; j < size; j++) {\n          int i = selected[j];\n          for (int k = 0; k < projectionSize; k++) {\n            int projIndex = projectedColumns[k];\n            ColumnVector cv = cols[projIndex];\n            if (k > 0) {\n              b.append('\\u0001');\n            }\n            // Check for type compatibility before writing value\n            if (cv instanceof DoubleColumnVector && valueWriters[k] instanceof VectorExpressionWriterLong) {\n              throw new HiveException(\"Type mismatch: Cannot write DoubleColumnVector as LongColumnVector\");\n            }\n            if (cv.isRepeating) {\n              b.append(toUTF8(valueWriters[k].writeValue(cv, 0)));\n            } else {\n              b.append(toUTF8(valueWriters[k].writeValue(cv, i)));\n            }\n          }\n          if (j < size - 1) {\n            b.append('\\n');\n          }\n        }\n      } else {\n        for (int i = 0; i < size; i++) {\n          for (int k = 0; k < projectionSize; k++) {\n            int projIndex = projectedColumns[k];\n            ColumnVector cv = cols[projIndex];\n            if (k > 0) {\n              b.append('\\u0001');\n            }\n            // Check for type compatibility before writing value\n            if (cv instanceof DoubleColumnVector && valueWriters[k] instanceof VectorExpressionWriterLong) {\n              throw new HiveException(\"Type mismatch: Cannot write DoubleColumnVector as LongColumnVector\");\n            }\n            if (cv.isRepeating) {\n              b.append(toUTF8(valueWriters[k].writeValue(cv, 0)));\n            } else {\n              b.append(toUTF8(valueWriters[k].writeValue(cv, i)));\n            }\n          }\n          if (i < size - 1) {\n            b.append('\\n');\n          }\n        }\n      }\n    } catch (HiveException ex) {\n      throw new RuntimeException(ex);\n    }\n    return b.toString();\n  }"
        }
    },
    {
        "filename": "HIVE-1712.json",
        "creation_time": "2010-10-14T17:17:44.000+0000",
        "bug_report": {
            "Title": "Migrating metadata from derby to mysql thrown NullPointerException",
            "Description": "Exported derby data to csv, loaded data into mysql and ran hive query which worked in derby and got the following exception:\n\n2010-10-16 08:57:29,080 INFO  metastore.ObjectStore (ObjectStore.java:setConf(106)) - Initialized ObjectStore\n2010-10-16 08:57:29,552 INFO  metastore.HiveMetaStore (HiveMetaStore.java:logStartFunction(171)) - 0: get_table : db=default tbl=testimport\n2010-10-16 08:57:30,140 ERROR metadata.Hive (Hive.java:getTable(395)) - java.lang.NullPointerException\n        at java.util.Hashtable.put(Hashtable.java:394)\n        at java.util.Hashtable.putAll(Hashtable.java:466)\n        at org.apache.hadoop.hive.metastore.MetaStoreUtils.getSchema(MetaStoreUtils.java:520)\n        at org.apache.hadoop.hive.metastore.MetaStoreUtils.getSchema(MetaStoreUtils.java:489)\n        at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:381)\n        at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:333)\n        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getMetaData(SemanticAnalyzer.java:683)\n        at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:5200)\n        at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:105)\n        at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:275)\n        at org.apache.hadoop.hive.ql.Driver.runCommand(Driver.java:320)\n        at org.apache.hadoop.hive.ql.Driver.run(Driver.java:312)\n        at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:123)\n        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:181)\n        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:287)\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)\n        at java.lang.reflect.Method.invoke(Method.java:597)\n        at org.apache.hadoop.util.RunJar.main(RunJar.java:156)",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at java.util.Hashtable.put(Hashtable.java:394)",
                "at java.util.Hashtable.putAll(Hashtable.java:466)",
                "at org.apache.hadoop.hive.metastore.MetaStoreUtils.getSchema(MetaStoreUtils.java:520)",
                "at org.apache.hadoop.hive.metastore.MetaStoreUtils.getSchema(MetaStoreUtils.java:489)",
                "at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:381)",
                "at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:333)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getMetaData(SemanticAnalyzer.java:683)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:5200)",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:105)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:275)",
                "at org.apache.hadoop.hive.ql.Driver.runCommand(Driver.java:320)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:312)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:123)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:181)",
                "at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:287)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "at java.lang.reflect.Method.invoke(Method.java:597)",
                "at org.apache.hadoop.util.RunJar.main(RunJar.java:156)"
            ],
            "RootCause": "The NullPointerException in the Hive metastore likely arises from an issue with the configuration of the metastore client, specifically related to the initialization of the HiveConf object and the values of metastore-related configuration variables.",
            "StepsToReproduce": [
                "Export data from Derby to CSV.",
                "Load the CSV data into MySQL.",
                "Run a Hive query that retrieves a table from the metastore."
            ],
            "ExpectedBehavior": "The Hive query should execute successfully and return the expected results without throwing a NullPointerException.",
            "ObservedBehavior": "A NullPointerException is thrown when attempting to retrieve a table from the Hive metastore.",
            "Suggestions": "Check the values of the metastore-related configuration variables such as METASTOREURIS and METASTOREDIRECTORY in the Hive configuration.",
            "problem_location": {
                "files": [
                    "Hive.java",
                    "MetaStoreUtils.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.ql.metadata.Hive",
                    "org.apache.hadoop.hive.metastore.MetaStoreUtils"
                ],
                "methods": [
                    "Hive.getTable",
                    "MetaStoreUtils.getSchema"
                ]
            },
            "possible_fix": "Verify and update the Hive configuration to ensure that the metastore-related properties (e.g., METASTOREURIS, METASTOREDIRECTORY) are correctly set. This may involve checking the configuration files or environment variables used to initialize Hive."
        },
        "possible_fix_code": {
            "Hive.getTable": "  public Table getTable(final String dbName, final String tableName,\n      boolean throwException) throws HiveException {\n\n    if (tableName == null || tableName.equals(\"\")) {\n      throw new HiveException(\"empty table creation??\");\n    }\n\n    // Get the table from metastore\n    org.apache.hadoop.hive.metastore.api.Table tTable = null;\n    try {\n      tTable = getMSC().getTable(dbName, tableName);\n    } catch (NoSuchObjectException e) {\n      if (throwException) {\n        LOG.error(StringUtils.stringifyException(e));\n        throw new InvalidTableException(\"Table not found \", tableName);\n      }\n      return null;\n    } catch (Exception e) {\n      throw new HiveException(\"Unable to fetch table \" + tableName, e);\n    }\n\n    // For non-views, we need to do some extra fixes\n    if (tTable != null && !TableType.VIRTUAL_VIEW.toString().equals(tTable.getTableType())) {\n      // Fix the non-printable chars\n      Map<String, String> parameters = tTable.getSd().getParameters();\n      String sf = parameters.get(SERIALIZATION_FORMAT);\n      if (sf != null) {\n        char[] b = sf.toCharArray();\n        if ((b.length == 1) && (b[0] < 10)) { // ^A, ^B, ^C, ^D, \\t\n          parameters.put(SERIALIZATION_FORMAT, Integer.toString(b[0]));\n        }\n      }\n\n      // Use LazySimpleSerDe for MetadataTypedColumnsetSerDe.\n      if (org.apache.hadoop.hive.serde2.MetadataTypedColumnsetSerDe.class\n          .getName().equals(\n            tTable.getSd().getSerdeInfo().getSerializationLib())\n          && tTable.getSd().getColsSize() > 0\n          && tTable.getSd().getCols().get(0).getType().indexOf('<') == -1) {\n        tTable.getSd().getSerdeInfo().setSerializationLib(\n            org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.class.getName());\n      }\n    }\n\n    Table table = new Table(tTable);\n\n    table.checkValidity();\n    return table;\n  }"
        }
    },
    {
        "filename": "HIVE-12608.json",
        "creation_time": "2015-12-07T21:26:01.000+0000",
        "bug_report": {
            "Title": "Parquet Schema Evolution doesn't work when a column is dropped from array<struct<>>",
            "Description": "The issue arises when attempting to drop a column from an array of structs in a Hive table defined with Parquet storage. The operation leads to a runtime exception indicating that a field (`c2`) cannot be found in the modified structure, which only contains `c1`. This suggests a schema mismatch between the expected and actual data being processed, particularly during the schema evolution process.",
            "StackTrace": [
                "2015-12-07 11:47:28,503 ERROR [main]: CliDriver (SessionState.java:printError(921)) - Failed with exception java.io.IOException:java.lang.RuntimeException: cannot find field c2 in [c1]",
                "java.io.IOException: java.lang.RuntimeException: cannot find field c2 in [c1]",
                "at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:507)",
                "at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:414)",
                "at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:138)",
                "at org.apache.hadoop.hive.ql.Driver.getResults(Driver.java:1655)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:227)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:159)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:370)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:305)",
                "at org.apache.hadoop.hive.ql.QTestUtil.executeClientInternal(QTestUtil.java:1029)",
                "at org.apache.hadoop.hive.ql.QTestUtil.executeClient(QTestUtil.java:1003)",
                "at org.apache.hadoop.hive.cli.TestCliDriver.runTest(TestCliDriver.java:139)",
                "at org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_parquet_type_promotion(TestCliDriver.java:123)"
            ],
            "RootCause": "The root cause of the issue is a schema mismatch during the schema evolution process in Hive. Specifically, the system expects to find the field `c2` in the data structure after the column drop operation, but it is not present, leading to a RuntimeException.",
            "StepsToReproduce": [
                "Create a Hive table with an array of structs.",
                "Insert data into the table.",
                "Perform an ALTER TABLE operation to drop a column from the array of structs.",
                "Attempt to select data from the modified table."
            ],
            "ExpectedBehavior": "The system should successfully drop the specified column from the array of structs and allow subsequent queries to return the modified data structure without errors.",
            "ObservedBehavior": "The system throws a RuntimeException indicating that the field `c2` cannot be found in the modified structure, which only contains `c1`.",
            "Suggestions": "Review the schema evolution logic in Hive to ensure that it correctly handles the removal of fields from nested structures. Consider implementing additional checks to validate the schema before executing the ALTER TABLE command.",
            "problem_location": {
                "files": [
                    "ql/src/java/org/apache/hadoop/hive/ql/exec/FetchOperator.java",
                    "ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/HiveStructConverter.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.ql.exec.FetchOperator",
                    "org.apache.hadoop.hive.ql.io.parquet.convert.HiveStructConverter"
                ],
                "methods": [
                    "FetchOperator.getNextRow",
                    "HiveStructConverter.getStructFieldTypeInfo",
                    "HiveStructConverter.getFieldTypeIgnoreCase"
                ]
            },
            "possible_fix": "To resolve the issue, ensure that the schema evolution process correctly updates the expected structure in Hive. This may involve modifying the `getStructFieldTypeInfo` and `getFieldTypeIgnoreCase` methods to handle cases where fields are dropped more gracefully, possibly by checking for the existence of fields before attempting to access them."
        },
        "possible_fix_code": {
            "HiveStructConverter.getStructFieldTypeInfo": "  private TypeInfo getStructFieldTypeInfo(String field, int fieldIndex) {\n    String fieldLowerCase = field.toLowerCase();\n    if (Boolean.valueOf(getMetadata().get(DataWritableReadSupport.PARQUET_COLUMN_INDEX_ACCESS))\n        && fieldIndex < hiveFieldNames.size()) {\n      return hiveFieldTypeInfos.get(fieldIndex);\n    }\n    for (int i = 0; i < hiveFieldNames.size(); i++) {\n      if (fieldLowerCase.equalsIgnoreCase(hiveFieldNames.get(i))) {\n        return hiveFieldTypeInfos.get(i);\n      }\n    }\n    // Added check for field existence before throwing exception\n    if (!hiveFieldNames.contains(fieldLowerCase)) {\n      // Log a warning or handle the missing field case gracefully\n      System.err.println(\"Warning: Field '\" + field + \"' not found in the schema.\");\n    }\n    throw new RuntimeException(\"cannot find field \" + field\n        + \" in \" + hiveFieldNames);\n  }"
        }
    },
    {
        "filename": "HIVE-17774.json",
        "creation_time": "2017-10-11T20:02:01.000+0000",
        "bug_report": {
            "Title": "Compaction may start with 0 splits and fail",
            "Description": "The compaction process in the Hadoop system is failing due to an attempt to submit a MapReduce job with zero splits. The logs indicate that the job was submitted with no delta directories available, leading to a `FileNotFoundException` when the job tried to access a temporary file that does not exist. This situation arises when the job configuration does not account for the absence of data to process, resulting in a failed job submission.",
            "StackTrace": [
                "2017-09-26 10:36:01,979 INFO  [...]: compactor.CompactorMR (CompactorMR.java:launchCompactionJob(295)) - Submitting MINOR compaction job .... (current delta dirs count=0, obsolete delta dirs count=0. TxnIdRange[9223372036854775807,-9223372036854775808]",
                "2017-09-26 10:36:02,350 INFO  [...]: mapreduce.JobSubmitter (JobSubmitter.java:submitJobInternal(198)) - number of splits:0",
                "2017-09-26 10:36:08,637 INFO  [...]: mapreduce.Job (Job.java:monitorAndPrintJob(1380)) - Job job_1503950256860_15982 failed with state FAILED due to: No of maps and reduces are 0 job_1503950256860_15982",
                "Job commit failed: java.io.FileNotFoundException: File .../hello_acid/load_date=2016-03-03/_tmp_a95346ad-bd89-4e66-9b05-e60fdfa11858 does not exist.",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.listStatusInternal(DistributedFileSystem.java:904)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.access$600(DistributedFileSystem.java:113)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$21.doCall(DistributedFileSystem.java:966)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$21.doCall(DistributedFileSystem.java:962)",
                "at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.listStatus(DistributedFileSystem.java:962)",
                "at org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorOutputCommitter.commitJob(CompactorMR.java:776)",
                "at org.apache.hadoop.mapred.OutputCommitter.commitJob(OutputCommitter.java:291)",
                "at org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler$EventProcessor.handleJobCommit(CommitterEventHandler.java:285)",
                "at org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler$EventProcessor.run(CommitterEventHandler.java:237)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "RootCause": "The root cause of the issue is that the MapReduce job is being submitted with zero splits due to the absence of delta directories, which leads to a `FileNotFoundException` when the job attempts to access a temporary file that does not exist.",
            "StepsToReproduce": [
                "1. Ensure there are no delta directories available for compaction.",
                "2. Trigger the compaction process.",
                "3. Observe the logs for job submission and failure messages."
            ],
            "ExpectedBehavior": "The compaction job should only be submitted when there are available splits to process, preventing any attempts to access non-existent files.",
            "ObservedBehavior": "The compaction job is submitted with zero splits, resulting in a failure due to a `FileNotFoundException` for a temporary file that does not exist.",
            "Suggestions": "To resolve this issue, ensure that the compaction job checks for available splits before submission. Additionally, implement error handling to prevent job submission when no data is available.",
            "problem_location": {
                "files": [
                    "CompactorMR.java",
                    "JobSubmitter.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.ql.txn.compactor.CompactorMR",
                    "org.apache.hadoop.mapreduce.JobSubmitter"
                ],
                "methods": [
                    "CompactorMR.launchCompactionJob",
                    "JobSubmitter.submitJobInternal"
                ]
            },
            "possible_fix": "Modify the `launchCompactionJob` method in `CompactorMR.java` to include a check for the number of splits before attempting to submit the job. If the number of splits is zero, log a warning and skip the job submission."
        },
        "possible_fix_code": {
            "CompactorMR.launchCompactionJob": "    public void launchCompactionJob() throws IOException {\n        // Check for available splits before submitting the job\n        if (getNumberOfSplits() == 0) {\n            LOG.warn(\"No splits available for compaction. Skipping job submission.\");\n            return;\n        }\n        // Existing logic to submit the compaction job\n        // ... (rest of the method implementation)\n    }"
        }
    },
    {
        "filename": "HIVE-14564.json",
        "creation_time": "2016-08-18T00:11:34.000+0000",
        "bug_report": {
            "Title": "Column Pruning generates out of order columns in SelectOperator which cause ArrayIndexOutOfBoundsException.",
            "Description": "The issue arises during the execution of a Hive query where column pruning in the SelectOperator leads to an ArrayIndexOutOfBoundsException. This occurs when the serialization and deserialization of data do not match due to the incorrect order of columns. The LazyBinarySerDe used in a previous MapReduce job serializes the data in a different order than expected, causing corrupted data to be processed in the current job. The stack trace indicates that the error originates from the ReduceSinkOperator and MapOperator during the processing of rows, specifically when accessing array indices that do not exist.",
            "StackTrace": [
                "2016-07-26 21:49:24,390 FATAL [main] org.apache.hadoop.hive.ql.exec.mr.ExecMapper: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row",
                "at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:507)",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:170)",
                "at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:453)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)",
                "at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)",
                "Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.ArrayIndexOutOfBoundsException",
                "at org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.processOp(ReduceSinkOperator.java:397)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)",
                "at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:95)",
                "at org.apache.hadoop.hive.ql.exec.MapOperator$MapOpCtx.forward(MapOperator.java:157)",
                "at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:497)",
                "... 9 more",
                "Caused by: java.lang.ArrayIndexOutOfBoundsException",
                "at java.lang.System.arraycopy(Native Method)",
                "at org.apache.hadoop.io.Text.set(Text.java:225)",
                "at org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryString.init(LazyBinaryString.java:48)",
                "at org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryStruct.uncheckedGetField(LazyBinaryStruct.java:264)",
                "at org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryStruct.getField(LazyBinaryStruct.java:201)",
                "at org.apache.hadoop.hive.serde2.lazybinary.objectinspector.LazyBinaryStructObjectInspector.getStructFieldData(LazyBinaryStructObjectInspector.java:64)",
                "at org.apache.hadoop.hive.ql.exec.ExprNodeColumnEvaluator._evaluate(ExprNodeColumnEvaluator.java:94)",
                "at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:77)",
                "at org.apache.hadoop.hive.ql.exec.ExprNodeEvaluator.evaluate(ExprNodeEvaluator.java:65)",
                "at org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.makeValueWritable(ReduceSinkOperator.java:550)",
                "at org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.processOp(ReduceSinkOperator.java:377)",
                "... 13 more"
            ],
            "RootCause": "The root cause of the ArrayIndexOutOfBoundsException is the mismatch in the order of columns during serialization and deserialization processes. The LazyBinarySerDe from a previous MapReduce job serialized the data in a different order than expected by the current job, leading to corrupted data being processed.",
            "StepsToReproduce": [
                "Run a MapReduce job that uses column pruning in the SelectOperator.",
                "Ensure that the previous job serializes the data with a different column order.",
                "Attempt to process the output of the previous job in a new MapReduce job."
            ],
            "ExpectedBehavior": "The system should correctly process rows without throwing an ArrayIndexOutOfBoundsException, ensuring that the data is serialized and deserialized in the same order.",
            "ObservedBehavior": "The system throws an ArrayIndexOutOfBoundsException during the processing of rows due to mismatched column order in serialization and deserialization.",
            "Suggestions": "Review the column order used in the serialization process of the previous MapReduce job. Ensure that the current job's deserialization matches the serialized format. Consider implementing validation checks for the data structure before processing.",
            "problem_location": {
                "files": [
                    "MapOperator.java",
                    "ReduceSinkOperator.java",
                    "LazyBinaryString.java",
                    "LazyBinaryStruct.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.ql.exec.MapOperator",
                    "org.apache.hadoop.hive.ql.exec.ReduceSinkOperator",
                    "org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryString",
                    "org.apache.hadoop.hive.serde2.lazybinary.LazyBinaryStruct"
                ],
                "methods": [
                    "MapOperator.process",
                    "ReduceSinkOperator.processOp",
                    "LazyBinaryString.init",
                    "LazyBinaryStruct.uncheckedGetField"
                ]
            },
            "possible_fix": "Ensure that the serialization and deserialization processes are aligned in terms of column order. This may involve modifying the LazyBinarySerDe implementation or adding checks to validate the data structure before processing."
        },
        "possible_fix_code": {
            "LazyBinaryString.init": "  @Override\n  public void init(ByteArrayRef bytes, int start, int length) {\n    if (start < 0 || length < 0 || start + length > bytes.getData().length) {\n      throw new IllegalArgumentException(\"Invalid start or length for byte array.\");\n    }\n    assert (length > -1);\n    data.set(bytes.getData(), start, length);\n  }"
        }
    },
    {
        "filename": "HIVE-3651.json",
        "creation_time": "2012-11-01T23:31:20.000+0000",
        "bug_report": {
            "Title": "bucketmapjoin?.q tests fail with hadoop 0.23",
            "Description": "The Hive job fails during execution due to a missing hashtable file, specifically 'MapJoin-b-11-srcbucket21.txt.hashtable'. The error occurs in the map phase of the job, as indicated by the stack trace. The issue arises from the `ExecMapper` class, which is responsible for processing input data and managing the output collection. The failure to create the hashtable file suggests potential problems in the input data processing or the job configuration.",
            "StackTrace": [
                "2012-11-01 15:51:20,253 WARN  mapred.LocalJobRunner (LocalJobRunner.java:run(479)) - job_local_0001",
                "java.lang.Exception: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: /home/prasadm/repos/apache/hive-patches/build/ql/scratchdir/local/hive_2012-11-01_15-51-06_176_6704298995984162430/-local-10003/HashTable-Stage-1/MapJoin-b-11-srcbucket21.txt.hashtable (No such file or directory)",
                "at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:400)",
                "Caused by: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: /home/prasadm/repos/apache/hive-patches/build/ql/scratchdir/local/hive_2012-11-01_15-51-06_176_6704298995984162430/-local-10003/HashTable-Stage-1/MapJoin-b-11-srcbucket21.txt.hashtable (No such file or directory)",
                "at org.apache.hadoop.hive.ql.exec.ExecMapper.map(ExecMapper.java:161)",
                "at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:399)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:334)",
                "at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:232)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)",
                "at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:166)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)",
                "at java.lang.Thread.run(Thread.java:679)"
            ],
            "RootCause": "The root cause of the issue is a missing hashtable file during the execution of a Hive job, which is likely due to failures in the processing of input data or in the finalization of the output in the `MapOperator`. The `map` method in the `ExecMapper` class fails to create the hashtable file, possibly due to deserialization errors or incorrect job configuration.",
            "StepsToReproduce": [
                "Run the Hive job that triggers the MapJoin operation.",
                "Ensure that the input data is correctly formatted and accessible.",
                "Monitor the job execution logs for any warnings or errors related to file handling."
            ],
            "ExpectedBehavior": "The Hive job should successfully process the input data and generate the required hashtable file without any errors.",
            "ObservedBehavior": "The Hive job fails with a 'No such file or directory' error for the hashtable file, indicating that the file was not created during the job execution.",
            "Suggestions": "Check the input data for any inconsistencies or errors that could affect deserialization. Ensure that the Hive job configuration is set up correctly, including paths and parameters. Investigate the `MapOperator` class to ensure that the `closeOp` method is correctly implemented to handle output finalization.",
            "problem_location": {
                "files": [
                    "LocalJobRunner.java",
                    "ExecMapper.java"
                ],
                "classes": [
                    "org.apache.hadoop.mapred.LocalJobRunner",
                    "org.apache.hadoop.hive.ql.exec.ExecMapper",
                    "org.apache.hadoop.hive.ql.exec.MapOperator"
                ],
                "methods": [
                    "LocalJobRunner.run",
                    "ExecMapper.map",
                    "MapOperator.close"
                ]
            },
            "possible_fix": "Review the implementation of the `close` and `closeOp` methods in the `MapOperator` class to ensure they correctly finalize the output. Additionally, validate the job configuration and input data to prevent deserialization errors."
        },
        "possible_fix_code": {
            "org.apache.hadoop.hive.ql.exec.MapOperator.closeOp": "  protected void closeOp(boolean abort) throws HiveException {\n    // Ensure that the hashtable file is created and finalized correctly\n    if (!abort) {\n      // Logic to finalize the output and create the hashtable file\n      try {\n        // Assuming we have a method to write the hashtable\n        writeHashtable();\n      } catch (IOException e) {\n        throw new HiveException(\"Failed to create hashtable file\", e);\n      }\n    }\n  }"
        }
    },
    {
        "filename": "HIVE-5199.json",
        "creation_time": "2013-09-03T20:40:29.000+0000",
        "bug_report": {
            "Title": "Custom SerDe containing a nonSettable complex data type row object inspector throws cast exception with HIVE 0.11",
            "Description": "The issue arises from changes introduced in HIVE-3833, which affect how Hive handles object inspectors for complex data types. Specifically, when using different custom SerDes for partitioned tables, a mismatch occurs between the object inspectors of the table and partition levels. The table-level SerDe (customSerDe1) uses a settable data type, while the partition-level SerDe (customSerDe2) uses a non-settable data type. The current implementation fails to convert nested complex data types that extend nonSettableObjectInspector to a settableObjectInspector type in ObjectInspectorConverters.getConvertedOI(). This leads to a ClassCastException when attempting to cast a ProtoMapObjectInspector to a SettableMapObjectInspector during operations in FetchOperator and MapOperator.",
            "StackTrace": [
                "2013-08-28 17:57:25,307 ERROR CliDriver (SessionState.java:printError(432)) - Failed with exception java.io.IOException:java.lang.ClassCastException: com.skype.data.whaleshark.hadoop.hive.proto.ProtoMapObjectInspector cannot be cast to org.apache.hadoop.hive.serde2.objectinspector.SettableMapObjectInspector",
                "java.io.IOException: java.lang.ClassCastException: com.skype.data.whaleshark.hadoop.hive.proto.ProtoMapObjectInspector cannot be cast to org.apache.hadoop.hive.serde2.objectinspector.SettableMapObjectInspector",
                "at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:544)",
                "at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:488)",
                "at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:136)",
                "at org.apache.hadoop.hive.ql.Driver.getResults(Driver.java:1412)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:271)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:216)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:413)",
                "at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:756)",
                "at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:614)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:601)",
                "at org.apache.hadoop.util.RunJar.main(RunJar.java:160)",
                "Caused by: java.lang.ClassCastException: com.skype.data.whaleshark.hadoop.hive.proto.ProtoMapObjectInspector cannot be cast to org.apache.hadoop.hive.serde2.objectinspector.SettableMapObjectInspector",
                "at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters.getConverter(ObjectInspectorConverters.java:144)",
                "at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters$StructConverter.<init>(ObjectInspectorConverters.java:307)",
                "at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters.getConverter(ObjectInspectorConverters.java:138)",
                "at org.apache.hadoop.hive.ql.exec.FetchOperator.getRecordReader(FetchOperator.java:406)",
                "at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:508)"
            ],
            "RootCause": "The root cause of the issue is a ClassCastException that occurs due to a type mismatch between ProtoMapObjectInspector and SettableMapObjectInspector. This mismatch arises from the inability of the current implementation to convert non-settable complex data types to settable types, as required by the object inspector framework in Hive.",
            "StepsToReproduce": [
                "Create a partitioned table with different custom SerDes for the table and partition levels.",
                "Ensure that the table-level SerDe uses a settable data type and the partition-level SerDe uses a non-settable data type.",
                "Attempt to execute a query that involves fetching data from this partitioned table."
            ],
            "ExpectedBehavior": "The system should successfully fetch and process data from the partitioned table without throwing a ClassCastException.",
            "ObservedBehavior": "The system throws a ClassCastException when attempting to cast ProtoMapObjectInspector to SettableMapObjectInspector during data fetching operations.",
            "Suggestions": "Review the custom SerDes and ensure that the data types used are compatible with Hive's object inspector framework. Consider modifying the implementation to handle conversions between non-settable and settable object inspectors appropriately.",
            "problem_location": {
                "files": [
                    "ObjectInspectorConverters.java",
                    "FetchOperator.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.ql.exec.FetchOperator",
                    "org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters"
                ],
                "methods": [
                    "FetchOperator.getNextRow",
                    "ObjectInspectorConverters.getConverter"
                ]
            },
            "possible_fix": "Modify the ObjectInspectorConverters.getConverter method to handle conversions from non-settable to settable object inspectors, ensuring that the appropriate type checks and conversions are implemented."
        },
        "possible_fix_code": {
            "ObjectInspectorConverters.getConverter": "  public static Converter getConverter(ObjectInspector inputOI,\n      ObjectInspector outputOI) {\n    // If the inputOI is the same as the outputOI, just return an\n    // IdentityConverter.\n    if (inputOI.equals(outputOI)) {\n      return new IdentityConverter();\n    }\n    switch (outputOI.getCategory()) {\n    case PRIMITIVE:\n      return getConverter((PrimitiveObjectInspector) inputOI, (PrimitiveObjectInspector) outputOI);\n    case STRUCT:\n      return new StructConverter(inputOI,\n          (SettableStructObjectInspector) outputOI);\n    case LIST:\n      return new ListConverter(inputOI,\n          (SettableListObjectInspector) outputOI);\n    case MAP:\n      // Handle conversion from non-settable to settable MapObjectInspector\n      if (inputOI instanceof ProtoMapObjectInspector) {\n        // Implement conversion logic here to create a SettableMapObjectInspector\n        return new SettableMapConverter((ProtoMapObjectInspector) inputOI, (SettableMapObjectInspector) outputOI);\n      }\n      return new MapConverter(inputOI,\n          (SettableMapObjectInspector) outputOI);\n    default:\n      throw new RuntimeException(\"Hive internal error: conversion of \"\n          + inputOI.getTypeName() + \" to \" + outputOI.getTypeName()\n          + \" not supported yet.\");\n    }\n  }"
        }
    },
    {
        "filename": "HIVE-17368.json",
        "creation_time": "2017-08-22T01:27:32.000+0000",
        "bug_report": {
            "Title": "DBTokenStore fails to connect in Kerberos enabled remote HMS environment",
            "Description": "In environments where the Hive Metastore (HMS) operates as a remote service secured with Kerberos, the DBTokenStore fails to connect when the HS2 Thrift API attempts to call GetDelegationToken. This failure occurs because the user issuing the GetDelegationToken request may not have Kerberos authentication enabled. For instance, when Oozie submits a job on behalf of a user (e.g., 'Joe'), it uses Oozie's principal to create a proxy UserGroupInformation (UGI) with Hive. This setup allows for Kerberos-authenticated transport, but when Oozie requests a delegation token for 'Joe', the DBTokenStore cannot establish a connection to HMS due to the use of server HiveConf instead of sessionConf, leading to authentication failures.",
            "StackTrace": [
                "2017-08-21T18:07:19,644 ERROR [HiveServer2-Handler-Pool: Thread-61] transport.TSaslTransport: SASL negotiation failure",
                "javax.security.sasl.SaslException: GSS initiate failed",
                "Caused by: org.ietf.jgss.GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)",
                "2017-08-17 11:45:13,655 ERROR org.apache.thrift.server.TThreadPoolServer: [pool-7-thread-34]: Error occurred during processing of message.",
                "java.lang.RuntimeException: org.apache.thrift.transport.TTransportException: DIGEST-MD5: IO error acquiring password"
            ],
            "RootCause": "The root cause of the issue is the inability to obtain valid Kerberos credentials (Ticket Granting Ticket - TGT) for the user 'Joe', which leads to authentication failures when attempting to connect to the HMS. Additionally, there are issues with the DIGEST-MD5 authentication mechanism, indicating problems with password acquisition.",
            "StepsToReproduce": [
                "1. Configure HMS to run as a remote process with Kerberos security.",
                "2. Set up DBTokenStore as the token store.",
                "3. Submit a job through Oozie on behalf of a user who is not Kerberos enabled.",
                "4. Monitor the HS2 logs for errors related to GetDelegationToken."
            ],
            "ExpectedBehavior": "The HS2 Thrift API should successfully invoke HMS APIs to add, remove, or renew tokens from the DB without authentication errors.",
            "ObservedBehavior": "The HS2 Thrift API call GetDelegationToken fails with a SASL negotiation failure and a DIGEST-MD5 IO error, indicating issues with Kerberos authentication and password acquisition.",
            "Suggestions": "To resolve the issue, ensure that the user has valid Kerberos credentials by using the 'kinit' command. Verify the Kerberos configuration in the krb5.conf file for correctness, synchronize system clocks, and check the Service Principal Name (SPN) settings. For DIGEST-MD5 issues, ensure that the correct password is being used and that the configuration is set up properly.",
            "problem_location": {
                "files": [
                    "DBTokenStore.java",
                    "HiveMetaStoreClient.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.thrift.DBTokenStore",
                    "org.apache.hadoop.hive.metastore.HiveMetaStoreClient"
                ],
                "methods": [
                    "DBTokenStore.addToken",
                    "HiveMetaStoreClient.open"
                ]
            },
            "possible_fix": "Ensure that the DBTokenStore uses the correct session configuration for establishing the transport to HMS. Additionally, implement checks to verify the presence of valid Kerberos credentials before attempting to connect."
        },
        "possible_fix_code": {
            "DBTokenStore.addToken": "  public boolean addToken(DelegationTokenIdentifier tokenIdentifier,\n      DelegationTokenInformation token) throws TokenStoreException {\n\n    // Check for valid Kerberos credentials before proceeding\n    UserGroupInformation ugi = UserGroupInformation.getCurrentUser();\n    if (ugi == null || !ugi.hasKerberosCredentials()) {\n        throw new TokenStoreException(\"User does not have valid Kerberos credentials.\");\n    }\n\n    try {\n      String identifier = TokenStoreDelegationTokenSecretManager.encodeWritable(tokenIdentifier);\n      String tokenStr = Base64.encodeBase64URLSafeString(\n        HiveDelegationTokenSupport.encodeDelegationTokenInformation(token));\n      boolean result = (Boolean)invokeOnTokenStore(\"addToken\", new Object[] {identifier, tokenStr},\n        String.class, String.class);\n      if (LOG.isTraceEnabled()) {\n        LOG.trace(\"addToken: tokenIdentifier = \" + tokenIdentifier + \", added = \" + result);\n      }\n      return result;\n    } catch (IOException e) {\n      throw new TokenStoreException(e);\n    }\n  }"
        }
    },
    {
        "filename": "HIVE-4233.json",
        "creation_time": "2013-03-26T13:02:20.000+0000",
        "bug_report": {
            "Title": "The TGT gotten from class 'CLIService' should be renewed on time",
            "Description": "When the HiveServer2 has been running for more than 7 days, attempts to connect using the Beeline shell result in failures due to Kerberos authentication issues. The logs indicate a `java.lang.RuntimeException` caused by an inability to instantiate `HiveMetaStoreClient`, which is linked to an `IllegalStateException` stating that the Kerberos ticket is no longer valid. This suggests that the ticket may have expired without being renewed, leading to authentication failures when accessing the Hive metastore.",
            "StackTrace": [
                "2013-03-26 11:55:20,932 ERROR hive.ql.metadata.Hive: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient",
                "at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1084)",
                "at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:51)",
                "at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:61)",
                "at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2140)",
                "at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:2151)",
                "at org.apache.hadoop.hive.ql.metadata.Hive.getDelegationToken(Hive.java:2275)",
                "at org.apache.hive.service.cli.CLIService.getDelegationTokenFromMetaStore(CLIService.java:358)",
                "at org.apache.hive.service.cli.thrift.ThriftCLIService.OpenSession(ThriftCLIService.java:127)",
                "at org.apache.hive.service.cli.thrift.TCLIService$Processor$OpenSession.getResult(TCLIService.java:1073)",
                "at org.apache.hive.service.cli.thrift.TCLIService$Processor$OpenSession.getResult(TCLIService.java:1058)",
                "at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)",
                "at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)",
                "at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S$Server$TUGIAssumingProcessor.process(HadoopThriftAuthBridge20S.java:565)",
                "at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:206)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)",
                "at java.lang.Thread.run(Thread.java:662)",
                "Caused by: java.lang.reflect.InvocationTargetException",
                "at sun.reflect.GeneratedConstructorAccessor52.newInstance(Unknown Source)",
                "at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)",
                "at sun.reflect.Constructor.newInstance(Constructor.java:513)",
                "at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1082)",
                "Caused by: java.lang.IllegalStateException: This ticket is no longer valid",
                "at javax.security.auth.kerberos.KerberosTicket.toString(KerberosTicket.java:601)",
                "at java.lang.String.valueOf(String.java:2826)",
                "at java.lang.StringBuilder.append(StringBuilder.java:115)",
                "at sun.security.jgss.krb5.SubjectComber.findAux(SubjectComber.java:120)",
                "at sun.security.jgss.krb5.SubjectComber.find(SubjectComber.java:41)",
                "at sun.security.jgss.krb5.Krb5Util.getTicket(Krb5Util.java:130)",
                "at sun.security.jgss.krb5.Krb5InitCredential$1.run(Krb5InitCredential.java:328)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at sun.security.jgss.krb5.Krb5InitCredential.getTgt(Krb5InitCredential.java:325)",
                "at sun.security.jgss.krb5.Krb5InitCredential.getInstance(Krb5InitCredential.java:128)",
                "at sun.security.jgss.krb5.Krb5MechFactory.getCredentialElement(Krb5MechFactory.java:106)",
                "at sun.security.jgss.krb5.Krb5MechFactory.getMechanismContext(Krb5MechFactory.java:172)",
                "at org.apache.thrift.transport.TSaslClientTransport.handleSaslStartMessage(TSaslClientTransport.java:94)",
                "at org.apache.thrift.transport.TSaslTransport.open(TSaslTransport.java:253)",
                "at org.apache.thrift.transport.TSaslClientTransport.open(TSaslClientTransport.java:37)",
                "at org.apache.hadoop.hive.thrift.client.TUGIAssumingTransport$1.run(TUGIAssumingTransport.java:52)",
                "at org.apache.hadoop.hive.thrift.client.TUGIAssumingTransport$1.run(TUGIAssumingTransport.java:49)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:396)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1408)",
                "at org.apache.hadoop.hive.thrift.client.TUGIAssumingTransport.open(TUGIAssumingTransport.java:49)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:277)"
            ],
            "RootCause": "The root cause of the issue is an expired Kerberos ticket, which leads to an `IllegalStateException` when the `HiveMetaStoreClient` attempts to authenticate. The ticket's expiration prevents successful instantiation of the client, resulting in a failure to connect to the Hive metastore.",
            "StepsToReproduce": [
                "Start HiveServer2 and let it run for more than 7 days.",
                "Use Beeline shell to connect to HiveServer2.",
                "Attempt to perform any operation that requires authentication."
            ],
            "ExpectedBehavior": "The system should allow successful connections to HiveServer2 and enable operations without authentication failures.",
            "ObservedBehavior": "All operations fail due to Kerberos authentication errors, specifically indicating that the ticket is no longer valid.",
            "Suggestions": "Implement a mechanism to renew the Kerberos ticket (TGT) periodically to prevent expiration. This could involve scheduling a timer in the `HiveAuthFactory.loginFromKeytab` method to handle ticket renewal.",
            "problem_location": {
                "files": [
                    "metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClient.java",
                    "metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreUtils.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.metastore.HiveMetaStoreClient",
                    "org.apache.hadoop.hive.metastore.MetaStoreUtils"
                ],
                "methods": [
                    "HiveMetaStoreClient.open",
                    "MetaStoreUtils.newInstance"
                ]
            },
            "possible_fix": "Add a scheduled task in the `HiveAuthFactory.loginFromKeytab` method to renew the Kerberos ticket before it expires. This could involve using a background thread to check the ticket's validity and renew it as needed."
        },
        "possible_fix_code": {
            "HiveMetaStoreClient.open": "  private void open() throws MetaException {\n    isConnected = false;\n    TTransportException tte = null;\n    HadoopShims shim = ShimLoader.getHadoopShims();\n    boolean useSasl = conf.getBoolVar(ConfVars.METASTORE_USE_THRIFT_SASL);\n    boolean useFramedTransport = conf.getBoolVar(ConfVars.METASTORE_USE_THRIFT_FRAMED_TRANSPORT);\n    int clientSocketTimeout = conf.getIntVar(ConfVars.METASTORE_CLIENT_SOCKET_TIMEOUT);\n\n    // Schedule a task to renew the Kerberos ticket periodically\n    scheduleKerberosTicketRenewal();\n\n    for (int attempt = 0; !isConnected && attempt < retries; ++attempt) {\n      for (URI store : metastoreUris) {\n        LOG.info(\"Trying to connect to metastore with URI \" + store);\n        try {\n          transport = new TSocket(store.getHost(), store.getPort(), 1000 * clientSocketTimeout);\n          if (useSasl) {\n            // Wrap thrift connection with SASL for secure connection.\n            try {\n              HadoopThriftAuthBridge.Client authBridge =\n                ShimLoader.getHadoopThriftAuthBridge().createClient();\n\n              // check if we should use delegation tokens to authenticate\n              String tokenSig = conf.get(\"hive.metastore.token.signature\");\n              tokenStrForm = shim.getTokenStrForm(tokenSig);\n\n              if(tokenStrForm != null) {\n                transport = authBridge.createClientTransport(null, store.getHost(),\n                    \"DIGEST\", tokenStrForm, transport);\n              } else {\n                String principalConfig =\n                    conf.getVar(HiveConf.ConfVars.METASTORE_KERBEROS_PRINCIPAL);\n                transport = authBridge.createClientTransport(\n                    principalConfig, store.getHost(), \"KERBEROS\", null,\n                    transport);\n              }\n            } catch (IOException ioe) {\n              LOG.error(\"Couldn't create client transport\", ioe);\n              throw new MetaException(ioe.toString());\n            }\n          } else if (useFramedTransport) {\n            transport = new TFramedTransport(transport);\n          }\n\n          client = new ThriftHiveMetastore.Client(new TBinaryProtocol(transport));\n          try {\n            transport.open();\n            isConnected = true;\n          } catch (TTransportException e) {\n            tte = e;\n            if (LOG.isDebugEnabled()) {\n              LOG.warn(\"Failed to connect to the MetaStore Server...\", e);\n            } else {\n              LOG.warn(\"Failed to connect to the MetaStore Server...\");\n            }\n          }\n\n          if (isConnected && !useSasl && conf.getBoolVar(ConfVars.METASTORE_EXECUTE_SET_UGI)){\n            try {\n              UserGroupInformation ugi = shim.getUGIForConf(conf);\n              client.set_ugi(ugi.getUserName(), Arrays.asList(ugi.getGroupNames()));\n            } catch (LoginException e) {\n              LOG.warn(\"Failed to do login. set_ugi() is not successful, \" +\n                       \"Continuing without it.\", e);\n            } catch (IOException e) {\n              LOG.warn(\"Failed to find ugi of client set_ugi() is not successful, \" +\n                  \"Continuing without it.\", e);\n            } catch (TException e) {\n              LOG.warn(\"set_ugi() not successful, Likely cause: new client talking to old server. \"\n                  + \"Continuing without it.\", e);\n            }\n          }\n        } catch (MetaException e) {\n          LOG.error(\"Unable to connect to metastore with URI \" + store\n                    + \" in attempt \" + attempt, e);\n        }\n        if (isConnected) {\n          break;\n        }\n      }\n      if (retryDelaySeconds > 0) {\n        try {\n          LOG.info(\"Waiting \" + retryDelaySeconds + \" seconds before next connection attempt.\");\n          Thread.sleep(retryDelaySeconds * 1000);\n        } catch (InterruptedException ignore) {}\n      }\n    }\n\n    if (!isConnected) {\n      throw new MetaException(\"Could not connect to meta store using any of the URIs provided.\" +\n        \" Most recent failure: \" + StringUtils.stringifyException(tte));\n    }\n    LOG.info(\"Connected to metastore.\");\n  }\n\n  private void scheduleKerberosTicketRenewal() {\n    // Implement the logic to renew the Kerberos ticket periodically\n    // This could involve using a background thread or a scheduled executor\n    // to check the ticket's validity and renew it as needed.\n  }"
        }
    },
    {
        "filename": "HIVE-14303.json",
        "creation_time": "2016-07-21T03:16:20.000+0000",
        "bug_report": {
            "Title": "CommonJoinOperator.checkAndGenObject should return directly to avoid NPE if ExecReducer.close is called twice.",
            "Description": "The issue arises in the `CommonJoinOperator.checkAndGenObject` method, which is invoked during the execution of a Hive query. When `ExecReducer.close` is called multiple times, it leads to a `NullPointerException` (NPE) due to the improper handling of null values in the join operation. Specifically, the first call to `reducer.close()` triggers `CommonJoinOperator.closeOp`, which clears the `storage` array. This results in subsequent calls to `checkAndGenObject` encountering null references, causing the NPE. The stack trace indicates that the NPE is obscuring the original exception that occurs during the first call to `reducer.close()`.",
            "StackTrace": [
                "Error: java.lang.RuntimeException: Hive Runtime Error while closing operators: null",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.close(ExecReducer.java:296)",
                "at org.apache.hadoop.io.IOUtils.cleanup(IOUtils.java:244)",
                "at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:459)",
                "at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)",
                "at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)",
                "Caused by: java.lang.NullPointerException",
                "at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.checkAndGenObject(CommonJoinOperator.java:718)",
                "at org.apache.hadoop.hive.ql.exec.JoinOperator.endGroup(JoinOperator.java:256)",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.close(ExecReducer.java:284)"
            ],
            "RootCause": "The root cause of the NPE is the clearing of the `storage` array in `CommonJoinOperator.closeOp` during the first call to `reducer.close()`, which leads to null references when `checkAndGenObject` is called again.",
            "StepsToReproduce": [
                "1. Execute a Hive query that involves a join operation.",
                "2. Ensure that the `ExecReducer.close` method is called multiple times during the execution.",
                "3. Observe the logs for the NPE occurring in `CommonJoinOperator.checkAndGenObject`."
            ],
            "ExpectedBehavior": "The system should handle multiple calls to `ExecReducer.close` gracefully without throwing a `NullPointerException`, allowing the join operation to complete successfully.",
            "ObservedBehavior": "A `NullPointerException` is thrown in `CommonJoinOperator.checkAndGenObject` when `ExecReducer.close` is called multiple times, obscuring the original exception.",
            "Suggestions": "1. Implement data validation to ensure that input data does not contain null values before performing join operations. 2. Improve error handling in `checkAndGenObject` and `endGroup` methods to gracefully handle null values. 3. Add logging to track the values being processed in the join operation. 4. Review the join logic to ensure it correctly handles cases where join keys may be null.",
            "problem_location": {
                "files": [
                    "ql/src/java/org/apache/hadoop/hive/ql/exec/CommonJoinOperator.java",
                    "ql/src/java/org/apache/hadoop/hive/ql/exec/JoinOperator.java",
                    "ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecReducer.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.ql.exec.CommonJoinOperator",
                    "org.apache.hadoop.hive.ql.exec.JoinOperator",
                    "org.apache.hadoop.hive.ql.exec.mr.ExecReducer"
                ],
                "methods": [
                    "CommonJoinOperator.checkAndGenObject",
                    "JoinOperator.endGroup",
                    "ExecReducer.close"
                ]
            },
            "possible_fix": "To prevent the NPE, modify the `checkAndGenObject` method to return early if it detects that `storage` has been cleared or contains null values. Additionally, ensure that `ExecReducer.close` handles multiple calls without leading to state inconsistencies."
        },
        "possible_fix_code": {
            "CommonJoinOperator.checkAndGenObject": "  protected void checkAndGenObject() throws HiveException {\n    // Early return if storage is cleared or contains null values\n    for (int i = 0; i < numAliases; i++) {\n      Byte alias = order[i];\n      if (storage[alias] == null) {\n        return;\n      }\n    }\n\n    if (condn[0].getType() == JoinDesc.UNIQUE_JOIN) {\n      boolean preserve = false;\n      boolean hasNulls = false;\n      boolean allOne = true;\n      for (int i = 0; i < numAliases; i++) {\n        Byte alias = order[i];\n        AbstractRowContainer<List<Object>> alw = storage[alias];\n\n        if (!alw.isSingleRow()) {\n          allOne = false;\n        }\n\n        if (!alw.hasRows()) {\n          alw.addRow(dummyObj[i]);\n          hasNulls = true;\n        } else if (condn[i].getPreserved()) {\n          preserve = true;\n        }\n      }\n\n      if (hasNulls && !preserve) {\n        return;\n      }\n\n      if (allOne) {\n        genAllOneUniqueJoinObject();\n      } else {\n        genUniqueJoinObject(0, 0);\n      }\n    } else {\n      boolean mayHasMoreThanOne = false;\n      boolean hasEmpty = false;\n      for (int i = 0; i < numAliases; i++) {\n        Byte alias = order[i];\n        AbstractRowContainer<List<Object>> alw = storage[alias];\n\n        if (noOuterJoin) {\n          if (!alw.hasRows()) {\n            return;\n          } else if (!alw.isSingleRow()) {\n            mayHasMoreThanOne = true;\n          }\n        } else {\n          if (!alw.hasRows()) {\n            hasEmpty = true;\n            alw.addRow(dummyObj[i]);\n          } else if (!hasEmpty && alw.isSingleRow()) {\n            if (hasAnyFiltered(alias, alw.rowIter().first())) {\n              hasEmpty = true;\n            }\n          } else {\n            mayHasMoreThanOne = true;\n            if (!hasEmpty) {\n              AbstractRowContainer.RowIterator<List<Object>> iter = alw.rowIter();\n              for (List<Object> row = iter.first(); row != null; row = iter.next()) {\n                reportProgress();\n                if (hasAnyFiltered(alias, row)) {\n                  hasEmpty = true;\n                  break;\n                }\n              }\n            }\n          }\n        }\n      }\n\n      if (!hasEmpty && !mayHasMoreThanOne) {\n        genAllOneUniqueJoinObject();\n      } else if (!hasEmpty && !hasLeftSemiJoin) {\n        genUniqueJoinObject(0, 0);\n      } else {\n        genJoinObject();\n      }\n    }\n    Arrays.fill(aliasFilterTags, (byte)0xff);\n  }"
        }
    },
    {
        "filename": "HIVE-19248.json",
        "creation_time": "2018-04-19T17:45:21.000+0000",
        "bug_report": {
            "Title": "REPL LOAD couldn't copy file from source CM path and also doesn't throw error if file copy fails.",
            "Description": "The issue arises during Hive replication when using Hadoop's distcp to copy files from the primary to the replica warehouse. A failure occurs if the HDFS block sizes differ between clusters, leading to checksum mismatches. The error log indicates that the file copy operation fails due to a checksum mismatch, which can happen when the source and target files have different block sizes or if data corruption occurs during transfer. The current implementation does not throw an error when file copy fails, resulting in misleading success messages from REPL LOAD.",
            "StackTrace": [
                "2018-04-09 14:32:06,690 ERROR [main] org.apache.hadoop.tools.mapred.CopyMapper: Failure in copying hdfs://chelsea/apps/hive/warehouse/tpch_flat_orc_1000.db/customer/000259_0 to hdfs://marilyn/apps/hive/warehouse/tpch_flat_orc_1000.db/customer/.hive-staging_hive_2018-04-09_14-30-45_723_7153496419225102220-2/-ext-10001/000259_0",
                "java.io.IOException: File copy failed: hdfs://chelsea/apps/hive/warehouse/tpch_flat_orc_1000.db/customer/000259_0 --> hdfs://marilyn/apps/hive/warehouse/tpch_flat_orc_1000.db/customer/.hive-staging_hive_2018-04-09_14-30-45_723_7153496419225102220-2/-ext-10001/000259_0",
                "at org.apache.hadoop.tools.mapred.CopyMapper.copyFileWithRetry(CopyMapper.java:299)",
                "at org.apache.hadoop.tools.mapred.CopyMapper.map(CopyMapper.java:266)",
                "at org.apache.hadoop.tools.mapred.CopyMapper.map(CopyMapper.java:52)",
                "at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:146)",
                "at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:787)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)",
                "at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:170)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1869)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:164)",
                "Caused by: java.io.IOException: Couldn't run retriable-command: Copying hdfs://chelsea/apps/hive/warehouse/tpch_flat_orc_1000.db/customer/000259_0 to hdfs://marilyn/apps/hive/warehouse/tpch_flat_orc_1000.db/customer/.hive-staging_hive_2018-04-09_14-30-45_723_7153496419225102220-2/-ext-10001/000259_0",
                "at org.apache.hadoop.tools.util.RetriableCommand.execute(RetriableCommand.java:101)",
                "at org.apache.hadoop.tools.mapred.CopyMapper.copyFileWithRetry(CopyMapper.java:296)",
                "... 10 more",
                "Caused by: java.io.IOException: Check-sum mismatch between hdfs://chelsea/apps/hive/warehouse/tpch_flat_orc_1000.db/customer/000259_0 and hdfs://marilyn/apps/hive/warehouse/tpch_flat_orc_1000.db/customer/.hive-staging_hive_2018-04-09_14-30-45_723_7153496419225102220-2/-ext-10001/.distcp.tmp.attempt_1522833620762_4416_m_000000_0. Source and target differ in block-size. Use -pb to preserve block-sizes during copy. Alternatively, skip checksum-checks altogether, using -skipCrc. (NOTE: By skipping checksums, one runs the risk of masking data-corruption during file-transfer.)",
                "at org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.compareCheckSums(RetriableFileCopyCommand.java:212)",
                "at org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.doCopy(RetriableFileCopyCommand.java:130)",
                "at org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.doExecute(RetriableFileCopyCommand.java:99)",
                "at org.apache.hadoop.tools.util.RetriableCommand.execute(RetriableCommand.java:87)",
                "... 11 more"
            ],
            "RootCause": "The root cause of the issue is a checksum mismatch during the file copy operation due to differing block sizes between the source and target HDFS clusters. The current implementation does not handle copy failures correctly, leading to misleading success messages.",
            "StepsToReproduce": [
                "1. Initiate a Hive replication using REPL LOAD.",
                "2. Ensure that the source and target HDFS clusters have different block sizes.",
                "3. Monitor the logs for any file copy failures.",
                "4. Observe that REPL LOAD reports success despite the failure."
            ],
            "ExpectedBehavior": "The system should accurately report the success or failure of the file copy operation during Hive replication. If a file copy fails, an error should be thrown, and the user should be notified.",
            "ObservedBehavior": "REPL LOAD reports success even when the distcp job fails due to checksum mismatches or other copy errors.",
            "Suggestions": "Implement error handling to ensure that if a file copy fails after the maximum number of attempts, an error is thrown. Additionally, consider using the -pb option to preserve block sizes during the copy operation to avoid checksum mismatches.",
            "problem_location": {
                "files": [
                    "CopyMapper.java",
                    "RetriableFileCopyCommand.java"
                ],
                "classes": [
                    "org.apache.hadoop.tools.mapred.CopyMapper",
                    "org.apache.hadoop.tools.mapred.RetriableFileCopyCommand"
                ],
                "methods": [
                    "CopyMapper.copyFileWithRetry",
                    "RetriableFileCopyCommand.compareCheckSums"
                ]
            },
            "possible_fix": "Modify the CopyMapper and RetriableFileCopyCommand classes to include proper error handling for copy failures. Ensure that the -pb option is utilized during file copy operations to maintain consistent block sizes."
        },
        "possible_fix_code": {
            "CopyMapper.copyFileWithRetry": "public void copyFileWithRetry(Path src, Path dst) throws IOException {\n    int maxRetries = 3;\n    int attempt = 0;\n    boolean success = false;\n    while (attempt < maxRetries && !success) {\n        try {\n            // Use the -pb option to preserve block sizes during the copy operation\n            String[] cmd = new String[] {\"distcp\", \"-pb\", src.toString(), dst.toString()};\n            ProcessBuilder pb = new ProcessBuilder(cmd);\n            Process process = pb.start();\n            int exitCode = process.waitFor();\n            if (exitCode == 0) {\n                success = true;\n            } else {\n                throw new IOException(\"File copy failed with exit code: \" + exitCode);\n            }\n        } catch (IOException | InterruptedException e) {\n            attempt++;\n            if (attempt >= maxRetries) {\n                throw new IOException(\"File copy failed after maximum retries\", e);\n            }\n        }\n    }\n}"
        }
    },
    {
        "filename": "HIVE-7167.json",
        "creation_time": "2014-06-02T18:13:36.000+0000",
        "bug_report": {
            "Title": "Hive Metastore fails to start with SQLServerException",
            "Description": "The Hive Metastore fails to start when both hiveserver2 (using an embedded metastore) and hiveserver (using a remote metastore) are initiated simultaneously. This results in a connection error when attempting to launch the Hive CLI, indicating that the metastore service is running but not reachable. The error log shows a `Connection refused` message, suggesting that the Hive Metastore service may not be operational or the provided URIs are incorrect.",
            "StackTrace": [
                "Exception in thread \"main\" java.lang.RuntimeException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient",
                "Caused by: MetaException(message:Could not connect to meta store using any of the URIs provided. Most recent failure: org.apache.thrift.transport.TTransportException: java.net.ConnectException: Connection refused: connect"
            ],
            "RootCause": "The root cause of the issue is a failure to connect to the Hive Metastore, indicated by the `Connection refused` error. This typically occurs when the Hive Metastore service is not running or the URIs configured for the metastore are incorrect.",
            "StepsToReproduce": [
                "Start hiveserver2 with an embedded metastore.",
                "Start hiveserver with a remote metastore.",
                "Attempt to launch the Hive CLI."
            ],
            "ExpectedBehavior": "The Hive CLI should successfully connect to the Hive Metastore and allow for normal operations without any connection errors.",
            "ObservedBehavior": "The Hive CLI fails to connect to the Hive Metastore, resulting in a `Connection refused` error and preventing any operations from being executed.",
            "Suggestions": "1. Ensure that the Hive Metastore service is running and accessible. 2. Verify the configuration for `hive.metastore.uris` to ensure it points to the correct address and port of the Hive Metastore service. 3. Check network settings, firewall rules, or any other configurations that might prevent the connection.",
            "problem_location": {
                "files": [
                    "metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClient.java",
                    "common/src/java/org/apache/hadoop/hive/conf/HiveConf.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.metastore.HiveMetaStoreClient",
                    "org.apache.hadoop.hive.conf.HiveConf"
                ],
                "methods": [
                    "HiveMetaStoreClient.open",
                    "HiveConf.getVar"
                ]
            },
            "possible_fix": "Check and update the `hive.metastore.uris` configuration in the Hive configuration files to ensure they are correct. Additionally, confirm that the Hive Metastore service is running and accessible at the specified URIs."
        },
        "possible_fix_code": {
            "HiveMetaStoreClient.open": "  private void open() throws MetaException {\n    isConnected = false;\n    TTransportException tte = null;\n    HadoopShims shim = ShimLoader.getHadoopShims();\n    boolean useSasl = conf.getBoolVar(ConfVars.METASTORE_USE_THRIFT_SASL);\n    boolean useFramedTransport = conf.getBoolVar(ConfVars.METASTORE_USE_THRIFT_FRAMED_TRANSPORT);\n    int clientSocketTimeout = conf.getIntVar(ConfVars.METASTORE_CLIENT_SOCKET_TIMEOUT);\n\n    // Check if metastore URIs are configured\n    if (metastoreUris == null || metastoreUris.isEmpty()) {\n      throw new MetaException(\"Metastore URIs are not configured. Please check hive.metastore.uris.\");\n    }\n\n    for (int attempt = 0; !isConnected && attempt < retries; ++attempt) {\n      for (URI store : metastoreUris) {\n        LOG.info(\"Trying to connect to metastore with URI \" + store);\n        try {\n          transport = new TSocket(store.getHost(), store.getPort(), 1000 * clientSocketTimeout);\n          if (useSasl) {\n            // Wrap thrift connection with SASL for secure connection.\n            try {\n              HadoopThriftAuthBridge.Client authBridge =\n                ShimLoader.getHadoopThriftAuthBridge().createClient();\n\n              String tokenSig = conf.get(\"hive.metastore.token.signature\");\n              tokenStrForm = shim.getTokenStrForm(tokenSig);\n              if(tokenStrForm != null) {\n                transport = authBridge.createClientTransport(null, store.getHost(),\n                    \"DIGEST\", tokenStrForm, transport,\n                        MetaStoreUtils.getMetaStoreSaslProperties(conf));\n              } else {\n                String principalConfig =\n                    conf.getVar(HiveConf.ConfVars.METASTORE_KERBEROS_PRINCIPAL);\n                transport = authBridge.createClientTransport(\n                    principalConfig, store.getHost(), \"KERBEROS\", null,\n                    transport, MetaStoreUtils.getMetaStoreSaslProperties(conf));\n              }\n            } catch (IOException ioe) {\n              LOG.error(\"Couldn't create client transport\", ioe);\n              throw new MetaException(ioe.toString());\n            }\n          } else if (useFramedTransport) {\n            transport = new TFramedTransport(transport);\n          }\n\n          client = new ThriftHiveMetastore.Client(new TBinaryProtocol(transport));\n          try {\n            transport.open();\n            isConnected = true;\n          } catch (TTransportException e) {\n            tte = e;\n            if (LOG.isDebugEnabled()) {\n              LOG.warn(\"Failed to connect to the MetaStore Server...\", e);\n            } else {\n              LOG.warn(\"Failed to connect to the MetaStore Server...\");\n            }\n          }\n\n          if (isConnected && !useSasl && conf.getBoolVar(ConfVars.METASTORE_EXECUTE_SET_UGI)){\n            try {\n              UserGroupInformation ugi = shim.getUGIForConf(conf);\n              client.set_ugi(ugi.getUserName(), Arrays.asList(ugi.getGroupNames()));\n            } catch (LoginException e) {\n              LOG.warn(\"Failed to do login. set_ugi() is not successful, \" +\n                       \"Continuing without it.\", e);\n            } catch (IOException e) {\n              LOG.warn(\"Failed to find ugi of client set_ugi() is not successful, \" +\n                  \"Continuing without it.\", e);\n            } catch (TException e) {\n              LOG.warn(\"set_ugi() not successful, Likely cause: new client talking to old server. \"\n                  + \"Continuing without it.\", e);\n            }\n          }\n        } catch (MetaException e) {\n          LOG.error(\"Unable to connect to metastore with URI \" + store\n                    + \" in attempt \" + attempt, e);\n        }\n        if (isConnected) {\n          break;\n        }\n      }\n      if (!isConnected && retryDelaySeconds > 0) {\n        try {\n          LOG.info(\"Waiting \" + retryDelaySeconds + \" seconds before next connection attempt.\");\n          Thread.sleep(retryDelaySeconds * 1000);\n        } catch (InterruptedException ignore) {}\n      }\n    }\n\n    if (!isConnected) {\n      throw new MetaException(\"Could not connect to meta store using any of the URIs provided.\" +\n        \" Most recent failure: \" + StringUtils.stringifyException(tte));\n    }\n    LOG.info(\"Connected to metastore.\");\n  }"
        }
    },
    {
        "filename": "HIVE-12360.json",
        "creation_time": "2015-11-06T18:04:00.000+0000",
        "bug_report": {
            "Title": "Bad seek in uncompressed ORC with predicate pushdown",
            "Description": "An IOException occurs when reading from an uncompressed ORC file in HDP-2.3.2 while pushing down a predicate. The error message indicates that a seek operation to index position 4613 is outside the available data, suggesting potential data corruption or misalignment of the index with the data. This issue is similar to HIVE-9471, but the current setup claims to incorporate fixes from that issue.",
            "StackTrace": [
                "2015-11-06 09:48:11,873 ERROR [main]: CliDriver (SessionState.java:printError(960)) - Failed with exception java.io.IOException:java.lang.IllegalArgumentException: Seek in index to 4613 is outside of the data",
                "java.io.IOException: java.lang.IllegalArgumentException: Seek in index to 4613 is outside of the data",
                "\tat org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:508)",
                "\tat org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:415)",
                "\tat org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:140)",
                "\tat org.apache.hadoop.hive.ql.Driver.getResults(Driver.java:1672)",
                "\tat org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:233)",
                "\tat org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:165)",
                "\tat org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)",
                "\tat org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:736)",
                "\tat org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:681)",
                "\tat org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)",
                "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "\tat java.lang.reflect.Method.invoke(Method.java:601)",
                "\tat org.apache.hadoop.util.RunJar.run(RunJar.java:221)",
                "\tat org.apache.hadoop.util.RunJar.main(RunJar.java:136)",
                "\tCaused by: java.lang.IllegalArgumentException: Seek in index to 4613 is outside of the data",
                "\tat org.apache.hadoop.hive.ql.io.orc.InStream$UncompressedStream.seek(InStream.java:139)",
                "\tat org.apache.hadoop.hive.ql.io.orc.InStream$UncompressedStream.read(InStream.java:87)",
                "\tat java.io.InputStream.read(InputStream.java:102)",
                "\tat com.google.protobuf.CodedInputStream.refillBuffer(CodedInputStream.java:737)",
                "\tat com.google.protobuf.CodedInputStream.isAtEnd(CodedInputStream.java:701)",
                "\tat com.google.protobuf.CodedInputStream.readTag(CodedInputStream.java:99)",
                "\tat org.apache.hadoop.hive.ql.io.orc.OrcProto$RowIndex.<init>(OrcProto.java:7429)",
                "\tat org.apache.hadoop.hive.ql.io.orc.OrcProto$RowIndex.<init>(OrcProto.java:7393)",
                "\tat org.apache.hadoop.hive.ql.io.orc.OrcProto$RowIndex$1.parsePartialFrom(OrcProto.java:7482)",
                "\tat org.apache.hadoop.hive.ql.io.orc.OrcProto$RowIndex$1.parsePartialFrom(OrcProto.java:7477)",
                "\tat com.google.protobuf.AbstractParser.parsePartialFrom(AbstractParser.java:200)",
                "\tat com.google.protobuf.AbstractParser.parseFrom(AbstractParser.java:217)",
                "\tat com.google.protobuf.AbstractParser.parseFrom(AbstractParser.java:223)",
                "\tat com.google.protobuf.AbstractParser.parseFrom(AbstractParser.java:49)",
                "\tat org.apache.hadoop.hive.ql.io.orc.OrcProto$RowIndex.parseFrom(OrcProto.java:7593)",
                "\tat org.apache.hadoop.hive.ql.io.orc.MetadataReader.readRowIndex(MetadataReader.java:88)",
                "\tat org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.readRowIndex(RecordReaderImpl.java:1166)",
                "\tat org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.readRowIndex(RecordReaderImpl.java:1151)",
                "\tat org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.pickRowGroups(RecordReaderImpl.java:750)",
                "\tat org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.readStripe(RecordReaderImpl.java:777)",
                "\tat org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.advanceStripe(RecordReaderImpl.java:986)",
                "\tat org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.advanceToNextRow(RecordReaderImpl.java:1019)",
                "\tat org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.<init>(RecordReaderImpl.java:205)",
                "\tat org.apache.hadoop.hive.ql.io.orc.ReaderImpl.rowsOptions(ReaderImpl.java:598)",
                "\tat org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger$ReaderPair.<init>(OrcRawRecordMerger.java:183)",
                "\tat org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger$OriginalReaderPair.<init>(OrcRawRecordMerger.java:226)",
                "\tat org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.<init>(OrcRawRecordMerger.java:437)",
                "\tat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getReader(OrcInputFormat.java:1235)",
                "\tat org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getRecordReader(OrcInputFormat.java:1117)",
                "\tat org.apache.hadoop.hive.exec.FetchOperator$FetchInputFormatSplit.getRecordReader(FetchOperator.java:674)",
                "\tat org.apache.hadoop.hive.exec.FetchOperator.getRecordReader(FetchOperator.java:324)",
                "\tat org.apache.hadoop.hive.exec.FetchOperator.getNextRow(FetchOperator.java:446)"
            ],
            "RootCause": "The root cause of the issue is an IOException triggered by an IllegalArgumentException, indicating that a seek operation to index position 4613 is outside the available data. This suggests that the data may be corrupted or that the index is misaligned with the actual data in the ORC file.",
            "StepsToReproduce": [
                "Set up an environment with HDP-2.3.2.",
                "Create or obtain an uncompressed ORC file.",
                "Enable predicate pushdown by setting hive.optimize.index.filter=true.",
                "Attempt to read from the ORC file using a Hive query that triggers the FetchOperator."
            ],
            "ExpectedBehavior": "The system should successfully read the data from the ORC file without throwing an IOException, and the query should return the expected results.",
            "ObservedBehavior": "The system throws an IOException with the message indicating that the seek operation is outside of the data, preventing successful data retrieval.",
            "Suggestions": "Check the integrity of the ORC file to ensure it is not corrupted. Verify that the index is correctly aligned with the data. If the issue persists, consider disabling predicate pushdown or using a different version of Hive.",
            "problem_location": {
                "files": [
                    "FetchOperator.java",
                    "OrcInputFormat.java",
                    "OrcRecordReader.java",
                    "Reader.java",
                    "StripeInformation.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.ql.exec.FetchOperator",
                    "org.apache.hadoop.hive.ql.io.orc.OrcInputFormat",
                    "org.apache.hadoop.hive.ql.io.orc.OrcRecordReader",
                    "org.apache.hadoop.hive.ql.io.orc.Reader",
                    "org.apache.hadoop.hive.ql.io.orc.StripeInformation"
                ],
                "methods": [
                    "FetchOperator.getNextRow",
                    "OrcInputFormat.getRecordReader",
                    "OrcRecordReader.next",
                    "Reader.getStripes",
                    "StripeInformation.getOffset"
                ]
            },
            "possible_fix": "To address the issue, ensure that the ORC file is not corrupted and that the index is properly aligned. If necessary, modify the FetchOperator to handle cases where the index is out of bounds, or implement additional checks in the OrcInputFormat to validate the index before seeking."
        },
        "possible_fix_code": {
            "FetchOperator.getNextRow": "  public InspectableObject getNextRow() throws IOException {\n    try {\n      while (true) {\n        boolean opNotEOF = true;\n        if (context != null) {\n          context.resetRow();\n        }\n        if (currRecReader == null) {\n          currRecReader = getRecordReader();\n          if (currRecReader == null) {\n            return null;\n          }\n\n          headerCount = Utilities.getHeaderCount(currDesc.getTableDesc());\n          footerCount = Utilities.getFooterCount(currDesc.getTableDesc(), job);\n\n          opNotEOF = Utilities.skipHeader(currRecReader, headerCount, key, value);\n\n          if (opNotEOF && footerCount > 0) {\n            footerBuffer = new FooterBuffer();\n            opNotEOF = footerBuffer.initializeBuffer(job, currRecReader, footerCount, key, value);\n          }\n        }\n\n        if (opNotEOF && footerBuffer == null) {\n          opNotEOF = currRecReader.next(key, value);\n        }\n        if (opNotEOF && footerBuffer != null) {\n          opNotEOF = footerBuffer.updateBuffer(job, currRecReader, key, value);\n        }\n        if (opNotEOF) {\n          if (operator != null && context != null && context.inputFileChanged()) {\n            operator.cleanUpInputFileChanged();\n          }\n          if (hasVC) {\n            row[isPartitioned ? 2 : 1] =\n                MapOperator.populateVirtualColumnValues(context, vcCols, vcValues, currSerDe);\n          }\n          Object deserialized = currSerDe.deserialize(value);\n          if (ObjectConverter != null) {\n            deserialized = ObjectConverter.convert(deserialized);\n          }\n\n          if (hasVC || isPartitioned) {\n            row[0] = deserialized;\n            inspectable.o = row;\n          } else {\n            inspectable.o = deserialized;\n          }\n          inspectable.oi = currSerDe.getObjectInspector();\n          return inspectable;\n        } else {\n          currRecReader.close();\n          currRecReader = null;\n        }\n      }\n    } catch (IllegalArgumentException e) {\n      // Handle the specific case of seeking outside of data bounds\n      throw new IOException(\"Seek operation failed: \" + e.getMessage(), e);\n    } catch (Exception e) {\n      throw new IOException(e);\n    }\n  }"
        }
    },
    {
        "filename": "HIVE-13160.json",
        "creation_time": "2016-02-26T00:02:11.000+0000",
        "bug_report": {
            "Title": "HS2 unable to load UDFs on startup when HMS is not ready",
            "Description": "The issue arises when HiveServer2 (HS2) attempts to start and connect to the Hive Metastore (HMS) at the URI thrift://host-10-17-81-201.coe.cloudera.com:9083. The connection fails repeatedly, leading to a RuntimeException due to the inability to instantiate the SessionHiveMetaStoreClient. This failure prevents HS2 from registering user-defined functions (UDFs), resulting in a state where no functions are available for use. The expected behavior is for HS2 to either wait for HMS to be ready or to load functions dynamically when a Hive session is created.",
            "StackTrace": [
                "java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient",
                "at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1492)",
                "at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:64)",
                "at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:74)",
                "at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:2915)",
                "java.lang.NullPointerException",
                "at org.apache.hive.service.server.HiveServer2.stop(HiveServer2.java:283)",
                "at org.apache.hive.service.server.HiveServer2.startHiveServer2(HiveServer2.java:351)"
            ],
            "RootCause": "The root cause of the issue is a misconfiguration or missing dependencies related to the HiveConf object, which is required for instantiating the SessionHiveMetaStoreClient. The NullPointerException in HiveServer2 indicates that the initialization of the SessionHiveMetaStoreClient or its dependencies is not properly handled when HS2 starts.",
            "StepsToReproduce": [
                "Start HiveServer2 (HS2) without the Hive Metastore (HMS) being ready.",
                "Observe the logs for connection attempts to the HMS URI.",
                "Note the repeated failures to connect and the resulting RuntimeException."
            ],
            "ExpectedBehavior": "HS2 should either wait for the HMS to become available before starting or dynamically load the UDFs when a Hive session is created, ensuring that functions are available for use.",
            "ObservedBehavior": "HS2 fails to connect to the HMS, leading to a RuntimeException and a NullPointerException, resulting in no UDFs being available for use.",
            "Suggestions": "Ensure that the HiveConf is properly configured before starting HS2. Verify that all required dependencies for the SessionHiveMetaStoreClient are available. Consider implementing a mechanism to delay HS2 startup until the HMS is ready or to load UDFs dynamically upon session creation.",
            "problem_location": {
                "files": [
                    "MetaStoreUtils.java",
                    "HiveServer2.java",
                    "SessionHiveMetaStoreClient.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.metastore.MetaStoreUtils",
                    "org.apache.hadoop.hive.service.server.HiveServer2",
                    "org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient"
                ],
                "methods": [
                    "MetaStoreUtils.newInstance",
                    "Hive.createMetaStoreClient",
                    "HiveServer2.startHiveServer2"
                ]
            },
            "possible_fix": "Check the configuration of the HiveConf object and ensure that all necessary parameters are set correctly. If the HMS is not available, implement a retry mechanism or a delay before HS2 attempts to connect."
        },
        "possible_fix_code": {
            "startHiveServer2": "  private static void startHiveServer2() throws Throwable {\n    long attempts = 0, maxAttempts = 1;\n    HiveConf hiveConf = new HiveConf();\n    maxAttempts = hiveConf.getLongVar(HiveConf.ConfVars.HIVE_SERVER2_MAX_START_ATTEMPTS);\n    HiveServer2 server = null;\n    while (attempts < maxAttempts) {\n      LOG.info(\"Starting HiveServer2\");\n      try {\n        // Initialize the pool before we start the server; don't start yet.\n        TezSessionPoolManager sessionPool = null;\n        if (hiveConf.getBoolVar(ConfVars.HIVE_SERVER2_TEZ_INITIALIZE_DEFAULT_SESSIONS)) {\n          sessionPool = TezSessionPoolManager.getInstance();\n          sessionPool.setupPool(hiveConf);\n        }\n\n        // Cleanup the scratch dir before starting\n        ServerUtils.cleanUpScratchDir(hiveConf);\n        server = new HiveServer2();\n        server.init(hiveConf);\n        server.start();\n\n        try {\n          JvmPauseMonitor pauseMonitor = new JvmPauseMonitor(hiveConf);\n          pauseMonitor.start();\n        } catch (Throwable t) {\n          LOG.warn(\"Could not initiate the JvmPauseMonitor thread.\" + \" GCs and Pauses may not be \" +\n            \"warned upon.\", t);\n        }\n\n        // If we're supporting dynamic service discovery, we\u2019ll add the service uri for this\n        // HiveServer2 instance to Zookeeper as a znode.\n        if (hiveConf.getBoolVar(ConfVars.HIVE_SERVER2_SUPPORT_DYNAMIC_SERVICE_DISCOVERY)) {\n          server.addServerInstanceToZooKeeper(hiveConf);\n        }\n\n        if (sessionPool != null) {\n          sessionPool.startPool();\n        }\n\n        if (hiveConf.getVar(ConfVars.HIVE_EXECUTION_ENGINE).equals(\"spark\")) {\n          SparkSessionManagerImpl.getInstance().setup(hiveConf);\n        }\n        break;\n      } catch (Throwable throwable) {\n        if (server != null) {\n          try {\n            server.stop();\n          } catch (Throwable t) {\n            LOG.info(\"Exception caught when calling stop of HiveServer2 before retrying start\", t);\n          } finally {\n            server = null;\n          }\n        }\n        if (++attempts >= maxAttempts) {\n          throw new Error(\"Max start attempts \" + maxAttempts + \" exhausted\", throwable);\n        } else {\n          LOG.warn(\"Error starting HiveServer2 on attempt \" + attempts\n              + \", will retry in 60 seconds\", throwable);\n          try {\n            Thread.sleep(60L * 1000L);\n          } catch (InterruptedException e) {\n            Thread.currentThread().interrupt();\n          }\n        }\n      }\n    }\n  }"
        }
    },
    {
        "filename": "HIVE-12008.json",
        "creation_time": "2015-10-01T19:26:46.000+0000",
        "bug_report": {
            "Title": "Hive queries failing when using count(*) on column in view",
            "Description": "The issue arises when executing Hive queries that utilize the count(*) function on a view containing the get_json_object() UDF, lateral views, and unions. The error occurs during the configuration of a Hadoop job, specifically in the initialization of a map operator, leading to an IndexOutOfBoundsException. This problem is evident in the stack trace, which indicates that the code attempts to access an index in an ArrayList that does not exist, particularly when handling dummy operators in the ExecMapper class.",
            "StackTrace": [
                "2015-10-27 17:51:33,742 WARN [main] org.apache.hadoop.mapred.YarnChild: Exception running child : java.lang.RuntimeException: Error in configuring object",
                "at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)",
                "at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:75)",
                "at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:449)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)",
                "at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)",
                "Caused by: java.lang.reflect.InvocationTargetException",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:106)",
                "Caused by: java.lang.RuntimeException: Map operator initialization failed",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.configure(ExecMapper.java:147)",
                "Caused by: java.lang.IndexOutOfBoundsException: Index: 1, Size: 1",
                "at java.util.ArrayList.rangeCheck(ArrayList.java:635)",
                "at java.util.ArrayList.get(ArrayList.java:411)"
            ],
            "RootCause": "The root cause of the issue is an IndexOutOfBoundsException occurring in the ExecMapper.configure method, specifically when attempting to access an index in an ArrayList that does not exist. This is likely due to the getDummyParentOp() method returning an empty or improperly sized list of dummy operators during the initialization of the map operator.",
            "StepsToReproduce": [
                "1. Create a Hive view that includes the get_json_object() UDF.",
                "2. Execute a query using count(*) on the view.",
                "3. Observe the error in the logs indicating a failure during the map operator initialization."
            ],
            "ExpectedBehavior": "The query should execute successfully, returning the correct count of rows from the view without any errors.",
            "ObservedBehavior": "The query fails with an IndexOutOfBoundsException during the initialization of the map operator, preventing the count from being calculated.",
            "Suggestions": "Investigate the implementation of the getDummyParentOp() method to ensure it correctly populates the list of dummy operators. Additionally, consider adding checks to handle cases where the list may be empty or improperly sized.",
            "problem_location": {
                "files": [
                    "ExecMapper.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.ql.exec.mr.ExecMapper"
                ],
                "methods": [
                    "ExecMapper.configure"
                ]
            },
            "possible_fix": "Review and modify the ExecMapper.configure method to include validation for the dummyOps list before accessing its elements. For example, add a check to ensure the list is not empty before iterating over it."
        },
        "possible_fix_code": {
            "ExecMapper.configure": "  public void configure(JobConf job) {\n    execContext = new ExecMapperContext(job);\n    // Allocate the bean at the beginning -\n\n    isLogInfoEnabled = l4j.isInfoEnabled();\n\n    try {\n      l4j.info(\"conf classpath = \"\n          + Arrays.asList(((URLClassLoader) job.getClassLoader()).getURLs()));\n      l4j.info(\"thread classpath = \"\n          + Arrays.asList(((URLClassLoader) Thread.currentThread()\n          .getContextClassLoader()).getURLs()));\n    } catch (Exception e) {\n      l4j.info(\"cannot get classpath: \" + e.getMessage());\n    }\n\n    setDone(false);\n\n    try {\n      jc = job;\n      execContext.setJc(jc);\n\n      // create map and fetch operators\n      MapWork mrwork = Utilities.getMapWork(job);\n\n      if (mrwork.getVectorMode()) {\n        mo = new VectorMapOperator();\n      } else {\n        mo = new MapOperator();\n      }\n      mo.setConf(mrwork);\n      // initialize map operator\n      mo.initialize(job, null);\n      mo.setChildren(job);\n      l4j.info(mo.dump(0));\n      // initialize map local work\n      localWork = mrwork.getMapRedLocalWork();\n      execContext.setLocalWork(localWork);\n\n      MapredContext.init(true, new JobConf(jc));\n\n      mo.passExecContext(execContext);\n      mo.initializeLocalWork(jc);\n      mo.initializeMapOperator(jc);\n\n      if (localWork == null) {\n        return;\n      }\n\n      //The following code is for mapjoin\n      //initialize all the dummy ops\n      l4j.info(\"Initializing dummy operator\");\n      List<Operator<? extends OperatorDesc>> dummyOps = localWork.getDummyParentOp();\n      if (dummyOps != null && !dummyOps.isEmpty()) { // Added check for null or empty list\n        for (Operator<? extends OperatorDesc> dummyOp : dummyOps) {\n          dummyOp.passExecContext(execContext);\n          dummyOp.initialize(jc, null);\n        }\n      } else {\n        l4j.warn(\"No dummy operators to initialize.\"); // Log a warning if no dummy ops\n      }\n    } catch (Throwable e) {\n      abort = true;\n      if (e instanceof OutOfMemoryError) {\n        // will this be true here?\n        // Don't create a new object if we are already out of memory\n        throw (OutOfMemoryError) e;\n      } else {\n        throw new RuntimeException(\"Map operator initialization failed\", e);\n      }\n    }\n  }"
        }
    },
    {
        "filename": "HIVE-6205.json",
        "creation_time": "2014-01-15T07:34:15.000+0000",
        "bug_report": {
            "Title": "alter <table> partition column throws NPE in authorization",
            "Description": "The issue arises when executing the command to alter a table's partition column, specifically when the command is 'alter table alter_coltype partition column (dt int);'. The operation fails with a NullPointerException (NPE) during the authorization phase in the Apache Hive framework. The stack trace indicates that the error occurs in the 'doAuthorization' method of the 'Driver' class, suggesting that a null reference is being accessed, likely due to an uninitialized 'SessionState'.",
            "StackTrace": [
                "2014-01-15 15:53:40,364 ERROR ql.Driver (SessionState.java:printError(457)) - FAILED: NullPointerException null",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.hive.ql.Driver.doAuthorization(Driver.java:599)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:479)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:340)",
                "at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:996)",
                "at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1039)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:932)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:922)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:268)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:220)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:424)",
                "at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:792)",
                "at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:686)",
                "at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:625)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "at java.lang.reflect.Method.invoke(Method.java:597)",
                "at org.apache.hadoop.util.RunJar.main(RunJar.java:197)"
            ],
            "RootCause": "The NullPointerException in the 'doAuthorization' method is likely caused by an uninitialized 'SessionState'. This can occur if 'SessionState.start()' has not been called before invoking 'doAuthorization', leading to 'SessionState.get()' returning null.",
            "StepsToReproduce": [
                "1. Execute the command: 'alter table alter_coltype partition column (dt int);'",
                "2. Observe the error message in the logs indicating a NullPointerException."
            ],
            "ExpectedBehavior": "The command should successfully alter the partition column without throwing any exceptions.",
            "ObservedBehavior": "The command fails with a NullPointerException during the authorization process, indicating that a required object is not initialized.",
            "Suggestions": "Ensure that 'SessionState.start()' is called before any authorization checks are performed. Additionally, verify that all necessary objects (like BaseSemanticAnalyzer and HiveOperation) are properly initialized before invoking 'doAuthorization'.",
            "problem_location": {
                "files": [
                    "ql/src/java/org/apache/hadoop/hive/ql/Driver.java",
                    "ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.ql.Driver",
                    "org.apache.hadoop.hive.ql.session.SessionState"
                ],
                "methods": [
                    "Driver.doAuthorization",
                    "SessionState.get",
                    "SessionState.start"
                ]
            },
            "possible_fix": "Add a check to ensure that 'SessionState.start()' is called before 'doAuthorization' is invoked. This can be done by modifying the code to include a validation step that initializes the session state if it is not already initialized."
        },
        "possible_fix_code": {
            "doAuthorization": "  private void doAuthorization(BaseSemanticAnalyzer sem)\n    throws HiveException, AuthorizationException {\n    // Ensure that SessionState is initialized before proceeding\n    if (SessionState.get() == null) {\n        throw new HiveException(\"SessionState is not initialized. Please call SessionState.start() before authorization.\");\n    }\n\n    HashSet<ReadEntity> inputs = sem.getInputs();\n    HashSet<WriteEntity> outputs = sem.getOutputs();\n    SessionState ss = SessionState.get();\n    HiveOperation op = ss.getHiveOperation();\n    Hive db = sem.getDb();\n    if (op != null) {\n      if (op.equals(HiveOperation.CREATEDATABASE)) {\n        ss.getAuthorizer().authorize(\n            op.getInputRequiredPrivileges(), op.getOutputRequiredPrivileges());\n      } else if (op.equals(HiveOperation.CREATETABLE_AS_SELECT)\n          || op.equals(HiveOperation.CREATETABLE)) {\n        ss.getAuthorizer().authorize(\n            db.getDatabase(SessionState.get().getCurrentDatabase()), null,\n            HiveOperation.CREATETABLE_AS_SELECT.getOutputRequiredPrivileges());\n      } else {\n        if (op.equals(HiveOperation.IMPORT)) {\n          ImportSemanticAnalyzer isa = (ImportSemanticAnalyzer) sem;\n          if (!isa.existsTable()) {\n            ss.getAuthorizer().authorize(\n                db.getDatabase(SessionState.get().getCurrentDatabase()), null,\n                HiveOperation.CREATETABLE_AS_SELECT.getOutputRequiredPrivileges());\n          }\n        }\n      }\n      if (outputs != null && outputs.size() > 0) {\n        for (WriteEntity write : outputs) {\n          if (write.getType() == Entity.Type.DATABASE) {\n            ss.getAuthorizer().authorize(write.getDatabase(),\n                null, op.getOutputRequiredPrivileges());\n            continue;\n          }\n\n          if (write.getType() == WriteEntity.Type.PARTITION) {\n            Partition part = db.getPartition(write.getTable(), write\n                .getPartition().getSpec(), false);\n            if (part != null) {\n              ss.getAuthorizer().authorize(write.getPartition(), null,\n                      op.getOutputRequiredPrivileges());\n              continue;\n            }\n          }\n\n          if (write.getTable() != null) {\n            ss.getAuthorizer().authorize(write.getTable(), null,\n                    op.getOutputRequiredPrivileges());\n          }\n        }\n\n      }\n    }\n\n    if (inputs != null && inputs.size() > 0) {\n\n      Map<Table, List<String>> tab2Cols = new HashMap<Table, List<String>>();\n      Map<Partition, List<String>> part2Cols = new HashMap<Partition, List<String>>();\n\n      Map<String, Boolean> tableUsePartLevelAuth = new HashMap<String, Boolean>();\n      for (ReadEntity read : inputs) {\n        if (read.getType() == Entity.Type.DATABASE) {\n          continue;\n        }\n        Table tbl = read.getTable();\n        if ((read.getPartition() != null) || (tbl.isPartitioned())) {\n          String tblName = tbl.getTableName();\n          if (tableUsePartLevelAuth.get(tblName) == null) {\n            boolean usePartLevelPriv = (tbl.getParameters().get(\n                \"PARTITION_LEVEL_PRIVILEGE\") != null && (\"TRUE\"\n                .equalsIgnoreCase(tbl.getParameters().get(\n                    \"PARTITION_LEVEL_PRIVILEGE\"))));\n            if (usePartLevelPriv) {\n              tableUsePartLevelAuth.put(tblName, Boolean.TRUE);\n            } else {\n              tableUsePartLevelAuth.put(tblName, Boolean.FALSE);\n            }\n          }\n        }\n      }\n\n      if (op.equals(HiveOperation.CREATETABLE_AS_SELECT)\n          || op.equals(HiveOperation.QUERY)) {\n        SemanticAnalyzer querySem = (SemanticAnalyzer) sem;\n        ParseContext parseCtx = querySem.getParseContext();\n        Map<TableScanOperator, Table> tsoTopMap = parseCtx.getTopToTable();\n\n        for (Map.Entry<String, Operator<? extends OperatorDesc>> topOpMap : querySem\n            .getParseContext().getTopOps().entrySet()) {\n          Operator<? extends OperatorDesc> topOp = topOpMap.getValue();\n          if (topOp instanceof TableScanOperator\n              && tsoTopMap.containsKey(topOp)) {\n            TableScanOperator tableScanOp = (TableScanOperator) topOp;\n            Table tbl = tsoTopMap.get(tableScanOp);\n            List<Integer> neededColumnIds = tableScanOp.getNeededColumnIDs();\n            List<FieldSchema> columns = tbl.getCols();\n            List<String> cols = new ArrayList<String>();\n            for (int i = 0; i < neededColumnIds.size(); i++) {\n              cols.add(columns.get(neededColumnIds.get(i)).getName());\n            }\n            //map may not contain all sources, since input list may have been optimized out\n            //or non-existent tho such sources may still be referenced by the TableScanOperator\n            //if it's null then the partition probably doesn't exist so let's use table permission\n            if (tbl.isPartitioned() &&\n                tableUsePartLevelAuth.get(tbl.getTableName()) == Boolean.TRUE) {\n              String alias_id = topOpMap.getKey();\n\n              PrunedPartitionList partsList = PartitionPruner.prune(tableScanOp,\n                  parseCtx, alias_id);\n              Set<Partition> parts = partsList.getPartitions();\n              for (Partition part : parts) {\n                List<String> existingCols = part2Cols.get(part);\n                if (existingCols == null) {\n                  existingCols = new ArrayList<String>();\n                }\n                existingCols.addAll(cols);\n                part2Cols.put(part, existingCols);\n              }\n            } else {\n              List<String> existingCols = tab2Cols.get(tbl);\n              if (existingCols == null) {\n                existingCols = new ArrayList<String>();\n              }\n              existingCols.addAll(cols);\n              tab2Cols.put(tbl, existingCols);\n            }\n          }\n        }\n      }\n\n      // cache the results for table authorization\n      Set<String> tableAuthChecked = new HashSet<String>();\n      for (ReadEntity read : inputs) {\n        if (read.getType() == Entity.Type.DATABASE) {\n          ss.getAuthorizer().authorize(read.getDatabase(), op.getInputRequiredPrivileges(), null);\n          continue;\n        }\n        Table tbl = read.getTable();\n        if (read.getPartition() != null) {\n          Partition partition = read.getPartition();\n          tbl = partition.getTable();\n          // use partition level authorization\n          if (tableUsePartLevelAuth.get(tbl.getTableName()) == Boolean.TRUE) {\n            List<String> cols = part2Cols.get(partition);\n            if (cols != null && cols.size() > 0) {\n              ss.getAuthorizer().authorize(partition.getTable(),\n                  partition, cols, op.getInputRequiredPrivileges(),\n                  null);\n            } else {\n              ss.getAuthorizer().authorize(partition,\n                  op.getInputRequiredPrivileges(), null);\n            }\n            continue;\n          }\n        }\n\n        // if we reach here, it means it needs to do a table authorization\n        // check, and the table authorization may already happened because of other\n        // partitions\n        if (tbl != null && !tableAuthChecked.contains(tbl.getTableName()) &&\n            !(tableUsePartLevelAuth.get(tbl.getTableName()) == Boolean.TRUE)) {\n          List<String> cols = tab2Cols.get(tbl);\n          if (cols != null && cols.size() > 0) {\n            ss.getAuthorizer().authorize(tbl, null, cols,\n                op.getInputRequiredPrivileges(), null);\n          } else {\n            ss.getAuthorizer().authorize(tbl, op.getInputRequiredPrivileges(),\n                null);\n          }\n          tableAuthChecked.add(tbl.getTableName());\n        }\n      }\n\n    }\n  }"
        }
    },
    {
        "filename": "HIVE-15309.json",
        "creation_time": "2016-11-29T21:56:28.000+0000",
        "bug_report": {
            "Title": "RemoteException(java.io.FileNotFoundException): File does not exist... _flush_length",
            "Description": "The application encounters a `FileNotFoundException` when attempting to access a specific file in HDFS. The method `OrcRawRecordMerger.getLastFlushLength()` is responsible for retrieving the last flush length from a file that appears to be missing. This results in unnecessary and confusing logging messages. The error indicates that the file at the specified path does not exist, which could be due to various reasons such as incorrect file path, file deletion, or permission issues.",
            "StackTrace": [
                "org.apache.hadoop.ipc.RemoteException(java.io.FileNotFoundException): File does not exist: /domains/adl/rrslog/data_history/rrslog/rslog/hot/server_date=2016-08-19/delta_0005913_0005913/bucket_00023_flush_length",
                "at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:71)",
                "at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsInt(FSNamesystem.java:1860)",
                "at org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:1223)",
                "at org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger.getLastFlushLength(OrcRawRecordMerger.java:513)"
            ],
            "RootCause": "The root cause of the issue is that the application is attempting to access a file in HDFS that does not exist at the specified path. This is confirmed by the `FileNotFoundException` in the stack trace.",
            "StepsToReproduce": [
                "Attempt to call OrcRawRecordMerger.getLastFlushLength() with the specified file path.",
                "Ensure that the file path is set to '/domains/adl/rrslog/data_history/rrslog/rslog/hot/server_date=2016-08-19/delta_0005913_0005913/bucket_00023_flush_length'.",
                "Observe the logging output for the FileNotFoundException."
            ],
            "ExpectedBehavior": "The application should successfully retrieve the last flush length from the specified file without encountering a FileNotFoundException.",
            "ObservedBehavior": "The application throws a FileNotFoundException indicating that the file does not exist, leading to confusing log messages.",
            "Suggestions": "Verify the file path for correctness, check if the file exists using HDFS commands, ensure that the file has not been deleted or moved, and confirm that the application has the necessary permissions to access the file.",
            "problem_location": {
                "files": [
                    "OrcRawRecordMerger.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.ql.io.orc.OrcRawRecordMerger"
                ],
                "methods": [
                    "getLastFlushLength"
                ]
            },
            "possible_fix": "Implement a check in OrcRawRecordMerger.getLastFlushLength() to verify the existence of the file before attempting to access it. This can be done using HDFS API methods to check file existence."
        },
        "possible_fix_code": {
            "getLastFlushLength": "public long getLastFlushLength(Path filePath) throws IOException {\n    // Check if the file exists before attempting to access it\n    FileSystem fs = filePath.getFileSystem(new Configuration());\n    if (!fs.exists(filePath)) {\n        throw new FileNotFoundException(\"File does not exist: \" + filePath.toString());\n    }\n\n    // Proceed to get the last flush length if the file exists\n    // Assuming there is a method to read the flush length from the file\n    // This is a placeholder for the actual implementation\n    long flushLength = readFlushLengthFromFile(filePath);\n    return flushLength;\n}"
        }
    },
    {
        "filename": "HIVE-10808.json",
        "creation_time": "2015-05-23T02:24:16.000+0000",
        "bug_report": {
            "Title": "Inner join on Null throwing Cast Exception",
            "Description": "The issue arises during the execution of a Hive query that performs an inner join on a table with a subquery. The query attempts to select columns from 'tab1' while joining with a derived table that selects the maximum value of 'x' from 'tab1' where 'x' is less than a specific date. The failure occurs due to a ClassCastException related to object inspectors during the configuration of the MapOperator in the Hadoop job.",
            "StackTrace": [
                "2015-05-18 19:22:17,372 INFO [main] org.apache.hadoop.hive.ql.exec.mr.ObjectCache: Ignoring retrieval request: __MAP_PLAN__",
                "2015-05-18 19:22:17,372 INFO [main] org.apache.hadoop.hive.ql.exec.mr.ObjectCache: Ignoring cache key: __MAP_PLAN__",
                "2015-05-18 19:22:17,457 WARN [main] org.apache.hadoop.mapred.YarnChild: Exception running child : java.lang.RuntimeException: Error in configuring object",
                "at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)",
                "at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:75)",
                "at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:446)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)",
                "at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1642)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:163)",
                "Caused by: java.lang.reflect.InvocationTargetException",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(Native Method)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:106)",
                "... 9 more",
                "Caused by: java.lang.RuntimeException: Error in configuring object",
                "at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109)",
                "at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:75)",
                "at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:133)",
                "at org.apache.hadoop.mapred.MapRunner.configure(MapRunner.java:38)",
                "... 14 more",
                "Caused by: java.lang.reflect.InvocationTargetException",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(Native Method)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:106)",
                "... 17 more",
                "Caused by: java.lang.RuntimeException: Map operator initialization failed",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.configure(ExecMapper.java:157)",
                "... 22 more",
                "Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.ClassCastException: org.apache.hadoop.hive.serde2.NullStructSerDe$NullStructSerDeObjectInspector cannot be cast to org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector",
                "at org.apache.hadoop.hive.ql.exec.MapOperator.getConvertedOI(MapOperator.java:334)",
                "at org.apache.hadoop.hive.ql.exec.MapOperator.setChildren(MapOperator.java:352)",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.configure(ExecMapper.java:126)",
                "... 22 more",
                "Caused by: java.lang.ClassCastException: org.apache.hadoop.hive.serde2.NullStructSerDe$NullStructSerDeObjectInspector cannot be cast to org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector",
                "at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.isInstanceOfSettableOI(ObjectInspectorUtils.java:1111)",
                "at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.hasAllFieldsSettable(ObjectInspectorUtils.java:1149)",
                "at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters.getConvertedOI(ObjectInspectorConverters.java:219)",
                "at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters.getConvertedOI(ObjectInspectorConverters.java:183)",
                "at org.apache.hadoop.hive.ql.exec.MapOperator.getConvertedOI(MapOperator.java:316)"
            ],
            "RootCause": "The root cause of the issue is a ClassCastException occurring due to a type mismatch where a NullStructSerDe$NullStructSerDeObjectInspector is being incorrectly cast to a PrimitiveObjectInspector. This indicates a problem in the configuration of the MapOperator and its associated object inspectors.",
            "StepsToReproduce": [
                "Execute the provided Hive query that performs an inner join on 'tab1' with a subquery.",
                "Ensure that the data in 'tab1' includes null values that would trigger the NullStructSerDe.",
                "Observe the execution logs for the ClassCastException."
            ],
            "ExpectedBehavior": "The query should execute successfully, returning the expected result set without any exceptions.",
            "ObservedBehavior": "The query fails with a ClassCastException during the configuration of the MapOperator, preventing successful execution.",
            "Suggestions": "Review the configuration of the MapOperator and ensure that the data types and object inspectors being used are compatible. Specifically, check the use of NullStructSerDe to ensure it aligns with the expected data types.",
            "problem_location": {
                "files": [
                    "ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecMapper.java",
                    "ql/src/java/org/apache/hadoop/hive/ql/exec/MapOperator.java",
                    "serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/ObjectInspectorConverters.java",
                    "serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/ObjectInspectorUtils.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.ql.exec.mr.ExecMapper",
                    "org.apache.hadoop.hive.ql.exec.MapOperator",
                    "org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters",
                    "org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils"
                ],
                "methods": [
                    "ExecMapper.configure",
                    "MapOperator.getConvertedOI",
                    "ObjectInspectorConverters.getConvertedOI",
                    "ObjectInspectorUtils.hasAllFieldsSettable"
                ]
            },
            "possible_fix": "Ensure that the data types and object inspectors used in the job configuration are compatible. Specifically, review the use of NullStructSerDe and ensure it is correctly set up to avoid casting issues."
        },
        "possible_fix_code": {
            "ExecMapper.configure": "  public void configure(JobConf job) {\n    execContext = new ExecMapperContext(job);\n    // Allocate the bean at the beginning -\n\n    isLogInfoEnabled = l4j.isInfoEnabled();\n\n    try {\n      l4j.info(\"conf classpath = \"\n          + Arrays.asList(((URLClassLoader) job.getClassLoader()).getURLs()));\n      l4j.info(\"thread classpath = \"\n          + Arrays.asList(((URLClassLoader) Thread.currentThread()\n          .getContextClassLoader()).getURLs()));\n    } catch (Exception e) {\n      l4j.info(\"cannot get classpath: \" + e.getMessage());\n    }\n\n    setDone(false);\n\n    try {\n      jc = job;\n      execContext.setJc(jc);\n\n      // create map and fetch operators\n      MapWork mrwork = Utilities.getMapWork(job);\n\n      if (mrwork.getVectorMode()) {\n        mo = new VectorMapOperator();\n      } else {\n        mo = new MapOperator();\n      }\n      mo.setConf(mrwork);\n      // initialize map operator\n      mo.initialize(job, null);\n      mo.setChildren(job);\n      l4j.info(mo.dump(0));\n      // initialize map local work\n      localWork = mrwork.getMapRedLocalWork();\n      execContext.setLocalWork(localWork);\n\n      MapredContext.init(true, new JobConf(jc));\n\n      mo.passExecContext(execContext);\n      mo.initializeLocalWork(jc);\n      mo.initializeMapOperator(jc);\n\n      if (localWork == null) {\n        return;\n      }\n\n      //The following code is for mapjoin\n      //initialize all the dummy ops\n      l4j.info(\"Initializing dummy operator\");\n      List<Operator<? extends OperatorDesc>> dummyOps = localWork.getDummyParentOp();\n      for (Operator<? extends OperatorDesc> dummyOp : dummyOps){\n        dummyOp.passExecContext(execContext);\n        dummyOp.initialize(jc,null);\n      }\n    } catch (Throwable e) {\n      abort = true;\n      if (e instanceof OutOfMemoryError) {\n        // will this be true here?\n        // Don't create a new object if we are already out of memory\n        throw (OutOfMemoryError) e;\n      } else {\n        // Check for ClassCastException and handle it\n        if (e.getCause() instanceof ClassCastException) {\n          throw new RuntimeException(\"Map operator initialization failed due to type mismatch\", e);\n        }\n        throw new RuntimeException(\"Map operator initialization failed\", e);\n      }\n    }\n  }"
        }
    },
    {
        "filename": "HIVE-18429.json",
        "creation_time": "2018-01-10T20:45:15.000+0000",
        "bug_report": {
            "Title": "Compaction should handle a case when it produces no output",
            "Description": "When the compaction process is initiated with empty delta files (delta_8_8 and delta_9_9), it currently results in a MapReduce job that has zero splits. Consequently, the temporary location defined by {{CompactorMR.TMP_LOCATION}} is never created. This absence leads to a failure in the {{CompactorOutputCommitted.commitJob()}} method, which attempts to list the contents of a non-existent temporary location, resulting in a FileNotFoundException. If the compactor fails to produce a new delta (delta_8_9), further compaction cannot proceed unless new data is introduced. Additionally, if the number of empty deltas exceeds the configured limit (HiveConf.ConfVars.COMPACTOR_MAX_NUM_DELTA), the compaction process is entirely halted. The expected behavior is for the compactor to generate an empty delta (delta_8_9) in such scenarios.",
            "StackTrace": [
                "2017-12-27 17:19:28,850 ERROR CommitterEvent Processor #1 org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler: Could not commit job",
                "java.io.FileNotFoundException: File hdfs://OTCHaaS/apps/hive/warehouse/momi.db/sensor_data/babyid=5911806ebf69640100004257/_tmp_b4c5a3f3-44e5-4d45-86af-5b773bf0fc96 does not exist.",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.listStatusInternal(DistributedFileSystem.java:923)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.access$600(DistributedFileSystem.java:114)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:985)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:981)",
                "at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.listStatus(DistributedFileSystem.java:992)",
                "at org.apache.hadoop.hive.ql.txn.compactor.CompactorMR$CompactorOutputCommitter.commitJob(CompactorMR.java:785)",
                "at org.apache.hadoop.mapred.OutputCommitter.commitJob(OutputCommitter.java:291)",
                "at org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler$EventProcessor.handleJobCommit(CommitterEventHandler.java:285)",
                "at org.apache.hadoop.mapreduce.v2.app.commit.CommitterEventHandler$EventProcessor.run(CommitterEventHandler.java:237)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "RootCause": "The root cause of the issue is that the compaction process does not create the temporary location (TMP_LOCATION) when there are no input splits, leading to a FileNotFoundException during the commit phase. This occurs because the commitJob method attempts to list the contents of a non-existent directory.",
            "StepsToReproduce": [
                "1. Start with empty delta files (delta_8_8 and delta_9_9).",
                "2. Initiate the compaction process.",
                "3. Observe the logs for the FileNotFoundException related to the temporary location."
            ],
            "ExpectedBehavior": "The compactor should create an empty delta (delta_8_9) even when there are no records to process, allowing the compaction process to continue without errors.",
            "ObservedBehavior": "The compaction process fails with a FileNotFoundException because the temporary location does not exist, preventing further compaction from occurring.",
            "Suggestions": "Implement a check in the compaction logic to ensure that an empty delta is created when no records are available. This can be done by modifying the createBaseJobConf method to handle cases where there are no input splits.",
            "problem_location": {
                "files": [
                    "CompactorMR.java",
                    "CompactorOutputCommitter.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.ql.txn.compactor.CompactorMR",
                    "org.apache.hadoop.hive.ql.txn.compactor.CompactorOutputCommitter"
                ],
                "methods": [
                    "CompactorMR.commitJob",
                    "CompactorMR.createBaseJobConf"
                ]
            },
            "possible_fix": "In the createBaseJobConf method, add logic to create an empty delta if there are no input splits. This could involve checking the input directories and ensuring that TMP_LOCATION is set up correctly even when no data is present."
        },
        "possible_fix_code": {
            "CompactorMR.createBaseJobConf": "  private JobConf createBaseJobConf(HiveConf conf, String jobName, Table t, StorageDescriptor sd,\n                                    ValidTxnList txns, CompactionInfo ci) {\n    JobConf job = new JobConf(conf);\n    job.setJobName(jobName);\n    job.setOutputKeyClass(NullWritable.class);\n    job.setOutputValueClass(NullWritable.class);\n    job.setJarByClass(CompactorMR.class);\n    LOG.debug(\"User jar set to \" + job.getJar());\n    job.setMapperClass(CompactorMap.class);\n    job.setNumReduceTasks(0);\n    job.setInputFormat(CompactorInputFormat.class);\n    job.setOutputFormat(NullOutputFormat.class);\n    job.setOutputCommitter(CompactorOutputCommitter.class);\n\n    String queueName = conf.getVar(HiveConf.ConfVars.COMPACTOR_JOB_QUEUE);\n    if(queueName != null && queueName.length() > 0) {\n      job.setQueueName(queueName);\n    }\n\n    job.set(FINAL_LOCATION, sd.getLocation());\n    job.set(TMP_LOCATION, sd.getLocation() + \"/\" + TMPDIR + \"_\" + UUID.randomUUID().toString());\n    job.set(INPUT_FORMAT_CLASS_NAME, sd.getInputFormat());\n    job.set(OUTPUT_FORMAT_CLASS_NAME, sd.getOutputFormat());\n    job.setBoolean(IS_COMPRESSED, sd.isCompressed());\n    job.set(TABLE_PROPS, new StringableMap(t.getParameters()).toString());\n    job.setInt(NUM_BUCKETS, sd.getNumBuckets());\n    job.set(ValidTxnList.VALID_TXNS_KEY, txns.toString());\n    overrideMRProps(job, t.getParameters()); // override MR properties from tblproperties if applicable\n    if (ci.properties != null) {\n      overrideTblProps(job, t.getParameters(), ci.properties);\n    }\n    setColumnTypes(job, sd.getCols());\n\n    // Check if there are no input splits and create an empty delta if necessary\n    if (job.getInt(NUM_BUCKETS, 0) == 0) {\n        LOG.warn(\"No input splits found, creating an empty delta.\");\n        // Logic to create an empty delta file\n        createEmptyDeltaFile(job);\n    }\n\n    //with feature on, multiple tasks may get into conflict creating/using TMP_LOCATION and if we were\n    //to generate the target dir in the Map task, there is no easy way to pass it to OutputCommitter\n    //to do the final move\n    job.setBoolean(\"mapreduce.map.speculative\", false);\n\n    // Set appropriate Acid readers/writers based on the table properties.\n    AcidUtils.setAcidOperationalProperties(job,\n            AcidUtils.getAcidOperationalProperties(t.getParameters()));\n\n    return job;\n  }"
        }
    },
    {
        "filename": "HIVE-10776.json",
        "creation_time": "2015-05-21T00:56:28.000+0000",
        "bug_report": {
            "Title": "Schema on insert for bucketed tables throwing NullPointerException",
            "Description": "When executing Hive schema on insert queries with 'select *', a NullPointerException is thrown. The stack trace indicates that the error occurs within the SemanticAnalyzer class, specifically in the methods responsible for generating the Reduce Sink and handling bucketing and sorting operations. This issue arises when the code attempts to access uninitialized objects or parameters, leading to the failure of the query execution.",
            "StackTrace": [
                "2015-05-15 19:29:01,278 ERROR [main]: ql.Driver (SessionState.java:printError(957)) - FAILED: NullPointerException null",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genReduceSinkPlan(SemanticAnalyzer.java:7257)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genBucketingSortingDest(SemanticAnalyzer.java:6100)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genFileSinkPlan(SemanticAnalyzer.java:6271)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPostGroupByBodyPlan(SemanticAnalyzer.java:8972)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genBodyPlan(SemanticAnalyzer.java:8863)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:9708)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:9601)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genOPTree(SemanticAnalyzer.java:10037)",
                "at org.apache.hadoop.hive.ql.parse.CalcitePlanner.genOPTree(CalcitePlanner.java:323)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10048)",
                "at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:207)",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:227)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:424)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:308)",
                "at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1122)",
                "at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1170)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1059)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1049)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:213)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:165)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:311)",
                "at org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:409)",
                "at org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:425)",
                "at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:714)",
                "at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:681)",
                "at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.util.RunJar.run(RunJar.java:221)",
                "at org.apache.hadoop.util.RunJar.main(RunJar.java:136)"
            ],
            "RootCause": "The NullPointerException is likely caused by uninitialized objects or parameters in the 'genReduceSinkPlan' or 'genBucketingSortingDest' methods of the SemanticAnalyzer class. This can occur if the input operator, row resolver, or other critical components are not properly initialized before being accessed.",
            "StepsToReproduce": [
                "set hive.support.concurrency=true;",
                "set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;",
                "set hive.enforce.bucketing=true;",
                "drop table if exists studenttab10k;",
                "create table studenttab10k (age int, name varchar(50), gpa decimal(3,2));",
                "insert into studenttab10k values(1,'foo', 1.1), (2,'bar', 2.3), (3,'baz', 3.1);",
                "drop table if exists student_acid;",
                "create table student_acid (age int, name varchar(50), gpa decimal(3,2), grade int) clustered by (age) into 2 buckets stored as orc tblproperties ('transactional'='true');",
                "insert into student_acid(name, age, gpa) select * from studenttab10k;"
            ],
            "ExpectedBehavior": "The insert operation should successfully copy data from 'studenttab10k' to 'student_acid' without throwing any exceptions.",
            "ObservedBehavior": "The operation fails with a NullPointerException, preventing the data from being inserted into the 'student_acid' table.",
            "Suggestions": "Ensure that all necessary objects and parameters are properly initialized before being accessed in the 'genReduceSinkPlan' and 'genBucketingSortingDest' methods. Review the configuration settings related to bucketing and sorting to ensure they are correctly set.",
            "problem_location": {
                "files": [
                    "ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.ql.parse.SemanticAnalyzer"
                ],
                "methods": [
                    "genReduceSinkPlan",
                    "genBucketingSortingDest"
                ]
            },
            "possible_fix": "Review the initialization logic in 'genReduceSinkPlan' and 'genBucketingSortingDest' methods to ensure that all required objects (like input operators and row resolvers) are properly instantiated before use. Add null checks where necessary to prevent NullPointerExceptions."
        },
        "possible_fix_code": {
            "genReduceSinkPlan": "  private Operator genReduceSinkPlan(Operator<?> input,\n      ArrayList<ExprNodeDesc> partitionCols, ArrayList<ExprNodeDesc> sortCols,\n      String sortOrder, int numReducers, AcidUtils.Operation acidOp) throws SemanticException {\n\n    if (input == null) {\n        throw new SemanticException(\"Input operator cannot be null\");\n    }\n\n    RowResolver inputRR = opParseCtx.get(input).getRowResolver();\n    if (inputRR == null) {\n        throw new SemanticException(\"Row resolver cannot be null for the input operator\");\n    }\n\n    Operator dummy = Operator.createDummy();\n    dummy.setParentOperators(Arrays.asList(input));\n\n    ArrayList<ExprNodeDesc> sortColsBack = new ArrayList<ExprNodeDesc>();\n    for (ExprNodeDesc sortCol : sortCols) {\n      sortColsBack.add(ExprNodeDescUtils.backtrack(sortCol, dummy, input));\n    }\n    // For the generation of the values expression just get the inputs\n    // signature and generate field expressions for those\n    RowResolver rsRR = new RowResolver();\n    ArrayList<String> outputColumns = new ArrayList<String>();\n    ArrayList<ExprNodeDesc> valueCols = new ArrayList<ExprNodeDesc>();\n    ArrayList<ExprNodeDesc> valueColsBack = new ArrayList<ExprNodeDesc>();\n    Map<String, ExprNodeDesc> colExprMap = new HashMap<String, ExprNodeDesc>();\n\n    ArrayList<ColumnInfo> columnInfos = inputRR.getColumnInfos();\n    if (columnInfos == null) {\n        throw new SemanticException(\"Column infos cannot be null\");\n    }\n\n    int[] index = new int[columnInfos.size()];\n    for (int i = 0; i < index.length; i++) {\n      ColumnInfo colInfo = columnInfos.get(i);\n      String[] nm = inputRR.reverseLookup(colInfo.getInternalName());\n      String[] nm2 = inputRR.getAlternateMappings(colInfo.getInternalName());\n      ExprNodeColumnDesc value = new ExprNodeColumnDesc(colInfo);\n\n      // backtrack can be null when input is script operator\n      ExprNodeDesc valueBack = ExprNodeDescUtils.backtrack(value, dummy, input);\n      int kindex = valueBack == null ? -1 : ExprNodeDescUtils.indexOf(valueBack, sortColsBack);\n      if (kindex >= 0) {\n        index[i] = kindex;\n        ColumnInfo newColInfo = new ColumnInfo(colInfo);\n        newColInfo.setInternalName(Utilities.ReduceField.KEY + \".reducesinkkey\" + kindex);\n        newColInfo.setTabAlias(nm[0]);\n        rsRR.put(nm[0], nm[1], newColInfo);\n        if (nm2 != null) {\n          rsRR.addMappingOnly(nm2[0], nm2[1], newColInfo);\n        }\n        continue;\n      }\n      int vindex = valueBack == null ? -1 : ExprNodeDescUtils.indexOf(valueBack, valueColsBack);\n      if (vindex >= 0) {\n        index[i] = -vindex - 1;\n        continue;\n      }\n      index[i] = -valueCols.size() - 1;\n      String outputColName = getColumnInternalName(valueCols.size());\n\n      valueCols.add(value);\n      valueColsBack.add(valueBack);\n\n      ColumnInfo newColInfo = new ColumnInfo(colInfo);\n      newColInfo.setInternalName(Utilities.ReduceField.VALUE + \".\" + outputColName);\n      newColInfo.setTabAlias(nm[0]);\n\n      rsRR.put(nm[0], nm[1], newColInfo);\n      if (nm2 != null) {\n        rsRR.addMappingOnly(nm2[0], nm2[1], newColInfo);\n      }\n      outputColumns.add(outputColName);\n    }\n\n    dummy.setParentOperators(null);\n\n    ReduceSinkDesc rsdesc = PlanUtils.getReduceSinkDesc(sortCols, valueCols, outputColumns,\n        false, -1, partitionCols, sortOrder, numReducers, acidOp);\n    Operator interim = putOpInsertMap(OperatorFactory.getAndMakeChild(rsdesc,\n        new RowSchema(rsRR.getColumnInfos()), input), rsRR);\n\n    List<String> keyColNames = rsdesc.getOutputKeyColumnNames();\n    for (int i = 0 ; i < keyColNames.size(); i++) {\n      colExprMap.put(Utilities.ReduceField.KEY + \".\" + keyColNames.get(i), sortCols.get(i));\n    }\n    List<String> valueColNames = rsdesc.getOutputValueColumnNames();\n    for (int i = 0 ; i < valueColNames.size(); i++) {\n      colExprMap.put(Utilities.ReduceField.VALUE + \".\" + valueColNames.get(i), valueCols.get(i));\n    }\n    interim.setColumnExprMap(colExprMap);\n\n    RowResolver selectRR = new RowResolver();\n    ArrayList<ExprNodeDesc> selCols = new ArrayList<ExprNodeDesc>();\n    ArrayList<String> selOutputCols = new ArrayList<String>();\n    Map<String, ExprNodeDesc> selColExprMap = new HashMap<String, ExprNodeDesc>();\n\n    for (int i = 0; i < index.length; i++) {\n      ColumnInfo prev = columnInfos.get(i);\n      String[] nm = inputRR.reverseLookup(prev.getInternalName());\n      String[] nm2 = inputRR.getAlternateMappings(prev.getInternalName());\n      ColumnInfo info = new ColumnInfo(prev);\n\n      String field;\n      if (index[i] >= 0) {\n        field = Utilities.ReduceField.KEY + \".\" + keyColNames.get(index[i]);\n      } else {\n        field = Utilities.ReduceField.VALUE + \".\" + valueColNames.get(-index[i] - 1);\n      }\n      String internalName = getColumnInternalName(i);\n      ExprNodeColumnDesc desc = new ExprNodeColumnDesc(info.getType(),\n          field, info.getTabAlias(), info.getIsVirtualCol());\n      selCols.add(desc);\n\n      info.setInternalName(internalName);\n      selectRR.put(nm[0], nm[1], info);\n      if (nm2 != null) {\n        selectRR.addMappingOnly(nm2[0], nm2[1], info);\n      }\n      selOutputCols.add(internalName);\n      selColExprMap.put(internalName, desc);\n    }\n    SelectDesc select = new SelectDesc(selCols, selOutputCols);\n    Operator output = putOpInsertMap(OperatorFactory.getAndMakeChild(select,\n        new RowSchema(selectRR.getColumnInfos()), interim), selectRR);\n    output.setColumnExprMap(selColExprMap);\n    return output;\n  }"
        }
    },
    {
        "filename": "HIVE-6301.json",
        "creation_time": "2014-01-24T01:42:18.000+0000",
        "bug_report": {
            "Title": "get_json_object throws java.lang.IllegalStateException: No match found exception.",
            "Description": "The bug occurs in the `UDFJson` class of Apache Hive when the `evaluate` method is called with a malformed JSON path. The method attempts to extract a value from a JSON string using regular expressions defined in the `extract` method. If the input path does not conform to the expected regex patterns, it results in a `java.lang.IllegalStateException` with the message 'No match found'. This issue persists in the latest version of Hive.",
            "StackTrace": [
                "2014-01-23 11:08:19,869 FATAL ExecReducer: org.apache.hadoop.hive.ql.metadata.HiveException: Unable to execute method public org.apache.hadoop.io.Text org.apache.hadoop.hive.ql.udf.UDFJson.evaluate(java.lang.String,java.lang.String) on object org.apache.hadoop.hive.ql.udf.UDFJson@c7056d5 of class org.apache.hadoop.hive.ql.udf.UDFJson with arguments {{ .... }:java.lang.String, $.6:java.lang.String} of size 2",
                "Caused by: java.lang.IllegalStateException: No match found",
                "at java.util.regex.Matcher.group(Matcher.java:468)",
                "at org.apache.hadoop.hive.ql.udf.UDFJson.extract(UDFJson.java:190)",
                "at org.apache.hadoop.hive.ql.udf.UDFJson.evaluate(UDFJson.java:154)",
                "... 24 more"
            ],
            "RootCause": "The root cause of the issue is that the input path string passed to the `extract` method does not match the expected regex patterns defined by `patternKey` and `patternIndex`. Specifically, the regex for `patternKey` expects keys to consist of alphanumeric characters, underscores, hyphens, colons, and spaces. If the path string is malformed or contains invalid characters, the matcher fails, leading to the 'No match found' exception.",
            "StepsToReproduce": [
                "1. Call the `UDFJson.evaluate` method with a JSON string and a malformed path string.",
                "2. Ensure the path string does not conform to the regex patterns defined in the `UDFJson` class.",
                "3. Observe the exception thrown during execution."
            ],
            "ExpectedBehavior": "The `evaluate` method should return the extracted JSON object as a string when provided with a valid JSON string and a correctly formatted path string.",
            "ObservedBehavior": "The method throws a `java.lang.IllegalStateException` with the message 'No match found' when the path string does not match the expected format.",
            "Suggestions": "Ensure that the path string adheres to the expected regex patterns defined in the `UDFJson` class. Validate the input before calling the `evaluate` method to prevent this exception.",
            "problem_location": {
                "files": [
                    "UDFJson.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.ql.udf.UDFJson"
                ],
                "methods": [
                    "UDFJson.evaluate",
                    "UDFJson.extract"
                ]
            },
            "possible_fix": "Modify the input validation in the `evaluate` method to check if the path string matches the `patternKey` regex before proceeding with the extraction. This can prevent the IllegalStateException from being thrown."
        },
        "possible_fix_code": {
            "UDFJson.evaluate": "  public Text evaluate(String jsonString, String pathString) {\n\n    if (jsonString == null || jsonString.isEmpty() || pathString == null || pathString.isEmpty()) {\n      return null;\n    }\n\n    // Cache pathExpr\n    String[] pathExpr = pathExprCache.get(pathString);\n    if (pathExpr == null) {\n      pathExpr = pathString.split(\"\\\\.\", -1);\n      pathExprCache.put(pathString, pathExpr);\n    }\n\n    if (!pathExpr[0].equalsIgnoreCase(\"$\")) {\n      return null;\n    }\n\n    // Validate pathString against patternKey\n    Matcher pathMatcher = patternKey.matcher(pathString);\n    if (!pathMatcher.matches()) {\n      return null; // Invalid path format\n    }\n\n    // Cache extractObject\n    Object extractObject = extractObjectCache.get(jsonString);\n    if (extractObject == null) {\n      try {\n        extractObject = MAPPER.readValue(jsonString, MAP_TYPE);\n      } catch (Exception e) {\n        return null;\n      }\n      extractObjectCache.put(jsonString, extractObject);\n    }\n\n    for (int i = 1; i < pathExpr.length; i++) {\n      if (extractObject == null) {\n          return null;\n      }\n      extractObject = extract(extractObject, pathExpr[i]);\n    }\n\n    if (extractObject instanceof Map || extractObject instanceof List) {\n      try {\n        result.set(MAPPER.writeValueAsString(extractObject));\n      } catch (Exception e) {\n        return null;\n      }\n    } else if (extractObject != null) {\n      result.set(extractObject.toString());\n    } else {\n      return null;\n    }\n    return result;\n  }"
        }
    },
    {
        "filename": "HIVE-8295.json",
        "creation_time": "2014-09-29T21:16:32.000+0000",
        "bug_report": {
            "Title": "Add batch retrieve partition objects for metastore direct sql",
            "Description": "The current implementation in `MetaStoreDirectSql` constructs partition objects by first fetching partition IDs. However, when the number of matching partition IDs exceeds 1000, the direct SQL query fails due to Oracle's limitation on the maximum number of expressions in an `IN` clause, resulting in a `JDODataStoreException`. This issue can be resolved by modifying the query to handle large datasets more efficiently, either by batching the requests or using temporary tables.",
            "StackTrace": [
                "2014-09-29 19:30:02,942 DEBUG [pool-1-thread-1] metastore.MetaStoreDirectSql (MetaStoreDirectSql.java:timingTrace(604)) - Direct SQL query in 122.085893ms + 13.048901ms, the query is [select \"PARTITIONS\".\"PART_ID\" from \"PARTITIONS\"  inner join \"TBLS\" on \"PARTITIONS\".\"TBL_ID\" = \"TBLS\".\"TBL_ID\"     and \"TBLS\".\"TBL_NAME\" = ?   inner join \"DBS\" on \"TBLS\".\"DB_ID\" = \"DBS\".\"DB_ID\"      and \"DBS\".\"NAME\" = ? inner join \"PARTITION_KEY_VALS\" \"FILTER2\" on \"FILTER2\".\"PART_ID\" = \"PARTITIONS\".\"PART_ID\" and \"FILTER2\".\"INTEGER_IDX\" = 2 where ((\"FILTER2\".\"PART_KEY_VAL\" = ?))]",
                "2014-09-29 19:30:02,949 ERROR [pool-1-thread-1] metastore.ObjectStore (ObjectStore.java:handleDirectSqlError(2248)) - Direct SQL failed, falling back to ORM",
                "javax.jdo.JDODataStoreException: Error executing SQL query \"select \"PARTITIONS\".\"PART_ID\", \"SDS\".\"SD_ID\", \"SDS\".\"CD_ID\", \"SERDES\".\"SERDE_ID\", \"PARTITIONS\".\"CREATE_TIME\", \"PARTITIONS\".\"LAST_ACCESS_TIME\", \"SDS\".\"INPUT_FORMAT\", \"SDS\".\"IS_COMPRESSED\", \"SDS\".\"IS_STOREDASSUBDIRECTORIES\", \"SDS\".\"LOCATION\", \"SDS\".\"NUM_BUCKETS\", \"SDS\".\"OUTPUT_FORMAT\", \"SERDES\".\"NAME\", \"SERDES\".\"SLIB\" from \"PARTITIONS\"  left outer join \"SDS\" on \"PARTITIONS\".\"SD_ID\" = \"SDS\".\"SD_ID\"   left outer join \"SERDES\" on \"SDS\".\"SERDE_ID\" = \"SERDES\".\"SERDE_ID\" where \"PART_ID\" in (136,140,143,147,152,156,160,163,167,171,174,180,185,191,196,198,203,208,212,217...\n) order by \"PART_NAME\" asc\".",
                "NestedThrowablesStackTrace: java.sql.SQLSyntaxErrorException: ORA-01795: maximum number of expressions in a list is 1000"
            ],
            "RootCause": "The root cause of the issue is the SQL syntax error caused by exceeding Oracle's limit of 1000 expressions in an `IN` clause, leading to a `JDODataStoreException` when attempting to execute the query.",
            "StepsToReproduce": [
                "1. Execute a query in `MetaStoreDirectSql` that results in more than 1000 partition IDs being fetched.",
                "2. Observe the error message indicating the SQL syntax error due to exceeding the maximum number of expressions."
            ],
            "ExpectedBehavior": "The system should successfully retrieve partition objects without exceeding the SQL expression limit, either by batching the requests or using an alternative method.",
            "ObservedBehavior": "The system fails to execute the SQL query when the number of partition IDs exceeds 1000, resulting in a `JDODataStoreException` and falling back to ORM.",
            "Suggestions": "To resolve the issue, consider implementing batch processing for partition IDs, using temporary tables to store the IDs, or rewriting the query to use `EXISTS` instead of `IN`.",
            "problem_location": {
                "files": [
                    "MetaStoreDirectSql.java",
                    "ObjectStore.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.metastore.MetaStoreDirectSql",
                    "org.apache.hadoop.hive.metastore.ObjectStore"
                ],
                "methods": [
                    "MetaStoreDirectSql.getPartitionsViaSqlFilterInternal",
                    "ObjectStore.getPartitionsByExprInternal"
                ]
            },
            "possible_fix": "Modify the `getPartitionsViaSqlFilterInternal` method to handle partition IDs in batches of 1000 or fewer, or implement a temporary table approach to store and query the partition IDs."
        },
        "possible_fix_code": {
            "getPartitionsViaSqlFilterInternal": "  private List<Partition> getPartitionsViaSqlFilterInternal(String dbName, String tblName,\n      Boolean isView, String sqlFilter, List<? extends Object> paramsForFilter,\n      List<String> joinsForFilter, Integer max) throws MetaException {\n    boolean doTrace = LOG.isDebugEnabled();\n    dbName = dbName.toLowerCase();\n    tblName = tblName.toLowerCase();\n    String orderForFilter = (max != null) ? \" order by \\\"PART_NAME\\\" asc\" : \"\";\n    if (isMySql) {\n      assert pm.currentTransaction().isActive();\n      setAnsiQuotesForMysql();\n    }\n\n    String queryText =\n        \"select \\\"PARTITIONS\\\".\\\"PART_ID\\\" from \\\"PARTITIONS\\\"\"\n      + \"  inner join \\\"TBLS\\\" on \\\"PARTITIONS\\\".\\\"TBL_ID\\\" = \\\"TBLS\\\".\\\"TBL_ID\\\" \"\n      + \"    and \\\"TBLS\\\".\\\"TBL_NAME\\\" = ? \"\n      + \"  inner join \\\"DBS\\\" on \\\"TBLS\\\".\\\"DB_ID\\\" = \\\"DBS\\\".\\\"DB_ID\\\" \"\n      + \"     and \\\"DBS\\\".\\\"NAME\\\" = ? \"\n      + join(joinsForFilter, ' ')\n      + (StringUtils.isBlank(sqlFilter) ? \"\" : (\" where \" + sqlFilter)) + orderForFilter;\n    Object[] params = new Object[paramsForFilter.size() + 2];\n    params[0] = tblName;\n    params[1] = dbName;\n    for (int i = 0; i < paramsForFilter.size(); ++i) {\n      params[i + 2] = paramsForFilter.get(i);\n    }\n\n    long start = doTrace ? System.nanoTime() : 0;\n    Query query = pm.newQuery(\"javax.jdo.query.SQL\", queryText);\n    if (max != null) {\n      query.setRange(0, max.shortValue());\n    }\n    @SuppressWarnings(\"unchecked\")\n    List<Object> sqlResult = (List<Object>)query.executeWithArray(params);\n    long queryTime = doTrace ? System.nanoTime() : 0;\n    if (sqlResult.isEmpty()) {\n      timingTrace(doTrace, queryText, start, queryTime);\n      return new ArrayList<Partition>();\n    }\n\n    // Prepare StringBuilder for batching partition IDs\n    List<Long> partitionIds = new ArrayList<>();\n    for (Object partitionId : sqlResult) {\n      partitionIds.add(StatObjectConverter.extractSqlLong(partitionId));\n    }\n\n    // Process partition IDs in batches of 1000\n    List<Partition> orderedResult = new ArrayList<>();\n    for (int i = 0; i < partitionIds.size(); i += 1000) {\n      List<Long> batch = partitionIds.subList(i, Math.min(i + 1000, partitionIds.size()));\n      String partIds = String.join(\",\", batch.stream().map(String::valueOf).collect(Collectors.toList()));\n\n      // Now get most of the other fields.\n      queryText =\n        \"select \\\"PARTITIONS\\\".\\\"PART_ID\\\", \\\"SDS\\\".\\\"SD_ID\\\", \\\"SDS\\\".\\\"CD_ID\\\",\\\"\n      + \" \\\"SERDES\\\".\\\"SERDE_ID\\\", \\\"PARTITIONS\\\".\\\"CREATE_TIME\\\",\\\"\n      + \" \\\"PARTITIONS\\\".\\\"LAST_ACCESS_TIME\\\", \\\"SDS\\\".\\\"INPUT_FORMAT\\\", \\\"SDS\\\".\\\"IS_COMPRESSED\\\",\\\"\n      + \" \\\"SDS\\\".\\\"IS_STOREDASSUBDIRECTORIES\\\", \\\"SDS\\\".\\\"LOCATION\\\", \\\"SDS\\\".\\\"NUM_BUCKETS\\\",\\\"\n      + \" \\\"SDS\\\".\\\"OUTPUT_FORMAT\\\", \\\"SERDES\\\".\\\"NAME\\\", \\\"SERDES\\\".\\\"SLIB\\\" \"\n      + \"from \\\"PARTITIONS\\\"\"\n      + \"  left outer join \\\"SDS\\\" on \\\"PARTITIONS\\\".\\\"SD_ID\\\" = \\\"SDS\\\".\\\"SD_ID\\\" \"\n      + \"  left outer join \\\"SERDES\\\" on \\\"SDS\\\".\\\"SERDE_ID\\\" = \\\"SERDES\\\".\\\"SERDE_ID\\\" \"\n      + \"where \\\"PART_ID\\\" in (\" + partIds + \") order by \\\"PART_NAME\\\" asc\";\n      query = pm.newQuery(\"javax.jdo.query.SQL\", queryText);\n      @SuppressWarnings(\"unchecked\")\n      List<Object[]> sqlResult2 = (List<Object[]>)query.executeWithArray(params);\n\n      // Read all the fields and create partitions, SDs and serdes.\n      for (Object[] fields : sqlResult2) {\n        // Process fields and create Partition objects\n        // (existing logic for creating partitions goes here)\n      }\n    }\n    query.closeAll();\n    timingTrace(doTrace, queryText, start, queryTime);\n\n    return orderedResult;\n  }"
        }
    },
    {
        "filename": "HIVE-8915.json",
        "creation_time": "2014-11-19T19:40:17.000+0000",
        "bug_report": {
            "Title": "Log file explosion due to non-existence of COMPACTION_QUEUE table",
            "Description": "A critical issue was encountered during the startup of the Hive metastore on a fresh VM setup, where the required database tables, specifically the `hive.COMPACTION_QUEUE`, were not created as per the initialization script `hive-txn-schema-0.14.0.mysql.sql`. This absence led to an infinite loop of error messages being logged, resulting in a log file size explosion to 1.7GB within a short period. The error indicates that the compactor cleaner is unable to connect to the transaction database due to the missing table, which suggests a failure in the schema initialization process.",
            "StackTrace": [
                "2014-11-19 01:44:57,654 ERROR compactor.Cleaner (Cleaner.java:run(143)) - Caught an exception in the main loop of compactor cleaner, MetaException(message:Unable to connect to transaction database com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Table 'hive.COMPACTION_QUEUE' doesn't exist",
                "at sun.reflect.GeneratedConstructorAccessor20.newInstance(Unknown Source)",
                "at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)",
                "at java.lang.reflect.Constructor.newInstance(Constructor.java:513)",
                "at com.mysql.jdbc.Util.handleNewInstance(Util.java:411)",
                "at com.mysql.jdbc.Util.getInstance(Util.java:386)",
                "at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1052)",
                "at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3597)",
                "at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3529)",
                "at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:1990)",
                "at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2151)",
                "at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2619)",
                "at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2569)",
                "at com.mysql.jdbc.StatementImpl.executeQuery(StatementImpl.java:1524)",
                "at com.jolbox.bonecp.StatementHandle.executeQuery(StatementHandle.java:464)",
                "at org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler.findReadyToClean(CompactionTxnHandler.java:266)",
                "at org.apache.hadoop.hive.ql.txn.compactor.Cleaner.run(Cleaner.java:86)",
                "at org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler.findReadyToClean(CompactionTxnHandler.java:291)",
                "at org.apache.hadoop.hive.ql.txn.compactor.Cleaner.run(Cleaner.java:86)"
            ],
            "RootCause": "The root cause of the issue is the absence of the `hive.COMPACTION_QUEUE` table, which is essential for the operation of the Hive compactor cleaner. This absence is likely due to a failure in the schema initialization process, possibly caused by not executing the required initialization scripts or database connection issues.",
            "StepsToReproduce": [
                "Set up a fresh Hive environment on a VM.",
                "Ensure that the initialization script `hive-txn-schema-0.14.0.mysql.sql` is not executed.",
                "Start the Hive metastore service.",
                "Observe the log files for repeated error messages regarding the missing `hive.COMPACTION_QUEUE` table."
            ],
            "ExpectedBehavior": "The Hive metastore should initialize correctly, creating all necessary tables, including `hive.COMPACTION_QUEUE`, and should not produce an excessive number of error logs.",
            "ObservedBehavior": "The Hive metastore fails to start properly due to the missing `hive.COMPACTION_QUEUE` table, resulting in an endless loop of error messages being logged, leading to a log file size explosion.",
            "Suggestions": "To mitigate this issue, ensure that the initialization script `hive-txn-schema-0.14.0.mysql.sql` is executed successfully during the setup process. Additionally, implement a delay or a fail-safe mechanism in the compactor cleaner to prevent it from entering an endless loop of error logging.",
            "problem_location": {
                "files": [
                    "Cleaner.java",
                    "CompactionTxnHandler.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.metastore.txn.Cleaner",
                    "org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler"
                ],
                "methods": [
                    "Cleaner.run",
                    "CompactionTxnHandler.findReadyToClean"
                ]
            },
            "possible_fix": "Ensure the execution of the initialization script `hive-txn-schema-0.14.0.mysql.sql` to create the necessary tables. Additionally, consider adding error handling in the `Cleaner.run` method to prevent infinite logging in case of missing tables."
        },
        "possible_fix_code": {
            "Cleaner.run": "          public Object run() throws Exception {\n            try {\n                // Check if the COMPACTION_QUEUE table exists before proceeding\n                if (!isTableExists(\"COMPACTION_QUEUE\")) {\n                    LOG.error(\"The COMPACTION_QUEUE table does not exist. Please ensure the initialization script is executed.\");\n                    return null;\n                }\n                removeFiles(location, txnList);\n            } catch (Exception e) {\n                LOG.error(\"An error occurred while running the cleaner: \" + e.getMessage());\n            }\n            return null;\n          }\n\n          private boolean isTableExists(String tableName) throws SQLException {\n              Connection dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);\n              Statement stmt = null;\n              try {\n                  stmt = dbConn.createStatement();\n                  ResultSet rs = stmt.executeQuery(\"SHOW TABLES LIKE '\" + tableName + \"'\");\n                  return rs.next(); // Returns true if the table exists\n              } finally {\n                  closeDbConn(dbConn);\n                  closeStmt(stmt);\n              }\n          }"
        }
    },
    {
        "filename": "HIVE-7249.json",
        "creation_time": "2014-06-18T00:09:42.000+0000",
        "bug_report": {
            "Title": "HiveTxnManager.closeTxnManger() throws if called after commitTxn()",
            "Description": "The issue arises when the method closeTxnManager() is called after commitTxn() in a transactional context. The sequence of operations involves opening a transaction and acquiring locks for an INSERT operation. After committing the transaction, an attempt to close the transaction manager results in a NoSuchLockException, indicating that the system is trying to unlock a lock that has already been released or does not exist. This suggests a failure in the lock management logic within the Hive Metastore.",
            "StackTrace": [
                "2014-06-17 15:54:40,804 ERROR metastore.RetryingHMSHandler (RetryingHMSHandler.java:invoke(143)) - NoSuchLockException(message:No such lock: 1)",
                "at org.apache.hadoop.hive.metastore.txn.TxnHandler.heartbeatLock(TxnHandler.java:1407)",
                "at org.apache.hadoop.hive.metastore.txn.TxnHandler.unlock(TxnHandler.java:477)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.unlock(HiveMetaStore.java:4817)",
                "at org.apache.hadoop.hive.ql.lockmgr.DbLockManager.unlock(DbLockManager.java:110)",
                "at org.apache.hadoop.hive.ql.lockmgr.DbLockManager.close(DbLockManager.java:162)",
                "at org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.destruct(DbTxnManager.java:300)",
                "at org.apache.hadoop.hive.ql.lockmgr.HiveTxnManagerImpl.closeTxnManager(HiveTxnManagerImpl.java:39)"
            ],
            "RootCause": "The NoSuchLockException is thrown when the unlock method in the TxnHandler class attempts to delete a lock from the database but finds that no such lock exists. This typically occurs because the lock has already been released during the commit process, leading to a mismatch in the expected state of the lock.",
            "StepsToReproduce": [
                "1. Call openTxn() to start a new transaction.",
                "2. Call acquireLocks() with a query that performs an INSERT operation.",
                "3. Call commitTxn() to commit the transaction.",
                "4. Call closeTxnManager() to close the transaction manager."
            ],
            "ExpectedBehavior": "The transaction manager should successfully close without throwing any exceptions, as the locks should have been released during the commit process.",
            "ObservedBehavior": "An exception is thrown indicating 'No such lock: 1' when closeTxnManager() is called after commitTxn().",
            "Suggestions": "Review the lock management logic in the TxnHandler class, particularly the unlock method, to ensure that locks are properly tracked and that the state is consistent after a transaction is committed.",
            "problem_location": {
                "files": [
                    "TxnHandler.java",
                    "DbLockManager.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.metastore.txn.TxnHandler",
                    "org.apache.hadoop.hive.ql.lockmgr.DbLockManager"
                ],
                "methods": [
                    "TxnHandler.unlock",
                    "DbLockManager.close"
                ]
            },
            "possible_fix": "In the unlock method of TxnHandler, ensure that the lock state is validated before attempting to unlock. Consider adding checks to confirm that the lock is still valid and has not been released before calling the delete operation."
        },
        "possible_fix_code": {
            "org.apache.hadoop.hive.metastore.txn.TxnHandler.unlock": "  public void unlock(UnlockRequest rqst)\n      throws NoSuchLockException, TxnOpenException, MetaException {\n    try {\n      Connection dbConn = getDbConn();\n      try {\n        // Odd as it seems, we need to heartbeat first because this touches the\n        // lock table and assures that our locks are still valid. If they are\n        // not, this will throw an exception and the heartbeat will fail.\n        long extLockId = rqst.getLockid();\n        heartbeatLock(dbConn, extLockId);\n        long txnid = getTxnIdFromLockId(dbConn, extLockId);\n        // If there is a valid txnid, throw an exception,\n        // as locks associated with transactions should be unlocked only when the\n        // transaction is committed or aborted.\n        if (txnid > 0) {\n          LOG.debug(\"Going to rollback\");\n          dbConn.rollback();\n          String msg = \"Unlocking locks associated with transaction\" +\n              \" not permitted. Lockid \" + extLockId + \" is associated with \" +\n              \"transaction \" + txnid;\n          LOG.error(msg);\n          throw new TxnOpenException(msg);\n        }\n        // Check if the lock exists before attempting to delete it\n        Statement stmt = dbConn.createStatement();\n        String checkLockQuery = \"SELECT COUNT(*) FROM HIVE_LOCKS WHERE hl_lock_ext_id = \" + extLockId;\n        ResultSet rs = stmt.executeQuery(checkLockQuery);\n        if (rs.next() && rs.getInt(1) == 0) {\n          throw new NoSuchLockException(\"No such lock: \" + extLockId);\n        }\n        String s = \"delete from HIVE_LOCKS where hl_lock_ext_id = \" + extLockId;\n        LOG.debug(\"Going to execute update <\" + s + \">\");\n        int rc = stmt.executeUpdate(s);\n        if (rc < 1) {\n          LOG.debug(\"Going to rollback\");\n          dbConn.rollback();\n          throw new NoSuchLockException(\"No such lock: \" + extLockId);\n        }\n        LOG.debug(\"Going to commit\");\n        dbConn.commit();\n      } catch (SQLException e) {\n        try {\n          LOG.debug(\"Going to rollback\");\n          dbConn.rollback();\n        } catch (SQLException e1) {\n        }\n        detectDeadlock(e, \"unlock\");\n        throw new MetaException(\"Unable to update transaction database \" +\n            StringUtils.stringifyException(e));\n      } finally {\n        closeDbConn(dbConn);\n      }\n    } catch (DeadlockException e) {\n      unlock(rqst);\n    } finally {\n      deadlockCnt = 0;\n    }\n  }"
        }
    },
    {
        "filename": "HIVE-11540.json",
        "creation_time": "2015-08-12T23:12:18.000+0000",
        "bug_report": {
            "Title": "Too many delta files during Compaction - OOM",
            "Description": "The issue arises during the compaction of a Hive table (`weblogs.vop_hs.dt=15-08-12`) where the compaction process fails due to an OutOfMemoryError (OOM). The system is configured to handle a high volume of records (approximately 20 million daily) with multiple compactors running concurrently. Despite this setup, the compactors are unable to manage the large number of delta files generated, leading to memory exhaustion and subsequent job failures. The error logs indicate that the compaction job fails with a `java.io.IOException` and is marked as clean to prevent repeated failures.",
            "StackTrace": [
                "2015-08-12 15:05:01,197 FATAL [main] org.apache.hadoop.mapred.YarnChild: Error running child : java.lang.OutOfMemoryError: Direct buffer memory",
                "2015-08-12 15:36:18,443 ERROR [upladevhwd04v.researchnow.com-18]: compactor.Worker (Worker.java:run(176)) - Caught exception while trying to compact weblogs.vop_hs.dt=15-08-12. Marking clean to avoid repeated failures, java.io.IOException: Job failed!"
            ],
            "RootCause": "The root cause of the issue is the excessive number of delta files generated during the compaction process, which leads to an OutOfMemoryError when the system attempts to process them. The `CompactorMR.run` method fails to handle the large volume of data efficiently, resulting in memory exhaustion.",
            "StepsToReproduce": [
                "Stream weblogs to Kafka and then to Flume using a Hive sink.",
                "Configure the system to handle approximately 20 million records per day.",
                "Run multiple compactors (5 in this case) at various intervals (30m/5m/5s).",
                "Monitor the compaction process for the table `weblogs.vop_hs.dt=15-08-12`."
            ],
            "ExpectedBehavior": "The compaction process should efficiently handle the delta files generated, successfully compacting them without running out of memory, and should not result in job failures.",
            "ObservedBehavior": "The compaction process fails with an OutOfMemoryError, leading to job failures and the system marking the compaction as clean to avoid repeated failures.",
            "Suggestions": "Consider increasing the memory allocated to the compactor processes. Additionally, evaluate the compaction strategy to reduce the number of delta files generated or implement a more efficient method for handling large volumes of data.",
            "problem_location": {
                "files": [
                    "CompactorMR.java",
                    "CompactionTxnHandler.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.ql.txn.compactor.CompactorMR",
                    "org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler"
                ],
                "methods": [
                    "CompactorMR.run",
                    "CompactionTxnHandler.markCleaned"
                ]
            },
            "possible_fix": "Increase the memory allocation for the compactor jobs in the configuration. Additionally, consider modifying the `CompactorMR.run` method to handle large numbers of delta files more efficiently, possibly by batching or optimizing the way files are processed."
        },
        "possible_fix_code": {
            "CompactorMR.run": "  void run(HiveConf conf, String jobName, Table t, StorageDescriptor sd,\n           ValidTxnList txns, boolean isMajor, Worker.StatsUpdater su) throws IOException {\n    JobConf job = new JobConf(conf);\n    job.setJobName(jobName);\n    job.setOutputKeyClass(NullWritable.class);\n    job.setOutputValueClass(NullWritable.class);\n    job.setJarByClass(CompactorMR.class);\n    LOG.debug(\"User jar set to \" + job.getJar());\n    job.setMapperClass(CompactorMap.class);\n    job.setNumReduceTasks(0);\n    job.setInputFormat(CompactorInputFormat.class);\n    job.setOutputFormat(NullOutputFormat.class);\n    job.setOutputCommitter(CompactorOutputCommitter.class);\n\n    job.set(FINAL_LOCATION, sd.getLocation());\n    job.set(TMP_LOCATION, sd.getLocation() + \"/\" + TMPDIR + \"_\" + UUID.randomUUID().toString());\n    job.set(INPUT_FORMAT_CLASS_NAME, sd.getInputFormat());\n    job.set(OUTPUT_FORMAT_CLASS_NAME, sd.getOutputFormat());\n    job.setBoolean(IS_MAJOR, isMajor);\n    job.setBoolean(IS_COMPRESSED, sd.isCompressed());\n    job.set(TABLE_PROPS, new StringableMap(t.getParameters()).toString());\n    job.setInt(NUM_BUCKETS, sd.getNumBuckets());\n    job.set(ValidTxnList.VALID_TXNS_KEY, txns.toString());\n    setColumnTypes(job, sd.getCols());\n\n    AcidUtils.Directory dir = AcidUtils.getAcidState(new Path(sd.getLocation()), conf, txns);\n    StringableList dirsToSearch = new StringableList();\n    Path baseDir = null;\n    if (isMajor) {\n      baseDir = dir.getBaseDirectory();\n      if (baseDir == null) {\n        List<FileStatus> originalFiles = dir.getOriginalFiles();\n        if (!(originalFiles == null) && !(originalFiles.size() == 0)) {\n          for (FileStatus stat : originalFiles) {\n            dirsToSearch.add(stat.getPath());\n            LOG.debug(\"Adding original file \" + stat.getPath().toString() + \" to dirs to search\");\n          }\n          baseDir = new Path(sd.getLocation());\n        }\n      } else {\n        LOG.debug(\"Adding base directory \" + baseDir + \" to dirs to search\");\n        dirsToSearch.add(baseDir);\n      }\n    }\n\n    List<AcidUtils.ParsedDelta> parsedDeltas = dir.getCurrentDirectories();\n\n    if (parsedDeltas == null || parsedDeltas.size() == 0) {\n      LOG.error(  \"No delta files found to compact in \" + sd.getLocation());\n      return;\n    }\n\n    StringableList deltaDirs = new StringableList();\n    long minTxn = Long.MAX_VALUE;\n    long maxTxn = Long.MIN_VALUE;\n    for (AcidUtils.ParsedDelta delta : parsedDeltas) {\n      LOG.debug(\"Adding delta \" + delta.getPath() + \" to directories to search\");\n      dirsToSearch.add(delta.getPath());\n      deltaDirs.add(delta.getPath());\n      minTxn = Math.min(minTxn, delta.getMinTransaction());\n      maxTxn = Math.max(maxTxn, delta.getMaxTransaction());\n    }\n\n    if (baseDir != null) job.set(BASE_DIR, baseDir.toString());\n    job.set(DELTA_DIRS, deltaDirs.toString());\n    job.set(DIRS_TO_SEARCH, dirsToSearch.toString());\n    job.setLong(MIN_TXN, minTxn);\n    job.setLong(MAX_TXN, maxTxn);\n    LOG.debug(\"Setting minimum transaction to \" + minTxn);\n    LOG.debug(\"Setting maximum transaction to \" + maxTxn);\n\n    // Increase memory allocation for the compactor job\n    job.setInt(\"mapreduce.map.memory.mb\", 2048); // Example value, adjust as necessary\n    job.setInt(\"mapreduce.map.java.opts\", \"-Xmx1536m\"); // Example value, adjust as necessary\n\n    JobClient.runJob(job).waitForCompletion();\n    su.gatherStats();\n  }"
        }
    },
    {
        "filename": "HIVE-15755.json",
        "creation_time": "2017-01-30T20:48:25.000+0000",
        "bug_report": {
            "Title": "NullPointerException on invalid table name in ON clause of Merge statement",
            "Description": "A NullPointerException occurs when an invalid table name is specified in the ON clause of a MERGE statement in Hive. This issue arises when the target table name in the ON clause does not match any existing table, leading to a failure in the SQL compilation process. The error message indicates that the `getPredicate` method in the `OnClauseAnalyzer` class is unable to retrieve the target columns due to a missing entry in the `table2column` mapping, resulting in a null reference.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer$OnClauseAnalyzer.getPredicate(UpdateDeleteSemanticAnalyzer.java:1143)",
                "at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer$OnClauseAnalyzer.access$400(UpdateDeleteSemanticAnalyzer.java:1049)",
                "at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.handleInsert(UpdateDeleteSemanticAnalyzer.java:1025)",
                "at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.analyzeMerge(UpdateDeleteSemanticAnalyzer.java:660)",
                "at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.analyzeInternal(UpdateDeleteSemanticAnalyzer.java:80)",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:230)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:465)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:321)",
                "at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1221)",
                "at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1215)"
            ],
            "RootCause": "The NullPointerException is caused by the `getPredicate` method in the `OnClauseAnalyzer` class, which fails when the `table2column` mapping does not contain an entry for the target table name specified in the ON clause. This results in the `targetCols` list being null, leading to a null reference when attempting to iterate over it.",
            "StepsToReproduce": [
                "Create a source table: `create table src (col1 int, col2 int);`",
                "Create a target table: `create table trgt (tcol1 int, tcol2 int);`",
                "Insert data into the source table: `insert into src values (1, 232);`",
                "Execute a MERGE statement with an invalid table name in the ON clause: `merge into trgt using (select * from src) sub on sub.col1 = *invalidtablename.tcol1* when not matched then insert values (sub.col1, sub.col2);`"
            ],
            "ExpectedBehavior": "The MERGE statement should execute successfully, updating or inserting records in the target table based on the specified conditions without throwing any exceptions.",
            "ObservedBehavior": "A NullPointerException is thrown, indicating that the SQL compilation failed due to an invalid table name in the ON clause.",
            "Suggestions": "Ensure that the target table name specified in the ON clause of the MERGE statement is valid and exists in the database. Additionally, implement validation checks to verify that the target table name is correctly mapped to its columns before executing the MERGE statement.",
            "problem_location": {
                "files": [
                    "UpdateDeleteSemanticAnalyzer.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer"
                ],
                "methods": [
                    "analyzeMerge",
                    "getPredicate"
                ]
            },
            "possible_fix": "Add validation logic in the `analyzeMerge` method to check if the target table name exists in the `table2column` mapping before proceeding with the execution of the MERGE statement. If the target table name is invalid, throw a more descriptive error message to inform the user."
        },
        "possible_fix_code": {
            "analyzeMerge": "  private void analyzeMerge(ASTNode tree) throws SemanticException {\n    currentOperation = Operation.MERGE;\n    quotedIdenfierHelper = new IdentifierQuoter(ctx.getTokenRewriteStream());\n    ASTNode target = (ASTNode)tree.getChild(0);\n    ASTNode source = (ASTNode)tree.getChild(1);\n    String targetName = getSimpleTableName(target);\n    String sourceName = getSimpleTableName(source);\n    ASTNode onClause = (ASTNode) tree.getChild(2);\n    String onClauseAsText = getMatchedText(onClause);\n\n    Table targetTable = getTargetTable(target);\n    validateTargetTable(targetTable);\n\n    // New validation logic to check if target table exists in table2column mapping\n    if (!table2column.containsKey(targetName.toLowerCase())) {\n        throw new SemanticException(\"Target table '\" + targetName + \"' does not exist in the mapping.\");\n    }\n\n    List<ASTNode> whenClauses = findWhenClauses(tree);\n\n    StringBuilder rewrittenQueryStr = new StringBuilder(\"FROM\\n\");\n    rewrittenQueryStr.append(Indent).append(getFullTableNameForSQL(target));\n    if(isAliased(target)) {\n      rewrittenQueryStr.append(\" \").append(targetName);\n    }\n    rewrittenQueryStr.append('\\n');\n    rewrittenQueryStr.append(Indent).append(chooseJoinType(whenClauses)).append(\"\\n\");\n    if(source.getType() == HiveParser.TOK_SUBQUERY) {\n      rewrittenQueryStr.append(Indent).append(getMatchedText(source));\n    }\n    else {\n      rewrittenQueryStr.append(Indent).append(getFullTableNameForSQL(source));\n      if(isAliased(source)) {\n        rewrittenQueryStr.append(\" \").append(sourceName);\n      }\n    }\n    rewrittenQueryStr.append('\\n');\n    rewrittenQueryStr.append(Indent).append(\"ON \").append(onClauseAsText).append('\\n');\n\n    String extraPredicate = null;\n    int numWhenMatchedUpdateClauses = 0, numWhenMatchedDeleteClauses = 0;\n    for(ASTNode whenClause : whenClauses) {\n      switch (getWhenClauseOperation(whenClause).getType()) {\n        case HiveParser.TOK_INSERT:\n          handleInsert(whenClause, rewrittenQueryStr, target, onClause, targetTable, targetName, onClauseAsText);\n          break;\n        case HiveParser.TOK_UPDATE:\n          numWhenMatchedUpdateClauses++;\n          String s = handleUpdate(whenClause, rewrittenQueryStr, target, onClauseAsText, targetTable, extraPredicate);\n          if(numWhenMatchedUpdateClauses + numWhenMatchedDeleteClauses == 1) {\n            extraPredicate = s;\n          }\n          break;\n        case HiveParser.TOK_DELETE:\n          numWhenMatchedDeleteClauses++;\n          String s1 = handleDelete(whenClause, rewrittenQueryStr, target, onClauseAsText, targetTable, extraPredicate);\n          if(numWhenMatchedUpdateClauses + numWhenMatchedDeleteClauses == 1) {\n            extraPredicate = s1;\n          }\n          break;\n        default:\n          throw new IllegalStateException(\"Unexpected WHEN clause type: \" + whenClause.getType() +\n            addParseInfo(whenClause));\n      }\n      if(numWhenMatchedDeleteClauses > 1) {\n        throw new SemanticException(ErrorMsg.MERGE_TOO_MANY_DELETE, ctx.getCmd());\n      }\n      if(numWhenMatchedUpdateClauses > 1) {\n        throw new SemanticException(ErrorMsg.MERGE_TOO_MANY_UPDATE, ctx.getCmd());\n      }\n    }\n    if(numWhenMatchedDeleteClauses + numWhenMatchedUpdateClauses == 2 && extraPredicate == null) {\n      throw new SemanticException(ErrorMsg.MERGE_PREDIACTE_REQUIRED, ctx.getCmd());\n    }\n    handleCardinalityViolation(rewrittenQueryStr, target, onClauseAsText, targetTable);\n    ReparseResult rr = parseRewrittenQuery(rewrittenQueryStr, ctx.getCmd());\n    Context rewrittenCtx = rr.rewrittenCtx;\n    ASTNode rewrittenTree = rr.rewrittenTree;\n\n    for(int insClauseIdx = 1, whenClauseIdx = 0;\n        insClauseIdx < rewrittenTree.getChildCount() - 1;\n        insClauseIdx++, whenClauseIdx++) {\n      ASTNode insertClause = (ASTNode) rewrittenTree.getChild(insClauseIdx);\n      switch (getWhenClauseOperation(whenClauses.get(whenClauseIdx)).getType()) {\n        case HiveParser.TOK_INSERT:\n          rewrittenCtx.addDestNamePrefix(insertClause, Context.DestClausePrefix.INSERT);\n          break;\n        case HiveParser.TOK_UPDATE:\n          rewrittenCtx.addDestNamePrefix(insertClause, Context.DestClausePrefix.UPDATE);\n          break;\n        case HiveParser.TOK_DELETE:\n          rewrittenCtx.addDestNamePrefix(insertClause, Context.DestClausePrefix.DELETE);\n          break;\n        default:\n          assert false;\n      }\n    }\n    try {\n      useSuper = true;\n      super.analyze(rewrittenTree, rewrittenCtx);\n    } finally {\n      useSuper = false;\n    }\n    updateOutputs(targetTable);\n  }"
        }
    },
    {
        "filename": "HIVE-9390.json",
        "creation_time": "2015-01-15T18:50:32.000+0000",
        "bug_report": {
            "Title": "Enhance retry logic wrt DB access in TxnHandler",
            "Description": "The bug report addresses a `MetaException` encountered when attempting to obtain a JDBC connection from the connection pool, resulting in a read timeout. This issue arises in the context of transaction handling within the HiveMetaStore, specifically when the `get_open_txns` method is invoked, which relies on the `getOpenTxns` method of the `TxnHandler`. The underlying cause appears to be related to the connection pool being exhausted or misconfigured, leading to failures in establishing database connections.",
            "StackTrace": [
                "2015-01-13 16:09:21,148 ERROR metastore.RetryingHMSHandler (RetryingHMSHandler.java:invoke(141)) - org.apache.thrift.TException: MetaException(message:Unable to get jdbc connection from pool, Read timed out)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_open_txns(HiveMetaStore.java:5324)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:102)",
                "at com.sun.proxy.$Proxy11.get_open_txns(Unknown Source)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getValidTxns(HiveMetaStoreClient.java:1696)",
                "at org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.getValidTxns(DbTxnManager.java:289)",
                "at org.apache.hadoop.hive.ql.Driver.recordValidTxns(Driver.java:882)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:399)",
                "at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1067)",
                "at org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:100)",
                "at org.apache.hive.service.cli.operation.SQLOperation.runInternal(SQLOperation.java:171)",
                "at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:376)",
                "at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:79)",
                "at org.apache.hive.service.cli.CLIService.executeStatementAsync(CLIService.java:247)",
                "at org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:401)",
                "at org.apache.thrift.server.TServlet.doPost(TServlet.java:83)",
                "at javax.servlet.http.HttpServlet.service(HttpServlet.java:727)",
                "at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:565)",
                "at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:225)",
                "at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1031)",
                "at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:111)",
                "at org.eclipse.jetty.server.Server.handle(Server.java:349)",
                "at org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:449)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.lang.Thread.run(Thread.java:745)",
                "Caused by: MetaException(message:Unable to get jdbc connection from pool, Read timed out)",
                "at org.apache.hadoop.hive.metastore.txn.TxnHandler.getDbConn(TxnHandler.java:850)",
                "at org.apache.hadoop.hive.metastore.txn.TxnHandler.getOpenTxns(TxnHandler.java:196)"
            ],
            "RootCause": "The root cause of the issue is the failure of the `getDbConn` method in the `TxnHandler` class to obtain a JDBC connection from the connection pool, which results in a `MetaException` due to a read timeout. This is likely caused by the connection pool being exhausted or misconfigured.",
            "StepsToReproduce": [
                "Attempt to execute a transaction that requires a JDBC connection.",
                "Monitor the connection pool usage to see if it is exhausted.",
                "Observe the logs for the `MetaException` indicating a read timeout."
            ],
            "ExpectedBehavior": "The system should successfully obtain a JDBC connection from the connection pool and execute the transaction without encountering a read timeout.",
            "ObservedBehavior": "The system fails to obtain a JDBC connection, resulting in a `MetaException` with a message indicating a read timeout.",
            "Suggestions": "Consider increasing the size of the connection pool, verifying the database availability, and checking network configurations to ensure that connections can be established successfully.",
            "problem_location": {
                "files": [
                    "TxnHandler.java",
                    "HiveMetaStore.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.metastore.txn.TxnHandler",
                    "org.apache.hadoop.hive.metastore.HiveMetaStore"
                ],
                "methods": [
                    "TxnHandler.getDbConn",
                    "HiveMetaStore.get_open_txns"
                ]
            },
            "possible_fix": "Review and adjust the connection pool settings in the configuration file. For example, increase the maximum number of connections allowed in the pool to prevent exhaustion. Additionally, ensure that the database server is operational and accessible."
        },
        "possible_fix_code": {
            "TxnHandler.getDbConn": "  protected Connection getDbConn(int isolationLevel) throws MetaException {\n    int retryCount = 0;\n    while (retryCount < 3) {\n      try {\n        Connection dbConn = connPool.getConnection();\n        dbConn.setAutoCommit(false);\n        dbConn.setTransactionIsolation(isolationLevel);\n        return dbConn;\n      } catch (SQLException e) {\n        retryCount++;\n        if (retryCount >= 3) {\n          String msg = \"Unable to get jdbc connection from pool after \" + retryCount + \" attempts, \" + e.getMessage();\n          throw new MetaException(msg);\n        }\n        // Optionally, add a small delay before retrying\n        try {\n          Thread.sleep(1000);\n        } catch (InterruptedException ie) {\n          Thread.currentThread().interrupt();\n        }\n      }\n    }\n    throw new MetaException(\"Unable to get jdbc connection from pool\");\n  }"
        }
    },
    {
        "filename": "HIVE-7623.json",
        "creation_time": "2014-08-05T23:58:27.000+0000",
        "bug_report": {
            "Title": "Hive partition rename fails if filesystem cache is disabled",
            "Description": "The issue arises when attempting to rename a partition in Hive using the `alterPartition` method. The operation fails with an `InvalidOperationException` because the new partition location is on a different file system than the old location. This behavior is consistent with the known issue documented in HIVE-3815. A workaround is to enable filesystem caching by setting `fs.hdfs.impl.disable.cache=false` and `fs.file.impl.disable.cache=false`.",
            "StackTrace": [
                "2014-08-05 21:46:14,522 ERROR [pool-3-thread-1]: metastore.RetryingHMSHandler (RetryingHMSHandler.java:invoke(143)) - InvalidOperationException(message:table new location hdfs://hadoop-namenode:8020/user/hive/warehouse/sample_logs/XX=AA/YY=123 is on a different file system than the old location hdfs://hadoop-namenode:8020/user/hive/warehouse/sample_logs/XX=AA/YY=456. This operation is not supported)",
                "at org.apache.hadoop.hive.metastore.HiveAlterHandler.alterPartition(HiveAlterHandler.java:361)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.rename_partition(HiveMetaStore.java:2629)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.rename_partition(HiveMetaStore.java:2602)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:622)",
                "at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:105)",
                "at com.sun.proxy.$Proxy5.rename_partition(Unknown Source)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$rename_partition.getResult(ThriftHiveMetastore.java:9057)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$rename_partition.getResult(ThriftHiveMetastore.java:9041)",
                "at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)",
                "at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:110)",
                "at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:107)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:416)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)"
            ],
            "RootCause": "The root cause of the issue is that the `alterPartition` method in the `HiveAlterHandler` class checks if the source and destination file systems are the same. If they are not, it throws an `InvalidOperationException`, indicating that the operation is not supported.",
            "StepsToReproduce": [
                "1. Disable filesystem caching by setting `fs.hdfs.impl.disable.cache=true` and `fs.file.impl.disable.cache=true`.",
                "2. Attempt to rename a partition in Hive using the `alterPartition` method.",
                "3. Observe the error message indicating that the new location is on a different file system."
            ],
            "ExpectedBehavior": "The partition should be renamed successfully if the new location is on the same file system as the old location.",
            "ObservedBehavior": "An `InvalidOperationException` is thrown, indicating that the new location is on a different file system than the old location, which prevents the rename operation.",
            "Suggestions": "Ensure that the new partition location is on the same file system as the old location. Alternatively, enable filesystem caching to avoid this issue.",
            "problem_location": {
                "files": [
                    "HiveAlterHandler.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.metastore.HiveAlterHandler"
                ],
                "methods": [
                    "alterPartition"
                ]
            },
            "possible_fix": "Modify the configuration to enable filesystem caching by setting `fs.hdfs.impl.disable.cache=false` and `fs.file.impl.disable.cache=false`. Additionally, ensure that the new partition location is on the same file system as the old location before attempting to rename."
        },
        "possible_fix_code": {
            "alterPartition": "  public Partition alterPartition(final RawStore msdb, Warehouse wh, final String dbname,\n      final String name, final List<String> part_vals, final Partition new_part)\n      throws InvalidOperationException, InvalidObjectException, AlreadyExistsException,\n      MetaException {\n    boolean success = false;\n\n    Path srcPath = null;\n    Path destPath = null;\n    FileSystem srcFs = null;\n    FileSystem destFs = null;\n    Partition oldPart = null;\n    String oldPartLoc = null;\n    String newPartLoc = null;\n\n    // Set DDL time to now if not specified\n    if (new_part.getParameters() == null ||\n        new_part.getParameters().get(hive_metastoreConstants.DDL_TIME) == null ||\n        Integer.parseInt(new_part.getParameters().get(hive_metastoreConstants.DDL_TIME)) == 0) {\n      new_part.putToParameters(hive_metastoreConstants.DDL_TIME, Long.toString(System\n          .currentTimeMillis() / 1000));\n    }\n\n    Table tbl = msdb.getTable(dbname, name);\n    //alter partition\n    if (part_vals == null || part_vals.size() == 0) {\n      try {\n        oldPart = msdb.getPartition(dbname, name, new_part.getValues());\n        if (MetaStoreUtils.requireCalStats(hiveConf, oldPart, new_part, tbl)) {\n          MetaStoreUtils.updatePartitionStatsFast(new_part, wh, false, true);\n        }\n        msdb.alterPartition(dbname, name, new_part.getValues(), new_part);\n      } catch (InvalidObjectException e) {\n        throw new InvalidOperationException(\"alter is not possible\");\n      } catch (NoSuchObjectException e){\n        //old partition does not exist\n        throw new InvalidOperationException(\"alter is not possible\");\n      }\n      return oldPart;\n    }\n    //rename partition\n    try {\n      msdb.openTransaction();\n      try {\n        oldPart = msdb.getPartition(dbname, name, part_vals);\n      } catch (NoSuchObjectException e) {\n        // this means there is no existing partition\n        throw new InvalidObjectException(\n            \"Unable to rename partition because old partition does not exist\");\n      }\n      Partition check_part = null;\n      try {\n        check_part = msdb.getPartition(dbname, name, new_part.getValues());\n      } catch(NoSuchObjectException e) {\n        // this means there is no existing partition\n        check_part = null;\n      }\n      if (check_part != null) {\n        throw new AlreadyExistsException(\"Partition already exists:\" + dbname + \".\" + name + \".\" +\n            new_part.getValues());\n      }\n      if (tbl == null) {\n        throw new InvalidObjectException(\n            \"Unable to rename partition because table or database do not exist\");\n      }\n\n      // if the external partition is renamed, the file should not change\n      if (tbl.getTableType().equals(TableType.EXTERNAL_TABLE.toString())) {\n        new_part.getSd().setLocation(oldPart.getSd().getLocation());\n        msdb.alterPartition(dbname, name, part_vals, new_part);\n      } else {\n        try {\n          destPath = new Path(wh.getTablePath(msdb.getDatabase(dbname), name),\n            Warehouse.makePartName(tbl.getPartitionKeys(), new_part.getValues()));\n          destPath = constructRenamedPath(destPath, new Path(new_part.getSd().getLocation()));\n        } catch (NoSuchObjectException e) {\n          LOG.debug(e);\n          throw new InvalidOperationException(\n            \"Unable to change partition or table. Database \" + dbname + \" does not exist\"\n              + \" Check metastore logs for detailed stack.\" + e.getMessage());\n        }\n        if (destPath != null) {\n          newPartLoc = destPath.toString();\n          oldPartLoc = oldPart.getSd().getLocation();\n\n          srcPath = new Path(oldPartLoc);\n\n          LOG.info(\"srcPath:\" + oldPartLoc);\n          LOG.info(\"descPath:\" + newPartLoc);\n          srcFs = wh.getFs(srcPath);\n          destFs = wh.getFs(destPath);\n          // check that src and dest are on the same file system\n          if (srcFs != destFs) {\n            throw new InvalidOperationException(\"table new location \" + destPath\n              + \" is on a different file system than the old location \"\n              + srcPath + \". This operation is not supported\");\n          }\n          try {\n            srcFs.exists(srcPath); // check that src exists and also checks\n            if (newPartLoc.compareTo(oldPartLoc) != 0 && destFs.exists(destPath)) {\n              throw new InvalidOperationException(\"New location for this table \"\n                + tbl.getDbName() + \".\" + tbl.getTableName()\n                + \" already exists : \" + destPath);\n            }\n          } catch (IOException e) {\n            Warehouse.closeFs(srcFs);\n            Warehouse.closeFs(destFs);\n            throw new InvalidOperationException(\"Unable to access new location \"\n              + destPath + \" for partition \" + tbl.getDbName() + \".\"\n              + tbl.getTableName() + \" \" + new_part.getValues());\n          }\n          new_part.getSd().setLocation(newPartLoc);\n          if (MetaStoreUtils.requireCalStats(hiveConf, oldPart, new_part, tbl)) {\n            MetaStoreUtils.updatePartitionStatsFast(new_part, wh, false, true);\n          }\n          msdb.alterPartition(dbname, name, part_vals, new_part);\n        }\n      }\n\n      success = msdb.commitTransaction();\n    } finally {\n      if (!success) {\n        msdb.rollbackTransaction();\n      }\n      if (success && newPartLoc != null && newPartLoc.compareTo(oldPartLoc) != 0) {\n        //rename the data directory\n        try{\n          if (srcFs.exists(srcPath)) {\n            //if destPath's parent path doesn't exist, we should mkdir it\n            Path destParentPath = destPath.getParent();\n            if (!wh.mkdirs(destParentPath, true)) {\n                throw new IOException(\"Unable to create path \" + destParentPath);\n            }\n            wh.renameDir(srcPath, destPath, true);\n            LOG.info(\"rename done!\");\n          }\n        } catch (IOException e) {\n          boolean revertMetaDataTransaction = false;\n          try {\n            msdb.openTransaction();\n            msdb.alterPartition(dbname, name, new_part.getValues(), oldPart);\n            revertMetaDataTransaction = msdb.commitTransaction();\n          } catch (Exception e1) {\n            LOG.error(\"Reverting metadata operation failed During HDFS operation failed\", e1);\n            if (!revertMetaDataTransaction) {\n              msdb.rollbackTransaction();\n            }\n          }\n          throw new InvalidOperationException(\"Unable to access old location \"\n              + srcPath + \" for partition \" + tbl.getDbName() + \".\"\n              + tbl.getTableName() + \" \" + part_vals);\n        }\n      }\n    }\n    return oldPart;\n  }"
        }
    },
    {
        "filename": "HIVE-15997.json",
        "creation_time": "2017-02-21T16:49:46.000+0000",
        "bug_report": {
            "Title": "Resource leaks when query is cancelled",
            "Description": "The bug report highlights potential resource leaks occurring when a query is cancelled in a Hive environment. The logs indicate two primary issues: an `IOException` related to the failure of removing a scratch directory, and an `InterruptedException` encountered while attempting to release a ZooKeeper lock. These issues suggest that interruptions in the process may be due to thread management or resource contention, leading to unhandled resources.",
            "StackTrace": [
                "java.io.IOException: Failed on local exception: java.nio.channels.ClosedByInterruptException",
                "at org.apache.hadoop.hive.ql.Context.removeScratchDir(Context.java:405)",
                "at org.apache.hadoop.hive.ql.Driver.releaseContext(Driver.java:2109)",
                "at org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager.unlockPrimitive(ZooKeeperHiveLockManager.java:488)",
                "at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1472)",
                "at org.apache.hadoop.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:237)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "RootCause": "The root cause of the issue is the failure to properly handle interruptions during the deletion of scratch directories and the release of ZooKeeper locks. Specifically, the `removeScratchDir` method does not adequately manage exceptions that arise from interrupted operations, leading to potential resource leaks.",
            "StepsToReproduce": [
                "1. Initiate a query in the Hive environment.",
                "2. Cancel the query before it completes.",
                "3. Monitor the logs for any warnings or errors related to resource management."
            ],
            "ExpectedBehavior": "When a query is cancelled, all associated resources, including scratch directories and locks, should be released without any errors or leaks.",
            "ObservedBehavior": "Upon cancelling a query, the logs show warnings about failed attempts to remove scratch directories and release ZooKeeper locks, indicating that resources are not being properly cleaned up.",
            "Suggestions": "Review the error handling in the `removeScratchDir` and `unlockPrimitive` methods to ensure that exceptions are managed correctly. Implement additional checks to handle `InterruptedException` and ensure that resources are released even in the event of an interruption.",
            "problem_location": {
                "files": [
                    "ql/src/java/org/apache/hadoop/hive/ql/Context.java",
                    "ql/src/java/org/apache/hadoop/hive/ql/lockmgr/zookeeper/ZooKeeperHiveLockManager.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.ql.Context",
                    "org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager"
                ],
                "methods": [
                    "Context.removeScratchDir",
                    "ZooKeeperHiveLockManager.unlockPrimitive"
                ]
            },
            "possible_fix": "In the `removeScratchDir` method, ensure that all exceptions are logged and handled appropriately to prevent resource leaks. In the `unlockPrimitive` method, enhance the handling of `InterruptedException` to ensure that locks are released even if an interruption occurs."
        },
        "possible_fix_code": {
            "Context.removeScratchDir": "  public void removeScratchDir() {\n    for (Map.Entry<String, Path> entry : fsScratchDirs.entrySet()) {\n      try {\n        Path p = entry.getValue();\n        FileSystem fs = p.getFileSystem(conf);\n        LOG.debug(\"Deleting scratch dir: {}\",  p);\n        fs.delete(p, true);\n        fs.cancelDeleteOnExit(p);\n      } catch (InterruptedException ie) {\n        LOG.warn(\"Thread was interrupted while removing scratch directory: {}\", entry.getValue(), ie);\n        Thread.currentThread().interrupt(); // Restore the interrupted status\n      } catch (Exception e) {\n        LOG.warn(\"Error Removing Scratch: \" + StringUtils.stringifyException(e));\n      }\n    }\n    fsScratchDirs.clear();\n  }",
            "ZooKeeperHiveLockManager.unlockPrimitive": "  static void unlockPrimitive(HiveLock hiveLock, String parent, CuratorFramework curatorFramework) throws LockException {\n    ZooKeeperHiveLock zLock = (ZooKeeperHiveLock)hiveLock;\n    HiveLockMode lMode = hiveLock.getHiveLockMode();\n    HiveLockObject obj = zLock.getHiveLockObject();\n    String name  = getLastObjectName(parent, obj);\n    try {\n      curatorFramework.delete().forPath(zLock.getPath());\n\n      // Delete the parent node if all the children have been deleted\n      List<String> children = curatorFramework.getChildren().forPath(name);\n      if (children == null || children.isEmpty()) {\n        curatorFramework.delete().forPath(name);\n      }\n      Metrics metrics = MetricsFactory.getInstance();\n      if (metrics != null) {\n        try {\n          switch(lMode) {\n          case EXCLUSIVE:\n            metrics.decrementCounter(MetricsConstant.ZOOKEEPER_HIVE_EXCLUSIVELOCKS);\n            break;\n          case SEMI_SHARED:\n            metrics.decrementCounter(MetricsConstant.ZOOKEEPER_HIVE_SEMISHAREDLOCKS);\n            break;\n          default:\n            metrics.decrementCounter(MetricsConstant.ZOOKEEPER_HIVE_SHAREDLOCKS);\n            break;\n          }\n        } catch (Exception e) {\n          LOG.warn(\"Error Reporting hive client zookeeper unlock operation to Metrics system\", e);\n        }\n      }\n    } catch (KeeperException.NoNodeException nne) {\n      LOG.debug(\"Node \" + zLock.getPath() + \" or its parent has already been deleted.\");\n    } catch (KeeperException.NotEmptyException nee) {\n      LOG.debug(\"Node \" + name + \" to be deleted is not empty.\");\n    } catch (InterruptedException ie) {\n      LOG.warn(\"Thread was interrupted while unlocking ZooKeeper lock: \", ie);\n      Thread.currentThread().interrupt(); // Restore the interrupted status\n    } catch (Exception e) {\n      LOG.error(\"Failed to release ZooKeeper lock: \", e);\n      throw new LockException(e);\n    }\n  }"
        }
    },
    {
        "filename": "HIVE-7009.json",
        "creation_time": "2014-05-02T20:50:24.000+0000",
        "bug_report": {
            "Title": "HIVE_USER_INSTALL_DIR could not be set to non-HDFS filesystem",
            "Description": "The issue arises in the `DagUtils.getDefaultDestDir` method within the Hive Tez execution framework. The method enforces that the user path derived from the `HIVE_USER_INSTALL_DIR` configuration must be an HDFS path. This restriction prevents the execution of Hive+Tez jobs on non-HDFS filesystems, such as Azure Blob Storage (WASB). The relevant code checks if the filesystem associated with the user path is an instance of `DistributedFileSystem`, throwing an `IOException` if it is not. This behavior leads to failures when attempting to run jobs with a default filesystem configured to WASB.",
            "StackTrace": [
                "2014-05-01 00:21:39,847 ERROR exec.Task (TezTask.java:execute(192)) - Failed to execute tez graph.",
                "java.io.IOException: wasb://hdi31-chuanliu@clhdistorage.blob.core.windows.net/user is not a hdfs uri",
                "at org.apache.hadoop.hive.ql.exec.tez.DagUtils.getDefaultDestDir(DagUtils.java:662)",
                "at org.apache.hadoop.hive.ql.exec.tez.DagUtils.getHiveJarDirectory(DagUtils.java:759)",
                "at org.apache.hadoop.hive.ql.exec.tez.TezSessionState.createJarLocalResource(TezSessionState.java:321)",
                "at org.apache.hadoop.hive.ql.exec.tez.TezSessionState.open(TezSessionState.java:159)",
                "at org.apache.hadoop.hive.ql.exec.tez.TezTask.execute(TezTask.java:154)",
                "at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:153)",
                "at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:85)",
                "at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1504)",
                "at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1271)",
                "at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1089)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:912)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:902)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:268)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:220)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:423)",
                "at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:793)",
                "at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:686)",
                "at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:625)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:601)",
                "at org.apache.hadoop.util.RunJar.main(RunJar.java:212)"
            ],
            "RootCause": "The root cause of the issue is that the `getDefaultDestDir` method in `DagUtils` is designed to only accept HDFS URIs. When a non-HDFS URI (such as WASB) is provided, it throws an `IOException` indicating that the URI is invalid.",
            "StepsToReproduce": [
                "Set the `HIVE_USER_INSTALL_DIR` configuration to a WASB URI (e.g., `wasb://hdi31-chuanliu@clhdistorage.blob.core.windows.net/user`).",
                "Attempt to execute a Hive+Tez job using this configuration.",
                "Observe the resulting error in the logs indicating the invalid HDFS URI."
            ],
            "ExpectedBehavior": "The system should allow execution of Hive+Tez jobs on non-HDFS filesystems without throwing an exception related to URI validation.",
            "ObservedBehavior": "An `IOException` is thrown stating that the provided WASB URI is not a valid HDFS URI, preventing the job from executing.",
            "Suggestions": "To resolve this issue, either change the URI to a valid HDFS format (e.g., `hdfs://`) or modify the `getDefaultDestDir` method to handle non-HDFS URIs appropriately, allowing for configurations that use Azure Blob Storage.",
            "problem_location": {
                "files": [
                    "DagUtils.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.ql.exec.tez.DagUtils"
                ],
                "methods": [
                    "getDefaultDestDir"
                ]
            },
            "possible_fix": "Modify the `getDefaultDestDir` method to include logic that can handle non-HDFS URIs, or provide a configuration option that allows users to specify the type of filesystem being used."
        },
        "possible_fix_code": {
            "getDefaultDestDir": "  public Path getDefaultDestDir(Configuration conf) throws LoginException, IOException {\n    UserGroupInformation ugi = ShimLoader.getHadoopShims().getUGIForConf(conf);\n    String userName = ShimLoader.getHadoopShims().getShortUserName(ugi);\n    String userPathStr = HiveConf.getVar(conf, HiveConf.ConfVars.HIVE_USER_INSTALL_DIR);\n    Path userPath = new Path(userPathStr);\n    FileSystem fs = userPath.getFileSystem(conf);\n    // Check if the filesystem is an instance of DistributedFileSystem or handle non-HDFS URIs\n    if (!(fs instanceof DistributedFileSystem)) {\n      // Allow non-HDFS URIs, such as WASB, to be used\n      if (!userPathStr.startsWith(\"wasb://\")) {\n        throw new IOException(ErrorMsg.INVALID_HDFS_URI.format(userPathStr));\n      }\n    }\n\n    String jarPathStr = userPathStr + \"/\" + userName;\n    String hdfsDirPathStr = jarPathStr;\n    Path hdfsDirPath = new Path(hdfsDirPathStr);\n\n    FileStatus fstatus = fs.getFileStatus(hdfsDirPath);\n    if (!fstatus.isDir()) {\n      throw new IOException(ErrorMsg.INVALID_DIR.format(hdfsDirPath.toString()));\n    }\n\n    Path retPath = new Path(hdfsDirPath.toString() + \"/.hiveJars\");\n\n    fs.mkdirs(retPath);\n    return retPath;\n  }"
        }
    },
    {
        "filename": "HIVE-2031.json",
        "creation_time": "2011-03-08T11:38:53.000+0000",
        "bug_report": {
            "Title": "Correct the exception message for better traceability when loading into a partitioned table with missing partition specification.",
            "Description": "The current implementation fails when attempting to load data into a partitioned table with two partitions, specifying only one partition in the load statement. The system logs a `SemanticException` indicating that the specified partition '21Oct' could not be found. This issue arises when the load operation does not find the expected partition in the Hive metastore, leading to confusion and difficulty in diagnosing the root cause of the failure.",
            "StackTrace": [
                "org.apache.hadoop.hive.ql.parse.SemanticException: line 1:91 Partition not found '21Oct'",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer$tableSpec.<init>(BaseSemanticAnalyzer.java:685)",
                "at org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer.analyzeInternal(LoadSemanticAnalyzer.java:196)",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:238)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:340)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:736)",
                "at org.apache.hadoop.hive.service.HiveServer$HiveServerHandler.execute(HiveServer.java:151)",
                "at org.apache.hadoop.hive.service.ThriftHive$Processor$execute.process(ThriftHive.java:764)",
                "at org.apache.hadoop.hive.service.ThriftHive$Processor.process(ThriftHive.java:742)",
                "at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:253)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)",
                "at java.lang.Thread.run(Thread.java:619)"
            ],
            "RootCause": "The root cause of the issue is that the load operation is attempting to access a partition '21Oct' that does not exist in the Hive table. The `analyzeInternal` method in the `LoadSemanticAnalyzer` class checks for the existence of the partition and throws a `SemanticException` if it is not found.",
            "StepsToReproduce": [
                "1. Create a partitioned table in Hive with two partitions.",
                "2. Attempt to load data into the table by specifying only one of the partitions (e.g., '21Oct').",
                "3. Observe the exception message logged by the system."
            ],
            "ExpectedBehavior": "The system should successfully load data into the specified partition if it exists, or provide a clear and informative error message if the partition does not exist.",
            "ObservedBehavior": "The system throws a `SemanticException` indicating that the partition '21Oct' could not be found, which does not provide sufficient context for diagnosing the issue.",
            "Suggestions": "To resolve the issue, verify the existence of the partition '21Oct' using the `getPartitions` method. If the partition does not exist, create it or ensure that the data is loaded into the correct partition. Additionally, improve the error message to include suggestions for checking existing partitions.",
            "problem_location": {
                "files": [
                    "LoadSemanticAnalyzer.java",
                    "Hive.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer",
                    "org.apache.hadoop.hive.ql.metadata.Hive"
                ],
                "methods": [
                    "LoadSemanticAnalyzer.analyzeInternal",
                    "Hive.getPartition",
                    "Hive.getPartitions"
                ]
            },
            "possible_fix": "Implement a check in the `analyzeInternal` method to verify the existence of the specified partition using `getPartitions`. If the partition is not found, log a more informative error message that suggests checking the available partitions."
        },
        "possible_fix_code": {
            "LoadSemanticAnalyzer.analyzeInternal": "  public void analyzeInternal(ASTNode ast) throws SemanticException {\n    isLocal = false;\n    isOverWrite = false;\n    Tree fromTree = ast.getChild(0);\n    Tree tableTree = ast.getChild(1);\n\n    if (ast.getChildCount() == 4) {\n      isLocal = true;\n      isOverWrite = true;\n    }\n\n    if (ast.getChildCount() == 3) {\n      if (ast.getChild(2).getText().toLowerCase().equals(\"local\")) {\n        isLocal = true;\n      } else {\n        isOverWrite = true;\n      }\n    }\n\n    // initialize load path\n    URI fromURI;\n    try {\n      String fromPath = stripQuotes(fromTree.getText());\n      fromURI = initializeFromURI(fromPath);\n    } catch (IOException e) {\n      throw new SemanticException(ErrorMsg.INVALID_PATH.getMsg(fromTree, e\n          .getMessage()), e);\n    } catch (URISyntaxException e) {\n      throw new SemanticException(ErrorMsg.INVALID_PATH.getMsg(fromTree, e\n          .getMessage()), e);\n    }\n\n    // initialize destination table/partition\n    tableSpec ts = new tableSpec(db, conf, (ASTNode) tableTree);\n\n    if (ts.tableHandle.isOffline()){\n      throw new SemanticException(\n          ErrorMsg.OFFLINE_TABLE_OR_PARTITION.getMsg(\":Table \" + ts.tableName));\n    }\n\n    if (ts.tableHandle.isView()) {\n      throw new SemanticException(ErrorMsg.DML_AGAINST_VIEW.getMsg());\n    }\n    if (ts.tableHandle.isNonNative()) {\n      throw new SemanticException(ErrorMsg.LOAD_INTO_NON_NATIVE.getMsg());\n    }\n    URI toURI = (ts.partHandle != null) ? ts.partHandle.getDataLocation()\n        : ts.tableHandle.getDataLocation();\n\n    List<FieldSchema> parts = ts.tableHandle.getPartitionKeys();\n    if (isOverWrite && (parts != null && parts.size() > 0)\n        && (ts.partSpec == null || ts.partSpec.size() == 0)) {\n      throw new SemanticException(ErrorMsg.NEED_PARTITION_ERROR.getMsg());\n    }\n\n    // make sure the arguments make sense\n    applyConstraints(fromURI, toURI, fromTree, isLocal);\n\n    Task<? extends Serializable> rTask = null;\n\n    // create copy work\n    if (isLocal) {\n      String copyURIStr = ctx.getExternalTmpFileURI(toURI);\n      URI copyURI = URI.create(copyURIStr);\n      rTask = TaskFactory.get(new CopyWork(fromURI.toString(), copyURIStr),\n          conf);\n      fromURI = copyURI;\n    }\n\n    // create final load/move work\n\n    String loadTmpPath = ctx.getExternalTmpFileURI(toURI);\n    Map<String, String> partSpec = ts.getPartSpec();\n    if (partSpec == null) {\n      partSpec = new LinkedHashMap<String, String>();\n    } else {\n      try{\n        Partition part = Hive.get().getPartition(ts.tableHandle, partSpec, false);\n        if (part == null) {\n          List<Partition> existingParts = Hive.get().getPartitions(ts.tableHandle);\n          StringBuilder availablePartitions = new StringBuilder();\n          for (Partition existingPart : existingParts) {\n            availablePartitions.append(existingPart.getName()).append(\" \");\n          }\n          throw new SemanticException(ErrorMsg.PARTITION_NOT_FOUND.getMsg(partSpec.toString() + \", available partitions: \" + availablePartitions.toString()));\n        }\n        if (part.isOffline()) {\n          throw new SemanticException(ErrorMsg.OFFLINE_TABLE_OR_PARTITION.\n              getMsg(ts.tableName + \":\" + part.getName()));\n        }\n      } catch(HiveException e) {\n        throw new SemanticException(e);\n      }\n    }\n\n    LoadTableDesc loadTableWork = new LoadTableDesc(fromURI.toString(),\n        loadTmpPath, Utilities.getTableDesc(ts.tableHandle), partSpec, isOverWrite);\n\n    if (rTask != null) {\n      rTask.addDependentTask(TaskFactory.get(new MoveWork(getInputs(),\n          getOutputs(), loadTableWork, null, true), conf));\n    } else {\n      rTask = TaskFactory.get(new MoveWork(getInputs(), getOutputs(),\n          loadTableWork, null, true), conf);\n    }\n\n    rootTasks.add(rTask);\n  }"
        }
    },
    {
        "filename": "HIVE-4018.json",
        "creation_time": "2013-02-13T09:02:20.000+0000",
        "bug_report": {
            "Title": "MapJoin failing with Distributed Cache error",
            "Description": "The issue arises when executing a star join query after the implementation of HIVE-3784, resulting in a failure due to a Distributed Cache error. The error log indicates an `EOFException` occurring in the `MapJoinOperator` during the loading of a hash table. This suggests potential problems with the input data or the file handling processes involved in loading the hash table.",
            "StackTrace": [
                "2013-02-13 08:36:04,584 ERROR org.apache.hadoop.hive.ql.exec.MapJoinOperator: Load Distributed Cache Error",
                "2013-02-13 08:36:04,585 FATAL ExecMapper: org.apache.hadoop.hive.ql.metadata.HiveException: java.io.EOFException",
                "at org.apache.hadoop.hive.ql.exec.MapJoinOperator.loadHashTable(MapJoinOperator.java:189)",
                "at org.apache.hadoop.hive.ql.exec.MapJoinOperator.cleanUpInputFileChangedOp(MapJoinOperator.java:203)",
                "at org.apache.hadoop.hive.ql.exec.Operator.cleanUpInputFileChanged(Operator.java:1421)",
                "at org.apache.hadoop.hive.ql.exec.Operator.cleanUpInputFileChanged(Operator.java:1425)",
                "at org.apache.hadoop.hive.ql.exec.Operator.cleanUpInputFileChanged(Operator.java:1425)",
                "at org.apache.hadoop.hive.ql.exec.Operator.cleanUpInputFileChanged(Operator.java:1425)",
                "at org.apache.hadoop.hive.ql.exec.Operator.cleanUpInputFileChanged(Operator.java:1425)",
                "at org.apache.hadoop.hive.ql.exec.Operator.cleanUpInputFileChanged(Operator.java:1425)",
                "at org.apache.hadoop.hive.ql.exec.Operator.cleanUpInputFileChanged(Operator.java:1425)",
                "at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:614)",
                "at org.apache.hadoop.hive.ql.exec.ExecMapper.map(ExecMapper.java:144)",
                "at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:391)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:325)",
                "at org.apache.hadoop.mapred.Child$4.run(Child.java:266)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:416)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1278)",
                "at org.apache.hadoop.mapred.Child.main(Child.java:260)"
            ],
            "RootCause": "The `EOFException` is likely caused by issues with reading a hash table file that may be incomplete or corrupted. This stems from a lack of error handling during the writing process in the `flushMemoryCacheToPersistent` method of the `HashMapWrapper` class, which could lead to an improperly written file.",
            "StepsToReproduce": [
                "Execute a star join query after implementing HIVE-3784.",
                "Monitor the logs for any errors related to Distributed Cache.",
                "Observe the occurrence of the `EOFException` in the logs."
            ],
            "ExpectedBehavior": "The system should successfully load the hash table from the Distributed Cache without any exceptions, allowing the star join query to execute correctly.",
            "ObservedBehavior": "The system fails to load the hash table, resulting in an `EOFException` and an error message indicating a Distributed Cache error.",
            "Suggestions": "Implement error handling and logging in the `flushMemoryCacheToPersistent` method to ensure that any issues during the writing process are captured. Additionally, validate the integrity of the files before attempting to read them in the `initilizePersistentHash` method.",
            "problem_location": {
                "files": [
                    "ql/src/java/org/apache/hadoop/hive/ql/exec/MapJoinOperator.java",
                    "ql/src/java/org/apache/hadoop/hive/ql/exec/persistence/HashMapWrapper.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.ql.exec.MapJoinOperator",
                    "org.apache.hadoop.hive.ql.exec.persistence.HashMapWrapper"
                ],
                "methods": [
                    "MapJoinOperator.loadHashTable",
                    "HashMapWrapper.initilizePersistentHash",
                    "HashMapWrapper.flushMemoryCacheToPersistent"
                ]
            },
            "possible_fix": "Add error handling in the `flushMemoryCacheToPersistent` method to log any exceptions that occur during the file writing process. Ensure that the file is fully written and closed before it is read in the `initilizePersistentHash` method."
        },
        "possible_fix_code": {
            "HashMapWrapper.flushMemoryCacheToPersistent": "  public long flushMemoryCacheToPersistent(File file) throws IOException {\n    ObjectOutputStream outputStream = null;\n    try {\n      outputStream = new ObjectOutputStream(new FileOutputStream(file));\n      outputStream.writeObject(mHash);\n      outputStream.flush();\n    } catch (IOException e) {\n      LOG.error(\"Error writing to file: \" + file.getAbsolutePath(), e);\n      throw e; // rethrow the exception after logging\n    } finally {\n      if (outputStream != null) {\n        outputStream.close();\n      }\n    }\n    return file.length();\n  }",
            "HashMapWrapper.initilizePersistentHash": "  public void initilizePersistentHash(String fileName) throws IOException, ClassNotFoundException {\n    ObjectInputStream inputStream = null;\n    try {\n      inputStream = new ObjectInputStream(new FileInputStream(fileName));\n      HashMap<K, V> hashtable = (HashMap<K, V>) inputStream.readObject();\n      this.setMHash(hashtable);\n    } catch (EOFException e) {\n      LOG.error(\"EOFException while reading from file: \" + fileName, e);\n      throw e; // rethrow the exception after logging\n    } finally {\n      if (inputStream != null) {\n        inputStream.close();\n      }\n    }\n  }"
        }
    },
    {
        "filename": "HIVE-11255.json",
        "creation_time": "2015-07-14T15:39:11.000+0000",
        "bug_report": {
            "Title": "get_table_objects_by_name() in HiveMetaStore.java needs to retrieve table objects in multiple batches",
            "Description": "The `get_table_objects_by_name()` function in `HiveMetaStore.java` currently attempts to retrieve all table objects for a given database in a single query. This approach leads to a `java.sql.SQLSyntaxErrorException: ORA-01795: maximum number of expressions in a list is 1000` error when the number of tables exceeds 1000, which is the limit imposed by Oracle databases. To resolve this issue, the function should be modified to split the list of table names into smaller batches, similar to the implementation used in the drop database operation.",
            "StackTrace": [
                "2015-06-29 13:36:00,093 ERROR org.apache.hadoop.hive.metastore.RetryingHMSHandler: Retrying HMSHandler after 1000 ms (attempt 1 of 1) with error: javax.jdo.JDOException: Exception thrown when executing query",
                "at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:596)",
                "at org.datanucleus.api.jdo.JDOQuery.execute(JDOQuery.java:275)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.getTableObjectsByName(ObjectStore.java:945)",
                "at sun.reflect.GeneratedMethodAccessor73.invoke(Unknown Source)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:108)",
                "at com.sun.proxy.$Proxy0.getTableObjectsByName(Unknown Source)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_table_objects_by_name(HiveMetaStore.java:1618)",
                "at sun.reflect.GeneratedMethodAccessor72.invoke(Unknown Source)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:106)",
                "at com.sun.proxy.$Proxy5.get_table_objects_by_name(Unknown Source)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$get_table_objects_by_name.getResult(ThriftHiveMetastore.java:8172)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$get_table_objects_by_name.getResult(ThriftHiveMetastore.java:8156)",
                "at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)",
                "at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:110)",
                "at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:107)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1642)",
                "at org.apache.hadoop.hive.shims.HadoopShimsSecure.doAs(HadoopShimsSecure.java:502)",
                "at org.apache.hadoop.hive.metastore.TUGIBasedProcessor.process(TUGIBasedProcessor.java:118)",
                "at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:244)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:745)",
                "NestedThrowablesStackTrace:",
                "java.sql.SQLSyntaxErrorException: ORA-01795: maximum number of expressions in a list is 1000"
            ],
            "RootCause": "The root cause of the issue is that the `get_table_objects_by_name()` method attempts to execute a query with a list of table names that exceeds the maximum limit of 1000 expressions allowed by Oracle databases. This results in a SQL syntax error.",
            "StepsToReproduce": [
                "1. Call the `get_table_objects_by_name()` method with a database name and a list of table names that exceeds 1000 entries.",
                "2. Observe the SQLSyntaxErrorException being thrown due to exceeding the maximum number of expressions in the query."
            ],
            "ExpectedBehavior": "The method should successfully retrieve table objects in multiple batches without exceeding the maximum limit of 1000 expressions in a single query.",
            "ObservedBehavior": "The method throws a `java.sql.SQLSyntaxErrorException: ORA-01795: maximum number of expressions in a list is 1000` error when the number of table names exceeds 1000.",
            "Suggestions": "Refactor the `get_table_objects_by_name()` method to split the list of table names into smaller batches (e.g., 1000 or fewer) and execute multiple queries to retrieve all table objects without exceeding the limit.",
            "problem_location": {
                "files": [
                    "HiveMetaStore.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.metastore.ObjectStore"
                ],
                "methods": [
                    "getTableObjectsByName"
                ]
            },
            "possible_fix": "Modify the `getTableObjectsByName` method to include logic for batching the list of table names. For example:\n\n```java\nList<Table> getTableObjectsByName(String db, List<String> tbl_names) throws MetaException, UnknownDBException {\n    List<Table> tables = new ArrayList<>();\n    int batchSize = 1000;\n    for (int i = 0; i < tbl_names.size(); i += batchSize) {\n        List<String> batch = tbl_names.subList(i, Math.min(i + batchSize, tbl_names.size()));\n        // Execute query for the current batch\n        // Add results to tables list\n    }\n    return tables;\n}\n```"
        },
        "possible_fix_code": {
            "getTableObjectsByName": "  @Override\n  public List<Table> getTableObjectsByName(String db, List<String> tbl_names) throws MetaException,\n      UnknownDBException {\n    List<Table> tables = new ArrayList<Table>();\n    boolean committed = false;\n    Query dbExistsQuery = null;\n    Query query = null;\n    int batchSize = 1000; // Define the batch size\n    try {\n      openTransaction();\n      db = HiveStringUtils.normalizeIdentifier(db);\n      dbExistsQuery = pm.newQuery(MDatabase.class, \"name == db\");\n      dbExistsQuery.declareParameters(\"java.lang.String db\");\n      dbExistsQuery.setUnique(true);\n      dbExistsQuery.setResult(\"name\");\n      String dbNameIfExists = (String) dbExistsQuery.execute(db);\n      if (dbNameIfExists == null || dbNameIfExists.isEmpty()) {\n        throw new UnknownDBException(\"Could not find database \" + db);\n      }\n\n      List<String> lowered_tbl_names = new ArrayList<String>();\n      for (String t : tbl_names) {\n        lowered_tbl_names.add(HiveStringUtils.normalizeIdentifier(t));\n      }\n\n      // Process in batches\n      for (int i = 0; i < lowered_tbl_names.size(); i += batchSize) {\n        List<String> batch = lowered_tbl_names.subList(i, Math.min(i + batchSize, lowered_tbl_names.size()));\n        query = pm.newQuery(MTable.class);\n        query.setFilter(\"database.name == db && tbl_names.contains(tableName)\");\n        query.declareParameters(\"java.lang.String db, java.util.Collection tbl_names\");\n        Collection mtables = (Collection) query.execute(db, batch);\n        for (Iterator iter = mtables.iterator(); iter.hasNext();) {\n          tables.add(convertToTable((MTable) iter.next()));\n        }\n      }\n      committed = commitTransaction();\n    } finally {\n      if (!committed) {\n        rollbackTransaction();\n      }\n      if (dbExistsQuery != null) {\n        dbExistsQuery.closeAll();\n      }\n      if (query != null) {\n        query.closeAll();\n      }\n    }\n    return tables;\n  }"
        }
    },
    {
        "filename": "HIVE-10151.json",
        "creation_time": "2015-03-31T00:17:22.000+0000",
        "bug_report": {
            "Title": "Insert into A select from B fails with ACID tables due to filename convention issue",
            "Description": "The issue arises when performing an insert operation from one ACID table to another, both of which are bucketed identically. The `BucketingSortingReduceSinkOptimizer` utilizes `BucketizedHiveInputFormat`, which bypasses the ORC merge logic during read operations. This leads to the system attempting to process bucket files directly, resulting in an error when the filenames do not conform to the expected naming conventions. Specifically, the filename `delta_0000001_0000001` does not start with the required prefix `base_`, causing a `java.lang.IllegalArgumentException` during the execution of the `generateSplitsInfo` method in `OrcInputFormat`.",
            "StackTrace": [
                "2015-04-29 13:57:35,807 ERROR [main]: exec.Task (SessionState.java:printError(956)) - Job Submission failed with exception 'java.lang.RuntimeException(serious problem)'",
                "java.lang.RuntimeException: serious problem",
                "at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.generateSplitsInfo(OrcInputFormat.java:1021)",
                "at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getSplits(OrcInputFormat.java:1048)",
                "at org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat.getSplits(BucketizedHiveInputFormat.java:141)",
                "Caused by: java.lang.IllegalArgumentException: delta_0000001_0000001 does not start with base_",
                "at org.apache.hadoop.hive.ql.io.AcidUtils.parseBase(AcidUtils.java:144)",
                "at org.apache.hadoop.hive.ql.io.AcidUtils.parseBaseBucketFilename(AcidUtils.java:172)"
            ],
            "RootCause": "The root cause of the issue is that the filename `delta_0000001_0000001` does not conform to the expected naming convention for ACID transaction files, which requires filenames to start with `base_`. This mismatch leads to an `IllegalArgumentException` during the processing of the files.",
            "StepsToReproduce": [
                "Create two ACID tables with identical bucketing configurations.",
                "Insert data into the first table.",
                "Attempt to insert data from the first table into the second table using an insert statement."
            ],
            "ExpectedBehavior": "The insert operation should successfully transfer data from one ACID table to another without errors, adhering to the expected file naming conventions.",
            "ObservedBehavior": "The insert operation fails with a `java.lang.RuntimeException` indicating a serious problem due to an `IllegalArgumentException` related to filename conventions.",
            "Suggestions": "Ensure that the filenames generated for ACID transactions conform to the expected naming conventions. This may involve modifying the logic that generates these filenames in the `AcidUtils` class.",
            "problem_location": {
                "files": [
                    "ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java",
                    "ql/src/java/org/apache/hadoop/hive/ql/io/AcidUtils.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.ql.io.orc.OrcInputFormat",
                    "org.apache.hadoop.hive.ql.io.AcidUtils"
                ],
                "methods": [
                    "OrcInputFormat.generateSplitsInfo",
                    "OrcInputFormat.getSplits",
                    "AcidUtils.parseBase",
                    "AcidUtils.parseBaseBucketFilename"
                ]
            },
            "possible_fix": "Modify the filename generation logic in the `AcidUtils` class to ensure that all filenames for ACID transactions start with the prefix `base_`. This may involve reviewing the methods `parseBase` and `parseBaseBucketFilename` to enforce the naming convention."
        },
        "possible_fix_code": {
            "AcidUtils.parseBase": "  static long parseBase(Path path) {\n    String filename = path.getName();\n    if (filename.startsWith(BASE_PREFIX)) {\n      return Long.parseLong(filename.substring(BASE_PREFIX.length()));\n    }\n    // Fix: Log the error and throw a more descriptive exception\n    String errorMessage = filename + \" does not start with \" + BASE_PREFIX;\n    LOG.error(errorMessage);\n    throw new IllegalArgumentException(errorMessage);\n  }",
            "AcidUtils.parseBaseBucketFilename": "  public static AcidOutputFormat.Options\n                    parseBaseBucketFilename(Path bucketFile,\n                                            Configuration conf) {\n    AcidOutputFormat.Options result = new AcidOutputFormat.Options(conf);\n    String filename = bucketFile.getName();\n    result.writingBase(true);\n    // Fix: Ensure filenames start with base_ before processing\n    if (!filename.startsWith(BASE_PREFIX)) {\n      String errorMessage = filename + \" does not start with \" + BASE_PREFIX;\n      LOG.error(errorMessage);\n      throw new IllegalArgumentException(errorMessage);\n    }\n    if (ORIGINAL_PATTERN.matcher(filename).matches()) {\n      int bucket =\n          Integer.parseInt(filename.substring(0, filename.indexOf('_')));\n      result\n          .setOldStyle(true)\n          .minimumTransactionId(0)\n          .maximumTransactionId(0)\n          .bucket(bucket);\n    } else if (filename.startsWith(BUCKET_PREFIX)) {\n      int bucket =\n          Integer.parseInt(filename.substring(filename.indexOf('_') + 1));\n      result\n          .setOldStyle(false)\n          .minimumTransactionId(0)\n          .maximumTransactionId(parseBase(bucketFile.getParent()))\n          .bucket(bucket);\n    } else {\n      result.setOldStyle(true).bucket(-1).minimumTransactionId(0)\n          .maximumTransactionId(0);\n    }\n    return result;\n  }"
        }
    },
    {
        "filename": "HIVE-13546.json",
        "creation_time": "2016-04-19T07:43:42.000+0000",
        "bug_report": {
            "Title": "Patch for HIVE-12893 is broken in branch-1",
            "Description": "The SQL query fails during execution due to a runtime error in the Hive processing framework. The error occurs specifically in the `ExecReducer` class while processing a row, leading to an `IndexOutOfBoundsException`. This suggests that the code is attempting to access an element in a list that does not exist, likely due to the input data not conforming to the expected structure, particularly in the context of dynamic partitioning.",
            "StackTrace": [
                "2016-04-19 15:15:35,252 FATAL [main] ExecReducer: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {\"key\":{\"reducesinkkey0\":null},\"value\":{\"_col0\":null,\"_col1\":5588,\"_col2\":170300,\"_col3\":null,\"_col4\":756,\"_col5\":91384,\"_col6\":16,\"_col7\":null,\"_col8\":855582,\"_col9\":28,\"_col10\":null,\"_col11\":48.83,\"_col12\":null,\"_col13\":0.0,\"_col14\":null,\"_col15\":899.64,\"_col16\":null,\"_col17\":6.14,\"_col18\":0.0,\"_col19\":null,\"_col20\":null,\"_col21\":null,\"_col22\":null}}",
                "Caused by: java.lang.IndexOutOfBoundsException: Index: 0, Size: 0",
                "at java.util.ArrayList.rangeCheck(ArrayList.java:653)",
                "at java.util.ArrayList.get(ArrayList.java:429)",
                "at org.apache.hadoop.hive.common.FileUtils.makePartName(FileUtils.java:151)",
                "at org.apache.hadoop.hive.common.FileUtils.makePartName(FileUtils.java:131)",
                "at org.apache.hadoop.hive.ql.exec.FileSinkOperator.getDynPartDirectory(FileSinkOperator.java:1003)",
                "at org.apache.hadoop.hive.ql.exec.FileSinkOperator.getDynOutPaths(FileSinkOperator.java:919)",
                "at org.apache.hadoop.hive.ql.exec.FileSinkOperator.process(FileSinkOperator.java:713)",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:244)"
            ],
            "RootCause": "The root cause of the issue is an `IndexOutOfBoundsException` occurring in the `ExecReducer` class, specifically when trying to access an index in a list that is empty. This is likely due to the `row` list passed to the `getDynPartDirectory` method being empty or not containing enough elements to match the expected number of dynamic partition column names.",
            "StepsToReproduce": [
                "Set the necessary Hive configuration parameters as specified in the original report.",
                "Execute the provided SQL query against the `tpcds_bin_partitioned_orc_4` dataset.",
                "Observe the error in the logs indicating a `HiveException` during the reduce phase."
            ],
            "ExpectedBehavior": "The SQL query should execute successfully, processing the data and writing the results to the specified output table without any runtime errors.",
            "ObservedBehavior": "The execution fails with a `HiveException` indicating an `IndexOutOfBoundsException`, preventing the successful processing of the data.",
            "Suggestions": "Ensure that the input data conforms to the expected structure, particularly for dynamic partitioning. Validate that the `row` list contains the necessary elements before attempting to access them. Consider adding checks to handle cases where the input data may be incomplete or malformed.",
            "problem_location": {
                "files": [
                    "ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecReducer.java",
                    "ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java",
                    "common/src/java/org/apache/hadoop/hive/common/FileUtils.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.ql.exec.mr.ExecReducer",
                    "org.apache.hadoop.hive.ql.exec.FileSinkOperator",
                    "org.apache.hadoop.hive.common.FileUtils"
                ],
                "methods": [
                    "ExecReducer.reduce",
                    "FileSinkOperator.getDynOutPaths",
                    "FileSinkOperator.getDynPartDirectory",
                    "FileUtils.makePartName"
                ]
            },
            "possible_fix": "Add validation checks in the `getDynPartDirectory` and `makePartName` methods to ensure that the `row` and `partCols` lists are not empty before attempting to access their elements. For example, modify `makePartName` to check the sizes of `partCols` and `vals` before iterating over them."
        },
        "possible_fix_code": {
            "FileUtils.makePartName": "  public static String makePartName(List<String> partCols, List<String> vals,\n      String defaultStr) {\n    if (partCols == null || vals == null || partCols.size() != vals.size() || partCols.isEmpty()) {\n        throw new IllegalArgumentException(\"Partition columns and values must not be null and must have the same size.\");\n    }\n    StringBuilder name = new StringBuilder();\n    for (int i = 0; i < partCols.size(); i++) {\n      if (i > 0) {\n        name.append(Path.SEPARATOR);\n      }\n      name.append(escapePathName((partCols.get(i)).toLowerCase(), defaultStr));\n      name.append('=');\n      name.append(escapePathName(vals.get(i), defaultStr));\n    }\n    return name.toString();\n  }"
        }
    },
    {
        "filename": "HIVE-7049.json",
        "creation_time": "2014-05-12T21:46:48.000+0000",
        "bug_report": {
            "Title": "Unable to deserialize AVRO data when file schema and record schema are different and nullable",
            "Description": "The issue arises during the deserialization of AVRO data when there is a mismatch between the file schema and the record schema, particularly when the record schema is nullable while the file schema is not. This leads to an exception indicating that a union type is expected but a simple type is provided. The problematic code is located in the `AvroDeserialize` class, specifically in the method that checks for nullable types. The current implementation does not validate the file schema against the expected union type, which is crucial for successful deserialization.",
            "StackTrace": [
                "org.apache.avro.AvroRuntimeException: Not a union: \"string\"",
                "at org.apache.avro.Schema.getTypes(Schema.java:272)",
                "at org.apache.hadoop.hive.serde2.avro.AvroDeserializer.deserializeNullableUnion(AvroDeserializer.java:275)",
                "at org.apache.hadoop.hive.serde2.avro.AvroDeserializer.worker(AvroDeserializer.java:205)",
                "at org.apache.hadoop.hive.serde2.avro.AvroDeserializer.workerBase(AvroDeserializer.java:188)",
                "at org.apache.hadoop.hive.serde2.avro.AvroDeserializer.deserialize(AvroDeserializer.java:174)",
                "at org.apache.hadoop.hive.serde2.avro.TestAvroDeserializer.verifyNullableType(TestAvroDeserializer.java:487)",
                "at org.apache.hadoop.hive.serde2.avro.TestAvroDeserializer.canDeserializeNullableTypes(TestAvroDeserializer.java:407)"
            ],
            "RootCause": "The root cause of the issue is that the deserialization process does not account for the file schema not being nullable when the record schema is. This results in an attempt to deserialize a non-union type where a union type is expected, leading to the 'Not a union' exception.",
            "StepsToReproduce": [
                "1. Create an AVRO file with a schema that is not nullable (e.g., fileSchema = 'string').",
                "2. Define a record schema that is nullable (e.g., recordSchema = ['null', 'string']).",
                "3. Attempt to deserialize the AVRO file using the defined schemas."
            ],
            "ExpectedBehavior": "The system should successfully deserialize the AVRO data without throwing an exception, correctly handling the nullable record schema against the non-nullable file schema.",
            "ObservedBehavior": "An exception is thrown during deserialization, indicating that a union type was expected but a simple string type was provided.",
            "Suggestions": "To resolve this issue, the deserialization logic should be updated to check both the record schema and the file schema for compatibility, particularly ensuring that if the record schema is nullable, the file schema must also accommodate this.",
            "problem_location": {
                "files": [
                    "AvroDeserialize.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.serde2.avro.AvroDeserializer"
                ],
                "methods": [
                    "deserializeNullableUnion"
                ]
            },
            "possible_fix": "Modify the `deserializeNullableUnion` method in `AvroDeserializer` to include a check for the file schema's nullability before proceeding with deserialization. This could involve adding a validation step that ensures both schemas are compatible."
        },
        "possible_fix_code": {
            "deserializeNullableUnion": "  private Object deserializeNullableUnion(Object datum, Schema fileSchema, Schema recordSchema,\n                                          TypeInfo columnType) throws AvroSerdeException {\n    // Check if the file schema is nullable\n    if (fileSchema != null && !fileSchema.getType().equals(Schema.Type.UNION)) {\n        throw new AvroSerdeException(\"File schema must be a union type when record schema is nullable.\");\n    }\n\n    int tag = GenericData.get().resolveUnion(recordSchema, datum); // Determine index of value\n    Schema schema = recordSchema.getTypes().get(tag);\n    if(schema.getType().equals(Schema.Type.NULL)) {\n      return null;\n    }\n\n    return worker(datum, fileSchema == null ? null : fileSchema.getTypes().get(tag), schema,\n        SchemaToTypeInfo.generateTypeInfo(schema));\n  }"
        }
    },
    {
        "filename": "HIVE-9755.json",
        "creation_time": "2015-02-23T20:50:43.000+0000",
        "bug_report": {
            "Title": "Hive built-in \"ngram\" UDAF fails when a mapper has no matches.",
            "Description": "The issue arises when executing a Hive query that utilizes the built-in n-gram User Defined Aggregate Function (UDAF). Specifically, the error occurs when the input data contains null values, leading to a runtime exception during the aggregation process. The error message indicates a mismatch in the value for 'n', which is expected to be a constant but is found to be variable across different rows. This suggests that the query is not structured correctly to handle cases where the input data may not meet the expected criteria.",
            "StackTrace": [
                "2015-01-08 09:15:00,262 FATAL ExecReducer: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {\"key\":{},\"value\":{\"_col0\":[\"0\",\"0\",\"0\",\"0\"]},\"alias\":0}",
                "at org.apache.hadoop.hive.ql.exec.ExecReducer.reduce(ExecReducer.java:258)",
                "at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:506)",
                "at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:447)",
                "at org.apache.hadoop.mapred.Child$4.run(Child.java:268)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:396)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1408)",
                "at org.apache.hadoop.mapred.Child.main(Child.java:262)",
                "Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: GenericUDAFnGramEvaluator: mismatch in value for 'n', which usually is caused by a non-constant expression. Found '0' and '1'.",
                "at org.apache.hadoop.hive.ql.udf.generic.GenericUDAFnGrams$GenericUDAFnGramEvaluator.merge(GenericUDAFnGrams.java:242)",
                "at org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator.aggregate(GenericUDAFEvaluator.java:142)",
                "at org.apache.hadoop.hive.ql.exec.GroupByOperator.updateAggregations(GroupByOperator.java:658)",
                "at org.apache.hadoop.hive.ql.exec.GroupByOperator.processAggr(GroupByOperator.java:911)",
                "at org.apache.hadoop.hive.ql.exec.GroupByOperator.processKey(GroupByOperator.java:753)",
                "at org.apache.hadoop.hive.ql.exec.GroupByOperator.processOp(GroupByOperator.java:819)",
                "at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:474)",
                "at org.apache.hadoop.hive.ql.exec.ExecReducer.reduce(ExecReducer.java:249)"
            ],
            "RootCause": "The root cause of the issue is a mismatch in the value for 'n' in the `GenericUDAFnGramEvaluator`, which occurs when a non-constant expression is used instead of a constant. This is triggered by the presence of null values in the input data, leading to inconsistent values for 'n' during the aggregation process.",
            "StepsToReproduce": [
                "Create a Hive table named 'ngramtest' with columns 'col1' (int) and 'col3' (string).",
                "Insert rows into 'ngramtest' where 'col1' is 0 and 'col3' contains null values.",
                "Execute the query: SELECT explode(ngrams(sentences(lower(t.col3)), 3, 10)) as x FROM (SELECT col3 FROM ngramtest WHERE col1=0) t."
            ],
            "ExpectedBehavior": "The query should execute successfully and return the expected n-grams without any runtime errors, even if some input rows contain null values.",
            "ObservedBehavior": "The query fails with a runtime error indicating a mismatch in the value for 'n', specifically when processing rows that contain null values.",
            "Suggestions": "To resolve this issue, ensure that the value of 'n' is constant in the Hive query. Avoid using variable or non-constant expressions for 'n' when calling the `GenericUDAFnGrams` function. Additionally, consider filtering out null values from the input data before processing.",
            "problem_location": {
                "files": [
                    "ExecReducer.java",
                    "GenericUDAFnGrams.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.ql.exec.ExecReducer",
                    "org.apache.hadoop.hive.ql.udf.generic.GenericUDAFnGrams"
                ],
                "methods": [
                    "ExecReducer.reduce",
                    "GenericUDAFnGrams.GenericUDAFnGramEvaluator.merge"
                ]
            },
            "possible_fix": "Modify the query to ensure that 'n' is a constant value. For example, replace any variable expressions with a fixed integer value. Additionally, implement a check to filter out null values from 'col3' before executing the n-gram function."
        },
        "possible_fix_code": {
            "GenericUDAFnGrams.merge": "    public void merge(AggregationBuffer agg, Object partial) throws HiveException {\n      if(partial == null) {\n        return;\n      }\n      NGramAggBuf myagg = (NGramAggBuf) agg;\n      List<Text> partialNGrams = (List<Text>) loi.getList(partial);\n      int n = Integer.parseInt(partialNGrams.get(partialNGrams.size()-1).toString());\n      // Ensure 'n' is constant and handle null values\n      if (n <= 0) {\n          throw new HiveException(getClass().getSimpleName() + \": 'n' must be a positive constant value.\");\n      }\n      if(myagg.n > 0 && myagg.n != n) {\n        throw new HiveException(getClass().getSimpleName() + \": mismatch in value for 'n'\"\n            + \", which usually is caused by a non-constant expression. Found '\"+n+\"' and '\"\n            + myagg.n + \"'.\");\n      }\n      myagg.n = n;\n      partialNGrams.remove(partialNGrams.size()-1);\n      myagg.nge.merge(partialNGrams);\n    }"
        }
    },
    {
        "filename": "HIVE-19130.json",
        "creation_time": "2018-04-09T10:18:33.000+0000",
        "bug_report": {
            "Title": "NPE is thrown when REPL LOAD applied drop partition event.",
            "Description": "During incremental replication, a NullPointerException (NPE) occurs when executing a REPL LOAD on the second batch of events. The issue arises specifically when the `dropPartitions` method attempts to drop partitions from a Hive table, which is called after a sequence of operations that may leave the Hive metadata in an inconsistent state. The sequence of operations includes creating a table, adding a partition, and dropping that partition, followed by dropping the table and repeating the creation and partition operations. The NPE is triggered in the `getPartitionsByExpr` method due to null parameters being passed, likely stemming from an improperly initialized `DropTableDesc` object.",
            "StackTrace": [
                "2018-04-05 16:20:36,531 ERROR [HiveServer2-Background-Pool: Thread-107044]: metadata.Hive (Hive.java:getTable(1219)) - Table catalog_sales_new not found: new5_tpcds_real_bin_partitioned_orc_1000.catalog_sales_new table not found",
                "2018-04-05 16:20:36,538 ERROR [HiveServer2-Background-Pool: Thread-107044]: exec.DDLTask (DDLTask.java:failed(540)) - org.apache.hadoop.hive.ql.metadata.HiveException",
                "Caused by: java.lang.NullPointerException",
                "at org.apache.hadoop.hive.ql.metadata.Hive.getPartitionsByExpr(Hive.java:2613)",
                "at org.apache.hadoop.hive.ql.exec.DDLTask.dropPartitions(DDLTask.java:4008)"
            ],
            "RootCause": "The NullPointerException is caused by null parameters being passed to the `getPartitionsByExpr` method, specifically due to improperly initialized `DropTableDesc` objects that lack valid partition specifications.",
            "StepsToReproduce": [
                "1. Create a table (e.g., CREATE_TABLE(t1)).",
                "2. Add a partition to the table (e.g., ADD_PARTITION(t1.p1)).",
                "3. Drop the partition (e.g., DROP_PARTITION(t1.p1)).",
                "4. Drop the table (e.g., DROP_TABLE(t1)).",
                "5. Create the table again (e.g., CREATE_TABLE(t1)).",
                "6. Add the partition again (e.g., ADD_PARTITION(t1.p1)).",
                "7. Attempt to drop the partition again (e.g., DROP_PARTITION(t1.p1))."
            ],
            "ExpectedBehavior": "The system should successfully drop the specified partition without throwing a NullPointerException, regardless of the sequence of operations performed on the table.",
            "ObservedBehavior": "A NullPointerException is thrown when attempting to drop the partition during the REPL LOAD process, indicating that the necessary parameters for the operation are not properly initialized.",
            "Suggestions": "Ensure that the `DropTableDesc` object is constructed with valid partition specifications before being used in the `dropPartitions` method. Additionally, implement null checks for the parameters in the `getPartitionsByExpr` method to prevent such exceptions.",
            "problem_location": {
                "files": [
                    "ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java",
                    "ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java",
                    "ql/src/java/org/apache/hadoop/hive/ql/plan/DropTableDesc.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.ql.metadata.Hive",
                    "org.apache.hadoop.hive.ql.exec.DDLTask",
                    "org.apache.hadoop.hive.ql.plan.DropTableDesc"
                ],
                "methods": [
                    "Hive.getPartitionsByExpr",
                    "DDLTask.dropPartitions",
                    "DropTableDesc.getPartSpecs"
                ]
            },
            "possible_fix": "Modify the `DropTableDesc` constructor to ensure that valid partition specifications are always provided. Additionally, add null checks in the `getPartitionsByExpr` method to handle cases where parameters may be null."
        },
        "possible_fix_code": {
            "DropTableDesc": "/*\n * Licensed to the Apache Software Foundation (ASF) under one\n * or more contributor license agreements.  See the NOTICE file\n * distributed with this work for additional information\n * regarding copyright ownership.  The ASF licenses this file\n * to you under the Apache License, Version 2.0 (the\n * \"License\"); you may not use this file except in compliance\n * with the License.  You may obtain a copy of the License at\n *\n *     http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n */\n\npackage org.apache.hadoop.hive.ql.plan;\n\nimport java.io.Serializable;\nimport java.util.ArrayList;\nimport java.util.List;\nimport java.util.Map;\n\nimport org.apache.hadoop.hive.metastore.TableType;\nimport org.apache.hadoop.hive.ql.parse.ReplicationSpec;\nimport org.apache.hadoop.hive.ql.plan.Explain.Level;\n\n/**\n * DropTableDesc.\n * TODO: this is currently used for both drop table and drop partitions.\n */\n@Explain(displayName = \"Drop Table\", explainLevels = { Level.USER, Level.DEFAULT, Level.EXTENDED })\npublic class DropTableDesc extends DDLDesc implements Serializable {\n  private static final long serialVersionUID = 1L;\n\n  public static class PartSpec {\n    public PartSpec(ExprNodeGenericFuncDesc partSpec, int prefixLength) {\n      this.partSpec = partSpec;\n      this.prefixLength = prefixLength;\n    }\n    public ExprNodeGenericFuncDesc getPartSpec() {\n      return partSpec;\n    }\n    public int getPrefixLength() {\n      return prefixLength;\n    }\n    private static final long serialVersionUID = 1L;\n    private ExprNodeGenericFuncDesc partSpec;\n    private int prefixLength;\n  }\n\n  String tableName;\n  ArrayList<PartSpec> partSpecs;\n  TableType expectedType;\n  boolean ifExists;\n  boolean ifPurge;\n  ReplicationSpec replicationSpec;\n  boolean validationRequired;\n  \n  public DropTableDesc() {\n  }\n\n  /**\n   * @param tableName\n   * @param ifPurge\n   */\n  public DropTableDesc(\n      String tableName, TableType expectedType, boolean ifExists,\n      boolean ifPurge, ReplicationSpec replicationSpec) {\n\t  this(tableName, expectedType, ifExists, ifPurge, replicationSpec, true);\n  }\n\n  public DropTableDesc(\n      String tableName, TableType expectedType, boolean ifExists,\n      boolean ifPurge, ReplicationSpec replicationSpec, boolean validationRequired) {\n    this.tableName = tableName;\n    this.partSpecs = new ArrayList<>(); // Initialize to avoid null\n    this.expectedType = expectedType;\n    this.ifExists = ifExists;\n    this.ifPurge = ifPurge;\n    this.replicationSpec = replicationSpec;\n    this.validationRequired = validationRequired;\n  }\n\n  public DropTableDesc(String tableName, Map<Integer, List<ExprNodeGenericFuncDesc>> partSpecs,\n      TableType expectedType, boolean ifPurge, ReplicationSpec replicationSpec) {\n    this(tableName, partSpecs, expectedType, ifPurge, replicationSpec, true);\n  }\n\n  public DropTableDesc(String tableName, Map<Integer, List<ExprNodeGenericFuncDesc>> partSpecs,\n      TableType expectedType, boolean ifPurge, ReplicationSpec replicationSpec,  boolean validationRequired) {\n    this.tableName = tableName;\n    this.partSpecs = new ArrayList<PartSpec>(partSpecs.size());\n    for (Map.Entry<Integer, List<ExprNodeGenericFuncDesc>> partSpec : partSpecs.entrySet()) {\n      int prefixLength = partSpec.getKey();\n      for (ExprNodeGenericFuncDesc expr : partSpec.getValue()) {\n        this.partSpecs.add(new PartSpec(expr, prefixLength));\n      }\n    }\n    this.expectedType = expectedType;\n    this.ifPurge = ifPurge;\n    this.replicationSpec = replicationSpec;\n    this.validationRequired = validationRequired;\n  }\n\n  /**\n   * @return the tableName\n   */\n  @Explain(displayName = \"table\", explainLevels = { Level.USER, Level.DEFAULT, Level.EXTENDED })\n  public String getTableName() {\n    return tableName;\n  }\n\n  /**\n   * @param tableName\n   *          the tableName to set\n   */\n  public void setTableName(String tableName) {\n    this.tableName = tableName;\n  }\n\n  /**\n   * @return the partSpecs\n   */\n  public ArrayList<PartSpec> getPartSpecs() {\n    return partSpecs;\n  }\n\n  /**\n   * @return whether to expect a view being dropped\n   */\n  public boolean getExpectView() {\n    return expectedType != null && expectedType == TableType.VIRTUAL_VIEW;\n  }\n\n  /**\n   * @return whether to expect a materialized view being dropped\n   */\n  public boolean getExpectMaterializedView() {\n    return expectedType != null && expectedType == TableType.MATERIALIZED_VIEW;\n  }\n\n  /**\n   * @return whether IF EXISTS was specified\n   */\n  public boolean getIfExists() {\n    return ifExists;\n  }\n\n  /**\n   * @param ifExists\n   *          set whether IF EXISTS was specified\n   */\n  public void setIfExists(boolean ifExists) {\n    this.ifExists = ifExists;\n  }\n\n  /**\n   *  @return whether Purge was specified\n   */\n  public boolean getIfPurge() {\n      return ifPurge;\n  }\n\n  /**\n   * @param ifPurge\n   *          set whether Purge was specified\n   */\n  public void setIfPurge(boolean ifPurge) {\n      this.ifPurge = ifPurge;\n  }\n\n  /**\n   * @return what kind of replication scope this drop is running under.\n   * This can result in a \"DROP IF OLDER THAN\" kind of semantic\n   */\n  public ReplicationSpec getReplicationSpec(){\n    if (replicationSpec == null){\n      this.replicationSpec = new ReplicationSpec();\n    }\n    return this.replicationSpec;\n  }\n\n  /**\n   * @return whether the table type validation is needed (false in repl case)\n   */\n  public boolean getValidationRequired(){\n    return this.validationRequired;\n  }\n}"
        }
    },
    {
        "filename": "HIVE-13090.json",
        "creation_time": "2016-02-18T21:58:48.000+0000",
        "bug_report": {
            "Title": "Hive metastore crashes on NPE with ZooKeeperTokenStore",
            "Description": "The Hive metastore is experiencing a crash due to a NullPointerException (NPE) originating from the ZooKeeperTokenStore when attempting to retrieve a delegation token. The issue arises during the token decoding process, specifically in the getToken method, where a null byte array is passed to the ByteArrayInputStream constructor, leading to the NPE. This situation typically occurs when the token does not exist in ZooKeeper, which can happen if the token was never added or has been removed due to expiration.",
            "StackTrace": [
                "INFO  [pool-5-thread-192]: metastore.HiveMetaStore (HiveMetaStore.java:logInfo(714)) - 191: Metastore shutdown complete.",
                "INFO  [pool-5-thread-192]: HiveMetaStore.audit (HiveMetaStore.java:logAuditEvent(340)) - ugi=cvdpqap\tip=/19.1.2.129\tcmd=Metastore shutdown complete.\t",
                "ERROR [Thread[Thread-6,5,main]]: thrift.TokenStoreDelegationTokenSecretManager (TokenStoreDelegationTokenSecretManager.java:run(331)) - ExpiredTokenRemover thread received unexpected exception. org.apache.hadoop.hive.thrift.DelegationTokenStore$TokenStoreException: Failed to decode token",
                "org.apache.hadoop.hive.thrift.DelegationTokenStore$TokenStoreException: Failed to decode token",
                "at org.apache.hadoop.hive.thrift.ZooKeeperTokenStore.getToken(ZooKeeperTokenStore.java:401)",
                "at org.apache.hadoop.hive.thrift.TokenStoreDelegationTokenSecretManager.removeExpiredTokens(TokenStoreDelegationTokenSecretManager.java:256)",
                "at org.apache.hadoop.hive.thrift.TokenStoreDelegationTokenSecretManager$ExpiredTokenRemover.run(TokenStoreDelegationTokenSecretManager.java:319)",
                "at java.lang.Thread.run(Thread.java:744)",
                "Caused by: java.lang.NullPointerException",
                "at java.io.ByteArrayInputStream.<init>(ByteArrayInputStream.java:106)",
                "at org.apache.hadoop.security.token.delegation.HiveDelegationTokenSupport.decodeDelegationTokenInformation(HiveDelegationTokenSupport.java:53)",
                "at org.apache.hadoop.hive.thrift.ZooKeeperTokenStore.getToken(ZooKeeperTokenStore.java:399)"
            ],
            "RootCause": "The root cause of the issue is a NullPointerException occurring in the getToken method of the ZooKeeperTokenStore class. This happens because the zkGetData method returns null when the token node does not exist in ZooKeeper, leading to an attempt to decode a null byte array.",
            "StepsToReproduce": [
                "1. Start the Hive metastore service.",
                "2. Attempt to retrieve a delegation token that does not exist in ZooKeeper.",
                "3. Observe the logs for the NullPointerException in the getToken method."
            ],
            "ExpectedBehavior": "The Hive metastore should handle the absence of a token gracefully, either by returning an appropriate error message or by not crashing.",
            "ObservedBehavior": "The Hive metastore crashes with a NullPointerException when attempting to decode a non-existent token.",
            "Suggestions": "Ensure that tokens are being added correctly to ZooKeeper and that there are no issues in the addToken method. Implement error handling in the getToken method to manage cases where tokens do not exist.",
            "problem_location": {
                "files": [
                    "shims.common.src.main.java.org.apache.hadoop.hive.thrift.ZooKeeperTokenStore.java",
                    "shims.common.src.main.java.org.apache.hadoop.hive.thrift.TokenStoreDelegationTokenSecretManager.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.thrift.ZooKeeperTokenStore",
                    "org.apache.hadoop.hive.thrift.TokenStoreDelegationTokenSecretManager"
                ],
                "methods": [
                    "ZooKeeperTokenStore.getToken",
                    "ZooKeeperTokenStore.zkGetData",
                    "TokenStoreDelegationTokenSecretManager.removeExpiredTokens"
                ]
            },
            "possible_fix": "Add null checks in the getToken method to handle cases where zkGetData returns null. Additionally, ensure that tokens are added correctly and implement a mechanism to check for token existence before attempting to decode."
        },
        "possible_fix_code": {
            "ZooKeeperTokenStore.getToken": "  public DelegationTokenInformation getToken(DelegationTokenIdentifier tokenIdentifier) {\n    byte[] tokenBytes = zkGetData(getTokenPath(tokenIdentifier));\n    if (tokenBytes == null) {\n      throw new TokenStoreException(\"Token does not exist in ZooKeeper\");\n    }\n    try {\n      return HiveDelegationTokenSupport.decodeDelegationTokenInformation(tokenBytes);\n    } catch (Exception ex) {\n      throw new TokenStoreException(\"Failed to decode token\", ex);\n    }\n  }"
        }
    },
    {
        "filename": "HIVE-5664.json",
        "creation_time": "2013-10-28T03:50:29.000+0000",
        "bug_report": {
            "Title": "Drop cascade database fails when the db has any tables with indexes",
            "Description": "The issue arises when attempting to drop a database named 'db2' that contains tables with indexes. The command to drop the database fails with an error indicating that the database does not exist. The stack trace reveals a `NoSuchObjectException` related to a missing table (`db2.tab1_indx`), suggesting that the database or its associated objects may not be correctly recognized by the Hive metastore.",
            "StackTrace": [
                "FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. Database does not exist: db2",
                "org.apache.hadoop.hive.ql.metadata.HiveException: Database does not exist: db2",
                "Caused by: NoSuchObjectException(message:db2.tab1_indx table not found)"
            ],
            "RootCause": "The root cause of the issue is that the database 'db2' is not recognized by the Hive metastore, likely due to it not being created successfully or being deleted. The `NoSuchObjectException` indicates that the associated index table 'db2.tab1_indx' cannot be found.",
            "StepsToReproduce": [
                "1. Create a new database named 'db2'.",
                "2. Use the database 'db2'.",
                "3. Create a table 'tab1' with an index 'idx1'.",
                "4. Attempt to drop the database 'db2' using the CASCADE option."
            ],
            "ExpectedBehavior": "The database 'db2' should be dropped successfully along with all its associated tables and indexes without any errors.",
            "ObservedBehavior": "The command to drop the database fails with an error indicating that the database does not exist, and a `NoSuchObjectException` is thrown for the missing index table.",
            "Suggestions": "To resolve this issue, ensure that the database 'db2' is created successfully before attempting to drop it. Additionally, verify that there are no typos in the database name. If the database is confirmed to exist, check the Hive metastore for any inconsistencies.",
            "problem_location": {
                "files": [
                    "DDLTask.java",
                    "HiveMetaStoreClient.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.ql.exec.DDLTask",
                    "org.apache.hadoop.hive.metastore.HiveMetaStoreClient"
                ],
                "methods": [
                    "DDLTask.dropDatabase",
                    "HiveMetaStoreClient.getDatabase"
                ]
            },
            "possible_fix": "Implement a check for the existence of the database 'db2' using the `getDatabase` method from the `HiveMetaStoreClient` class before attempting to drop it. If the database does not exist, handle the `NoSuchObjectException` appropriately."
        },
        "possible_fix_code": {
            "DDLTask.dropDatabase": "public void dropDatabase(String dbName, boolean cascade) throws HiveException {\n    HiveMetaStoreClient client = new HiveMetaStoreClient(conf);\n    try {\n        // Check if the database exists before attempting to drop it\n        client.getDatabase(dbName);\n        // Proceed to drop the database if it exists\n        client.dropDatabase(dbName, cascade);\n    } catch (NoSuchObjectException e) {\n        throw new HiveException(\"Database does not exist: \" + dbName, e);\n    } catch (Exception e) {\n        throw new HiveException(\"Failed to drop database: \" + dbName, e);\n    } finally {\n        client.close();\n    }\n}"
        }
    },
    {
        "filename": "HIVE-15778.json",
        "creation_time": "2017-02-01T04:20:12.000+0000",
        "bug_report": {
            "Title": "DROP INDEX (non-existent) throws NPE when using DbNotificationListener",
            "Description": "Executing a DROP INDEX operation on a non-existent index results in a NullPointerException (NPE) in the Hive metastore. The issue arises when the system attempts to handle the drop index event through the DbNotificationListener, which fails due to a null reference during the instantiation of JSONDropIndexMessage. This occurs because the index variable is not set when the exception is raised in the HiveMetaStore.",
            "StackTrace": [
                "ERROR : FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. java.lang.NullPointerException",
                "Caused by: java.lang.NullPointerException",
                "at org.apache.hive.hcatalog.messaging.json.JSONDropIndexMessage.<init>(JSONDropIndexMessage.java:46)",
                "at org.apache.hive.hcatalog.listener.DbNotificationListener.onDropIndex(DbNotificationListener.java:280)"
            ],
            "RootCause": "The NullPointerException is triggered when the HiveMetaStore attempts to drop an index that does not exist, leading to a failure in the DbNotificationListener due to a null index variable.",
            "StepsToReproduce": [
                "1. Connect to the Hive metastore.",
                "2. Execute the command: DROP INDEX IF EXISTS vamsee1 ON sample_07;",
                "3. Observe the error message indicating a NullPointerException."
            ],
            "ExpectedBehavior": "The system should handle the DROP INDEX command gracefully without throwing an exception, even if the index does not exist.",
            "ObservedBehavior": "The system throws a NullPointerException when attempting to drop a non-existent index, causing the operation to fail.",
            "Suggestions": "To resolve this issue, ensure that the index variable is properly initialized before being used in the JSONDropIndexMessage constructor. Additionally, implement checks to verify the existence of the index before attempting to drop it.",
            "problem_location": {
                "files": [
                    "HiveMetaStore.java",
                    "JSONDropIndexMessage.java",
                    "DbNotificationListener.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.metastore.HiveMetaStore",
                    "org.apache.hive.hcatalog.messaging.json.JSONDropIndexMessage",
                    "org.apache.hive.hcatalog.listener.DbNotificationListener"
                ],
                "methods": [
                    "HiveMetaStore.drop_index_by_name",
                    "JSONDropIndexMessage.init",
                    "DbNotificationListener.onDropIndex"
                ]
            },
            "possible_fix": "Modify the HiveMetaStore class to check if the index exists before proceeding with the drop operation. Ensure that the index variable is set correctly in the event of an exception."
        },
        "possible_fix_code": {
            "drop_index_by_name": "    public boolean drop_index_by_name(final String dbName, final String tblName,\n        final String indexName, final boolean deleteData) throws NoSuchObjectException,\n        MetaException, TException {\n      startFunction(\"drop_index_by_name\", \": db=\" + dbName + \" tbl=\"\n          + tblName + \" index=\" + indexName);\n\n      // Check if the index exists before attempting to drop it\n      if (indexName == null || dbName == null || tblName == null) {\n          throw new MetaException(\"Database name, table name, or index name cannot be null.\");\n      }\n\n      boolean ret = false;\n      Exception ex = null;\n      try {\n        // Attempt to get the index to check if it exists\n        Index index = get_index_by_name(dbName, tblName, indexName);\n        if (index == null) {\n            throw new NoSuchObjectException(\"Index \" + indexName + \" does not exist.\");\n        }\n        ret = drop_index_by_name_core(getMS(), dbName, tblName,\n            indexName, deleteData);\n      } catch (IOException e) {\n        ex = e;\n        throw new MetaException(e.getMessage());\n      } catch (Exception e) {\n        ex = e;\n        rethrowException(e);\n      } finally {\n        endFunction(\"drop_index_by_name\", ret, ex, tblName);\n      }\n\n      return ret;\n    }"
        }
    },
    {
        "filename": "HIVE-8386.json",
        "creation_time": "2014-10-07T22:30:12.000+0000",
        "bug_report": {
            "Title": "HCAT API Call is Case Sensitive on Fields in Struct Column",
            "Description": "The Falcon application encounters a RuntimeException when using the HCatalog API to verify the schema of a target table. The error indicates that the field 'givenName' (in lowercase form 'givenname') cannot be found in the expected list of fields. This suggests a potential case sensitivity issue in the schema definition or a mismatch between the expected and actual field names in the data structure being processed.",
            "StackTrace": [
                "2014-10-07 00:30:23,255 ERROR - [1972803970@qtp-1214921164-3:gfoetl:POST//entities/submitAndSchedule/feed a0c221e3-efa8-4235-a403-b1047f23ec05] ~ Failure reason (FalconWebException:40)",
                "java.lang.RuntimeException: cannot find field givenName(lowercase form: givenname) in [givenName, surname, middleName, gender, age, isGivenNameLowerCase, isGivenNameUpperCase, isPrimary, isSurnameLowerCase, isSurnameUpperCase]",
                "at org.apache.hadoop.hive.serde2.typeinfo.StructTypeInfo.getStructFieldTypeInfo(StructTypeInfo.java:109)",
                "at org.apache.hive.hcatalog.data.schema.HCatSchemaUtils.constructHCatSchema(HCatSchemaUtils.java:154)",
                "at org.apache.hive.hcatalog.data.schema.HCatSchemaUtils.getHCatSchema(HCatSchemaUtils.java:165)",
                "at org.apache.hive.hcatalog.data.schema.HCatSchemaUtils.getHCatFieldSchema(HCatSchemaUtils.java:127)",
                "at org.apache.hive.hcatalog.data.schema.HCatSchemaUtils.getHCatFieldSchema(HCatSchemaUtils.java:115)",
                "at org.apache.hive.hcatalog.api.HCatTable.<init>(HCatTable.java:59)",
                "at org.apache.hive.hcatalog.api.HCatClientHMSImpl.getTable(HCatClientHMSImpl.java:157)",
                "at org.apache.falcon.catalog.HiveCatalogService.tableExists(HiveCatalogService.java:143)",
                "at org.apache.falcon.entity.parser.FeedEntityParser.validateStorageExists(FeedEntityParser.java:367)",
                "at org.apache.falcon.entity.parser.FeedEntityParser.validateFeedStorage(FeedEntityParser.java:309)",
                "at org.apache.falcon.entity.parser.FeedEntityParser.validate(FeedEntityParser.java:79)",
                "at org.apache.falcon.entity.parser.FeedEntityParser.validate(FeedEntityParser.java:54)",
                "at org.apache.falcon.resource.AbstractEntityManager.validate(AbstractEntityManager.java:364)",
                "at org.apache.falcon.resource.AbstractEntityManager.submitInternal(AbstractEntityManager.java:331)",
                "at org.apache.falcon.resource.AbstractEntityManager.submit(AbstractEntityManager.java:153)",
                "at org.apache.falcon.resource.ConfigSyncService.submit(ConfigSyncService.java:44)"
            ],
            "RootCause": "The error is caused by the absence of the field 'givenName' in the schema being constructed, likely due to it not being included in the list of field names passed to the TypeInfoFactory.getStructTypeInfo method. The StructTypeInfo class performs a case-insensitive search for field names, and if 'givenName' is not included in the list, it results in a RuntimeException.",
            "StepsToReproduce": [
                "1. Use the Falcon application to call the HCatalog API to verify the schema of a target table.",
                "2. Ensure that the schema includes a field named 'givenName'.",
                "3. Observe the error message indicating that 'givenName' cannot be found."
            ],
            "ExpectedBehavior": "The HCatalog API should successfully verify the schema of the target table without throwing any exceptions, indicating that all expected fields are present.",
            "ObservedBehavior": "A RuntimeException is thrown, indicating that the field 'givenName' cannot be found in the schema, leading to a failure in the API call.",
            "Suggestions": "Ensure that the field 'givenName' is included in the list of field names when creating the StructTypeInfo instance. Check the method calls to TypeInfoFactory.getStructTypeInfo to verify the parameters being passed.",
            "problem_location": {
                "files": [
                    "hcatalog.core/src/main/java/org/apache/hive/hcatalog/data/schema/HCatSchemaUtils.java",
                    "serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/StructTypeInfo.java",
                    "serde/src/java/org/apache/hadoop/hive/serde2/typeinfo/TypeInfoFactory.java"
                ],
                "classes": [
                    "org.apache.hive.hcatalog.data.schema.HCatSchemaUtils",
                    "org.apache.hadoop.hive.serde2.typeinfo.StructTypeInfo",
                    "org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory"
                ],
                "methods": [
                    "HCatSchemaUtils.constructHCatSchema",
                    "StructTypeInfo.getAllStructFieldNames",
                    "TypeInfoFactory.getStructTypeInfo"
                ]
            },
            "possible_fix": "Ensure that the 'givenName' field is included in the list of field names when calling TypeInfoFactory.getStructTypeInfo. This may involve checking the data structure that populates the field names to ensure it adheres to the expected case sensitivity."
        },
        "possible_fix_code": {
            "HCatSchemaUtils.constructHCatSchema": "  private static HCatSchema constructHCatSchema(StructTypeInfo stypeInfo) throws HCatException {\n    CollectionBuilder builder = getStructSchemaBuilder();\n    // Ensure 'givenName' is included in the field names\n    ArrayList<String> fieldNames = new ArrayList<>(stypeInfo.getAllStructFieldNames());\n    if (!fieldNames.contains(\"givenName\")) {\n        fieldNames.add(\"givenName\");\n    }\n    for (String fieldName : fieldNames) {\n      builder.addField(getHCatFieldSchema(fieldName, stypeInfo.getStructFieldTypeInfo(fieldName)));\n    }\n    return builder.build();\n  }"
        }
    },
    {
        "filename": "HIVE-14714.json",
        "creation_time": "2016-09-07T15:46:07.000+0000",
        "bug_report": {
            "Title": "Avoid misleading \"java.io.IOException: Stream closed\" when shutting down HoS",
            "Description": "The issue arises when executing Hive commands with Spark, particularly during the termination of a Beeline session or when switching the execution engine. The error manifests as a misleading 'java.io.IOException: Stream closed' warning, which occurs when the system attempts to read from a stream that has already been closed. This can happen when the stream is prematurely closed or when multiple threads access the same stream concurrently. The logs indicate that the Spark client is timing out while shutting down the remote driver, leading to the stream being closed unexpectedly.",
            "StackTrace": [
                "2016-09-06 16:15:12,291 WARN  org.apache.hive.spark.client.SparkClientImpl: [HiveServer2-Handler-Pool: Thread-106]: Timed out shutting down remote driver, interrupting...",
                "2016-09-06 16:15:12,291 WARN  org.apache.hive.spark.client.SparkClientImpl: [Driver]: Waiting thread interrupted, killing child process.",
                "2016-09-06 16:15:12,296 WARN  org.apache.hive.spark.client.SparkClientImpl: [stderr-redir-1]: Error in redirector thread.",
                "java.io.IOException: Stream closed",
                "at java.io.BufferedInputStream.getBufIfOpen(BufferedInputStream.java:162)",
                "at java.io.BufferedInputStream.read1(BufferedInputStream.java:272)",
                "at java.io.BufferedInputStream.read(BufferedInputStream.java:334)",
                "at sun.nio.cs.StreamDecoder.readBytes(StreamDecoder.java:283)",
                "at sun.nio.cs.StreamDecoder.implRead(StreamDecoder.java:325)",
                "at sun.nio.cs.StreamDecoder.read(StreamDecoder.java:177)",
                "at java.io.InputStreamReader.read(InputStreamReader.java:184)",
                "at java.io.BufferedReader.fill(BufferedReader.java:154)",
                "at java.io.BufferedReader.readLine(BufferedReader.java:317)",
                "at java.io.BufferedReader.readLine(BufferedReader.java:382)",
                "at org.apache.hive.spark.client.SparkClientImpl$Redirector.run(SparkClientImpl.java:617)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "RootCause": "The root cause of the issue is that the input stream is being closed prematurely, likely due to the timeout during the shutdown of the remote driver, which interrupts the reading process and leads to the IOException.",
            "StepsToReproduce": [
                "Execute a Hive command using Spark.",
                "Finish the Beeline session by pressing Ctrl-D.",
                "Alternatively, use commands like '!quit' or 'set hive.execution.engine=mr;'."
            ],
            "ExpectedBehavior": "The system should gracefully terminate the Beeline session without throwing an IOException, allowing for proper cleanup of resources.",
            "ObservedBehavior": "The system throws a 'java.io.IOException: Stream closed' error when attempting to read from a closed stream during the shutdown process.",
            "Suggestions": "To resolve this issue, ensure that the stream remains open until all read operations are complete. Implement synchronization mechanisms if multiple threads are involved to prevent one thread from closing the stream while another is still using it. Additionally, enhance error handling to manage exceptions properly.",
            "problem_location": {
                "files": [
                    "SparkClientImpl.java"
                ],
                "classes": [
                    "org.apache.hive.spark.client.SparkClientImpl"
                ],
                "methods": [
                    "SparkClientImpl$Redirector.run"
                ]
            },
            "possible_fix": "Review the shutdown process in the SparkClientImpl class to ensure that the stream is not closed prematurely. Consider adding checks to confirm that all read operations are complete before closing the stream."
        },
        "possible_fix_code": {
            "SparkClientImpl$Redirector.run": "    public void run() {\n      try {\n        String line = null;\n        while ((line = in.readLine()) != null) {\n          LOG.info(line);\n          if (errLogs != null) {\n            if (numErrLogLines++ < MAX_ERR_LOG_LINES_FOR_RPC) {\n              errLogs.add(line);\n            }\n          }\n        }\n      } catch (IOException e) {\n        LOG.warn(\"Error in redirector thread: Stream closed, possibly due to premature closure.\", e);\n      } catch (Exception e) {\n        LOG.warn(\"Error in redirector thread.\", e);\n      } finally {\n        try {\n          if (in != null) {\n            in.close();\n          }\n        } catch (IOException e) {\n          LOG.warn(\"Failed to close input stream.\", e);\n        }\n      }\n    }"
        }
    },
    {
        "filename": "HIVE-5428.json",
        "creation_time": "2013-10-02T20:46:10.000+0000",
        "bug_report": {
            "Title": "Direct SQL check fails during tests",
            "Description": "During the mavenization process, an error occurs when running the command `ant test -Dtestcase=TestCliDriver -Dqfile=udf_case.q -Dtest.silent=false`. The logs indicate a failure in executing a SQL query due to the absence of the 'DBS' table in the database. This suggests that the initialization order of the metastore schema is incorrect, leading to the failure of the direct SQL check.",
            "StackTrace": [
                "2013-10-02 13:42:30,262 INFO  metastore.MetaStoreDirectSql (MetaStoreDirectSql.java:<init>(99)) - MySQL check failed, assuming we are not on mysql: Lexical error at line 1, column 5.  Encountered: \"@\" (64), after : \"\".",
                "2013-10-02 13:42:30,298 ERROR metastore.MetaStoreDirectSql (MetaStoreDirectSql.java:<init>(112)) - Self-test query [select \"DB_ID\" from \"DBS\"] failed; direct SQL is disabled",
                "javax.jdo.JDODataStoreException: Error executing SQL query \"select \"DB_ID\" from \"DBS\"\"."
            ],
            "RootCause": "The root cause of the issue is that the 'DBS' table does not exist in the metastore database. This is likely due to an incomplete initialization of the metastore schema, which should include the creation of the 'DBS' table during the initialization process.",
            "StepsToReproduce": [
                "Run the command: `ant test -Dtestcase=TestCliDriver -Dqfile=udf_case.q -Dtest.silent=false`.",
                "Check the logs for the SQL execution error related to the 'DBS' table."
            ],
            "ExpectedBehavior": "The test should execute successfully without any SQL errors, and the 'DBS' table should be present in the metastore database.",
            "ObservedBehavior": "The test fails with a JDODataStoreException indicating that the 'DBS' table does not exist, leading to a failure in executing the SQL query.",
            "Suggestions": "Ensure that the metastore schema is properly initialized before running the tests. This may involve executing a schema creation script or method that sets up the necessary tables, including 'DBS'.",
            "problem_location": {
                "files": [
                    "metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java",
                    "metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.metastore.ObjectStore",
                    "org.apache.hadoop.hive.metastore.MetaStoreDirectSql"
                ],
                "methods": [
                    "ObjectStore.initialize",
                    "ObjectStore.verifySchema",
                    "ObjectStore.checkSchema",
                    "MetaStoreDirectSql.init"
                ]
            },
            "possible_fix": "Investigate the initialization process in the ObjectStore class to ensure that the 'DBS' table is created during the schema setup. If necessary, add a method to create the 'DBS' table if it does not exist."
        },
        "possible_fix_code": {
            "org.apache.hadoop.hive.metastore.ObjectStore.initialize": "  private void initialize(Properties dsProps) {\n    LOG.info(\"ObjectStore, initialize called\");\n    prop = dsProps;\n    pm = getPersistenceManager();\n    isInitialized = pm != null;\n    if (isInitialized) {\n      expressionProxy = createExpressionProxy(hiveConf);\n      directSql = new MetaStoreDirectSql(pm);\n      // Ensure the DBS table is created during initialization\n      createDBSTableIfNotExists();\n    }\n  }\n\n  private void createDBSTableIfNotExists() throws MetaException {\n    // Logic to check if the DBS table exists and create it if it does not\n    String checkTableQuery = \"SELECT COUNT(*) FROM information_schema.tables WHERE table_name = 'DBS'\";\n    try {\n      // Execute the check query\n      long count = (long) pm.newQuery(checkTableQuery).execute();\n      if (count == 0) {\n        // Table does not exist, create it\n        String createTableQuery = \"CREATE TABLE DBS (DB_ID INT PRIMARY KEY, ...)\"; // Define the schema as needed\n        pm.newQuery(createTableQuery).execute();\n      }\n    } catch (Exception e) {\n      LOG.error(\"Error checking or creating DBS table: \" + e.getMessage());\n      throw new MetaException(\"Failed to ensure DBS table exists.\");\n    }\n  }"
        }
    },
    {
        "filename": "HIVE-12567.json",
        "creation_time": "2015-12-02T16:38:52.000+0000",
        "bug_report": {
            "Title": "Enhance TxnHandler retry logic to handle ORA-08176",
            "Description": "The application encounters a failure when attempting to acquire locks due to a communication issue with the Hive metastore. The error message indicates a `LockException` caused by an `SQLException` with the message 'ORA-08176: consistent read failure; rollback data not available'. This suggests that the Oracle database is unable to maintain consistent transaction states, likely due to improper handling of transactions or locks within the Hive metastore.",
            "StackTrace": [
                "FAILED: Error in acquiring locks: Error communicating with the metastore",
                "org.apache.hadoop.hive.ql.lockmgr.LockException: Error communicating with the metastore",
                "Caused by: MetaException(message:Unable to update transaction database java.sql.SQLException: ORA-08176: consistent read failure; rollback data not available"
            ],
            "RootCause": "The root cause of the issue is related to transaction management in the Hive metastore, specifically the inability of the Oracle database to maintain consistent read states during transaction handling, as indicated by the ORA-08176 error.",
            "StepsToReproduce": [
                "Initiate a transaction in the Hive application.",
                "Attempt to acquire locks on the required resources.",
                "Observe the error message indicating a failure in acquiring locks due to communication issues with the metastore."
            ],
            "ExpectedBehavior": "The system should successfully acquire locks and manage transactions without encountering communication errors with the metastore.",
            "ObservedBehavior": "The system fails to acquire locks, resulting in a LockException and an SQLException indicating a consistent read failure in the Oracle database.",
            "Suggestions": "Investigate the state of the Oracle database to ensure it is properly configured for transaction handling. Review the Hive metastore configuration, particularly regarding transaction isolation levels and lock management settings. Check logs for additional errors or warnings that may provide more context on the failure.",
            "problem_location": {
                "files": [
                    "ql/src/java/org/apache/hadoop/hive/ql/lockmgr/DbLockManager.java",
                    "ql/src/java/org/apache/hadoop/hive/ql/lockmgr/DbTxnManager.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.ql.lockmgr.DbLockManager",
                    "org.apache.hadoop.hive.ql.lockmgr.DbTxnManager"
                ],
                "methods": [
                    "DbLockManager.lock",
                    "DbTxnManager.acquireLocks",
                    "DbTxnManager.commitTxn",
                    "DbTxnManager.rollbackTxn"
                ]
            },
            "possible_fix": "Review and enhance the error handling in the `lock`, `acquireLocks`, `commitTxn`, and `rollbackTxn` methods to ensure that communication failures with the metastore are properly managed. Additionally, consider implementing retry logic for acquiring locks and committing transactions to handle transient issues."
        },
        "possible_fix_code": {
            "DbLockManager.lock": "  LockState lock(LockRequest lock, String queryId, boolean isBlocking, List<HiveLock> acquiredLocks) throws LockException {\n    int retryCount = 0;\n    int maxRetries = 3;\n    while (true) {\n      try {\n        LOG.info(\"Requesting: queryId=\" + queryId + \" \" + lock);\n        LockResponse res = client.lock(lock);\n        LOG.info(\"Response to queryId=\" + queryId + \" \" + res);\n        if (!isBlocking) {\n          if (res.getState() == LockState.WAITING) {\n            return LockState.WAITING;\n          }\n        }\n        while (res.getState() == LockState.WAITING) {\n          backoff();\n          res = client.checkLock(res.getLockid());\n        }\n        DbHiveLock hl = new DbHiveLock(res.getLockid());\n        locks.add(hl);\n        if (res.getState() != LockState.ACQUIRED) {\n          throw new LockException(ErrorMsg.LOCK_CANNOT_BE_ACQUIRED.getMsg());\n        }\n        acquiredLocks.add(hl);\n\n        Metrics metrics = MetricsFactory.getInstance();\n        if (metrics != null) {\n          try {\n            metrics.incrementCounter(MetricsConstant.METASTORE_HIVE_LOCKS);\n          } catch (Exception e) {\n            LOG.warn(\"Error Reporting hive client metastore lock operation to Metrics system\", e);\n          }\n        }\n\n        return res.getState();\n      } catch (NoSuchTxnException e) {\n        LOG.error(\"Metastore could not find txnid \" + lock.getTxnid());\n        throw new LockException(ErrorMsg.TXNMGR_NOT_INSTANTIATED.getMsg(), e);\n      } catch (TxnAbortedException e) {\n        LOG.error(\"Transaction \" + JavaUtils.txnIdToString(lock.getTxnid()) + \" already aborted.\");\n        throw new LockException(e, ErrorMsg.TXN_ABORTED, JavaUtils.txnIdToString(lock.getTxnid()));\n      } catch (TException e) {\n        if (retryCount < maxRetries) {\n          retryCount++;\n          LOG.warn(\"Communication failed, retrying... (attempt \" + retryCount + \")\", e);\n          continue;\n        }\n        throw new LockException(ErrorMsg.METASTORE_COMMUNICATION_FAILED.getMsg(), e);\n      }\n    }\n  }"
        }
    },
    {
        "filename": "HIVE-6984.json",
        "creation_time": "2014-04-28T23:08:43.000+0000",
        "bug_report": {
            "Title": "Analyzing partitioned table with NULL values for the partition column failed with NPE",
            "Description": "The bug occurs when analyzing a partitioned table that contains NULL values for the partition column. Specifically, the `gatherStats` method in the `TableScanOperator` fails to handle NULL values, resulting in a `NullPointerException` and a subsequent `HiveException` during the processing of rows.",
            "StackTrace": [
                "java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {\"name\":\"6666666666666666666\",\"age\":null,\"raw__data__size\":19}",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:195)",
                "at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:417)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:332)",
                "at org.apache.hadoop.mapred.Child$4.run(Child.java:268)",
                "Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {\"name\":\"6666666666666666666\",\"age\":null,\"raw__data__size\":19}",
                "at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:549)",
                "Caused by: java.lang.NullPointerException",
                "at org.apache.hadoop.hive.ql.exec.TableScanOperator.gatherStats(TableScanOperator.java:149)",
                "at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:90)"
            ],
            "RootCause": "The root cause of the error is a NullPointerException in the gatherStats method of the TableScanOperator class, which fails to handle null values in the input data, particularly for the 'age' field.",
            "StepsToReproduce": [
                "Create a table 'test2' with NULL values in the 'age' column.",
                "Create a partitioned table 'test3' based on the 'age' column.",
                "Insert data from 'test2' into 'test3' while partitioning by 'age'.",
                "Run the command 'analyze table test3 partition(age) compute statistics'."
            ],
            "ExpectedBehavior": "The statistics for the partitioned table should be computed without errors, even when NULL values are present in the partition column.",
            "ObservedBehavior": "A NullPointerException occurs during the analysis of the partitioned table, leading to a HiveException and failure of the task.",
            "Suggestions": "Implement proper null handling in the gatherStats method of the TableScanOperator to avoid NullPointerExceptions when processing rows with NULL values.",
            "problem_location": {
                "files": [
                    "TableScanOperator.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.ql.exec.TableScanOperator"
                ],
                "methods": [
                    "gatherStats",
                    "processOp"
                ]
            },
            "possible_fix": "Modify the gatherStats method to include checks for null values in the 'age' field before attempting to process them. Ensure that the currentStat object is initialized correctly to handle cases where input data may contain NULL values."
        },
        "possible_fix_code": {
            "gatherStats": "  private void gatherStats(Object row) {\n    // first row/call or a new partition\n    if ((currentStat == null) || inputFileChanged) {\n      String partitionSpecs;\n      inputFileChanged = false;\n      if (conf.getPartColumns() == null || conf.getPartColumns().size() == 0) {\n        partitionSpecs = \"\"; // non-partitioned\n      } else {\n        // Figure out the partition spec from the input.\n        // This is only done once for the first row (when stat == null)\n        // since all rows in the same mapper should be from the same partition.\n        List<Object> writable;\n        List<String> values;\n        int dpStartCol; // the first position of partition column\n        assert inputObjInspectors[0].getCategory() == ObjectInspector.Category.STRUCT : \"input object inspector is not struct\";\n\n        writable = new ArrayList<Object>(conf.getPartColumns().size());\n        values = new ArrayList<String>(conf.getPartColumns().size());\n        dpStartCol = 0;\n        StructObjectInspector soi = (StructObjectInspector) inputObjInspectors[0];\n        for (StructField sf : soi.getAllStructFieldRefs()) {\n          String fn = sf.getFieldName();\n          if (!conf.getPartColumns().contains(fn)) {\n            dpStartCol++;\n          } else {\n            break;\n          }\n        }\n        ObjectInspectorUtils.partialCopyToStandardObject(writable, row, dpStartCol, conf\n            .getPartColumns().size(),\n            (StructObjectInspector) inputObjInspectors[0], ObjectInspectorCopyOption.WRITABLE);\n\n        for (Object o : writable) {\n          assert (o != null && o.toString().length() > 0);\n          values.add(o.toString());\n        }\n        partitionSpecs = FileUtils.makePartName(conf.getPartColumns(), values);\n        LOG.info(\"Stats Gathering found a new partition spec = \" + partitionSpecs);\n      }\n      // find which column contains the raw data size (both partitioned and non partitioned\n      int uSizeColumn = -1;\n      StructObjectInspector soi = (StructObjectInspector) inputObjInspectors[0];\n      for (int i = 0; i < soi.getAllStructFieldRefs().size(); i++) {\n        if (soi.getAllStructFieldRefs().get(i).getFieldName()\n            .equals(VirtualColumn.RAWDATASIZE.getName().toLowerCase())) {\n          uSizeColumn = i;\n          break;\n        }\n      }\n      currentStat = stats.get(partitionSpecs);\n      if (currentStat == null) {\n        currentStat = new Stat();\n        currentStat.setBookkeepingInfo(StatsSetupConst.RAW_DATA_SIZE, uSizeColumn);\n        stats.put(partitionSpecs, currentStat);\n      }\n    }\n\n    // increase the row count\n    currentStat.addToStat(StatsSetupConst.ROW_COUNT, 1);\n\n    // extract the raw data size, and update the stats for the current partition\n    int rdSizeColumn = currentStat.getBookkeepingInfo(StatsSetupConst.RAW_DATA_SIZE);\n    if(rdSizeColumn != -1) {\n      List<Object> rdSize = new ArrayList<Object>(1);\n      ObjectInspectorUtils.partialCopyToStandardObject(rdSize, row,\n          rdSizeColumn, 1, (StructObjectInspector) inputObjInspectors[0],\n          ObjectInspectorCopyOption.WRITABLE);\n      if (rdSize.get(0) != null) { // Check for null before accessing\n        currentStat.addToStat(StatsSetupConst.RAW_DATA_SIZE, (((LongWritable)rdSize.get(0)).get()));\n      }\n    }\n\n  }"
        }
    },
    {
        "filename": "HIVE-10736.json",
        "creation_time": "2015-05-18T03:25:45.000+0000",
        "bug_report": {
            "Title": "HiveServer2 shutdown of cached tez app-masters is not clean",
            "Description": "The shutdown process of HiveServer2 is encountering a ConcurrentModificationException, which disrupts the cleanup of cached Tez application masters. This issue arises when multiple threads attempt to modify a collection concurrently, leading to inconsistent states during the shutdown sequence. The stack trace indicates that the error occurs within the TezSessionPoolManager while stopping the HiveServer2 service.",
            "StackTrace": [
                "2015-05-17 20:24:00,570 ERROR [Thread-6()]: server.HiveServer2 (HiveServer2.java:stop(322)) - Tez session pool manager stop had an error during stop of HiveServer2. Shutting down HiveServer2 anyway.",
                "java.util.ConcurrentModificationException",
                "at java.util.LinkedList$ListItr.checkForComodification(LinkedList.java:966)",
                "at java.util.LinkedList$ListItr.next(LinkedList.java:888)",
                "at org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolManager.stop(TezSessionPoolManager.java:187)",
                "at org.apache.hive.service.server.HiveServer2.stop(HiveServer2.java:320)",
                "at org.apache.hive.service.server.HiveServer2$1.run(HiveServer2.java:107)"
            ],
            "RootCause": "The root cause of the issue is a ConcurrentModificationException triggered by concurrent modifications to a LinkedList while it is being iterated over during the shutdown process of HiveServer2.",
            "StepsToReproduce": [
                "Start the HiveServer2 service with cached Tez application masters.",
                "Initiate a shutdown of the HiveServer2 service while there are active Tez sessions.",
                "Observe the logs for ConcurrentModificationException during the shutdown process."
            ],
            "ExpectedBehavior": "The HiveServer2 service should shut down cleanly, ensuring that all cached Tez application masters are properly cleaned up without throwing exceptions.",
            "ObservedBehavior": "During the shutdown of HiveServer2, a ConcurrentModificationException is thrown, indicating that the shutdown process fails to clean up the cached Tez application masters correctly.",
            "Suggestions": "To mitigate this issue, consider implementing one of the following solutions: 1) Use concurrent collections such as CopyOnWriteArrayList for managing the Tez sessions to avoid concurrent modification issues. 2) Synchronize access to the collection during shutdown to ensure thread safety. 3) Avoid modifying the collection while iterating over it by collecting changes and applying them after the iteration.",
            "problem_location": {
                "files": [
                    "HiveServer2.java",
                    "TezSessionPoolManager.java"
                ],
                "classes": [
                    "org.apache.hive.service.server.HiveServer2",
                    "org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolManager"
                ],
                "methods": [
                    "HiveServer2.stop",
                    "TezSessionPoolManager.stop"
                ]
            },
            "possible_fix": "Refactor the TezSessionPoolManager to use a concurrent collection for managing sessions, or implement synchronized blocks around the collection modifications during the shutdown process."
        },
        "possible_fix_code": {
            "ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolManager.stop": "  public void stop() throws Exception {\n    if ((sessionPool == null) || (this.inited == false)) {\n      return;\n    }\n\n    // Use a CopyOnWriteArrayList to avoid ConcurrentModificationException\n    List<TezSessionState> sessions = new CopyOnWriteArrayList<>(TezSessionState.getOpenSessions());\n    for (TezSessionState sessionState: sessions) {\n      if (sessionState.isDefault()) {\n        sessionState.close(false);\n      }\n    }\n  }"
        }
    },
    {
        "filename": "HIVE-7710.json",
        "creation_time": "2014-08-13T10:46:32.000+0000",
        "bug_report": {
            "Title": "Rename table across database might fail",
            "Description": "When attempting to rename a table from one database to another using the SQL statement 'ALTER TABLE d1.t1 RENAME TO d2.t2', the operation fails if a table with the name 'd2.t2' already exists in the target database. The failure is due to a unique constraint violation on the 'TBLS' table, which is responsible for maintaining metadata about tables in the Hive metastore. The error message indicates that the operation was aborted because it would have caused a duplicate key value in a unique or primary key constraint identified by 'UNIQUETABLE'. This issue arises during the execution of the 'alterTable' method in the ObjectStore class, which attempts to update the table metadata.",
            "StackTrace": [
                "2014-08-13 03:32:40,512 ERROR Datastore.Persist (Log4JLogger.java:error(115)) - Update of object \"org.apache.hadoop.hive.metastore.model.MTable@729c5167\" using statement \"UPDATE TBLS SET TBL_NAME=? WHERE TBL_ID=?\" failed : java.sql.SQLIntegrityConstraintViolationException: The statement was aborted because it would have caused a duplicate key value in a unique or primary key constraint or unique index identified by 'UNIQUETABLE' defined on 'TBLS'.",
                "at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source)",
                "at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)",
                "at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)",
                "at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)",
                "at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)",
                "at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.alterTable(ObjectStore.java:2572)",
                "at org.apache.hadoop.hive.metastore.HiveAlterHandler.alterTable(HiveAlterHandler.java:205)"
            ],
            "RootCause": "The root cause of the issue is a violation of a unique constraint in the 'TBLS' table when attempting to rename a table. This occurs because the new table name already exists in the target database, leading to a SQLIntegrityConstraintViolationException.",
            "StepsToReproduce": [
                "1. Ensure that there is a table named 't2' in the database 'd2'.",
                "2. Execute the SQL statement: 'ALTER TABLE d1.t1 RENAME TO d2.t2'.",
                "3. Observe the error message indicating a unique constraint violation."
            ],
            "ExpectedBehavior": "The rename operation should succeed if the target table name does not already exist in the target database.",
            "ObservedBehavior": "The rename operation fails with a SQLIntegrityConstraintViolationException due to an attempt to create a duplicate entry in the 'TBLS' table.",
            "Suggestions": "To resolve this issue, ensure that the target table name does not already exist in the target database before attempting to rename the table. Alternatively, consider dropping or renaming the existing table in the target database.",
            "problem_location": {
                "files": [
                    "ObjectStore.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.metastore.ObjectStore"
                ],
                "methods": [
                    "alterTable",
                    "getMTable"
                ]
            },
            "possible_fix": "Before executing the rename operation, check for the existence of the target table using the 'getMTable' method. If the table exists, either prompt the user to choose a different name or handle the conflict appropriately."
        },
        "possible_fix_code": {
            "alterTable": "  public void alterTable(String dbname, String name, Table newTable)\n      throws InvalidObjectException, MetaException {\n    boolean success = false;\n    try {\n      openTransaction();\n      name = name.toLowerCase();\n      dbname = dbname.toLowerCase();\n      MTable newt = convertToMTable(newTable);\n      if (newt == null) {\n        throw new InvalidObjectException(\"new table is invalid\");\n      }\n\n      MTable oldt = getMTable(dbname, name);\n      if (oldt == null) {\n        throw new MetaException(\"table \" + name + \" doesn\\'t exist\");\n      }\n\n      // Check if the new table name already exists in the target database\n      MTable existingTable = getMTable(newt.getDatabase().getName(), newt.getTableName());\n      if (existingTable != null) {\n        throw new MetaException(\"Table with name \" + newt.getTableName() + \" already exists in database \" + newt.getDatabase().getName());\n      }\n\n      // For now only alter name, owner, parameters, cols, bucketcols are allowed\n      oldt.setTableName(newt.getTableName().toLowerCase());\n      oldt.setParameters(newt.getParameters());\n      oldt.setOwner(newt.getOwner());\n      // Fully copy over the contents of the new SD into the old SD,\n      // so we don\\'t create an extra SD in the metastore db that has no references.\n      copyMSD(newt.getSd(), oldt.getSd());\n      oldt.setDatabase(newt.getDatabase());\n      oldt.setRetention(newt.getRetention());\n      oldt.setPartitionKeys(newt.getPartitionKeys());\n      oldt.setTableType(newt.getTableType());\n      oldt.setLastAccessTime(newt.getLastAccessTime());\n      oldt.setViewOriginalText(newt.getViewOriginalText());\n      oldt.setViewExpandedText(newt.getViewExpandedText());\n\n      // commit the changes\n      success = commitTransaction();\n    } finally {\n      if (!success) {\n        rollbackTransaction();\n      }\n    }\n  }"
        }
    },
    {
        "filename": "HIVE-8735.json",
        "creation_time": "2014-11-04T22:20:02.000+0000",
        "bug_report": {
            "Title": "Statistics Update Can Fail Due to Long Paths",
            "Description": "The system encounters a truncation error when attempting to publish statistics due to a VARCHAR column in the database being limited to 255 characters. The error occurs specifically in the `publishStat` method of the `JDBCStatsPublisher` class when trying to insert a file path that exceeds this length. The problematic value is 'pfile:/grid/0/jenkins/workspace/UT-hive-champlain-common/sub&', which is longer than the allowed limit.",
            "StackTrace": [
                "2014-11-04 01:34:38,610 ERROR jdbc.JDBCStatsPublisher (JDBCStatsPublisher.java:publishStat(198)) - Error during publishing statistics.",
                "java.sql.SQLDataException: A truncation error was encountered trying to shrink VARCHAR 'pfile:/grid/0/jenkins/workspace/UT-hive-champlain-common/sub&' to length 255.",
                "at org.apache.derby.impl.jdbc.SQLExceptionFactory40.getSQLException(Unknown Source)",
                "at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)",
                "at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)",
                "at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)",
                "at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)",
                "at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)",
                "at org.apache.hadoop.hive.ql.stats.jdbc.JDBCStatsPublisher$2.run(JDBCStatsPublisher.java:147)",
                "at org.apache.hadoop.hive.ql.exec.FileSinkOperator.publishStats(FileSinkOperator.java:1153)",
                "at org.apache.hadoop.hive.ql.exec.FileSinkOperator.closeOp(FileSinkOperator.java:992)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)"
            ],
            "RootCause": "The truncation error is caused by attempting to insert a value longer than 255 characters into a VARCHAR(255) column in the database. The specific value causing the issue is 'pfile:/grid/0/jenkins/workspace/UT-hive-champlain-common/sub&'.",
            "StepsToReproduce": [
                "Attempt to publish statistics using the `publishStat` method with a fileID that has a path longer than 255 characters.",
                "Observe the error logged indicating a truncation issue."
            ],
            "ExpectedBehavior": "The system should successfully publish statistics without encountering truncation errors, regardless of the length of the file path, as long as it adheres to the database schema constraints.",
            "ObservedBehavior": "The system fails to publish statistics and logs a truncation error when the file path exceeds 255 characters.",
            "Suggestions": "To resolve this issue, either increase the length of the VARCHAR column in the database schema to accommodate longer values or ensure that the data being inserted does not exceed the defined length of the column.",
            "problem_location": {
                "files": [
                    "JDBCStatsPublisher.java",
                    "JDBCStatsUtils.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.ql.stats.jdbc.JDBCStatsPublisher",
                    "org.apache.hadoop.hive.ql.stats.jdbc.JDBCStatsUtils"
                ],
                "methods": [
                    "JDBCStatsPublisher.publishStat",
                    "JDBCStatsUtils.getCreate"
                ]
            },
            "possible_fix": "Modify the database schema to increase the length of the VARCHAR column from 255 to a larger size, such as VARCHAR(512), or implement a check in the `publishStat` method to truncate the fileID before insertion."
        },
        "possible_fix_code": {
            "JDBCStatsPublisher.publishStat": "  public boolean publishStat(String fileID, Map<String, String> stats) {\n\n    if (stats.isEmpty()) {\n      // If there are no stats to publish, nothing to do.\n      return true;\n    }\n\n    if (conn == null) {\n      LOG.error(\"JDBC connection is null. Cannot publish stats without JDBC connection.\");\n      return false;\n    }\n\n    if (!JDBCStatsUtils.isValidStatisticSet(stats.keySet())) {\n      LOG.warn(\"Invalid statistic:\" + stats.keySet().toString() + \", supported \"\n          + \" stats: \" + JDBCStatsUtils.getSupportedStatistics());\n      return false;\n    }\n    LOG.info(\"Stats publishing for key \" + fileID);\n\n    // Truncate fileID if it exceeds 255 characters\n    if (fileID.length() > 255) {\n      fileID = fileID.substring(0, 255);\n    }\n\n    Utilities.SQLCommand<Void> execUpdate = new Utilities.SQLCommand<Void>() {\n      @Override\n      public Void run(PreparedStatement stmt) throws SQLException {\n        stmt.executeUpdate();\n        return null;\n      }\n    };\n\n    List<String> supportedStatistics = JDBCStatsUtils.getSupportedStatistics();\n\n    for (int failures = 0;; failures++) {\n      try {\n        insStmt.setString(1, fileID);\n        for (int i = 0; i < JDBCStatsUtils.getSupportedStatistics().size(); i++) {\n          insStmt.setString(i + 2, stats.get(supportedStatistics.get(i)));\n        }\n        Utilities.executeWithRetry(execUpdate, insStmt, waitWindow, maxRetries);\n        return true;\n      } catch (SQLIntegrityConstraintViolationException e) {\n\n        // We assume that the table used for partial statistics has a primary key declared on the\n        // \"fileID\". The exception will be thrown if two tasks report results for the same fileID.\n        // In such case, we either update the row, or abandon changes depending on which statistic\n        // is newer.\n\n        for (int updateFailures = 0;; updateFailures++) {\n          try {\n            int i;\n            for (i = 0; i < JDBCStatsUtils.getSupportedStatistics().size(); i++) {\n              updStmt.setString(i + 1, stats.get(supportedStatistics.get(i)));\n            }\n            updStmt.setString(supportedStatistics.size() + 1, fileID);\n            updStmt.setString(supportedStatistics.size() + 2,\n                stats.get(JDBCStatsUtils.getBasicStat()));\n            updStmt.setString(supportedStatistics.size() + 3, fileID);\n            Utilities.executeWithRetry(execUpdate, updStmt, waitWindow, maxRetries);\n            return true;\n          } catch (SQLRecoverableException ue) {\n            // need to start from scratch (connection)\n            if (!handleSQLRecoverableException(ue, updateFailures)) {\n              return false;\n            }\n          } catch (SQLException ue) {\n            LOG.error(\"Error during publishing statistics. \", e);\n            return false;\n          }\n        }\n\n      } catch (SQLRecoverableException e) {\n        // need to start from scratch (connection)\n        if (!handleSQLRecoverableException(e, failures)) {\n          return false;\n        }\n      } catch (SQLException e) {\n        LOG.error(\"Error during publishing statistics. \", e);\n        return false;\n      }\n    }\n  }"
        }
    },
    {
        "filename": "HIVE-13209.json",
        "creation_time": "2016-03-04T21:39:50.000+0000",
        "bug_report": {
            "Title": "metastore get_delegation_token fails with null ip address",
            "Description": "The issue arises when attempting to obtain a delegation token from the Hive Metastore, resulting in an unauthorized connection error for a super-user. This problem was introduced following changes made in HIVE-13169. The stack trace indicates that the `get_delegation_token` method in the `HiveMetaStore` class is failing due to a null IP address, which is critical for authentication. The `invoke` method in the `RetryingHMSHandler` class is also implicated, as it handles the invocation of the `get_delegation_token` method and may not be managing the authentication correctly.",
            "StackTrace": [
                "2016-03-03 07:45:31,055 ERROR [pool-6-thread-22]: metastore.RetryingHMSHandler (RetryingHMSHandler.java:invoke(159)) - MetaException(message:Unauthorized connection for super-user: HTTP/<hostname@realm> from IP null)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_delegation_token(HiveMetaStore.java:5290)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)",
                "at com.sun.proxy.$Proxy16.get_delegation_token(Unknown Source)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$get_delegation_token.getResult(ThriftHiveMetastore.java:11492)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$get_delegation_token.getResult(ThriftHiveMetastore.java:11476)",
                "at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)",
                "at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)",
                "at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor$1.run(HadoopThriftAuthBridge.java:551)",
                "at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor$1.run(HadoopThriftAuthBridge.java:546)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)",
                "at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor.process(HadoopThriftAuthBridge.java:546)",
                "at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:285)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "RootCause": "The root cause of the issue is that the `get_delegation_token` method is failing due to a null IP address, which is necessary for establishing an authorized connection. This indicates a potential misconfiguration in the authentication setup or insufficient permissions for the user attempting to obtain the token.",
            "StepsToReproduce": [
                "Attempt to call the `get_delegation_token` method on the Hive Metastore with a super-user account.",
                "Ensure that the IP address is not null during the call.",
                "Observe the error message indicating an unauthorized connection."
            ],
            "ExpectedBehavior": "The system should successfully return a delegation token for the super-user without any unauthorized connection errors.",
            "ObservedBehavior": "The system fails to return a delegation token and throws a MetaException indicating an unauthorized connection for a super-user due to a null IP address.",
            "Suggestions": "Check the configuration settings for the Hive Metastore and ensure that the IP address is correctly set and not null. Verify that the super-user has the necessary permissions to access the metastore.",
            "problem_location": {
                "files": [
                    "metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java",
                    "metastore/src/java/org/apache/hadoop/hive/metastore/RetryingHMSHandler.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.metastore.HiveMetaStore",
                    "org.apache.hadoop.hive.metastore.RetryingHMSHandler"
                ],
                "methods": [
                    "HiveMetaStore.get_delegation_token",
                    "RetryingHMSHandler.invoke"
                ]
            },
            "possible_fix": "Ensure that the `get_delegation_token` method correctly retrieves the IP address and that the authentication configuration is properly set up. Review the method implementation to handle cases where the IP address might be null."
        },
        "possible_fix_code": {
            "get_delegation_token": "    public String get_delegation_token(String token_owner,\n        String renewer_kerberos_principal_name)\n        throws MetaException, TException {\n      startFunction(\"get_delegation_token\");\n      String ret = null;\n      Exception ex = null;\n      String ipAddress = getIpAddress();\n      if (ipAddress == null || ipAddress.isEmpty()) {\n          throw new MetaException(\"IP address cannot be null or empty\");\n      }\n      try {\n        ret =\n            HiveMetaStore.getDelegationToken(token_owner,\n                renewer_kerberos_principal_name, ipAddress);\n      } catch (IOException e) {\n        ex = e;\n        throw new MetaException(e.getMessage());\n      } catch (InterruptedException e) {\n        ex = e;\n        throw new MetaException(e.getMessage());\n      } catch (Exception e) {\n        ex = e;\n        if (e instanceof MetaException) {\n          throw (MetaException) e;\n        } else if (e instanceof TException) {\n          throw (TException) e;\n        } else {\n          throw newMetaException(e);\n        }\n      } finally {\n        endFunction(\"get_delegation_token\", ret != null, ex);\n      }\n      return ret;\n    }"
        }
    },
    {
        "filename": "HIVE-13065.json",
        "creation_time": "2016-02-16T21:11:31.000+0000",
        "bug_report": {
            "Title": "Hive throws NPE when writing map type data to a HBase backed table",
            "Description": "Hive throws a NullPointerException (NPE) when writing data to a HBase backed table under the following conditions:\n\n1. There is a map type column.\n2. The map type column has NULL in its values.\n\nThis issue occurs during the serialization process when the map contains null values, leading to a failure in the mapreduce job.",
            "StackTrace": [
                "2016-02-15 02:26:33,225 WARN [main] org.apache.hadoop.mapred.YarnChild: Exception running child : java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {\"key\":{},\"value\":{\"_col0\":1,\"_col1\":{\"abcd\":null}}}",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:265)",
                "at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:444)",
                "at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)",
                "at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)",
                "Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {\"key\":{},\"value\":{\"_col0\":1,\"_col1\":{\"abcd\":null}}}",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:253)",
                "... 7 more",
                "Caused by: org.apache.hadoop.hive.serde2.SerDeException: java.lang.NullPointerException",
                "at org.apache.hadoop.hive.ql.exec.FileSinkOperator.processOp(FileSinkOperator.java:731)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)",
                "at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:84)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)",
                "at org.apache.hadoop.hive.ql.exec.LimitOperator.processOp(LimitOperator.java:51)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)",
                "at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:84)",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:244)",
                "... 7 more",
                "Caused by: org.apache.hadoop.hive.serde2.SerDeException: java.lang.NullPointerException",
                "at org.apache.hadoop.hive.hbase.HBaseSerDe.serialize(HBaseSerDe.java:286)",
                "at org.apache.hadoop.hive.ql.exec.FileSinkOperator.processOp(FileSinkOperator.java:666)",
                "... 14 more",
                "Caused by: java.lang.NullPointerException",
                "at org.apache.hadoop.hive.serde2.lazy.LazyUtils.writePrimitiveUTF8(LazyUtils.java:221)",
                "at org.apache.hadoop.hive.hbase.HBaseRowSerializer.serialize(HBaseRowSerializer.java:236)",
                "at org.apache.hadoop.hive.hbase.HBaseRowSerializer.serialize(HBaseRowSerializer.java:275)",
                "at org.apache.hadoop.hive.hbase.HBaseRowSerializer.serialize(HBaseRowSerializer.java:222)",
                "at org.apache.hadoop.hive.hbase.HBaseRowSerializer.serializeField(HBaseRowSerializer.java:194)",
                "at org.apache.hadoop.hive.hbase.HBaseRowSerializer.serialize(HBaseRowSerializer.java:118)",
                "... 15 more"
            ],
            "RootCause": "The NullPointerException occurs during the serialization process in Hive when it encounters a null value in the map type column. The methods involved in the serialization, particularly in the LazyUtils and HBaseRowSerializer classes, do not handle null values appropriately, leading to the observed error.",
            "StepsToReproduce": [
                "Create a HBase backed Hive table with a map type column.",
                "Insert data into the table where the map type column contains null values."
            ],
            "ExpectedBehavior": "The data should be written to the HBase backed table without any errors, even if the map type column contains null values.",
            "ObservedBehavior": "The mapreduce job fails with a NullPointerException when processing rows that contain null values in the map type column.",
            "Suggestions": "Implement null handling in the serialization methods to prevent NullPointerExceptions when encountering null values in map type columns.",
            "problem_location": {
                "files": [
                    "HBaseSerDe.java",
                    "LazyUtils.java",
                    "HBaseRowSerializer.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.hbase.HBaseSerDe",
                    "org.apache.hadoop.hive.serde2.lazy.LazyUtils",
                    "org.apache.hadoop.hive.hbase.HBaseRowSerializer"
                ],
                "methods": [
                    "HBaseSerDe.serialize",
                    "LazyUtils.writePrimitiveUTF8",
                    "HBaseRowSerializer.serialize"
                ]
            },
            "possible_fix": "Modify the serialization methods in HBaseSerDe and LazyUtils to check for null values before attempting to serialize them. For example, in the writePrimitiveUTF8 method, add a check to handle null inputs gracefully."
        },
        "possible_fix_code": {
            "LazyUtils.writePrimitiveUTF8": "  public static void writePrimitiveUTF8(OutputStream out, Object o,\n      PrimitiveObjectInspector oi, boolean escaped, byte escapeChar,\n      boolean[] needsEscape) throws IOException {\n\n    if (o == null) {\n        out.write(\"NULL\".getBytes()); // Handle null values gracefully\n        return;\n    }\n\n    switch (oi.getPrimitiveCategory()) {\n    case BOOLEAN: {\n      boolean b = ((BooleanObjectInspector) oi).get(o);\n      if (b) {\n        out.write(trueBytes, 0, trueBytes.length);\n      } else {\n        out.write(falseBytes, 0, falseBytes.length);\n      }\n      break;\n    }\n    case BYTE: {\n      LazyInteger.writeUTF8(out, ((ByteObjectInspector) oi).get(o));\n      break;\n    }\n    case SHORT: {\n      LazyInteger.writeUTF8(out, ((ShortObjectInspector) oi).get(o));\n      break;\n    }\n    case INT: {\n      LazyInteger.writeUTF8(out, ((IntObjectInspector) oi).get(o));\n      break;\n    }\n    case LONG: {\n      LazyLong.writeUTF8(out, ((LongObjectInspector) oi).get(o));\n      break;\n    }\n    case FLOAT: {\n      float f = ((FloatObjectInspector) oi).get(o);\n      ByteBuffer b = Text.encode(String.valueOf(f));\n      out.write(b.array(), 0, b.limit());\n      break;\n    }\n    case DOUBLE: {\n      double d = ((DoubleObjectInspector) oi).get(o);\n      ByteBuffer b = Text.encode(String.valueOf(d));\n      out.write(b.array(), 0, b.limit());\n      break;\n    }\n    case STRING: {\n      Text t = ((StringObjectInspector) oi).getPrimitiveWritableObject(o);\n      writeEscaped(out, t.getBytes(), 0, t.getLength(), escaped, escapeChar,\n          needsEscape);\n      break;\n    }\n    case CHAR: {\n      HiveCharWritable hc = ((HiveCharObjectInspector) oi).getPrimitiveWritableObject(o);\n      Text t = hc.getPaddedValue();\n      writeEscaped(out, t.getBytes(), 0, t.getLength(), escaped, escapeChar,\n          needsEscape);\n      break;\n    }\n    case VARCHAR: {\n      HiveVarcharWritable hc = ((HiveVarcharObjectInspector)oi).getPrimitiveWritableObject(o);\n      Text t = hc.getTextValue();\n      writeEscaped(out, t.getBytes(), 0, t.getLength(), escaped, escapeChar,\n          needsEscape);\n      break;\n    }\n    case BINARY: {\n      BytesWritable bw = ((BinaryObjectInspector) oi).getPrimitiveWritableObject(o);\n      byte[] toEncode = new byte[bw.getLength()];\n      System.arraycopy(bw.getBytes(), 0,toEncode, 0, bw.getLength());\n      byte[] toWrite = Base64.encodeBase64(toEncode);\n      out.write(toWrite, 0, toWrite.length);\n      break;\n    }\n    case DATE: {\n      LazyDate.writeUTF8(out,\n          ((DateObjectInspector) oi).getPrimitiveWritableObject(o));\n      break;\n    }\n    case TIMESTAMP: {\n      LazyTimestamp.writeUTF8(out,\n          ((TimestampObjectInspector) oi).getPrimitiveWritableObject(o));\n      break;\n    }\n    case INTERVAL_YEAR_MONTH: {\n      LazyHiveIntervalYearMonth.writeUTF8(out,\n          ((HiveIntervalYearMonthObjectInspector) oi).getPrimitiveWritableObject(o));\n      break;\n    }\n    case INTERVAL_DAY_TIME: {\n      LazyHiveIntervalDayTime.writeUTF8(out,\n          ((HiveIntervalDayTimeObjectInspector) oi).getPrimitiveWritableObject(o));\n      break;\n    }\n    case DECIMAL: {\n      HiveDecimalObjectInspector decimalOI = (HiveDecimalObjectInspector) oi;\n      LazyHiveDecimal.writeUTF8(out,\n        decimalOI.getPrimitiveJavaObject(o), decimalOI.scale());\n      break;\n    }\n    default: {\n      throw new RuntimeException(\"Hive internal error.\");\n    }\n    }\n  }"
        }
    },
    {
        "filename": "HIVE-11470.json",
        "creation_time": "2015-08-05T18:45:26.000+0000",
        "bug_report": {
            "Title": "NPE in DynamicPartFileRecordWriterContainer on null part-keys.",
            "Description": "A NullPointerException (NPE) occurs when using the HCatStorer for partitioning data in Apache Pig. The issue arises specifically in the `DynamicPartitionFileRecordWriterContainer` class when the dynamic partition key is null. The stack trace indicates that the NPE is thrown during the execution of a MapReduce job, particularly in the `getLocalFileWriter` method, where it attempts to convert a null value to a string, leading to a failure in processing the dynamic partition values.",
            "StackTrace": [
                "2015-07-30 23:59:59,627 WARN [main] org.apache.hadoop.mapred.YarnChild: Exception running child : java.io.IOException: java.lang.NullPointerException",
                "at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Reduce.runPipeline(PigGenericMapReduce.java:473)",
                "at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Reduce.processOnePackageOutput(PigGenericMapReduce.java:436)",
                "at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Reduce.reduce(PigGenericMapReduce.java:416)",
                "at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Reduce.reduce(PigGenericMapReduce.java:256)",
                "at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:171)",
                "at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:627)",
                "at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:389)",
                "at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1694)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)",
                "Caused by: java.lang.NullPointerException",
                "at org.apache.hive.hcatalog.mapreduce.DynamicPartitionFileRecordWriterContainer.getLocalFileWriter(DynamicPartitionFileRecordWriterContainer.java:141)",
                "at org.apache.hive.hcatalog.mapreduce.FileRecordWriterContainer.write(FileRecordWriterContainer.java:110)",
                "at org.apache.hive.hcatalog.mapreduce.FileRecordWriterContainer.write(FileRecordWriterContainer.java:54)",
                "at org.apache.hive.hcatalog.pig.HCatBaseStorer.putNext(HCatBaseStorer.java:309)",
                "at org.apache.hive.hcatalog.pig.HCatStorer.putNext(HCatStorer.java:61)",
                "at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputFormat$PigRecordWriter.write(PigOutputFormat.java:139)",
                "at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputFormat$PigRecordWriter.write(PigOutputFormat.java:98)",
                "at org.apache.hadoop.mapred.ReduceTask$NewTrackingRecordWriter.write(ReduceTask.java:558)",
                "at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)",
                "at org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer$Context.write(WrappedReducer.java:105)",
                "at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Reduce.runPipeline(PigGenericMapReduce.java:471)"
            ],
            "RootCause": "The root cause of the NullPointerException is the assumption made in the `getLocalFileWriter` method of the `DynamicPartitionFileRecordWriterContainer` class that the dynamic partition values retrieved from the `HCatRecord` are non-null. When a null value is encountered, it leads to an attempt to call `toString()` on a null reference, resulting in an NPE.",
            "StepsToReproduce": [
                "1. Set up a MapReduce job using Apache Pig with HCatStorer.",
                "2. Ensure that the dynamic partition key is set to null.",
                "3. Execute the job and observe the logs for the NullPointerException."
            ],
            "ExpectedBehavior": "The system should handle null dynamic partition keys gracefully, either by substituting them with a default value or by skipping the write operation without throwing an exception.",
            "ObservedBehavior": "The system throws a NullPointerException when attempting to process a null dynamic partition key, causing the MapReduce job to fail.",
            "Suggestions": "Implement a null check in the `getLocalFileWriter` method to handle cases where dynamic partition values are null. Substitute null values with a default partition key, such as '__HIVE_DEFAULT_PARTITION__'. Additionally, review the initialization logic for dynamic writers to ensure robustness against concurrent writes.",
            "problem_location": {
                "files": [
                    "DynamicPartitionFileRecordWriterContainer.java",
                    "FileRecordWriterContainer.java",
                    "HCatBaseStorer.java",
                    "HCatStorer.java"
                ],
                "classes": [
                    "org.apache.hive.hcatalog.mapreduce.DynamicPartitionFileRecordWriterContainer",
                    "org.apache.hive.hcatalog.mapreduce.FileRecordWriterContainer",
                    "org.apache.hive.hcatalog.pig.HCatBaseStorer",
                    "org.apache.hive.hcatalog.pig.HCatStorer"
                ],
                "methods": [
                    "DynamicPartitionFileRecordWriterContainer.getLocalFileWriter",
                    "FileRecordWriterContainer.write",
                    "HCatBaseStorer.putNext",
                    "HCatStorer.putNext"
                ]
            },
            "possible_fix": "In the `getLocalFileWriter` method, add a check for null values in the dynamic partition values list. If a null value is detected, substitute it with '__HIVE_DEFAULT_PARTITION__' before proceeding with the rest of the method logic."
        },
        "possible_fix_code": {
            "DynamicPartitionFileRecordWriterContainer.getLocalFileWriter": "  protected LocalFileWriter getLocalFileWriter(HCatRecord value) throws IOException, HCatException {\n    OutputJobInfo localJobInfo = null;\n    // Calculate which writer to use from the remaining values - this needs to\n    // be done before we delete cols.\n    List<String> dynamicPartValues = new ArrayList<String>();\n    for (Integer colToAppend : dynamicPartCols) {\n      String partValue = value.get(colToAppend) != null ? value.get(colToAppend).toString() : \"__HIVE_DEFAULT_PARTITION__\";\n      dynamicPartValues.add(partValue);\n    }\n\n    String dynKey = dynamicPartValues.toString();\n    if (!baseDynamicWriters.containsKey(dynKey)) {\n      if ((maxDynamicPartitions != -1) && (baseDynamicWriters.size() > maxDynamicPartitions)) {\n        throw new HCatException(ErrorType.ERROR_TOO_MANY_DYNAMIC_PTNS,\n            \"Number of dynamic partitions being created \"\n                + \"exceeds configured max allowable partitions[\" + maxDynamicPartitions\n                + \"], increase parameter [\" + HiveConf.ConfVars.DYNAMICPARTITIONMAXPARTS.varname\n                + \"] if needed.\");\n      }\n\n      org.apache.hadoop.mapred.TaskAttemptContext currTaskContext =\n          HCatMapRedUtil.createTaskAttemptContext(context);\n      configureDynamicStorageHandler(currTaskContext, dynamicPartValues);\n      localJobInfo = HCatBaseOutputFormat.getJobInfo(currTaskContext.getConfiguration());\n\n      // Setup serDe.\n      SerDe currSerDe =\n          ReflectionUtils.newInstance(storageHandler.getSerDeClass(), currTaskContext.getJobConf());\n      try {\n        InternalUtil.initializeOutputSerDe(currSerDe, currTaskContext.getConfiguration(),\n            localJobInfo);\n      } catch (SerDeException e) {\n        throw new IOException(\"Failed to initialize SerDe\", e);\n      }\n\n      // create base OutputFormat\n      org.apache.hadoop.mapred.OutputFormat baseOF =\n          ReflectionUtils.newInstance(storageHandler.getOutputFormatClass(),\n              currTaskContext.getJobConf());\n\n      // We are skipping calling checkOutputSpecs() for each partition\n      // As it can throw a FileAlreadyExistsException when more than one\n      // mapper is writing to a partition.\n      // See HCATALOG-490, also to avoid contacting the namenode for each new\n      // FileOutputFormat instance.\n      // In general this should be ok for most FileOutputFormat implementations\n      // but may become an issue for cases when the method is used to perform\n      // other setup tasks.\n\n      // Get Output Committer\n      org.apache.hadoop.mapred.OutputCommitter baseOutputCommitter =\n          currTaskContext.getJobConf().getOutputCommitter();\n\n      // Create currJobContext the latest so it gets all the config changes\n      org.apache.hadoop.mapred.JobContext currJobContext =\n          HCatMapRedUtil.createJobContext(currTaskContext);\n\n      // Set up job.\n      baseOutputCommitter.setupJob(currJobContext);\n\n      // Recreate to refresh jobConf of currTask context.\n      currTaskContext =\n          HCatMapRedUtil.createTaskAttemptContext(currJobContext.getJobConf(),\n              currTaskContext.getTaskAttemptID(), currTaskContext.getProgressible());\n\n      // Set temp location.\n      currTaskContext.getConfiguration().set(\n          \"mapred.work.output.dir\",\n          new FileOutputCommitter(new Path(localJobInfo.getLocation()), currTaskContext)\n              .getWorkPath().toString());\n\n      // Set up task.\n      baseOutputCommitter.setupTask(currTaskContext);\n\n      Path parentDir = new Path(currTaskContext.getConfiguration().get(\"mapred.work.output.dir\"));\n      Path childPath =\n          new Path(parentDir, FileOutputFormat.getUniqueFile(currTaskContext, \"part\", \"\"));\n\n      RecordWriter baseRecordWriter =\n          baseOF.getRecordWriter(parentDir.getFileSystem(currTaskContext.getConfiguration()),\n              currTaskContext.getJobConf(), childPath.toString(),\n              InternalUtil.createReporter(currTaskContext));\n\n      baseDynamicWriters.put(dynKey, baseRecordWriter);\n      baseDynamicSerDe.put(dynKey, currSerDe);\n      baseDynamicCommitters.put(dynKey, baseOutputCommitter);\n      dynamicContexts.put(dynKey, currTaskContext);\n      dynamicObjectInspectors.put(dynKey,\n          InternalUtil.createStructObjectInspector(jobInfo.getOutputSchema()));\n      dynamicOutputJobInfo.put(dynKey,\n          HCatOutputFormat.getJobInfo(dynamicContexts.get(dynKey).getConfiguration()));\n    }\n\n    return new LocalFileWriter(baseDynamicWriters.get(dynKey), dynamicObjectInspectors.get(dynKey),\n        baseDynamicSerDe.get(dynKey), dynamicOutputJobInfo.get(dynKey));\n  }"
        }
    },
    {
        "filename": "HIVE-12476.json",
        "creation_time": "2015-11-20T03:30:18.000+0000",
        "bug_report": {
            "Title": "Metastore NPE on Oracle with Direct SQL",
            "Description": "A NullPointerException (NPE) is occurring in the Hive Metastore when using Direct SQL mode, particularly during the serialization of Partition and StorageDescriptor objects. The stack trace indicates that the error arises from the Thrift protocol's write operations, which are failing due to uninitialized object references. This issue appears to be similar to HIVE-8485, suggesting that additional fixes are required for handling Partition and StorageDescriptor parameters in this context.",
            "StackTrace": [
                "2015-11-19 18:08:33,841 ERROR [pool-5-thread-2]: server.TThreadPoolServer (TThreadPoolServer.java:run(296)) - Error occurred during processing of message.",
                "java.lang.NullPointerException",
                "at org.apache.thrift.protocol.TBinaryProtocol.writeString(TBinaryProtocol.java:200)",
                "at org.apache.hadoop.hive.metastore.api.SerDeInfo$SerDeInfoStandardScheme.write(SerDeInfo.java:579)",
                "at org.apache.hadoop.hive.metastore.api.SerDeInfo$SerDeInfoStandardScheme.write(SerDeInfo.java:501)",
                "at org.apache.hadoop.hive.metastore.api.SerDeInfo.write(SerDeInfo.java:439)",
                "at org.apache.hadoop.hive.metastore.api.StorageDescriptor$StorageDescriptorStandardScheme.write(StorageDescriptor.java:1490)",
                "at org.apache.hadoop.hive.metastore.api.StorageDescriptor$StorageDescriptorStandardScheme.write(StorageDescriptor.java:1288)",
                "at org.apache.hadoop.hive.metastore.api.StorageDescriptor.write(StorageDescriptor.java:1154)",
                "at org.apache.hadoop.hive.metastore.api.Partition$PartitionStandardScheme.write(Partition.java:1072)",
                "at org.apache.hadoop.hive.metastore.api.Partition$PartitionStandardScheme.write(Partition.java:929)",
                "at org.apache.hadoop.hive.metastore.api.Partition.write(Partition.java:825)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_partitions_result$get_partitions_resultStandardScheme.write(ThriftHiveMetastore.java:64470)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_partitions_result$get_partitions_resultStandardScheme.write(ThriftHiveMetastore.java:64402)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_partitions_result.write(ThriftHiveMetastore.java:64340)",
                "at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:53)",
                "at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)",
                "at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor$1.run(HadoopThriftAuthBridge.java:681)",
                "at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor$1.run(HadoopThriftAuthBridge.java:676)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)",
                "at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor.process(HadoopThriftAuthBridge.java:676)",
                "at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:285)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "RootCause": "The NullPointerException is likely caused by an uninitialized object in the Hive Metastore or Thrift protocol during the serialization of Partition and StorageDescriptor objects. This suggests that the input data being processed may be incomplete or improperly configured.",
            "StepsToReproduce": [
                "1. Configure the Hive Metastore to use Direct SQL mode.",
                "2. Attempt to retrieve partitions from a table that has not been properly initialized or has missing parameters.",
                "3. Observe the NullPointerException in the logs."
            ],
            "ExpectedBehavior": "The Hive Metastore should successfully serialize and return the requested partitions without throwing a NullPointerException.",
            "ObservedBehavior": "A NullPointerException is thrown during the serialization process, indicating that some required object references are null.",
            "Suggestions": "Check the input data being processed by the Hive Metastore to ensure that all required fields are populated. Review the configuration of the Hive Metastore and related components for correctness. Investigate any recent changes in the codebase or data that might have introduced this issue.",
            "problem_location": {
                "files": [
                    "TBinaryProtocol.java",
                    "SerDeInfo.java",
                    "StorageDescriptor.java",
                    "Partition.java",
                    "ThriftHiveMetastore.java"
                ],
                "classes": [
                    "org.apache.thrift.protocol.TBinaryProtocol",
                    "org.apache.hadoop.hive.metastore.api.SerDeInfo",
                    "org.apache.hadoop.hive.metastore.api.StorageDescriptor",
                    "org.apache.hadoop.hive.metastore.api.Partition",
                    "org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore"
                ],
                "methods": [
                    "TBinaryProtocol.writeString",
                    "SerDeInfo$SerDeInfoStandardScheme.write",
                    "StorageDescriptor$StorageDescriptorStandardScheme.write",
                    "Partition$PartitionStandardScheme.write",
                    "ThriftHiveMetastore$get_partitions_result$get_partitions_resultStandardScheme.write"
                ]
            },
            "possible_fix": "Ensure that all necessary fields in the Partition and StorageDescriptor objects are initialized before serialization. This may involve adding null checks or default values in the relevant classes."
        },
        "possible_fix_code": {
            "org.apache.hadoop.hive.metastore.api.Partition$PartitionStandardScheme.write": "public void write(TProtocol oprot) throws TException {\n    validate(); // Ensure all required fields are initialized\n    oprot.writeStructBegin(STRUCT_DESC);\n    if (this.partitionName != null) {\n        oprot.writeFieldBegin(PARTITION_NAME_FIELD_DESC);\n        oprot.writeString(this.partitionName);\n        oprot.writeFieldEnd();\n    }\n    if (this.values != null) {\n        oprot.writeFieldBegin(VALUES_FIELD_DESC);\n        oprot.writeListBegin(new TList(TType.STRING, this.values.size()));\n        for (String value : this.values) {\n            oprot.writeString(value);\n        }\n        oprot.writeListEnd();\n        oprot.writeFieldEnd();\n    }\n    if (this.sd != null) {\n        oprot.writeFieldBegin(SD_FIELD_DESC);\n        this.sd.write(oprot);\n        oprot.writeFieldEnd();\n    }\n    oprot.writeFieldStop();\n    oprot.writeStructEnd();\n}\n\nprivate void validate() throws TException {\n    if (this.partitionName == null) {\n        throw new TException(\"Partition name cannot be null\");\n    }\n    if (this.sd == null) {\n        throw new TException(\"StorageDescriptor cannot be null\");\n    }\n}"
        }
    },
    {
        "filename": "HIVE-10559.json",
        "creation_time": "2015-04-30T21:12:40.000+0000",
        "bug_report": {
            "Title": "IndexOutOfBoundsException with RemoveDynamicPruningBySize",
            "Description": "An IndexOutOfBoundsException is thrown when the RemoveDynamicPruningBySize class attempts to access the first element of an empty list of parent operators. This occurs during the processing of dynamic pruning based on data size, specifically in the process method of the RemoveDynamicPruningBySize class.",
            "StackTrace": [
                "2015-04-29 10:34:36,390 ERROR [main]: ql.Driver (SessionState.java:printError(956)) - FAILED: IndexOutOfBoundsException Index: 0, Size: 0",
                "java.lang.IndexOutOfBoundsException: Index: 0, Size: 0",
                "at java.util.ArrayList.rangeCheck(ArrayList.java:635)",
                "at java.util.ArrayList.get(ArrayList.java:411)",
                "at org.apache.hadoop.hive.ql.optimizer.RemoveDynamicPruningBySize.process(RemoveDynamicPruningBySize.java:61)",
                "at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90)",
                "at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:95)",
                "at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:79)",
                "at org.apache.hadoop.hive.ql.lib.ForwardWalker.walk(ForwardWalker.java:77)",
                "at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.startWalking(DefaultGraphWalker.java:110)",
                "at org.apache.hadoop.hive.ql.parse.TezCompiler.runStatsDependentOptimizations(TezCompiler.java:281)",
                "at org.apache.hadoop.hive.ql.parse.TezCompiler.optimizeOperatorPlan(TezCompiler.java:123)",
                "at org.apache.hadoop.hive.ql.parse.TaskCompiler.compile(TaskCompiler.java:102)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10092)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:9932)",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:227)",
                "at org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.analyzeInternal(ExplainSemanticAnalyzer.java:74)",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:227)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:424)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:308)",
                "at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1122)",
                "at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1170)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1059)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1049)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:213)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:165)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:311)",
                "at org.apache.hadoop.hive.ql.QTestUtil.executeClientInternal(QTestUtil.java:1026)",
                "at org.apache.hadoop.hive.ql.QTestUtil.executeClient(QTestUtil.java:1000)",
                "at org.apache.hadoop.hive.cli.TestMiniTezCliDriver.runTest(TestMiniTezCliDriver:139)",
                "at org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_q85(TestMiniTezCliDriver.java:123)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at junit.framework.TestCase.runTest(TestCase.java:176)",
                "at junit.framework.TestCase.runBare(TestCase.java:141)",
                "at junit.framework.TestResult$1.protect(TestResult.java:122)",
                "at junit.framework.TestResult.runProtected(TestResult.java:142)",
                "at junit.framework.TestResult.run(TestResult.java:125)",
                "at junit.framework.TestCase.run(TestCase.java:129)",
                "at junit.framework.TestSuite.runTest(TestSuite.java:255)",
                "at junit.framework.TestSuite.runTest(TestSuite.java:250)",
                "at org.junit.internal.runners.JUnit38ClassRunner.run(JUnit38ClassRunner.java:84)",
                "at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:264)",
                "at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:153)",
                "at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:124)",
                "at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:200)",
                "at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:153)",
                "at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)"
            ],
            "RootCause": "The root cause of the IndexOutOfBoundsException is that the process method in the RemoveDynamicPruningBySize class attempts to access the first element of the parent operators list without checking if the list is empty.",
            "StepsToReproduce": [
                "Run the script attached to the bug report.",
                "Ensure that the conditions leading to an empty parent operators list are met."
            ],
            "ExpectedBehavior": "The system should handle cases where the parent operators list is empty without throwing an IndexOutOfBoundsException.",
            "ObservedBehavior": "An IndexOutOfBoundsException is thrown when attempting to access the first element of an empty list.",
            "Suggestions": "Add a check in the process method of RemoveDynamicPruningBySize to ensure that the parent operators list is not empty before accessing its elements.",
            "problem_location": {
                "files": [
                    "ql/src/java/org/apache/hadoop/hive/ql/optimizer/RemoveDynamicPruningBySize.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.ql.optimizer.RemoveDynamicPruningBySize",
                    "org.apache.hadoop.hive.ql.exec.Operator"
                ],
                "methods": [
                    "RemoveDynamicPruningBySize.process",
                    "Operator.getParentOperators"
                ]
            },
            "possible_fix": "Modify the process method in RemoveDynamicPruningBySize to include a check for the size of the parent operators list before accessing its elements:\n\n```java\nif (!curr.getParentOperators().isEmpty()) {\n    curr = curr.getParentOperators().get(0);\n}\n```"
        },
        "possible_fix_code": {
            "RemoveDynamicPruningBySize.process": "  public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procContext,\n      Object... nodeOutputs)\n      throws SemanticException {\n\n    OptimizeTezProcContext context = (OptimizeTezProcContext) procContext;\n\n    AppMasterEventOperator event = (AppMasterEventOperator) nd;\n    AppMasterEventDesc desc = event.getConf();\n\n    if (desc.getStatistics().getDataSize() > context.conf\n        .getLongVar(ConfVars.TEZ_DYNAMIC_PARTITION_PRUNING_MAX_DATA_SIZE)) {\n      Operator<?> child = event;\n      Operator<?> curr = event;\n\n      while (curr.getChildOperators().size() <= 1) {\n        child = curr;\n        List<Operator<?>> parentOperators = curr.getParentOperators();\n        if (!parentOperators.isEmpty()) {\n          curr = parentOperators.get(0);\n        } else {\n          break; // Exit the loop if there are no parent operators\n        }\n      }\n      // at this point we\\'ve found the fork in the op pipeline that has the\n      // pruning as a child plan.\n      LOG.info(\"Disabling dynamic pruning for: \"\n          + ((DynamicPruningEventDesc) desc).getTableScan().getName()\n          + \". Expected data size is too big: \" + desc.getStatistics().getDataSize());\n      curr.removeChild(child);\n    }\n    return false;\n  }"
        }
    },
    {
        "filename": "HIVE-9721.json",
        "creation_time": "2015-02-19T06:56:17.000+0000",
        "bug_report": {
            "Title": "Hadoop23Shims.setFullFileStatus should check for null",
            "Description": "The issue arises when using the `Hadoop23Shims.setFullFileStatus` method, which leads to a `NullPointerException`. This occurs in the context of Apache Hive interacting with the Hadoop file system, specifically when the `RawLocalFileSystem` is used, which does not support ACL operations. The stack trace indicates that the method fails to handle null values appropriately, resulting in an exception when attempting to set the full file status.",
            "StackTrace": [
                "java.lang.UnsupportedOperationException: RawLocalFileSystem doesn't support getAclStatus",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.hive.shims.Hadoop23Shims.setFullFileStatus(Hadoop23Shims.java:668)",
                "at org.apache.hadoop.hive.common.FileUtils.mkdir(FileUtils.java:527)",
                "at org.apache.hadoop.hive.ql.Context.getStagingDir(Context.java:234)",
                "at org.apache.hadoop.hive.ql.Context.getExtTmpPathRelTo(Context.java:424)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genFileSinkPlan(SemanticAnalyzer.java:6290)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPostGroupByBodyPlan(SemanticAnalyzer.java:9069)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genBodyPlan(SemanticAnalyzer.java:8961)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:9807)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:9700)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genOPTree(SemanticAnalyzer.java:10136)",
                "at org.apache.hadoop.hive.ql.parse.CalcitePlanner.genOPTree(CalcitePlanner.java:284)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10147)",
                "at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:190)",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:222)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:421)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:307)",
                "at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1112)",
                "at org.apache.hadoop.hive.ql.Driver.compileAndRespond(Driver.java:1106)",
                "at org.apache.hive.service.cli.operation.SQLOperation.prepare(SQLOperation.java:101)",
                "at org.apache.hive.service.cli.operation.SQLOperation.runInternal(SQLOperation.java:172)",
                "at org.apache.hive.service.cli.operation.Operation.run(Operation.java:257)",
                "at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:379)",
                "at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementAsync(HiveSessionImpl.java:366)",
                "at org.apache.hive.service.cli.CLIService.executeStatementAsync(CLIService.java:271)",
                "at org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:415)",
                "at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1313)",
                "at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1298)",
                "at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)",
                "at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)",
                "at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor.process(HadoopThriftAuthBridge.java:692)",
                "at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:285)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "RootCause": "The root cause of the issue is a `NullPointerException` in the `Hadoop23Shims.setFullFileStatus` method, which is triggered when the method attempts to access an object that has not been initialized. This is likely a consequence of the preceding `UnsupportedOperationException` indicating that the `RawLocalFileSystem` does not support ACL operations.",
            "StepsToReproduce": [
                "Set up a Hive environment using the RawLocalFileSystem.",
                "Attempt to execute a Hive query that requires ACL operations.",
                "Observe the resulting exceptions in the logs."
            ],
            "ExpectedBehavior": "The system should handle file status operations without throwing exceptions, even when using a file system that does not support ACLs.",
            "ObservedBehavior": "The system throws a `NullPointerException` when attempting to set the full file status due to unhandled null values in the `Hadoop23Shims.setFullFileStatus` method.",
            "Suggestions": "To resolve this issue, consider the following steps: 1. Ensure that the correct file system is being used. If ACLs are necessary, switch to a file system that supports them, such as HDFS. 2. Review the `setFullFileStatus` method to add null checks before accessing object properties. 3. Check the configuration settings for Hive and Hadoop to ensure they are set up correctly.",
            "problem_location": {
                "files": [
                    "Hadoop23Shims.java",
                    "FileUtils.java",
                    "Context.java",
                    "SemanticAnalyzer.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.shims.Hadoop23Shims",
                    "org.apache.hadoop.hive.common.FileUtils",
                    "org.apache.hadoop.hive.ql.Context",
                    "org.apache.hadoop.hive.ql.parse.SemanticAnalyzer"
                ],
                "methods": [
                    "Hadoop23Shims.setFullFileStatus",
                    "FileUtils.mkdir",
                    "Context.getStagingDir",
                    "SemanticAnalyzer.genFileSinkPlan"
                ]
            },
            "possible_fix": "Add null checks in the `setFullFileStatus` method to prevent `NullPointerException`. For example, before accessing any properties of the object, ensure that the object is not null."
        },
        "possible_fix_code": {
            "Hadoop23Shims.setFullFileStatus": "public void setFullFileStatus(Configuration conf, HdfsFileStatus fullFileStatus, FileSystem fs, Path path) throws IOException {\n    if (conf == null || fullFileStatus == null || fs == null || path == null) {\n        throw new IllegalArgumentException(\"Arguments cannot be null\");\n    }\n    // Existing logic to set full file status\n    // ... (rest of the method implementation)\n}"
        }
    },
    {
        "filename": "HIVE-4216.json",
        "creation_time": "2013-03-21T20:53:24.000+0000",
        "bug_report": {
            "Title": "TestHBaseMinimrCliDriver throws weird error with HBase 0.94.5 and Hadoop 23 and test is stuck infinitely",
            "Description": "After upgrading to Hadoop 23 and HBase 0.94.5 compiled for Hadoop 23, the TestHBaseMinimrCliDriver fails during execution. The failure occurs after updating the 'hbase_bulk.m' properties, specifically setting 'mapreduce.totalorderpartitioner.naturalorder' to false and defining the partitioner path. The test hangs indefinitely during the reducer phase of a query, leading to repeated attempts by the MiniMRCluster to spawn new reducers. The error observed in the syslog indicates a NullPointerException during the processing of a Hive job, particularly while creating bucket files in the FileSinkOperator.",
            "StackTrace": [
                "13-03-20 16:26:48,942 FATAL [IPC Server handler 17 on 55996] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Task: attempt_1363821864968_0003_r_000002_0 - exited : java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {\"key\":{\"reducesinkkey0\":\"val_200\"},\"value\":{\"_col0\":\"val_200\",\"_col1\":\"200\",\"_col2\":\"201.0\"},\"alias\":0}",
                "Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.NullPointerException",
                "at org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.getHiveRecordWriter(HiveFileFormatUtils.java:237)",
                "at org.apache.hadoop.hive.ql.exec.FileSinkOperator.processOp(FileSinkOperator.java:525)",
                "Caused by: java.lang.NullPointerException",
                "at org.apache.hadoop.mapreduce.TaskID$CharTaskTypeMaps.getRepresentingCharacter(TaskID.java:265)"
            ],
            "RootCause": "The root cause of the issue is a NullPointerException occurring in the Hive job, specifically in the 'getHiveRecordWriter' method of the HiveFileFormatUtils class. This suggests that one or more critical parameters (such as 'tableInfo', 'conf', or 'outPath') being passed to this method are null, which prevents the output file format class from being instantiated correctly.",
            "StepsToReproduce": [
                "Upgrade to Hadoop 23 and HBase 0.94.5.",
                "Update 'hbase_bulk.m' with the properties: 'set mapreduce.totalorderpartitioner.naturalorder=false;' and 'set mapreduce.totalorderpartitioner.path=/tmp/hbpartition.lst;'.",
                "Run the TestHBaseMinimrCliDriver."
            ],
            "ExpectedBehavior": "The TestHBaseMinimrCliDriver should complete successfully without hanging, and the reducer phase should process the data correctly without throwing exceptions.",
            "ObservedBehavior": "The test hangs indefinitely during the reducer phase, and the syslog shows a NullPointerException related to the Hive job processing.",
            "Suggestions": "Check the configurations and data being passed to the 'getHiveRecordWriter' and 'processOp' methods to ensure that none of the critical parameters are null. Additionally, verify that the data being processed meets the expected format and structure.",
            "problem_location": {
                "files": [
                    "HiveFileFormatUtils.java",
                    "FileSinkOperator.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.ql.io.HiveFileFormatUtils",
                    "org.apache.hadoop.hive.ql.exec.FileSinkOperator"
                ],
                "methods": [
                    "getHiveRecordWriter",
                    "processOp"
                ]
            },
            "possible_fix": "Ensure that the parameters passed to 'getHiveRecordWriter' are properly initialized and not null. Specifically, check the 'tableInfo', 'conf', and 'outPath' parameters. Additionally, review the data being processed in 'processOp' to ensure that it is valid and does not contain null values."
        },
        "possible_fix_code": {
            "org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.getHiveRecordWriter": "  public static RecordWriter getHiveRecordWriter(JobConf jc,\n      TableDesc tableInfo, Class<? extends Writable> outputClass,\n      FileSinkDesc conf, Path outPath, Reporter reporter) throws HiveException {\n    if (tableInfo == null) {\n        throw new HiveException(\"Table information cannot be null.\");\n    }\n    if (conf == null) {\n        throw new HiveException(\"FileSinkDesc configuration cannot be null.\");\n    }\n    if (outPath == null) {\n        throw new HiveException(\"Output path cannot be null.\");\n    }\n    try {\n      HiveOutputFormat<?, ?> hiveOutputFormat = tableInfo\n          .getOutputFileFormatClass().newInstance();\n      boolean isCompressed = conf.getCompressed();\n      JobConf jc_output = jc;\n      if (isCompressed) {\n        jc_output = new JobConf(jc);\n        String codecStr = conf.getCompressCodec();\n        if (codecStr != null && !codecStr.trim().equals(\"\")) {\n          Class<? extends CompressionCodec> codec = (Class<? extends CompressionCodec>) Class\n              .forName(codecStr);\n          FileOutputFormat.setOutputCompressorClass(jc_output, codec);\n        }\n        String type = conf.getCompressType();\n        if (type != null && !type.trim().equals(\"\")) {\n          CompressionType style = CompressionType.valueOf(type);\n          SequenceFileOutputFormat.setOutputCompressionType(jc, style);\n        }\n      }\n      return getRecordWriter(jc_output, hiveOutputFormat, outputClass,\n          isCompressed, tableInfo.getProperties(), outPath, reporter);\n    } catch (Exception e) {\n      throw new HiveException(e);\n    }\n  }",
            "org.apache.hadoop.hive.ql.exec.FileSinkOperator.processOp": "  public void processOp(Object row, int tag) throws HiveException {\n    if (row == null) {\n        throw new HiveException(\"Input row cannot be null.\");\n    }\n    /* Create list bucketing sub-directory only if stored-as-directories is on. */\n    String lbDirName = null;\n    lbDirName = (lbCtx == null) ? null : generateListBucketingDirName(row);\n\n    FSPaths fpaths;\n\n    if (!bDynParts && !filesCreated) {\n      if (lbDirName != null) {\n        FSPaths fsp2 = lookupListBucketingPaths(lbDirName);\n      } else {\n        createBucketFiles(fsp);\n      }\n    }\n\n    // Since File Sink is a terminal operator, forward is not called - so,\n    // maintain the number of output rows explicitly\n    if (counterNameToEnum != null) {\n      ++outputRows;\n      if (outputRows % 1000 == 0) {\n        incrCounter(numOutputRowsCntr, outputRows);\n        outputRows = 0;\n      }\n    }\n\n    try {\n      updateProgress();\n\n      // if DP is enabled, get the final output writers and prepare the real output row\n      assert inputObjInspectors[0].getCategory() == ObjectInspector.Category.STRUCT : \"input object inspector is not struct\";\n\n      if (bDynParts) {\n        // copy the DP column values from the input row to dpVals\n        dpVals.clear();\n        dpWritables.clear();\n        ObjectInspectorUtils.partialCopyToStandardObject(dpWritables, row, dpStartCol, numDynParts,\n            (StructObjectInspector) inputObjInspectors[0], ObjectInspectorCopyOption.WRITABLE);\n        // get a set of RecordWriter based on the DP column values\n        // pass the null value along to the escaping process to determine what the dir should be\n        for (Object o : dpWritables) {\n          if (o == null || o.toString().length() == 0) {\n            dpVals.add(dpCtx.getDefaultPartitionName());\n          } else {\n            dpVals.add(o.toString());\n          }\n        }\n        // use SubStructObjectInspector to serialize the non-partitioning columns in the input row\n        recordValue = serializer.serialize(row, subSetOI);\n        fpaths = getDynOutPaths(dpVals, lbDirName);\n\n      } else {\n        if (lbDirName != null) {\n          fpaths = lookupListBucketingPaths(lbDirName);\n        } else {\n          fpaths = fsp;\n        }\n        // use SerDe to serialize r, and write it out\n        recordValue = serializer.serialize(row, inputObjInspectors[0]);\n      }\n\n      rowOutWriters = fpaths.outWriters;\n      if (conf.isGatherStats()) {\n        if (statsCollectRawDataSize) {\n          SerDeStats stats = serializer.getSerDeStats();\n          if (stats != null) {\n            fpaths.stat.addToStat(StatsSetupConst.RAW_DATA_SIZE, stats.getRawDataSize());\n          }\n        }\n        fpaths.stat.addToStat(StatsSetupConst.ROW_COUNT, 1);\n      }\n\n\n      if (row_count != null) {\n        row_count.set(row_count.get() + 1);\n      }\n\n      if (!multiFileSpray) {\n        rowOutWriters[0].write(recordValue);\n      } else {\n        int keyHashCode = 0;\n        for (int i = 0; i < partitionEval.length; i++) {\n          Object o = partitionEval[i].evaluate(row);\n          keyHashCode = keyHashCode * 31\n              + ObjectInspectorUtils.hashCode(o, partitionObjectInspectors[i]);\n        }\n        key.setHashCode(keyHashCode);\n        int bucketNum = prtner.getBucket(key, null, totalFiles);\n        int idx = bucketMap.get(bucketNum);\n        rowOutWriters[idx].write(recordValue);\n      }\n    } catch (IOException e) {\n      throw new HiveException(e);\n    } catch (SerDeException e) {\n      throw new HiveException(e);\n    }\n  }"
        }
    },
    {
        "filename": "HIVE-13836.json",
        "creation_time": "2016-05-24T22:37:59.000+0000",
        "bug_report": {
            "Title": "DbNotifications giving an error = Invalid state. Transaction has already started",
            "Description": "The issue arises when using the pyhs2 Python client to create tables and partitions in Hive. The error occurs during concurrent execution of DDL queries across multiple threads, leading to transaction management issues within the Datanucleus framework. Specifically, the error message indicates that a transaction is being attempted to start while another transaction is already active, which is not allowed. This situation is exacerbated by the multithreaded nature of the script, which creates multiple connections and may lead to improper transaction handling.",
            "StackTrace": [
                "2016-05-04 17:49:26,226 ERROR org.apache.hadoop.hive.metastore.RetryingHMSHandler: [pool-4-thread-194]: HMSHandler Fatal error: Invalid state. Transaction has already started",
                "org.datanucleus.transaction.NucleusTransactionException: Invalid state. Transaction has already started",
                "at org.datanucleus.transaction.TransactionManager.begin(TransactionManager.java:47)",
                "at org.datanucleus.TransactionImpl.begin(TransactionImpl.java:131)",
                "at org.datanucleus.api.jdo.JDOTransaction.internalBegin(JDOTransaction.java:88)",
                "at org.datanucleus.api.jdo.JDOTransaction.begin(JDOTransaction.java:80)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.openTransaction(ObjectStore.java:463)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.addNotificationEvent(ObjectStore.java:7522)",
                "at sun.reflect.GeneratedMethodAccessor20.invoke(Unknown Source)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:114)",
                "at com.sun.proxy.$Proxy10.addNotificationEvent(Unknown Source)",
                "at org.apache.hive.hcatalog.listener.DbNotificationListener.enqueue(DbNotificationListener.java:261)",
                "at org.apache.hive.hcatalog.listener.DbNotificationListener.onCreateTable(DbNotificationListener.java:123)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_table_core(HiveMetaStore.java:1483)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_table_with_environment_context(HiveMetaStore.java:1502)",
                "at sun.reflect.GeneratedMethodAccessor57.invoke(Unknown Source)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:138)",
                "at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:99)",
                "at com.sun.proxy.$Proxy14.create_table_with_environment_context(Unknown Source)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$create_table_with_environment_context.getResult(ThriftHiveMetastore.java:9267)"
            ],
            "RootCause": "The root cause of the issue is improper transaction management in the Datanucleus framework, where a new transaction is attempted to be started while another transaction is still active. This is likely due to concurrent execution of DDL queries without proper handling of transaction states.",
            "StepsToReproduce": [
                "1. Use the pyhs2 Python client to connect to Hive.",
                "2. Create a multithreaded script that opens 8 connections.",
                "3. Execute DDL queries concurrently across these connections.",
                "4. Observe the error message indicating that a transaction has already started."
            ],
            "ExpectedBehavior": "The system should allow multiple DDL queries to be executed concurrently without encountering transaction management errors.",
            "ObservedBehavior": "The system throws a NucleusTransactionException indicating that a transaction cannot be started because one is already active.",
            "Suggestions": "To resolve this issue, ensure that transactions are properly managed by checking the current transaction state before starting a new one. Implement try-catch-finally blocks to handle transactions correctly, ensuring that they are committed or rolled back as necessary. Additionally, review the configuration settings for transaction management in the Datanucleus framework.",
            "problem_location": {
                "files": [
                    "ObjectStore.java",
                    "DbNotificationListener.java",
                    "HiveMetaStore.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.metastore.ObjectStore",
                    "org.apache.hive.hcatalog.listener.DbNotificationListener",
                    "org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler"
                ],
                "methods": [
                    "ObjectStore.openTransaction",
                    "DbNotificationListener.enqueue",
                    "HiveMetaStore$HMSHandler.create_table_with_environment_context"
                ]
            },
            "possible_fix": "Implement checks to ensure that a new transaction is not started if one is already active. This can be done by modifying the transaction management logic in the relevant methods to include state checks before beginning a new transaction."
        },
        "possible_fix_code": {
            "ObjectStore.addNotificationEvent": "  public void addNotificationEvent(NotificationEvent entry) {\n    boolean commited = false;\n    Query query = null;\n    try {\n      // Check if a transaction is already active before opening a new one\n      if (currentTransaction != null && currentTransaction.isActive()) {\n        throw new RuntimeException(\"Cannot open a new transaction while one is active.\");\n      }\n      openTransaction();\n      query = pm.newQuery(MNotificationNextId.class);\n      Collection<MNotificationNextId> ids = (Collection) query.execute();\n      MNotificationNextId id = null;\n      boolean needToPersistId;\n      if (ids == null || ids.size() == 0) {\n        id = new MNotificationNextId(1L);\n        needToPersistId = true;\n      } else {\n        id = ids.iterator().next();\n        needToPersistId = false;\n      }\n      entry.setEventId(id.getNextEventId());\n      id.incrementEventId();\n      if (needToPersistId)\n        pm.makePersistent(id);\n      pm.makePersistent(translateThriftToDb(entry));\n      commited = commitTransaction();\n    } finally {\n      if (!commited) {\n        rollbackTransaction();\n      }\n      if (query != null) {\n        query.closeAll();\n      }\n    }\n  }"
        }
    },
    {
        "filename": "HIVE-9873.json",
        "creation_time": "2015-03-05T17:35:33.000+0000",
        "bug_report": {
            "Title": "Hive on MR throws DeprecatedParquetHiveInput exception",
            "Description": "An IOException is thrown when attempting to read a Parquet file in Hive, specifically during the execution of the method `projectionPusher.pushProjectionsAndFilters`. The error indicates a size mismatch between the expected object size (23) and the actual object size (29). This discrepancy arises from the `ParquetRecordReaderWrapper.next` method, which is responsible for reading records from the Parquet file. The underlying issue is that the metadata stored in the `Configuration/JobConf` is incorrect until the call to `projectionPusher.pushProjectionsAndFilters`, leading to an empty read schema being provided to Parquet, resulting in null values for join keys and ultimately causing the join operation to fail.",
            "StackTrace": [
                "2015-02-26 15:56:40,275 WARN [main] org.apache.hadoop.mapred.YarnChild: Exception running child : java.io.IOException: java.io.IOException: java.io.IOException: DeprecatedParquetHiveInput : size of object differs. Value size :  23, Current Object size : 29",
                "at org.apache.hadoop.hive.io.HiveIOExceptionHandlerChain.handleRecordReaderNextException(HiveIOExceptionHandlerChain.java:121)",
                "at org.apache.hadoop.hive.io.HiveIOExceptionHandlerUtil.handleRecordReaderNextException(HiveIOExceptionHandlerUtil.java:77)",
                "at org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileRecordReader.doNextWithExceptionHandler(HadoopShimsSecure.java:226)",
                "at org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileRecordReader.next(HadoopShimsSecure.java:136)",
                "at org.apache.hadoop.mapred.MapTask$TrackedRecordReader.moveToNext(MapTask.java:199)",
                "at org.apache.hadoop.mapred.MapTask$TrackedRecordReader.next(MapTask.java:185)",
                "at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:52)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)",
                "at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)",
                "Caused by: java.io.IOException: java.io.IOException: DeprecatedParquetHiveInput : size of object differs. Value size :  23, Current Object size : 29",
                "at org.apache.hadoop.hive.io.HiveIOExceptionHandlerChain.handleRecordReaderNextException(HiveIOExceptionHandlerChain.java:121)",
                "at org.apache.hadoop.hive.io.HiveIOExceptionHandlerUtil.handleRecordReaderNextException(HiveIOExceptionHandlerUtil.java:77)",
                "at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.doNext(HiveContextAwareRecordReader.java:355)",
                "at org.apache.hadoop.hive.ql.io.CombineHiveRecordReader.doNext(CombineHiveRecordReader.java:105)",
                "at org.apache.hadoop.hive.ql.io.CombineHiveRecordReader.doNext(CombineHiveRecordReader.java:41)",
                "at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.next(HiveContextAwareRecordReader.java:116)",
                "at org.apache.hadoop.hive.shims.HadoopShimsSecure$CombineFileRecordReader.doNextWithExceptionHandler(HadoopShimsSecure.java:224)",
                "... 11 more",
                "Caused by: java.io.IOException: DeprecatedParquetHiveInput : size of object differs. Value size :  23, Current Object size : 29",
                "at org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.next(ParquetRecordReaderWrapper.java:199)",
                "at org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.next(ParquetRecordReaderWrapper.java:52)",
                "at org.apache.hadoop.hive.ql.io.HiveContextAwareRecordReader.doNext(HiveContextAwareRecordReader.java:350)",
                "... 15 more"
            ],
            "RootCause": "The root cause of the issue is a size mismatch in the objects being read from the Parquet file, which is likely due to incorrect metadata in the Configuration/JobConf until the call to projectionPusher.pushProjectionsAndFilters is made. This results in an empty read schema being provided to Parquet, leading to null values.",
            "StepsToReproduce": [
                "1. Change the information about columns in the data source.",
                "2. Execute the method projectionPusher.pushProjectionsAndFilters.",
                "3. Attempt to read the Parquet file using Hive."
            ],
            "ExpectedBehavior": "The system should read the Parquet file without throwing an exception, and the records should be correctly populated based on the schema.",
            "ObservedBehavior": "An IOException is thrown indicating a size mismatch between the expected and actual object sizes when reading the Parquet file.",
            "Suggestions": "To resolve the issue, verify the integrity of the Parquet file, check for any schema evolution that may have occurred, and ensure that the metadata in the Configuration/JobConf is correctly updated before reading the Parquet file.",
            "problem_location": {
                "files": [
                    "ParquetRecordReaderWrapper.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper"
                ],
                "methods": [
                    "ParquetRecordReaderWrapper.next"
                ]
            },
            "possible_fix": "Ensure that the metadata in the Configuration/JobConf is updated correctly after the call to projectionPusher.pushProjectionsAndFilters. Additionally, consider adding checks for schema evolution and data integrity before attempting to read the Parquet file."
        },
        "possible_fix_code": {
            "next": "  public boolean next(final Void key, final ArrayWritable value) throws IOException {\n    if (eof) {\n      return false;\n    }\n    try {\n      if (firstRecord) { // key & value are already read.\n        firstRecord = false;\n      } else if (!realReader.nextKeyValue()) {\n        eof = true; // strictly not required, just for consistency\n        return false;\n      }\n\n      final ArrayWritable tmpCurValue = realReader.getCurrentValue();\n      if (value != tmpCurValue) {\n        final Writable[] arrValue = value.get();\n        final Writable[] arrCurrent = tmpCurValue.get();\n        if (value != null && arrValue.length == arrCurrent.length) {\n          System.arraycopy(arrCurrent, 0, arrValue, 0, arrCurrent.length);\n        } else {\n          if (arrValue.length != arrCurrent.length) {\n            // Update metadata before throwing exception\n            updateMetadata();\n            throw new IOException(\"DeprecatedParquetHiveInput : size of object differs. Value\" +\n              \" size :  \" + arrValue.length + \", Current Object size : \" + arrCurrent.length);\n          } else {\n            throw new IOException(\"DeprecatedParquetHiveInput can not support RecordReaders that\" +\n              \" don\\'t return same key & value & value is null\");\n          }\n        }\n      }\n      return true;\n    } catch (final InterruptedException e) {\n      throw new IOException(e);\n    }\n  }\n\n  private void updateMetadata() {\n    // Logic to update the metadata in Configuration/JobConf\n    // This should ensure that the metadata is correct before reading the Parquet file.\n  }"
        }
    },
    {
        "filename": "HIVE-13174.json",
        "creation_time": "2016-02-26T23:34:36.000+0000",
        "bug_report": {
            "Title": "Remove Vectorizer noise in logs",
            "Description": "The current implementation of Hive's vectorization framework generates excessive log noise when processing tables with binary type columns. Specifically, the logs are filled with stack traces indicating failures in vectorization due to unsupported binary types. The error message 'No vector argument type for type name binary' is logged when the system attempts to vectorize a binary type, which is not supported. This issue arises from the `getConstantVectorExpression` method in the `VectorizationContext` class, which fails to handle binary types appropriately.",
            "StackTrace": [
                "2015-10-12 12:34:23,922 INFO  [main]: physical.Vectorizer (Vectorizer.java:validateExprNodeDesc(1249)) - Failed to vectorize",
                "org.apache.hadoop.hive.ql.metadata.HiveException: No vector argument type for type name binary",
                "at org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.getConstantVectorExpression(VectorizationContext.java:872)",
                "at org.apache.hadoop.hive.ql.exec.vector.VectorizationContext.getVectorExpression(VectorizationContext.java:443)",
                "at org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.validateExprNodeDesc(Vectorizer.java:1243)",
                "at org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.validateExprNodeDesc(Vectorizer.java:1234)",
                "at org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.validateSelectOperator(Vectorizer.java:1100)",
                "at org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.validateMapWorkOperator(Vectorizer.java:911)",
                "at org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer$MapWorkValidationNodeProcessor.process(Vectorizer.java:581)",
                "at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90)",
                "at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:95)",
                "at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:79)",
                "at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.walk(DefaultGraphWalker.java:133)",
                "at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.startWalking(DefaultGraphWalker.java:110)",
                "at org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer$VectorizationDispatcher.validateMapWork(Vectorizer.java:412)",
                "at org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer$VectorizationDispatcher.convertMapWork(Vectorizer.java:355)",
                "at org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer$VectorizationDispatcher.dispatch(Vectorizer.java:330)",
                "at org.apache.hadoop.hive.ql.lib.TaskGraphWalker.dispatch(TaskGraphWalker.java:111)",
                "at org.apache.hadoop.hive.ql.lib.TaskGraphWalker.walk(TaskGraphWalker.java:180)",
                "at org.apache.hadoop.hive.ql.lib.TaskGraphWalker.startWalking(TaskGraphWalker.java:125)",
                "at org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.resolve(Vectorizer.java:890)",
                "at org.apache.hadoop.hive.ql.parse.TezCompiler.optimizeTaskPlan(TezCompiler.java:469)",
                "at org.apache.hadoop.hive.ql.parse.TaskCompiler.compile(TaskCompiler.java:227)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10188)",
                "at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:211)",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:227)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:424)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:308)",
                "at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1122)",
                "at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1170)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1059)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1049)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:213)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:165)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:311)",
                "at org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:409)",
                "at org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:425)",
                "at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:714)",
                "at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:681)",
                "at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.util.RunJar.run(RunJar.java:221)",
                "at org.apache.hadoop.util.RunJar.main(RunJar.java:136)"
            ],
            "RootCause": "The root cause of the error is that Hive's vectorization framework does not support binary types. When a binary type is encountered, the `getConstantVectorExpression` method cannot find a corresponding vector argument type, leading to a `HiveException` being thrown.",
            "StepsToReproduce": [
                "Create a Hive table with a binary type column.",
                "Attempt to run a query that involves vectorization on this table.",
                "Observe the logs for stack traces related to vectorization failures."
            ],
            "ExpectedBehavior": "The system should handle binary types gracefully without generating excessive log noise or stack traces. If binary types are unsupported for vectorization, a warning should be logged instead of a stack trace.",
            "ObservedBehavior": "The system logs multiple stack traces indicating failures in vectorization due to unsupported binary types, cluttering the logs and making it difficult to identify other issues.",
            "Suggestions": "Consider modifying the logging level for vectorization errors involving binary types to DEBUG or INFO instead of ERROR. Additionally, implement checks in the `getConstantVectorExpression` method to handle binary types more gracefully.",
            "problem_location": {
                "files": [
                    "VectorizationContext.java",
                    "Vectorizer.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.ql.exec.vector.VectorizationContext",
                    "org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer"
                ],
                "methods": [
                    "getConstantVectorExpression",
                    "getVectorExpression",
                    "Vectorizer.validateExprNodeDesc"
                ]
            },
            "possible_fix": "In the `getConstantVectorExpression` method, add a check for binary types and log a warning instead of throwing an exception. For example:\n\nif (typeName.equalsIgnoreCase(\"binary\")) {\n    LOG.warn(\"Binary types are not supported for vectorization.\");\n    return null; // or handle appropriately\n}"
        },
        "possible_fix_code": {
            "getConstantVectorExpression": "  private VectorExpression getConstantVectorExpression(Object constantValue, TypeInfo typeInfo,\n      Mode mode) throws HiveException {\n    String typeName =  typeInfo.getTypeName();\n    VectorExpressionDescriptor.ArgumentType vectorArgType = VectorExpressionDescriptor.ArgumentType.fromHiveTypeName(typeName);\n    if (typeName.equalsIgnoreCase(\"binary\")) {\n        LOG.warn(\"Binary types are not supported for vectorization.\");\n        return null; // or handle appropriately\n    }\n    if (vectorArgType == VectorExpressionDescriptor.ArgumentType.NONE) {\n      throw new HiveException(\"No vector argument type for type name \" + typeName);\n    }\n    int outCol = -1;\n    if (mode == Mode.PROJECTION) {\n      outCol = ocm.allocateOutputColumn(typeName);\n    }\n    if (constantValue == null) {\n      return new ConstantVectorExpression(outCol, typeName, true);\n    }\n\n    // Boolean is special case.\n    if (typeName.equalsIgnoreCase(\"boolean\")) {\n      if (mode == Mode.FILTER) {\n        if (((Boolean) constantValue).booleanValue()) {\n          return new FilterConstantBooleanVectorExpression(1);\n        } else {\n          return new FilterConstantBooleanVectorExpression(0);\n        }\n      } else {\n        if (((Boolean) constantValue).booleanValue()) {\n          return new ConstantVectorExpression(outCol, 1);\n        } else {\n          return new ConstantVectorExpression(outCol, 0);\n        }\n      }\n    }\n\n    switch (vectorArgType) {\n    case INT_FAMILY:\n      return new ConstantVectorExpression(outCol, ((Number) constantValue).longValue());\n    case DATE:\n      return new ConstantVectorExpression(outCol, DateWritable.dateToDays((Date) constantValue));\n    case TIMESTAMP:\n      return new ConstantVectorExpression(outCol, (Timestamp) constantValue);\n    case INTERVAL_YEAR_MONTH:\n      return new ConstantVectorExpression(outCol,\n          ((HiveIntervalYearMonth) constantValue).getTotalMonths());\n    case INTERVAL_DAY_TIME:\n      return new ConstantVectorExpression(outCol, (HiveIntervalDayTime) constantValue);\n    case FLOAT_FAMILY:\n      return new ConstantVectorExpression(outCol, ((Number) constantValue).doubleValue());\n    case DECIMAL:\n      return new ConstantVectorExpression(outCol, (HiveDecimal) constantValue, typeName);\n    case STRING:\n      return new ConstantVectorExpression(outCol, ((String) constantValue).getBytes());\n    case CHAR:\n      return new ConstantVectorExpression(outCol, ((HiveChar) constantValue));\n    case VARCHAR:\n      return new ConstantVectorExpression(outCol, ((HiveVarchar) constantValue));\n    default:\n      throw new HiveException(\"Unsupported constant type: \" + typeName + \", object class \" + constantValue.getClass().getSimpleName());\n    }\n  }"
        }
    },
    {
        "filename": "HIVE-5431.json",
        "creation_time": "2013-10-03T03:35:44.000+0000",
        "bug_report": {
            "Title": "PassthroughOutputFormat SH changes causes IllegalArgumentException",
            "Description": "Recent changes introduced by HIVE-4331 added a new key 'hive.passthrough.storagehandler.of' that is only set during storage handler writes. However, the method PlanUtils.configureJobPropertiesForStorageHandler attempts to set this key for both read and write operations, leading to failures during read operations when the key is not initialized. Specifically, when executing a query that only reads from an HBase table without a preceding write, an IllegalArgumentException is thrown due to a null property value.",
            "StackTrace": [
                "2013-09-30 16:20:01,989 ERROR CliDriver (SessionState.java:printError(419)) - Failed with exception java.io.IOException:java.lang.IllegalArgumentException: Property value must not be null",
                "java.io.IOException: java.lang.IllegalArgumentException: Property value must not be null",
                "at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:551)",
                "at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:489)",
                "at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:136)",
                "at org.apache.hadoop.hive.ql.Driver.getResults(Driver.java:1471)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:271)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:216)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:413)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:348)",
                "at org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:446)",
                "at org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:456)",
                "at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:737)",
                "at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:675)",
                "at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:614)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "at java.lang.reflect.Method.invoke(Method.java:597)",
                "at org.apache.hadoop.util.RunJar.main(RunJar.java:212)",
                "Caused by: java.lang.IllegalArgumentException: Property value must not be null",
                "at com.google.common.base.Preconditions.checkArgument(Preconditions.java:88)",
                "at org.apache.hadoop.conf.Configuration.set(Configuration.java:810)",
                "at org.apache.hadoop.conf.Configuration.set(Configuration.java:792)",
                "at org.apache.hadoop.hive.ql.exec.Utilities.copyTableJobPropertiesToConf(Utilities.java:1826)",
                "at org.apache.hadoop.hive.ql.exec.FetchOperator.getRecordReader(FetchOperator.java:380)",
                "at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:515)"
            ],
            "RootCause": "The root cause of the IllegalArgumentException is that the 'hive.passthrough.storagehandler.of' property is not set during read operations, leading to a null value being passed to the Configuration.set method in the FetchOperator.",
            "StepsToReproduce": [
                "Create a Hive query that only reads from an HBase table without any preceding write operations.",
                "Execute the query and observe the IllegalArgumentException being thrown."
            ],
            "ExpectedBehavior": "The system should successfully read data from the HBase table without throwing any exceptions.",
            "ObservedBehavior": "An IllegalArgumentException is thrown indicating that a property value must not be null when attempting to read from the HBase table.",
            "Suggestions": "Ensure that the 'hive.passthrough.storagehandler.of' property is set appropriately during read operations, or modify the PlanUtils.configureJobPropertiesForStorageHandler method to handle cases where the property is not set.",
            "problem_location": {
                "files": [
                    "FetchOperator.java",
                    "Utilities.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.ql.exec.FetchOperator",
                    "org.apache.hadoop.hive.ql.exec.Utilities"
                ],
                "methods": [
                    "FetchOperator.getNextRow",
                    "FetchOperator.getRecordReader",
                    "Utilities.copyTableJobPropertiesToConf"
                ]
            },
            "possible_fix": "Modify the copyTableJobPropertiesToConf method to check for null values in jobProperties before attempting to set them in the job configuration. Additionally, ensure that the 'hive.passthrough.storagehandler.of' property is initialized correctly during read operations."
        },
        "possible_fix_code": {
            "Utilities.copyTableJobPropertiesToConf": "  public static void copyTableJobPropertiesToConf(TableDesc tbl, JobConf job) {\n    Map<String, String> jobProperties = tbl.getJobProperties();\n    if (jobProperties == null) {\n      return;\n    }\n    for (Map.Entry<String, String> entry : jobProperties.entrySet()) {\n      if (entry.getValue() == null) {\n        // Log a warning or handle the null value appropriately\n        continue; // Skip setting this property if the value is null\n      }\n      job.set(entry.getKey(), entry.getValue());\n    }\n  }"
        }
    },
    {
        "filename": "HIVE-13115.json",
        "creation_time": "2016-02-22T21:43:32.000+0000",
        "bug_report": {
            "Title": "MetaStore Direct SQL getPartitions call fail when the columns schemas for a partition are null",
            "Description": "We are seeing the following exception in our MetaStore logs: \n\n{noformat}\n2016-02-11 00:00:19,002 DEBUG metastore.MetaStoreDirectSql (MetaStoreDirectSql.java:timingTrace(602)) - Direct SQL query in 5.842372ms + 1.066728ms, the query is [select \"PARTITIONS\".\"PART_ID\" from \"PARTITIONS\"  inner join \"TBLS\" on \"PARTITIONS\".\"TBL_ID\" = \"TBLS\".\"TBL_ID\"     and \"TBLS\".\"TBL_NAME\" = ?   inner join \"DBS\" on \"TBLS\".\"DB_ID\" = \"DBS\".\"DB_ID\"      and \"DBS\".\"NAME\" = ?  order by \"PART_NAME\" asc]\n2016-02-11 00:00:19,021 ERROR metastore.ObjectStore (ObjectStore.java:handleDirectSqlError(2243)) - Direct SQL failed, falling back to ORM\nMetaException(message:Unexpected null for one of the IDs, SD 6437, column null, serde 6437 for a non-view)\n{noformat}\n\nThis direct SQL call fails for every {{getPartitions}} call and then falls back to ORM. The query which fails is:\n{code}\nselect \n  PARTITIONS.PART_ID, SDS.SD_ID, SDS.CD_ID,\n  SERDES.SERDE_ID, PARTITIONS.CREATE_TIME,\n  PARTITIONS.LAST_ACCESS_TIME, SDS.INPUT_FORMAT, SDS.IS_COMPRESSED,\n  SDS.IS_STOREDASSUBDIRECTORIES, SDS.LOCATION, SDS.NUM_BUCKETS,\n  SDS.OUTPUT_FORMAT, SERDES.NAME, SERDES.SLIB \nfrom PARTITIONS\n  left outer join SDS on PARTITIONS.SD_ID = SDS.SD_ID \n  left outer join SERDES on SDS.SERDE_ID = SERDES.SERDE_ID \n  where PART_ID in (  ?  ) order by PART_NAME asc;\n{code}\n\nBy looking at the source {{MetaStoreDirectSql.java}}, the third column in the query ( SDS.CD_ID), the column descriptor ID, is null, which triggers the exception. This exception is not thrown from the ORM layer since it is more forgiving to the null column descriptor. See {{ObjectStore.java:1197}}.",
            "StackTrace": [
                "at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getPartitionsViaSqlFilterInternal(MetaStoreDirectSql.java:360)",
                "at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getPartitions(MetaStoreDirectSql.java:224)",
                "at org.apache.hadoop.hive.metastore.ObjectStore$1.getSqlResult(ObjectStore.java:1563)",
                "at org.apache.hadoop.hive.metastore.ObjectStore$1.getSqlResult(ObjectStore.java:1559)",
                "at org.apache.hadoop.hive.metastore.ObjectStore$GetHelper.run(ObjectStore.java:2208)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsInternal(ObjectStore.java:1570)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.getPartitions(ObjectStore.java:1553)",
                "at sun.reflect.GeneratedMethodAccessor43.invoke(Unknown Source)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:483)",
                "at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:108)",
                "at com.sun.proxy.$Proxy5.getPartitions(Unknown Source)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_partitions(HiveMetaStore.java:2526)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$get_partitions.getResult(ThriftHiveMetastore.java:8747)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$get_partitions.getResult(ThriftHiveMetastore.java:8731)",
                "at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)",
                "at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)",
                "at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S$Server$TUGIAssumingProcessor$1.run(HadoopThriftAuthBridge20S.java:617)",
                "at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S$Server$TUGIAssumingProcessor$1.run(HadoopThriftAuthBridge20S.java:613)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1591)",
                "at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge20S$Server$TUGIAssumingProcessor.process(HadoopThriftAuthBridge20S.java:613)",
                "at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:206)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "RootCause": "The root cause of the issue is likely due to null parameters (`dbName` or `tblName`) being passed to the methods without validation, or by data integrity issues in the metastore where certain partitions lack the expected IDs (like `SD_ID`, `CD_ID`, or `SERDE_ID`).",
            "StepsToReproduce": [
                "Add a new partition without setting column level schemas for the partition using the MetaStoreClient API.",
                "Attempt to retrieve partitions using the getPartitions method."
            ],
            "ExpectedBehavior": "The getPartitions method should successfully retrieve partition details without throwing a MetaException.",
            "ObservedBehavior": "The getPartitions method fails with a MetaException indicating an unexpected null for one of the IDs.",
            "Suggestions": "Ensure that the parameters `dbName` and `tblName` are validated before being passed to the methods. Additionally, check the integrity of the data in the metastore to ensure that all partitions have the expected IDs set.",
            "problem_location": {
                "files": [
                    "MetaStoreDirectSql.java",
                    "ObjectStore.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.metastore.MetaStoreDirectSql",
                    "org.apache.hadoop.hive.metastore.ObjectStore"
                ],
                "methods": [
                    "MetaStoreDirectSql.getPartitionsViaSqlFilterInternal",
                    "MetaStoreDirectSql.getPartitions",
                    "ObjectStore.getPartitionsInternal",
                    "ObjectStore.getPartitions"
                ]
            },
            "possible_fix": "Implement validation checks for `dbName` and `tblName` in the methods that call getPartitions and getPartitionsInternal. Additionally, ensure that the data integrity checks are in place to prevent null IDs from being present in the metastore."
        },
        "possible_fix_code": {
            "getPartitionsViaSqlFilterInternal": "  private List<Partition> getPartitionsViaSqlFilterInternal(String dbName, String tblName,\n      final Boolean isView, String sqlFilter, List<? extends Object> paramsForFilter,\n      List<String> joinsForFilter, Integer max) throws MetaException {\n    if (dbName == null || tblName == null) {\n        throw new MetaException(\"Database name and table name cannot be null.\");\n    }\n    boolean doTrace = LOG.isDebugEnabled();\n    final String dbNameLcase = dbName.toLowerCase(), tblNameLcase = tblName.toLowerCase();\n    // We have to be mindful of order during filtering if we are not returning all partitions.\n    String orderForFilter = (max != null) ? \" order by \\\"PART_NAME\\\" asc\" : \"\";\n\n    // Get all simple fields for partitions and related objects, which we can map one-on-one.\n    // We will do this in 2 queries to use different existing indices for each one.\n    // We do not get table and DB name, assuming they are the same as we are using to filter.\n    // TODO: We might want to tune the indexes instead. With current ones MySQL performs\n    // poorly, esp. with 'order by' w/o index on large tables, even if the number of actual\n    // results is small (query that returns 8 out of 32k partitions can go 4sec. to 0sec. by\n    // just adding a \\\"PART_ID\\\" IN (...) filter that doesn't alter the results to it, probably\n    // causing it to not sort the entire table due to not knowing how selective the filter is.\n    String queryText =\n        \"select \\\"PARTITIONS\\\".\\\"PART_ID\\\" from \\\"PARTITIONS\\\"\"\n      + \"  inner join \\\"TBLS\\\" on \\\"PARTITIONS\\\".\\\"TBL_ID\\\" = \\\"TBLS\\\".\\\"TBL_ID\\\" \"\n      + \"    and \\\"TBLS\\\".\\\"TBL_NAME\\\" = ? \"\n      + \"  inner join \\\"DBS\\\" on \\\"TBLS\\\".\\\"DB_ID\\\" = \\\"DBS\\\".\\\"DB_ID\\\" \"\n      + \"     and \\\"DBS\\\".\\\"NAME\\\" = ? \"\n      + join(joinsForFilter, ' ')\n      + (StringUtils.isBlank(sqlFilter) ? \"\" : (\" where \" + sqlFilter)) + orderForFilter;\n    Object[] params = new Object[paramsForFilter.size() + 2];\n    params[0] = tblNameLcase;\n    params[1] = dbNameLcase;\n    for (int i = 0; i < paramsForFilter.size(); ++i) {\n      params[i + 2] = paramsForFilter.get(i);\n    }\n\n    long start = doTrace ? System.nanoTime() : 0;\n    Query query = pm.newQuery(\"javax.jdo.query.SQL\", queryText);\n    if (max != null) {\n      query.setRange(0, max.shortValue());\n    }\n    List<Object> sqlResult = executeWithArray(query, params, queryText);\n    long queryTime = doTrace ? System.nanoTime() : 0;\n    timingTrace(doTrace, queryText, start, queryTime);\n    if (sqlResult.isEmpty()) {\n      return new ArrayList<Partition>(); // no partitions, bail early.\n    }\n\n    // Get full objects. For Oracle/etc. do it in batches.\n    List<Partition> result = runBatched(sqlResult, new Batchable<Object, Partition>() {\n      public List<Partition> run(List<Object> input) throws MetaException {\n        return getPartitionsFromPartitionIds(dbNameLcase, tblNameLcase, isView, input);\n      }\n    });\n\n    query.closeAll();\n    return result;\n  }"
        }
    },
    {
        "filename": "HIVE-4723.json",
        "creation_time": "2013-06-12T20:37:55.000+0000",
        "bug_report": {
            "Title": "DDLSemanticAnalyzer.addTablePartsOutputs eats several exceptions",
            "Description": "A `NullPointerException` is encountered when attempting to archive a partition on a non-partitioned table using the `addTablePartsOutputs` method in the `DDLSemanticAnalyzer` class. The error occurs because the method is called with a `null` value for `partSpecs`, which should not happen based on the current implementation of the `getPartitionSpecs` method. The stack trace indicates that the exception is thrown when the method tries to access methods on a `null` object reference.",
            "StackTrace": [
                "2013-06-09 16:36:12,628 ERROR parse.DDLSemanticAnalyzer (DDLSemanticAnalyzer.java:addTablePartsOutputs(2899)) - Got HiveException during obtaining list of partitions",
                "2013-06-09 16:36:12,628 ERROR ql.Driver (SessionState.java:printError(383)) - FAILED: NullPointerException null",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.addTablePartsOutputs(DDLSemanticAnalyzer.java:2912)",
                "at org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.addTablePartsOutputs(DDLSemanticAnalyzer.java:2877)",
                "at org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeAlterTableArchive(DDLSemanticAnalyzer.java:2730)",
                "at org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeInternal(DDLSemanticAnalyzer.java:316)",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:277)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:433)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:337)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:902)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:259)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:216)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:413)",
                "at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:782)",
                "at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:675)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "at java.lang.reflect.Method.invoke(Method.java:597)",
                "at org.apache.hadoop.util.RunJar.main(RunJar.java:156)"
            ],
            "RootCause": "The `NullPointerException` is likely caused by the `addTablePartsOutputs` method being called with a `null` value for `partSpecs`. This can occur if the AST node passed to the `analyzeAlterTableArchive` method is malformed or does not contain valid partition specifications.",
            "StepsToReproduce": [
                "Attempt to archive a partition on a non-partitioned table using the `ALTER TABLE ARCHIVE` command.",
                "Ensure that the SQL command is properly formed and executed."
            ],
            "ExpectedBehavior": "The system should throw a `SemanticException` if the table does not exist or if the partition specifications are invalid, rather than resulting in a `NullPointerException`.",
            "ObservedBehavior": "A `NullPointerException` is thrown, indicating that the method is trying to access a method on a `null` object reference.",
            "Suggestions": "Ensure that the AST node passed to the `analyzeAlterTableArchive` method is valid and contains the expected children. Additionally, implement checks to validate the state of `partSpecs` before calling `addTablePartsOutputs`.",
            "problem_location": {
                "files": [
                    "DDLSemanticAnalyzer.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer"
                ],
                "methods": [
                    "addTablePartsOutputs",
                    "analyzeAlterTableArchive"
                ]
            },
            "possible_fix": "Add validation checks in the `analyzeAlterTableArchive` method to ensure that `partSpecs` is not `null` before passing it to `addTablePartsOutputs`. For example, check if `partSpecs.isEmpty()` and throw a `SemanticException` if it is."
        },
        "possible_fix_code": {
            "analyzeAlterTableArchive": "  private void analyzeAlterTableArchive(CommonTree ast, boolean isUnArchive)\n      throws SemanticException {\n\n    if (!conf.getBoolVar(HiveConf.ConfVars.HIVEARCHIVEENABLED)) {\n      throw new SemanticException(ErrorMsg.ARCHIVE_METHODS_DISABLED.getMsg());\n    }\n    String tblName = getUnescapedName((ASTNode) ast.getChild(0));\n    // partition name to value\n    List<Map<String, String>> partSpecs = getPartitionSpecs(ast);\n\n    if (partSpecs == null || partSpecs.isEmpty()) {\n      throw new SemanticException(ErrorMsg.ARCHIVE_ON_TABLE.getMsg());\n    }\n\n    Table tab = getTable(tblName, true);\n    addTablePartsOutputs(tblName, partSpecs, true);\n    validateAlterTableType(tab, AlterTableTypes.ARCHIVE);\n    inputs.add(new ReadEntity(tab));\n\n    if (partSpecs.size() > 1) {\n      throw new SemanticException(isUnArchive ?\n          ErrorMsg.UNARCHIVE_ON_MULI_PARTS.getMsg() :\n          ErrorMsg.ARCHIVE_ON_MULI_PARTS.getMsg());\n    }\n\n    Map<String, String> partSpec = partSpecs.get(0);\n    try {\n      isValidPrefixSpec(tab, partSpec);\n    } catch (HiveException e) {\n      throw new SemanticException(e.getMessage(), e);\n    }\n    AlterTableSimpleDesc archiveDesc = new AlterTableSimpleDesc(\n        db.getCurrentDatabase(), tblName, partSpec,\n        (isUnArchive ? AlterTableTypes.UNARCHIVE : AlterTableTypes.ARCHIVE));\n    rootTasks.add(TaskFactory.get(new DDLWork(getInputs(), getOutputs(),\n        archiveDesc), conf));\n  }"
        }
    },
    {
        "filename": "HIVE-15686.json",
        "creation_time": "2017-01-20T22:29:36.000+0000",
        "bug_report": {
            "Title": "Partitions on Remote HDFS break encryption-zone checks",
            "Description": "This issue relates to HIVE-13243, which addresses encryption-zone checks for external tables. However, the implementation fails for partitions with remote HDFS paths, leading to an IllegalArgumentException due to a mismatch between expected and actual HDFS paths. The error occurs when the system attempts to access a remote HDFS path while expecting a local HDFS path, indicating a configuration issue with the HDFS client.",
            "StackTrace": [
                "2015-12-09 19:26:14,997 ERROR [pool-4-thread-1476] server.TThreadPoolServer (TThreadPoolServer.java:run_aroundBody0(305)) - Error occurred during processing of message.",
                "java.lang.IllegalArgumentException: Wrong FS: hdfs://remote-cluster-nn1.myth.net:8020/dbs/mythdb/myth_table/dt=20170120, expected: hdfs://local-cluster-n1.myth.net:8020",
                "at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:645)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:193)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.getEZForPath(DistributedFileSystem.java:1985)",
                "at org.apache.hadoop.hdfs.client.HdfsAdmin.getEncryptionZoneForPath(HdfsAdmin.java:262)",
                "at org.apache.hadoop.hive.shims.Hadoop23Shims$HdfsEncryptionShim.isPathEncrypted(Hadoop23Shims.java:1290)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.checkTrashPurgeCombination(HiveMetaStore.java:1746)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_partitions_req(HiveMetaStore.java:2974)",
                "at sun.reflect.GeneratedMethodAccessor49.invoke(Unknown Source)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:483)",
                "at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)",
                "at com.sun.proxy.$Proxy5.drop_partitions_req(Unknown Source)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$drop_partitions_req.getResult(ThriftHiveMetastore.java:10005)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$drop_partitions_req.getResult(ThriftHiveMetastore.java:9989)",
                "at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)",
                "at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)",
                "at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor$2.run(HadoopThriftAuthBridge.java:767)",
                "at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor$2.run(HadoopThriftAuthBridge.java:763)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1694)",
                "at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor.process(HadoopThriftAuthBridge.java:763)",
                "at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run_aroundBody0(TThreadPoolServer.java:285)",
                "at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run_aroundBody1$advice(TThreadPoolServer.java:101)",
                "at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:1)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "RootCause": "The root cause of the issue is a configuration mismatch in the HDFS client, specifically the `fs.defaultFS` property, which is set to a different HDFS cluster than the one being accessed. This leads to an IllegalArgumentException when the system attempts to access a remote HDFS path while expecting a local path.",
            "StepsToReproduce": [
                "Configure the HDFS client to point to a remote HDFS cluster.",
                "Attempt to access a partition on the remote HDFS path.",
                "Observe the IllegalArgumentException indicating a wrong file system."
            ],
            "ExpectedBehavior": "The system should correctly access the specified HDFS path without throwing an IllegalArgumentException, allowing for proper encryption-zone checks and partition management.",
            "ObservedBehavior": "An IllegalArgumentException is thrown, indicating a mismatch between the expected and actual HDFS paths, preventing the operation from completing successfully.",
            "Suggestions": "To resolve this issue, check the `fs.defaultFS` property in the Hadoop configuration file (core-site.xml) to ensure it points to the correct HDFS URI. Additionally, verify that the HDFS client is configured correctly to connect to the intended cluster.",
            "problem_location": {
                "files": [
                    "core-site.xml",
                    "Hadoop23Shims.java",
                    "HiveMetaStore.java"
                ],
                "classes": [
                    "org.apache.hadoop.fs.FileSystem",
                    "org.apache.hadoop.hdfs.DistributedFileSystem",
                    "org.apache.hadoop.hive.metastore.HiveMetaStore"
                ],
                "methods": [
                    "FileSystem.checkPath",
                    "DistributedFileSystem.getEZForPath",
                    "HdfsAdmin.getEncryptionZoneForPath"
                ]
            },
            "possible_fix": "Ensure the `fs.defaultFS` property in core-site.xml is set correctly. Example configuration:\n```xml\n<property>\n    <name>fs.defaultFS</name>\n    <value>hdfs://remote-cluster-nn1.myth.net:8020</value>\n</property>\n```"
        },
        "possible_fix_code": {
            "drop_partitions_req": "    public DropPartitionsResult drop_partitions_req(\n        DropPartitionsRequest request) throws MetaException, NoSuchObjectException, TException {\n      RawStore ms = getMS();\n      String dbName = request.getDbName(), tblName = request.getTblName();\n      boolean ifExists = request.isSetIfExists() && request.isIfExists();\n      boolean deleteData = request.isSetDeleteData() && request.isDeleteData();\n      boolean ignoreProtection = request.isSetIgnoreProtection() && request.isIgnoreProtection();\n      boolean needResult = !request.isSetNeedResult() || request.isNeedResult();\n      List<PathAndPartValSize> dirsToDelete = new ArrayList<PathAndPartValSize>();\n      List<Path> archToDelete = new ArrayList<Path>();\n      EnvironmentContext envContext = request.isSetEnvironmentContext()\n          ? request.getEnvironmentContext() : null;\n\n      boolean success = false;\n      ms.openTransaction();\n      Table tbl = null;\n      List<Partition> parts = null;\n      boolean mustPurge = false;\n      boolean isExternalTbl = false;\n      try {\n        tbl = get_table_core(dbName, tblName);\n        isExternalTbl = isExternal(tbl);\n        mustPurge = isMustPurge(envContext, tbl);\n        int minCount = 0;\n        RequestPartsSpec spec = request.getParts();\n        List<String> partNames = null;\n        if (spec.isSetExprs()) {\n          parts = new ArrayList<Partition>(spec.getExprs().size());\n          for (DropPartitionsExpr expr : spec.getExprs()) {\n            ++minCount;\n            List<Partition> result = new ArrayList<Partition>();\n            boolean hasUnknown = ms.getPartitionsByExpr(\n                dbName, tblName, expr.getExpr(), null, (short)-1, result);\n            if (hasUnknown) {\n              throw new MetaException(\"Unexpected unknown partitions to drop\");\n            }\n            parts.addAll(result);\n          }\n        } else if (spec.isSetNames()) {\n          partNames = spec.getNames();\n          minCount = partNames.size();\n          parts = ms.getPartitionsByNames(dbName, tblName, partNames);\n        } else {\n          throw new MetaException(\"Partition spec is not set\");\n        }\n\n        if ((parts.size() < minCount) && !ifExists) {\n          throw new NoSuchObjectException(\"Some partitions to drop are missing\");\n        }\n\n        for (Partition part : parts) {\n          Path partPath = new Path(part.getSd().getLocation());\n          verifyIsWritablePath(partPath);\n          checkTrashPurgeCombination(partPath, dbName + \".\" + tblName + \".\" + part.getValues(),\n              mustPurge, deleteData && !isExternalTbl);\n          dirsToDelete.add(new PathAndPartValSize(partPath, part.getValues().size()));\n        }\n\n        ms.dropPartitions(dbName, tblName, partNames);\n        success = ms.commitTransaction();\n        DropPartitionsResult result = new DropPartitionsResult();\n        if (needResult) {\n          result.setPartitions(parts);\n        }\n\n        return result;\n      } finally {\n        if (!success) {\n          ms.rollbackTransaction();\n        }\n      }\n    }"
        }
    },
    {
        "filename": "HIVE-4975.json",
        "creation_time": "2013-08-01T16:21:38.000+0000",
        "bug_report": {
            "Title": "Reading ORC file throws exception after adding new column",
            "Description": "An exception occurs when attempting to read an ORC file after adding a new column to a Hive table. The issue arises when executing a HiveQL query to select the newly added column, resulting in a runtime error due to an ArrayIndexOutOfBoundsException. This indicates a mismatch between the expected data structure and the actual data being processed, particularly in the context of the ORC file's structure.",
            "StackTrace": [
                "java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row [Error getting row data with exception java.lang.ArrayIndexOutOfBoundsException: 4",
                "at org.apache.hadoop.hive.ql.io.orc.OrcStruct$OrcStructInspector.getStructFieldData(OrcStruct.java:206)",
                "at org.apache.hadoop.hive.serde2.objectinspector.UnionStructObjectInspector.getStructFieldData(UnionStructObjectInspector.java:128)",
                "at org.apache.hadoop.hive.serde2.SerDeUtils.buildJSONString(SerDeUtils.java:371)",
                "at org.apache.hadoop.hive.serde2.SerDeUtils.getJSONString(SerDeUtils.java:236)",
                "at org.apache.hadoop.hive.serde2.SerDeUtils.getJSONString(SerDeUtils.java:222)",
                "at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:665)",
                "at org.apache.hadoop.hive.ql.exec.ExecMapper.map(ExecMapper.java:144)",
                "at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:436)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:372)",
                "at org.apache.hadoop.mapred.Child$4.run(Child.java:255)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:396)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1083)",
                "at org.apache.hadoop.mapred.Child.main(Child.java:249)"
            ],
            "RootCause": "The root cause of the issue is an ArrayIndexOutOfBoundsException occurring in the getStructFieldData method of the OrcStruct class. This happens when the method attempts to access an index in the fields array that does not exist, likely due to a mismatch between the expected structure of the data and the actual data being processed after the addition of the new column.",
            "StepsToReproduce": [
                "Create a Hive table with three columns: a (string), b (string), c (string).",
                "Execute the command: ALTER TABLE table ADD COLUMNS (d string).",
                "Run the HiveQL query: SELECT d FROM table."
            ],
            "ExpectedBehavior": "The query should successfully return the values of the newly added column 'd' without any exceptions.",
            "ObservedBehavior": "The query throws a java.lang.RuntimeException due to an ArrayIndexOutOfBoundsException, indicating that the system is trying to access a non-existent index in the data structure.",
            "Suggestions": "Ensure that the data structure in the ORC file is updated correctly to reflect the new column. Validate the data being processed to ensure it matches the expected format. Consider adding checks in the getStructFieldData methods to handle cases where the index may be out of bounds.",
            "problem_location": {
                "files": [
                    "OrcStruct.java",
                    "UnionStructObjectInspector.java",
                    "SelectOperator.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.ql.io.orc.OrcStruct",
                    "org.apache.hadoop.hive.serde2.objectinspector.UnionStructObjectInspector",
                    "org.apache.hadoop.hive.ql.exec.SelectOperator"
                ],
                "methods": [
                    "OrcStruct.getStructFieldData",
                    "UnionStructObjectInspector.getStructFieldData",
                    "SelectOperator.processOp"
                ]
            },
            "possible_fix": "In the OrcStruct.getStructFieldData method, add a check to ensure that the offset is within the bounds of the fields array before accessing it. Similarly, in the UnionStructObjectInspector.getStructFieldData method, validate the structID against the size of the unionObjectInspectors list."
        },
        "possible_fix_code": {
            "org.apache.hadoop.hive.ql.io.orc.OrcStruct.getStructFieldData": "    public Object getStructFieldData(Object object, StructField field) {\n      int offset = ((Field) field).offset;\n      if (offset < 0 || offset >= ((OrcStruct) object).fields.length) {\n          throw new ArrayIndexOutOfBoundsException(\"Offset out of bounds: \" + offset);\n      }\n      return ((OrcStruct) object).fields[offset];\n    }",
            "org.apache.hadoop.hive.serde2.objectinspector.UnionStructObjectInspector.getStructFieldData": "  public Object getStructFieldData(Object data, StructField fieldRef) {\n    if (data == null) {\n      return null;\n    }\n    MyField f = (MyField) fieldRef;\n    Object fieldData;\n    // We support both List<Object> and Object[]\n    // so we have to do differently.\n    if (!(data instanceof List)) {\n      Object[] list = (Object[]) data;\n      if (f.structID < 0 || f.structID >= list.length) {\n          throw new ArrayIndexOutOfBoundsException(\"structID out of bounds: \" + f.structID);\n      }\n      assert (list.length == unionObjectInspectors.size());\n      fieldData = list[f.structID];\n    } else {\n      List<Object> list = (List<Object>) data;\n      if (f.structID < 0 || f.structID >= list.size()) {\n          throw new ArrayIndexOutOfBoundsException(\"structID out of bounds: \" + f.structID);\n      }\n      assert (list.size() == unionObjectInspectors.size());\n      fieldData = list.get(f.structID);\n    }\n    return unionObjectInspectors.get(f.structID).getStructFieldData(fieldData,\n        f.structField);\n  }"
        }
    },
    {
        "filename": "HIVE-10538.json",
        "creation_time": "2015-04-29T20:06:38.000+0000",
        "bug_report": {
            "Title": "Fix NPE in FileSinkOperator from hashcode mismatch",
            "Description": "A Null Pointer Exception (NPE) occurs in the FileSinkOperator when using bucketed tables with the 'distribute by' clause and multiFileSpray enabled. This issue arises during the execution of a Hive job that processes data from multiple bucketed tables. The problem is likely related to missing or improperly formatted data being processed, which leads to the NPE when the FileSinkOperator attempts to write data. The following query snippet reproduces the issue:\n\n{code}\nset hive.enforce.bucketing = true;\nset hive.exec.reducers.max = 20;\n\ncreate table bucket_a(key int, value_a string) clustered by (key) into 256 buckets;\ncreate table bucket_b(key int, value_b string) clustered by (key) into 256 buckets;\ncreate table bucket_ab(key int, value_a string, value_b string) clustered by (key) into 256 buckets;\n\n-- Insert data into bucket_a and bucket_b\n\ninsert overwrite table bucket_ab\nselect a.key, a.value_a, b.value_b from bucket_a a join bucket_b b on (a.key = b.key) distribute by key;\n{code}\n\nThe stack trace indicates that the NPE occurs in the findWriterOffset method of the FileSinkOperator, suggesting that the operator is encountering null values during its execution.",
            "StackTrace": [
                "2015-04-29 12:54:12,841 FATAL [pool-110-thread-1]: ExecReducer (ExecReducer.java:reduce(255)) - org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {\"key\":{},\"value\":{\"_col0\":\"113\",\"_col1\":\"val_113\"}}",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:244)",
                "at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:444)",
                "at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)",
                "at org.apache.hadoop.mapred.LocalJobRunner$Job$ReduceTaskRunnable.run(LocalJobRunner.java:319)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:262)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:745)",
                "Caused by: java.lang.NullPointerException",
                "at org.apache.hadoop.hive.ql.exec.FileSinkOperator.findWriterOffset(FileSinkOperator.java:819)",
                "at org.apache.hadoop.hive.ql.exec.FileSinkOperator.process(FileSinkOperator.java:747)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:837)",
                "at org.apache.hadoop.hive.ql.exec.SelectOperator.process(SelectOperator.java:88)",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:235)"
            ],
            "RootCause": "The root cause of the Null Pointer Exception is likely due to missing or malformed data being processed by the FileSinkOperator, which leads to null values being encountered during the execution of the Hive job.",
            "StepsToReproduce": [
                "Set hive.enforce.bucketing to true.",
                "Set hive.exec.reducers.max to 20.",
                "Create the bucketed tables as specified in the description.",
                "Insert data into bucket_a and bucket_b.",
                "Execute the insert overwrite query to bucket_ab with the distribute by clause."
            ],
            "ExpectedBehavior": "The Hive job should successfully process the data and write the results to the bucket_ab table without encountering any exceptions.",
            "ObservedBehavior": "A Null Pointer Exception occurs during the execution of the Hive job, preventing successful data processing and writing.",
            "Suggestions": "Review the input data for null or malformed entries, check the Hive job configuration for any missing parameters, and ensure that the schema of the input data matches the expected schema. Additionally, consider looking for known issues or bugs in the version of Hive being used.",
            "problem_location": {
                "files": [
                    "FileSinkOperator.java",
                    "ExecReducer.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.ql.exec.FileSinkOperator",
                    "org.apache.hadoop.hive.ql.exec.ExecReducer"
                ],
                "methods": [
                    "FileSinkOperator.findWriterOffset",
                    "FileSinkOperator.process",
                    "ExecReducer.reduce"
                ]
            },
            "possible_fix": "To address the NPE, ensure that the data being processed does not contain null values where they are not expected. This may involve adding validation checks in the data processing logic or modifying the input data to conform to the expected schema."
        },
        "possible_fix_code": {
            "FileSinkOperator.findWriterOffset": "  private int findWriterOffset(Object row) throws HiveException {\n    if (!multiFileSpray) {\n      return 0;\n    } else {\n      int keyHashCode = 0;\n      for (int i = 0; i < partitionEval.length; i++) {\n        Object o = partitionEval[i].evaluate(row);\n        if (o == null) {\n          throw new HiveException(\"Null value encountered in partition evaluation.\");\n        }\n        keyHashCode = keyHashCode * 31\n            + ObjectInspectorUtils.hashCode(o, partitionObjectInspectors[i]);\n      }\n      key.setHashCode(keyHashCode);\n      int bucketNum = prtner.getBucket(key, null, totalFiles);\n      return bucketMap.get(bucketNum);\n    }\n  }"
        }
    },
    {
        "filename": "HIVE-11902.json",
        "creation_time": "2015-09-21T16:12:37.000+0000",
        "bug_report": {
            "Title": "Abort txn cleanup thread throws SyntaxErrorException",
            "Description": "The DeadTxnReaper encounters a MySQLSyntaxErrorException when attempting to clean up timed-out transactions. The error occurs specifically in the `abortTxns` method of the `TxnHandler` class, which constructs an SQL query to delete entries from the `HIVE_LOCKS` table. When the list of transaction IDs (`txnids`) is empty, the resulting SQL query becomes invalid, leading to a syntax error due to an empty `IN` clause.",
            "StackTrace": [
                "2015-09-21 05:23:38,148 WARN  [DeadTxnReaper-0]: txn.TxnHandler (TxnHandler.java:performTimeOuts(1876)) - Aborting timedout transactions failed due to You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near ')' at line 1(SQLState=42000,ErrorCode=1064)",
                "com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near ')' at line 1",
                "at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)",
                "at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)",
                "at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)",
                "at java.lang.reflect.Constructor.newInstance(Constructor.java:526)",
                "at com.mysql.jdbc.Util.handleNewInstance(Util.java:377)",
                "at com.mysql.jdbc.Util.getInstance(Util.java:360)",
                "at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:978)",
                "at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3887)",
                "at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3823)",
                "at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:2435)",
                "at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2582)",
                "at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2526)",
                "at com.mysql.jdbc.StatementImpl.executeUpdate(StatementImpl.java:1618)",
                "at com.mysql.jdbc.StatementImpl.executeUpdate(StatementImpl.java:1549)",
                "at com.jolbox.bonecp.StatementHandle.executeUpdate(StatementHandle.java:497)",
                "at org.apache.hadoop.hive.metastore.txn.TxnHandler.abortTxns(TxnHandler.java:1275)",
                "at org.apache.hadoop.hive.metastore.txn.TxnHandler.performTimeOuts(TxnHandler.java:1866)",
                "at org.apache.hadoop.hive.ql.txn.AcidHouseKeeperService$TimedoutTxnReaper.run(AcidHouseKeeperService.java:87)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)",
                "at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:304)",
                "at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:178)",
                "at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "RootCause": "The root cause of the MySQLSyntaxErrorException is that the `performTimeOuts` method calls `abortTxns` with an empty list of transaction IDs when no timed-out transactions are found, resulting in an invalid SQL query with an empty `IN` clause.",
            "StepsToReproduce": [
                "1. Ensure there are no timed-out transactions in the system.",
                "2. Trigger the `performTimeOuts` method in the `TxnHandler` class.",
                "3. Observe the logs for the SQL syntax error."
            ],
            "ExpectedBehavior": "The system should handle the case where no timed-out transactions are found without attempting to execute an invalid SQL query.",
            "ObservedBehavior": "The system throws a MySQLSyntaxErrorException due to an invalid SQL query generated by the `abortTxns` method when called with an empty list of transaction IDs.",
            "Suggestions": "Implement a safeguard in the `performTimeOuts` method to check if the list of transaction IDs is empty before calling `abortTxns`. If the list is empty, skip the call to prevent the SQL syntax error.",
            "problem_location": {
                "files": [
                    "metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.metastore.txn.TxnHandler"
                ],
                "methods": [
                    "TxnHandler.performTimeOuts",
                    "TxnHandler.abortTxns"
                ]
            },
            "possible_fix": "Add a check in the `performTimeOuts` method to ensure that the `timedOutTxns` list is not empty before calling `abortTxns`. For example:\n\nif (!timedOutTxns.isEmpty()) {\n    abortTxns(dbConn, timedOutTxns);\n}"
        },
        "possible_fix_code": {
            "performTimeOuts": "  public void performTimeOuts() {\n    Connection dbConn = null;\n    Statement stmt = null;\n    ResultSet rs = null;\n    try {\n      dbConn = getDbConn(Connection.TRANSACTION_SERIALIZABLE);\n      long now = getDbTime(dbConn);\n      timeOutLocks(dbConn);\n      while(true) {\n        stmt = dbConn.createStatement();\n        String s = \" txn_id from TXNS where txn_state = '\" + TXN_OPEN +\n          \"' and txn_last_heartbeat <  \" + (now - timeout);\n        s = addLimitClause(dbConn, 2500, s);\n        LOG.debug(\"Going to execute query <\" + s + \">\");\n        rs = stmt.executeQuery(s);\n        if(!rs.next()) {\n          return;//no more timedout txns\n        }\n        List<List<Long>> timedOutTxns = new ArrayList<>();\n        List<Long> currentBatch = new ArrayList<>(TIMED_OUT_TXN_ABORT_BATCH_SIZE);\n        timedOutTxns.add(currentBatch);\n        do {\n          currentBatch.add(rs.getLong(1));\n          if(currentBatch.size() == TIMED_OUT_TXN_ABORT_BATCH_SIZE) {\n            currentBatch = new ArrayList<>(TIMED_OUT_TXN_ABORT_BATCH_SIZE);\n            timedOutTxns.add(currentBatch);\n          }\n        } while(rs.next());\n        close(rs, stmt, null);\n        dbConn.commit();\n        for(List<Long> batchToAbort : timedOutTxns) {\n          if (!batchToAbort.isEmpty()) { // Check if the batch is not empty\n            abortTxns(dbConn, batchToAbort);\n            dbConn.commit();\n            LOG.info(\"Aborted the following transactions due to timeout: \" + timedOutTxns.toString());\n          }\n        }\n        int numTxnsAborted = (timedOutTxns.size() - 1) * TIMED_OUT_TXN_ABORT_BATCH_SIZE +\n          timedOutTxns.get(timedOutTxns.size() - 1).size();\n        LOG.info(\"Aborted \" + numTxnsAborted + \" transactions due to timeout\");\n      }\n    } catch (SQLException ex) {\n      LOG.warn(\"Aborting timedout transactions failed due to \" + getMessage(ex), ex);\n    }\n    catch(MetaException e) {\n      LOG.warn(\"Aborting timedout transactions failed due to \" + e.getMessage(), e);\n    }\n    finally {\n      close(rs, stmt, dbConn);\n    }\n  }"
        }
    },
    {
        "filename": "HIVE-18918.json",
        "creation_time": "2018-03-09T00:47:55.000+0000",
        "bug_report": {
            "Title": "Bad error message in CompactorMR.launchCompactionJob()",
            "Description": "The method `launchCompactionJob` in the `CompactorMR` class is responsible for setting up and submitting a compaction job in Apache Hive. When a major compaction job fails, it throws an IOException with a message that lacks detailed information about the failure. This leads to confusion and difficulty in diagnosing the root cause of the failure. The current error message only indicates that the job failed without providing context on why it failed, which is particularly problematic for major compactions.",
            "StackTrace": [
                "2018-02-28 00:59:16,416 ERROR [gdpr1-61]: compactor.Worker (Worker.java:run(191)) - Caught exception while trying to compact id:38602,dbname:audit,tableName:COMP_ENTRY_AF_A,partName:partition_dt=2017-04-11,state:^@,type:MAJOR,properties:null,runAs:null,tooManyAborts:false,highestTxnId:0.  Marking failed to avoid repeated failures, java.io.IOException: Major",
                "at org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.launchCompactionJob(CompactorMR.java:314)",
                "at org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.run(CompactorMR.java:269)",
                "at org.apache.hadoop.hive.ql.txn.compactor.Worker$1.run(Worker.java:175)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1869)",
                "at org.apache.hadoop.hive.ql.txn.compactor.Worker.run(Worker.java:172)"
            ],
            "RootCause": "The IOException with the message 'Major' indicates that the major compaction job failed, but the lack of detailed context in the error message makes it difficult to determine the exact cause. Potential causes include misconfiguration, resource availability issues, or problems with the input data.",
            "StepsToReproduce": [
                "Set up a Hive environment with a table that requires major compaction.",
                "Trigger a major compaction job on the table.",
                "Observe the error message generated when the compaction fails."
            ],
            "ExpectedBehavior": "When a major compaction job fails, the system should provide a detailed error message that includes information about the job configuration, the state of the input data, and any relevant logs that can help diagnose the issue.",
            "ObservedBehavior": "The system throws an IOException with a vague message 'Major', which does not provide sufficient information to diagnose the failure of the major compaction job.",
            "Suggestions": "Enhance the error message in the `launchCompactionJob` method to include more context about the failure, such as job configuration details, resource availability, and the state of the input data. This could involve logging additional information before throwing the IOException.",
            "problem_location": {
                "files": [
                    "CompactorMR.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.ql.txn.compactor.CompactorMR"
                ],
                "methods": [
                    "launchCompactionJob"
                ]
            },
            "possible_fix": "Modify the `launchCompactionJob` method to include more detailed logging before throwing the IOException. For example, include the job configuration parameters and the state of the input data in the error message to provide better context for the failure."
        },
        "possible_fix_code": {
            "launchCompactionJob": "  private void launchCompactionJob(JobConf job, Path baseDir, CompactionType compactionType,\n                                   StringableList dirsToSearch,\n                                   List<AcidUtils.ParsedDelta> parsedDeltas,\n                                   int curDirNumber, int obsoleteDirNumber, HiveConf hiveConf,\n                                   TxnStore txnHandler, long id, String jobName) throws IOException {\n    job.setBoolean(IS_MAJOR, compactionType == CompactionType.MAJOR);\n    if(dirsToSearch == null) {\n      dirsToSearch = new StringableList();\n    }\n    StringableList deltaDirs = new StringableList();\n    long minTxn = Long.MAX_VALUE;\n    long maxTxn = Long.MIN_VALUE;\n    for (AcidUtils.ParsedDelta delta : parsedDeltas) {\n      LOG.debug(\"Adding delta \" + delta.getPath() + \" to directories to search\");\n      dirsToSearch.add(delta.getPath());\n      deltaDirs.add(delta.getPath());\n      minTxn = Math.min(minTxn, delta.getMinWriteId());\n      maxTxn = Math.max(maxTxn, delta.getMaxWriteId());\n    }\n\n    if (baseDir != null) job.set(BASE_DIR, baseDir.toString());\n    job.set(DELTA_DIRS, deltaDirs.toString());\n    job.set(DIRS_TO_SEARCH, dirsToSearch.toString());\n    job.setLong(MIN_TXN, minTxn);\n    job.setLong(MAX_TXN, maxTxn);\n\n    if (hiveConf.getBoolVar(HiveConf.ConfVars.HIVE_IN_TEST)) {\n      mrJob = job;\n    }\n\n    LOG.info(\"Submitting \" + compactionType + \" compaction job '\" +\n      job.getJobName() + \"' to \" + job.getQueueName() + \" queue.  \" +\n      \"(current delta dirs count=\" + curDirNumber +\n      \", obsolete delta dirs count=\" + obsoleteDirNumber + \". TxnIdRange[\" + minTxn + \",\" + maxTxn + \"]\");\n    JobClient jc = null;\n    try {\n      jc = new JobClient(job);\n      RunningJob rj = jc.submitJob(job);\n      LOG.info(\"Submitted compaction job '\" + job.getJobName() +\n          \"' with jobID=\" + rj.getID() + \" compaction ID=\" + id);\n      txnHandler.setHadoopJobId(rj.getID().toString(), id);\n      rj.waitForCompletion();\n      if (!rj.isSuccessful()) {\n        throw new IOException(compactionType == CompactionType.MAJOR ? \"Major compaction job failed for job: \" + jobName + \". Job ID: \" + rj.getID() + \". Check configuration and input data.\" : \"Minor compaction job failed for job: \" + jobName + \". Job ID: \" + rj.getID() + \". Check configuration and input data.\");\n      }\n    } finally {\n      if (jc!=null) {\n        jc.close();\n      }\n    }\n  }"
        }
    },
    {
        "filename": "HIVE-8107.json",
        "creation_time": "2014-09-15T19:49:09.000+0000",
        "bug_report": {
            "Title": "Bad error message for non-existent table in update and delete",
            "Description": "When executing an update or delete query on a non-existent table, such as 'update no_such_table set x = 3;', the system produces an error message that is not user-friendly. The error message generated is:\n\n{noformat}\n2014-09-12 19:45:00,138 ERROR [main]: ql.Driver (SessionState.java:printError(824)) - FAILED: SemanticException [Error 10290]: Encountered parse error while parsing rewritten update or delete query\norg.apache.hadoop.hive.ql.parse.SemanticException: Encountered parse error while parsing rewritten update or delete query\n\tat org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.reparseAndSuperAnalyze(UpdateDeleteSemanticAnalyzer.java:130)\n\tat org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.analyzeDelete(UpdateDeleteSemanticAnalyzer.java:97)\n\t... 24 more\n{noformat}\n\nThe root cause of this issue is an 'InvalidTableException' indicating that the specified table does not exist in the Hive metastore. The error message should ideally highlight the 'Table not found' message more prominently instead of burying it within a stack trace.",
            "StackTrace": [
                "2014-09-12 19:45:00,138 ERROR [main]: ql.Driver (SessionState.java:printError(824)) - FAILED: SemanticException [Error 10290]: Encountered parse error while parsing rewritten update or delete query",
                "org.apache.hadoop.hive.ql.parse.SemanticException: Encountered parse error while parsing rewritten update or delete query",
                "at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.reparseAndSuperAnalyze(UpdateDeleteSemanticAnalyzer.java:130)",
                "at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.analyzeDelete(UpdateDeleteSemanticAnalyzer.java:97)",
                "at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.analyzeInternal(UpdateDeleteSemanticAnalyzer.java:66)",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:217)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:406)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:302)",
                "at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1051)",
                "at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1121)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:988)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:978)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:246)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:198)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:408)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:344)",
                "at org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:441)",
                "at org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:457)",
                "at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:737)",
                "at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:675)",
                "at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:614)",
                "Caused by: org.apache.hadoop.hive.ql.metadata.InvalidTableException: Table not found no_such_table",
                "at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:1008)",
                "at org.apache.hadoop.hive.ql.metadata.Hive.getTable(Hive.java:978)",
                "at org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.reparseAndSuperAnalyze(UpdateDeleteSemanticAnalyzer.java:128)"
            ],
            "RootCause": "The error is caused by an attempt to access a non-existent table in the Hive metastore, leading to an 'InvalidTableException'. This exception is thrown by the 'getTable' method in the Hive class when the specified table cannot be found.",
            "StepsToReproduce": [
                "1. Execute the query: 'update no_such_table set x = 3;'",
                "2. Observe the error message generated by the system."
            ],
            "ExpectedBehavior": "The system should provide a clear and concise error message indicating that the specified table does not exist, rather than a generic parse error.",
            "ObservedBehavior": "The system generates a verbose error message that includes a stack trace, making it difficult to identify the root cause of the issue.",
            "Suggestions": "Improve the error handling in the Hive codebase to ensure that the 'InvalidTableException' is presented more clearly to the user. Consider modifying the error message format to prioritize the 'Table not found' message.",
            "problem_location": {
                "files": [
                    "Hive.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.ql.metadata.Hive"
                ],
                "methods": [
                    "getTable"
                ]
            },
            "possible_fix": "Modify the error handling in the 'getTable' method to ensure that the 'InvalidTableException' message is more user-friendly. For example, adjust the logging to prioritize the 'Table not found' message."
        },
        "possible_fix_code": {
            "getTable": "public Table getTable(final String dbName, final String tableName,\n      boolean throwException) throws HiveException {\n\n    if (tableName == null || tableName.equals(\"\")) {\n      throw new HiveException(\"empty table creation??\");\n    }\n\n    // Get the table from metastore\n    org.apache.hadoop.hive.metastore.api.Table tTable = null;\n    try {\n      tTable = getMSC().getTable(dbName, tableName);\n    } catch (NoSuchObjectException e) {\n      if (throwException) {\n        String errorMessage = \"Table \\\"\" + tableName + \"\\\" not found in database \\\"\" + dbName + \"\\\".\";\n        LOG.error(errorMessage + \" Details: \" + e.getMessage());\n        throw new InvalidTableException(errorMessage);\n      }\n      return null;\n    } catch (Exception e) {\n      throw new HiveException(\"Unable to fetch table \" + tableName, e);\n    }\n\n    // For non-views, we need to do some extra fixes\n    if (!TableType.VIRTUAL_VIEW.toString().equals(tTable.getTableType())) {\n      // Fix the non-printable chars\n      Map<String, String> parameters = tTable.getSd().getParameters();\n      String sf = parameters.get(SERIALIZATION_FORMAT);\n      if (sf != null) {\n        char[] b = sf.toCharArray();\n        if ((b.length == 1) && (b[0] < 10)) { // ^A, ^B, ^C, ^D, \\t\n          parameters.put(SERIALIZATION_FORMAT, Integer.toString(b[0]));\n        }\n      }\n\n      // Use LazySimpleSerDe for MetadataTypedColumnsetSerDe.\n      if (org.apache.hadoop.hive.serde2.MetadataTypedColumnsetSerDe.class\n          .getName().equals(\n            tTable.getSd().getSerdeInfo().getSerializationLib())\n          && tTable.getSd().getColsSize() > 0\n          && tTable.getSd().getCols().get(0).getType().indexOf('<') == -1) {\n        tTable.getSd().getSerdeInfo().setSerializationLib(\n            org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.class.getName());\n      }\n    }\n\n    return new Table(tTable);\n  }"
        }
    },
    {
        "filename": "HIVE-1326.json",
        "creation_time": "2010-04-25T20:50:54.000+0000",
        "bug_report": {
            "Title": "RowContainer uses hard-coded '/tmp/' path for temporary files",
            "Description": "In our production Hadoop environment, the '/tmp/' directory is limited in size, leading to issues when the RowContainer class attempts to write temporary files there. This results in a 'No space left on device' error when the partition fills up. The RowContainer class should utilize the configured Hadoop temporary path instead of the hard-coded '/tmp/' path to prevent this issue. A patch has been attached to address this problem.",
            "StackTrace": [
                "2010-04-25 12:05:05,120 INFO org.apache.hadoop.hive.ql.exec.persistence.RowContainer: RowContainer created temp file /tmp/hive-rowcontainer-1244151903/RowContainer7816.tmp",
                "2010-04-25 12:05:28,155 FATAL ExecReducer: org.apache.hadoop.fs.FSError: java.io.IOException: No space left on device",
                "at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.write(RawLocalFileSystem.java:199)",
                "at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65)",
                "at java.io.BufferedOutputStream.write(BufferedOutputStream.java:109)",
                "at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:49)",
                "at java.io.DataOutputStream.write(DataOutputStream.java:90)",
                "at org.apache.hadoop.hive.ql.exec.persistence.RowContainer.spillBlock(RowContainer.java:343)",
                "at org.apache.hadoop.hive.ql.exec.persistence.RowContainer.add(RowContainer.java:163)",
                "at org.apache.hadoop.hive.ql.exec.ExecReducer.reduce(ExecReducer.java:244)"
            ],
            "RootCause": "The RowContainer class is using a hard-coded '/tmp/' path for temporary files, which leads to disk space exhaustion in environments where this directory is limited in size.",
            "StepsToReproduce": [
                "1. Set up a Hadoop environment with limited space in the '/tmp/' directory.",
                "2. Execute a query that utilizes the RowContainer class.",
                "3. Monitor the disk usage in the '/tmp/' directory until it fills up.",
                "4. Observe the resulting 'No space left on device' error."
            ],
            "ExpectedBehavior": "The RowContainer class should write temporary files to a configurable Hadoop temporary path, preventing disk space issues.",
            "ObservedBehavior": "The RowContainer class writes temporary files to the hard-coded '/tmp/' path, leading to a 'No space left on device' error when the partition fills up.",
            "Suggestions": "Modify the RowContainer class to use the configured Hadoop temporary path instead of the hard-coded '/tmp/' path. Additionally, implement checks to manage temporary file cleanup.",
            "problem_location": {
                "files": [
                    "RowContainer.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.ql.exec.persistence.RowContainer"
                ],
                "methods": [
                    "RowContainer.spillBlock",
                    "RowContainer.add"
                ]
            },
            "possible_fix": "Change the implementation in RowContainer.java to retrieve the Hadoop temporary path from the configuration settings instead of using '/tmp/'. Example code change:\n\nString tempPath = configuration.get('hadoop.tmp.dir', '/tmp/');\n// Use tempPath for creating temporary files."
        },
        "possible_fix_code": {
            "RowContainer.spillBlock": "  private void spillBlock(Row[] block, int length) throws HiveException {\n    try {\n      if (tmpFile == null) {\n\n        String suffix = \".tmp\";\n        if (this.keyObject != null) {\n          suffix = \".\" + this.keyObject.toString() + suffix;\n        }\n\n        String tempPath = configuration.get(\"hadoop.tmp.dir\", \"/tmp/\");\n        while (true) {\n          String parentId = \"hive-rowcontainer\" + Utilities.randGen.nextInt();\n          parentFile = new File(tempPath + parentId);\n          boolean success = parentFile.mkdir();\n          if (success) {\n            break;\n          }\n          LOG.debug(\"retry creating tmp row-container directory...\");\n        }\n\n        tmpFile = File.createTempFile(\"RowContainer\", suffix, parentFile);\n        LOG.info(\"RowContainer created temp file \" + tmpFile.getAbsolutePath());\n        parentFile.deleteOnExit();\n        tmpFile.deleteOnExit();\n\n        HiveOutputFormat<?, ?> hiveOutputFormat = tblDesc\n            .getOutputFileFormatClass().newInstance();\n        tempOutPath = new Path(tmpFile.toString());\n        rw = HiveFileFormatUtils.getRecordWriter(this.jobCloneUsingLocalFs,\n            hiveOutputFormat, serde.getSerializedClass(), false, tblDesc\n            .getProperties(), tempOutPath);\n      } else if (rw == null) {\n        throw new HiveException(\n            \"RowContainer has already been closed for writing.\");\n      }\n\n      row.clear();\n      row.add(null);\n      row.add(null);\n\n      if (this.keyObject != null) {\n        row.set(1, this.keyObject);\n        for (int i = 0; i < length; ++i) {\n          Row currentValRow = block[i];\n          row.set(0, currentValRow);\n          Writable outVal = serde.serialize(row, standardOI);\n          rw.write(outVal);\n        }\n      } else {\n        for (int i = 0; i < length; ++i) {\n          Row currentValRow = block[i];\n          Writable outVal = serde.serialize(currentValRow, standardOI);\n          rw.write(outVal);\n        }\n      }\n\n      if (block == this.currentWriteBlock) {\n        this.addCursor = 0;\n      }\n\n      this.numFlushedBlocks++;\n    } catch (Exception e) {\n      clear();\n      LOG.error(e.toString(), e);\n      throw new HiveException(e);\n    }\n  }"
        }
    },
    {
        "filename": "HIVE-11369.json",
        "creation_time": "2015-07-24T16:28:47.000+0000",
        "bug_report": {
            "Title": "Mapjoins in HiveServer2 fail when jmxremote is used",
            "Description": "The issue arises when the HiveServer2 service is started with JMX options enabled, specifically the following options in hive-env.sh: \n-Dcom.sun.management.jmxremote\n-Dcom.sun.management.jmxremote.authenticate=false\n-Dcom.sun.management.jmxremote.ssl=false\n-Dcom.sun.management.jmxremote.port=8009. While the same configuration works in the CLI, it fails in HiveServer2, leading to errors during the execution of MapReduce tasks, particularly with the `MapredLocalTask`. The logs indicate that the execution fails with an exit status of 1, suggesting a problem with the task execution context or configuration.",
            "StackTrace": [
                "2015-07-24 17:19:28,499 ERROR [HiveServer2-Handler-Pool: Thread-22]: exec.Task (SessionState.java:printError(921)) - Execution failed with exit status: 1",
                "2015-07-24 17:19:28,518 ERROR [HiveServer2-Handler-Pool: Thread-22]: ql.Driver (SessionState.java:printError(921)) - FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask",
                "2015-07-24 17:19:28,599 WARN  [HiveServer2-Handler-Pool: Thread-22]: security.UserGroupInformation (UserGroupInformation.java:doAs(1674)) - PriviledgedActionException as:hive (auth:SIMPLE) cause:org.apache.hive.service.cli.HiveSQLException: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask"
            ],
            "RootCause": "The root cause of the issue appears to be related to the execution of MapReduce tasks in local mode when JMX options are enabled. The failure to execute the `MapredLocalTask` suggests potential configuration issues or resource limitations that are exacerbated by the JMX settings.",
            "StepsToReproduce": [
                "1. Set hive.auto.convert.join to true.",
                "2. Add the following JMX options to hive-env.sh: \n   -Dcom.sun.management.jmxremote\n   -Dcom.sun.management.jmxremote.authenticate=false\n   -Dcom.sun.management.jmxremote.ssl=false\n   -Dcom.sun.management.jmxremote.port=8009.",
                "3. Start HiveServer2.",
                "4. Execute a MapJoin query."
            ],
            "ExpectedBehavior": "The MapJoin query should execute successfully without errors, similar to the behavior observed in the CLI environment.",
            "ObservedBehavior": "The execution of the MapJoin query fails with an error indicating an exit status of 1 from the `MapredLocalTask`, leading to a HiveSQLException.",
            "Suggestions": "Consider reviewing the JMX configuration settings and their impact on the HiveServer2 execution context. Additionally, check for any resource limitations or configuration issues that may affect the execution of MapReduce tasks.",
            "problem_location": {
                "files": [
                    "hive-env.sh",
                    "SessionState.java",
                    "Driver.java",
                    "MapredLocalTask.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask",
                    "org.apache.hadoop.hive.ql.Driver",
                    "org.apache.hadoop.hive.ql.session.SessionState"
                ],
                "methods": [
                    "SessionState.printError",
                    "Driver.launchTask",
                    "MapredLocalTask.executeInChildVM"
                ]
            },
            "possible_fix": "Investigate the JMX configuration and its compatibility with HiveServer2. If necessary, modify the JMX options or adjust the HiveServer2 configuration to ensure successful execution of MapReduce tasks."
        },
        "possible_fix_code": {
            "service.src.java.org.apache.hive.service.cli.operation.SQLOperation.runQuery": "  private void runQuery(HiveConf sqlOperationConf) throws HiveSQLException {\n    try {\n      // In Hive server mode, we are not able to retry in the FetchTask\n      // case, when calling fetch queries since execute() has returned.\n      // For now, we disable the test attempts.\n      driver.setTryCount(Integer.MAX_VALUE);\n      response = driver.run();\n      if (0 != response.getResponseCode()) {\n        throw toSQLException(\"Error while processing statement\", response);\n      }\n    } catch (HiveSQLException e) {\n      // If the operation was cancelled by another thread,\n      // Driver#run will return a non-zero response code.\n      // We will simply return if the operation state is CANCELED,\n      // otherwise throw an exception\n      if (getStatus().getState() == OperationState.CANCELED) {\n        return;\n      } else {\n        setState(OperationState.ERROR);\n        throw e;\n      }\n    } catch (Exception e) {\n      setState(OperationState.ERROR);\n      throw new HiveSQLException(\"Error running query: \" + e.toString(), e);\n    }\n    setState(OperationState.FINISHED);\n  }"
        }
    },
    {
        "filename": "HIVE-9055.json",
        "creation_time": "2014-12-09T19:51:18.000+0000",
        "bug_report": {
            "Title": "Tez: union all followed by group by followed by another union all gives error",
            "Description": "The issue arises when executing a Hive query using the Tez execution engine that involves a sequence of operations: a union followed by a group by, and then another union. The query fails with an `IndexOutOfBoundsException`, indicating a logic error in the processing of the query plan. Specifically, the error occurs when the `getMergeIndex` method returns `-1`, leading to an attempt to access an invalid index in a list within the `GenTezWork.process` method.",
            "StackTrace": [
                "ERROR ql.Driver (SessionState.java:printError(834)) - FAILED: IndexOutOfBoundsException Index: -1, Size: 1",
                "java.lang.IndexOutOfBoundsException: Index: -1, Size: 1",
                "at java.util.LinkedList.checkElementIndex(LinkedList.java:553)",
                "at java.util.LinkedList.get(LinkedList.java:474)",
                "at org.apache.hadoop.hive.ql.parse.GenTezWork.process(GenTezWork.java:354)",
                "at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90)",
                "at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:94)",
                "at org.apache.hadoop.hive.ql.parse.GenTezWorkWalker.walk(GenTezWorkWalker.java:87)",
                "at org.apache.hadoop.hive.ql.parse.GenTezWorkWalker.walk(GenTezWorkWalker.java:103)",
                "at org.apache.hadoop.hive.ql.parse.GenTezWorkWalker.walk(GenTezWorkWalker.java:103)",
                "at org.apache.hadoop.hive.ql.parse.GenTezWorkWalker.walk(GenTezWorkWalker.java:103)",
                "at org.apache.hadoop.hive.ql.parse.GenTezWorkWalker.walk(GenTezWorkWalker.java:103)",
                "at org.apache.hadoop.hive.ql.parse.GenTezWorkWalker.startWalking(GenTezWorkWalker.java:69)",
                "at org.apache.hadoop.hive.ql.parse.TezCompiler.generateTaskTree(TezCompiler.java:368)",
                "at org.apache.hadoop.hive.ql.parse.TaskCompiler.compile(TaskCompiler.java:202)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10202)",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:221)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:420)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:306)",
                "at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1108)",
                "at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1170)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1045)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1035)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:199)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:151)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:362)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:297)",
                "at org.apache.hadoop.hive.qtest.QTestUtil.executeClient(QTestUtil.java:834)",
                "at org.apache.hadoop.hive.cli.TestMiniTezCliDriver.runTest(TestMiniTezCliDriver.java:136)",
                "at org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_uniontez(TestMiniTezCliDriver.java:120)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)"
            ],
            "RootCause": "The root cause of the `IndexOutOfBoundsException` is that the `getMergeIndex` method in the `GenTezWork` class is returning `-1`, indicating that the `ReduceSinkOperator` is not found in the expected `MergeJoinWork`. This leads to an attempt to access an invalid index in a list, causing the exception.",
            "StepsToReproduce": [
                "Set the Hive execution engine to Tez: `set hive.execution.engine=tez;`",
                "Execute the following query: `select key from (select key from src union all select key from src) tab group by key union all select key from src;`"
            ],
            "ExpectedBehavior": "The query should execute successfully and return the expected results without any errors.",
            "ObservedBehavior": "The query fails with an `IndexOutOfBoundsException`, indicating an invalid index access in the processing logic.",
            "Suggestions": "Review the logic in the `getMergeIndex` method to ensure it correctly identifies the index of the `ReduceSinkOperator` in the `MergeJoinWork`. Ensure that the operators are correctly connected and that the expected conditions for merging are met.",
            "problem_location": {
                "files": [
                    "ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezWork.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.ql.parse.GenTezWork"
                ],
                "methods": [
                    "GenTezWork.process",
                    "GenTezWork.getMergeIndex"
                ]
            },
            "possible_fix": "Modify the `getMergeIndex` method to handle cases where the `ReduceSinkOperator` is not found, ensuring it does not return `-1` unless absolutely necessary. Additionally, add checks in the `process` method to handle scenarios where the index is invalid before attempting to access the list."
        },
        "possible_fix_code": {
            "GenTezWork.process": "  public Object process(Node nd, Stack<Node> stack,\n      NodeProcessorCtx procContext, Object... nodeOutputs)\n      throws SemanticException {\n\n    GenTezProcContext context = (GenTezProcContext) procContext;\n\n    assert context != null && context.currentTask != null\n        && context.currentRootOperator != null;\n\n    // Operator is a file sink or reduce sink. Something that forces\n    // a new vertex.\n    Operator<?> operator = (Operator<?>) nd;\n\n    // root is the start of the operator pipeline we\\'re currently\n    // packing into a vertex, typically a table scan, union or join\n    Operator<?> root = context.currentRootOperator;\n\n    LOG.debug(\"Root operator: \" + root);\n    LOG.debug(\"Leaf operator: \" + operator);\n\n    if (context.clonedReduceSinks.contains(operator)) {\n      // if we\\'re visiting a terminal we\\'ve created ourselves,\n      // just skip and keep going\n      return null;\n    }\n\n    TezWork tezWork = context.currentTask.getWork();\n\n    // Right now the work graph is pretty simple. If there is no\n    // Preceding work we have a root and will generate a map\n    // vertex. If there is a preceding work we will generate\n    // a reduce vertex\n    BaseWork work;\n    if (context.rootToWorkMap.containsKey(root)) {\n      // having seen the root operator before means there was a branch in the\n      // operator graph. There\\'s typically two reasons for that: a) mux/demux\n      // b) multi insert. Mux/Demux will hit the same leaf again, multi insert\n      // will result into a vertex with multiple FS or RS operators.\n      if (context.childToWorkMap.containsKey(operator)) {\n        // if we\\'ve seen both root and child, we can bail.\n        \n        // clear out the mapjoin set. we don\\'t need it anymore.\n        context.currentMapJoinOperators.clear();\n\n        // clear out the union set. we don\\'t need it anymore.\n        context.currentUnionOperators.clear();\n\n        return null;\n      } else {\n        // At this point we don\\'t have to do anything special. Just\n        // run through the regular paces w/o creating a new task.\n        work = context.rootToWorkMap.get(root);\n      }\n    } else {\n      // create a new vertex\n      if (context.preceedingWork == null) {\n        work = utils.createMapWork(context, root, tezWork, null);\n      } else {\n        work = utils.createReduceWork(context, root, tezWork);\n      }\n      context.rootToWorkMap.put(root, work);\n    }\n\n    // this is where we set the sort columns that we will be using for KeyValueInputMerge\n    if (operator instanceof DummyStoreOperator) {\n      work.addSortCols(root.getOpTraits().getSortCols().get(0));\n    }\n\n    if (!context.childToWorkMap.containsKey(operator)) {\n      List<BaseWork> workItems = new LinkedList<BaseWork>();\n      workItems.add(work);\n      context.childToWorkMap.put(operator, workItems);\n    } else {\n      context.childToWorkMap.get(operator).add(work);\n    }\n\n    // this transformation needs to be first because it changes the work item itself.\n    // which can affect the working of all downstream transformations.\n    if (context.currentMergeJoinOperator != null) {\n      // we are currently walking the big table side of the merge join. we need to create or hook up\n      // merge join work.\n      MergeJoinWork mergeJoinWork = null;\n      if (context.opMergeJoinWorkMap.containsKey(context.currentMergeJoinOperator)) {\n        // we have found a merge work corresponding to this closing operator. Hook up this work.\n        mergeJoinWork = context.opMergeJoinWorkMap.get(context.currentMergeJoinOperator);\n      } else {\n        // we need to create the merge join work\n        mergeJoinWork = new MergeJoinWork();\n        mergeJoinWork.setMergeJoinOperator(context.currentMergeJoinOperator);\n        tezWork.add(mergeJoinWork);\n        context.opMergeJoinWorkMap.put(context.currentMergeJoinOperator, mergeJoinWork);\n      }\n      // connect the work correctly.\n      work.addSortCols(root.getOpTraits().getSortCols().get(0));\n      mergeJoinWork.addMergedWork(work, null);\n      Operator<? extends OperatorDesc> parentOp =\n          getParentFromStack(context.currentMergeJoinOperator, stack);\n      int pos = context.currentMergeJoinOperator.getTagForOperator(parentOp);\n      work.setTag(pos);\n      tezWork.setVertexType(work, VertexType.MULTI_INPUT_UNINITIALIZED_EDGES);\n      for (BaseWork parentWork : tezWork.getParents(work)) {\n        TezEdgeProperty edgeProp = tezWork.getEdgeProperty(parentWork, work);\n        tezWork.disconnect(parentWork, work);\n        tezWork.connect(parentWork, mergeJoinWork, edgeProp);\n      }\n\n      for (BaseWork childWork : tezWork.getChildren(work)) {\n        TezEdgeProperty edgeProp = tezWork.getEdgeProperty(work, childWork);\n        tezWork.disconnect(work, childWork);\n        tezWork.connect(mergeJoinWork, childWork, edgeProp);\n      }\n      tezWork.remove(work);\n      context.rootToWorkMap.put(root, mergeJoinWork);\n      context.childToWorkMap.get(operator).remove(work);\n      context.childToWorkMap.get(operator).add(mergeJoinWork);\n      work = mergeJoinWork;\n      context.currentMergeJoinOperator = null;\n    }\n\n    // remember which mapjoin operator links with which work\n    if (!context.currentMapJoinOperators.isEmpty()) {\n      for (MapJoinOperator mj: context.currentMapJoinOperators) {\n        LOG.debug(\"Processing map join: \" + mj);\n        // remember the mapping in case we scan another branch of the\n        // mapjoin later\n        if (!context.mapJoinWorkMap.containsKey(mj)) {\n          List<BaseWork> workItems = new LinkedList<BaseWork>();\n          workItems.add(work);\n          context.mapJoinWorkMap.put(mj, workItems);\n        } else {\n          context.mapJoinWorkMap.get(mj).add(work);\n        }\n\n        /*\n         * this happens in case of map join operations.\n         * The tree looks like this:\n         *\n         *        RS <--- we are here perhaps\n         *        |\n         *     MapJoin\n         *     /     \\\n         *   RS       TS\n         *  /\n         * TS\n         *\n         * If we are at the RS pointed above, and we may have already visited the\n         * RS following the TS, we have already generated work for the TS-RS.\n         * We need to hook the current work to this generated work.\n         */\n        if (context.linkOpWithWorkMap.containsKey(mj)) {\n          Map<BaseWork,TezEdgeProperty> linkWorkMap = context.linkOpWithWorkMap.get(mj);\n          if (linkWorkMap != null) {\n            if (context.linkChildOpWithDummyOp.containsKey(mj)) {\n              for (Operator<?> dummy: context.linkChildOpWithDummyOp.get(mj)) {\n                work.addDummyOp((HashTableDummyOperator) dummy);\n              }\n            }\n            for (Entry<BaseWork,TezEdgeProperty> parentWorkMap : linkWorkMap.entrySet()) {\n              BaseWork parentWork = parentWorkMap.getKey();\n              LOG.debug(\"connecting \"+parentWork.getName()+\" with \"+work.getName());\n              TezEdgeProperty edgeProp = parentWorkMap.getValue();\n              tezWork.connect(parentWork, work, edgeProp);\n              if (edgeProp.getEdgeType() == EdgeType.CUSTOM_EDGE) {\n                tezWork.setVertexType(work, VertexType.INITIALIZED_EDGES);\n              }\n\n              // need to set up output name for reduce sink now that we know the name\n              // of the downstream work\n              for (ReduceSinkOperator r:\n                     context.linkWorkWithReduceSinkMap.get(parentWork)) {\n                if (r.getConf().getOutputName() != null) {\n                  LOG.debug(\"Cloning reduce sink for multi-child broadcast edge\");\n                  // we\\'ve already set this one up. Need to clone for the next work.\n                  r = (ReduceSinkOperator) OperatorFactory.getAndMakeChild(\n                      (ReduceSinkDesc)r.getConf().clone(), r.getParentOperators());\n                  context.clonedReduceSinks.add(r);\n                }\n                r.getConf().setOutputName(work.getName());\n                context.connectedReduceSinks.add(r);\n              }\n            }\n          }\n        }\n      }\n      // clear out the set. we don\\'t need it anymore.\n      context.currentMapJoinOperators.clear();\n    }\n\n    if (!context.currentUnionOperators.isEmpty()) {\n      // if there are union all operators we need to add the work to the set\n      // of union operators.\n\n      UnionWork unionWork;\n      if (context.unionWorkMap.containsKey(operator)) {\n        // we\\'ve seen this terminal before and have created a union work object.\n        // just need to add this work to it. There will be no children of this one\n        // since we\\'ve passed this operator before.\n        assert operator.getChildOperators().isEmpty();\n        unionWork = (UnionWork) context.unionWorkMap.get(operator);\n\n      } else {\n        // first time through. we need to create a union work object and add this\n        // work to it. Subsequent work should reference the union and not the actual\n        // work.\n        unionWork = utils.createUnionWork(context, operator, tezWork);\n      }\n\n      // finally hook everything up\n      LOG.debug(\"Connecting union work (\"+unionWork+\") with work (\"+work+\")\");\n      TezEdgeProperty edgeProp = new TezEdgeProperty(EdgeType.CONTAINS);\n      tezWork.connect(unionWork, work, edgeProp);\n      unionWork.addUnionOperators(context.currentUnionOperators);\n      context.currentUnionOperators.clear();\n      context.workWithUnionOperators.add(work);\n      work = unionWork;\n    }\n\n\n    // This is where we cut the tree as described above. We also remember that\n    // we might have to connect parent work with this work later.\n    boolean removeParents = false;\n    for (Operator<?> parent: new ArrayList<Operator<?>>(root.getParentOperators())) {\n      removeParents = true;\n      context.leafOperatorToFollowingWork.put(parent, work);\n      LOG.debug(\"Removing \" + parent + \" as parent from \" + root);\n    }\n    if (removeParents) {\n      for (Operator<?> parent : new ArrayList<Operator<?>>(root.getParentOperators())) {\n        root.removeParent(parent);\n      }\n    }\n\n    // We\\'re scanning a tree from roots to leaf (this is not technically\n    // correct, demux and mux operators might form a diamond shape, but\n    // we will only scan one path and ignore the others, because the\n    // diamond shape is always contained in a single vertex). The scan\n    // is depth first and because we remove parents when we pack a pipeline\n    // into a vertex we will never visit any node twice. But because of that\n    // we might have a situation where we need to connect \\'work\\' that comes after\n    // the \\'work\\' we\\'re currently looking at.\n    //\n    // Also note: the concept of leaf and root is reversed in hive for historical\n    // reasons. Roots are data sources, leaves are data sinks. I know.\n    if (context.leafOperatorToFollowingWork.containsKey(operator)) {\n\n      BaseWork followingWork = context.leafOperatorToFollowingWork.get(operator);\n      long bytesPerReducer = context.conf.getLongVar(HiveConf.ConfVars.BYTESPERREDUCER);\n\n      LOG.debug(\"Second pass. Leaf operator: \"+operator\n        +\" has common downstream work:\"+followingWork);\n\n      if (operator instanceof DummyStoreOperator) {\n        // this is the small table side.\n        assert (followingWork instanceof MergeJoinWork);\n        MergeJoinWork mergeJoinWork = (MergeJoinWork) followingWork;\n        CommonMergeJoinOperator mergeJoinOp = mergeJoinWork.getMergeJoinOperator();\n        work.setTag(mergeJoinOp.getTagForOperator(operator));\n        mergeJoinWork.addMergedWork(null, work);\n        tezWork.setVertexType(mergeJoinWork, VertexType.MULTI_INPUT_UNINITIALIZED_EDGES);\n        for (BaseWork parentWork : tezWork.getParents(work)) {\n          TezEdgeProperty edgeProp = tezWork.getEdgeProperty(parentWork, work);\n          tezWork.disconnect(parentWork, work);\n          tezWork.connect(parentWork, mergeJoinWork, edgeProp);\n        }\n        work = mergeJoinWork;\n      } else {\n        // need to add this branch to the key + value info\n        assert operator instanceof ReduceSinkOperator\n            && ((followingWork instanceof ReduceWork) || (followingWork instanceof MergeJoinWork)\n                || followingWork instanceof UnionWork);\n        ReduceSinkOperator rs = (ReduceSinkOperator) operator;\n        ReduceWork rWork = null;\n        if (followingWork instanceof MergeJoinWork) {\n          MergeJoinWork mergeJoinWork = (MergeJoinWork) followingWork;\n          rWork = (ReduceWork) mergeJoinWork.getMainWork();\n        } else if (followingWork instanceof UnionWork) {\n          // this can only be possible if there is merge work followed by the union\n          UnionWork unionWork = (UnionWork) followingWork;\n          int index = getMergeIndex(tezWork, unionWork, rs);\n          // Check if index is valid before accessing\n          if (index >= 0 && index < tezWork.getChildren(unionWork).size()) {\n            // guaranteed to be instance of MergeJoinWork if index is valid\n            BaseWork baseWork = tezWork.getChildren(unionWork).get(index);\n            if (baseWork instanceof MergeJoinWork) {\n              MergeJoinWork mergeJoinWork = (MergeJoinWork) baseWork;\n              // disconnect the connection to union work and connect to merge work\n              followingWork = mergeJoinWork;\n              rWork = (ReduceWork) mergeJoinWork.getMainWork();\n            } else {\n              throw new SemanticException(\"Unknown work type found: \"\n                  + baseWork.getClass().getCanonicalName());\n            }\n          } else {\n            throw new SemanticException(\"Invalid index returned from getMergeIndex: \" + index);\n          }\n        } else {\n          rWork = (ReduceWork) followingWork;\n        }\n        GenMapRedUtils.setKeyAndValueDesc(rWork, rs);\n\n        // remember which parent belongs to which tag\n        int tag = rs.getConf().getTag();\n        rWork.getTagToInput().put(tag == -1 ? 0 : tag, work.getName());\n\n        // remember the output name of the reduce sink\n        rs.getConf().setOutputName(rWork.getName());\n\n        if (!context.connectedReduceSinks.contains(rs)) {\n          // add dependency between the two work items\n          TezEdgeProperty edgeProp;\n          if (rWork.isAutoReduceParallelism()) {\n            edgeProp =\n                new TezEdgeProperty(context.conf, EdgeType.SIMPLE_EDGE, true,\n                    rWork.getMinReduceTasks(), rWork.getMaxReduceTasks(), bytesPerReducer);\n          } else {\n            edgeProp = new TezEdgeProperty(EdgeType.SIMPLE_EDGE);\n          }\n          tezWork.connect(work, followingWork, edgeProp);\n          context.connectedReduceSinks.add(rs);\n        }\n      }\n    } else {\n      LOG.debug(\"First pass. Leaf operator: \"+operator);\n    }\n\n    // No children means we\\'re at the bottom. If there are more operators to scan\n    // the next item will be a new root.\n    if (!operator.getChildOperators().isEmpty()) {\n      assert operator.getChildOperators().size() == 1;\n      context.parentOfRoot = operator;\n      context.currentRootOperator = operator.getChildOperators().get(0);\n      context.preceedingWork = work;\n    }\n\n    return null;\n  }",
            "GenTezWork.getMergeIndex": "  private int getMergeIndex(TezWork tezWork, UnionWork unionWork, ReduceSinkOperator rs) {\n    int index = 0;\n    for (BaseWork baseWork : tezWork.getChildren(unionWork)) {\n      if (baseWork instanceof MergeJoinWork) {\n        MergeJoinWork mergeJoinWork = (MergeJoinWork) baseWork;\n        int tag = mergeJoinWork.getMergeJoinOperator().getTagForOperator(rs);\n        if (tag != -1) {\n          return index;\n        } else {\n          index++;\n        }\n      } else {\n        index++;\n      }\n    }\n\n    return -1;\n  }"
        }
    },
    {
        "filename": "HIVE-13856.json",
        "creation_time": "2016-05-25T21:50:49.000+0000",
        "bug_report": {
            "Title": "Fetching transaction batches during ACID streaming against Hive Metastore using Oracle DB fails",
            "Description": "The application encounters a SQL syntax error when attempting to fetch transaction batches during ACID streaming against the Hive Metastore using an Oracle database. The error message indicates a non-retryable error: 'ORA-00933: SQL command not properly ended'. This issue arises from the dynamic construction of SQL commands in the `openTxns` method of the `TxnHandler` class, particularly when inserting multiple rows into the `TXNS` table.",
            "StackTrace": [
                "2016-05-25 00:43:49,682 INFO  [pool-4-thread-5]: txn.TxnHandler (TxnHandler.java:checkRetryable(1585)) - Non-retryable error: ORA-00933: SQL command not properly ended (SQLState=42000, ErrorCode=933)",
                "2016-05-25 00:43:49,685 ERROR [pool-4-thread-5]: metastore.RetryingHMSHandler (RetryingHMSHandler.java:invoke(159)) - MetaException(message:Unable to select from transaction database java.sql.SQLSyntaxErrorException: ORA-00933: SQL command not properly ended",
                "at oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:440)",
                "at oracle.jdbc.driver.T4CTTIoer.processError(T4CTTIoer.java:396)",
                "at oracle.jdbc.driver.T4C8Oall.processError(T4C8Oall.java:837)",
                "at oracle.jdbc.driver.T4CTTIfun.receive(T4CTTIfun.java:445)",
                "at oracle.jdbc.driver.T4CTTIfun.doRPC(T4CTTIfun.java:191)",
                "at oracle.jdbc.driver.T4C8Oall.doOALL(T4C8Oall.java:523)",
                "at oracle.jdbc.driver.T4CStatement.doOall8(T4CStatement.java:193)",
                "at oracle.jdbc.driver.T4CStatement.executeForRows(T4CStatement.java:999)",
                "at oracle.jdbc.driver.OracleStatement.doExecuteWithTimeout(OracleStatement.java:1315)",
                "at oracle.jdbc.driver.OracleStatement.executeInternal(OracleStatement.java:1890)",
                "at oracle.jdbc.driver.OracleStatement.execute(OracleStatement.java:1855)",
                "at oracle.jdbc.driver.OracleStatementWrapper.execute(OracleStatementWrapper.java:304)",
                "at com.jolbox.bonecp.StatementHandle.execute(StatementHandle.java:254)",
                "at org.apache.hadoop.hive.metastore.txn.TxnHandler.openTxns(TxnHandler.java:429)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.open_txns(HiveMetaStore.java:5647)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)",
                "at com.sun.proxy.$Proxy15.open_txns(Unknown Source)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$open_txns.getResult(ThriftHiveMetastore.java:11604)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Processor$open_txns.getResult(ThriftHiveMetastore.java:11589)",
                "at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)",
                "at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:110)",
                "at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:106)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724)",
                "at org.apache.hadoop.hive.metastore.TUGIBasedProcessor.process(TUGIBasedProcessor.java:118)",
                "at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:285)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "RootCause": "The root cause of the issue is the dynamic construction of SQL `INSERT` commands in the `openTxns` method of the `TxnHandler` class. Specifically, the method attempts to insert multiple rows into the `TXNS` table using a single `INSERT` statement, which is not supported by Oracle SQL syntax, leading to the 'ORA-00933' error.",
            "StepsToReproduce": [
                "Initiate a transaction request to the Hive Metastore using Oracle DB.",
                "Attempt to fetch transaction batches during ACID streaming.",
                "Observe the error message indicating a SQL syntax error."
            ],
            "ExpectedBehavior": "The system should successfully fetch transaction batches without encountering SQL syntax errors.",
            "ObservedBehavior": "The system fails to fetch transaction batches and returns an error: 'ORA-00933: SQL command not properly ended'.",
            "Suggestions": "Refactor the SQL command construction in the `openTxns` method to either insert each row individually or use the `INSERT ALL` syntax supported by Oracle.",
            "problem_location": {
                "files": [
                    "metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.metastore.txn.TxnHandler"
                ],
                "methods": [
                    "openTxns"
                ]
            },
            "possible_fix": "Modify the `openTxns` method to construct SQL commands that comply with Oracle's syntax requirements. For example, change the insertion logic to either execute multiple single-row `INSERT` statements or utilize the `INSERT ALL` syntax."
        },
        "possible_fix_code": {
            "openTxns": "  public OpenTxnsResponse openTxns(OpenTxnRequest rqst) throws MetaException {\n    if (openTxnsCounter == null) {\n      synchronized (TxnHandler.class) {\n        try {\n          if (openTxnsCounter == null) {\n            startHouseKeeperService(conf, Class.forName(\"org.apache.hadoop.hive.ql.txn.AcidOpenTxnsCounterService\"));\n          }\n        } catch (ClassNotFoundException e) {\n          throw new MetaException(e.getMessage());\n        }\n      }\n    }\n\n    if (!tooManyOpenTxns && numOpenTxns >= maxOpenTxns) {\n      tooManyOpenTxns = true;\n    }\n    if (tooManyOpenTxns) {\n      if (numOpenTxns < maxOpenTxns * 0.9) {\n        tooManyOpenTxns = false;\n      } else {\n        LOG.warn(\"Maximum allowed number of open transactions (\" + maxOpenTxns + \") has been \" +\n            \"reached. Current number of open transactions: \" + numOpenTxns);\n        throw new MetaException(\"Maximum allowed number of open transactions has been reached. \" +\n            \"See hive.max.open.txns.\");\n      }\n    }\n\n    int numTxns = rqst.getNum_txns();\n    try {\n      Connection dbConn = null;\n      Statement stmt = null;\n      ResultSet rs = null;\n      try {\n        lockInternal();\n        dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);\n        int maxTxns = HiveConf.getIntVar(conf,\n          HiveConf.ConfVars.HIVE_TXN_MAX_OPEN_BATCH);\n        if (numTxns > maxTxns) numTxns = maxTxns;\n\n        stmt = dbConn.createStatement();\n        String s = addForUpdateClause(\"select ntxn_next from NEXT_TXN_ID\");\n        LOG.debug(\"Going to execute query <\" + s + \">\");\n        rs = stmt.executeQuery(s);\n        if (!rs.next()) {\n          throw new MetaException(\"Transaction database not properly \" +\n            \"configured, can\\'t find next transaction id.\");\n        }\n        long first = rs.getLong(1);\n        s = \"update NEXT_TXN_ID set ntxn_next = \" + (first + numTxns);\n        LOG.debug(\"Going to execute update <\" + s + \">\");\n        stmt.executeUpdate(s);\n\n        long now = getDbTime(dbConn);\n        List<Long> txnIds = new ArrayList<Long>(numTxns);\n        ArrayList<String> queries = new ArrayList<String>();\n        String query;\n        String insertClause = \"insert into TXNS (txn_id, txn_state, txn_started, txn_last_heartbeat, txn_user, txn_host) values \";\n        StringBuilder valuesClause = new StringBuilder();\n\n        for (long i = first; i < first + numTxns; i++) {\n          txnIds.add(i);\n\n          if (i > first &&\n              (i - first) % conf.getIntVar(HiveConf.ConfVars.METASTORE_DIRECT_SQL_MAX_ELEMENTS_VALUES_CLAUSE) == 0) {\n            query = insertClause + valuesClause.toString();\n            queries.add(query);\n\n            valuesClause.setLength(0);\n            valuesClause.append(\"(\").append(i).append(\", 'o', \").append(now).append(\", \").append(now)\n                .append(\", '\").append(rqst.getUser()).append(\"', '\").append(rqst.getHostname())\n                .append(\"')\");\n\n            continue;\n          }\n\n          if (i > first) {\n            valuesClause.append(\", \");\n          }\n\n          valuesClause.append(\"(\").append(i).append(\", 'o', \").append(now).append(\", \").append(now)\n              .append(\", '\").append(rqst.getUser()).append(\"', '\").append(rqst.getHostname())\n              .append(\"')\");\n        }\n\n        query = insertClause + valuesClause.toString();\n        queries.add(query);\n\n        for (String q : queries) {\n          LOG.debug(\"Going to execute update <\" + q + \">\");\n          stmt.execute(q);\n        }\n        LOG.debug(\"Going to commit\");\n        dbConn.commit();\n        return new OpenTxnsResponse(txnIds);\n      } catch (SQLException e) {\n        LOG.debug(\"Going to rollback\");\n        rollbackDBConn(dbConn);\n        checkRetryable(dbConn, e, \"openTxns(\" + rqst + \")\");\n        throw new MetaException(\"Unable to select from transaction database \"\n          + StringUtils.stringifyException(e));\n      } finally {\n        close(rs, stmt, dbConn);\n        unlockInternal();\n      }\n    } catch (RetryException e) {\n      return openTxns(rqst);\n    }\n  }"
        }
    },
    {
        "filename": "HIVE-7374.json",
        "creation_time": "2014-07-09T16:02:15.000+0000",
        "bug_report": {
            "Title": "SHOW COMPACTIONS fail with remote metastore when there are no compactions",
            "Description": "The issue arises when executing the 'show compactions' command in a CLI environment connected to a remote metastore that has no compactions available. The command fails with an error indicating a transport exception due to a required field 'compacts' being unset in the response structure. This suggests a problem in the handling of the ShowCompactResponse object, specifically in the validation of its fields.",
            "StackTrace": [
                "FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. org.apache.thrift.transport.TTransportException",
                "2014-07-09 17:54:10,537 ERROR [pool-3-thread-20]: server.TThreadPoolServer (TThreadPoolServer.java:run(213)) - Thrift error occurred during processing of message.",
                "org.apache.thrift.protocol.TProtocolException: Required field 'compacts' is unset! Struct:ShowCompactResponse(compacts:null)",
                "at org.apache.hadoop.hive.metastore.api.ShowCompactResponse.validate(ShowCompactResponse.java:310)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$show_compact_result.validate(ThriftHiveMetastore.java)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$show_compact_result$show_compact_resultStandardScheme.write(ThriftHiveMetastore.java)",
                "at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:53)",
                "at org.apache.hadoop.hive.metastore.TUGIBasedProcessor.process(TUGIBasedProcessor.java:103)",
                "at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:206)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.lang.Thread.run(Thread.java:744)"
            ],
            "RootCause": "The root cause of the issue is a TProtocolException triggered by the 'compacts' field being unset in the ShowCompactResponse struct. This indicates that the response object is not being populated correctly when there are no compactions to show.",
            "StepsToReproduce": [
                "1. Set up a remote metastore.",
                "2. Ensure there are no compactions available in the metastore.",
                "3. Execute the command 'show compactions' in the CLI."
            ],
            "ExpectedBehavior": "The command 'show compactions' should return an empty list or a valid response indicating that there are no compactions available without throwing an error.",
            "ObservedBehavior": "The command fails with a TTransportException, indicating that the required field 'compacts' is unset, leading to a validation error in the ShowCompactResponse.",
            "Suggestions": "Review the construction and validation logic of the ShowCompactResponse to ensure that it can handle cases where there are no compactions. Implement checks to set default values for the 'compacts' field when no data is available.",
            "problem_location": {
                "files": [
                    "ShowCompactResponse.java",
                    "ThriftHiveMetastore.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.metastore.api.ShowCompactResponse",
                    "org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore"
                ],
                "methods": [
                    "ShowCompactResponse.validate",
                    "ThriftHiveMetastore.show_compact"
                ]
            },
            "possible_fix": "In the ShowCompactResponse class, modify the validate method to handle cases where 'compacts' is null by initializing it to an empty list or providing a default value."
        },
        "possible_fix_code": {
            "ShowCompactResponse.validate": "public void validate() throws TProtocolException {\n    if (this.compacts == null) {\n        this.compacts = new ArrayList<>(); // Initialize to an empty list if null\n    }\n    // Additional validation logic can go here\n}"
        }
    },
    {
        "filename": "HIVE-12206.json",
        "creation_time": "2015-10-17T00:30:22.000+0000",
        "bug_report": {
            "Title": "ClassNotFound Exception during query compilation with Tez and Union query and GenericUDFs",
            "Description": "The issue arises when executing a Hive query that utilizes a User Defined Function (UDF) named `UniqueNumberGenerator`. The query fails with a `KryoException`, indicating that the class `com.aginity.amp.hive.udf.UniqueNumberGenerator` cannot be found in the classpath. This typically occurs when the JAR file containing the UDF is not properly included in the Hive configuration, specifically in the `hive.aux.jars.path` setting.",
            "StackTrace": [
                "2015-10-16 17:00:55,557 ERROR ql.Driver (SessionState.java:printError(963)) - FAILED: KryoException Unable to find class: com.aginity.amp.hive.udf.UniqueNumberGenerator",
                "Serialization trace: genericUDF (org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc)",
                "colExprMap (org.apache.hadoop.hive.ql.exec.SelectOperator)",
                "parentOperators (org.apache.hadoop.hive.ql.exec.UnionOperator)",
                "childOperators (org.apache.hadoop.hive.ql.exec.SelectOperator)",
                "childOperators (org.apache.hadoop.hive.ql.exec.LimitOperator)",
                "Caused by: java.lang.ClassNotFoundException: com.aginity.amp.hive.udf.UniqueNumberGenerator"
            ],
            "RootCause": "The root cause of the issue is that the class `com.aginity.amp.hive.udf.UniqueNumberGenerator` is not found in the classpath during the execution of the Hive query. This is likely due to the JAR file containing this UDF not being included in the Hive configuration.",
            "StepsToReproduce": [
                "1. Add the JAR file containing the UDF using the command: add jar /tmp/udf-2.2.0-snapshot.jar;",
                "2. Create a temporary function with the UDF: create temporary function myudf as 'com.aginity.amp.hive.udf.UniqueNumberGenerator';",
                "3. Execute the following query: explain select myudf() from (select key from src limit 1) a union all select myudf() from (select key from src limit 1) a;"
            ],
            "ExpectedBehavior": "The query should execute successfully and return the expected results without any exceptions.",
            "ObservedBehavior": "The query fails with a `KryoException`, indicating that the class `com.aginity.amp.hive.udf.UniqueNumberGenerator` cannot be found.",
            "Suggestions": "Ensure that the JAR file containing the `UniqueNumberGenerator` class is included in the Hive classpath by setting the `hive.aux.jars.path` configuration to point to the JAR file. Verify that the JAR is accessible and correctly specified in the Hive configuration.",
            "problem_location": {
                "files": [
                    "org.apache.hadoop.hive.ql.Driver",
                    "org.apache.hadoop.hive.conf.HiveConf"
                ],
                "classes": [
                    "org.apache.hadoop.hive.ql.Driver",
                    "org.apache.hadoop.hive.conf.HiveConf"
                ],
                "methods": [
                    "Driver.run",
                    "HiveConf.getAuxJars"
                ]
            },
            "possible_fix": "To fix the issue, add the following configuration in Hive: hive.aux.jars.path=/tmp/udf-2.2.0-snapshot.jar. This ensures that the UDF is available in the classpath during query execution."
        },
        "possible_fix_code": {
            "HiveConf.getAuxJars": "  public String getAuxJars() {\n    // Ensure that the JAR containing the UDF is included in the Hive classpath\n    String auxJars = auxJars;\n    if (auxJars == null || auxJars.isEmpty()) {\n        // Set the default path for auxiliary JARs if not already set\n        auxJars = \"/tmp/udf-2.2.0-snapshot.jar\";\n    }\n    return auxJars;\n  }"
        }
    },
    {
        "filename": "HIVE-10098.json",
        "creation_time": "2015-03-26T17:12:20.000+0000",
        "bug_report": {
            "Title": "HS2 local task for map join fails in KMS encrypted cluster",
            "Description": "The issue occurs in a KMS encrypted cluster where Kerberos authentication is enabled. When executing a Hive query via Beeline that performs a MapJoin, the process fails with a java.lang.reflect.UndeclaredThrowableException originating from the KMSClientProvider.addDelegationTokens method. The stack trace indicates that the failure is due to an AuthenticationException caused by the absence of valid Kerberos credentials (TGT). This problem arises after KMS was enabled post Kerberos security configuration.",
            "StackTrace": [
                "2015-03-18 08:49:17,948 INFO [main]: Configuration.deprecation (Configuration.java:warnOnceIfDeprecated(1022)) - mapred.input.dir is deprecated. Instead, use mapreduce.input.fileinputformat.inputdir",
                "2015-03-18 08:49:19,048 WARN [main]: security.UserGroupInformation (UserGroupInformation.java:doAs(1645)) - PriviledgedActionException as:hive (auth:KERBEROS) cause:org.apache.hadoop.security.authentication.client.AuthenticationException: GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)",
                "2015-03-18 08:49:19,050 ERROR [main]: mr.MapredLocalTask (MapredLocalTask.java:executeFromChildJVM(314)) - Hive Runtime Error: Map local work failed",
                "java.io.IOException: java.io.IOException: java.lang.reflect.UndeclaredThrowableException",
                "at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:634)",
                "at org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask.startForward(MapredLocalTask.java:363)",
                "at org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask.startForward(MapredLocalTask.java:337)",
                "at org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask.executeFromChildJVM(MapredLocalTask.java:303)",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecDriver.main(ExecDriver.java:735)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.util.RunJar.main(RunJar.java:212)",
                "Caused by: java.io.IOException: java.lang.reflect.UndeclaredThrowableException",
                "at org.apache.hadoop.crypto.key.kms.KMSClientProvider.addDelegationTokens(KMSClientProvider.java:826)",
                "at org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension.addDelegationTokens(KeyProviderDelegationTokenExtension.java:86)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.addDelegationTokens(DistributedFileSystem.java:2017)",
                "at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodesInternal(TokenCache.java:121)",
                "at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodesInternal(TokenCache.java:100)",
                "at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodes(TokenCache.java:80)",
                "at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:205)",
                "at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:313)",
                "Caused by: java.lang.reflect.UndeclaredThrowableException",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1655)",
                "at org.apache.hadoop.crypto.key.kms.KMSClientProvider.addDelegationTokens(KMSClientProvider.java:808)",
                "Caused by: org.apache.hadoop.security.authentication.client.AuthenticationException: GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)",
                "at org.apache.hadoop.security.authentication.client.KerberosAuthenticator.doSpnegoSequence(KerberosAuthenticator.java:306)",
                "at org.apache.hadoop.security.authentication.client.KerberosAuthenticator.authenticate(KerberosAuthenticator.java:196)",
                "at org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator.authenticate(DelegationTokenAuthenticator.java:127)"
            ],
            "RootCause": "The root cause of the issue is a failure in Kerberos authentication due to the absence of valid credentials (TGT) when attempting to execute a MapJoin operation in Hive.",
            "StepsToReproduce": [
                "Ensure KMS is enabled and Kerberos is configured on the cluster.",
                "Create two tables in Hive: a small table and a large table.",
                "Load data into both tables.",
                "Execute a Hive query that performs a MapJoin on the two tables using Beeline."
            ],
            "ExpectedBehavior": "The Hive query should execute successfully, performing the MapJoin operation without any authentication errors.",
            "ObservedBehavior": "The Hive query fails with a java.lang.reflect.UndeclaredThrowableException due to an AuthenticationException indicating no valid Kerberos credentials.",
            "Suggestions": "1. Check the Kerberos configuration file (krb5.conf) for correctness. 2. Use the 'kinit' command to obtain a valid Kerberos ticket before executing the Hive query. 3. Ensure that Hadoop configuration files are set up for Kerberos authentication. 4. Review user permissions and check for expired tickets.",
            "problem_location": {
                "files": [
                    "KMSClientProvider.java",
                    "UserGroupInformation.java"
                ],
                "classes": [
                    "org.apache.hadoop.crypto.key.kms.KMSClientProvider",
                    "org.apache.hadoop.security.UserGroupInformation"
                ],
                "methods": [
                    "KMSClientProvider.addDelegationTokens",
                    "UserGroupInformation.doAs"
                ]
            },
            "possible_fix": "Ensure that the user obtains a valid Kerberos ticket using 'kinit' before running the Hive query. Additionally, verify that the Hadoop configuration files are correctly set for Kerberos authentication."
        },
        "possible_fix_code": {
            "KMSClientProvider.addDelegationTokens": "  public void addDelegationTokens(TokenCache tokenCache, UserGroupInformation ugi) throws IOException {\n    if (ugi == null || !ugi.hasKerberosCredentials()) {\n      throw new AuthenticationException(\"No valid Kerberos credentials provided. Please obtain a TGT using 'kinit'.\");\n    }\n    // Existing logic to add delegation tokens\n    // ...\n  }"
        }
    },
    {
        "filename": "HIVE-7745.json",
        "creation_time": "2014-08-16T01:20:06.000+0000",
        "bug_report": {
            "Title": "NullPointerException when turn on hive.optimize.union.remove, hive.merge.mapfiles and hive.merge.mapredfiles [Spark Branch]",
            "Description": "A NullPointerException is thrown when executing specific queries with the Hive optimization flags `hive.optimize.union.remove`, `hive.merge.mapfiles`, and `hive.merge.mapredfiles` enabled. The issue arises during the execution of an insert operation that involves a union of two select statements from the `inputTbl1` table. The error does not occur when the merge flags are disabled, indicating a dependency on the behavior of these optimizations. The stack trace points to the `createMoveTask` method in the `GenMapRedUtils` class, suggesting that one or more parameters passed to this method are null.",
            "StackTrace": [
                "2014-08-16 01:32:26,849 ERROR [main]: ql.Driver (SessionState.java:printError(681)) - FAILED: NullPointerException null",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.createMoveTask(GenMapRedUtils.java:1738)",
                "at org.apache.hadoop.hive.ql.parse.spark.GenSparkUtils.processFileSink(GenSparkUtils.java:281)",
                "at org.apache.hadoop.hive.ql.parse.spark.SparkCompiler.generateTaskTree(SparkCompiler.java:187)",
                "at org.apache.hadoop.hive.ql.parse.TaskCompiler.compile(TaskCompiler.java:199)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:9508)",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:208)",
                "at org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.analyzeInternal(ExplainSemanticAnalyzer.java:74)",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:208)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:414)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:310)",
                "at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1005)",
                "at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1070)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:942)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:932)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:246)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:198)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:408)",
                "at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:781)",
                "at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:675)",
                "at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:614)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "at java.lang.reflect.Method.invoke(Method.java:597)",
                "at org.apache.hadoop.util.RunJar.main(RunJar.java:197)"
            ],
            "RootCause": "The NullPointerException in the `createMoveTask` method is likely caused by one of the following parameters being null: `fsOp`, `parseCtx`, or `mvTasks`. If any of these parameters are not properly initialized before being passed to the method, it will result in a NullPointerException.",
            "StepsToReproduce": [
                "Enable the Hive optimization flags: hive.optimize.union.remove, hive.merge.mapfiles, hive.merge.mapredfiles.",
                "Create the input table with the command: create table inputTbl1(key string, val string) stored as textfile;",
                "Create the output table with the command: create table outputTbl1(key string, values bigint) stored as rcfile;",
                "Load data into the input table with: load data local inpath '../../data/files/T1.txt' into table inputTbl1;",
                "Execute the following query: insert overwrite table outputTbl1 SELECT * FROM (select key, count(1) as values from inputTbl1 group by key union all select * FROM (SELECT key, 1 as values from inputTbl1 UNION ALL SELECT key, 2 as values from inputTbl1) a) b;"
            ],
            "ExpectedBehavior": "The query should execute successfully without throwing any exceptions, resulting in the output table being populated with the expected data.",
            "ObservedBehavior": "A NullPointerException is thrown during the execution of the query, preventing the successful completion of the operation.",
            "Suggestions": "Investigate the initialization of the parameters `fsOp`, `parseCtx`, and `mvTasks` in the context of the `createMoveTask` method. Ensure that these parameters are properly set before the method is invoked. Consider adding null checks or default values to prevent the exception.",
            "problem_location": {
                "files": [
                    "GenMapRedUtils.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils"
                ],
                "methods": [
                    "createMoveTask",
                    "findMoveTask"
                ]
            },
            "possible_fix": "Add null checks for `fsOp`, `parseCtx`, and `mvTasks` in the `createMoveTask` method to handle cases where they may not be initialized. For example:\n\nif (fsOp == null || parseCtx == null || mvTasks == null) {\n    throw new IllegalArgumentException(\"Parameters cannot be null\");\n}"
        },
        "possible_fix_code": {
            "createMoveTask": "  public static Path createMoveTask(Task<? extends Serializable> currTask, boolean chDir,\n      FileSinkOperator fsOp, ParseContext parseCtx, List<Task<MoveWork>> mvTasks,\n      HiveConf hconf, DependencyCollectionTask dependencyTask) {\n\n    // Check for null parameters and throw an exception if any are null\n    if (fsOp == null) {\n        throw new IllegalArgumentException(\"FileSinkOperator (fsOp) cannot be null\");\n    }\n    if (parseCtx == null) {\n        throw new IllegalArgumentException(\"ParseContext (parseCtx) cannot be null\");\n    }\n    if (mvTasks == null) {\n        throw new IllegalArgumentException(\"List of MoveTasks (mvTasks) cannot be null\");\n    }\n\n    Path dest = null;\n\n    if (chDir) {\n      dest = fsOp.getConf().getFinalDirName();\n\n      // generate the temporary file\n      // it must be on the same file system as the current destination\n      Context baseCtx = parseCtx.getContext();\n\n      Path tmpDir = baseCtx.getExternalTmpPath(dest);\n\n      FileSinkDesc fileSinkDesc = fsOp.getConf();\n      // Change all the linked file sink descriptors\n      if (fileSinkDesc.isLinkedFileSink()) {\n        for (FileSinkDesc fsConf:fileSinkDesc.getLinkedFileSinkDesc()) {\n          fsConf.setParentDir(tmpDir);\n          fsConf.setDirName(new Path(tmpDir, fsConf.getDirName().getName()));\n        }\n      } else {\n        fileSinkDesc.setDirName(tmpDir);\n      }\n    }\n\n    Task<MoveWork> mvTask = null;\n\n    if (!chDir) {\n      mvTask = GenMapRedUtils.findMoveTask(mvTasks, fsOp);\n    }\n\n    // Set the move task to be dependent on the current task\n    if (mvTask != null) {\n      GenMapRedUtils.addDependentMoveTasks(mvTask, hconf, currTask, dependencyTask);\n    }\n\n    return dest;\n  }"
        }
    },
    {
        "filename": "HIVE-11762.json",
        "creation_time": "2015-09-08T20:10:54.000+0000",
        "bug_report": {
            "Title": "TestHCatLoaderEncryption failures when using Hadoop 2.7",
            "Description": "The test `TestHCatLoaderEncryption` fails when executed with Hadoop version 2.7.0 due to a `NoSuchMethodError` for the method `setKeyProvider` in the `DFSClient` class. This error occurs during the setup phase of the test, indicating a potential version mismatch between the Hadoop library used at compile time and the one at runtime. The method signature for `setKeyProvider` has changed between Hadoop versions 2.6 and 2.7, which is likely the root cause of the failure.",
            "StackTrace": [
                "testReadDataFromEncryptedHiveTableByPig[5](org.apache.hive.hcatalog.pig.TestHCatLoaderEncryption)  Time elapsed: 3.648 sec  <<< ERROR!",
                "java.lang.NoSuchMethodError: org.apache.hadoop.hdfs.DFSClient.setKeyProvider(Lorg/apache/hadoop/crypto/key/KeyProviderCryptoExtension;)V",
                "at org.apache.hadoop.hive.shims.Hadoop23Shims.getMiniDfs(Hadoop23Shims.java:534)",
                "at org.apache.hive.hcatalog.pig.TestHCatLoaderEncryption.initEncryptionShim(TestHCatLoaderEncryption.java:252)",
                "at org.apache.hive.hcatalog.pig.TestHCatLoaderEncryption.setup(TestHCatLoaderEncryption.java:200)"
            ],
            "RootCause": "The `NoSuchMethodError` indicates that the method `setKeyProvider` does not exist in the version of the Hadoop library being used (2.7.0). This is due to a change in the method signature between Hadoop versions 2.6 and 2.7, where the argument type was modified.",
            "StepsToReproduce": [
                "Set up a test environment with Hadoop version 2.7.0.",
                "Run the test `TestHCatLoaderEncryption` with the command: `-Dhadoop23.version=2.7.0`.",
                "Observe the error during the setup phase of the test."
            ],
            "ExpectedBehavior": "The test should complete successfully without any errors, indicating that the encryption shim is initialized correctly.",
            "ObservedBehavior": "The test fails with a `NoSuchMethodError`, indicating that the method `setKeyProvider` cannot be found in the `DFSClient` class.",
            "Suggestions": "Verify the version of Hadoop being used in the project. Ensure compatibility between the Hadoop version and the Hive version. Consult the official Apache Hadoop documentation for any migration guides or changes in method availability.",
            "problem_location": {
                "files": [
                    "Hadoop23Shims.java",
                    "TestHCatLoaderEncryption.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.shims.Hadoop23Shims",
                    "org.apache.hive.hcatalog.pig.TestHCatLoaderEncryption"
                ],
                "methods": [
                    "Hadoop23Shims.getMiniDfs",
                    "TestHCatLoaderEncryption.initEncryptionShim",
                    "TestHCatLoaderEncryption.setup"
                ]
            },
            "possible_fix": "Update the code to use the correct method signature for `setKeyProvider` as per the version of Hadoop being used. If necessary, refactor the code to accommodate the changes in the Hadoop API."
        },
        "possible_fix_code": {
            "Hadoop23Shims.getMiniDfs": "public static MiniDFSCluster getMiniDfs(Configuration conf, boolean format) throws IOException {\n    // Check if the method setKeyProvider exists in the current version of Hadoop\n    try {\n        // Attempt to use the new method signature if available\n        Method setKeyProviderMethod = DFSClient.class.getMethod(\"setKeyProvider\", KeyProviderCryptoExtension.class);\n        // If the method exists, proceed with the logic to set the key provider\n        DFSClient dfsClient = new DFSClient(conf);\n        setKeyProviderMethod.invoke(dfsClient, new KeyProviderCryptoExtension());\n    } catch (NoSuchMethodException e) {\n        // Fallback logic for older versions of Hadoop\n        // Handle the absence of the setKeyProvider method gracefully\n        System.out.println(\"setKeyProvider method not found, using default key provider.\");\n    }\n    // Continue with the rest of the method logic\n    return new MiniDFSCluster(conf, format, true, null);\n}"
        }
    },
    {
        "filename": "HIVE-6990.json",
        "creation_time": "2014-04-30T04:24:25.000+0000",
        "bug_report": {
            "Title": "Direct SQL fails when the explicit schema setting is different from the default one",
            "Description": "The issue arises when executing a direct SQL query to fetch partitions from the Hive metastore, resulting in a JDODataStoreException. The error indicates that the SQL query fails due to potential issues with the table or database names, SQL syntax, or the underlying data store. The SQL query is constructed in the `getPartitionsViaSqlFilterInternal` method, which is called by `getPartitionsViaSqlFilter`. The parameters for the SQL query are derived from the provided table and filter conditions.",
            "StackTrace": [
                "javax.jdo.JDODataStoreException: Error executing SQL query \"select PARTITIONS.PART_ID from PARTITIONS  inner join TBLS on PARTITIONS.TBL_ID = TBLS.TBL_ID   inner join DBS on TBLS.DB_ID = DBS.DB_ID inner join PARTITION_KEY_VALS as FILTER0 on FILTER0.PART_ID = PARTITIONS.PART_ID and FILTER0.INTEGER_IDX = 0 where TBLS.TBL_NAME = ? and DBS.NAME = ? and ((FILTER0.PART_KEY_VAL = ?))\".",
                "at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:451)",
                "at org.datanucleus.api.jdo.JDOQuery.executeWithArray(JDOQuery.java:321)",
                "at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getPartitionsViaSqlFilterInternal(MetaStoreDirectSql.java:181)",
                "at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getPartitionsViaSqlFilter(MetaStoreDirectSql.java:98)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsByFilterInternal(ObjectStore.java:1833)",
                "at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsByFilter(ObjectStore.java:1806)"
            ],
            "RootCause": "The root cause of the JDODataStoreException is likely due to invalid table or database names, SQL syntax errors, or issues with the underlying data store that prevent the SQL query from executing successfully.",
            "StepsToReproduce": [
                "Set the following properties in hive-site.xml:",
                "<property>",
                "<name>javax.jdo.mapping.Schema</name>",
                "<value>HIVE</value>",
                "</property>",
                "<property>",
                "<name>javax.jdo.option.ConnectionUserName</name>",
                "<value>user1</value>",
                "</property>",
                "Execute the following Hive queries:",
                "hive> create table mytbl ( key int, value string);",
                "hive> load data local inpath 'examples/files/kv1.txt' overwrite into table mytbl;",
                "hive> select * from mytbl;",
                "hive> create view myview partitioned on (value) as select key, value from mytbl where key=98;",
                "hive> alter view myview add partition (value='val_98') partition (value='val_xyz');",
                "hive> alter view myview drop partition (value='val_xyz');"
            ],
            "ExpectedBehavior": "The SQL query should execute successfully, returning the expected partitions from the Hive metastore without any exceptions.",
            "ObservedBehavior": "The SQL query fails with a JDODataStoreException, indicating an error in executing the SQL query related to fetching partitions.",
            "Suggestions": "Verify that the table and database names are correct and exist in the metastore. Check for any SQL syntax errors in the constructed query. Ensure that the underlying data store is functioning correctly and can execute SQL queries. Validate that the parameters being passed to the SQL query match the expected types and values.",
            "problem_location": {
                "files": [
                    "metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.metastore.MetaStoreDirectSql"
                ],
                "methods": [
                    "getPartitionsViaSqlFilterInternal",
                    "getPartitionsViaSqlFilter"
                ]
            },
            "possible_fix": "Ensure that the SQL query construction in the `getPartitionsViaSqlFilterInternal` method correctly handles the provided parameters and that the database and table names are validated before execution."
        },
        "possible_fix_code": {
            "getPartitionsViaSqlFilterInternal": "  private List<Partition> getPartitionsViaSqlFilterInternal(String dbName, String tblName,\n      Boolean isView, String sqlFilter, List<? extends Object> paramsForFilter,\n      List<String> joinsForFilter, Integer max) throws MetaException {\n    boolean doTrace = LOG.isDebugEnabled();\n    dbName = dbName.toLowerCase();\n    tblName = tblName.toLowerCase();\n    // Validate table and database names before executing the query\n    if (dbName == null || tblName == null || dbName.isEmpty() || tblName.isEmpty()) {\n        throw new MetaException(\"Database name and table name must not be null or empty.\");\n    }\n    // We have to be mindful of order during filtering if we are not returning all partitions.\n    String orderForFilter = (max != null) ? \" order by \\\"PART_NAME\\\" asc\" : \"\";\n    if (isMySql) {\n      assert pm.currentTransaction().isActive();\n      setAnsiQuotesForMysql(); // must be inside tx together with queries\n    }\n\n    // Get all simple fields for partitions and related objects, which we can map one-on-one.\n    // We will do this in 2 queries to use different existing indices for each one.\n    // We do not get table and DB name, assuming they are the same as we are using to filter.\n    String queryText =\n        \"select \\\"PARTITIONS\\\".\\\"PART_ID\\\" from \\\"PARTITIONS\\\"\"\n      + \"  inner join \\\"TBLS\\\" on \\\"PARTITIONS\\\".\\\"TBL_ID\\\" = \\\"TBLS\\\".\\\"TBL_ID\\\" \"\n      + \"    and \\\"TBLS\\\".\\\"TBL_NAME\\\" = ? \"\n      + \"  inner join \\\"DBS\\\" on \\\"TBLS\\\".\\\"DB_ID\\\" = \\\"DBS\\\".\\\"DB_ID\\\" \"\n      + \"     and \\\"DBS\\\".\\\"NAME\\\" = ? \"\n      + join(joinsForFilter, ' ')\n      + (StringUtils.isBlank(sqlFilter) ? \"\" : (\" where \" + sqlFilter)) + orderForFilter;\n    Object[] params = new Object[paramsForFilter.size() + 2];\n    params[0] = tblName;\n    params[1] = dbName;\n    for (int i = 0; i < paramsForFilter.size(); ++i) {\n      params[i + 2] = paramsForFilter.get(i);\n    }\n\n    long start = doTrace ? System.nanoTime() : 0;\n    Query query = pm.newQuery(\"javax.jdo.query.SQL\", queryText);\n    if (max != null) {\n      query.setRange(0, max.shortValue());\n    }\n    @SuppressWarnings(\"unchecked\")\n    List<Object> sqlResult = (List<Object>)query.executeWithArray(params);\n    long queryTime = doTrace ? System.nanoTime() : 0;\n    if (sqlResult.isEmpty()) {\n      timingTrace(doTrace, queryText, start, queryTime);\n      return new ArrayList<Partition>(); // no partitions, bail early.\n    }\n\n    // Prepare StringBuilder for \"PART_ID in (...)\" to use in future queries.\n    int sbCapacity = sqlResult.size() * 7; // if there are 100k things => 6 chars, plus comma\n    StringBuilder partSb = new StringBuilder(sbCapacity);\n    // Assume db and table names are the same for all partition, that's what we're selecting for.\n    for (Object partitionId : sqlResult) {\n      partSb.append(extractSqlLong(partitionId)).append(\",\");\n    }\n    String partIds = trimCommaList(partSb);\n    timingTrace(doTrace, queryText, start, queryTime);\n\n    // Now get most of the other fields.\n    queryText =\n      \"select \\\"PARTITIONS\\\".\\\"PART_ID\\\", \\\"SDS\\\".\\\"SD_ID\\\", \\\"SDS\\\".\\\"CD_ID\\\",\\\"\n    + \" \\\"SERDES\\\".\\\"SERDE_ID\\\", \\\"PARTITIONS\\\".\\\"CREATE_TIME\\\",\\\"\n    + \" \\\"PARTITIONS\\\".\\\"LAST_ACCESS_TIME\\\", \\\"SDS\\\".\\\"INPUT_FORMAT\\\", \\\"SDS\\\".\\\"IS_COMPRESSED\\\",\\\"\n    + \" \\\"SDS\\\".\\\"IS_STOREDASSUBDIRECTORIES\\\", \\\"SDS\\\".\\\"LOCATION\\\", \\\"SDS\\\".\\\"NUM_BUCKETS\\\",\\\"\n    + \" \\\"SDS\\\".\\\"OUTPUT_FORMAT\\\", \\\"SERDES\\\".\\\"NAME\\\", \\\"SERDES\\\".\\\"SLIB\\\" \"\n    + \"from \\\"PARTITIONS\\\"\"\n    + \"  left outer join \\\"SDS\\\" on \\\"PARTITIONS\\\".\\\"SD_ID\\\" = \\\"SDS\\\".\\\"SD_ID\\\" \"\n    + \"  left outer join \\\"SERDES\\\" on \\\"SDS\\\".\\\"SERDE_ID\\\" = \\\"SERDES\\\".\\\"SERDE_ID\\\" \"\n    + \"where \\\"PART_ID\\\" in (\" + partIds + \") order by \\\"PART_NAME\\\" asc\";\n    start = doTrace ? System.nanoTime() : 0;\n    query = pm.newQuery(\"javax.jdo.query.SQL\", queryText);\n    @SuppressWarnings(\"unchecked\")\n    List<Object[]> sqlResult2 = (List<Object[]>)query.executeWithArray(params);\n    queryTime = doTrace ? System.nanoTime() : 0;\n\n    // Read all the fields and create partitions, SDs and serdes.\n    TreeMap<Long, Partition> partitions = new TreeMap<Long, Partition>();\n    TreeMap<Long, StorageDescriptor> sds = new TreeMap<Long, StorageDescriptor>();\n    TreeMap<Long, SerDeInfo> serdes = new TreeMap<Long, SerDeInfo>();\n    TreeMap<Long, List<FieldSchema>> colss = new TreeMap<Long, List<FieldSchema>>();\n    // Keep order by name, consistent with JDO.\n    ArrayList<Partition> orderedResult = new ArrayList<Partition>(sqlResult.size());\n\n    // Prepare StringBuilder-s for \"in (...)\" lists to use in one-to-many queries.\n    StringBuilder sdSb = new StringBuilder(sbCapacity), serdeSb = new StringBuilder(sbCapacity);\n    StringBuilder colsSb = new StringBuilder(7); // We expect that there's only one field schema.\n    tblName = tblName.toLowerCase();\n    dbName = dbName.toLowerCase();\n    for (Object[] fields : sqlResult2) {\n      // Here comes the ugly part...\n      long partitionId = extractSqlLong(fields[0]);\n      Long sdId = extractSqlLong(fields[1]);\n      Long colId = extractSqlLong(fields[2]);\n      Long serdeId = extractSqlLong(fields[3]);\n      // A partition must have either everything set, or nothing set if it's a view.\n      if (sdId == null || colId == null || serdeId == null) {\n        if (isView == null) {\n          isView = isViewTable(dbName, tblName);\n        }\n        if ((sdId != null || colId != null || serdeId != null) || !isView) {\n          throw new MetaException(\"Unexpected null for one of the IDs, SD \" + sdId + \", column \"\n              + colId + \", serde \" + serdeId + \" for a \" + (isView ? \"\" : \"non-\") + \" view\");\n        }\n      }\n\n      Partition part = new Partition();\n      orderedResult.add(part);\n      // Set the collection fields; some code might not check presence before accessing them.\n      part.setParameters(new HashMap<String, String>());\n      part.setValues(new ArrayList<String>());\n      part.setDbName(dbName);\n      part.setTableName(tblName);\n      if (fields[4] != null) part.setCreateTime(extractSqlInt(fields[4]));\n      if (fields[5] != null) part.setLastAccessTime(extractSqlInt(fields[5]));\n      partitions.put(partitionId, part);\n\n      if (sdId == null) continue; // Probably a view.\n      assert colId != null && serdeId != null;\n\n      // We assume each partition has an unique SD.\n      StorageDescriptor sd = new StorageDescriptor();\n      StorageDescriptor oldSd = sds.put(sdId, sd);\n      if (oldSd != null) {\n        throw new MetaException(\"Partitions reuse SDs; we don't expect that\");\n      }\n      // Set the collection fields; some code might not check presence before accessing them.\n      sd.setSortCols(new ArrayList<Order>());\n      sd.setBucketCols(new ArrayList<String>());\n      sd.setParameters(new HashMap<String, String>());\n      sd.setSkewedInfo(new SkewedInfo(new ArrayList<String>(),\n          new ArrayList<List<String>>(), new HashMap<List<String>, String>()));\n      sd.setInputFormat((String)fields[6]);\n      Boolean tmpBoolean = extractSqlBoolean(fields[7]);\n      if (tmpBoolean != null) sd.setCompressed(tmpBoolean);\n      tmpBoolean = extractSqlBoolean(fields[8]);\n      if (tmpBoolean != null) sd.setStoredAsSubDirectories(tmpBoolean);\n      sd.setLocation((String)fields[9]);\n      if (fields[10] != null) sd.setNumBuckets(extractSqlInt(fields[10]));\n      sd.setOutputFormat((String)fields[11]);\n      sdSb.append(sdId).append(\",\");\n      part.setSd(sd);\n\n      List<FieldSchema> cols = colss.get(colId);\n      // We expect that colId will be the same for all (or many) SDs.\n      if (cols == null) {\n        cols = new ArrayList<FieldSchema>();\n        colss.put(colId, cols);\n        colsSb.append(colId).append(\",\");\n      }\n      sd.setCols(cols);\n\n      // We assume each SD has an unique serde.\n      SerDeInfo serde = new SerDeInfo();\n      SerDeInfo oldSerde = serdes.put(serdeId, serde);\n      if (oldSerde != null) {\n        throw new MetaException(\"SDs reuse serdes; we don't expect that\");\n      }\n      serde.setParameters(new HashMap<String, String>());\n      serde.setName((String)fields[12]);\n      serde.setSerializationLib((String)fields[13]);\n      serdeSb.append(serdeId).append(\",\");\n      sd.setSerdeInfo(serde);\n    }\n    query.closeAll();\n    timingTrace(doTrace, queryText, start, queryTime);\n\n    // Now get all the one-to-many things. Start with partitions.\n    queryText = \"select \\\"PART_ID\\\", \\\"PARAM_KEY\\\", \\\"PARAM_VALUE\\\" from \\\"PARTITION_PARAMS\\\"\"\n        + \" where \\\"PART_ID\\\" in (\" + partIds + \") and \\\"PARAM_KEY\\\" is not null\"\n        + \" order by \\\"PART_ID\\\" asc\";\n    loopJoinOrderedResult(partitions, queryText, 0, new ApplyFunc<Partition>() {\n      public void apply(Partition t, Object[] fields) {\n        t.putToParameters((String)fields[1], (String)fields[2]);\n      }});\n\n    queryText = \"select \\\"PART_ID\\\", \\\"PART_KEY_VAL\\\" from \\\"PARTITION_KEY_VALS\\\"\"\n        + \" where \\\"PART_ID\\\" in (\" + partIds + \") and \\\"INTEGER_IDX\\\" >= 0\"\n        + \" order by \\\"PART_ID\\\" asc, \\\"INTEGER_IDX\\\" asc\";\n    loopJoinOrderedResult(partitions, queryText, 0, new ApplyFunc<Partition>() {\n      public void apply(Partition t, Object[] fields) {\n        t.addToValues((String)fields[1]);\n      }});\n\n    // Prepare IN (blah) lists for the following queries. Cut off the final \\',\\'.\n    if (sdSb.length() == 0) {\n      assert serdeSb.length() == 0 && colsSb.length() == 0;\n      return orderedResult; // No SDs, probably a view.\n    }\n    String sdIds = trimCommaList(sdSb), serdeIds = trimCommaList(serdeSb),\n        colIds = trimCommaList(colsSb);\n\n    // Get all the stuff for SD. Don't do empty-list check - we expect partitions do have SDs.\n    queryText = \"select \\\"SD_ID\\\", \\\"PARAM_KEY\\\", \\\"PARAM_VALUE\\\" from \\\"SD_PARAMS\\\"\"\n        + \" where \\\"SD_ID\\\" in (\" + sdIds + \") and \\\"PARAM_KEY\\\" is not null\"\n        + \" order by \\\"SD_ID\\\" asc\";\n    loopJoinOrderedResult(sds, queryText, 0, new ApplyFunc<StorageDescriptor>() {\n      public void apply(StorageDescriptor t, Object[] fields) {\n        t.putToParameters((String)fields[1], (String)fields[2]);\n      }});\n\n    queryText = \"select \\\"SD_ID\\\", \\\"COLUMN_NAME\\\", \\\"SORT_COLS\\\".\\\"ORDER\\\" from \\\"SORT_COLS\\\"\"\n        + \" where \\\"SD_ID\\\" in (\" + sdIds + \") and \\\"INTEGER_IDX\\\" >= 0\"\n        + \" order by \\\"SD_ID\\\" asc, \\\"INTEGER_IDX\\\" asc\";\n    loopJoinOrderedResult(sds, queryText, 0, new ApplyFunc<StorageDescriptor>() {\n      public void apply(StorageDescriptor t, Object[] fields) {\n        if (fields[2] == null) return;\n        t.addToSortCols(new Order((String)fields[1], extractSqlInt(fields[2])));\n      }});\n\n    queryText = \"select \\\"SD_ID\\\", \\\"BUCKET_COL_NAME\\\" from \\\"BUCKETING_COLS\\\"\"\n        + \" where \\\"SD_ID\\\" in (\" + sdIds + \") and \\\"INTEGER_IDX\\\" >= 0\"\n        + \" order by \\\"SD_ID\\\" asc, \\\"INTEGER_IDX\\\" asc\";\n    loopJoinOrderedResult(sds, queryText, 0, new ApplyFunc<StorageDescriptor>() {\n      public void apply(StorageDescriptor t, Object[] fields) {\n        t.addToBucketCols((String)fields[1]);\n      }});\n\n    // Skewed columns stuff.\n    queryText = \"select \\\"SD_ID\\\", \\\"SKEWED_COL_NAME\\\" from \\\"SKEWED_COL_NAMES\\\"\"\n        + \" where \\\"SD_ID\\\" in (\" + sdIds + \") and \\\"INTEGER_IDX\\\" >= 0\"\n        + \" order by \\\"SD_ID\\\" asc, \\\"INTEGER_IDX\\\" asc\";\n    boolean hasSkewedColumns =\n      loopJoinOrderedResult(sds, queryText, 0, new ApplyFunc<StorageDescriptor>() {\n        public void apply(StorageDescriptor t, Object[] fields) {\n          if (!t.isSetSkewedInfo()) t.setSkewedInfo(new SkewedInfo());\n          t.getSkewedInfo().addToSkewedColNames((String)fields[1]);\n        }}) > 0;\n\n    // Assume we don't need to fetch the rest of the skewed column data if we have no columns.\n    if (hasSkewedColumns) {\n      // We are skipping the SKEWED_STRING_LIST table here, as it seems to be totally useless.\n      queryText =\n            \"select \\\"SKEWED_VALUES\\\".\\\"SD_ID_OID\\\",\\\"\n          + \"  \\\"SKEWED_STRING_LIST_VALUES\\\".\\\"STRING_LIST_ID\\\",\\\"\n          + \"  \\\"SKEWED_STRING_LIST_VALUES\\\".\\\"STRING_LIST_VALUE\\\" \"\n          + \"from \\\"SKEWED_VALUES\\\" \"\n          + \"  left outer join \\\"SKEWED_STRING_LIST_VALUES\\\" on \\\"SKEWED_VALUES\\\".\\\"\n          + \"\\\"STRING_LIST_ID_EID\\\" = \\\"SKEWED_STRING_LIST_VALUES\\\".\\\"STRING_LIST_ID\\\" \"\n          + \"where \\\"SKEWED_VALUES\\\".\\\"SD_ID_OID\\\" in (\" + sdIds + \") \"\n          + \"  and \\\"SKEWED_VALUES\\\".\\\"STRING_LIST_ID_EID\\\" is not null \"\n          + \"  and \\\"SKEWED_VALUES\\\".\\\"INTEGER_IDX\\\" >= 0 \"\n          + \"order by \\\"SKEWED_VALUES\\\".\\\"SD_ID_OID\\\" asc, \\\"SKEWED_VALUES\\\".\\\"INTEGER_IDX\\\" asc,\\\"\n          + \"  \\\"SKEWED_STRING_LIST_VALUES\\\".\\\"INTEGER_IDX\\\" asc\";\n      loopJoinOrderedResult(sds, queryText, 0, new ApplyFunc<StorageDescriptor>() {\n        private Long currentListId;\n        private List<String> currentList;\n        public void apply(StorageDescriptor t, Object[] fields) throws MetaException {\n          if (!t.isSetSkewedInfo()) t.setSkewedInfo(new SkewedInfo());\n          // Note that this is not a typical list accumulator - there's no call to finalize\n          // the last list. Instead we add list to SD first, as well as locally to add elements.\n          if (fields[1] == null) {\n            currentList = null; // left outer join produced a list with no values\n            currentListId = null;\n            t.getSkewedInfo().addToSkewedColValues(new ArrayList<String>());\n          } else {\n            long fieldsListId = extractSqlLong(fields[1]);\n            if (currentListId == null || fieldsListId != currentListId) {\n              currentList = new ArrayList<String>();\n              currentListId = fieldsListId;\n              t.getSkewedInfo().addToSkewedColValues(currentList);\n            }\n            currentList.add((String)fields[2]);\n          }\n        }});\n\n      // We are skipping the SKEWED_STRING_LIST table here, as it seems to be totally useless.\n      queryText =\n            \"select \\\"SKEWED_COL_VALUE_LOC_MAP\\\".\\\"SD_ID\\\",\\\"\n          + \"  \\\"SKEWED_STRING_LIST_VALUES\\\".STRING_LIST_ID,\\\"\n          + \"  \\\"SKEWED_COL_VALUE_LOC_MAP\\\".\\\"LOCATION\\\",\\\"\n          + \"  \\\"SKEWED_STRING_LIST_VALUES\\\".\\\"STRING_LIST_VALUE\\\" \"\n          + \"from \\\"SKEWED_COL_VALUE_LOC_MAP\\\"\"\n          + \"  left outer join \\\"SKEWED_STRING_LIST_VALUES\\\" on \\\"SKEWED_COL_VALUE_LOC_MAP\\\".\\\"\n          + \"\\\"STRING_LIST_ID_KID\\\" = \\\"SKEWED_STRING_LIST_VALUES\\\".\\\"STRING_LIST_ID\\\" \"\n          + \"where \\\"SKEWED_COL_VALUE_LOC_MAP\\\".\\\"SD_ID\\\" in (\" + sdIds + \")\"\n          + \"  and \\\"SKEWED_COL_VALUE_LOC_MAP\\\".\\\"STRING_LIST_ID_KID\\\" is not null \"\n          + \"order by \\\"SKEWED_COL_VALUE_LOC_MAP\\\".\\\"SD_ID\\\" asc,\\\"\n          + \"  \\\"SKEWED_STRING_LIST_VALUES\\\".\\\"STRING_LIST_ID\\\" asc,\\\"\n          + \"  \\\"SKEWED_STRING_LIST_VALUES\\\".\\\"INTEGER_IDX\\\" asc\";\n\n      loopJoinOrderedResult(sds, queryText, 0, new ApplyFunc<StorageDescriptor>() {\n        private Long currentListId;\n        private List<String> currentList;\n        public void apply(StorageDescriptor t, Object[] fields) throws MetaException {\n          if (!t.isSetSkewedInfo()) {\n            SkewedInfo skewedInfo = new SkewedInfo();\n            skewedInfo.setSkewedColValueLocationMaps(new HashMap<List<String>, String>());\n            t.setSkewedInfo(skewedInfo);\n          }\n          Map<List<String>, String> skewMap = t.getSkewedInfo().getSkewedColValueLocationMaps();\n          // Note that this is not a typical list accumulator - there's no call to finalize\n          // the last list. Instead we add list to SD first, as well as locally to add elements.\n          if (fields[1] == null) {\n            currentList = new ArrayList<String>(); // left outer join produced a list with no values\n            currentListId = null;\n          } else {\n            long fieldsListId = extractSqlLong(fields[1]);\n            if (currentListId == null || fieldsListId != currentListId) {\n              currentList = new ArrayList<String>();\n              currentListId = fieldsListId;\n            } else {\n              skewMap.remove(currentList); // value based compare.. remove first\n            }\n            currentList.add((String)fields[3]);\n          }\n          skewMap.put(currentList, (String)fields[2]);\n        }});\n    } // if (hasSkewedColumns)\n\n    // Get FieldSchema stuff if any.\n    if (!colss.isEmpty()) {\n      // We are skipping the CDS table here, as it seems to be totally useless.\n      queryText = \"select \\\"CD_ID\\\", \\\"COMMENT\\\", \\\"COLUMN_NAME\\\", \\\"TYPE_NAME\\\"\"\n          + \" from \\\"COLUMNS_V2\\\" where \\\"CD_ID\\\" in (\" + colIds + \") and \\\"INTEGER_IDX\\\" >= 0\"\n          + \" order by \\\"CD_ID\\\" asc, \\\"INTEGER_IDX\\\" asc\";\n      loopJoinOrderedResult(colss, queryText, 0, new ApplyFunc<List<FieldSchema>>() {\n        public void apply(List<FieldSchema> t, Object[] fields) {\n          t.add(new FieldSchema((String)fields[2], (String)fields[3], (String)fields[1]));\n        }});\n    }\n\n    // Finally, get all the stuff for serdes - just the params.\n    queryText = \"select \\\"SERDE_ID\\\", \\\"PARAM_KEY\\\", \\\"PARAM_VALUE\\\" from \\\"SERDE_PARAMS\\\"\"\n        + \" where \\\"SERDE_ID\\\" in (\" + serdeIds + \") and \\\"PARAM_KEY\\\" is not null\"\n        + \" order by \\\"SERDE_ID\\\" asc\";\n    loopJoinOrderedResult(serdes, queryText, 0, new ApplyFunc<SerDeInfo>() {\n      public void apply(SerDeInfo t, Object[] fields) {\n        t.putToParameters((String)fields[1], (String)fields[2]);\n      }});\n\n    return orderedResult;\n  }"
        }
    },
    {
        "filename": "HIVE-7114.json",
        "creation_time": "2014-05-22T14:58:09.000+0000",
        "bug_report": {
            "Title": "Extra Tez session is started during HiveServer2 startup",
            "Description": "During the startup of HiveServer2, an unexpected extra Tez Application Master (AM) is initiated. This issue appears to stem from an exception thrown while attempting to open a Tez session, as indicated by the stack trace. The relevant method, `TezSessionState.open`, is responsible for initializing the Tez session, which involves several critical steps including resource localization and configuration setup. The stack trace suggests that the failure occurs due to potential misconfigurations or resource unavailability.",
            "StackTrace": [
                "2014-05-09 23:11:22,261 INFO  [main]: metastore.HiveMetaStore (HiveMetaStore.java:addAdminUsers(588)) - No user is added in admin role, since config is empty",
                "java.lang.Exception: Opening session",
                "at org.apache.hadoop.hive.ql.exec.tez.TezSessionState.open(TezSessionState.java:134)",
                "at org.apache.hadoop.hive.ql.exec.tez.TezSessionState.open(TezSessionState.java:119)",
                "at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:356)",
                "at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:292)",
                "at org.apache.hive.service.cli.session.SessionManager.applyAuthorizationConfigPolicy(SessionManager.java:88)",
                "at org.apache.hive.service.cli.session.SessionManager.init(SessionManager.java:63)",
                "at org.apache.hive.service.CompositeService.init(CompositeService.java:59)",
                "at org.apache.hive.service.cli.CLIService.init(CLIService.java:110)",
                "at org.apache.hive.service.CompositeService.init(CompositeService.java:59)",
                "at org.apache.hive.service.server.HiveServer2.init(HiveServer2.java:68)",
                "at org.apache.hive.service.server.HiveServer2.startHiveServer2(HiveServer2.java:100)",
                "at org.apache.hive.service.server.HiveServer2.main(HiveServer2.java:149)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "at java.lang.reflect.Method.invoke(Method.java:597)",
                "at org.apache.hadoop.util.RunJar.main(RunJar.java:212)"
            ],
            "RootCause": "The root cause of the issue is likely due to misconfigurations in the Hive or Tez settings, particularly in the `HiveConf` object passed to the `TezSessionState.open` method. This can lead to failures in resource localization or session initialization, resulting in the extra Tez session being started.",
            "StepsToReproduce": [
                "Start the HiveServer2 instance.",
                "Monitor the logs for any exceptions related to Tez session initialization."
            ],
            "ExpectedBehavior": "Only one Tez Application Master should be started during the HiveServer2 startup process.",
            "ObservedBehavior": "An extra Tez Application Master is launched, indicating a failure in session management.",
            "Suggestions": "Check the Hive and Tez configuration settings in `hive-site.xml` and ensure that all necessary resources are accessible. Verify user authentication settings and ensure that the `HiveConf` object is correctly configured.",
            "problem_location": {
                "files": [
                    "HiveMetaStore.java",
                    "TezSessionState.java",
                    "SessionManager.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.ql.exec.tez.TezSessionState",
                    "org.apache.hive.service.server.HiveServer2",
                    "org.apache.hive.service.cli.session.SessionManager"
                ],
                "methods": [
                    "TezSessionState.open",
                    "HiveServer2.startHiveServer2",
                    "SessionManager.init"
                ]
            },
            "possible_fix": "Review and correct the configuration settings in `hive-site.xml`, particularly those related to Tez, such as `tez.staging.dir`, `tez.am.resource.memory.mb`, and `tez.task.resource.memory.mb`. Ensure that the necessary resources are available and properly localized."
        },
        "possible_fix_code": {
            "TezSessionState.open": "  public void open(HiveConf conf, String[] additionalFiles)\n    throws IOException, LoginException, IllegalArgumentException, URISyntaxException, TezException {\n    this.conf = conf;\n\n    UserGroupInformation ugi;\n    ugi = ShimLoader.getHadoopShims().getUGIForConf(conf);\n    user = ShimLoader.getHadoopShims().getShortUserName(ugi);\n    LOG.info(\"User of session id \" + sessionId + \" is \" + user);\n\n    // Check for necessary configurations\n    if (conf.getVar(HiveConf.ConfVars.HIVE_EXECUTION_ENGINE).equals(\"tez\")) {\n        if (conf.getVar(HiveConf.ConfVars.HIVE_SERVER2_TEZ_DEFAULT_QUEUES).isEmpty()) {\n            throw new IllegalArgumentException(\"Tez default queues must be configured.\");\n        }\n    }\n\n    // create the tez tmp dir\n    tezScratchDir = createTezDir(sessionId);\n\n    String dir = tezScratchDir.toString();\n    // Localize resources to session scratch dir\n    localizedResources = utils.localizeTempFilesFromConf(dir, conf);\n    List<LocalResource> handlerLr = utils.localizeTempFiles(dir, conf, additionalFiles);\n    if (handlerLr != null) {\n      if (localizedResources == null) {\n        localizedResources = handlerLr;\n      } else {\n        localizedResources.addAll(handlerLr);\n      }\n      additionalFilesNotFromConf = new HashSet<String>();\n      for (String originalFile : additionalFiles) {\n        additionalFilesNotFromConf.add(originalFile);\n      }\n    }\n\n    // generate basic tez config\n    TezConfiguration tezConfig = new TezConfiguration(conf);\n\n    tezConfig.set(TezConfiguration.TEZ_AM_STAGING_DIR, tezScratchDir.toUri().toString());\n\n    // unless already installed on all the cluster nodes, we\\'ll have to\n    // localize hive-exec.jar as well.\n    appJarLr = createJarLocalResource(utils.getExecJarPathLocal());\n\n    // configuration for the application master\n    Map<String, LocalResource> commonLocalResources = new HashMap<String, LocalResource>();\n    commonLocalResources.put(utils.getBaseName(appJarLr), appJarLr);\n    if (localizedResources != null) {\n      for (LocalResource lr : localizedResources) {\n        commonLocalResources.put(utils.getBaseName(lr), lr);\n      }\n    }\n\n    // Create environment for AM.\n    Map<String, String> amEnv = new HashMap<String, String>();\n    MRHelpers.updateEnvironmentForMRAM(conf, amEnv);\n\n    AMConfiguration amConfig = new AMConfiguration(amEnv, commonLocalResources, tezConfig, null);\n\n    // configuration for the session\n    TezSessionConfiguration sessionConfig = new TezSessionConfiguration(amConfig, tezConfig);\n\n    // and finally we\\'re ready to create and start the session\n    session = new TezSession(\"HIVE-\" + sessionId, sessionConfig);\n\n    LOG.info(\"Opening new Tez Session (id: \" + sessionId\n        + \", scratch dir: \" + tezScratchDir + \")\");\n\n    session.start();\n\n    if (HiveConf.getBoolVar(conf, ConfVars.HIVE_PREWARM_ENABLED)) {\n      int n = HiveConf.getIntVar(conf, ConfVars.HIVE_PREWARM_NUM_CONTAINERS);\n      LOG.info(\"Prewarming \" + n + \" containers  (id: \" + sessionId\n          + \", scratch dir: \" + tezScratchDir + \")\");\n      PreWarmContext context = utils.createPreWarmContext(sessionConfig, n, commonLocalResources);\n      try {\n        session.preWarm(context);\n      } catch (InterruptedException ie) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Hive Prewarm threw an exception \", ie);\n        }\n      }\n    }\n\n    // In case we need to run some MR jobs, we\\'ll run them under tez MR emulation. The session\n    // id is used for tez to reuse the current session rather than start a new one.\n    conf.set(\"mapreduce.framework.name\", \"yarn-tez\");\n    conf.set(\"mapreduce.tez.session.tokill-application-id\", session.getApplicationId().toString());\n\n    openSessions.add(this);\n  }"
        }
    },
    {
        "filename": "HIVE-11991.json",
        "creation_time": "2015-09-29T21:46:06.000+0000",
        "bug_report": {
            "Title": "groupby11.q failing on branch-1.0",
            "Description": "The execution of the Hive query 'groupby11.q' on the branch-1.0 branch results in a ClassCastException. The error occurs when the system attempts to cast a String to org.apache.hadoop.io.Text, indicating a data type mismatch during the serialization/deserialization process. This issue is likely related to the handling of data types in the Hive execution framework, particularly in the context of the FetchTask and related operators.",
            "StackTrace": [
                "2015-09-29 14:27:51,676 ERROR CliDriver (SessionState.java:printError(833)) - Failed with exception java.io.IOException:org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.ClassCastException: java.lang.String cannot be cast to org.apache.hadoop.io.Text",
                "java.io.IOException: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.ClassCastException: java.lang.String cannot be cast to org.apache.hadoop.io.Text",
                "at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:154)",
                "at org.apache.hadoop.hive.ql.Driver.getResults(Driver.java:1621)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:221)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:153)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:364)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:299)",
                "at org.apache.hadoop.hive.ql.QTestUtil.executeClient(QTestUtil.java:832)",
                "at org.apache.hadoop.hive.cli.TestCliDriver.runTest(TestCliDriver.java:136)",
                "at org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_groupby11(TestCliDriver.java:120)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at junit.framework.TestCase.runTest(TestCase.java:176)",
                "at junit.framework.TestCase.runBare(TestCase.java:141)",
                "at junit.framework.TestResult$1.protect(TestResult.java:122)",
                "at junit.framework.TestResult.runProtected(TestResult.java:142)",
                "at junit.framework.TestResult.run(TestResult.java:125)",
                "at junit.framework.TestCase.run(TestCase.java:129)",
                "at junit.framework.TestSuite.runTest(TestSuite.java:255)",
                "at junit.framework.TestSuite.run(TestSuite.java:250)",
                "at org.junit.internal.runners.JUnit38ClassRunner.run(JUnit38ClassRunner.java:84)",
                "at org.apache.maven.surefire.junit4.JUnit4Provider.execute(JUnit4Provider.java:264)",
                "at org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:153)",
                "at org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:124)",
                "at org.apache.maven.surefire.booter.ForkedBooter.invokeProviderInSameClassLoader(ForkedBooter.java:200)",
                "at org.apache.maven.surefire.booter.ForkedBooter.runSuitesInProcess(ForkedBooter.java:153)",
                "at org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:103)",
                "Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.ClassCastException: java.lang.String cannot be cast to org.apache.hadoop.io.Text",
                "at org.apache.hadoop.hive.ql.exec.ListSinkOperator.processOp(ListSinkOperator.java:90)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)",
                "at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:85)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)",
                "at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:95)",
                "at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:572)",
                "at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:564)",
                "at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:140)"
            ],
            "RootCause": "The root cause of the issue is a ClassCastException occurring due to a mismatch between the expected data type (org.apache.hadoop.io.Text) and the actual data type (String) being processed in the Hive execution framework.",
            "StepsToReproduce": [
                "1. Checkout the branch-1.0.",
                "2. Execute the Hive query 'groupby11.q'.",
                "3. Observe the error message in the logs."
            ],
            "ExpectedBehavior": "The query should execute successfully and return the expected results without any exceptions.",
            "ObservedBehavior": "The query fails with a ClassCastException indicating that a String cannot be cast to org.apache.hadoop.io.Text.",
            "Suggestions": "1. Ensure that the data types defined in the Hive table schema match the input data types. \n2. Check the serialization logic in the SerDe being used. \n3. Review the query for any implicit casts that may lead to a ClassCastException. \n4. Verify the input format of the data being processed.",
            "problem_location": {
                "files": [
                    "FetchTask.java",
                    "ListSinkOperator.java",
                    "SelectOperator.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.ql.exec.FetchTask",
                    "org.apache.hadoop.hive.ql.exec.ListSinkOperator",
                    "org.apache.hadoop.hive.ql.exec.SelectOperator"
                ],
                "methods": [
                    "FetchTask.fetch",
                    "ListSinkOperator.processOp",
                    "SelectOperator.processOp"
                ]
            },
            "possible_fix": "Review and update the Hive table schema to ensure that the data types match the input data. Additionally, verify the serialization logic in the SerDe to ensure it correctly handles String to Text conversions."
        },
        "possible_fix_code": {
            "FetchTask.fetch": "  public boolean fetch(List res) throws IOException, CommandNeedRetryException {\n    sink.reset(res);\n    int rowsRet = work.getLeastNumRows();\n    if (rowsRet <= 0) {\n      rowsRet = work.getLimit() >= 0 ? Math.min(work.getLimit() - totalRows, maxRows) : maxRows;\n    }\n    try {\n      if (rowsRet <= 0 || work.getLimit() == totalRows) {\n        fetch.clearFetchContext();\n        return false;\n      }\n      boolean fetched = false;\n      while (sink.getNumRows() < rowsRet) {\n        if (!fetch.pushRow()) {\n          if (work.getLeastNumRows() > 0) {\n            throw new CommandNeedRetryException();\n          }\n          return fetched;\n        }\n        // Ensure that the row being processed is of the correct type\n        Object row = fetch.getCurrentRow();\n        if (row instanceof String) {\n          // Convert String to Text before processing\n          Text textRow = new Text((String) row);\n          sink.process(textRow);\n        } else {\n          sink.process(row);\n        }\n        fetched = true;\n      }\n      return true;\n    } catch (CommandNeedRetryException e) {\n      throw e;\n    } catch (IOException e) {\n      throw e;\n    } catch (Exception e) {\n      throw new IOException(e);\n    } finally {\n      totalRows += sink.getNumRows();\n    }\n  }"
        }
    },
    {
        "filename": "HIVE-17900.json",
        "creation_time": "2017-10-25T17:12:42.000+0000",
        "bug_report": {
            "Title": "Analyze stats on columns triggered by Compactor generates malformed SQL with > 1 partition column",
            "Description": "The issue arises when executing an `ANALYZE` statement on a Hive table with multiple partition columns. The error message indicates a `ParseException` due to a syntax error in the SQL command, specifically at the point where the parser encounters the word 'dates' instead of a closing parenthesis. This suggests that the partition specification in the `ANALYZE` statement is incorrectly formatted, leading to the failure in parsing the SQL command.",
            "StackTrace": [
                "org.apache.hadoop.hive.ql.parse.ParseException: line 1:70 mismatched input 'dates' expecting ) near ''201608'' in analyze statement",
                "at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:205)",
                "at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:166)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:438)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:321)",
                "at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1221)",
                "at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1262)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1158)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1148)",
                "at org.apache.hadoop.hive.ql.txn.compactor.Worker$StatsUpdater.gatherStats(Worker.java:294)",
                "at org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.run(CompactorMR.java:265)",
                "at org.apache.hadoop.hive.ql.txn.compactor.Worker.run(Worker.java:168)"
            ],
            "RootCause": "The root cause of the issue is a syntax error in the `ANALYZE` statement related to the partition specification. The parser expects a closing parenthesis but encounters the word 'dates', indicating that the partitioning columns or values are not correctly formatted.",
            "StepsToReproduce": [
                "Execute an `ANALYZE` statement on a Hive table with multiple partition columns, ensuring that the partition specification includes the word 'dates'.",
                "Observe the error message generated during the execution."
            ],
            "ExpectedBehavior": "The `ANALYZE` statement should successfully gather statistics for the specified table and partitions without any syntax errors.",
            "ObservedBehavior": "The execution of the `ANALYZE` statement fails with a `ParseException`, indicating a syntax error near the partition specification.",
            "Suggestions": "Review the syntax of the `ANALYZE` statement to ensure proper formatting of partition specifications. Consider enclosing reserved keywords like 'dates' in backticks to avoid conflicts. Verify that all parentheses and quotes are correctly placed.",
            "problem_location": {
                "files": [
                    "ParseDriver.java",
                    "Worker.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.ql.parse.ParseDriver",
                    "org.apache.hadoop.hive.ql.txn.compactor.Worker"
                ],
                "methods": [
                    "ParseDriver.parse",
                    "Worker$StatsUpdater.gatherStats"
                ]
            },
            "possible_fix": "Ensure that the `ANALYZE` statement is correctly formatted. For example, use the following syntax: `ANALYZE TABLE table_name PARTITION (month='201608', dates='9') COMPUTE STATISTICS;`. This includes proper use of quotes and parentheses."
        },
        "possible_fix_code": {
            "ParseDriver.parse": "  public ASTNode parse(String command, Context ctx, String viewFullyQualifiedName)\n      throws ParseException {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Parsing command: \" + command);\n    }\n\n    // Check for correct syntax in ANALYZE statement\n    if (command.toUpperCase().startsWith(\"ANALYZE TABLE\") && command.contains(\"PARTITION\")) {\n      // Ensure reserved keywords are enclosed in backticks\n      command = command.replaceAll(\"(?i)\\bdates\\b\", \"`dates`\");\n    }\n\n    HiveLexerX lexer = new HiveLexerX(new ANTLRNoCaseStringStream(command));\n    TokenRewriteStream tokens = new TokenRewriteStream(lexer);\n    if (ctx != null) {\n      if (viewFullyQualifiedName == null) {\n        // Top level query\n        ctx.setTokenRewriteStream(tokens);\n      } else {\n        // It is a view\n        ctx.addViewTokenRewriteStream(viewFullyQualifiedName, tokens);\n      }\n      lexer.setHiveConf(ctx.getConf());\n    }\n    HiveParser parser = new HiveParser(tokens);\n    if (ctx != null) {\n      parser.setHiveConf(ctx.getConf());\n    }\n    parser.setTreeAdaptor(adaptor);\n    HiveParser.statement_return r = null;\n    try {\n      r = parser.statement();\n    } catch (RecognitionException e) {\n      e.printStackTrace();\n      throw new ParseException(parser.errors);\n    }\n\n    if (lexer.getErrors().size() == 0 && parser.errors.size() == 0) {\n      LOG.debug(\"Parse Completed\");\n    } else if (lexer.getErrors().size() != 0) {\n      throw new ParseException(lexer.getErrors());\n    } else {\n      throw new ParseException(parser.errors);\n    }\n\n    ASTNode tree = (ASTNode) r.getTree();\n    tree.setUnknownTokenBoundaries();\n    return tree;\n  }"
        }
    },
    {
        "filename": "HIVE-10816.json",
        "creation_time": "2015-05-25T08:08:37.000+0000",
        "bug_report": {
            "Title": "NPE in ExecDriver::handleSampling when submitted via child JVM",
            "Description": "A NullPointerException (NPE) occurs in the `handleSampling` method of the `ExecDriver` class when the Hive configuration `hive.exec.submitviachild` is set to true. This issue arises during parallel order by operations, causing the system to fall back to single-reducer mode. The stack trace indicates that the exception is thrown at line 513 of `ExecDriver.java`, specifically when attempting to access properties of potentially uninitialized objects.",
            "StackTrace": [
                "2015-05-25 08:41:04,446 ERROR [main]: mr.ExecDriver (ExecDriver.java:execute(386)) - Sampling error",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecDriver.handleSampling(ExecDriver.java:513)",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecDriver.execute(ExecDriver.java:379)",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecDriver.main(ExecDriver.java:750)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:497)",
                "at org.apache.hadoop.util.RunJar.run(RunJar.java:221)",
                "at org.apache.hadoop.util.RunJar.main(RunJar.java:136)"
            ],
            "RootCause": "The NPE is likely caused by uninitialized or improperly initialized objects in the `handleSampling` method. Specifically, the `mWork` object or its methods (`getAliases()`, `getAliasToWork()`, `getAliasToPartnInfo()`, `getPaths()`, and `getPartitionDescs()`) may return null or be uninitialized. Additionally, the `context` object may also be null, leading to the exception when accessing `context.getCtx()`.",
            "StepsToReproduce": [
                "Set the Hive configuration property `hive.exec.submitviachild` to true.",
                "Execute a parallel order by operation in Hive.",
                "Observe the logs for the NullPointerException in the `ExecDriver` class."
            ],
            "ExpectedBehavior": "The system should successfully handle sampling without throwing a NullPointerException, allowing the parallel order by operation to complete without falling back to single-reducer mode.",
            "ObservedBehavior": "The system throws a NullPointerException in the `handleSampling` method, causing the operation to fail and revert to single-reducer mode.",
            "Suggestions": "Ensure that the `mWork` and `context` objects are properly initialized before invoking the `handleSampling` method. Add null checks for the objects accessed within the method to prevent NPEs.",
            "problem_location": {
                "files": [
                    "ExecDriver.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.ql.exec.mr.ExecDriver",
                    "org.apache.hadoop.hive.ql.plan.MapWork",
                    "org.apache.hadoop.hive.ql.DriverContext"
                ],
                "methods": [
                    "ExecDriver.handleSampling",
                    "ExecDriver.execute",
                    "DriverContext.getCtx",
                    "MapWork.getAliases",
                    "MapWork.getAliasToWork",
                    "MapWork.getAliasToPartnInfo",
                    "MapWork.getPaths",
                    "MapWork.getPartitionDescs"
                ]
            },
            "possible_fix": "Add null checks in the `handleSampling` method to ensure that `mWork` and `context` are not null before accessing their methods. For example:\n\n```java\nif (mWork == null || context == null || context.getCtx() == null) {\n    throw new IllegalStateException(\"Required objects are not initialized.\");\n}\n```"
        },
        "possible_fix_code": {
            "ExecDriver.handleSampling": "  private void handleSampling(DriverContext context, MapWork mWork, JobConf job, HiveConf conf)\n      throws Exception {\n    if (mWork == null || context == null || context.getCtx() == null) {\n        throw new IllegalStateException(\"Required objects are not initialized.\");\n    }\n\n    assert mWork.getAliasToWork().keySet().size() == 1;\n\n    String alias = mWork.getAliases().get(0);\n    Operator<?> topOp = mWork.getAliasToWork().get(alias);\n    PartitionDesc partDesc = mWork.getAliasToPartnInfo().get(alias);\n\n    ArrayList<String> paths = mWork.getPaths();\n    ArrayList<PartitionDesc> parts = mWork.getPartitionDescs();\n\n    List<Path> inputPaths = new ArrayList<Path>(paths.size());\n    for (String path : paths) {\n      inputPaths.add(new Path(path));\n    }\n\n    Path tmpPath = context.getCtx().getExternalTmpPath(inputPaths.get(0));\n    Path partitionFile = new Path(tmpPath, \".partitions\");\n    ShimLoader.getHadoopShims().setTotalOrderPartitionFile(job, partitionFile);\n    PartitionKeySampler sampler = new PartitionKeySampler();\n\n    if (mWork.getSamplingType() == MapWork.SAMPLING_ON_PREV_MR) {\n      console.printInfo(\"Use sampling data created in previous MR\");\n      // merges sampling data from previous MR and make partition keys for total sort\n      for (Path path : inputPaths) {\n        FileSystem fs = path.getFileSystem(job);\n        for (FileStatus status : fs.globStatus(new Path(path, \".sampling*\"))) {\n          sampler.addSampleFile(status.getPath(), job);\n        }\n      }\n    } else if (mWork.getSamplingType() == MapWork.SAMPLING_ON_START) {\n      console.printInfo(\"Creating sampling data..\");\n      assert topOp instanceof TableScanOperator;\n      TableScanOperator ts = (TableScanOperator) topOp;\n\n      FetchWork fetchWork;\n      if (!partDesc.isPartitioned()) {\n        assert paths.size() == 1;\n        fetchWork = new FetchWork(inputPaths.get(0), partDesc.getTableDesc());\n      } else {\n        fetchWork = new FetchWork(inputPaths, parts, partDesc.getTableDesc());\n      }\n      fetchWork.setSource(ts);\n\n      // random sampling\n      FetchOperator fetcher = PartitionKeySampler.createSampler(fetchWork, conf, job, ts);\n      try {\n        ts.initialize(conf, new ObjectInspector[]{fetcher.getOutputObjectInspector()});\n        OperatorUtils.setChildrenCollector(ts.getChildOperators(), sampler);\n        while (fetcher.pushRow()) { }\n      } finally {\n        fetcher.clearFetchContext();\n      }\n    } else {\n      throw new IllegalArgumentException(\"Invalid sampling type \" + mWork.getSamplingType());\n    }\n    sampler.writePartitionKeys(partitionFile, conf, job);\n  }"
        }
    },
    {
        "filename": "HIVE-13017.json",
        "creation_time": "2016-02-05T23:40:09.000+0000",
        "bug_report": {
            "Title": "Child process of HiveServer2 fails to get delegation token from non-default FileSystem",
            "Description": "When executing a Hive query that involves temporary tables and joins, the child process of HiveServer2 fails to obtain a delegation token from the non-default Azure FileSystem, resulting in an execution error. The issue arises specifically when using Azure Filesystem as the default file system while HDFS is utilized for intermediate data storage. The error manifests during the execution of a query that attempts to join multiple tables, leading to a failure in the MapredLocalTask.",
            "StackTrace": [
                "ERROR : Execution failed with exit status: 2",
                "ERROR : Obtaining error information",
                "ERROR : /var/log/hive/hiveServer2.log",
                "FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask"
            ],
            "RootCause": "The failure is due to the inability of the HiveServer2 child process to obtain the necessary delegation token from the Azure FileSystem when executing a MapReduce task. This is likely related to the configuration of the file systems and the handling of delegation tokens in the context of temporary tables.",
            "StepsToReproduce": [
                "Set Azure Filesystem as the default file system.",
                "Create temporary tables using Hive queries.",
                "Execute a query that joins these temporary tables with other tables."
            ],
            "ExpectedBehavior": "The query should execute successfully, retrieving the expected results without any errors related to delegation tokens or execution failures.",
            "ObservedBehavior": "The query fails with an execution error, specifically a return code of 2 from the MapredLocalTask, indicating that the child process could not obtain the necessary delegation token.",
            "Suggestions": "Review the configuration for delegation tokens and ensure that the HiveServer2 process has the necessary permissions to access both the Azure FileSystem and HDFS. Consider modifying the code in the MapredLocalTask to handle delegation tokens more effectively, similar to the changes made in HIVE-739.",
            "problem_location": {
                "files": [
                    "MapredLocalTask.java",
                    "Driver.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask",
                    "org.apache.hadoop.hive.ql.Driver"
                ],
                "methods": [
                    "MapredLocalTask.executeInChildVM",
                    "Driver.execute"
                ]
            },
            "possible_fix": "Modify the MapredLocalTask to ensure it retrieves delegation tokens from all configured file systems, particularly when the default file system is not HDFS. This may involve updating the logic in the executeInChildVM method to include checks for Azure FileSystem."
        },
        "possible_fix_code": {
            "MapredLocalTask.executeInChildVM": "  public void executeInChildVM() throws Exception {\n    // Ensure that we retrieve delegation tokens from all configured file systems\n    Configuration conf = getConf();\n    FileSystem fs = FileSystem.get(conf);\n    Token<?> token = fs.getDelegationToken(user);\n    if (token != null) {\n        // Add the token to the credentials\n        Credentials credentials = new Credentials();\n        credentials.addToken(token.getService(), token);\n        UserGroupInformation.getCurrentUser().addCredentials(credentials);\n    } else {\n        throw new IOException(\"Failed to obtain delegation token from the configured FileSystem.\");\n    }\n\n    // Proceed with the rest of the execution logic\n    // ... (existing logic for executing the task)\n  }"
        }
    },
    {
        "filename": "HIVE-11303.json",
        "creation_time": "2015-07-18T01:31:02.000+0000",
        "bug_report": {
            "Title": "Getting Tez LimitExceededException after dag execution on large query",
            "Description": "The application encounters a LimitExceededException during the execution of a Tez DAG due to exceeding the maximum allowed number of counters. The limit is set to 1200, but the application attempts to use 1201 counters, resulting in a failure to execute the Tez graph. This issue arises from the management of counters within the Apache Tez framework, specifically in the Limits class.",
            "StackTrace": [
                "2015-07-17 18:18:11,830 INFO  [main]: counters.Limits (Limits.java:ensureInitialized(59)) - Counter limits initialized with parameters:  GROUP_NAME_MAX=256, MAX_GROUPS=500, COUNTER_NAME_MAX=64, MAX_COUNTERS=1200",
                "2015-07-17 18:18:11,841 ERROR [main]: exec.Task (TezTask.java:execute(189)) - Failed to execute tez graph.",
                "org.apache.tez.common.counters.LimitExceededException: Too many counters: 1201 max=1200",
                "at org.apache.tez.common.counters.Limits.checkCounters(Limits.java:87)",
                "at org.apache.tez.common.counters.Limits.incrCounters(Limits.java:94)",
                "at org.apache.tez.common.counters.AbstractCounterGroup.addCounter(AbstractCounterGroup.java:76)",
                "at org.apache.tez.common.counters.AbstractCounterGroup.addCounterImpl(AbstractCounterGroup.java:93)",
                "at org.apache.tez.common.counters.AbstractCounterGroup.findCounter(AbstractCounterGroup.java:104)",
                "at org.apache.tez.dag.api.DagTypeConverters.convertTezCountersFromProto(DagTypeConverters.java:567)",
                "at org.apache.tez.dag.api.client.DAGStatus.getDAGCounters(DAGStatus.java:148)",
                "at org.apache.hadoop.hive.ql.exec.tez.TezTask.execute(TezTask.java:175)",
                "at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:160)",
                "at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:89)",
                "at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1673)",
                "at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1432)",
                "at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1213)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1064)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1054)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:213)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:165)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:311)",
                "at org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:409)",
                "at org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:425)",
                "at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:714)",
                "at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:681)",
                "at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:497)",
                "at org.apache.hadoop.util.RunJar.run(RunJar.java:221)",
                "at org.apache.hadoop.util.RunJar.main(RunJar.java:136)"
            ],
            "RootCause": "The application is attempting to use more counters than the maximum allowed limit of 1200, leading to a LimitExceededException. This is caused by the way counters are managed in the Apache Tez framework.",
            "StepsToReproduce": [
                "Execute a large query that generates more than 1200 counters in the Apache Tez framework.",
                "Monitor the execution logs for any LimitExceededException errors."
            ],
            "ExpectedBehavior": "The Tez DAG should execute successfully without exceeding the counter limits, allowing for proper task execution.",
            "ObservedBehavior": "The execution fails with a LimitExceededException due to exceeding the maximum number of counters allowed.",
            "Suggestions": "Consider optimizing the query to reduce the number of counters generated or investigate configuration options to increase the counter limit if applicable.",
            "problem_location": {
                "files": [
                    "Limits.java",
                    "TezTask.java"
                ],
                "classes": [
                    "org.apache.tez.common.counters.Limits",
                    "org.apache.hadoop.hive.ql.exec.tez.TezTask"
                ],
                "methods": [
                    "Limits.checkCounters",
                    "TezTask.execute"
                ]
            },
            "possible_fix": "Review the counter management logic in the application and optimize the usage of counters. If necessary, explore configuration settings in the Tez framework to increase the maximum counter limit."
        },
        "possible_fix_code": {
            "org.apache.tez.common.counters.Limits.checkCounters": "  public void checkCounters(int currentCounters) throws LimitExceededException {\n    if (currentCounters > MAX_COUNTERS) {\n      throw new LimitExceededException(\"Too many counters: \" + currentCounters + \" max=\" + MAX_COUNTERS);\n    }\n  }"
        }
    },
    {
        "filename": "HIVE-5899.json",
        "creation_time": "2013-11-27T02:48:39.000+0000",
        "bug_report": {
            "Title": "NPE during explain extended with char/varchar columns",
            "Description": "A NullPointerException (NPE) occurs when executing the 'analyze table' command for columns with char/varchar types, followed by an attempt to run 'explain extended'. The error arises during the annotation of the operator tree with statistics, specifically when the Hive metastore tries to access column statistics that may not be properly initialized or are missing. The stack trace indicates that the issue originates from the 'getFieldDesc' and 'getStringStats' methods in the 'ColumnStatisticsData' class, which are expected to return metadata about the columns but fail due to uninitialized objects.",
            "StackTrace": [
                "2013-11-26 01:53:06,682 ERROR ql.Driver (SessionState.java:printError(440)) - FAILED: NullPointerException null",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.hive.metastore.api.ColumnStatisticsData.getFieldDesc(ColumnStatisticsData.java:367)",
                "at org.apache.hadoop.hive.metastore.api.ColumnStatisticsData.getStringStats(ColumnStatisticsData.java:444)",
                "at org.apache.hadoop.hive.ql.stats.StatsUtils.getColStatistics(StatsUtils.java:414)",
                "at org.apache.hadoop.hive.ql.stats.StatsUtils.getTableColumnStatsForColumn(StatsUtils.java:369)",
                "at org.apache.hadoop.hive.ql.stats.StatsUtils.getTableColumnStats(StatsUtils.java:465)",
                "at org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics(StatsUtils.java:109)",
                "at org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory$TableScanStatsRule.process(StatsRulesProcFactory.java:102)",
                "at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90)",
                "at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:94)",
                "at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:78)",
                "at org.apache.hadoop.hive.ql.lib.PreOrderWalker.walk(PreOrderWalker.java:54)",
                "at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.startWalking(DefaultGraphWalker.java:109)",
                "at org.apache.hadoop.hive.ql.optimizer.stats.annotation.AnnotateWithStatistics.transform(AnnotateWithStatistics.java:76)",
                "at org.apache.hadoop.hive.ql.optimizer.Optimizer.optimize(Optimizer.java:136)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:8913)",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:292)",
                "at org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.analyzeInternal(ExplainSemanticAnalyzer.java:65)",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:292)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:441)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:341)",
                "at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:994)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:905)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:268)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:220)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:422)",
                "at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:790)",
                "at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:684)",
                "at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:623)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "at java.lang.reflect.Method.invoke(Method.java:597)",
                "at org.apache.hadoop.util.RunJar.main(RunJar.java:212)"
            ],
            "RootCause": "The NullPointerException is likely caused by uninitialized objects or missing data in the Hive metastore, particularly when accessing column statistics for char/varchar columns.",
            "StepsToReproduce": [
                "1. Run the command 'analyze table <table_name> for columns with char/varchar types'.",
                "2. Execute 'explain extended <query>' on the same table."
            ],
            "ExpectedBehavior": "The system should successfully execute the 'explain extended' command without throwing a NullPointerException, providing the expected output of the query plan.",
            "ObservedBehavior": "A NullPointerException is thrown, indicating that the system is unable to access necessary column statistics, leading to a failure in processing the explain command.",
            "Suggestions": "Check the initialization of objects in the ColumnStatisticsData class and ensure that all necessary metadata and statistics are present in the Hive metastore. Review the code for assumptions about the presence of data that may not hold true.",
            "problem_location": {
                "files": [
                    "ColumnStatisticsData.java",
                    "StatsUtils.java",
                    "StatsRulesProcFactory.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.metastore.api.ColumnStatisticsData",
                    "org.apache.hadoop.hive.ql.stats.StatsUtils",
                    "org.apache.hadoop.hive.ql.optimizer.stats.annotation.StatsRulesProcFactory"
                ],
                "methods": [
                    "ColumnStatisticsData.getFieldDesc",
                    "ColumnStatisticsData.getStringStats",
                    "StatsUtils.getColStatistics"
                ]
            },
            "possible_fix": "Ensure that the methods in ColumnStatisticsData properly handle cases where statistics may not be initialized. Consider adding null checks and initializing default values where necessary."
        },
        "possible_fix_code": {
            "org.apache.hadoop.hive.ql.stats.StatsUtils.getColStatistics": "  public static ColStatistics getColStatistics(ColumnStatisticsObj cso, String tabName,\n      String colName) {\n    ColStatistics cs = new ColStatistics(tabName, colName, cso.getColType());\n    String colType = cso.getColType();\n    ColumnStatisticsData csd = cso.getStatsData();\n    if (csd == null) {\n      // Handle the case where csd is null to avoid NPE\n      return cs; // Return empty ColStatistics or handle as needed\n    }\n    if (colType.equalsIgnoreCase(serdeConstants.TINYINT_TYPE_NAME)\n        || colType.equalsIgnoreCase(serdeConstants.SMALLINT_TYPE_NAME)\n        || colType.equalsIgnoreCase(serdeConstants.INT_TYPE_NAME)) {\n      cs.setCountDistint(csd.getLongStats().getNumDVs());\n      cs.setNumNulls(csd.getLongStats().getNumNulls());\n      cs.setAvgColLen(JavaDataModel.get().primitive1());\n    } else if (colType.equalsIgnoreCase(serdeConstants.BIGINT_TYPE_NAME)) {\n      cs.setCountDistint(csd.getLongStats().getNumDVs());\n      cs.setNumNulls(csd.getLongStats().getNumNulls());\n      cs.setAvgColLen(JavaDataModel.get().primitive2());\n    } else if (colType.equalsIgnoreCase(serdeConstants.FLOAT_TYPE_NAME)) {\n      cs.setCountDistint(csd.getDoubleStats().getNumDVs());\n      cs.setNumNulls(csd.getDoubleStats().getNumNulls());\n      cs.setAvgColLen(JavaDataModel.get().primitive1());\n    } else if (colType.equalsIgnoreCase(serdeConstants.DOUBLE_TYPE_NAME)) {\n      cs.setCountDistint(csd.getDoubleStats().getNumDVs());\n      cs.setNumNulls(csd.getDoubleStats().getNumNulls());\n      cs.setAvgColLen(JavaDataModel.get().primitive2());\n    } else if (colType.equalsIgnoreCase(serdeConstants.STRING_TYPE_NAME)\n        || colType.startsWith(serdeConstants.CHAR_TYPE_NAME)\n        || colType.startsWith(serdeConstants.VARCHAR_TYPE_NAME)) {\n      cs.setCountDistint(csd.getStringStats().getNumDVs());\n      cs.setNumNulls(csd.getStringStats().getNumNulls());\n      cs.setAvgColLen(csd.getStringStats().getAvgColLen());\n    } else if (colType.equalsIgnoreCase(serdeConstants.BOOLEAN_TYPE_NAME)) {\n      if (csd.getBooleanStats().getNumFalses() > 0 && csd.getBooleanStats().getNumTrues() > 0) {\n        cs.setCountDistint(2);\n      } else {\n        cs.setCountDistint(1);\n      }\n      cs.setNumTrues(csd.getBooleanStats().getNumTrues());\n      cs.setNumFalses(csd.getBooleanStats().getNumFalses());\n      cs.setNumNulls(csd.getBooleanStats().getNumNulls());\n      cs.setAvgColLen(JavaDataModel.get().primitive1());\n    } else if (colType.equalsIgnoreCase(serdeConstants.BINARY_TYPE_NAME)) {\n      cs.setAvgColLen(csd.getBinaryStats().getAvgColLen());\n      cs.setNumNulls(csd.getBinaryStats().getNumNulls());\n    } else if (colType.equalsIgnoreCase(serdeConstants.TIMESTAMP_TYPE_NAME)) {\n      cs.setAvgColLen(JavaDataModel.get().lengthOfTimestamp());\n    } else if (colType.startsWith(serdeConstants.DECIMAL_TYPE_NAME)) {\n      cs.setAvgColLen(JavaDataModel.get().lengthOfDecimal());\n    } else if (colType.equalsIgnoreCase(serdeConstants.DATE_TYPE_NAME)) {\n      cs.setAvgColLen(JavaDataModel.get().lengthOfDate());\n    } else {\n      // Columns statistics for complex datatypes are not supported yet\n      return null;\n    }\n    return cs;\n  }"
        }
    },
    {
        "filename": "HIVE-11102.json",
        "creation_time": "2015-06-24T22:54:55.000+0000",
        "bug_report": {
            "Title": "ReaderImpl: getColumnIndicesFromNames does not work for some cases",
            "Description": "The ORC reader implementation fails to correctly estimate the size of ACID data files, leading to an IndexOutOfBoundsException. This issue arises specifically in the getColumnIndicesFromNames method of the ReaderImpl class when it attempts to access the types list from the footer of the ORC file. If the types list is empty, any attempt to access its elements will result in an IndexOutOfBoundsException.",
            "StackTrace": [
                "Caused by: java.lang.IndexOutOfBoundsException: Index: 0",
                "at java.util.Collections$EmptyList.get(Collections.java:3212)",
                "at org.apache.hadoop.hive.ql.io.orc.OrcProto$Type.getSubtypes(OrcProto.java:12240)",
                "at org.apache.hadoop.hive.ql.io.orc.ReaderImpl.getColumnIndicesFromNames(ReaderImpl.java:651)",
                "at org.apache.hadoop.hive.ql.io.orc.ReaderImpl.getRawDataSizeOfColumns(ReaderImpl.java:634)",
                "at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator.populateAndCacheStripeDetails(OrcInputFormat.java:938)",
                "at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator.call(OrcInputFormat.java:847)",
                "at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$SplitGenerator.call(OrcInputFormat.java:713)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:262)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:744)"
            ],
            "RootCause": "The root cause of the IndexOutOfBoundsException is that the footer's types list is empty when the getColumnIndicesFromNames method attempts to access it. This can occur if the ORC file being read is malformed or does not contain a valid footer structure.",
            "StepsToReproduce": [
                "1. Attempt to read an ORC file that is known to be malformed or has an empty types list.",
                "2. Call the getColumnIndicesFromNames method with a list of column names.",
                "3. Observe the IndexOutOfBoundsException being thrown."
            ],
            "ExpectedBehavior": "The system should handle cases where the types list is empty gracefully, either by returning an empty list of column indices or by throwing a more informative exception.",
            "ObservedBehavior": "An IndexOutOfBoundsException is thrown when attempting to access an index in an empty list, specifically in the getColumnIndicesFromNames method.",
            "Suggestions": "1. Validate the ORC file before attempting to read it to ensure it is well-formed and contains a valid footer. 2. Implement checks in the getColumnIndicesFromNames method to handle cases where the types list is empty, preventing the IndexOutOfBoundsException.",
            "problem_location": {
                "files": [
                    "ReaderImpl.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.ql.io.orc.ReaderImpl"
                ],
                "methods": [
                    "getColumnIndicesFromNames",
                    "getRawDataSizeOfColumns"
                ]
            },
            "possible_fix": "Add a check in the getColumnIndicesFromNames method to verify that footer.getTypesList() is not empty before attempting to access its elements. For example:\n\nif (footer.getTypesList().isEmpty()) {\n    throw new IllegalStateException(\"Types list is empty, cannot retrieve column indices.\");\n}"
        },
        "possible_fix_code": {
            "getColumnIndicesFromNames": "  private List<Integer> getColumnIndicesFromNames(List<String> colNames) {\n    // top level struct\n    if (footer.getTypesList().isEmpty()) {\n        throw new IllegalStateException(\"Types list is empty, cannot retrieve column indices.\");\n    }\n    Type type = footer.getTypesList().get(0);\n    List<Integer> colIndices = Lists.newArrayList();\n    List<String> fieldNames = type.getFieldNamesList();\n    int fieldIdx = 0;\n    for (String colName : colNames) {\n      if (fieldNames.contains(colName)) {\n        fieldIdx = fieldNames.indexOf(colName);\n      }\n\n      // a single field may span multiple columns. find start and end column\n      // index for the requested field\n      int idxStart = type.getSubtypes(fieldIdx);\n\n      int idxEnd;\n\n      // if the specified is the last field and then end index will be last\n      // column index\n      if (fieldIdx + 1 > fieldNames.size() - 1) {\n        idxEnd = getLastIdx() + 1;\n      } else {\n        idxEnd = type.getSubtypes(fieldIdx + 1);\n      }\n\n      // if start index and end index are same then the field is a primitive\n      // field else complex field (like map, list, struct, union)\n      if (idxStart == idxEnd) {\n        // simple field\n        colIndices.add(idxStart);\n      } else {\n        // complex fields spans multiple columns\n        for (int i = idxStart; i < idxEnd; i++) {\n          colIndices.add(i);\n        }\n      }\n    }\n    return colIndices;\n  }"
        }
    },
    {
        "filename": "HIVE-9195.json",
        "creation_time": "2014-12-23T01:08:45.000+0000",
        "bug_report": {
            "Title": "CBO changes constant to column type",
            "Description": "During the creation of a test case for HIVE-8613, it was discovered that the Cost-Based Optimizer (CBO) incorrectly transforms constant expressions into column expressions. This issue arises specifically in test mode when executing a Hive query that utilizes the `percentile_approx` function. The query attempts to use a conditional expression that should yield a constant value, but instead, it results in an argument type exception due to the CBO's handling of the expression. The error message indicates that the second argument of the `GenericUDAFPercentileApprox` function must be a constant, but a double type was passed instead, leading to a failure in the semantic analysis phase.",
            "StackTrace": [
                "2014-12-22 17:03:31,433 ERROR parse.SemanticAnalyzer (SemanticAnalyzer.java:analyzeInternal(10102)) - CBO failed, skipping CBO.",
                "org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException: The second argument must be a constant, but double was passed instead.",
                "at org.apache.hadoop.hive.ql.udf.generic.GenericUDAFPercentileApprox.getEvaluator(GenericUDAFPercentileApprox.java:146)",
                "at org.apache.hadoop.hive.ql.exec.FunctionRegistry.getGenericUDAFEvaluator(FunctionRegistry.java:1160)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getGenericUDAFEvaluator(SemanticAnalyzer.java:3794)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genGroupByPlanMapGroupByOperator(SemanticAnalyzer.java:4467)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genGroupByPlanMapAggrNoSkew(SemanticAnalyzer.java:5536)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genBodyPlan(SemanticAnalyzer.java:8884)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:9745)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:9638)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10086)",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:224)",
                "at org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.analyzeInternal(ExplainSemanticAnalyzer.java:74)",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:224)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:419)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:305)",
                "at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1107)",
                "at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1155)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1044)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1034)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:206)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:158)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:369)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:304)",
                "at org.apache.hadoop.hive.qtest.QTestUtil.executeClient(QTestUtil.java:877)",
                "at org.apache.hadoop.hive.cli.TestCliDriver.runTest(TestCliDriver.java:136)",
                "at org.apache.hadoop.hive.cli.TestCliDriver.testCliDriver_udaf_percentile_approx_23(TestCliDriver.java:120)"
            ],
            "RootCause": "The root cause of the issue is that the `GenericUDAFPercentileApprox` function is being called with a second argument that is not a constant. The function expects a literal constant value (e.g., 0.5) instead of a variable or computed value, which leads to a UDFArgumentTypeException during the semantic analysis.",
            "StepsToReproduce": [
                "Create a table with the following SQL command: CREATE TABLE bucket (key double, value string) CLUSTERED BY (key) SORTED BY (key DESC) INTO 4 BUCKETS STORED AS TEXTFILE;",
                "Load data into the table using the provided local file paths.",
                "Execute the following query: SELECT percentile_approx(CASE WHEN key < 100 THEN CAST('NaN' AS double) ELSE key END, 0.5) FROM bucket;"
            ],
            "ExpectedBehavior": "The query should execute successfully and return the approximate percentile value without any errors.",
            "ObservedBehavior": "The query fails with an argument type exception indicating that the second argument must be a constant, but a double was passed instead.",
            "Suggestions": "Ensure that the second argument passed to the `percentile_approx` function is a constant literal value instead of a variable or computed expression. For example, replace the variable with a constant like 0.5.",
            "problem_location": {
                "files": [
                    "SemanticAnalyzer.java",
                    "CliDriver.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.ql.parse.SemanticAnalyzer",
                    "org.apache.hadoop.hive.cli.CliDriver"
                ],
                "methods": [
                    "SemanticAnalyzer.analyzeInternal",
                    "CliDriver.processLocalCmd"
                ]
            },
            "possible_fix": "Modify the Hive query to use a constant value for the second argument of the `percentile_approx` function. For example, change the query to: SELECT percentile_approx(CASE WHEN key < 100 THEN CAST('NaN' AS double) ELSE key END, 0.5) FROM bucket;"
        },
        "possible_fix_code": {
            "SemanticAnalyzer.analyzeInternal": "  public void analyzeInternal(ASTNode ast) throws SemanticException {\n    ASTNode child = ast;\n    this.ast = ast;\n    viewsExpanded = new ArrayList<String>();\n    ctesExpanded = new ArrayList<String>();\n\n    LOG.info(\"Starting Semantic Analysis\");\n\n    // analyze and process the position alias\n    processPositionAlias(ast);\n    // Check configuration for CBO first.\n    runCBO = runCBO && HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVE_CBO_ENABLED);\n\n    // analyze create table command\n    PreCboCtx cboCtx = runCBO ? new PreCboCtx() : null;\n    if (ast.getToken().getType() == HiveParser.TOK_CREATETABLE) {\n      // if it is not CTAS, we don\\'t need to go further and just return\n      if ((child = analyzeCreateTable(ast, qb, cboCtx)) == null) {\n        return;\n      }\n    } else {\n      SessionState.get().setCommandType(HiveOperation.QUERY);\n    }\n\n    // analyze create view command\n    if (ast.getToken().getType() == HiveParser.TOK_CREATEVIEW ||\n        (ast.getToken().getType() == HiveParser.TOK_ALTERVIEW\n          && ast.getChild(1).getType() == HiveParser.TOK_QUERY)) {\n      child = analyzeCreateView(ast, qb);\n      SessionState.get().setCommandType(HiveOperation.CREATEVIEW);\n      if (child == null) {\n        return;\n      }\n      viewSelect = child;\n      // prevent view from referencing itself\n      viewsExpanded.add(createVwDesc.getViewName());\n    }\n\n    // continue analyzing from the child ASTNode.\n    Phase1Ctx ctx_1 = initPhase1Ctx();\n    if (!doPhase1(child, qb, ctx_1, cboCtx)) {\n      // if phase1Result false return\n      return;\n    }\n\n    LOG.info(\"Completed phase 1 of Semantic Analysis\");\n\n    getMetaData(qb);\n    LOG.info(\"Completed getting MetaData in Semantic Analysis\");\n\n    // Note: for now, we don\\'t actually pass the queryForCbo to CBO, because it accepts qb, not\n    //    AST, and can also access all the private stuff in SA. We rely on the fact that CBO\n    //    ignores the unknown tokens (create table, destination), so if the query is otherwise ok,\n    //    it is as if we did remove those and gave CBO the proper AST. That is kinda hacky.\n    if (runCBO) {\n      ASTNode queryForCbo = ast;\n      if (cboCtx.type == PreCboCtx.Type.CTAS) {\n        queryForCbo = cboCtx.nodeOfInterest; // nodeOfInterest is the query\n      }\n      runCBO = canHandleAstForCbo(queryForCbo, qb, cboCtx);\n    }\n\n    // Save the result schema derived from the sink operator produced\n    // by genPlan. This has the correct column names, which clients\n    // such as JDBC would prefer instead of the c0, c1 we\\'ll end\n    // up with later.\n    Operator sinkOp = null;\n\n    if (runCBO) {\n      disableJoinMerge = true;\n      OptiqBasedPlanner optiqPlanner = new OptiqBasedPlanner();\n      boolean reAnalyzeAST = false;\n\n      try {\n        // 1. Gen Optimized AST\n        ASTNode newAST = optiqPlanner.getOptimizedAST(prunedPartitions);\n\n        // 1.1. Fix up the query for insert/ctas\n        newAST = fixUpCtasAndInsertAfterCbo(ast, newAST, cboCtx);\n\n        // 2. Regen OP plan from optimized AST\n        init(false);\n        if (cboCtx.type == PreCboCtx.Type.CTAS) {\n          // Redo create-table analysis, because it\\'s not part of doPhase1.\n          newAST = reAnalyzeCtasAfterCbo(newAST);\n        }\n        ctx_1 = initPhase1Ctx();\n        if (!doPhase1(newAST, qb, ctx_1, null)) {\n          throw new RuntimeException(\n              \\\"Couldn\\'t do phase1 on CBO optimized query plan\\\");\n        }\n        // unfortunately making prunedPartitions immutable is not possible here\n        // with SemiJoins not all tables are costed in CBO,\n        // so their PartitionList is not evaluated until the run phase.\n        //prunedPartitions = ImmutableMap.copyOf(prunedPartitions);\n        getMetaData(qb);\n\n        disableJoinMerge = false;\n        sinkOp = genPlan(qb);\n        LOG.info(\"CBO Succeeded; optimized logical plan.\");\n        LOG.debug(newAST.dump());\n\n        /*\n         * Use non CBO Result Set Schema so as to preserve user specified names.\n         * Hive seems to have bugs with OB/LIMIT in sub queries. // 3. Reset\n         * result set schema resultSchema =\n         * convertRowSchemaToResultSetSchema(opParseCtx.get(sinkOp)\n         * .getRowResolver(), true);\n         */\n      } catch (Exception e) {\n        boolean isMissingStats = optiqPlanner.noColsMissingStats.get() > 0;\n        if (isMissingStats) {\n          LOG.error(\"CBO failed due to missing column stats (see previous errors), skipping CBO\");\n        } else {\n          LOG.error(\"CBO failed, skipping CBO. \", e);\n        }\n        if (!conf.getBoolVar(ConfVars.HIVE_IN_TEST) || isMissingStats\n            || e instanceof OptiqSemanticException) {\n          reAnalyzeAST = true;\n        } else if (e instanceof SemanticException) {\n          throw (SemanticException)e;\n        } else if (e instanceof RuntimeException) {\n          throw (RuntimeException)e;\n        } else {\n          throw new SemanticException(e);\n        }\n      } finally {\n        runCBO = false;\n        disableJoinMerge = false;\n        if (reAnalyzeAST) {\n          init(true);\n          prunedPartitions.clear();\n          analyzeInternal(ast);\n          return;\n        }\n      }\n    } else {\n      sinkOp = genPlan(qb);\n    }\n\n    if (createVwDesc != null)\n      resultSchema = convertRowSchemaToViewSchema(opParseCtx.get(sinkOp).getRowResolver());\n    else\n      resultSchema = convertRowSchemaToResultSetSchema(opParseCtx.get(sinkOp).getRowResolver(),\n          HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVE_RESULTSET_USE_UNIQUE_COLUMN_NAMES));\n\n    ParseContext pCtx = new ParseContext(conf, qb, child, opToPartPruner,\n        opToPartList, topOps, topSelOps, opParseCtx, joinContext, smbMapJoinContext,\n        topToTable, topToTableProps, fsopToTable,\n        loadTableWork, loadFileWork, ctx, idToTableNameMap, destTableId, uCtx,\n        listMapJoinOpsNoReducer, groupOpToInputTables, prunedPartitions,\n        opToSamplePruner, globalLimitCtx, nameToSplitSample, inputs, rootTasks,\n        opToPartToSkewedPruner, viewAliasToInput,\n        reduceSinkOperatorsAddedByEnforceBucketingSorting, queryProperties);\n\n    if (createVwDesc != null) {\n      saveViewDefinition();\n\n      // validate the create view statement\n      // at this point, the createVwDesc gets all the information for semantic check\n      validateCreateView(createVwDesc);\n\n      // Since we\\'re only creating a view (not executing it), we\n      // don\\'t need to optimize or translate the plan (and in fact, those\n      // procedures can interfere with the view creation). So\n      // skip the rest of this method.\n      ctx.setResDir(null);\n      ctx.setResFile(null);\n\n      try {\n        PlanUtils.addInputsForView(pCtx);\n      } catch (HiveException e) {\n        throw new SemanticException(e);\n      }\n      return;\n    }\n\n    // Generate table access stats if required\n    if (HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVE_STATS_COLLECT_TABLEKEYS) == true) {\n      TableAccessAnalyzer tableAccessAnalyzer = new TableAccessAnalyzer(pCtx);\n      setTableAccessInfo(tableAccessAnalyzer.analyzeTableAccess());\n    }\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Before logical optimization\\\\n\" + Operator.toString(pCtx.getTopOps().values()));\n    }\n\n    Optimizer optm = new Optimizer();\n    optm.setPctx(pCtx);\n    optm.initialize(conf);\n    pCtx = optm.optimize();\n\n    FetchTask origFetchTask = pCtx.getFetchTask();\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"After logical optimization\\\\n\" + Operator.toString(pCtx.getTopOps().values()));\n    }\n\n    // Generate column access stats if required - wait until column pruning takes place\n    // during optimization\n    boolean isColumnInfoNeedForAuth = SessionState.get().isAuthorizationModeV2()\n        && HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVE_AUTHORIZATION_ENABLED);\n\n    if (isColumnInfoNeedForAuth\n        || HiveConf.getBoolVar(this.conf, HiveConf.ConfVars.HIVE_STATS_COLLECT_SCANCOLS) == true) {\n      ColumnAccessAnalyzer columnAccessAnalyzer = new ColumnAccessAnalyzer(pCtx);\n      setColumnAccessInfo(columnAccessAnalyzer.analyzeColumnAccess());\n    }\n\n    if (!ctx.getExplainLogical()) {\n      // At this point we have the complete operator tree\n      // from which we want to create the map-reduce plan\n      TaskCompiler compiler = TaskCompilerFactory.getCompiler(conf, pCtx);\n      compiler.init(conf, console, db);\n      compiler.compile(pCtx, rootTasks, inputs, outputs);\n      fetchTask = pCtx.getFetchTask();\n    }\n\n    LOG.info(\"Completed plan generation\");\n\n    // put accessed columns to readEntity\n    if (HiveConf.getBoolVar(this.conf, HiveConf.ConfVars.HIVE_STATS_COLLECT_SCANCOLS)) {\n      putAccessedColumnsToReadEntity(inputs, columnAccessInfo);\n    }\n\n    if (!ctx.getExplain()) {\n      // if desired check we\\'re not going over partition scan limits\n      enforceScanLimits(pCtx, origFetchTask);\n    }\n\n    return;\n  }"
        }
    },
    {
        "filename": "HIVE-11285.json",
        "creation_time": "2015-07-16T20:46:22.000+0000",
        "bug_report": {
            "Title": "ObjectInspector for partition columns in FetchOperator in SMBJoin causes exception",
            "Description": "The bug report describes a `ClassCastException` occurring during a Hive job when attempting to join two tables, `smb_table` and `smb_table_part`. The error arises specifically when the system tries to cast an `IntWritable` object to a `java.lang.Integer`, indicating a type mismatch in the data being processed. This issue is likely related to the data types of the columns involved in the join operation, particularly the partition column `p1` in `smb_table_part`.",
            "StackTrace": [
                "2015-07-15 13:39:04,333 WARN main org.apache.hadoop.mapred.YarnChild: Exception running child : java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row",
                "{\"key\":1,\"value\":\"One\"}",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:185)",
                "at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)",
                "at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)",
                "Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row",
                "{\"key\":1,\"value\":\"One\"}",
                "at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:503)",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:176)",
                "... 8 more",
                "Caused by: java.lang.RuntimeException: Map local work failed",
                "at org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.fetchOneRow(SMBMapJoinOperator.java:569)",
                "at org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.fetchNextGroup(SMBMapJoinOperator.java:429)",
                "at org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.processOp(SMBMapJoinOperator.java:260)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)",
                "at org.apache.hadoop.hive.ql.exec.FilterOperator.processOp(FilterOperator.java:120)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)",
                "at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:95)",
                "at org.apache.hadoop.hive.ql.exec.MapOperator$MapOpCtx.forward(MapOperator.java:157)",
                "at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:493)",
                "... 9 more",
                "Caused by: java.lang.ClassCastException: org.apache.hadoop.io.IntWritable cannot be cast to java.lang.Integer",
                "at org.apache.hadoop.hive.serde2.objectinspector.primitive.JavaIntObjectInspector.getPrimitiveWritableObject(JavaIntObjectInspector.java:35)",
                "at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.copyToStandardObject(ObjectInspectorUtils.java:305)",
                "at org.apache.hadoop.hive.ql.exec.JoinUtil.computeValues(JoinUtil.java:193)",
                "at org.apache.hadoop.hive.ql.exec.CommonJoinOperator.getFilteredValue(CommonJoinOperator.java:408)",
                "at org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.processOp(SMBMapJoinOperator.java:270)",
                "at org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator.fetchOneRow(SMBMapJoinOperator.java:558)",
                "... 17 more"
            ],
            "RootCause": "The root cause of the issue is a ClassCastException due to a type mismatch when processing the join operation between `smb_table` and `smb_table_part`. Specifically, the system attempts to cast an `IntWritable` to a `java.lang.Integer`, which is not valid.",
            "StepsToReproduce": [
                "Create a table `data_table` with columns `key` and `value`.",
                "Load data into `data_table` from a local file.",
                "Create a partitioned table `smb_table_part` with a partition column `p1` of type DECIMAL.",
                "Insert data into `smb_table` and `smb_table_part`.",
                "Set Hive execution engine and optimization settings.",
                "Execute a join query between `smb_table` and `smb_table_part`."
            ],
            "ExpectedBehavior": "The join operation should successfully return the joined results without any exceptions, processing all rows correctly.",
            "ObservedBehavior": "The join operation fails with a ClassCastException, preventing the successful execution of the query.",
            "Suggestions": "Ensure that the data types of the columns involved in the join operation are compatible. Specifically, check the data type of the partition column `p1` in `smb_table_part` and ensure it matches the expected type in the join condition.",
            "problem_location": {
                "files": [
                    "FetchOperator.java",
                    "SMBMapJoinOperator.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.ql.exec.FetchOperator",
                    "org.apache.hadoop.hive.ql.exec.SMBMapJoinOperator"
                ],
                "methods": [
                    "SMBMapJoinOperator.fetchOneRow",
                    "SMBMapJoinOperator.processOp"
                ]
            },
            "possible_fix": "Review the schema of the tables involved in the join and ensure that the data types are consistent. If necessary, modify the data type of the partition column `p1` in `smb_table_part` to match the expected type in the join operation."
        },
        "possible_fix_code": {
            "SMBMapJoinOperator.process": "  public void process(Object row, int tag) throws HiveException {\n\n    if (tag == posBigTable) {\n      if (inputFileChanged) {\n        if (firstFetchHappened) {\n          // we need to first join and flush out data left by the previous file.\n          joinFinalLeftData();\n        }\n        // set up the fetch operator for the new input file.\n        for (Map.Entry<String, MergeQueue> entry : aliasToMergeQueue.entrySet()) {\n          String alias = entry.getKey();\n          MergeQueue mergeQueue = entry.getValue();\n          setUpFetchContexts(alias, mergeQueue);\n        }\n        firstFetchHappened = false;\n        inputFileChanged = false;\n      }\n    }\n\n    if (!firstFetchHappened) {\n      firstFetchHappened = true;\n      // fetch the first group for all small table aliases\n      for (byte pos = 0; pos < order.length; pos++) {\n        if (pos != posBigTable) {\n          fetchNextGroup(pos);\n        }\n      }\n    }\n\n    byte alias = (byte) tag;\n\n    // compute keys and values as StandardObjects\n    List<Object> key = smbJoinComputeKeys(row, alias);\n\n    List<Object> value = getFilteredValue(alias, row);\n\n    // Ensure that the value is compatible with the expected type\n    if (value.get(0) instanceof IntWritable) {\n        value.set(0, ((IntWritable) value.get(0)).get());\n    }\n\n    //have we reached a new key group?\n    boolean nextKeyGroup = processKey(alias, key);\n    if (nextKeyGroup) {\n      //assert this.nextGroupStorage[alias].size() == 0;\n      this.nextGroupStorage[alias].addRow(value);\n      foundNextKeyGroup[tag] = true;\n      if (tag != posBigTable) {\n        return;\n      }\n    }\n\n    reportProgress();\n    numMapRowsRead++;\n\n    // the big table has reached a new key group. try to let the small tables\n    // catch up with the big table.\n    if (nextKeyGroup) {\n      assert tag == posBigTable;\n      List<Byte> smallestPos = null;\n      do {\n        smallestPos = joinOneGroup();\n        //jump out the loop if we need input from the big table\n      } while (smallestPos != null && smallestPos.size() > 0\n          && !smallestPos.contains(this.posBigTable));\n\n      return;\n    }\n\n    assert !nextKeyGroup;\n    candidateStorage[tag].addRow(value);\n  }"
        }
    },
    {
        "filename": "HIVE-10288.json",
        "creation_time": "2015-04-09T23:24:16.000+0000",
        "bug_report": {
            "Title": "Cannot call permanent UDFs",
            "Description": "After building the latest Hive binary from the trunk, an issue arises when attempting to call a permanent User Defined Function (UDF) after exiting and re-entering the CLI. The UDF call fails with a `NullPointerException`, while it succeeds immediately after registration. This behavior is inconsistent with the previous Apache Hive 1.0.0 release, where the UDF could be called without issues after exiting the CLI.",
            "StackTrace": [
                "15-04-13 17:04:54,435 ERROR org.apache.hadoop.hive.ql.Driver (SessionState.java:printError(958)) - FAILED: NullPointerException null",
                "java.lang.NullPointerException",
                "\tat org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc.newInstance(ExprNodeGenericFuncDesc.java:232)",
                "\tat org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory$DefaultExprProcessor.getXpathOrFuncExprNodeDesc(TypeCheckProcFactory.java:1048)",
                "\tat org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory$DefaultExprProcessor.process(TypeCheckProcFactory.java:1265)",
                "\tat org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90)",
                "\tat org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:95)",
                "\tat org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:79)",
                "\tat org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.walk(DefaultGraphWalker.java:133)",
                "\tat org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.startWalking(DefaultGraphWalker.java:110)",
                "\tat org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.genExprNode(TypeCheckProcFactory.java:205)",
                "\tat org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.genExprNode(TypeCheckProcFactory.java:149)",
                "\tat org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genAllExprNodeDesc(SemanticAnalyzer.java:10383)",
                "\tat org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genExprNodeDesc(SemanticAnalyzer.java:10338)",
                "\tat org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genSelectPlan(SemanticAnalyzer.java:3815)",
                "\tat org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genSelectPlan(SemanticAnalyzer.java:3594)",
                "\tat org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPostGroupByBodyPlan(SemanticAnalyzer.java:8864)",
                "\tat org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genBodyPlan(SemanticAnalyzer.java:8819)",
                "\tat org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:9663)",
                "\tat org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:9556)",
                "\tat org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genOPTree(SemanticAnalyzer.java:9992)",
                "\tat org.apache.hadoop.hive.ql.parse.CalcitePlanner.genOPTree(CalcitePlanner.java:306)",
                "\tat org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10003)",
                "\tat org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:195)",
                "\tat org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:224)",
                "\tat org.apache.hadoop.hive.ql.Driver.compile(Driver.java:424)",
                "\tat org.apache.hadoop.hive.ql.Driver.compile(Driver.java:308)",
                "\tat org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1122)",
                "\tat org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1170)",
                "\tat org.apache.hadoop.hive.ql.Driver.run(Driver.java:1059)",
                "\tat org.apache.hadoop.hive.ql.Driver.run(Driver.java:1049)",
                "\tat org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:213)",
                "\tat org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:165)",
                "\tat org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)",
                "\tat org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:736)",
                "\tat org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:681)",
                "\tat org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)",
                "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)",
                "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "\tat java.lang.reflect.Method.invoke(Method.java:483)",
                "\tat org.apache.hadoop.util.RunJar.main(RunJar.java:212)"
            ],
            "RootCause": "The `NullPointerException` is likely caused by the `children` parameter being `null` when passed to `ExprNodeGenericFuncDesc.newInstance`. This can occur if the `TypeCheckProcFactory` class fails to properly generate or process the parameters for the function node.",
            "StepsToReproduce": [
                "Build the latest Hive binary from the trunk.",
                "Register a permanent UDF in the CLI.",
                "Exit the CLI.",
                "Reopen the CLI and attempt to call the registered UDF."
            ],
            "ExpectedBehavior": "The UDF should be callable after exiting and re-entering the CLI without any exceptions.",
            "ObservedBehavior": "A `NullPointerException` is thrown when attempting to call the UDF after reopening the CLI.",
            "Suggestions": "Investigate the `TypeCheckProcFactory` class to ensure it correctly generates the parameters for `ExprNodeGenericFuncDesc.newInstance`. Ensure that the `children` list is not null before passing it to the method.",
            "problem_location": {
                "files": [
                    "ql/src/java/org/apache/hadoop/hive/ql/plan/ExprNodeGenericFuncDesc.java",
                    "ql/src/java/org/apache/hadoop/hive/ql/parse/TypeCheckProcFactory.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc",
                    "org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory"
                ],
                "methods": [
                    "ExprNodeGenericFuncDesc.newInstance",
                    "TypeCheckProcFactory$DefaultExprProcessor.getXpathOrFuncExprNodeDesc"
                ]
            },
            "possible_fix": "Ensure that the `children` parameter is properly initialized and not null before calling `ExprNodeGenericFuncDesc.newInstance`. This may involve adding checks in the `TypeCheckProcFactory` methods that generate or process the parameters."
        },
        "possible_fix_code": {
            "ExprNodeGenericFuncDesc.newInstance": "  public static ExprNodeGenericFuncDesc newInstance(GenericUDF genericUDF,\n    List<ExprNodeDesc> children) throws UDFArgumentException {\n    if (children == null) {\n        throw new UDFArgumentException(\"Children list cannot be null\");\n    }\n    return newInstance(genericUDF, null, children);\n  }"
        }
    },
    {
        "filename": "HIVE-8771.json",
        "creation_time": "2014-11-07T00:30:25.000+0000",
        "bug_report": {
            "Title": "Abstract merge file operator does not move/rename incompatible files correctly",
            "Description": "The `AbstractFileMergeOperator` in Hive is failing to move incompatible files to the final destination due to a `FileNotFoundException`. This occurs when the destination path already exists as a file instead of a directory, which is a requirement for the operation to succeed. The issue manifests during the execution of the `closeOp` method, where the operator attempts to rename the output path to a final path. If the final path already exists as a file, the operation fails, leading to an IOException. This bug specifically affects the execution of the `orc_merge_incompat2.q` test case under CentOS.",
            "StackTrace": [
                "2014-11-05 02:38:56,588 DEBUG fs.FileSystem (RawLocalFileSystem.java:rename(337)) - Falling through to a copy of file:/home/prasanth/hive/itests/qtest/target/warehouse/orc_merge5a/st=80.0/000000_0 to file:/home/prasanth/hive/itests/qtest/target/tmp/scratchdir/prasanth/0de64e52-6615-4c5a-bdfb-c3b2c28131f6/hive_2014-11-05_02-38-55_511_7578595409877157627-1/_tmp.-ext-10000/000000_0/000000_0",
                "2014-11-05 02:38:56,589 INFO  mapred.LocalJobRunner (LocalJobRunner.java:runTasks(456)) - map task executor complete.",
                "2014-11-05 02:38:56,590 WARN  mapred.LocalJobRunner (LocalJobRunner.java:run(560)) - job_local1144733438_0036",
                "java.lang.Exception: java.io.IOException: org.apache.hadoop.hive.ql.metadata.HiveException: Failed to close AbstractFileMergeOperator",
                "Caused by: java.io.FileNotFoundException: Destination exists and is not a directory: /home/prasanth/hive/itests/qtest/target/tmp/scratchdir/prasanth/0de64e52-6615-4c5a-bdfb-c3b2c28131f6/hive_2014-11-05_02-38-55_511_7578595409877157627-1/_tmp.-ext-10000/000000_0"
            ],
            "RootCause": "The root cause of the issue is that the `finalPath` or `outPath` in the `AbstractFileMergeOperator` is not being set correctly, leading to a conflict where the destination path already exists as a file instead of a directory. This results in a `FileNotFoundException` when the `closeOp` method attempts to rename or move files.",
            "StepsToReproduce": [
                "Run the `orc_merge_incompat2.q` test case under CentOS.",
                "Ensure that the destination path for the merge operation already contains a file instead of a directory.",
                "Observe the resulting IOException indicating that the destination exists and is not a directory."
            ],
            "ExpectedBehavior": "The `AbstractFileMergeOperator` should successfully move incompatible files to the final destination without throwing an exception, provided that the destination path is a valid directory.",
            "ObservedBehavior": "The operation fails with a `FileNotFoundException` indicating that the destination exists and is not a directory, preventing the successful completion of the merge operation.",
            "Suggestions": "To resolve this issue, ensure that the `finalPath` and `outPath` are set correctly to avoid conflicts with existing files. Additionally, modify the `renameOrMoveFiles` method to handle cases where the destination is a file more gracefully, possibly by deleting or renaming the existing file before proceeding.",
            "problem_location": {
                "files": [
                    "ql/src/java/org/apache/hadoop/hive/ql/exec/AbstractFileMergeOperator.java",
                    "ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.ql.exec.AbstractFileMergeOperator",
                    "org.apache.hadoop.hive.ql.exec.Utilities"
                ],
                "methods": [
                    "AbstractFileMergeOperator.closeOp",
                    "Utilities.renameOrMoveFiles",
                    "AbstractFileMergeOperator.jobCloseOp",
                    "AbstractFileMergeOperator.initializeOp",
                    "AbstractFileMergeOperator.updatePaths"
                ]
            },
            "possible_fix": "Modify the `renameOrMoveFiles` method to check if the destination path exists as a file and handle it appropriately, such as by deleting the file or renaming it before attempting to move the source file."
        },
        "possible_fix_code": {
            "AbstractFileMergeOperator.closeOp": "  public void closeOp(boolean abort) throws HiveException {\n    try {\n      if (!exception) {\n        FileStatus fss = fs.getFileStatus(outPath);\n        if (!fs.rename(outPath, finalPath)) {\n          throw new IOException(\n              \"Unable to rename \" + outPath + \" to \" + finalPath);\n        }\n        LOG.info(\"renamed path \" + outPath + \" to \" + finalPath + \" . File\" +\n            \" size is \" + fss.getLen());\n\n        // move any incompatible files to final path\n        if (!incompatFileSet.isEmpty()) {\n          for (Path incompatFile : incompatFileSet) {\n            String fileName = incompatFile.getName();\n            Path destFile = new Path(finalPath.getParent(), fileName);\n            // Check if destination exists and is a file\n            if (fs.exists(destFile) && !fs.isDirectory(destFile)) {\n              // Handle existing file: either delete or rename it\n              fs.delete(destFile, false); // or use fs.rename(destFile, new Path(destFile.toString() + \"_old\"));\n            }\n            try {\n              Utilities.renameOrMoveFiles(fs, incompatFile, destFile);\n              LOG.info(\"Moved incompatible file \" + incompatFile + \" to \" + destFile);\n            } catch (HiveException e) {\n              LOG.error(\"Unable to move \" + incompatFile + \" to \" + destFile);\n              throw new IOException(e);\n            }\n          }\n        }\n      } else {\n        if (!autoDelete) {\n          fs.delete(outPath, true);\n        }\n      }\n    } catch (IOException e) {\n      throw new HiveException(\"Failed to close AbstractFileMergeOperator\", e);\n    }\n  }"
        }
    },
    {
        "filename": "HIVE-8008.json",
        "creation_time": "2014-09-05T23:00:11.000+0000",
        "bug_report": {
            "Title": "NPE while reading null decimal value",
            "Description": "The issue arises when querying the `dec_test` table, which contains a decimal column. If a row in this table has a decimal value of 9999999999.5, executing the query `select * from dec_test;` results in a NullPointerException (NPE). The stack trace indicates that the error occurs during the fetching of results, specifically in the `FetchTask`, `ListSinkOperator`, and `LazySimpleSerDe` classes. The NPE is likely due to uninitialized or null objects in these classes, particularly when handling the serialization of the decimal value.",
            "StackTrace": [
                "2014-09-05 14:08:56,023 ERROR [main]: CliDriver (SessionState.java:printError(545)) - Failed with exception java.io.IOException:org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.NullPointerException",
                "java.io.IOException: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.NullPointerException",
                "at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:151)",
                "at org.apache.hadoop.hive.ql.Driver.getResults(Driver.java:1531)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:285)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:220)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:423)",
                "at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:792)",
                "at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:686)",
                "at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:625)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.util.RunJar.main(RunJar.java:212)",
                "Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.NullPointerException",
                "at org.apache.hadoop.hive.ql.exec.ListSinkOperator.processOp(ListSinkOperator.java:90)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:796)",
                "at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:87)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:796)",
                "at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:92)",
                "at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:544)",
                "at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:536)",
                "at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:137)",
                "... 12 more",
                "Caused by: java.lang.NullPointerException",
                "at org.apache.hadoop.hive.serde2.lazy.LazyUtils.writePrimitiveUTF8(LazyUtils.java:265)",
                "at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serialize(LazySimpleSerDe.java:486)",
                "at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serializeField(LazySimpleSerDe.java:439)",
                "at org.apache.hadoop.hive.serde2.DelimitedJSONSerDe.serializeField(DelimitedJSONSerDe.java:71)",
                "at org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe.serializeField(LazySimpleSerDe.java:423)",
                "at org.apache.hadoop.hive.ql.exec.DefaultFetchFormatter.convert(DefaultFetchFormatter.java:70)",
                "at org.apache.hadoop.hive.ql.exec.DefaultFetchFormatter.convert(DefaultFetchFormatter.java:39)",
                "at org.apache.hadoop.hive.ql.exec.ListSinkOperator.processOp(ListSinkOperator.java:87)",
                "... 19 more"
            ],
            "RootCause": "The NullPointerException is likely caused by uninitialized or null objects in the `FetchTask`, `ListSinkOperator`, or `LazySimpleSerDe` classes. Specifically, the `sink` in `FetchTask`, the `fetcher` in `ListSinkOperator`, and the `objInspector` in `LazySimpleSerDe` may not be properly initialized or may be receiving null values.",
            "StepsToReproduce": [
                "Create a table `dec_test` with a decimal column.",
                "Insert a row with a decimal value of 9999999999.5.",
                "Execute the query `select * from dec_test;`."
            ],
            "ExpectedBehavior": "The query should return the row with the decimal value without throwing any exceptions.",
            "ObservedBehavior": "The query throws a NullPointerException, causing the application to crash.",
            "Suggestions": "Check the initialization of the `sink` in `FetchTask`, the `fetcher` in `ListSinkOperator`, and the `objInspector` in `LazySimpleSerDe`. Ensure that they are properly initialized and that the data being passed is valid.",
            "problem_location": {
                "files": [
                    "FetchTask.java",
                    "ListSinkOperator.java",
                    "LazySimpleSerDe.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.ql.exec.FetchTask",
                    "org.apache.hadoop.hive.ql.exec.ListSinkOperator",
                    "org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe"
                ],
                "methods": [
                    "FetchTask.fetch",
                    "ListSinkOperator.processOp",
                    "LazySimpleSerDe.serialize"
                ]
            },
            "possible_fix": "Ensure that the `sink` in `FetchTask`, the `fetcher` in `ListSinkOperator`, and the `objInspector` in `LazySimpleSerDe` are properly initialized before they are used. Add null checks where necessary to prevent the NullPointerException."
        },
        "possible_fix_code": {
            "FetchTask.fetch": "  public boolean fetch(List res) throws IOException, CommandNeedRetryException {\n    if (sink == null) {\n      throw new IllegalStateException(\"Sink is not initialized.\");\n    }\n    sink.reset(res);\n    int rowsRet = work.getLeastNumRows();\n    if (rowsRet <= 0) {\n      rowsRet = work.getLimit() >= 0 ? Math.min(work.getLimit() - totalRows, maxRows) : maxRows;\n    }\n    try {\n      if (rowsRet <= 0) {\n        fetch.clearFetchContext();\n        return false;\n      }\n      boolean fetched = false;\n      while (sink.getNumRows() < rowsRet) {\n        if (!fetch.pushRow()) {\n          if (work.getLeastNumRows() > 0) {\n            throw new CommandNeedRetryException();\n          }\n          return fetched;\n        }\n        fetched = true;\n      }\n      return true;\n    } catch (CommandNeedRetryException e) {\n      throw e;\n    } catch (IOException e) {\n      throw e;\n    } catch (Exception e) {\n      throw new IOException(e);\n    } finally {\n      totalRows += sink.getNumRows();\n    }\n  }",
            "ListSinkOperator.processOp": "  public void processOp(Object row, int tag) throws HiveException {\n    if (fetcher == null) {\n      throw new IllegalStateException(\"Fetcher is not initialized.\");\n    }\n    if (inputObjInspectors == null || inputObjInspectors.length == 0) {\n      throw new IllegalStateException(\"Input object inspectors are not initialized.\");\n    }\n    try {\n      res.add(fetcher.convert(row, inputObjInspectors[0]));\n      numRows++;\n    } catch (Exception e) {\n      throw new HiveException(e);\n    }\n  }",
            "LazySimpleSerDe.serialize": "  public static void serialize(ByteStream.Output out, Object obj,\n      ObjectInspector objInspector, byte[] separators, int level,\n      Text nullSequence, boolean escaped, byte escapeChar, boolean[] needsEscape)\n      throws IOException, SerDeException {\n\n    if (obj == null) {\n      out.write(nullSequence.getBytes(), 0, nullSequence.getLength());\n      return;\n    }\n    if (objInspector == null) {\n      throw new IllegalStateException(\"Object inspector is not initialized.\");\n    }\n\n    char separator;\n    List<?> list;\n    switch (objInspector.getCategory()) {\n    case PRIMITIVE:\n      LazyUtils.writePrimitiveUTF8(out, obj,\n          (PrimitiveObjectInspector) objInspector, escaped, escapeChar,\n          needsEscape);\n      return;\n    case LIST:\n      separator = (char) LazyUtils.getSeparator(separators, level);\n      ListObjectInspector loi = (ListObjectInspector) objInspector;\n      list = loi.getList(obj);\n      ObjectInspector eoi = loi.getListElementObjectInspector();\n      if (list == null) {\n        out.write(nullSequence.getBytes(), 0, nullSequence.getLength());\n      } else {\n        for (int i = 0; i < list.size(); i++) {\n          if (i > 0) {\n            out.write(separator);\n          }\n          serialize(out, list.get(i), eoi, separators, level + 1, nullSequence,\n              escaped, escapeChar, needsEscape);\n        }\n      }\n      return;\n    case MAP:\n      separator = (char) LazyUtils.getSeparator(separators, level);\n      char keyValueSeparator =\n           (char) LazyUtils.getSeparator(separators, level + 1);\n\n      MapObjectInspector moi = (MapObjectInspector) objInspector;\n      ObjectInspector koi = moi.getMapKeyObjectInspector();\n      ObjectInspector voi = moi.getMapValueObjectInspector();\n      Map<?, ?> map = moi.getMap(obj);\n      if (map == null) {\n        out.write(nullSequence.getBytes(), 0, nullSequence.getLength());\n      } else {\n        boolean first = true;\n        for (Map.Entry<?, ?> entry : map.entrySet()) {\n          if (first) {\n            first = false;\n          } else {\n            out.write(separator);\n          }\n          serialize(out, entry.getKey(), koi, separators, level + 2,\n              nullSequence, escaped, escapeChar, needsEscape);\n          out.write(keyValueSeparator);\n          serialize(out, entry.getValue(), voi, separators, level + 2,\n              nullSequence, escaped, escapeChar, needsEscape);\n        }\n      }\n      return;\n    case STRUCT:\n      separator = (char) LazyUtils.getSeparator(separators, level);\n      StructObjectInspector soi = (StructObjectInspector) objInspector;\n      List<? extends StructField> fields = soi.getAllStructFieldRefs();\n      list = soi.getStructFieldsDataAsList(obj);\n      if (list == null) {\n        out.write(nullSequence.getBytes(), 0, nullSequence.getLength());\n      } else {\n        for (int i = 0; i < list.size(); i++) {\n          if (i > 0) {\n            out.write(separator);\n          }\n          serialize(out, list.get(i), fields.get(i).getFieldObjectInspector(),\n              separators, level + 1, nullSequence, escaped, escapeChar,\n              needsEscape);\n        }\n      }\n      return;\n    case UNION:\n      separator = (char) LazyUtils.getSeparator(separators, level);\n      UnionObjectInspector uoi = (UnionObjectInspector) objInspector;\n      List<? extends ObjectInspector> ois = uoi.getObjectInspectors();\n      if (ois == null) {\n        out.write(nullSequence.getBytes(), 0, nullSequence.getLength());\n      } else {\n        LazyUtils.writePrimitiveUTF8(out, new Byte(uoi.getTag(obj)),\n            PrimitiveObjectInspectorFactory.javaByteObjectInspector,\n            escaped, escapeChar, needsEscape);\n        out.write(separator);\n        serialize(out, uoi.getField(obj), ois.get(uoi.getTag(obj)),\n            separators, level + 1, nullSequence, escaped, escapeChar,\n            needsEscape);\n      }\n      return;\n    default:\n      break;\n    }\n\n    throw new RuntimeException(\"Unknown category type: \"\n        + objInspector.getCategory());\n  }"
        }
    },
    {
        "filename": "HIVE-6915.json",
        "creation_time": "2014-04-15T20:20:15.000+0000",
        "bug_report": {
            "Title": "Hive Hbase queries fail on secure Tez cluster",
            "Description": "Hive queries that interact with HBase are failing in a secure Tez cluster environment due to Kerberos authentication issues. The error message indicates that the application is unable to find a valid Kerberos Ticket Granting Ticket (TGT), which is essential for establishing a secure connection. The stack trace reveals that the failure occurs during the SASL authentication process, specifically when attempting to connect to HBase. The issue arises despite the client performing 'kinit' to obtain credentials, suggesting a potential misconfiguration in the Kerberos setup or the HBase security configuration.",
            "StackTrace": [
                "2014-04-14 13:47:05,644 FATAL [InputInitializer [Map 1] #0] org.apache.hadoop.ipc.RpcClient: SASL authentication failed. The most likely cause is missing or invalid credentials. Consider 'kinit'.",
                "javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]",
                "at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:212)",
                "at org.apache.hadoop.hbase.security.HBaseSaslRpcClient.saslConnect(HBaseSaslRpcClient.java:152)",
                "at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupSaslConnection(RpcClient.java:792)",
                "at org.apache.hadoop.hbase.ipc.RpcClient$Connection.access$800(RpcClient.java:349)",
                "at org.apache.hadoop.hbase.ipc.RpcClient$Connection$2.run(RpcClient.java:918)",
                "at org.apache.hadoop.hbase.ipc.RpcClient$Connection$2.run(RpcClient.java:915)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1557)",
                "at org.apache.hadoop.hbase.ipc.RpcClient$Connection.setupIOstreams(RpcClient.java:915)",
                "at org.apache.hadoop.hbase.ipc.RpcClient$Connection.writeRequest(RpcClient.java:1065)",
                "at org.apache.hadoop.hbase.ipc.RpcClient$Connection.tracedWriteRequest(RpcClient.java:1032)",
                "at org.apache.hadoop.hbase.ipc.RpcClient.call(RpcClient.java:1474)",
                "at org.apache.hadoop.hbase.ipc.RpcClient.callBlockingMethod(RpcClient.java:1684)",
                "at org.apache.hadoop.hbase.ipc.RpcClient$BlockingRpcChannelImplementation.callBlockingMethod(RpcClient.java:1737)",
                "at org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$BlockingStub.execService(ClientProtos.java:29288)",
                "at org.apache.hadoop.hbase.protobuf.ProtobufUtil.execService(ProtobufUtil.java:1562)",
                "at org.apache.hadoop.hbase.ipc.RegionCoprocessorRpcChannel$1.call(RegionCoprocessorRpcChannel.java:87)",
                "at org.apache.hadoop.hbase.ipc.RegionCoprocessorRpcChannel$1.call(RegionCoprocessorRpcChannel.java:84)",
                "at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries(RpcRetryingCaller.java:121)",
                "at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries(RpcRetryingCaller.java:97)",
                "at org.apache.hadoop.hbase.ipc.RegionCoprocessorRpcChannel.callExecService(RegionCoprocessorRpcChannel.java:90)",
                "at org.apache.hadoop.hbase.ipc.CoprocessorRpcChannel.callBlockingMethod(CoprocessorRpcChannel.java:67)",
                "at org.apache.hadoop.hbase.protobuf.generated.AuthenticationProtos$AuthenticationService$BlockingStub.getAuthenticationToken(AuthenticationProtos.java:4512)",
                "at org.apache.hadoop.hbase.security.token.TokenUtil.obtainToken(TokenUtil.java:60)",
                "at org.apache.hadoop.hbase.security.token.TokenUtil$3.run(TokenUtil.java:174)",
                "at org.apache.hadoop.hbase.security.token.TokenUtil$3.run(TokenUtil.java:172)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1557)",
                "at org.apache.hadoop.hbase.security.token.TokenUtil.obtainTokenForJob(TokenUtil.java:171)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:601)",
                "at org.apache.hadoop.hbase.util.Methods.call(Methods.java:39)",
                "at org.apache.hadoop.hbase.security.User$SecureHadoopUser.obtainAuthTokenForJob(User.java:334)",
                "at org.apache.hadoop.hbase.mapred.TableMapReduceUtil.initCredentials(TableMapReduceUtil.java:201)",
                "at org.apache.hadoop.hive.hbase.HiveHBaseTableInputFormat.getSplits(HiveHBaseTableInputFormat.java:415)",
                "at org.apache.hadoop.hive.ql.io.HiveInputFormat.addSplitsForGroup(HiveInputFormat.java:291)",
                "at org.apache.hadoop.hive.ql.io.HiveInputFormat.getSplits(HiveInputFormat.java:372)",
                "at org.apache.hadoop.mapred.split.TezGroupedSplitsInputFormat.getSplits(TezGroupedSplitsInputFormat.java:68)",
                "at org.apache.tez.mapreduce.hadoop.MRHelpers.generateOldSplits(MRHelpers.java:263)",
                "at org.apache.tez.mapreduce.common.MRInputAMSplitGenerator.initialize(MRInputAMSplitGenerator.java:139)",
                "at org.apache.tez.dag.app.dag.RootInputInitializerRunner$InputInitializerCallable$1.run(RootInputInitializerRunner.java:154)",
                "at org.apache.tez.dag.app.dag.RootInputInitializerRunner$InputInitializerCallable$1.run(RootInputInitializerRunner.java:146)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1557)",
                "at org.apache.tez.dag.app.dag.RootInputInitializerRunner$InputInitializerCallable.call(RootInputInitializerRunner.java:146)",
                "at org.apache.tez.dag.app.dag.RootInputInitializerRunner$InputInitializerCallable.call(RootInputInitializerRunner.java:114)",
                "at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:166)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:722)",
                "Caused by: GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)",
                "at sun.security.jgss.krb5.Krb5InitCredential.getInstance(Krb5InitCredential.java:147)",
                "at sun.security.jgss.krb5.Krb5MechFactory.getCredentialElement(Krb5MechFactory.java:121)",
                "at sun.security.jgss.krb5.Krb5MechFactory.getMechanismContext(Krb5MechFactory.java:187)",
                "at sun.security.jgss.GSSManagerImpl.getMechanismContext(GSSManagerImpl.java:223)",
                "at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:212)",
                "at sun.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:179)",
                "at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:193)"
            ],
            "RootCause": "The root cause of the issue is the failure of Kerberos authentication due to the absence of a valid Ticket Granting Ticket (TGT). This indicates that the Kerberos client may not be properly configured or that the user has not successfully obtained a TGT.",
            "StepsToReproduce": [
                "1. Ensure the Hive execution engine is set to Tez.",
                "2. Attempt to run a Hive query that reads from or writes to HBase.",
                "3. Observe the error in the Tez application logs."
            ],
            "ExpectedBehavior": "The Hive query should execute successfully, reading from or writing to HBase without authentication errors.",
            "ObservedBehavior": "The Hive query fails with a SASL authentication error indicating that no valid credentials were provided.",
            "Suggestions": "1. Verify that the Kerberos client is properly configured. \n2. Ensure that the user has obtained a valid TGT using the 'kinit' command. \n3. Check the Kerberos configuration file (krb5.conf) for correctness. \n4. Confirm that the service principal names (SPNs) are correctly configured.",
            "problem_location": {
                "files": [
                    "HBaseSaslRpcClient.java",
                    "UserGroupInformation.java",
                    "HiveHBaseTableInputFormat.java"
                ],
                "classes": [
                    "org.apache.hadoop.hbase.security.HBaseSaslRpcClient",
                    "org.apache.hadoop.security.UserGroupInformation",
                    "org.apache.hadoop.hive.hbase.HiveHBaseTableInputFormat"
                ],
                "methods": [
                    "HBaseSaslRpcClient.saslConnect",
                    "UserGroupInformation.doAs",
                    "HiveHBaseTableInputFormat.getSplits"
                ]
            },
            "possible_fix": "Ensure that the Kerberos client is configured correctly and that the user has a valid TGT. This may involve checking the krb5.conf file and ensuring that the correct SPNs are being used."
        },
        "possible_fix_code": {
            "HiveHBaseTableInputFormat.getSplits": "  public InputSplit[] getSplits(JobConf jobConf, int numSplits) throws IOException {\n\n    // Obtain delegation tokens for the job\n    TableMapReduceUtil.initCredentials(jobConf);\n\n    // Ensure that the user has a valid Kerberos TGT\n    UserGroupInformation ugi = UserGroupInformation.getLoginUser();\n    if (ugi == null || !ugi.hasKerberosCredentials()) {\n        throw new IOException(\"User does not have valid Kerberos credentials. Please run 'kinit' to obtain a valid TGT.\");\n    }\n\n    String hbaseTableName = jobConf.get(HBaseSerDe.HBASE_TABLE_NAME);\n    setHTable(new HTable(HBaseConfiguration.create(jobConf), Bytes.toBytes(hbaseTableName)));\n    String hbaseColumnsMapping = jobConf.get(HBaseSerDe.HBASE_COLUMNS_MAPPING);\n    boolean doColumnRegexMatching = jobConf.getBoolean(HBaseSerDe.HBASE_COLUMNS_REGEX_MATCHING, true);\n\n    if (hbaseColumnsMapping == null) {\n      throw new IOException(\"hbase.columns.mapping required for HBase Table.\");\n    }\n\n    List<ColumnMapping> columnsMapping = null;\n    try {\n      columnsMapping = HBaseSerDe.parseColumnsMapping(hbaseColumnsMapping, doColumnRegexMatching);\n    } catch (SerDeException e) {\n      throw new IOException(e);\n    }\n\n    int iKey;\n\n    try {\n      iKey = HBaseSerDe.getRowKeyColumnOffset(columnsMapping);\n    } catch (SerDeException e) {\n      throw new IOException(e);\n    }\n\n    // Take filter pushdown into account while calculating splits\n    Scan scan = createFilterScan(jobConf, iKey,\n        getStorageFormatOfKey(columnsMapping.get(iKey).mappingSpec,\n            jobConf.get(HBaseSerDe.HBASE_TABLE_DEFAULT_STORAGE_TYPE, \"string\")));\n\n    // The list of families that have been added to the scan\n    List<String> addedFamilies = new ArrayList<String>();\n\n    for (int i = 0; i < columnsMapping.size(); i++) {\n      ColumnMapping colMap = columnsMapping.get(i);\n      if (colMap.hbaseRowKey) {\n        continue;\n      }\n\n      if (colMap.qualifierName == null) {\n        scan.addFamily(colMap.familyNameBytes);\n        addedFamilies.add(colMap.familyName);\n      } else {\n        if (!addedFamilies.contains(colMap.familyName)) {\n          scan.addColumn(colMap.familyNameBytes, colMap.qualifierNameBytes);\n        }\n      }\n    }\n    setScan(scan);\n\n    Job job = new Job(jobConf);\n    JobContext jobContext = ShimLoader.getHadoopShims().newJobContext(job);\n    Path[] tablePaths = FileInputFormat.getInputPaths(jobContext);\n\n    List<org.apache.hadoop.mapreduce.InputSplit> splits =\n      super.getSplits(jobContext);\n    InputSplit[] results = new InputSplit[splits.size()];\n\n    for (int i = 0; i < splits.size(); i++) {\n      results[i] = new HBaseSplit((TableSplit) splits.get(i), tablePaths[0]);\n    }\n\n    return results;\n  }"
        }
    },
    {
        "filename": "HIVE-12364.json",
        "creation_time": "2015-11-07T02:04:25.000+0000",
        "bug_report": {
            "Title": "Distcp job fails when run under Tez",
            "Description": "The issue arises when executing a Hive query that involves inserting data into a directory using the DistCp process under the Tez execution engine. The operation fails during the moveTask phase, specifically when attempting to move files from a temporary staging directory to the final destination. The error indicates an incompatibility between the input format class and the map compatibility mode in Hadoop, which is likely due to misconfiguration or version mismatches between Hive and Hadoop.",
            "StackTrace": [
                "org.apache.hadoop.hive.ql.metadata.Hive.moveFile(Hive.java:2665)",
                "org.apache.hadoop.hive.ql.exec.MoveTask.moveFile(MoveTask.java:105)",
                "org.apache.hadoop.hive.ql.exec.MoveTask.execute(MoveTask.java:222)",
                "org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:160)",
                "org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:89)",
                "org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1653)",
                "org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1414)",
                "org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1195)",
                "org.apache.hadoop.hive.ql.Driver.run(Driver.java:1059)",
                "org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:213)",
                "org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:165)",
                "org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)",
                "org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:736)",
                "org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:681)",
                "org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)"
            ],
            "RootCause": "The root cause of the failure is an incompatibility between the 'mapreduce.job.inputformat.class' and the map compatibility mode in Hadoop. This issue arises during the execution of the DistCp process initiated by the moveFile method in the Hive class.",
            "StepsToReproduce": [
                "Set the configuration 'hive.exec.copyfile.maxsize' to 40000.",
                "Execute the Hive query: 'insert overwrite into '/tmp/testinser' select * from customer;'",
                "Observe the failure during the moveTask phase."
            ],
            "ExpectedBehavior": "The DistCp process should successfully move files from the temporary staging directory to the specified destination directory without errors.",
            "ObservedBehavior": "The operation fails with a HiveException indicating an inability to move the source file due to incompatibility issues with the input format class and the map compatibility mode.",
            "Suggestions": "1. Review the Hadoop configuration files (e.g., core-site.xml, mapred-site.xml) to ensure that 'mapreduce.job.inputformat.class' is set correctly. 2. Verify that the compatibility mode settings in your Hadoop configuration align with the input format class being used. 3. Ensure that the versions of Hive and Hadoop are compatible with each other. 4. If using DistCp explicitly, check the parameters and configurations being passed to it.",
            "problem_location": {
                "files": [
                    "Hive.java",
                    "MoveTask.java",
                    "Hadoop23Shims.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.ql.metadata.Hive",
                    "org.apache.hadoop.hive.ql.exec.MoveTask",
                    "org.apache.hadoop.hive.shims.Hadoop23Shims"
                ],
                "methods": [
                    "Hive.moveFile",
                    "MoveTask.moveFile",
                    "Hadoop23Shims.runDistCp"
                ]
            },
            "possible_fix": "Check and update the Hadoop configuration settings related to 'mapreduce.job.inputformat.class' and ensure compatibility with the current Hive version. Additionally, review the DistCp usage and parameters to ensure they are correctly configured."
        },
        "possible_fix_code": {
            "Hive.moveFile": "  public static boolean moveFile(HiveConf conf, Path srcf, Path destf,\n      boolean replace, boolean isSrcLocal) throws HiveException {\n    boolean success = false;\n    FileSystem srcFs, destFs;\n    try {\n      destFs = destf.getFileSystem(conf);\n    } catch (IOException e) {\n      LOG.error(\"Failed to get dest fs\", e);\n      throw new HiveException(e.getMessage(), e);\n    }\n    try {\n      srcFs = srcf.getFileSystem(conf);\n    } catch (IOException e) {\n      LOG.error(\"Failed to get src fs\", e);\n      throw new HiveException(e.getMessage(), e);\n    }\n\n    // Check for compatibility between input format and map compatibility mode\n    String inputFormatClass = conf.get(\"mapreduce.job.inputformat.class\");\n    if (inputFormatClass == null || inputFormatClass.isEmpty()) {\n      throw new HiveException(\"Input format class is not set in the configuration.\");\n    }\n\n    //needed for perm inheritance.\n    boolean inheritPerms = HiveConf.getBoolVar(conf,\n        HiveConf.ConfVars.HIVE_WAREHOUSE_SUBDIR_INHERIT_PERMS);\n    HadoopShims shims = ShimLoader.getHadoopShims();\n    HadoopShims.HdfsFileStatus destStatus = null;\n\n    // If source path is a subdirectory of the destination path:\n    boolean destIsSubDir = isSubDir(srcf, destf, srcFs, destFs, isSrcLocal);\n    try {\n      if (inheritPerms || replace) {\n        try{\n          destStatus = shims.getFullFileStatus(conf, destFs, destf.getParent());\n          if (replace && !destIsSubDir) {\n            LOG.debug(\"The path \" + destf.toString() + \" is deleted\");\n            destFs.delete(destf, true);\n          }\n        } catch (FileNotFoundException ignore) {\n          if (inheritPerms) {\n            destStatus = shims.getFullFileStatus(conf, destFs, destf.getParent());\n          }\n        }\n      }\n      if (isSrcLocal) {\n        destFs.copyFromLocalFile(srcf, destf);\n        success = true;\n      } else {\n        if (needToCopy(srcf, destf, srcFs, destFs)) {\n          LOG.info(\"Copying source \" + srcf + \" to \" + destf + \" because HDFS encryption zones are different.\");\n          success = FileUtils.copy(srcf.getFileSystem(conf), srcf, destf.getFileSystem(conf), destf,\n              true,    // delete source\n              replace, // overwrite destination\n              conf);\n        } else {\n          if (destIsSubDir) {\n            FileStatus[] srcs = destFs.listStatus(srcf, FileUtils.HIDDEN_FILES_PATH_FILTER);\n            if (srcs.length == 0) {\n              success = true; // Nothing to move.\n            }\n\n            for (FileStatus status : srcs) {\n              Path destFile;\n              if (destFs.isDirectory(destf)) {\n                destFile = new Path(destf, status.getPath().getName());\n              } else {\n                destFile = destf;\n              }\n\n              if (destFs.exists(destFile)) {\n                if (!destFs.delete(destFile, true)) {\n                  throw new HiveException(String.format(\"File to replace could not be deleted: %s\", destFile));\n                }\n              }\n\n              if (!(destFs.rename(status.getPath(), destFile))) {\n                throw new HiveException(\"Unable to move source \" + status.getPath() + \" to destination \" + destf);\n              }\n            }\n\n            success = true;\n          } else {\n            success = destFs.rename(srcf, destf);\n          }\n        }\n      }\n\n      LOG.info((replace ? \"Replacing src:\" : \"Renaming src: \") + srcf.toString()\n          + \", dest: \" + destf.toString()  + \", Status:\" + success);\n    } catch (IOException ioe) {\n      throw new HiveException(\"Unable to move source \" + srcf + \" to destination \" + destf, ioe);\n    }\n\n    if (success && inheritPerms) {\n      try {\n        ShimLoader.getHadoopShims().setFullFileStatus(conf, destStatus, destFs, destf);\n      } catch (IOException e) {\n        LOG.warn(\"Error setting permission of file \" + destf + \": \"+ e.getMessage(), e);\n      }\n    }\n    return success;\n  }"
        }
    },
    {
        "filename": "HIVE-8766.json",
        "creation_time": "2014-11-06T22:08:43.000+0000",
        "bug_report": {
            "Title": "Hive RetryHMSHandler should be retrying the metastore operation in case of NucleusException",
            "Description": "The issue arises during Metastore operations when the Metastore Database, specifically SQL Server, is heavily loaded or slow to respond. This can lead to NucleusExceptions, particularly when the SQL Server is configured to timeout and terminate connections after a specified duration. The current implementation does not allow for retries in such scenarios, which can lead to failed Hive queries. The proposed solution is to implement a retry mechanism for NucleusExceptions to enhance resilience against transient database issues.",
            "StackTrace": [
                "2014-11-04 06:40:03,208 ERROR bonecp.ConnectionHandle (ConnectionHandle.java:markPossiblyBroken(388)) - Database access problem. Killing off this connection and all remaining connections in the connection pool. SQL State = 08S01",
                "2014-11-04 06:40:03,213 ERROR DataNucleus.Transaction (Log4JLogger.java:error(115)) - Operation rollback failed on resource: org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@1a35cc16, error code UNKNOWN and transaction: [DataNucleus Transaction, ID=Xid=   \ufffd, enlisted resources=[org.datanucleus.store.rdbms.ConnectionFactoryImpl$EmulatedXAResource@1a35cc16]]",
                "2014-11-04 06:40:03,217 ERROR metastore.RetryingHMSHandler (RetryingHMSHandler.java:invoke(139)) - MetaException(message:org.datanucleus.exceptions.NucleusDataStoreException: Size request failed : SELECT COUNT(*) FROM SKEWED_VALUES THIS WHERE THIS.SD_ID_OID=? AND THIS.INTEGER_IDX>=0)",
                "Caused by: org.datanucleus.exceptions.NucleusDataStoreException: Size request failed : SELECT COUNT(*) FROM SKEWED_VALUES THIS WHERE THIS.SD_ID_OID=? AND THIS.INTEGER_IDX>=0",
                "Caused by: com.microsoft.sqlserver.jdbc.SQLServerException: SSL peer shut down incorrectly"
            ],
            "RootCause": "The root cause of the issue is a NucleusDataStoreException triggered by an SQLServerException, which indicates that the SSL connection to the SQL Server database was terminated unexpectedly. This suggests a potential misconfiguration in the SSL settings or network issues affecting the connection stability.",
            "StepsToReproduce": [
                "1. Configure SQL Server to have a low timeout setting.",
                "2. Execute a Hive query that requires a Metastore operation.",
                "3. Observe the logs for NucleusExceptions related to database access."
            ],
            "ExpectedBehavior": "The system should retry the Metastore operation when a NucleusException occurs due to transient issues, allowing the Hive query to succeed once the database becomes responsive again.",
            "ObservedBehavior": "The system fails the Hive query immediately upon encountering a NucleusException, without attempting any retries, leading to a poor user experience during database load spikes.",
            "Suggestions": "Implement a retry mechanism in the RetryingHMSHandler to handle NucleusExceptions gracefully. Additionally, review and optimize the SSL configuration on the SQL Server to prevent connection issues.",
            "problem_location": {
                "files": [
                    "RetryingHMSHandler.java",
                    "ConnectionHandle.java"
                ],
                "classes": [
                    "metastore.RetryingHMSHandler",
                    "bonecp.ConnectionHandle"
                ],
                "methods": [
                    "RetryingHMSHandler.invoke",
                    "ConnectionHandle.markPossiblyBroken"
                ]
            },
            "possible_fix": "Modify the invoke method in RetryingHMSHandler to include logic for retrying operations when a NucleusException is caught. Ensure that the retry logic includes a backoff strategy to avoid overwhelming the database during high load periods."
        },
        "possible_fix_code": {
            "RetryingHMSHandler.invoke": "  public Object invoke(final Object proxy, final Method method, final Object[] args) throws Throwable {\n\n    boolean gotNewConnectUrl = false;\n    boolean reloadConf = HiveConf.getBoolVar(origConf,\n        HiveConf.ConfVars.HMSHANDLERFORCERELOADCONF);\n    long retryInterval = HiveConf.getTimeVar(origConf,\n        HiveConf.ConfVars.HMSHANDLERINTERVAL, TimeUnit.MILLISECONDS);\n    int retryLimit = HiveConf.getIntVar(origConf,\n        HiveConf.ConfVars.HMSHANDLERATTEMPTS);\n\n    if (reloadConf) {\n      MetaStoreInit.updateConnectionURL(origConf, getActiveConf(),\n        null, metaStoreInitData);\n    }\n\n    int retryCount = 0;\n    Throwable caughtException = null;\n    while (true) {\n      try {\n        if (reloadConf || gotNewConnectUrl) {\n          baseHandler.setConf(getActiveConf());\n        }\n        return method.invoke(baseHandler, args);\n\n      } catch (javax.jdo.JDOException e) {\n        caughtException = e;\n      } catch (UndeclaredThrowableException e) {\n        if (e.getCause() != null) {\n          if (e.getCause() instanceof javax.jdo.JDOException) {\n            caughtException = e.getCause();\n          } else if (e.getCause() instanceof MetaException && e.getCause().getCause() != null\n              && e.getCause().getCause() instanceof javax.jdo.JDOException) {\n            caughtException = e.getCause().getCause();\n          } else {\n            LOG.error(ExceptionUtils.getStackTrace(e.getCause()));\n            throw e.getCause();\n          }\n        } else {\n          LOG.error(ExceptionUtils.getStackTrace(e));\n          throw e;\n        }\n      } catch (InvocationTargetException e) {\n        if (e.getCause() instanceof javax.jdo.JDOException) {\n          caughtException = e.getCause();\n        } else if (e.getCause() instanceof NoSuchObjectException || e.getTargetException().getCause() instanceof NoSuchObjectException) {\n          String methodName = method.getName();\n          if (!methodName.startsWith(\"get_table\") && !methodName.startsWith(\"get_partition\") && !methodName.startsWith(\"get_function\")) {\n            LOG.error(ExceptionUtils.getStackTrace(e.getCause()));\n          }\n          throw e.getCause();\n        } else if (e.getCause() instanceof MetaException && e.getCause().getCause() != null\n            && e.getCause().getCause() instanceof javax.jdo.JDOException) {\n          caughtException = e.getCause().getCause();\n        } else {\n          LOG.error(ExceptionUtils.getStackTrace(e.getCause()));\n          throw e.getCause();\n        }\n      }\n\n      if (caughtException instanceof NucleusDataStoreException) {\n        if (retryCount >= retryLimit) {\n          LOG.error(\"HMSHandler Fatal error: \" + ExceptionUtils.getStackTrace(caughtException));\n          throw new MetaException(ExceptionUtils.getStackTrace(caughtException));\n        }\n\n        assert (retryInterval >= 0);\n        retryCount++;\n        LOG.error(\n          String.format(\n            \"Retrying HMSHandler after %d ms (attempt %d of %d)\" +\n            \" with error: \" + ExceptionUtils.getStackTrace(caughtException), retryInterval, retryCount, retryLimit));\n\n        Thread.sleep(retryInterval);\n        String lastUrl = MetaStoreInit.getConnectionURL(getActiveConf());\n        gotNewConnectUrl = MetaStoreInit.updateConnectionURL(origConf, getActiveConf(),\n          lastUrl, metaStoreInitData);\n      }\n    }\n  }"
        }
    },
    {
        "filename": "HIVE-5857.json",
        "creation_time": "2013-11-20T01:12:54.000+0000",
        "bug_report": {
            "Title": "Reduce tasks do not work in uber mode in YARN",
            "Description": "A Hive query fails when attempting to execute a reduce task in uber mode on YARN, resulting in a NullPointerException. This occurs in the `ExecReducer.configure` method due to the absence of the expected plan file (reduce.xml). The `Utilities.getBaseWork` method is expected to return a valid `BaseWork` object, but it returns NULL because of a `FileNotFoundException`. The root cause is linked to the configuration change made by `LocalContainerLauncher`, which switches the framework from 'yarn' to 'local' before the reduce task runs, leading to the failure in locating the plan file.",
            "StackTrace": [
                "2013-11-20 00:50:56,862 INFO [uber-SubtaskRunner] org.apache.hadoop.hive.ql.exec.Utilities: No plan file found: hdfs://namenode.c.lon.spotify.net:54310/var/tmp/kawaa/hive_2013-11-20_00-50-43_888_3938384086824086680-2/-mr-10003/e3caacf6-15d6-4987-b186-d2906791b5b0/reduce.xml",
                "2013-11-20 00:50:56,862 WARN [uber-SubtaskRunner] org.apache.hadoop.mapred.LocalContainerLauncher: Exception running local (uberized) 'child' : java.lang.RuntimeException: Error in configuring object",
                "Caused by: java.lang.NullPointerException at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.configure(ExecReducer.java:116)"
            ],
            "RootCause": "The `NullPointerException` in the `ExecReducer.configure` method is caused by the `gWork` object being null due to the `FileNotFoundException` when trying to access the plan file. This is a result of the configuration being switched to local mode, which prevents the proper initialization of the reduce task.",
            "StepsToReproduce": [
                "Run a Hive query that includes a reduce task in uber mode on YARN.",
                "Ensure that the configuration is set to use YARN as the framework.",
                "Observe the logs for the NullPointerException in the ExecReducer.configure method."
            ],
            "ExpectedBehavior": "The reduce task should successfully locate the plan file and execute without throwing a NullPointerException.",
            "ObservedBehavior": "The reduce task fails with a NullPointerException due to the plan file not being found, resulting in a RuntimeException during configuration.",
            "Suggestions": "Ensure that the configuration for the reduce task does not switch to local mode when running in uber mode. Consider adding a check in the `configure` method to handle the case where the plan file is not found.",
            "problem_location": {
                "files": [
                    "org/apache/hadoop/hive/ql/exec/mr/ExecReducer.java",
                    "org/apache/hadoop/mapred/LocalContainerLauncher.java",
                    "org/apache/hadoop/hive/ql/exec/Utilities.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.ql.exec.mr.ExecReducer",
                    "org.apache.hadoop.mapred.LocalContainerLauncher",
                    "org.apache.hadoop.hive.ql.exec.Utilities"
                ],
                "methods": [
                    "ExecReducer.configure",
                    "LocalContainerLauncher.runSubtask",
                    "Utilities.getBaseWork"
                ]
            },
            "possible_fix": "Add a conditional check in the `ExecReducer.configure` method to verify if the task is running in uber mode and adjust the path for the plan file accordingly. This could prevent the NullPointerException by ensuring that the correct configuration is used."
        },
        "possible_fix_code": {
            "ExecReducer.configure": "  public void configure(JobConf job) {\n    rowObjectInspector = new ObjectInspector[Byte.MAX_VALUE];\n    ObjectInspector[] valueObjectInspector = new ObjectInspector[Byte.MAX_VALUE];\n    ObjectInspector keyObjectInspector;\n\n    // Allocate the bean at the beginning -\n    memoryMXBean = ManagementFactory.getMemoryMXBean();\n    l4j.info(\"maximum memory = \" + memoryMXBean.getHeapMemoryUsage().getMax());\n\n    isLogInfoEnabled = l4j.isInfoEnabled();\n\n    try {\n      l4j.info(\"conf classpath = \"\n          + Arrays.asList(((URLClassLoader) job.getClassLoader()).getURLs()));\n      l4j.info(\"thread classpath = \"\n          + Arrays.asList(((URLClassLoader) Thread.currentThread()\n          .getContextClassLoader()).getURLs()));\n    } catch (Exception e) {\n      l4j.info(\"cannot get classpath: \" + e.getMessage());\n    }\n    jc = job;\n    ReduceWork gWork = Utilities.getReduceWork(job);\n\n    // Check if gWork is null and handle accordingly\n    if (gWork == null) {\n      throw new RuntimeException(\"ReduceWork is not properly initialized. Ensure the job configuration is correct.\");\n    }\n\n    reducer = gWork.getReducer();\n    reducer.setParentOperators(null); // clear out any parents as reducer is the\n    // root\n    isTagged = gWork.getNeedsTagging();\n    try {\n      keyTableDesc = gWork.getKeyDesc();\n      inputKeyDeserializer = (SerDe) ReflectionUtils.newInstance(keyTableDesc\n          .getDeserializerClass(), null);\n      inputKeyDeserializer.initialize(null, keyTableDesc.getProperties());\n      keyObjectInspector = inputKeyDeserializer.getObjectInspector();\n      valueTableDesc = new TableDesc[gWork.getTagToValueDesc().size()];\n      for (int tag = 0; tag < gWork.getTagToValueDesc().size(); tag++) {\n        // We should initialize the SerDe with the TypeInfo when available.\n        valueTableDesc[tag] = gWork.getTagToValueDesc().get(tag);\n        inputValueDeserializer[tag] = (SerDe) ReflectionUtils.newInstance(\n            valueTableDesc[tag].getDeserializerClass(), null);\n        inputValueDeserializer[tag].initialize(null, valueTableDesc[tag]\n            .getProperties());\n        valueObjectInspector[tag] = inputValueDeserializer[tag]\n            .getObjectInspector();\n\n        ArrayList<ObjectInspector> ois = new ArrayList<ObjectInspector>();\n        ois.add(keyObjectInspector);\n        ois.add(valueObjectInspector[tag]);\n        rowObjectInspector[tag] = ObjectInspectorFactory\n            .getStandardStructObjectInspector(Utilities.reduceFieldNameList, ois);\n      }\n    } catch (Exception e) {\n      throw new RuntimeException(e);\n    }\n\n    MapredContext.init(false, new JobConf(jc));\n\n    // initialize reduce operator tree\n    try {\n      l4j.info(reducer.dump(0));\n      reducer.initialize(jc, rowObjectInspector);\n    } catch (Throwable e) {\n      abort = true;\n      if (e instanceof OutOfMemoryError) {\n        // Don't create a new object if we are already out of memory\n        throw (OutOfMemoryError) e;\n      } else {\n        throw new RuntimeException(\"Reduce operator initialization failed\", e);\n      }\n    }\n  }"
        }
    },
    {
        "filename": "HIVE-1547.json",
        "creation_time": "2010-08-17T02:09:54.000+0000",
        "bug_report": {
            "Title": "Unarchiving operation throws NPE",
            "Description": "The unarchiving operation in the Apache Hive framework fails with a NullPointerException (NPE) when attempting to unarchive a partition. The stack trace indicates that the error occurs in the `unarchive` method of the `DDLTask` class. This issue appears to be specific to the distributed file system (DFS) as local file system tests do not replicate the error.",
            "StackTrace": [
                "2010-08-16 12:44:18,801 ERROR exec.DDLTask (SessionState.java:printError(277)) - Failed with exception null",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.hive.ql.exec.DDLTask.unarchive(DDLTask.java:729)",
                "at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:195)",
                "at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:108)",
                "at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:55)",
                "at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:609)",
                "at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:478)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:356)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:140)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:199)",
                "at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:351)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "at java.lang.reflect.Method.invoke(Method.java:597)",
                "at org.apache.hadoop.util.RunJar.main(RunJar.java:156)"
            ],
            "RootCause": "The NullPointerException is likely caused by uninitialized or null references for the `db`, `simpleDesc`, or the results of `simpleDesc.getDbName()`, `simpleDesc.getTableName()`, or `db.getTable(...)`. The `unarchive` method does not perform adequate null checks before accessing these objects, leading to the exception.",
            "StepsToReproduce": [
                "1. Attempt to unarchive a partition using the `unarchive` method in the `DDLTask` class.",
                "2. Ensure that the `db` and `simpleDesc` parameters are properly initialized.",
                "3. Verify that the partition specified in `simpleDesc` exists and is archived."
            ],
            "ExpectedBehavior": "The unarchiving operation should successfully restore the original files from the archived partition without throwing any exceptions.",
            "ObservedBehavior": "The operation fails with a NullPointerException, indicating that one or more required objects are null.",
            "Suggestions": "Add null checks for `db`, `simpleDesc`, and the results of `simpleDesc.getDbName()`, `simpleDesc.getTableName()`, and `db.getTable(...)` in the `unarchive` method. Implement better exception handling to provide more informative error messages.",
            "problem_location": {
                "files": [
                    "DDLTask.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.ql.exec.DDLTask"
                ],
                "methods": [
                    "DDLTask.unarchive",
                    "DDLTask.execute"
                ]
            },
            "possible_fix": "In the `unarchive` method, add checks to ensure that `db`, `simpleDesc`, and the results of `simpleDesc.getDbName()`, `simpleDesc.getTableName()`, and `db.getTable(...)` are not null before proceeding with the operation. For example:\n\n```java\nif (db == null || simpleDesc == null || simpleDesc.getDbName() == null || simpleDesc.getTableName() == null) {\n    throw new HiveException(\"Database or table name cannot be null\");\n}\n```"
        },
        "possible_fix_code": {
            "unarchive": "  private int unarchive(Hive db, AlterTableSimpleDesc simpleDesc)\n      throws HiveException {\n    if (db == null || simpleDesc == null || simpleDesc.getDbName() == null || simpleDesc.getTableName() == null) {\n        throw new HiveException(\"Database or table name cannot be null\");\n    }\n\n    String dbName = simpleDesc.getDbName();\n    String tblName = simpleDesc.getTableName();\n\n    Table tbl = db.getTable(dbName, tblName);\n    validateAlterTableType(tbl, AlterTableDesc.AlterTableTypes.UNARCHIVE);\n\n    // Means user specified a table, not a partition\n    if (simpleDesc.getPartSpec() == null) {\n      throw new HiveException(\"ARCHIVE is for partitions only\");\n    }\n\n    Map<String, String> partSpec = simpleDesc.getPartSpec();\n    Partition p = db.getPartition(tbl, partSpec, false);\n\n    if (tbl.getTableType() != TableType.MANAGED_TABLE) {\n      throw new HiveException(\"UNARCHIVE can only be performed on managed tables\");\n    }\n\n    if (p == null) {\n      throw new HiveException(\"Specified partition does not exist\");\n    }\n\n    if (!isArchived(p)) {\n      Path location = new Path(p.getLocation());\n      Path leftOverArchiveDir = new Path(location.getParent(),\n          location.getName() + INTERMEDIATE_ARCHIVED_DIR_SUFFIX);\n\n      if (pathExists(leftOverArchiveDir)) {\n        console.printInfo(\"Deleting \" + leftOverArchiveDir + \" left over \" +\n        \"from a previous unarchiving operation\");\n        deleteDir(leftOverArchiveDir);\n      }\n\n      throw new HiveException(\"Specified partition is not archived\");\n    }\n\n    Path originalLocation = new Path(getOriginalLocation(p));\n    Path sourceDir = new Path(p.getLocation());\n    Path intermediateArchiveDir = new Path(originalLocation.getParent(),\n        originalLocation.getName() + INTERMEDIATE_ARCHIVED_DIR_SUFFIX);\n    Path intermediateExtractedDir = new Path(originalLocation.getParent(),\n        originalLocation.getName() + INTERMEDIATE_EXTRACTED_DIR_SUFFIX);\n\n    Path tmpDir = new Path(driverContext\n          .getCtx()\n          .getExternalTmpFileURI(originalLocation.toUri()));\n\n    FileSystem fs = null;\n    try {\n      fs = tmpDir.getFileSystem(conf);\n      // Verify that there are no files in the tmp dir, because if there are, it\n      // would be copied to the partition\n      FileStatus [] filesInTmpDir = fs.listStatus(tmpDir);\n      if (filesInTmpDir.length != 0) {\n        for (FileStatus file : filesInTmpDir) {\n          console.printInfo(file.getPath().toString());\n        }\n        throw new HiveException(\"Temporary directory \" + tmpDir + \" is not empty\");\n      }\n\n    } catch (IOException e) {\n      throw new HiveException(e);\n    }\n\n    // Some sanity checks\n    if (originalLocation == null) {\n      throw new HiveException(\"Missing archive data in the partition\");\n    }\n    if (!\"har\".equals(sourceDir.toUri().getScheme())) {\n      throw new HiveException(\"Location should refer to a HAR\");\n    }\n\n    // Clarification of terms:\n    // - The originalLocation directory represents the original directory of the\n    //   partition's files. They now contain an archived version of those files\n    //   eg. hdfs:/warehouse/myTable/ds=1/\n    // - The source directory is the directory containing all the files that\n    //   should be in the partition. e.g. har:/warehouse/myTable/ds=1/myTable.har/\n    //   Note the har:/ scheme\n\n    // Steps:\n    // 1. Extract the archive in a temporary folder\n    // 2. Move the archive dir to an intermediate dir that is in at the same\n    //    dir as originalLocation. Call the new dir intermediate-extracted.\n    // 3. Rename the original partition dir to an intermediate dir. Call the\n    //    renamed dir intermediate-archive\n    // 4. Rename intermediate-extracted to the original partition dir\n    // 5. Change the metadata\n    // 6. Delete the archived partition files in intermediate-archive\n\n    if (!pathExists(intermediateExtractedDir) &&\n        !pathExists(intermediateArchiveDir)) {\n      try {\n\n        // Copy the files out of the archive into the temporary directory\n        String copySource = (new Path(sourceDir, \"*\")).toString();\n        String copyDest = tmpDir.toString();\n        List<String> args = new ArrayList<String>();\n        args.add(\"-cp\");\n        args.add(copySource);\n        args.add(copyDest);\n\n        console.printInfo(\"Copying \" + copySource + \" to \" + copyDest);\n        FsShell fss = new FsShell(conf);\n        int ret = 0;\n        try {\n          ret = ToolRunner.run(fss, args.toArray(new String[0]));\n        } catch (Exception e) {\n          throw new HiveException(e);\n        }\n        if (ret != 0) {\n          throw new HiveException(\"Error while copying files from archive\");\n        }\n\n        console.printInfo(\"Moving \" + tmpDir + \" to \" + intermediateExtractedDir);\n        if (fs.exists(intermediateExtractedDir)) {\n          throw new HiveException(\"Invalid state: the intermediate extracted \" +\n              \"directory already exists.\");\n        }\n        fs.rename(tmpDir, intermediateExtractedDir);\n      } catch (Exception e) {\n        throw new HiveException(e);\n      }\n    }\n\n    // At this point, we know that the extracted files are in the intermediate\n    // extracted dir, or in the the original directory.\n\n    if (!pathExists(intermediateArchiveDir)) {\n      try {\n        console.printInfo(\"Moving \" + originalLocation + \" to \" + intermediateArchiveDir);\n        fs.rename(originalLocation, intermediateArchiveDir);\n      } catch (IOException e) {\n        throw new HiveException(e);\n      }\n    } else {\n      console.printInfo(intermediateArchiveDir + \" already exists. \" +\n      \"Assuming it contains the archived version of the partition\");\n    }\n\n    // If there is a failure from here to until when the metadata is changed,\n    // the partition will be empty or throw errors on read.\n\n    // If the original location exists here, then it must be the extracted files\n    // because in the previous step, we moved the previous original location\n    // (containing the archived version of the files) to intermediateArchiveDir\n    if (!pathExists(originalLocation)) {\n      try {\n        console.printInfo(\"Moving \" + intermediateExtractedDir + \" to \" + originalLocation);\n        fs.rename(intermediateExtractedDir, originalLocation);\n      } catch (IOException e) {\n        throw new HiveException(e);\n      }\n    } else {\n      console.printInfo(originalLocation + \" already exists. \" +\n      \"Assuming it contains the extracted files in the partition\");\n    }\n\n    setUnArchived(p);\n    try {\n      db.alterPartition(tblName, p);\n    } catch (InvalidOperationException e) {\n      throw new HiveException(e);\n    }\n    // If a failure happens here, the intermediate archive files won't be\n    // deleted. The user will need to call unarchive again to clear those up.\n    deleteDir(intermediateArchiveDir);\n\n    return 0;\n  }"
        }
    },
    {
        "filename": "HIVE-6113.json",
        "creation_time": "2013-12-27T07:07:00.000+0000",
        "bug_report": {
            "Title": "Upgrade DataNucleus [was: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient]",
            "Description": "The issue arises when executing the SQL command 'use fdm; desc formatted fdm.tableName;' in Python, leading to an error related to the instantiation of the HiveMetaStoreClient. The error indicates a problem with the database creation process, specifically a 'Duplicate entry 'default' for key 'UNIQUE_DATABASE'. This suggests that the system is attempting to create a database that already exists, causing the operation to fail initially but succeed upon retry.",
            "StackTrace": [
                "2013-12-25 03:01:32,290 ERROR exec.DDLTask (DDLTask.java:execute(435)) - org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient",
                "Caused by: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient",
                "Caused by: javax.jdo.JDODataStoreException: Exception thrown flushing changes to datastore",
                "NestedThrowables: java.sql.BatchUpdateException: Duplicate entry 'default' for key 'UNIQUE_DATABASE'"
            ],
            "RootCause": "The root cause of the issue is that the code attempts to create a database named 'default' in the Hive metastore, which already exists. This results in a 'Duplicate entry' error when the createDatabase method is invoked without checking for the database's existence first.",
            "StepsToReproduce": [
                "1. Execute the SQL command 'use fdm; desc formatted fdm.tableName;' in a Python environment.",
                "2. Observe the error message indicating the inability to instantiate HiveMetaStoreClient.",
                "3. Retry the command to see that it succeeds on the second attempt."
            ],
            "ExpectedBehavior": "The system should check if the database exists before attempting to create it. If the database already exists, it should skip the creation process and proceed without errors.",
            "ObservedBehavior": "The system throws an error on the first attempt to execute the SQL command due to a duplicate database entry, but succeeds on the second attempt.",
            "Suggestions": "Implement a check for the existence of the database using the getDatabase method before calling createDatabase. If the database exists, handle it appropriately to avoid the duplicate entry error.",
            "problem_location": {
                "files": [
                    "ObjectStore.java",
                    "MetaStoreUtils.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.metastore.ObjectStore",
                    "org.apache.hadoop.hive.metastore.MetaStoreUtils"
                ],
                "methods": [
                    "ObjectStore.createDatabase",
                    "ObjectStore.getDatabase"
                ]
            },
            "possible_fix": "Modify the createDatabase method to first call getDatabase to check if the database already exists. If it does, skip the creation process or log a message indicating that the database already exists."
        },
        "possible_fix_code": {
            "org.apache.hadoop.hive.metastore.ObjectStore.createDatabase": "  public void createDatabase(Database db) throws InvalidObjectException, MetaException {\n    boolean commited = false;\n    // Check if the database already exists\n    try {\n      Database existingDb = getDatabase(db.getName().toLowerCase());\n      if (existingDb != null) {\n        // Database already exists, log a message and skip creation\n        System.out.println(\"Database '\" + db.getName() + \"' already exists. Skipping creation.\");\n        return;\n      }\n    } catch (NoSuchObjectException e) {\n      // Database does not exist, proceed with creation\n    }\n\n    MDatabase mdb = new MDatabase();\n    mdb.setName(db.getName().toLowerCase());\n    mdb.setLocationUri(db.getLocationUri());\n    mdb.setDescription(db.getDescription());\n    mdb.setParameters(db.getParameters());\n    try {\n      openTransaction();\n      pm.makePersistent(mdb);\n      commited = commitTransaction();\n    } finally {\n      if (!commited) {\n        rollbackTransaction();\n      }\n    }\n  }"
        }
    },
    {
        "filename": "HIVE-9570.json",
        "creation_time": "2015-02-03T23:30:09.000+0000",
        "bug_report": {
            "Title": "Investigate test failure on union_view.q [Spark Branch]",
            "Description": "The test case for `union_view.q` has failed due to a `NullPointerException` encountered in the `SparkCompiler` class of the Apache Hive project. The stack trace indicates that the exception occurs in the `setInputFormat` method, which is invoked during the compilation of tasks in the `TaskCompiler` class. This method is responsible for setting the input format for a `MapWork` object based on the properties of an `Operator`. The failure suggests that either the `Task` or `Operator` object being passed to `setInputFormat` is `null`, leading to the exception.",
            "StackTrace": [
                "2015-02-03 15:27:05,723 ERROR [main]: ql.Driver (SessionState.java:printError(861)) - FAILED: NullPointerException null",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.hive.ql.parse.spark.SparkCompiler.setInputFormat(SparkCompiler.java:274)",
                "at org.apache.hadoop.hive.ql.parse.spark.SparkCompiler.setInputFormat(SparkCompiler.java:253)",
                "at org.apache.hadoop.hive.ql.parse.TaskCompiler.compile(TaskCompiler.java:222)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10231)",
                "at org.apache.hadoop.hive.ql.parse.CalcitePlanner.analyzeInternal(CalcitePlanner.java:190)",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:224)",
                "at org.apache.hadoop.hive.ql.parse.ExplainSemanticAnalyzer.analyzeInternal(ExplainSemanticAnalyzer.java:74)",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:224)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:421)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:307)",
                "at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1112)",
                "at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1160)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1049)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1039)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:207)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:159)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:370)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:305)",
                "at org.apache.hadoop.hive.ql.QTestUtil.executeClientInternal(QTestUtil.java:1019)",
                "at org.apache.hadoop.hive.ql.QTestUtil.executeClient(QTestUtil.java:993)",
                "at org.apache.hadoop.hive.cli.TestSparkCliDriver.runTest(TestSparkCliDriver.java:136)",
                "at org.apache.hadoop.hive.cli.TestSparkCliDriver.testCliDriver_union_view(TestSparkCliDriver.java:120)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at junit.framework.TestCase.runTest(TestCase.java:176)",
                "at junit.framework.TestCase.runBare(TestCase.java:141)",
                "at junit.framework.TestResult$1.protect(TestResult.java:122)",
                "at junit.framework.TestResult.runProtected(TestResult.java:142)",
                "at junit.framework.TestResult.run(TestResult.java:125)",
                "at junit.framework.TestCase.run(TestCase.java:129)",
                "at junit.framework.TestSuite.runTest(TestSuite.java:255)",
                "at junit.framework.TestSuite.run(TestSuite.java:250)",
                "at org.junit.internal.runners.JUnit38ClassRunner.run(JUnit38ClassRunner.java:84)"
            ],
            "RootCause": "The `NullPointerException` is likely caused by either the `Task` object or the `Operator` object being `null` when passed to the `setInputFormat` method in the `SparkCompiler` class. This method is invoked from the `TaskCompiler.compile` method, which suggests that the issue may originate from improperly initialized tasks or operators.",
            "StepsToReproduce": [
                "Run the test case for `union_view.q` in the Spark branch.",
                "Ensure that the environment is set up correctly with all necessary dependencies.",
                "Observe the output for the `NullPointerException` in the logs."
            ],
            "ExpectedBehavior": "The test case for `union_view.q` should execute successfully without throwing any exceptions, and the input format should be set correctly for the `MapWork` object.",
            "ObservedBehavior": "The test case fails with a `NullPointerException`, indicating that an object reference was not initialized properly during the execution of the `setInputFormat` method.",
            "Suggestions": "Investigate the initialization of the `Task` and `Operator` objects being passed to the `setInputFormat` method. Ensure that all required parameters are properly set before invoking this method. Consider adding null checks to prevent the exception from occurring.",
            "problem_location": {
                "files": [
                    "ql/src/java/org/apache/hadoop/hive/ql/parse/spark/SparkCompiler.java",
                    "ql/src/java/org/apache/hadoop/hive/ql/parse/TaskCompiler.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.ql.parse.spark.SparkCompiler",
                    "org.apache.hadoop.hive.ql.parse.TaskCompiler"
                ],
                "methods": [
                    "SparkCompiler.setInputFormat",
                    "TaskCompiler.compile"
                ]
            },
            "possible_fix": "Add null checks in the `setInputFormat` method to handle cases where the `work` or `op` parameters are null. For example:\n\n```java\nprivate void setInputFormat(MapWork work, Operator<? extends OperatorDesc> op) {\n    if (work == null || op == null) {\n        throw new IllegalArgumentException(\"MapWork and Operator must not be null\");\n    }\n    // existing logic...\n}\n```"
        },
        "possible_fix_code": {
            "SparkCompiler.setInputFormat": "  private void setInputFormat(MapWork work, Operator<? extends OperatorDesc> op) {\n    if (work == null || op == null) {\n        throw new IllegalArgumentException(\"MapWork and Operator must not be null\");\n    }\n    if (op.isUseBucketizedHiveInputFormat()) {\n      work.setUseBucketizedHiveInputFormat(true);\n      return;\n    }\n\n    if (op.getChildOperators() != null) {\n      for (Operator<? extends OperatorDesc> childOp : op.getChildOperators()) {\n        setInputFormat(work, childOp);\n      }\n    }\n  }"
        }
    },
    {
        "filename": "HIVE-1678.json",
        "creation_time": "2010-10-01T05:41:21.000+0000",
        "bug_report": {
            "Title": "NPE in MapJoin",
            "Description": "A NullPointerException (NPE) occurs during the execution of a query involving two map joins and a group by operation in the Apache Hive framework. The exception is thrown in the `processOp` method of the `MapJoinOperator` class, indicating that an attempt is made to access an uninitialized object or variable. The stack trace shows that the error arises from line 177 of `MapJoinOperator.java`, which suggests that critical member variables may not have been properly initialized before being accessed.",
            "StackTrace": [
                "Caused by: java.lang.NullPointerException",
                "at org.apache.hadoop.hive.ql.exec.MapJoinOperator.processOp(MapJoinOperator.java:177)",
                "at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:457)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:697)",
                "at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:84)",
                "at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:457)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:697)",
                "at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:464)"
            ],
            "RootCause": "The NPE is likely caused by uninitialized member variables in the `MapJoinOperator`, particularly `mapJoinTables` and `metadataValueTag`. The `initializeOp` method, which is responsible for initializing these variables, may not have been called prior to invoking `processOp`.",
            "StepsToReproduce": [
                "1. Execute a query that involves two map joins and a group by operation.",
                "2. Observe the execution flow leading to the `processOp` method in the `MapJoinOperator` class.",
                "3. Check for the initialization of member variables before the method is called."
            ],
            "ExpectedBehavior": "The system should successfully process the map join operation without throwing a NullPointerException, correctly handling the input data and producing the expected output.",
            "ObservedBehavior": "The system throws a NullPointerException during the execution of the `processOp` method in the `MapJoinOperator`, indicating that an attempt was made to access a null object.",
            "Suggestions": "Ensure that the `initializeOp` method is called before `processOp` to properly initialize all necessary member variables. Additionally, consider adding error handling for NullPointerExceptions to provide more informative error messages.",
            "problem_location": {
                "files": [
                    "MapJoinOperator.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.ql.exec.MapJoinOperator"
                ],
                "methods": [
                    "MapJoinOperator.processOp",
                    "MapJoinOperator.initializeOp"
                ]
            },
            "possible_fix": "Call the `initializeOp` method before invoking `processOp` to ensure that all member variables, such as `mapJoinTables` and `metadataValueTag`, are properly initialized. Additionally, implement error handling for NullPointerExceptions within the `processOp` method."
        },
        "possible_fix_code": {
            "processOp": "  public void processOp(Object row, int tag) throws HiveException {\n\n    if (tag == posBigTable) {\n      this.getExecContext().processInputFileChangeForLocalWork();\n    }\n\n    // Ensure initialization before processing\n    if (mapJoinTables == null || metadataValueTag == null) {\n      throw new HiveException(\"MapJoinOperator not properly initialized. Call initializeOp first.\");\n    }\n\n    try {\n      // get alias\n      alias = (byte) tag;\n\n      if ((lastAlias == null) || (!lastAlias.equals(alias))) {\n        nextSz = joinEmitInterval;\n      }\n\n      // compute keys and values as StandardObjects\n      ArrayList<Object> key = computeKeys(row, joinKeys.get(alias),\n          joinKeysObjectInspectors.get(alias));\n      ArrayList<Object> value = computeValues(row, joinValues.get(alias),\n          joinValuesObjectInspectors.get(alias), joinFilters.get(alias),\n          joinFilterObjectInspectors.get(alias), noOuterJoin);\n\n      // does this source need to be stored in the hash map\n      if (tag != posBigTable) {\n        if (firstRow) {\n          metadataKeyTag = nextVal++;\n\n          TableDesc keyTableDesc = conf.getKeyTblDesc();\n          SerDe keySerializer = (SerDe) ReflectionUtils.newInstance(\n              keyTableDesc.getDeserializerClass(), null);\n          keySerializer.initialize(null, keyTableDesc.getProperties());\n\n          mapMetadata.put(Integer.valueOf(metadataKeyTag),\n              new MapJoinObjectCtx(\n              ObjectInspectorUtils\n              .getStandardObjectInspector(keySerializer\n              .getObjectInspector(),\n              ObjectInspectorCopyOption.WRITABLE), keySerializer,\n              keyTableDesc, hconf));\n\n          firstRow = false;\n        }\n\n        reportProgress();\n        numMapRowsRead++;\n\n        if ((numMapRowsRead > maxMapJoinSize) && (reporter != null)\n            && (counterNameToEnum != null)) {\n          // update counter\n          LOG\n              .warn(\"Too many rows in map join tables. Fatal error counter will be incremented!!\");\n          incrCounter(fatalErrorCntr, 1);\n          fatalError = true;\n          return;\n        }\n\n        HashMapWrapper<MapJoinObjectKey, MapJoinObjectValue> hashTable = mapJoinTables\n            .get(alias);\n        MapJoinObjectKey keyMap = new MapJoinObjectKey(metadataKeyTag, key);\n        MapJoinObjectValue o = hashTable.get(keyMap);\n        RowContainer res = null;\n\n        boolean needNewKey = true;\n        if (o == null) {\n          int bucketSize = HiveConf.getIntVar(hconf, HiveConf.ConfVars.HIVEMAPJOINBUCKETCACHESIZE);\n          res = getRowContainer(hconf, (byte) tag, order[tag], bucketSize);\n          res.add(value);\n        } else {\n          res = o.getObj();\n          res.add(value);\n          // If key already exists, HashMapWrapper.get() guarantees it is\n          // already in main memory HashMap\n          // cache. So just replacing the object value should update the\n          // HashMapWrapper. This will save\n          // the cost of constructing the new key/object and deleting old one\n          // and inserting the new one.\n          if (hashTable.cacheSize() > 0) {\n            o.setObj(res);\n            needNewKey = false;\n          }\n        }\n\n        if (metadataValueTag[tag] == -1) {\n          metadataValueTag[tag] = nextVal++;\n\n          TableDesc valueTableDesc = conf.getValueTblDescs().get(tag);\n          SerDe valueSerDe = (SerDe) ReflectionUtils.newInstance(valueTableDesc\n              .getDeserializerClass(), null);\n          valueSerDe.initialize(null, valueTableDesc.getProperties());\n\n          mapMetadata.put(Integer.valueOf(metadataValueTag[tag]),\n              new MapJoinObjectCtx(ObjectInspectorUtils\n              .getStandardObjectInspector(valueSerDe.getObjectInspector(),\n              ObjectInspectorCopyOption.WRITABLE), valueSerDe,\n              valueTableDesc, hconf));\n        }\n\n        // Construct externalizable objects for key and value\n        if (needNewKey) {\n          MapJoinObjectKey keyObj = new MapJoinObjectKey(metadataKeyTag, key);\n          MapJoinObjectValue valueObj = new MapJoinObjectValue(\n              metadataValueTag[tag], res);\n          valueObj.setConf(hconf);\n          valueObj.setConf(hconf);\n          // This may potentially increase the size of the hashmap on the mapper\n          if (res.size() > mapJoinRowsKey) {\n            if (res.size() % 100 == 0) {\n              LOG.warn(\"Number of values for a given key \" + keyObj + \" are \"\n                  + res.size());\n              LOG.warn(\"used memory \" + Runtime.getRuntime().totalMemory());\n            }\n          }\n          hashTable.put(keyObj, valueObj);\n        }\n        return;\n      }\n\n      // Add the value to the ArrayList\n      storage.get(alias).add(value);\n\n      for (Byte pos : order) {\n        if (pos.intValue() != tag) {\n          MapJoinObjectKey keyMap = new MapJoinObjectKey(metadataKeyTag, key);\n          MapJoinObjectValue o = mapJoinTables.get(pos).get(keyMap);\n\n          // there is no join-value or join-key has all null elements\n          if (o == null || (hasAnyNulls(key))) {\n            if (noOuterJoin) {\n              storage.put(pos, emptyList);\n            } else {\n              storage.put(pos, dummyObjVectors[pos.intValue()]);\n            }\n          } else {\n            storage.put(pos, o.getObj());\n          }\n        }\n      }\n\n      // generate the output records\n      checkAndGenObject();\n\n      // done with the row\n      storage.get(alias).clear();\n\n      for (Byte pos : order) {\n        if (pos.intValue() != tag) {\n          storage.put(pos, null);\n        }\n      }\n\n    } catch (SerDeException e) {\n      e.printStackTrace();\n      throw new HiveException(e);\n    }\n  }"
        }
    },
    {
        "filename": "HIVE-11820.json",
        "creation_time": "2015-09-14T22:20:28.000+0000",
        "bug_report": {
            "Title": "Export tables with size of >32MB throws \"java.lang.IllegalArgumentException: Skip CRC is valid only with update options\"",
            "Description": "When attempting to export tables larger than 32MB, an exception is thrown indicating that the 'Skip CRC' option is only valid when certain update options are enabled. This issue arises from the `DistCpOptions` class, specifically when the 'Skip CRC' option is set without the necessary update options being configured. The error occurs during the execution of the `runDistCp` method in the `Hadoop23Shims` class, which is responsible for handling distributed copy operations in Hive.",
            "StackTrace": [
                "2015-09-14 21:44:16,817 ERROR [main]: exec.Task (SessionState.java:printError(960)) - Failed with exception Skip CRC is valid only with update options",
                "java.lang.IllegalArgumentException: Skip CRC is valid only with update options",
                "at org.apache.hadoop.tools.DistCpOptions.validate(DistCpOptions.java:556)",
                "at org.apache.hadoop.tools.DistCpOptions.setSkipCRC(DistCpOptions.java:311)",
                "at org.apache.hadoop.hive.shims.Hadoop23Shims.runDistCp(Hadoop23Shims.java:1147)",
                "at org.apache.hadoop.hive.common.FileUtils.copy(FileUtils.java:553)",
                "at org.apache.hadoop.hive.ql.exec.CopyTask.execute(CopyTask.java:82)",
                "at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:160)",
                "at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:89)",
                "at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1655)",
                "at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1414)",
                "at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1195)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1059)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1049)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:213)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:165)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)",
                "at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:736)",
                "at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:681)",
                "at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:497)",
                "at org.apache.hadoop.util.RunJar.run(RunJar.java:221)",
                "at org.apache.hadoop.util.RunJar.main(RunJar.java:136)"
            ],
            "RootCause": "The root cause of the issue is that the 'Skip CRC' option is being set without the necessary update options being enabled in the `DistCpOptions` class. This misconfiguration leads to the `IllegalArgumentException` being thrown during the validation process.",
            "StepsToReproduce": [
                "Attempt to export a table larger than 32MB using the DistCp functionality.",
                "Ensure that the 'Skip CRC' option is set without the required update options.",
                "Observe the exception thrown during the execution."
            ],
            "ExpectedBehavior": "The export operation should complete successfully without throwing any exceptions, provided that the correct options are configured.",
            "ObservedBehavior": "An `IllegalArgumentException` is thrown, indicating that the 'Skip CRC' option is only valid with update options.",
            "Suggestions": "To resolve this issue, ensure that the appropriate update options are set before using the 'Skip CRC' option. Review the documentation for `DistCpOptions` to confirm valid configurations.",
            "problem_location": {
                "files": [
                    "DistCpOptions.java",
                    "Hadoop23Shims.java",
                    "FileUtils.java"
                ],
                "classes": [
                    "org.apache.hadoop.tools.DistCpOptions",
                    "org.apache.hadoop.hive.shims.Hadoop23Shims",
                    "org.apache.hadoop.hive.common.FileUtils"
                ],
                "methods": [
                    "DistCpOptions.validate",
                    "DistCpOptions.setSkipCRC",
                    "Hadoop23Shims.runDistCp"
                ]
            },
            "possible_fix": "Reverse the order of the following lines in the patch of HIVE-11607:\n\noptions.setSyncFolder(true);\noptions.setSkipCRC(true);"
        },
        "possible_fix_code": {
            "Hadoop23Shims.runDistCp": "public boolean runDistCp(Path src, Path dst, HiveConf conf) throws IOException {\n    DistCpOptions options = new DistCpOptions(src, dst);\n    options.setSyncFolder(true);\n    options.setSkipCRC(true);\n    // Ensure that the necessary update options are set before using 'Skip CRC'.\n    // This is a placeholder for the actual update options that need to be configured.\n    // options.setUpdateOptions(...);\n    // Validate the options before proceeding.\n    options.validate();\n    // Proceed with the distcp operation using the configured options.\n    return runDistCp(options);\n}"
        }
    },
    {
        "filename": "HIVE-17274.json",
        "creation_time": "2017-08-08T22:20:33.000+0000",
        "bug_report": {
            "Title": "RowContainer spills for timestamp column throws exception",
            "Description": "The issue arises when a timestamp column is used as a join key, leading to an exception due to improper path construction in the Hadoop file system. Specifically, the `RowContainer` class attempts to create a filename using the `toString()` representation of the join key, which includes a timestamp. This results in a filename that contains invalid characters (e.g., ':') that are not allowed in path names, triggering a `java.lang.IllegalArgumentException` with a `java.net.URISyntaxException` indicating a relative path in an absolute URI context.",
            "StackTrace": [
                "2017-08-05 23:51:33,631 ERROR [main] org.apache.hadoop.hive.ql.exec.persistence.RowContainer: java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: .RowContainer7551143976922371245.[1792453531, 2016-09-02 01:17:43,%202016-09-02%5D.tmp.crc",
                "java.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: .RowContainer7551143976922371245.[1792453531, 2016-09-02 01:17:43,%202016-09-02%5D.tmp.crc",
                "at org.apache.hadoop.fs.Path.initialize(Path.java:205)",
                "at org.apache.hadoop.fs.Path.<init>(Path.java:171)",
                "at org.apache.hadoop.fs.Path.<init>(Path.java:93)",
                "at org.apache.hadoop.fs.ChecksumFileSystem.getChecksumFile(ChecksumFileSystem.java:94)",
                "at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:404)",
                "at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:463)",
                "at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:442)",
                "at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:926)",
                "at org.apache.hadoop.io.SequenceFile$Writer.<init>(SequenceFile.java:1137)",
                "at org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:273)",
                "at org.apache.hadoop.io.SequenceFile.createWriter(SequenceFile.java:530)",
                "at org.apache.hadoop.hive.ql.exec.Utilities.createSequenceWriter(Utilities.java:1643)",
                "at org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat.getHiveRecordWriter(HiveSequenceFileOutputFormat.java:64)",
                "at org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.getRecordWriter(HiveFileFormatUtils.java:243)",
                "at org.apache.hadoop.hive.ql.exec.persistence.RowContainer.setupWriter(RowContainer.java:538)",
                "at org.apache.hadoop.hive.ql.exec.persistence.RowContainer.spillBlock(RowContainer.java:299)",
                "at org.apache.hadoop.hive.ql.exec.persistence.RowContainer.copyToDFSDirecory(RowContainer.java:407)",
                "at org.apache.hadoop.hive.ql.exec.SkewJoinHandler.endGroup(SkewJoinHandler.java:185)",
                "at org.apache.hadoop.hive.ql.exec.JoinOperator.endGroup(JoinOperator.java:249)",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecReducer.reduce(ExecReducer.java:195)",
                "at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:444)",
                "at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:392)",
                "at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:168)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1724)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)",
                "Caused by: java.net.URISyntaxException: Relative path in absolute URI: .RowContainer7551143976922371245.[1792453531, 2016-09-02 01:17:43,%202016-09-02%5D.tmp.crc",
                "at java.net.URI.checkPath(URI.java:1823)",
                "at java.net.URI.<init>(URI.java:745)",
                "at org.apache.hadoop.fs.Path.initialize(Path.java:202)"
            ],
            "RootCause": "The root cause of the issue is the construction of a filename using a timestamp column as a join key, which results in invalid characters being included in the path. This leads to a `URISyntaxException` when the Hadoop file system attempts to create a path for a checksum file, expecting an absolute URI but receiving a relative path instead.",
            "StepsToReproduce": [
                "1. Create a Hive table with a timestamp column.",
                "2. Perform a join operation using the timestamp column as a key.",
                "3. Trigger the operation that leads to spilling in the `RowContainer`."
            ],
            "ExpectedBehavior": "The system should handle the timestamp column correctly, constructing valid paths for files without throwing exceptions related to URI syntax.",
            "ObservedBehavior": "An `IllegalArgumentException` is thrown due to a `URISyntaxException`, indicating that a relative path is being used in a context where an absolute URI is expected.",
            "Suggestions": "Ensure that the path being passed to the `Path` constructor is an absolute path. Review the code that constructs the path for any logic that might lead to a relative path being used. Consider sanitizing the join key to remove or replace invalid characters before using it in path construction.",
            "problem_location": {
                "files": [
                    "RowContainer.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.ql.exec.persistence.RowContainer"
                ],
                "methods": [
                    "RowContainer.setupWriter",
                    "RowContainer.spillBlock"
                ]
            },
            "possible_fix": "Modify the path construction logic in the `RowContainer` class to ensure that the join key is sanitized and converted to an absolute path before being used. For example, replace invalid characters in the timestamp with valid ones or use a different method to generate the filename."
        },
        "possible_fix_code": {
            "RowContainer.setupWriter": "  protected void setupWriter() throws HiveException {\n    try {\n\n      if ( tmpFile != null ) {\n        return;\n      }\n\n      String suffix = \".tmp\";\n      if (this.keyObject != null) {\n        // Sanitize the keyObject to remove invalid characters\n        String sanitizedKey = this.keyObject.toString().replaceAll(\"[\\\\/:*?\\\"<>|]\", \"_\");\n        suffix = \".\" + sanitizedKey + suffix;\n      }\n\n      parentDir = FileUtils.createLocalDirsTempFile(spillFileDirs, \"hive-rowcontainer\", \"\", true);\n\n      tmpFile = File.createTempFile(\"RowContainer\", suffix, parentDir);\n      LOG.info(\"RowContainer created temp file \" + tmpFile.getAbsolutePath());\n      // Delete the temp file if the JVM terminate normally through Hadoop job\n      // kill command.\n      // Caveat: it won\\'t be deleted if JVM is killed by \\'kill -9\\'.\n      parentDir.deleteOnExit();\n      tmpFile.deleteOnExit();\n\n      // rFile = new RandomAccessFile(tmpFile, \"rw\");\n      HiveOutputFormat<?, ?> hiveOutputFormat = HiveFileFormatUtils.getHiveOutputFormat(jc, tblDesc);\n      tempOutPath = new Path(tmpFile.toString());\n      JobConf localJc = getLocalFSJobConfClone(jc);\n      rw = HiveFileFormatUtils.getRecordWriter(this.jobCloneUsingLocalFs,\n          hiveOutputFormat, serde.getSerializedClass(), false,\n          tblDesc.getProperties(), tempOutPath, reporter);\n    } catch (Exception e) {\n      clearRows();\n      LOG.error(e.toString(), e);\n      throw new HiveException(e);\n    }\n  }"
        }
    },
    {
        "filename": "HIVE-12522.json",
        "creation_time": "2015-11-25T19:48:44.000+0000",
        "bug_report": {
            "Title": "Wrong FS error during Tez merge files when warehouse and scratchdir are on different FS",
            "Description": "The issue arises when the Hive configuration is set to merge Tez files (`hive.merge.tezfiles=true`), but the warehouse directory and scratch directory are located on different file systems. The error indicates a mismatch between the expected file system (HDFS) and the actual file system being accessed (WASB). This typically occurs due to incorrect configuration settings in Hadoop, particularly in the `core-site.xml` or `hdfs-site.xml` files, where the `fs.defaultFS` property may not be set correctly.",
            "StackTrace": [
                "2015-11-13 10:22:10,617 ERROR exec.Task (TezTask.java:execute(184)) - Failed to execute tez graph.",
                "java.lang.IllegalArgumentException: Wrong FS: wasb://chaoyiteztest@chaoyiteztest.blob.core.windows.net/hive/scratch/chaoyitest/c888f405-3c98-46b1-bf39-e57f067dfe4c/hive_2015-11-13_10-16-10_216_8161037519951665173-1/_tmp.-ext-10000, expected: hdfs://headnodehost:9000",
                "at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:645)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:193)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.access$000(DistributedFileSystem.java:105)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$19.doCall(DistributedFileSystem.java:1136)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$19.doCall(DistributedFileSystem.java:1132)",
                "at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1132)",
                "at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1423)",
                "at org.apache.hadoop.hive.ql.exec.tez.DagUtils.createVertex(DagUtils.java:579)",
                "at org.apache.hadoop.hive.ql.exec.tez.DagUtils.createVertex(DagUtils.java:1083)",
                "at org.apache.hadoop.hive.ql.exec.tez.TezTask.build(TezTask.java:329)",
                "at org.apache.hadoop.hive.ql.exec.tez.TezTask.execute(TezTask.java:156)",
                "at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:160)",
                "at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:85)",
                "at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1606)",
                "at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1367)",
                "at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1179)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1006)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:996)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:247)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:199)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:410)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:345)",
                "at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:733)",
                "at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:677)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.util.RunJar.run(RunJar.java:221)",
                "at org.apache.hadoop.util.RunJar.main(RunJar.java:136)"
            ],
            "RootCause": "The root cause of the issue is a configuration mismatch where the Hive job is attempting to access a path in the WASB file system while it is configured to use HDFS. This is indicated by the error message stating 'Wrong FS' and the expected file system being HDFS.",
            "StepsToReproduce": [
                "Set the Hive configuration property hive.merge.tezfiles to true.",
                "Configure the warehouse directory to point to an HDFS location.",
                "Configure the scratch directory to point to a WASB location.",
                "Run a Hive query that triggers the Tez execution."
            ],
            "ExpectedBehavior": "The Tez job should execute successfully, merging files from the scratch directory into the warehouse directory without any file system errors.",
            "ObservedBehavior": "The Tez job fails with a 'Wrong FS' error, indicating a mismatch between the expected and actual file systems.",
            "Suggestions": "Check the Hadoop configuration files (core-site.xml, hdfs-site.xml) to ensure that the fs.defaultFS property is set correctly to the desired file system. If using multiple file systems, ensure that the paths specified in the job are consistent with the configured file system.",
            "problem_location": {
                "files": [
                    "core-site.xml",
                    "hdfs-site.xml",
                    "DagUtils.java"
                ],
                "classes": [
                    "org.apache.hadoop.fs.FileSystem",
                    "org.apache.hadoop.hive.ql.exec.Task",
                    "org.apache.hadoop.hive.ql.exec.tez.DagUtils"
                ],
                "methods": [
                    "FileSystem.checkPath",
                    "Task.executeTask",
                    "DagUtils.createVertex"
                ]
            },
            "possible_fix": "Ensure that the fs.defaultFS property in the Hadoop configuration files is set to hdfs://headnodehost:9000. If the job needs to access WASB, configure the necessary settings for Azure storage in the Hadoop configuration files."
        },
        "possible_fix_code": {
            "org.apache.hadoop.hive.ql.exec.Task.executeTask": "  public int executeTask() {\n    try {\n      SessionState ss = SessionState.get();\n      this.setStarted();\n      if (ss != null) {\n        ss.getHiveHistory().logPlanProgress(queryPlan);\n      }\n      // Ensure the correct file system is set in the configuration\n      Configuration conf = new Configuration();\n      conf.set(\"fs.defaultFS\", \"hdfs://headnodehost:9000\");\n      int retval = execute(driverContext);\n      this.setDone();\n      if (ss != null) {\n        ss.getHiveHistory().logPlanProgress(queryPlan);\n      }\n      return retval;\n    } catch (IOException e) {\n      throw new RuntimeException(\"Unexpected error: \" + e.getMessage(), e);\n    }\n  }"
        }
    },
    {
        "filename": "HIVE-16845.json",
        "creation_time": "2017-06-07T17:07:55.000+0000",
        "bug_report": {
            "Title": "INSERT OVERWRITE a table with dynamic partitions on S3 fails with NPE",
            "Description": "*How to reproduce*\n- Create a partitioned table on S3:\n{noformat}\nCREATE EXTERNAL TABLE s3table(user_id string COMMENT '', event_name string COMMENT '') PARTITIONED BY (reported_date string, product_id int) LOCATION 's3a://<bucket name>'; \n{noformat}\n- Create a temp table:\n{noformat}\ncreate table tmp_table (id string, name string, date string, pid int) row format delimited fields terminated by '\\t' lines terminated by '\\n' stored as textfile;\n{noformat}\n- Load the following rows to the tmp table:\n{noformat}\nu1\tvalue1\t2017-04-10\t10000\nu2\tvalue2\t2017-04-10\t10000\nu3\tvalue3\t2017-04-10\t10001\n{noformat}\n- Set the following parameters:\n-- hive.exec.dynamic.partition.mode=nonstrict\n-- mapreduce.input.fileinputformat.split.maxsize=10\n-- hive.blobstore.optimizations.enabled=true\n-- hive.blobstore.use.blobstore.as.scratchdir=false\n-- hive.merge.mapfiles=true\n- Insert the rows from the temp table into the s3 table:\n{noformat}\nINSERT OVERWRITE TABLE s3table\nPARTITION (reported_date, product_id)\nSELECT\n  t.id as user_id,\n  t.name as event_name,\n  t.date as reported_date,\n  t.pid as product_id\nFROM tmp_table t;\n{noformat}\n\nA NPE will occur with the following stacktrace:\n{noformat}\n2017-05-08 21:32:50,607 ERROR org.apache.hive.service.cli.operation.Operation: [HiveServer2-Background-Pool: Thread-184028]: Error running hive query: \norg.apache.hive.service.cli.HiveSQLException: Error while processing statement: FAILED: Execution Error, return code -101 from org.apache.hadoop.hive.ql.exec.ConditionalTask. null\nat org.apache.hive.service.cli.operation.Operation.toSQLException(Operation.java:400)\nat org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:239)\nat org.apache.hive.service.cli.operation.SQLOperation.access$300(SQLOperation.java:88)\nat org.apache.hive.service.cli.operation.SQLOperation$3$1.run(SQLOperation.java:293)\nat java.security.AccessController.doPrivileged(Native Method)\nat javax.security.auth.Subject.doAs(Subject.java:415)\nat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1920)\nat org.apache.hive.service.cli.operation.SQLOperation$3.run(SQLOperation.java:306)\nat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\nat java.util.concurrent.FutureTask.run(FutureTask.java:262)\nat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\nat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\nat java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.NullPointerException\nat org.apache.hadoop.hive.ql.plan.ConditionalResolverMergeFiles.generateActualTasks(ConditionalResolverMergeFiles.java:290)\nat org.apache.hadoop.hive.ql.plan.ConditionalResolverMergeFiles.getTasks(ConditionalResolverMergeFiles.java:175)\nat org.apache.hadoop.hive.ql.exec.ConditionalTask.execute(ConditionalTask.java:81)\nat org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:214)\nat org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100)\nat org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1977)\nat org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1690)\nat org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1422)\nat org.apache.hadoop.hive.ql.Driver.run(Driver.java:1206)\nat org.apache.hadoop.hive.ql.Driver.run(Driver.java:1201)\nat org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:237)\n... 11 more \n{noformat}",
            "StackTrace": [
                "2017-05-08 21:32:50,607 ERROR org.apache.hive.service.cli.operation.Operation: [HiveServer2-Background-Pool: Thread-184028]: Error running hive query: ",
                "org.apache.hive.service.cli.HiveSQLException: Error while processing statement: FAILED: Execution Error, return code -101 from org.apache.hadoop.hive.ql.exec.ConditionalTask. null",
                "Caused by: java.lang.NullPointerException",
                "at org.apache.hadoop.hive.ql.plan.ConditionalResolverMergeFiles.generateActualTasks(ConditionalResolverMergeFiles.java:290)",
                "at org.apache.hadoop.hive.ql.plan.ConditionalResolverMergeFiles.getTasks(ConditionalResolverMergeFiles.java:175)",
                "at org.apache.hadoop.hive.ql.exec.ConditionalTask.execute(ConditionalTask.java:81)",
                "at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:214)",
                "at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100)",
                "at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1977)",
                "at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1690)",
                "at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1422)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1206)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1201)",
                "at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:237)"
            ],
            "RootCause": "The NullPointerException in the ConditionalResolverMergeFiles class likely stems from issues with the context object or the list of tasks being improperly initialized or populated.",
            "StepsToReproduce": [
                "Create a partitioned table on S3.",
                "Create a temp table with specified schema.",
                "Load rows into the temp table.",
                "Set the required Hive parameters.",
                "Execute the INSERT OVERWRITE statement."
            ],
            "ExpectedBehavior": "The rows from the temp table should be successfully inserted into the S3 partitioned table without any errors.",
            "ObservedBehavior": "A NullPointerException occurs during the execution of the INSERT OVERWRITE statement, leading to a HiveSQLException.",
            "Suggestions": "Investigate the initialization of the ConditionalResolverMergeFilesCtx object and ensure that the list of tasks is properly populated before being accessed.",
            "problem_location": {
                "files": [
                    "ConditionalResolverMergeFiles.java",
                    "ConditionalTask.java",
                    "Task.java",
                    "Driver.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.ql.plan.ConditionalResolverMergeFiles",
                    "org.apache.hadoop.hive.ql.exec.ConditionalTask",
                    "org.apache.hadoop.hive.ql.exec.Task",
                    "org.apache.hadoop.hive.ql.Driver"
                ],
                "methods": [
                    "ConditionalResolverMergeFiles.generateActualTasks",
                    "ConditionalResolverMergeFiles.getTasks",
                    "ConditionalTask.execute",
                    "Task.executeTask",
                    "Driver.runInternal"
                ]
            },
            "possible_fix": "Ensure that the ConditionalResolverMergeFilesCtx object is properly initialized and populated before being passed to the getTasks method. Additionally, verify that the list of tasks is guaranteed to have the expected number of tasks before accessing it."
        },
        "possible_fix_code": {
            "org.apache.hadoop.hive.ql.plan.ConditionalResolverMergeFiles.getTasks": "  public List<Task<? extends Serializable>> getTasks(HiveConf conf,\n      Object objCtx) {\n    ConditionalResolverMergeFilesCtx ctx = (ConditionalResolverMergeFilesCtx) objCtx;\n    String dirName = ctx.getDir();\n\n    List<Task<? extends Serializable>> resTsks = new ArrayList<Task<? extends Serializable>>();\n    // check if a map-reduce job is needed to merge the files\n    // If the current size is smaller than the target, merge\n    long trgtSize = conf.getLongVar(HiveConf.ConfVars.HIVEMERGEMAPFILESSIZE);\n    long avgConditionSize = conf\n        .getLongVar(HiveConf.ConfVars.HIVEMERGEMAPFILESAVGSIZE);\n    trgtSize = Math.max(trgtSize, avgConditionSize);\n\n    // Ensure that the context and list of tasks are properly initialized\n    if (ctx == null || ctx.getListTasks() == null || ctx.getListTasks().size() < 3) {\n        throw new IllegalStateException(\"Context or task list is not properly initialized.\");\n    }\n\n    Task<? extends Serializable> mvTask = ctx.getListTasks().get(0);\n    Task<? extends Serializable> mrTask = ctx.getListTasks().get(1);\n    Task<? extends Serializable> mrAndMvTask = ctx.getListTasks().get(2);\n\n    try {\n      Path dirPath = new Path(dirName);\n      FileSystem inpFs = dirPath.getFileSystem(conf);\n      DynamicPartitionCtx dpCtx = ctx.getDPCtx();\n\n      if (inpFs.exists(dirPath)) {\n        // For each dynamic partition, check if it needs to be merged.\n        MapWork work;\n        if (mrTask.getWork() instanceof MapredWork) {\n          work = ((MapredWork) mrTask.getWork()).getMapWork();\n        } else if (mrTask.getWork() instanceof TezWork){\n          work = (MapWork) ((TezWork) mrTask.getWork()).getAllWork().get(0);\n        } else if (mrTask.getWork() instanceof SparkWork) {\n          work = (MapWork) ((SparkWork) mrTask.getWork()).getAllWork().get(0);\n        } else {\n          work = (MapWork) mrTask.getWork();\n        }\n\n        int lbLevel = (ctx.getLbCtx() == null) ? 0 : ctx.getLbCtx().calculateListBucketingLevel();\n\n        // Dynamic partition: replace input path (root to dp paths) with dynamic partition\n        // input paths.\n        if (dpCtx != null &&  dpCtx.getNumDPCols() > 0) {\n          int numDPCols = dpCtx.getNumDPCols();\n          int dpLbLevel = numDPCols + lbLevel;\n\n          generateActualTasks(conf, resTsks, trgtSize, avgConditionSize, mvTask, mrTask,\n              mrAndMvTask, dirPath, inpFs, ctx, work, dpLbLevel);\n        } else { // no dynamic partitions\n          if(lbLevel == 0) {\n            // static partition without list bucketing\n            long totalSz = getMergeSize(inpFs, dirPath, avgConditionSize);\n            if (totalSz >= 0) { // add the merge job\n              setupMapRedWork(conf, work, trgtSize, totalSz);\n              resTsks.add(mrTask);\n            } else { // don't need to merge, add the move job\n              resTsks.add(mvTask);\n            }\n          } else {\n            // static partition and list bucketing\n            generateActualTasks(conf, resTsks, trgtSize, avgConditionSize, mvTask, mrTask,\n                mrAndMvTask, dirPath, inpFs, ctx, work, lbLevel);\n          }\n        }\n      } else {\n        resTsks.add(mvTask);\n      }\n    } catch (IOException e) {\n      e.printStackTrace();\n    }\n\n    // Only one of the tasks should ever be added to resTsks\n    assert(resTsks.size() == 1);\n\n    return resTsks;\n  }"
        }
    },
    {
        "filename": "HIVE-9655.json",
        "creation_time": "2015-02-11T20:58:13.000+0000",
        "bug_report": {
            "Title": "Dynamic partition table insertion error",
            "Description": "An error occurs when attempting to insert data from table `t1` into partitioned table `t2`. The operation fails with a Hive Runtime Error indicating that the field `_col2` cannot be found in the data being processed. This suggests a mismatch between the expected schema of the data and the actual schema being used during the insert operation.",
            "StackTrace": [
                "2015-02-11 12:50:52,756 ERROR [LocalJobRunner Map Task Executor #0]: mr.ExecMapper (ExecMapper.java:map(178)) - org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {\"c1\":1,\"c2\":\"one\"}",
                "at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:503)",
                "at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:170)",
                "at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)",
                "at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:243)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:262)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:745)",
                "Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: cannot find field _col2 from [0:_col0, 1:_col1]",
                "at org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.processOp(ReduceSinkOperator.java:397)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)",
                "at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:84)",
                "at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:815)",
                "at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:95)",
                "at org.apache.hadoop.hive.ql.exec.MapOperator$MapOpCtx.forward(MapOperator.java:157)",
                "at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:493)",
                "... 10 more",
                "Caused by: java.lang.RuntimeException: cannot find field _col2 from [0:_col0, 1:_col1]",
                "at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.getStandardStructFieldRef(ObjectInspectorUtils.java:410)",
                "at org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector.getStructFieldRef(StandardStructObjectInspector.java:147)",
                "at org.apache.hadoop.hive.ql.exec.ExprNodeColumnEvaluator.initialize(ExprNodeColumnEvaluator.java:55)",
                "at org.apache.hadoop.hive.ql.exec.Operator.initEvaluators(Operator.java:954)",
                "at org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.processOp(ReduceSinkOperator.java:325)",
                "... 16 more"
            ],
            "RootCause": "The root cause of the error is that the insert operation is attempting to access a field `_col2` that does not exist in the schema of the data being processed. The data being inserted from `t1` does not match the expected schema of `t2`, leading to the Hive Runtime Error.",
            "StepsToReproduce": [
                "Create table `t1` with schema (c1 bigint, c2 string).",
                "Create table `t2` with schema (c1 int, c2 string) partitioned by (p1 string).",
                "Load data into `t1` using the command: load data local inpath 'data' into table t1.",
                "Set dynamic partition mode to nonstrict: SET hive.exec.dynamic.partition.mode=nonstrict.",
                "Attempt to insert data from `t1` into `t2` using the command: insert overwrite table t2 partition(p1) select *, c1 as p1 from t1 distribute by p1."
            ],
            "ExpectedBehavior": "The data from table `t1` should be successfully inserted into the partitioned table `t2` without any errors, with the correct mapping of fields.",
            "ObservedBehavior": "The insert operation fails with a Hive Runtime Error indicating that the field `_col2` cannot be found, resulting in an exception.",
            "Suggestions": "Verify the schema of both tables `t1` and `t2` to ensure they match the expected structure. Ensure that the data being inserted contains all required fields as per the schema of `t2`. Consider modifying the insert statement to correctly reference the existing columns.",
            "problem_location": {
                "files": [
                    "ExecMapper.java",
                    "MapOperator.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.ql.exec.ExecMapper",
                    "org.apache.hadoop.hive.ql.exec.MapOperator"
                ],
                "methods": [
                    "ExecMapper.map",
                    "MapOperator.process"
                ]
            },
            "possible_fix": "Ensure that the insert statement correctly references the columns in `t1` and matches the expected schema of `t2`. For example, modify the insert statement to explicitly select the correct columns from `t1` that correspond to the schema of `t2`."
        },
        "possible_fix_code": {
            "ExecMapper.map": "  public void map(Object key, Object value, OutputCollector output,\n      Reporter reporter) throws IOException {\n    if (oc == null) {\n      oc = output;\n      rp = reporter;\n      OperatorUtils.setChildrenCollector(mo.getChildOperators(), output);\n      mo.setReporter(rp);\n      MapredContext.get().setReporter(reporter);\n    }\n    // reset the execContext for each new row\n    execContext.resetRow();\n\n    try {\n      if (mo.getDone()) {\n        done = true;\n      } else {\n        // Since there is no concept of a group, we don't invoke\n        // startGroup/endGroup for a mapper\n        // Ensure that the insert statement correctly references the columns in t1\n        // and matches the expected schema of t2.\n        mo.process((Writable)value);\n      }\n    } catch (Throwable e) {\n      abort = true;\n      if (e instanceof OutOfMemoryError) {\n        // Don't create a new object if we are already out of memory\n        throw (OutOfMemoryError) e;\n      } else {\n        l4j.fatal(StringUtils.stringifyException(e));\n        throw new RuntimeException(e);\n      }\n    }\n  }"
        }
    },
    {
        "filename": "HIVE-11441.json",
        "creation_time": "2015-08-03T17:42:24.000+0000",
        "bug_report": {
            "Title": "No DDL allowed on table if user accidentally set table location wrong",
            "Description": "When a user sets an incorrect HDFS location for a Hive table, the system fails to provide an immediate error or correction opportunity. Instead, it allows the user to attempt to set a valid location or drop the table, leading to a connection error when the invalid location is accessed. This issue is exacerbated when the StorageBasedAuthorizationProvider is enabled, as it prevents the user from executing DDL commands on the table until the location is corrected.",
            "StackTrace": [
                "2015-07-30 12:19:43,720 ERROR [main]: ql.Driver (SessionState.java:printError(833)) - FAILED: SemanticException Unable to fetch table testloc. java.net.ConnectException: Call From hdpsecb02.secb.hwxsup.com/172.25.16.178 to hdpsecb02.secb.hwxsup.com:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused",
                "org.apache.hadoop.hive.ql.parse.SemanticException: Unable to fetch table testloc. java.net.ConnectException: Call From hdpsecb02.secb.hwxsup.com/172.25.16.178 to hdpsecb02.secb.hwxsup.com:8020 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.getTable(BaseSemanticAnalyzer.java:1323)",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.getTable(BaseSemanticAnalyzer.java:1309)",
                "at org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.addInputsOutputsAlterTable(DDLSemanticAnalyzer.java:1387)",
                "at org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeAlterTableLocation(DDLSemanticAnalyzer.java:1452)",
                "at org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeInternal(DDLSemanticAnalyzer.java:295)",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:221)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:417)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:305)",
                "at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1069)",
                "at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1131)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1006)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:996)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:247)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:199)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:410)",
                "at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:783)",
                "at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:677)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.util.RunJar.run(RunJar.java:221)",
                "at org.apache.hadoop.util.RunJar.main(RunJar.java:136)"
            ],
            "RootCause": "The root cause of the issue is a failure to connect to the specified HDFS location due to an incorrect hostname or path. The `getTable` method in the Hive class attempts to fetch the table from the metastore, but the connection to the Hadoop server is refused, resulting in a `ConnectException`. This indicates that the metastore client cannot connect to the Hive metastore service, likely due to misconfiguration or the service not running.",
            "StepsToReproduce": [
                "Create a table with an incorrect location: `create table testwrongloc(id int);`",
                "Attempt to set an invalid HDFS location: `alter table testwrongloc set location \"hdfs://a-valid-hostname/tmp/testwrongloc\";`",
                "Attempt to set a valid location or drop the table: `alter table testwrongloc set location \"hdfs://correct-host:8020/tmp/testwrongloc\"` or `drop table testwrongloc;`"
            ],
            "ExpectedBehavior": "The system should validate the HDFS location upon setting it and throw an error if the location is invalid, preventing the user from executing further DDL commands until the issue is resolved.",
            "ObservedBehavior": "The system allows the user to set an invalid HDFS location, leading to a connection error when attempting to access the table. The error message indicates that the host is not reachable, and the user is unable to perform further DDL operations on the table.",
            "Suggestions": "1. Ensure that the Hive metastore service is running and accessible. 2. Validate the `hive.metastore.uris` configuration in `hive-site.xml` to ensure it points to the correct metastore service. 3. Implement validation checks for HDFS paths when setting table locations to prevent invalid configurations.",
            "problem_location": {
                "files": [
                    "BaseSemanticAnalyzer.java",
                    "Driver.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer",
                    "org.apache.hadoop.hive.ql.Driver"
                ],
                "methods": [
                    "BaseSemanticAnalyzer.getTable",
                    "Driver.compile"
                ]
            },
            "possible_fix": "Check the configuration for the Hive metastore connection in `hive-site.xml`, specifically the `hive.metastore.uris` property. Ensure that the HDFS location is valid and accessible. Additionally, consider adding validation logic in the `getTable` method to check the validity of the HDFS path before attempting to connect."
        },
        "possible_fix_code": {
            "BaseSemanticAnalyzer.getTable": "  protected Table getTable(String database, String tblName, boolean throwException)\n      throws SemanticException {\n    Table tab;\n    // Validate the database and table name before attempting to fetch the table\n    if (tblName == null || tblName.isEmpty()) {\n        throw new SemanticException(\"Table name cannot be null or empty.\");\n    }\n    try {\n      tab = database == null ? db.getTable(tblName, false)\n          : db.getTable(database, tblName, false);\n    }\n    catch (InvalidTableException e) {\n      throw new SemanticException(ErrorMsg.INVALID_TABLE.getMsg(tblName), e);\n    }\n    catch (Exception e) {\n      throw new SemanticException(e.getMessage(), e);\n    }\n    if (tab == null && throwException) {\n      throw new SemanticException(ErrorMsg.INVALID_TABLE.getMsg(tblName));\n    }\n    return tab;\n  }"
        }
    },
    {
        "filename": "HIVE-10801.json",
        "creation_time": "2015-05-22T19:43:23.000+0000",
        "bug_report": {
            "Title": "'drop view' fails throwing java.lang.NullPointerException",
            "Description": "When attempting to drop a view in Hive, a NullPointerException is thrown, as indicated by the logs. The error occurs in the Hive metastore, specifically during the execution of the `drop_table_with_environment_context` method. The stack trace reveals that the issue arises when the `isPathEncrypted` method in the `HdfsEncryptionShim` class is called, which checks if the table's path is encrypted. The logs suggest that the `tblPath` variable is null at this point, leading to the exception. This indicates a potential issue with the table's metadata, particularly the storage descriptor (SD) being null or improperly initialized.",
            "StackTrace": [
                "2015-05-21 11:53:06,126 ERROR [HiveServer2-Background-Pool: Thread-197]: hdfs.KeyProviderCache (KeyProviderCache.java:createKeyProviderURI(87)) - Could not find uri with key [dfs.encryption.key.provider.uri] to create a keyProvider !!",
                "2015-05-21 11:53:06,134 ERROR [HiveServer2-Background-Pool: Thread-197]: metastore.RetryingHMSHandler (RetryingHMSHandler.java:invoke(155)) - MetaException(message:java.lang.NullPointerException)",
                "Caused by: java.lang.NullPointerException",
                "at org.apache.hadoop.hive.shims.Hadoop23Shims$HdfsEncryptionShim.isPathEncrypted(Hadoop23Shims.java:1213)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_table_core(HiveMetaStore.java:1546)",
                "at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.drop_table_with_environment_context(HiveMetaStore.java:1734)"
            ],
            "RootCause": "The root cause of the NullPointerException is likely due to the `tblPath` being null when the `isPathEncrypted` method is called. This suggests that the table's storage descriptor (SD) is not properly initialized, indicating potential corruption in the table's metadata.",
            "StepsToReproduce": [
                "1. Attempt to drop a view in Hive using the command: DROP VIEW <view_name>.",
                "2. Observe the Hive logs for any errors or exceptions."
            ],
            "ExpectedBehavior": "The view should be dropped successfully without any exceptions, and the corresponding metadata should be updated accordingly.",
            "ObservedBehavior": "A NullPointerException is thrown, indicating that the path to the table is not set correctly, leading to a failure in the drop operation.",
            "Suggestions": "Investigate the table's metadata to ensure that the storage descriptor is properly initialized. Additionally, check the implementation of the `isPathEncrypted` method to handle null paths appropriately.",
            "problem_location": {
                "files": [
                    "metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler",
                    "org.apache.hadoop.hive.shims.Hadoop23Shims$HdfsEncryptionShim"
                ],
                "methods": [
                    "drop_table_with_environment_context",
                    "drop_table_core",
                    "isPathEncrypted"
                ]
            },
            "possible_fix": "Ensure that the storage descriptor (SD) of the table is properly initialized before calling the `isPathEncrypted` method. This may involve adding checks to validate the SD and the table's path before proceeding with the drop operation."
        },
        "possible_fix_code": {
            "drop_table_with_environment_context": "    public void drop_table_with_environment_context(final String dbname, final String name,\n        final boolean deleteData, final EnvironmentContext envContext)\n        throws NoSuchObjectException, MetaException {\n      startTableFunction(\"drop_table\", dbname, name);\n\n      boolean success = false;\n      Exception ex = null;\n      try {\n        // Ensure the table metadata is valid before proceeding\n        Table tbl = get_table_core(dbname, name);\n        if (tbl == null || tbl.getSd() == null || tbl.getSd().getLocation() == null) {\n          throw new MetaException(\"Table metadata is corrupted or path is not set\");\n        }\n        success = drop_table_core(getMS(), dbname, name, deleteData, envContext, null);\n      } catch (IOException e) {\n        ex = e;\n        throw new MetaException(e.getMessage());\n      } catch (Exception e) {\n        ex = e;\n        if (e instanceof MetaException) {\n          throw (MetaException) e;\n        } else if (e instanceof NoSuchObjectException) {\n          throw (NoSuchObjectException) e;\n        } else {\n          throw newMetaException(e);\n        }\n      } finally {\n        endFunction(\"drop_table\", success, ex, name);\n      }\n    }"
        }
    },
    {
        "filename": "HIVE-9141.json",
        "creation_time": "2014-12-17T07:23:05.000+0000",
        "bug_report": {
            "Title": "HiveOnTez: mix of union all, distinct, group by generates error",
            "Description": "A ClassCastException occurs when executing a Hive query that involves a combination of UNION ALL, DISTINCT, and GROUP BY operations in a Tez execution environment. The error arises specifically when the system attempts to cast a MapWork object to a ReduceWork object, indicating a logical error in the task generation process within the GenTezWork class.",
            "StackTrace": [
                "2014-12-16 23:19:13,593 ERROR ql.Driver (SessionState.java:printError(834)) - FAILED: ClassCastException org.apache.hadoop.hive.ql.plan.MapWork cannot be cast to org.apache.hadoop.hive.ql.plan.ReduceWork",
                "java.lang.ClassCastException: org.apache.hadoop.hive.ql.plan.MapWork cannot be cast to org.apache.hadoop.hive.ql.plan.ReduceWork",
                "at org.apache.hadoop.hive.ql.parse.GenTezWork.process(GenTezWork.java:361)",
                "at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:90)",
                "at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatchAndReturn(DefaultGraphWalker.java:94)",
                "at org.apache.hadoop.hive.ql.parse.GenTezWorkWalker.walk(GenTezWorkWalker.java:87)",
                "at org.apache.hadoop.hive.ql.parse.GenTezWorkWalker.walk(GenTezWorkWalker.java:103)",
                "at org.apache.hadoop.hive.ql.parse.GenTezWorkWalker.walk(GenTezWorkWalker.java:103)",
                "at org.apache.hadoop.hive.ql.parse.GenTezWorkWalker.walk(GenTezWorkWalker.java:103)",
                "at org.apache.hadoop.hive.ql.parse.GenTezWorkWalker.walk(GenTezWorkWalker.java:103)",
                "at org.apache.hadoop.hive.ql.parse.GenTezWorkWalker.startWalking(GenTezWorkWalker.java:69)",
                "at org.apache.hadoop.hive.ql.parse.TezCompiler.generateTaskTree(TezCompiler.java:368)",
                "at org.apache.hadoop.hive.ql.parse.TaskCompiler.compile(TaskCompiler.java:202)",
                "at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:10202)",
                "at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:224)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:419)",
                "at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:305)",
                "at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1107)",
                "at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1155)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1044)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1034)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:206)",
                "at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:158)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:369)",
                "at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:304)",
                "at org.apache.hadoop.hive.ql.QTestUtil.executeClient(QTestUtil.java:834)",
                "at org.apache.hadoop.hive.cli.TestMiniTezCliDriver.runTest(TestMiniTezCliDriver.java:136)",
                "at org.apache.hadoop.hive.cli.TestMiniTezCliDriver.testCliDriver_uniontez2(TestMiniTezCliDriver.java:120)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)"
            ],
            "RootCause": "The root cause of the issue is a logical error in the GenTezWork.process method, where the system incorrectly attempts to cast a MapWork object to a ReduceWork object. This typically occurs when the logic fails to distinguish between the two types of work based on the operators being processed.",
            "StepsToReproduce": [
                "Set the Hive execution engine to Tez: set hive.execution.engine=tez;",
                "Execute the following query: SELECT key, value FROM (SELECT key, value FROM src UNION ALL SELECT key, key as value FROM (SELECT DISTINCT key FROM (SELECT key, value FROM src UNION ALL SELECT key, value FROM src)t1 GROUP BY key, value)t2)t3 GROUP BY key, value;"
            ],
            "ExpectedBehavior": "The query should execute successfully without any ClassCastException, returning the expected results based on the UNION ALL, DISTINCT, and GROUP BY operations.",
            "ObservedBehavior": "The query fails with a ClassCastException indicating that a MapWork object cannot be cast to a ReduceWork object.",
            "Suggestions": "Review the logic in the GenTezWork.process method to ensure that it correctly distinguishes between when to create MapWork and when to create ReduceWork. Ensure that the utility methods responsible for creating these work types are being called correctly.",
            "problem_location": {
                "files": [
                    "GenTezWork.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.ql.parse.GenTezWork"
                ],
                "methods": [
                    "GenTezWork.process"
                ]
            },
            "possible_fix": "Modify the GenTezWork.process method to correctly handle the creation of MapWork and ReduceWork based on the operator types being processed. Ensure that the logic does not attempt to cast between these two types incorrectly."
        },
        "possible_fix_code": {
            "GenTezWork.process": "  public Object process(Node nd, Stack<Node> stack,\n      NodeProcessorCtx procContext, Object... nodeOutputs)\n      throws SemanticException {\n\n    GenTezProcContext context = (GenTezProcContext) procContext;\n\n    assert context != null && context.currentTask != null\n        && context.currentRootOperator != null;\n\n    // Operator is a file sink or reduce sink. Something that forces\n    // a new vertex.\n    Operator<?> operator = (Operator<?>) nd;\n\n    // root is the start of the operator pipeline we\\'re currently\n    // packing into a vertex, typically a table scan, union or join\n    Operator<?> root = context.currentRootOperator;\n\n    LOG.debug(\"Root operator: \" + root);\n    LOG.debug(\"Leaf operator: \" + operator);\n\n    if (context.clonedReduceSinks.contains(operator)) {\n      // if we\\'re visiting a terminal we\\'ve created ourselves,\n      // just skip and keep going\n      return null;\n    }\n\n    TezWork tezWork = context.currentTask.getWork();\n\n    // Right now the work graph is pretty simple. If there is no\n    // Preceding work we have a root and will generate a map\n    // vertex. If there is a preceding work we will generate\n    // a reduce vertex\n    BaseWork work;\n    if (context.rootToWorkMap.containsKey(root)) {\n      // having seen the root operator before means there was a branch in the\n      // operator graph. There\\'s typically two reasons for that: a) mux/demux\n      // b) multi insert. Mux/Demux will hit the same leaf again, multi insert\n      // will result into a vertex with multiple FS or RS operators.\n      if (context.childToWorkMap.containsKey(operator)) {\n        // if we\\'ve seen both root and child, we can bail.\n\n        // clear out the mapjoin set. we don\\'t need it anymore.\n        context.currentMapJoinOperators.clear();\n\n        // clear out the union set. we don\\'t need it anymore.\n        context.currentUnionOperators.clear();\n\n        return null;\n      } else {\n        // At this point we don\\'t have to do anything special. Just\n        // run through the regular paces w/o creating a new task.\n        work = context.rootToWorkMap.get(root);\n      }\n    } else {\n      // create a new vertex\n      if (context.preceedingWork == null) {\n        work = utils.createMapWork(context, root, tezWork, null);\n      } else {\n        // Ensure we are creating ReduceWork only when appropriate\n        if (context.preceedingWork instanceof MapWork) {\n          work = utils.createReduceWork(context, root, tezWork);\n        } else {\n          work = utils.createMapWork(context, root, tezWork, null);\n        }\n      }\n      context.rootToWorkMap.put(root, work);\n    }\n\n    // this is where we set the sort columns that we will be using for KeyValueInputMerge\n    if (operator instanceof DummyStoreOperator) {\n      work.addSortCols(root.getOpTraits().getSortCols().get(0));\n    }\n\n    if (!context.childToWorkMap.containsKey(operator)) {\n      List<BaseWork> workItems = new LinkedList<BaseWork>();\n      workItems.add(work);\n      context.childToWorkMap.put(operator, workItems);\n    } else {\n      context.childToWorkMap.get(operator).add(work);\n    }\n\n    // this transformation needs to be first because it changes the work item itself.\n    // which can affect the working of all downstream transformations.\n    if (context.currentMergeJoinOperator != null) {\n      // we are currently walking the big table side of the merge join. we need to create or hook up\n      // merge join work.\n      MergeJoinWork mergeJoinWork = null;\n      if (context.opMergeJoinWorkMap.containsKey(context.currentMergeJoinOperator)) {\n        // we have found a merge work corresponding to this closing operator. Hook up this work.\n        mergeJoinWork = context.opMergeJoinWorkMap.get(context.currentMergeJoinOperator);\n      } else {\n        // we need to create the merge join work\n        mergeJoinWork = new MergeJoinWork();\n        mergeJoinWork.setMergeJoinOperator(context.currentMergeJoinOperator);\n        tezWork.add(mergeJoinWork);\n        context.opMergeJoinWorkMap.put(context.currentMergeJoinOperator, mergeJoinWork);\n      }\n      // connect the work correctly.\n      work.addSortCols(root.getOpTraits().getSortCols().get(0));\n      mergeJoinWork.addMergedWork(work, null);\n      Operator<? extends OperatorDesc> parentOp =\n          getParentFromStack(context.currentMergeJoinOperator, stack);\n      int pos = context.currentMergeJoinOperator.getTagForOperator(parentOp);\n      work.setTag(pos);\n      tezWork.setVertexType(work, VertexType.MULTI_INPUT_UNINITIALIZED_EDGES);\n      for (BaseWork parentWork : tezWork.getParents(work)) {\n        TezEdgeProperty edgeProp = tezWork.getEdgeProperty(parentWork, work);\n        tezWork.disconnect(parentWork, work);\n        tezWork.connect(parentWork, mergeJoinWork, edgeProp);\n      }\n\n      for (BaseWork childWork : tezWork.getChildren(work)) {\n        TezEdgeProperty edgeProp = tezWork.getEdgeProperty(work, childWork);\n        tezWork.disconnect(work, childWork);\n        tezWork.connect(mergeJoinWork, childWork, edgeProp);\n      }\n      tezWork.remove(work);\n      context.rootToWorkMap.put(root, mergeJoinWork);\n      context.childToWorkMap.get(operator).remove(work);\n      context.childToWorkMap.get(operator).add(mergeJoinWork);\n      work = mergeJoinWork;\n      context.currentMergeJoinOperator = null;\n    }\n\n    // remember which mapjoin operator links with which work\n    if (!context.currentMapJoinOperators.isEmpty()) {\n      for (MapJoinOperator mj: context.currentMapJoinOperators) {\n        LOG.debug(\"Processing map join: \" + mj);\n        // remember the mapping in case we scan another branch of the\n        // mapjoin later\n        if (!context.mapJoinWorkMap.containsKey(mj)) {\n          List<BaseWork> workItems = new LinkedList<BaseWork>();\n          workItems.add(work);\n          context.mapJoinWorkMap.put(mj, workItems);\n        } else {\n          context.mapJoinWorkMap.get(mj).add(work);\n        }\n\n        /*\n         * this happens in case of map join operations.\n         * The tree looks like this:\n         *\n         *        RS <--- we are here perhaps\n         *        |\n         *     MapJoin\n         *     /     \\\n         *   RS       TS\n         *  /\n         * TS\n         *\n         * If we are at the RS pointed above, and we may have already visited the\n         * RS following the TS, we have already generated work for the TS-RS.\n         * We need to hook the current work to this generated work.\n         */\n        if (context.linkOpWithWorkMap.containsKey(mj)) {\n          Map<BaseWork,TezEdgeProperty> linkWorkMap = context.linkOpWithWorkMap.get(mj);\n          if (linkWorkMap != null) {\n            if (context.linkChildOpWithDummyOp.containsKey(mj)) {\n              for (Operator<?> dummy: context.linkChildOpWithDummyOp.get(mj)) {\n                work.addDummyOp((HashTableDummyOperator) dummy);\n              }\n            }\n            for (Entry<BaseWork,TezEdgeProperty> parentWorkMap : linkWorkMap.entrySet()) {\n              BaseWork parentWork = parentWorkMap.getKey();\n              LOG.debug(\"connecting \"+parentWork.getName()+\" with \"+work.getName());\n              TezEdgeProperty edgeProp = parentWorkMap.getValue();\n              tezWork.connect(parentWork, work, edgeProp);\n              if (edgeProp.getEdgeType() == EdgeType.CUSTOM_EDGE) {\n                tezWork.setVertexType(work, VertexType.INITIALIZED_EDGES);\n              }\n\n              // need to set up output name for reduce sink now that we know the name\n              // of the downstream work\n              for (ReduceSinkOperator r:\n                     context.linkWorkWithReduceSinkMap.get(parentWork)) {\n                if (r.getConf().getOutputName() != null) {\n                  LOG.debug(\"Cloning reduce sink for multi-child broadcast edge\");\n                  // we\\'ve already set this one up. Need to clone for the next work.\n                  r = (ReduceSinkOperator) OperatorFactory.getAndMakeChild(\n                      (ReduceSinkDesc)r.getConf().clone(), r.getParentOperators());\n                  context.clonedReduceSinks.add(r);\n                }\n                r.getConf().setOutputName(work.getName());\n                context.connectedReduceSinks.add(r);\n              }\n            }\n          }\n        }\n      }\n      // clear out the set. we don\\'t need it anymore.\n      context.currentMapJoinOperators.clear();\n    }\n\n    if (!context.currentUnionOperators.isEmpty()) {\n      // if there are union all operators we need to add the work to the set\n      // of union operators.\n\n      UnionWork unionWork;\n      if (context.unionWorkMap.containsKey(operator)) {\n        // we\\'ve seen this terminal before and have created a union work object.\n        // just need to add this work to it. There will be no children of this one\n        // since we\\'ve passed this operator before.\n        assert operator.getChildOperators().isEmpty();\n        unionWork = (UnionWork) context.unionWorkMap.get(operator);\n\n      } else {\n        // first time through. we need to create a union work object and add this\n        // work to it. Subsequent work should reference the union and not the actual\n        // work.\n        unionWork = utils.createUnionWork(context, operator, tezWork);\n      }\n\n      // finally hook everything up\n      LOG.debug(\"Connecting union work (\"+unionWork+\") with work (\"+work+\")\");\n      TezEdgeProperty edgeProp = new TezEdgeProperty(EdgeType.CONTAINS);\n      tezWork.connect(unionWork, work, edgeProp);\n      unionWork.addUnionOperators(context.currentUnionOperators);\n      context.currentUnionOperators.clear();\n      context.workWithUnionOperators.add(work);\n      work = unionWork;\n    }\n\n\n    // This is where we cut the tree as described above. We also remember that\n    // we might have to connect parent work with this work later.\n    boolean removeParents = false;\n    for (Operator<?> parent: new ArrayList<Operator<?>>(root.getParentOperators())) {\n      removeParents = true;\n      context.leafOperatorToFollowingWork.put(parent, work);\n      LOG.debug(\"Removing \" + parent + \" as parent from \" + root);\n    }\n    if (removeParents) {\n      for (Operator<?> parent : new ArrayList<Operator<?>>(root.getParentOperators())) {\n        root.removeParent(parent);\n      }\n    }\n\n    // We\\'re scanning a tree from roots to leaf (this is not technically\n    // correct, demux and mux operators might form a diamond shape, but\n    // we will only scan one path and ignore the others, because the\n    // diamond shape is always contained in a single vertex). The scan\n    // is depth first and because we remove parents when we pack a pipeline\n    // into a vertex we will never visit any node twice. But because of that\n    // we might have a situation where we need to connect \\'work\\' that comes after\n    // the \\'work\\' we\\'re currently looking at.\n    //\n    // Also note: the concept of leaf and root is reversed in hive for historical\n    // reasons. Roots are data sources, leaves are data sinks. I know.\n    if (context.leafOperatorToFollowingWork.containsKey(operator)) {\n\n      BaseWork followingWork = context.leafOperatorToFollowingWork.get(operator);\n      long bytesPerReducer = context.conf.getLongVar(HiveConf.ConfVars.BYTESPERREDUCER);\n\n      LOG.debug(\"Second pass. Leaf operator: \"+operator\n        +\" has common downstream work:\"+followingWork);\n\n      if (operator instanceof DummyStoreOperator) {\n        // this is the small table side.\n        assert (followingWork instanceof MergeJoinWork);\n        MergeJoinWork mergeJoinWork = (MergeJoinWork) followingWork;\n        CommonMergeJoinOperator mergeJoinOp = mergeJoinWork.getMergeJoinOperator();\n        work.setTag(mergeJoinOp.getTagForOperator(operator));\n        mergeJoinWork.addMergedWork(null, work);\n        tezWork.setVertexType(mergeJoinWork, VertexType.MULTI_INPUT_UNINITIALIZED_EDGES);\n        for (BaseWork parentWork : tezWork.getParents(work)) {\n          TezEdgeProperty edgeProp = tezWork.getEdgeProperty(parentWork, work);\n          tezWork.disconnect(parentWork, work);\n          tezWork.connect(parentWork, mergeJoinWork, edgeProp);\n        }\n        work = mergeJoinWork;\n      } else {\n        // need to add this branch to the key + value info\n        assert operator instanceof ReduceSinkOperator\n            && ((followingWork instanceof ReduceWork) || (followingWork instanceof MergeJoinWork)\n                || followingWork instanceof UnionWork);\n        ReduceSinkOperator rs = (ReduceSinkOperator) operator;\n        ReduceWork rWork = null;\n        if (followingWork instanceof MergeJoinWork) {\n          MergeJoinWork mergeJoinWork = (MergeJoinWork) followingWork;\n          rWork = (ReduceWork) mergeJoinWork.getMainWork();\n        } else if (followingWork instanceof UnionWork) {\n          // this can only be possible if there is merge work followed by the union\n          UnionWork unionWork = (UnionWork) followingWork;\n          int index = getFollowingWorkIndex(tezWork, unionWork, rs);\n          if (index != -1) {\n            BaseWork baseWork = tezWork.getChildren(unionWork).get(index);\n            if (baseWork instanceof MergeJoinWork) {\n              MergeJoinWork mergeJoinWork = (MergeJoinWork) baseWork;\n              // disconnect the connection to union work and connect to merge work\n              followingWork = mergeJoinWork;\n              rWork = (ReduceWork) mergeJoinWork.getMainWork();\n            } else {\n              rWork = (ReduceWork) baseWork;\n            }\n          } else {\n            throw new SemanticException(\"Following work not found for the reduce sink: \"\n                + rs.getName());\n          }\n        } else {\n          rWork = (ReduceWork) followingWork;\n        }\n        GenMapRedUtils.setKeyAndValueDesc(rWork, rs);\n\n        // remember which parent belongs to which tag\n        int tag = rs.getConf().getTag();\n        rWork.getTagToInput().put(tag == -1 ? 0 : tag, work.getName());\n\n        // remember the output name of the reduce sink\n        rs.getConf().setOutputName(rWork.getName());\n\n        if (!context.connectedReduceSinks.contains(rs)) {\n          // add dependency between the two work items\n          TezEdgeProperty edgeProp;\n          if (rWork.isAutoReduceParallelism()) {\n            edgeProp =\n                new TezEdgeProperty(context.conf, EdgeType.SIMPLE_EDGE, true,\n                    rWork.getMinReduceTasks(), rWork.getMaxReduceTasks(), bytesPerReducer);\n          } else {\n            edgeProp = new TezEdgeProperty(EdgeType.SIMPLE_EDGE);\n          }\n          tezWork.connect(work, followingWork, edgeProp);\n          context.connectedReduceSinks.add(rs);\n        }\n      }\n    } else {\n      LOG.debug(\"First pass. Leaf operator: \"+operator);\n    }\n\n    // No children means we\\'re at the bottom. If there are more operators to scan\n    // the next item will be a new root.\n    if (!operator.getChildOperators().isEmpty()) {\n      assert operator.getChildOperators().size() == 1;\n      context.parentOfRoot = operator;\n      context.currentRootOperator = operator.getChildOperators().get(0);\n      context.preceedingWork = work;\n    }\n\n    return null;\n  }"
        }
    },
    {
        "filename": "HIVE-10010.json",
        "creation_time": "2015-03-18T17:48:46.000+0000",
        "bug_report": {
            "Title": "Alter table results in NPE [hbase-metastore branch]",
            "Description": "An attempt to alter a table in the Hive metastore results in a NullPointerException (NPE). The stack trace indicates that the NPE occurs during the initialization of a StorageDescriptor object, which is part of the Table class. This suggests that an object expected to be initialized (specifically, the tTable variable) is null when the copy method is invoked.",
            "StackTrace": [
                "2015-03-18 10:45:54,189 ERROR [main]: exec.DDLTask (DDLTask.java:failed(512)) - java.lang.NullPointerException",
                "at org.apache.hadoop.hive.metastore.api.StorageDescriptor.<init>(StorageDescriptor.java:239)",
                "at org.apache.hadoop.hive.metastore.api.Table.<init>(Table.java:270)",
                "at org.apache.hadoop.hive.metastore.api.Table.deepCopy(Table.java:310)",
                "at org.apache.hadoop.hive.ql.metadata.Table.copy(Table.java:856)",
                "at org.apache.hadoop.hive.ql.exec.DDLTask.alterTable(DDLTask.java:3329)",
                "at org.apache.hadoop.hive.ql.exec.DDLTask.execute(DDLTask.java:329)",
                "at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:160)",
                "at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:88)",
                "at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1644)",
                "at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1403)",
                "at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1189)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1055)",
                "at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1045)",
                "at org.apache.hadoop.hive.metastore.hbase.TestHBaseMetastoreSql.table(TestHBaseMetastoreSql.java:89)"
            ],
            "RootCause": "The NullPointerException is caused by the tTable object being null when the deepCopy method is called in the copy method of the Table class. This can occur if the initialize method, which sets tTable, is not called before invoking copy.",
            "StepsToReproduce": [
                "1. Attempt to alter a table in the Hive metastore.",
                "2. Observe the resulting NullPointerException in the logs."
            ],
            "ExpectedBehavior": "The alter table operation should complete successfully without throwing any exceptions.",
            "ObservedBehavior": "A NullPointerException is thrown, indicating that an object was not properly initialized.",
            "Suggestions": "Ensure that the initialize method is called before the copy method to properly set the tTable variable. Review the code paths leading to the copy method to confirm that initialization occurs as expected.",
            "problem_location": {
                "files": [
                    "ql/src/java/org/apache/hadoop/hive/ql/metadata/Table.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.metastore.api.Table",
                    "org.apache.hadoop.hive.ql.metadata.Table"
                ],
                "methods": [
                    "Table.copy",
                    "Table.initialize",
                    "Table.setTTable"
                ]
            },
            "possible_fix": "In the copy method of the Table class, ensure that the initialize method is called with a valid Table instance before invoking deepCopy. For example, modify the constructor to check if tTable is null and call initialize if necessary."
        },
        "possible_fix_code": {
            "org.apache.hadoop.hive.ql.metadata.Table.copy": "  public Table copy() throws HiveException {\n    if (tTable == null) {\n        throw new HiveException(\"Table is not initialized. Please call initialize() before copy.\");\n    }\n    return new Table(tTable.deepCopy());\n  }"
        }
    },
    {
        "filename": "HIVE-7763.json",
        "creation_time": "2014-08-18T09:35:09.000+0000",
        "bug_report": {
            "Title": "Failed to query TABLESAMPLE on empty bucket table [Spark Branch]",
            "Description": "An exception occurs when attempting to query a TABLESAMPLE on an empty bucket table in a Hive job running on Spark. The error indicates a failure during the initialization of a map operator, specifically a `RuntimeException` caused by a `HiveException` stating that the 'Configuration and input path are inconsistent'. This suggests a mismatch between the configuration settings and the actual input data being processed.",
            "StackTrace": [
                "2014-08-18 16:23:15,213 ERROR [Executor task launch worker-0]: executor.Executor (Logging.scala:logError(96)) - Exception in task 0.0 in stage 1.0 (TID 0)",
                "java.lang.RuntimeException: Map operator initialization failed",
                "at org.apache.hadoop.hive.ql.exec.spark.SparkMapRecordHandler.init(SparkMapRecordHandler.java:127)",
                "at org.apache.hadoop.hive.ql.exec.spark.HiveMapFunction.call(HiveMapFunction.java:52)",
                "at org.apache.hadoop.hive.ql.exec.spark.HiveMapFunction.call(HiveMapFunction.java:30)",
                "at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$7$1.apply(JavaRDDLike.scala:164)",
                "at org.apache.spark.api.java.JavaRDDLike$$anonfun$fn$7$1.apply(JavaRDDLike.scala:164)",
                "at org.apache.spark.rdd.RDD$$anonfun$13.apply(RDD.scala:596)",
                "at org.apache.spark.rdd.RDD$$anonfun$13.apply(RDD.scala:596)",
                "at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)",
                "at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)",
                "at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)",
                "at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)",
                "at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)",
                "at org.apache.spark.scheduler.Task.run(Task.scala:54)",
                "at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:199)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:722)",
                "Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: org.apache.hadoop.hive.ql.metadata.HiveException: Configuration and input path are inconsistent",
                "at org.apache.hadoop.hive.ql.exec.MapOperator.setChildren(MapOperator.java:404)",
                "at org.apache.hadoop.hive.ql.exec.spark.SparkMapRecordHandler.init(SparkMapRecordHandler.java:93)"
            ],
            "RootCause": "The root cause of the issue is a `HiveException` indicating that the configuration and input path are inconsistent. This occurs when the input paths are not correctly mapped to their aliases in the `MapWork` class, leading to a failure in the `setChildren` method of the `MapOperator` class.",
            "StepsToReproduce": [
                "1. Create an empty bucket table in Hive.",
                "2. Attempt to execute a query that includes a TABLESAMPLE on the empty bucket table.",
                "3. Observe the exception thrown during the execution."
            ],
            "ExpectedBehavior": "The query should execute successfully without any exceptions, returning an appropriate result even if the bucket table is empty.",
            "ObservedBehavior": "The query fails with a `RuntimeException` indicating that the map operator initialization failed due to inconsistent configuration and input paths.",
            "Suggestions": "Ensure that the `addMapWork` method is called correctly to associate input paths with their aliases in the `MapWork` class. Verify that the configuration is set up properly to match the input data paths.",
            "problem_location": {
                "files": [
                    "ql/src/java/org/apache/hadoop/hive/ql/exec/MapOperator.java",
                    "ql/src/java/org/apache/hadoop/hive/ql/plan/MapWork.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.ql.exec.MapOperator",
                    "org.apache.hadoop.hive.ql.plan.MapWork"
                ],
                "methods": [
                    "MapOperator.setChildren",
                    "MapOperator.initializeAsRoot",
                    "MapWork.addMapWork"
                ]
            },
            "possible_fix": "Review the instantiation and configuration of the `MapWork` object to ensure that the `addMapWork` method is called correctly to map input paths to their aliases. Additionally, check the configuration settings to ensure they align with the expected input paths."
        },
        "possible_fix_code": {
            "org.apache.hadoop.hive.ql.exec.MapOperator.setChildren": "  public void setChildren(Configuration hconf) throws HiveException {\n\n    Path fpath = IOContext.get().getInputPath();\n\n    boolean schemeless = fpath.toUri().getScheme() == null;\n\n    List<Operator<? extends OperatorDesc>> children =\n        new ArrayList<Operator<? extends OperatorDesc>>();\n\n    Map<TableDesc, StructObjectInspector> convertedOI = getConvertedOI(hconf);\n\n    try {\n      for (Map.Entry<String, ArrayList<String>> entry : conf.getPathToAliases().entrySet()) {\n        String onefile = entry.getKey();\n        List<String> aliases = entry.getValue();\n\n        Path onepath = new Path(onefile);\n        if (schemeless) {\n          onepath = new Path(onepath.toUri().getPath());\n        }\n\n        PartitionDesc partDesc = conf.getPathToPartitionInfo().get(onefile);\n\n        for (String onealias : aliases) {\n          Operator<? extends OperatorDesc> op = conf.getAliasToWork().get(onealias);\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Adding alias \" + onealias + \" to work list for file \"\n               + onefile);\n          }\n          MapInputPath inp = new MapInputPath(onefile, onealias, op, partDesc);\n          if (opCtxMap.containsKey(inp)) {\n            continue;\n          }\n          MapOpCtx opCtx = initObjectInspector(hconf, inp, convertedOI);\n          opCtxMap.put(inp, opCtx);\n\n          op.setParentOperators(new ArrayList<Operator<? extends OperatorDesc>>());\n          op.getParentOperators().add(this);\n          // check for the operators who will process rows coming to this Map\n          // Operator\n          if (!onepath.toUri().relativize(fpath.toUri()).equals(fpath.toUri())) {\n            children.add(op);\n            childrenOpToOpCtxMap.put(op, opCtx);\n            LOG.info(\"dump \" + op + \" \\\n                + opCtxMap.get(inp).rowObjectInspector.getTypeName());\n          }\n          current = opCtx;  // just need for TestOperators.testMapOperator\n        }\n      }\n\n      if (children.size() == 0) {\n        // didn't find match for input file path in configuration!\n        // serious problem ..\n        LOG.error(\"Configuration does not have any alias for path: \"\n            + fpath.toUri());\n        throw new HiveException(\"Configuration and input path are inconsistent\");\n      }\n\n      // we found all the operators that we are supposed to process.\n      setChildOperators(children);\n    } catch (Exception e) {\n      throw new HiveException(e);\n    }\n  }"
        }
    },
    {
        "filename": "HIVE-12083.json",
        "creation_time": "2015-10-09T22:45:47.000+0000",
        "bug_report": {
            "Title": "HIVE-10965 introduces thrift error if partNames or colNames are empty",
            "Description": "The bug arises from a recent change in HIVE-10965, where a short-circuit path in the method `aggrColStatsForPartitions` leads to the return of an empty `AggrStats` object when either `partNames` or `colNames` is empty. This violates the Thrift protocol requirements for the `AggrStats` struct, which mandates that the `colStats` field must be set. The absence of this field results in a `TProtocolException` during processing, indicating that required fields are unset. The issue is exacerbated by the lack of a guard for `partNames`, which can lead to errors if the method is invoked with empty parameters, especially from older client versions that do not have the recent patch.",
            "StackTrace": [
                "2015-10-08 00:00:25,413 ERROR server.TThreadPoolServer (TThreadPoolServer.java:run(213)) - Thrift error occurred during processing of message.",
                "org.apache.thrift.protocol.TProtocolException: Required field 'colStats' is unset! Struct:AggrStats(colStats:null, partsFound:0)",
                "at org.apache.hadoop.hive.metastore.api.AggrStats.validate(AggrStats.java:389)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_aggr_stats_for_result.validate(ThriftHiveMetastore.java)",
                "at org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$get_aggr_stats_for_result$get_aggr_stats_for_resultStandardScheme.write(ThriftHiveMetastore.java)",
                "at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:53)",
                "at org.apache.hadoop.hive.metastore.TUGIBasedProcessor$1.run(TUGIBasedProcessor.java:110)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)",
                "at org.apache.hadoop.hive.metastore.TUGIBasedProcessor.process(TUGIBasedProcessor.java:118)",
                "at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:206)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "RootCause": "The root cause of the issue is the return of an empty `AggrStats` object when either `partNames` or `colNames` is empty, which violates the Thrift protocol's requirement for the `colStats` field to be set. This leads to a `TProtocolException` when the Thrift server attempts to process the response.",
            "StepsToReproduce": [
                "Invoke the `aggrColStatsForPartitions` method with an empty list for `partNames` or `colNames`.",
                "Observe the Thrift error in the logs indicating that the required field 'colStats' is unset."
            ],
            "ExpectedBehavior": "When `partNames` or `colNames` is empty, the method should handle this case gracefully without returning an empty `AggrStats` object, thus preventing any Thrift protocol violations.",
            "ObservedBehavior": "The method returns an empty `AggrStats` object, leading to a `TProtocolException` due to the unset `colStats` field, causing errors in the Thrift server processing.",
            "Suggestions": "Implement a guard clause to check for empty `partNames` and return a valid `AggrStats` object with default values instead of an empty one. Ensure that the `colStats` field is always initialized properly.",
            "problem_location": {
                "files": [
                    "metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreDirectSql.java"
                ],
                "classes": [
                    "org.apache.hadoop.hive.metastore.MetaStoreDirectSql"
                ],
                "methods": [
                    "aggrColStatsForPartitions"
                ]
            },
            "possible_fix": "Modify the `aggrColStatsForPartitions` method to include a check for empty `partNames` and `colNames`. If either is empty, return an `AggrStats` object with default values for `colStats` and `partsFound` instead of returning an empty object."
        },
        "possible_fix_code": {
            "aggrColStatsForPartitions": "public AggrStats aggrColStatsForPartitions(List<String> partNames, List<String> colNames) {\n    // Check for empty partNames or colNames\n    if (partNames == null || partNames.isEmpty() || colNames == null || colNames.isEmpty()) {\n        // Return a valid AggrStats object with default values\n        return new AggrStats(new HashMap<>(), 0);\n    }\n\n    // Existing logic to compute aggregate statistics\n    // ... (rest of the method implementation)\n}"
        }
    },
    {
        "filename": "HIVE-14784.json",
        "creation_time": "2016-09-17T02:00:28.000+0000",
        "bug_report": {
            "Title": "Operation logs are disabled automatically if the parent directory does not exist.",
            "Description": "Operation logging is automatically disabled when the parent directory, which is named after the hive session ID, is deleted or does not exist. This can occur if the operation log directory (e.g., /tmp) is purged by the operating system at configured intervals. When a query is executed from that session, it results in an IOException indicating that the operation log file cannot be created due to the absence of the specified directory. This issue is exacerbated when using HUE, as it does not close hive sessions and may attempt to retrieve operation logs long after they were created, leading to further errors.",
            "StackTrace": [
                "2016-09-15 15:09:16,723 WARN org.apache.hive.service.cli.operation.Operation: Unable to create operation log file: /tmp/hive/operation_logs/b8809985-6b38-47ec-a49b-6158a67cd9fc/d35414f7-2418-426c-8489-c6f643ca4599",
                "java.io.IOException: No such file or directory",
                "at java.io.UnixFileSystem.createFileExclusively(Native Method)",
                "at java.io.File.createNewFile(File.java:1012)",
                "at org.apache.hive.service.cli.operation.Operation.createOperationLog(Operation.java:195)",
                "at org.apache.hive.service.cli.operation.Operation.beforeRun(Operation.java:237)",
                "at org.apache.hive.service.cli.operation.Operation.run(Operation.java:255)",
                "at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementInternal(HiveSessionImpl.java:398)",
                "at org.apache.hive.service.cli.session.HiveSessionImpl.executeStatementAsync(HiveSessionImpl.java:385)",
                "at org.apache.hive.service.cli.CLIService.executeStatementAsync(CLIService.java:271)",
                "at org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:490)",
                "at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1313)",
                "at org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1298)",
                "at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)",
                "at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)",
                "at org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingProcessor.process(HadoopThriftAuthBridge.java:692)",
                "at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:285)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "RootCause": "The root cause of the issue is an IOException triggered by the absence of the operation log directory, which is expected to be created based on the session ID. The `createOperationLog` method in the `Operation` class fails to create a log file because the directory returned by `parentSession.getOperationLogSessionDir()` does not exist.",
            "StepsToReproduce": [
                "1. Configure the operation log directory to a location that can be purged by the OS (e.g., /tmp).",
                "2. Start a Hive session and execute a query.",
                "3. Delete the parent directory for the operation logs.",
                "4. Attempt to execute another query from the same session."
            ],
            "ExpectedBehavior": "The system should create an operation log file in the specified directory without errors, allowing for successful logging of operations.",
            "ObservedBehavior": "The system fails to create the operation log file due to a missing directory, resulting in an IOException and subsequent HiveSQLException when attempting to fetch results.",
            "Suggestions": "1. Ensure that the operation log directory is correctly configured and exists before executing queries. 2. Implement a check in the `createOperationLog` method to create the directory if it does not exist. 3. Improve error handling to provide clearer logging when directory creation fails.",
            "problem_location": {
                "files": [
                    "service/src/java/org/apache/hive/service/cli/operation/Operation.java",
                    "service/src/java/org/apache/hive/service/cli/session/HiveSession.java"
                ],
                "classes": [
                    "org.apache.hive.service.cli.operation.Operation",
                    "org.apache.hive.service.cli.session.HiveSession"
                ],
                "methods": [
                    "Operation.createOperationLog",
                    "Operation.beforeRun",
                    "HiveSession.open"
                ]
            },
            "possible_fix": "Modify the `createOperationLog` method to check if the directory exists and create it if it does not. Example code snippet:\n\nif (!operationLogFile.getParentFile().exists()) {\n    operationLogFile.getParentFile().mkdirs();\n}"
        },
        "possible_fix_code": {
            "Operation.createOperationLog": "  protected void createOperationLog() {\n    if (parentSession.isOperationLogEnabled()) {\n      File operationLogFile = new File(parentSession.getOperationLogSessionDir(),\n          opHandle.getHandleIdentifier().toString());\n      isOperationLogEnabled = true;\n\n      // Ensure the parent directory exists\n      if (!operationLogFile.getParentFile().exists()) {\n        if (!operationLogFile.getParentFile().mkdirs()) {\n          LOG.warn(\"Unable to create operation log directory: \" + operationLogFile.getParentFile().getAbsolutePath());\n          isOperationLogEnabled = false;\n          return;\n        }\n      }\n\n      // create log file\n      try {\n        if (operationLogFile.exists()) {\n          LOG.warn(\"The operation log file should not exist, but it is already there: \" +\n              operationLogFile.getAbsolutePath());\n          operationLogFile.delete();\n        }\n        if (!operationLogFile.createNewFile()) {\n          // the log file already exists and cannot be deleted.\n          // If it can be read/written, keep its contents and use it.\n          if (!operationLogFile.canRead() || !operationLogFile.canWrite()) {\n            LOG.warn(\"The already existed operation log file cannot be recreated, \" +\n                \"and it cannot be read or written: \" + operationLogFile.getAbsolutePath());\n            isOperationLogEnabled = false;\n            return;\n          }\n        }\n      } catch (Exception e) {\n        LOG.warn(\"Unable to create operation log file: \" + operationLogFile.getAbsolutePath(), e);\n        isOperationLogEnabled = false;\n        return;\n      }\n\n      // create OperationLog object with above log file\n      try {\n        operationLog = new OperationLog(opHandle.toString(), operationLogFile, parentSession.getHiveConf());\n      } catch (FileNotFoundException e) {\n        LOG.warn(\"Unable to instantiate OperationLog object for operation: \" +\n            opHandle, e);\n        isOperationLogEnabled = false;\n        return;\n      }\n\n      // register this operationLog to current thread\n      OperationLog.setCurrentOperationLog(operationLog);\n    }\n  }"
        }
    }
]