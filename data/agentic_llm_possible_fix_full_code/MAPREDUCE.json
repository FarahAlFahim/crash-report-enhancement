[
    {
        "filename": "MAPREDUCE-6633.json",
        "creation_time": "2016-02-10T21:12:09.000+0000",
        "bug_report": {
            "Title": "AM should retry map attempts if the reduce task encounters compression related errors.",
            "Description": "The issue arises when a reduce task encounters compression-related errors during the shuffle phase of a Hadoop MapReduce job. Specifically, an `ArrayIndexOutOfBoundsException` occurs in the `LzoDecompressor`, indicating potential problems with the data being processed, particularly with the LZO compression format. In the observed case, the node running the map task had a faulty drive, which contributed to the failure. The Application Master (AM) does not retry the corresponding map task, leading to job failure.",
            "StackTrace": [
                "2016-01-27 13:44:28,915 WARN [main] org.apache.hadoop.mapred.YarnChild: Exception running child : org.apache.hadoop.mapreduce.task.reduce.Shuffle$ShuffleError: error in shuffle in fetcher#29",
                "at org.apache.hadoop.mapreduce.task.reduce.Shuffle.run(Shuffle.java:134)",
                "at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:376)",
                "at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1694)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)",
                "Caused by: java.lang.ArrayIndexOutOfBoundsException",
                "at com.hadoop.compression.lzo.LzoDecompressor.setInput(LzoDecompressor.java:196)",
                "at org.apache.hadoop.io.compress.BlockDecompressorStream.decompress(BlockDecompressorStream.java:104)",
                "at org.apache.hadoop.io.compress.DecompressorStream.read(DecompressorStream.java:85)",
                "at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:192)",
                "at org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput.shuffle(InMemoryMapOutput.java:97)",
                "at org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyMapOutput(Fetcher.java:537)",
                "at org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyFromHost(Fetcher.java:336)",
                "at org.apache.hadoop.mapreduce.task.reduce.Fetcher.run(Fetcher.java:193)"
            ],
            "RootCause": "The root cause of the issue is an `ArrayIndexOutOfBoundsException` in the `LzoDecompressor` during the shuffle phase, likely due to corrupted input data or issues with the LZO compression format.",
            "StepsToReproduce": [
                "Run a Hadoop MapReduce job that involves LZO compressed data.",
                "Ensure that the map task is executed on a node with a faulty drive.",
                "Observe the behavior of the reduce task during the shuffle phase."
            ],
            "ExpectedBehavior": "The Application Master (AM) should retry the failed map task on a different node if a compression-related error occurs during the reduce task.",
            "ObservedBehavior": "The AM does not retry the corresponding map task, leading to job failure when the reduce task encounters compression-related errors.",
            "Suggestions": "1. Check the integrity of the input data to ensure it is not corrupted. 2. Update to the latest versions of Hadoop and LZO libraries. 3. Increase memory allocation for reducer tasks. 4. Enable detailed logging for better error diagnostics. 5. Test with a smaller dataset to isolate the issue. 6. Consult Hadoop and LZO documentation for known issues.",
            "problem_location": {
                "files": [
                    "LzoDecompressor.java",
                    "Shuffle.java",
                    "Fetcher.java"
                ],
                "classes": [
                    "com.hadoop.compression.lzo.LzoDecompressor",
                    "org.apache.hadoop.mapreduce.task.reduce.Shuffle",
                    "org.apache.hadoop.mapreduce.task.reduce.Fetcher"
                ],
                "methods": [
                    "LzoDecompressor.setInput",
                    "Shuffle.run",
                    "Fetcher.copyMapOutput"
                ]
            },
            "possible_fix": "Consider implementing a retry mechanism in the Application Master to handle failures during the shuffle phase, particularly when compression-related errors are detected."
        },
        "possible_fix_code": {
            "org.apache.hadoop.mapreduce.task.reduce.Shuffle.run": "  public RawKeyValueIterator run() throws IOException, InterruptedException {\n    // Scale the maximum events we fetch per RPC call to mitigate OOM issues\n    // on the ApplicationMaster when a thundering herd of reducers fetch events\n    // TODO: This should not be necessary after HADOOP-8942\n    int eventsPerReducer = Math.max(MIN_EVENTS_TO_FETCH,\n        MAX_RPC_OUTSTANDING_EVENTS / jobConf.getNumReduceTasks());\n    int maxEventsToFetch = Math.min(MAX_EVENTS_TO_FETCH, eventsPerReducer);\n\n    // Start the map-completion events fetcher thread\n    final EventFetcher<K,V> eventFetcher = \n      new EventFetcher<K,V>(reduceId, umbilical, scheduler, this,\n          maxEventsToFetch);\n    eventFetcher.start();\n    \n    // Start the map-output fetcher threads\n    boolean isLocal = localMapFiles != null;\n    final int numFetchers = isLocal ? 1 :\n      jobConf.getInt(MRJobConfig.SHUFFLE_PARALLEL_COPIES, 5);\n    Fetcher<K,V>[] fetchers = new Fetcher[numFetchers];\n    if (isLocal) {\n      fetchers[0] = new LocalFetcher<K, V>(jobConf, reduceId, scheduler,\n          merger, reporter, metrics, this, reduceTask.getShuffleSecret(),\n          localMapFiles);\n      fetchers[0].start();\n    } else {\n      for (int i=0; i < numFetchers; ++i) {\n        fetchers[i] = new Fetcher<K,V>(jobConf, reduceId, scheduler, merger, \n                                       reporter, metrics, this, \n                                       reduceTask.getShuffleSecret());\n        fetchers[i].start();\n      }\n    }\n    \n    // Wait for shuffle to complete successfully\n    while (!scheduler.waitUntilDone(PROGRESS_FREQUENCY)) {\n      reporter.progress();\n      \n      synchronized (this) {\n        if (throwable != null) {\n          // Retry logic for compression-related errors\n          if (throwable instanceof IOException && \n              throwable.getMessage().contains(\"compression\")) {\n            LOG.warn(\"Compression error encountered, retrying map task...\");\n            // Logic to retry the map task\n            // This could involve notifying the Application Master to retry the task\n            // For example, we could call a method to handle retries\n            handleRetry();\n            return null; // Indicate that we are retrying\n          }\n          throw new ShuffleError(\"error in shuffle in \" + throwingThreadName,\n                                 throwable);\n        }\n      }\n    }\n\n    // Stop the event-fetcher thread\n    eventFetcher.shutDown();\n    \n    // Stop the map-output fetcher threads\n    for (Fetcher<K,V> fetcher : fetchers) {\n      fetcher.shutDown();\n    }\n    \n    // stop the scheduler\n    scheduler.close();\n\n    copyPhase.complete(); // copy is already complete\n    taskStatus.setPhase(TaskStatus.Phase.SORT);\n    reduceTask.statusUpdate(umbilical);\n\n    // Finish the on-going merges...\n    RawKeyValueIterator kvIter = null;\n    try {\n      kvIter = merger.close();\n    } catch (Throwable e) {\n      throw new ShuffleError(\"Error while doing final merge \" , e);\n    }\n\n    // Sanity check\n    synchronized (this) {\n      if (throwable != null) {\n        throw new ShuffleError(\"error in shuffle in \" + throwingThreadName,\n                               throwable);\n      }\n    }\n    \n    return kvIter;\n  }"
        }
    },
    {
        "filename": "MAPREDUCE-6577.json",
        "creation_time": "2015-12-17T06:46:30.000+0000",
        "bug_report": {
            "Title": "MR AM unable to load native library without MR_AM_ADMIN_USER_ENV set",
            "Description": "The issue arises when the Hadoop MapReduce Application Master (MR AM) fails to load the native LZ4 library due to the absence of the `LD_LIBRARY_PATH` configuration in the `yarn.app.mapreduce.am.admin.user.env` or `yarn.app.mapreduce.am.env` settings. This results in a `RuntimeException` when attempting to use LZ4 compression, as the required native library is not available. The stack trace indicates that the failure occurs during the initialization of the `Lz4Codec` class, specifically in the `getCompressorType` method, which checks for the native library's availability.",
            "StackTrace": [
                "2015-12-15 21:30:17,575 WARN [uber-SubtaskRunner] org.apache.hadoop.mapred.LocalContainerLauncher: Exception running local (uberized) 'child' : java.lang.RuntimeException: native lz4 library not available",
                "at org.apache.hadoop.io.compress.Lz4Codec.getCompressorType(Lz4Codec.java:125)",
                "at org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:148)",
                "at org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:163)",
                "at org.apache.hadoop.mapred.IFile$Writer.<init>(IFile.java:114)",
                "at org.apache.hadoop.mapred.IFile$Writer.<init>(IFile.java:97)",
                "at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1602)",
                "at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.flush(MapTask.java:1482)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:457)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)",
                "at org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler.runSubtask(LocalContainerLauncher.java:391)",
                "at org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler.runTask(LocalContainerLauncher.java:309)",
                "at org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler.access$200(LocalContainerLauncher.java:195)",
                "at org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler$1.run(LocalContainerLauncher.java:238)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:262)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "RootCause": "The root cause of the issue is that the native LZ4 library is not loaded due to the `LD_LIBRARY_PATH` not being set in the Hadoop configuration, which is necessary for the `Lz4Codec.getCompressorType` method to function correctly.",
            "StepsToReproduce": [
                "Ensure that the Hadoop configuration does not set the `yarn.app.mapreduce.am.admin.user.env` or `yarn.app.mapreduce.am.env` to include `LD_LIBRARY_PATH`.",
                "Attempt to run a MapReduce job that utilizes LZ4 compression.",
                "Observe the logs for warnings or errors related to the native LZ4 library."
            ],
            "ExpectedBehavior": "The MR AM should successfully load the native LZ4 library and allow the MapReduce job to execute without errors related to library availability.",
            "ObservedBehavior": "The MR AM fails to load the native LZ4 library, resulting in a `RuntimeException` when attempting to use LZ4 compression, causing the job to fail.",
            "Suggestions": "To resolve this issue, ensure that the native LZ4 library is installed, set the `LD_LIBRARY_PATH` correctly in the Hadoop configuration, and verify that the library is accessible in the environment.",
            "problem_location": {
                "files": [
                    "Lz4Codec.java"
                ],
                "classes": [
                    "org.apache.hadoop.io.compress.Lz4Codec"
                ],
                "methods": [
                    "Lz4Codec.getCompressorType"
                ]
            },
            "possible_fix": "1. Check and update the Hadoop configuration files (e.g., `core-site.xml`, `hdfs-site.xml`) to include the correct paths for native libraries.\n2. Install the native LZ4 library if it is not already installed.\n3. Set the `LD_LIBRARY_PATH` environment variable to include the path to the LZ4 library directory."
        },
        "possible_fix_code": {
            "org.apache.hadoop.io.compress.Lz4Codec.getCompressorType": "  public Class<? extends Compressor> getCompressorType() {\n    if (!isNativeCodeLoaded()) {\n      throw new RuntimeException(\"native lz4 library not available\");\n    }\n\n    // Additional check to ensure LD_LIBRARY_PATH is set correctly\n    String libraryPath = System.getenv(\"LD_LIBRARY_PATH\");\n    if (libraryPath == null || libraryPath.isEmpty()) {\n      throw new RuntimeException(\"LD_LIBRARY_PATH is not set. Please set it to include the path to the LZ4 library.\");\n    }\n\n    return Lz4Compressor.class;\n  }"
        }
    },
    {
        "filename": "MAPREDUCE-5137.json",
        "creation_time": "2013-04-09T15:14:34.000+0000",
        "bug_report": {
            "Title": "AM web UI: clicking on Map Task results in 500 error",
            "Description": "When navigating to a running MapReduce application master web UI, clicking on the job and subsequently selecting the MAP task type leads to a 500 error. This issue does not occur in version 0.23.6. The error is logged as a NotFoundException from the Jersey framework, indicating that the requested URI is not found on the server. This could be due to the task not existing, server misconfiguration, or an incorrect URI.",
            "StackTrace": [
                "2013-04-09 13:53:01,587 DEBUG [1088374@qtp-13877033-2 - /mapreduce/task/task_1365457322543_0004_m_000000] org.apache.hadoop.yarn.webapp.GenericExceptionHandler: GOT EXCEPITION",
                "com.sun.jersey.api.NotFoundException: null for uri: http://host.com:38158/mapreduce/task/task_1365457322543_0004_m_000000",
                "at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1470)",
                "at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1400)",
                "at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1349)",
                "at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1339)",
                "at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:416)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:537)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:886)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:834)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:795)"
            ],
            "RootCause": "The root cause of the issue is a NotFoundException triggered by the Jersey framework, indicating that the requested URI for the specific map task does not exist on the server.",
            "StepsToReproduce": [
                "1. Access the running MapReduce application master web UI.",
                "2. Click on the job to view its details.",
                "3. Select the MAP task type to display the list of map tasks.",
                "4. Click on a specific map task."
            ],
            "ExpectedBehavior": "The system should display the details of the selected map task without any errors.",
            "ObservedBehavior": "A 500 error is encountered, and the details of the selected map task are not displayed.",
            "Suggestions": "Verify the existence of the requested map task and ensure that the server is correctly configured to handle the request. Check the URI for correctness.",
            "problem_location": {
                "files": [
                    "WebApplicationImpl.java",
                    "AmIpFilter.java"
                ],
                "classes": [
                    "com.sun.jersey.server.impl.application.WebApplicationImpl",
                    "org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter"
                ],
                "methods": [
                    "WebApplicationImpl.handleRequest",
                    "AmIpFilter.doFilter"
                ]
            },
            "possible_fix": "Investigate the server configuration and the mapping of URIs to ensure that the requested map task exists and is accessible. If the task is missing, ensure that it is correctly registered in the system."
        },
        "possible_fix_code": {
            "HttpServer.doFilter": "    public void doFilter(ServletRequest request, \n                         ServletResponse response,\n                         FilterChain chain\n                         ) throws IOException, ServletException {\n      HttpServletRequestWrapper quoted = \n        new RequestQuoter((HttpServletRequest) request);\n      HttpServletResponse httpResponse = (HttpServletResponse) response;\n\n      String mime = inferMimeType(request);\n      if (mime == null) {\n        httpResponse.setContentType(\"text/plain; charset=utf-8\");\n      } else if (mime.startsWith(\"text/html\")) {\n        // HTML with unspecified encoding, we want to\n        // force HTML with utf-8 encoding\n        // This is to avoid the following security issue:\n        // http://openmya.hacker.jp/hasegawa/security/utf7cs.html\n        httpResponse.setContentType(\"text/html; charset=utf-8\");\n      } else if (mime.startsWith(\"application/xml\")) {\n        httpResponse.setContentType(\"text/xml; charset=utf-8\");\n      }\n\n      // Check if the requested task exists before proceeding\n      String taskId = ((HttpServletRequest) request).getRequestURI();\n      if (!taskExists(taskId)) {\n          httpResponse.sendError(HttpServletResponse.SC_NOT_FOUND, \"Task not found\");\n          return;\n      }\n\n      chain.doFilter(quoted, httpResponse);\n    }"
        }
    },
    {
        "filename": "MAPREDUCE-4008.json",
        "creation_time": "2012-03-14T10:08:39.000+0000",
        "bug_report": {
            "Title": "ResourceManager throws MetricsException on start up saying QueueMetrics MBean already exists",
            "Description": "During the startup of the ResourceManager, an exception is thrown indicating that the MBean for QueueMetrics already exists. This is caused by an attempt to register a metrics object with the name 'Hadoop:service=ResourceManager,name=QueueMetrics,q0=default', which has already been registered in the metrics system. The issue arises from multiple invocations of the metrics registration process without proper shutdown of the ResourceManager.",
            "StackTrace": [
                "2012-03-14 15:22:23,089 WARN org.apache.hadoop.metrics2.util.MBeans: Error creating MBean object name: Hadoop:service=ResourceManager,name=QueueMetrics,q0=default",
                "org.apache.hadoop.metrics2.MetricsException: org.apache.hadoop.metrics2.MetricsException: Hadoop:service=ResourceManager,name=QueueMetrics,q0=default already exists!",
                "at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.newObjectName(DefaultMetricsSystem.java:117)",
                "at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.newMBeanName(DefaultMetricsSystem.java:102)",
                "at org.apache.hadoop.metrics2.util.MBeans.getMBeanName(MBeans.java:91)",
                "at org.apache.hadoop.metrics2.util.MBeans.register(MBeans.java:55)",
                "at org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.startMBeans(MetricsSourceAdapter.java:218)",
                "at org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.startMBeans(MetricsSourceAdapter.java:93)",
                "at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.registerSource(MetricsSystemImpl.java:243)",
                "at org.apache.hadoop.metrics2.impl.MetricsSystemImpl$1.postStart(MetricsSystemImpl.java:227)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "at java.lang.reflect.Method.invoke(Method.java:597)",
                "at org.apache.hadoop.metrics2.impl.MetricsSystemImpl$3.invoke(MetricsSystemImpl.java:288)",
                "at $Proxy6.postStart(Unknown Source)",
                "at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.start(MetricsSystemImpl.java:183)",
                "at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.init(MetricsSystemImpl.java:155)",
                "at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.init(DefaultMetricsSystem.java:54)",
                "at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.initialize(DefaultMetricsSystem.java:50)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.start(ResourceManager.java:454)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:588)"
            ],
            "RootCause": "The root cause of the MetricsException is the multiple invocations of the ResourceManager.start() method without proper shutdown, leading to duplicate registration attempts of the same MBean in the metrics system.",
            "StepsToReproduce": [
                "Start the ResourceManager without shutting it down properly.",
                "Observe the logs for the MetricsException indicating that the MBean already exists."
            ],
            "ExpectedBehavior": "The ResourceManager should start without any exceptions, successfully registering the necessary MBeans for metrics.",
            "ObservedBehavior": "The ResourceManager throws a MetricsException indicating that the MBean for QueueMetrics already exists, preventing successful startup.",
            "Suggestions": "Ensure that the ResourceManager is properly shut down before attempting to start it again. Review the code to prevent multiple invocations of the start method without shutdown.",
            "problem_location": {
                "files": [
                    "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.ResourceManager",
                    "org.apache.hadoop.metrics2.impl.MetricsSourceAdapter",
                    "org.apache.hadoop.metrics2.impl.MetricsSystemImpl"
                ],
                "methods": [
                    "ResourceManager.start",
                    "MetricsSourceAdapter.startMBeans",
                    "MetricsSystemImpl.registerSource"
                ]
            },
            "possible_fix": "Add checks to ensure that the ResourceManager is not started multiple times without proper shutdown. Consider implementing a mechanism to track the state of the ResourceManager to prevent duplicate starts."
        },
        "possible_fix_code": {
            "ResourceManager.start": "  public void start() {\n    if (isRunning) {\n      LOG.warn(\"ResourceManager is already running.\");\n      return;\n    }\n    isRunning = true;\n    try {\n      doSecureLogin();\n    } catch(IOException ie) {\n      throw new YarnException(\"Failed to login\", ie);\n    }\n\n    startWepApp();\n    DefaultMetricsSystem.initialize(\"ResourceManager\");\n    JvmMetrics.initSingleton(\"ResourceManager\", null);\n    try {\n      rmDTSecretManager.startThreads();\n    } catch(IOException ie) {\n      throw new YarnException(\"Failed to start secret manager threads\", ie);\n    }\n    \n    super.start();\n  }"
        }
    },
    {
        "filename": "MAPREDUCE-6259.json",
        "creation_time": "2015-02-13T03:20:41.000+0000",
        "bug_report": {
            "Title": "IllegalArgumentException due to missing job submit time",
            "Description": "An IllegalArgumentException is thrown when attempting to parse the job history file name due to a missing job submit time, which is represented as -1 in the JobIndexInfo. The specific job history file causing the issue is 'job_1418398645407_115853--1-worun-kafka%2Dto%2Dhdfs%5Btwo%5D%5B15+topic%28s%29%5D-1423572836007-0-0-FAILED-root.journaling-1423572836007.jhist'. The stack trace indicates that the error occurs when the system tries to access a non-existent enum constant in the JobState enum, specifically '0'. This suggests that the job status is being set incorrectly, leading to the exception.",
            "StackTrace": [
                "2015-02-10 04:54:01,863 WARN org.apache.hadoop.mapreduce.v2.hs.PartialJob: Exception while parsing job state. Defaulting to KILLED",
                "java.lang.IllegalArgumentException: No enum constant org.apache.hadoop.mapreduce.v2.api.records.JobState.0",
                "at java.lang.Enum.valueOf(Enum.java:236)",
                "at org.apache.hadoop.mapreduce.v2.api.records.JobState.valueOf(JobState.java:21)",
                "at org.apache.hadoop.mapreduce.v2.hs.PartialJob.getState(PartialJob.java:82)",
                "at org.apache.hadoop.mapreduce.v2.hs.PartialJob.<init>(PartialJob.java:59)",
                "at org.apache.hadoop.mapreduce.v2.hs.CachedHistoryStorage.getAllPartialJobs(CachedHistoryStorage.java:159)",
                "at org.apache.hadoop.mapreduce.v2.hs.CachedHistoryStorage.getPartialJobs(CachedHistoryStorage.java:173)",
                "at org.apache.hadoop.mapreduce.v2.hs.JobHistory.getPartialJobs(JobHistory.java:284)",
                "at org.apache.hadoop.mapreduce.v2.hs.webapp.HsWebServices.getJobs(HsWebServices.java:212)"
            ],
            "RootCause": "The root cause of the IllegalArgumentException is that the job status is being set to an invalid value ('0'), which does not correspond to any defined constants in the JobState enum. This occurs because the job submit time is not updated due to an IOException during the job setup process, leaving it at its initial value of -1.",
            "StepsToReproduce": [
                "1. Create a job using MRAppMaster#serviceStart.",
                "2. Send a JOB_INIT event to JobImpl from MRAppMaster#serviceStart.",
                "3. Trigger the InitTransition#transition method, which calls setup(job).",
                "4. Ensure that an IOException occurs during the setup process.",
                "5. Observe that the job remains in the NEW state and the submit time is not updated, leading to the IllegalArgumentException when the job state is accessed."
            ],
            "ExpectedBehavior": "The job should transition to the INITED state after successful setup, and the job submit time should be correctly set to a valid timestamp.",
            "ObservedBehavior": "The job remains in the NEW state with a submit time of -1, leading to an IllegalArgumentException when attempting to access the job state.",
            "Suggestions": "Investigate the IOException occurring in JobImpl#setup and ensure that the job submit time is updated correctly. Consider adding validation to prevent invalid job statuses from being set.",
            "problem_location": {
                "files": [
                    "JobImpl.java",
                    "PartialJob.java",
                    "CachedHistoryStorage.java"
                ],
                "classes": [
                    "org.apache.hadoop.mapreduce.v2.hs.PartialJob",
                    "org.apache.hadoop.mapreduce.v2.hs.JobHistory",
                    "org.apache.hadoop.mapreduce.v2.hs.CachedHistoryStorage"
                ],
                "methods": [
                    "JobImpl.setup",
                    "PartialJob.getState",
                    "CachedHistoryStorage.getAllPartialJobs"
                ]
            },
            "possible_fix": "In JobImpl#setup, ensure that the job submit time is set correctly even if an IOException occurs. Additionally, validate the job status before passing it to JobState.valueOf to prevent invalid values from causing exceptions."
        },
        "possible_fix_code": {
            "PartialJob.getState": "  @Override\n  public JobState getState() {\n    JobState js = null;\n    try {\n      js = JobState.valueOf(jobIndexInfo.getJobStatus());\n    } catch (IllegalArgumentException e) {\n      // Log the invalid job status and default to KILLED\n      LOG.warn(\"Invalid job status: \" + jobIndexInfo.getJobStatus() + \". Defaulting to KILLED\", e);\n      js = JobState.KILLED;\n    } catch (Exception e) {\n      // Meant for use by the display UI. Exception would prevent it from being rendered. Defaulting to KILLED\n      LOG.warn(\"Exception while parsing job state. Defaulting to KILLED\", e);\n      js = JobState.KILLED;\n    }\n    return js;\n  }"
        }
    },
    {
        "filename": "MAPREDUCE-3333.json",
        "creation_time": "2011-11-02T14:00:54.000+0000",
        "bug_report": {
            "Title": "MR AM for sort-job going out of memory",
            "Description": "A sort job on a 350 node cluster encountered an OutOfMemoryError, causing it to hang and eventually fail after an hour, significantly longer than the usual 20 minutes. The stack trace indicates that the failure is due to an `UndeclaredThrowableException`, which is caused by a `ServiceException` stemming from an `OutOfMemoryError`. This suggests that the application is unable to create new threads, likely due to insufficient memory or thread limits in the Hadoop configuration.",
            "StackTrace": [
                "2011-11-02 11:40:36,438 ERROR [ContainerLauncher #258] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Container launch failed for container_1320233407485_0002_01_001434 : java.lang.reflect.UndeclaredThrowableException",
                "at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagerPBClientImpl.startContainer(ContainerManagerPBClientImpl.java:88)",
                "at org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl$EventProcessor.run(ContainerLauncherImpl.java:290)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)",
                "at java.lang.Thread.run(Thread.java:619)",
                "Caused by: com.google.protobuf.ServiceException: java.io.IOException: Failed on local exception: java.io.IOException: Couldn't set up IO streams; Host Details : local host is: \"gsbl91281.blue.ygrid.yahoo.com/98.137.101.189\"; destination host is: \"gsbl91525.blue.ygrid.yahoo.com\":45450;",
                "at org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Invoker.invoke(ProtoOverHadoopRpcEngine.java:139)",
                "at $Proxy20.startContainer(Unknown Source)",
                "at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagerPBClientImpl.startContainer(ContainerManagerPBClientImpl.java:81)",
                "Caused by: java.io.IOException: Couldn't set up IO streams",
                "at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:621)",
                "Caused by: java.lang.OutOfMemoryError: unable to create new native thread",
                "at java.lang.Thread.start0(Native Method)",
                "at java.lang.Thread.start(Thread.java:597)",
                "at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:614)"
            ],
            "RootCause": "The OutOfMemoryError is caused by the inability to create new threads, likely due to insufficient memory or thread limits set in the Hadoop configuration.",
            "StepsToReproduce": [
                "Deploy a sort job on a 350 node cluster.",
                "Monitor the job execution and resource allocation.",
                "Observe the job hanging and eventually failing due to OutOfMemoryError."
            ],
            "ExpectedBehavior": "The sort job should complete successfully within the usual time frame of approximately 20 minutes without encountering memory issues.",
            "ObservedBehavior": "The sort job hangs and fails after an hour due to an OutOfMemoryError, indicating resource constraints.",
            "Suggestions": "Check and adjust the following configuration keys in Hadoop to manage memory and thread limits: `yarn.nodemanager.resource.memory-mb`, `yarn.scheduler.maximum-allocation-mb`, and `yarn.nodemanager.threads`.",
            "problem_location": {
                "files": [
                    "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Client.java",
                    "hadoop-yarn-project/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/api/impl/pb/client/ContainerManagerPBClientImpl.java",
                    "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/conf/Configuration.java"
                ],
                "classes": [
                    "org.apache.hadoop.ipc.Client",
                    "org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagerPBClientImpl",
                    "org.apache.hadoop.conf.Configuration"
                ],
                "methods": [
                    "Client.getConnection",
                    "ContainerManagerPBClientImpl.startContainer",
                    "Configuration.getInt",
                    "Configuration.getLong"
                ]
            },
            "possible_fix": "To resolve the OutOfMemoryError, consider increasing the values for the configuration keys: `yarn.nodemanager.resource.memory-mb` to allocate more memory to the NodeManager, `yarn.scheduler.maximum-allocation-mb` to allow larger container allocations, and `yarn.nodemanager.threads` to increase the number of threads available."
        },
        "possible_fix_code": {
            "Configuration.getInt": "  public int getInt(String name, int defaultValue) {\n    String valueString = getTrimmed(name);\n    if (valueString == null)\n      return defaultValue;\n    String hexString = getHexDigits(valueString);\n    if (hexString != null) {\n      return Integer.parseInt(hexString, 16);\n    }\n    return Integer.parseInt(valueString);\n  }\n\n  // Additional method to adjust memory and thread configurations\n  public void adjustMemoryAndThreadSettings(Configuration conf) {\n    // Increase memory allocation for NodeManager\n    conf.setInt(\"yarn.nodemanager.resource.memory-mb\", 8192); // Example value\n    // Increase maximum allocation for each container\n    conf.setInt(\"yarn.scheduler.maximum-allocation-mb\", 4096); // Example value\n    // Increase number of threads available to NodeManager\n    conf.setInt(\"yarn.nodemanager.threads\", 200); // Example value\n  }"
        }
    },
    {
        "filename": "MAPREDUCE-7059.json",
        "creation_time": "2018-02-26T09:38:39.000+0000",
        "bug_report": {
            "Title": "Downward Compatibility issue: MR job fails because of unknown setErasureCodingPolicy method from 3.x client to HDFS 2.x cluster",
            "Description": "The issue arises when running the teragen job using Hadoop version 3.1 against an HDFS cluster running version 2.8. The job fails due to an attempt to call the `setErasureCodingPolicy` method, which is not available in the HDFS 2.8 version. This incompatibility leads to a `RemoteException` indicating that the method is unknown. The error suggests a version mismatch between the Hadoop client and the HDFS server, as the method is not defined in the `ClientProtocol` interface for the server version being used.",
            "StackTrace": [
                "2018-02-26 11:22:53,178 INFO mapreduce.JobSubmitter: Cleaning up the staging area /tmp/hadoop-yarn/staging/hadoop/.staging/job_1518615699369_0006",
                "org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RpcNoSuchMethodException): Unknown method setErasureCodingPolicy called on org.apache.hadoop.hdfs.protocol.ClientProtocol protocol.",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:436)",
                "at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)",
                "at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:846)",
                "at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:789)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1804)",
                "at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2457)",
                "at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1491)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1437)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1347)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)",
                "at com.sun.proxy.$Proxy11.setErasureCodingPolicy(Unknown Source)",
                "at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.setErasureCodingPolicy(ClientNamenodeProtocolTranslatorPB.java:1583)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:498)",
                "at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)",
                "at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)",
                "at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)",
                "at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)",
                "at com.sun.proxy.$Proxy12.setErasureCodingPolicy(Unknown Source)",
                "at org.apache.hadoop.hdfs.DFSClient.setErasureCodingPolicy(DFSClient.java:2678)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$63.doCall(DistributedFileSystem.java:2665)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$63.doCall(DistributedFileSystem.java:2662)",
                "at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.setErasureCodingPolicy(DistributedFileSystem.java:2680)",
                "at org.apache.hadoop.mapreduce.JobResourceUploader.disableErasureCodingForPath(JobResourceUploader.java:882)",
                "at org.apache.hadoop.mapreduce.JobResourceUploader.uploadResourcesInternal(JobResourceUploader.java:174)",
                "at org.apache.hadoop.mapreduce.JobResourceUploader.uploadResources(JobResourceUploader.java:131)",
                "at org.apache.hadoop.mapreduce.JobSubmitter.copyAndConfigureFiles(JobSubmitter.java:102)",
                "at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:197)",
                "at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1570)",
                "at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1567)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1965)",
                "at org.apache.hadoop.mapreduce.Job.submit(Job.java:1567)",
                "at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1588)",
                "at org.apache.hadoop.examples.terasort.TeraGen.run(TeraGen.java:304)",
                "at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)",
                "at org.apache.hadoop.examples.terasort.TeraGen.main(TeraGen.java:308)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:498)",
                "at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:71)",
                "at org.apache.hadoop.util.ProgramDriver.run(ProgramDriver.java:144)",
                "at org.apache.hadoop.examples.ExampleDriver.main(ExampleDriver.java:74)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(Native Method)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:498)",
                "at org.apache.hadoop.util.RunJar.run(RunJar.java:304)",
                "at org.apache.hadoop.util.RunJar.main(RunJar.java:218)"
            ],
            "RootCause": "The root cause of the issue is the absence of the `setErasureCodingPolicy` method in the `ClientProtocol` interface of the HDFS 2.8 version. This indicates a version mismatch between the Hadoop client (3.1) and the HDFS server (2.8), where the client is attempting to call a method that does not exist in the server's API.",
            "StepsToReproduce": [
                "Set up an HDFS cluster running version 2.8.",
                "Use a Hadoop client version 3.1.",
                "Run the teragen job using the command: `bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.0-SNAPSHOT.jar teragen 1000000 /teragen`."
            ],
            "ExpectedBehavior": "The teragen job should complete successfully without any exceptions, and the specified output path should be populated with generated data.",
            "ObservedBehavior": "The teragen job fails with a `RemoteException` indicating that the `setErasureCodingPolicy` method is unknown, leading to job termination.",
            "Suggestions": "To resolve this issue, consider using a compatible version of the Hadoop client that matches the HDFS server version. Alternatively, modify the job to avoid calling the `setErasureCodingPolicy` method if it is not supported by the server.",
            "problem_location": {
                "files": [
                    "JobResourceUploader.java"
                ],
                "classes": [
                    "org.apache.hadoop.mapreduce.JobResourceUploader"
                ],
                "methods": [
                    "JobResourceUploader.disableErasureCodingForPath"
                ]
            },
            "possible_fix": "Modify the `disableErasureCodingForPath` method in `JobResourceUploader` to handle the absence of the `setErasureCodingPolicy` method more gracefully, possibly by checking the Hadoop version before attempting to call it."
        },
        "possible_fix_code": {
            "JobResourceUploader.disableErasureCodingForPath": "  private void disableErasureCodingForPath(FileSystem fs, Path path)\n      throws IOException {\n    if (jtFs instanceof DistributedFileSystem) {\n      LOG.info(\"Disabling Erasure Coding for path: \" + path);\n      // Check if the method exists before calling it\n      try {\n        DistributedFileSystem dfs = (DistributedFileSystem) jtFs;\n        dfs.setErasureCodingPolicy(path,\n            SystemErasureCodingPolicies.getReplicationPolicy().getName());\n      } catch (NoSuchMethodError e) {\n        LOG.warn(\"setErasureCodingPolicy method not available in this HDFS version. Skipping.\");\n      }\n    }\n  }"
        }
    },
    {
        "filename": "MAPREDUCE-4843.json",
        "creation_time": "2012-12-04T13:39:40.000+0000",
        "bug_report": {
            "Title": "When using DefaultTaskController, JobLocalizer not thread safe",
            "Description": "In our Hadoop cluster, jobs occasionally fail due to a DiskErrorException indicating that the TaskTracker cannot locate the job configuration file (`job.xml`) in the specified local directories. The issue arises from the non-thread-safe nature of the JobLocalizer when multiple TaskLauncher threads attempt to initialize jobs simultaneously. This leads to conflicts where the configuration for one job can inadvertently overwrite that of another, resulting in the failure to find the required job.xml file.",
            "StackTrace": [
                "2012-12-03 23:11:54,811 WARN org.apache.hadoop.mapred.TaskTracker: Error initializing attempt_201212031626_1115_r_000023_0:",
                "org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find taskTracker/$username/jobcache/job_201212031626_1115/job.xml in any of the configured local directories",
                "at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathToRead(LocalDirAllocator.java:424)",
                "at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathToRead(LocalDirAllocator.java:160)",
                "at org.apache.hadoop.mapred.TaskTracker.initializeJob(TaskTracker.java:1175)",
                "at org.apache.hadoop.mapred.TaskTracker.localizeJob(TaskTracker.java:1058)",
                "at org.apache.hadoop.mapred.TaskTracker.startNewTask(TaskTracker.java:2213)"
            ],
            "RootCause": "The root cause of the issue is that the JobLocalizer is not thread-safe. When multiple TaskLauncher threads (mapLauncher and reduceLauncher) attempt to initialize jobs concurrently, they share a single JobConf instance, leading to race conditions where one job's configuration can overwrite another's.",
            "StepsToReproduce": [
                "Set up a Hadoop cluster with multiple TaskLauncher threads.",
                "Submit multiple jobs that require localization of job files simultaneously.",
                "Monitor the TaskTracker logs for DiskErrorException related to job.xml not being found."
            ],
            "ExpectedBehavior": "The TaskTracker should successfully locate and initialize the job configuration file (`job.xml`) for each job without conflicts, allowing all jobs to run concurrently without errors.",
            "ObservedBehavior": "The TaskTracker fails to find the job configuration file (`job.xml`) for some jobs, resulting in a DiskErrorException and job failures.",
            "Suggestions": "To mitigate this issue, consider synchronizing access to the JobLocalizer or refactoring the JobLocalizer to ensure thread safety. Additionally, review the configuration of local directories to ensure they are correctly set up for concurrent job execution.",
            "problem_location": {
                "files": [
                    "TaskTracker.java",
                    "JobLocalizer.java"
                ],
                "classes": [
                    "org.apache.hadoop.mapred.TaskTracker",
                    "org.apache.hadoop.mapred.JobLocalizer"
                ],
                "methods": [
                    "TaskTracker.initializeJob",
                    "TaskTracker.localizeJob"
                ]
            },
            "possible_fix": "Refactor the JobLocalizer to manage its own configuration state in a thread-safe manner, possibly by using synchronization mechanisms or by creating separate instances of JobLocalizer for each job."
        },
        "possible_fix_code": {
            "TaskTracker.initializeJob": "public void initializeJob(JobID jobId, JobConf conf) throws IOException {\n    synchronized (this) {\n        // Ensure that the JobLocalizer is thread-safe\n        JobLocalizer localizer = new JobLocalizer(conf);\n        localizer.localizeJob(jobId);\n    }\n    // Other initialization logic...\n}"
        }
    },
    {
        "filename": "MAPREDUCE-5028.json",
        "creation_time": "2013-02-26T03:54:25.000+0000",
        "bug_report": {
            "Title": "Maps fail when io.sort.mb is set to high value",
            "Description": "The issue occurs in branch-1 when running a Hadoop MapReduce job with a specific configuration: pseudo-distributed mode with 2 maps and 1 reduce, where `mapred.child.java.opts` is set to -Xmx2048m, `io.sort.mb` is set to 1280, and `dfs.block.size` is 2147483648. The problem manifests during the execution of the `wordcount` job after generating 4 GB of data using `teragen`. The error encountered is an `IOException` indicating that the spill operation failed, which is traced back to an `EOFException` during data processing.",
            "StackTrace": [
                "java.io.IOException: Spill failed",
                "at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:1031)",
                "at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:692)",
                "at org.apache.hadoop.mapreduce.TaskInputOutputContext.write(TaskInputOutputContext.java:80)",
                "at org.apache.hadoop.examples.WordCount$TokenizerMapper.map(WordCount.java:45)",
                "at org.apache.hadoop.examples.WordCount$TokenizerMapper.map(WordCount.java:34)",
                "at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)",
                "at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:766)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:370)",
                "at org.apache.hadoop.mapred.Child$4.run(Child.java:255)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:396)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1149)",
                "at org.apache.hadoop.mapred.Child.main(Child.java:249)",
                "Caused by: java.io.EOFException",
                "at java.io.DataInputStream.readInt(DataInputStream.java:375)",
                "at org.apache.hadoop.io.IntWritable.readFields(IntWritable.java:38)",
                "at org.apache.hadoop.io.serializer.WritableSerialization$WritableDeserializer.deserialize(WritableSerialization.java:67)",
                "at org.apache.hadoop.io.serializer.WritableSerialization$WritableDeserializer.deserialize(WritableSerialization.java:40)",
                "at org.apache.hadoop.mapreduce.ReduceContext.nextKeyValue(ReduceContext.java:116)",
                "at org.apache.hadoop.mapreduce.ReduceContext.nextKey(ReduceContext.java:92)",
                "at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:175)",
                "at org.apache.hadoop.mapred.Task$NewCombinerRunner.combine(Task.java:1505)",
                "at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1438)",
                "at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.access$1800(MapTask.java:855)",
                "at org.apache.hadoop.mapred.MapTask$MapOutputBuffer$SpillThread.run(MapTask.java:1346)"
            ],
            "RootCause": "The root cause of the issue is an `EOFException` that occurs during the spill operation in the `MapTask.collect` method. This exception indicates that the end of a file or stream was reached unexpectedly, likely due to data corruption or misconfiguration in the spill process.",
            "StepsToReproduce": [
                "Set up a Hadoop cluster in pseudo-distributed mode.",
                "Configure the following settings: mapred.child.java.opts=-Xmx2048m, io.sort.mb=1280, dfs.block.size=2147483648.",
                "Run the `teragen` command to generate 4 GB of data.",
                "Execute the `wordcount` job on the generated data."
            ],
            "ExpectedBehavior": "The MapReduce job should complete successfully without any errors during the spill operation, producing the expected word count output.",
            "ObservedBehavior": "The job fails with an `IOException: Spill failed` error, indicating an issue during the spill process, specifically an `EOFException`.",
            "Suggestions": "Investigate the data integrity and ensure that the input data is not corrupted. Additionally, review the configuration settings for potential misconfigurations that could lead to excessive memory usage or buffer overflows during the spill process.",
            "problem_location": {
                "files": [
                    "hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/MapTask.java",
                    "hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/MapOutputBuffer.java"
                ],
                "classes": [
                    "org.apache.hadoop.mapred.MapTask",
                    "org.apache.hadoop.mapred.MapOutputBuffer"
                ],
                "methods": [
                    "MapTask.collect",
                    "MapOutputBuffer.checkSpillException"
                ]
            },
            "possible_fix": "Review the implementation of the `checkSpillException` method in the `MapOutputBuffer` class to ensure that it correctly handles spill exceptions. Additionally, consider adjusting the `io.sort.mb` configuration to a lower value to prevent memory overflow during the spill process."
        },
        "possible_fix_code": {
            "MapTask.collect": "    public synchronized void collect(K key, V value, final int partition\n                                     ) throws IOException {\n      reporter.progress();\n      if (key.getClass() != keyClass) {\n        throw new IOException(\"Type mismatch in key from map: expected \"\n                              + keyClass.getName() + \", received \"\n                              + key.getClass().getName());\n      }\n      if (value.getClass() != valClass) {\n        throw new IOException(\"Type mismatch in value from map: expected \"\n                              + valClass.getName() + \", received \"\n                              + value.getClass().getName());\n      }\n      if (partition < 0 || partition >= partitions) {\n        throw new IOException(\"Illegal partition for \" + key + \" (\" +\n            partition + \")\");\n      }\n      checkSpillException();\n      bufferRemaining -= METASIZE;\n      if (bufferRemaining <= 0) {\n        // start spill if the thread is not running and the soft limit has been\n        // reached\n        spillLock.lock();\n        try {\n          do {\n            if (!spillInProgress) {\n              final int kvbidx = 4 * kvindex;\n              final int kvbend = 4 * kvend;\n              // serialized, unspilled bytes always lie between kvindex and\n              // bufindex, crossing the equator. Note that any void space\n              // created by a reset must be included in \"used\" bytes\n              final int bUsed = distanceTo(kvbidx, bufindex);\n              final boolean bufsoftlimit = bUsed >= softLimit;\n              if ((kvbend + METASIZE) % kvbuffer.length !=\n                  equator - (equator % METASIZE)) {\n                // spill finished, reclaim space\n                resetSpill();\n                bufferRemaining = Math.min(\n                    distanceTo(bufindex, kvbidx) - 2 * METASIZE,\n                    softLimit - bUsed) - METASIZE;\n                continue;\n              } else if (bufsoftlimit && kvindex != kvend) {\n                // spill records, if any collected; check latter, as it may\n                // be possible for metadata alignment to hit spill pcnt\n                startSpill();\n                final int avgRec = (int)\n                  (mapOutputByteCounter.getCounter() /\n                  mapOutputRecordCounter.getCounter());\n                // leave at least half the split buffer for serialization data\n                // ensure that kvindex >= bufindex\n                final int distkvi = distanceTo(bufindex, kvbidx);\n                final int newPos = (bufindex +\n                  Math.max(2 * METASIZE - 1,\n                          Math.min(distkvi / 2,\n                                   distkvi / (METASIZE + avgRec) * METASIZE)))\n                  % kvbuffer.length;\n                setEquator(newPos);\n                bufmark = bufindex = newPos;\n                final int serBound = 4 * kvend;\n                // bytes remaining before the lock must be held and limits\n                // checked is the minimum of three arcs: the metadata space, the\n                // serialization space, and the soft limit\n                bufferRemaining = Math.min(\n                    // metadata max\n                    distanceTo(bufend, newPos),\n                    Math.min(\n                      // serialization max\n                      distanceTo(newPos, serBound),\n                      // soft limit\n                      softLimit)) - 2 * METASIZE;\n              }\n            }\n          } while (false);\n        } finally {\n          spillLock.unlock();\n        }\n      }\n\n      try {\n        // serialize key bytes into buffer\n        int keystart = bufindex;\n        keySerializer.serialize(key);\n        if (bufindex < keystart) {\n          // wrapped the key; must make contiguous\n          bb.shiftBufferedKey();\n          keystart = 0;\n        }\n        // serialize value bytes into buffer\n        final int valstart = bufindex;\n        valSerializer.serialize(value);\n        // It's possible for records to have zero length, i.e. the serializer\n        // will perform no writes. To ensure that the boundary conditions are\n        // checked and that the kvindex invariant is maintained, perform a\n        // zero-length write into the buffer. The logic monitoring this could be\n        // moved into collect, but this is cleaner and inexpensive. For now, it\n        // is acceptable.\n        bb.write(b0, 0, 0);\n\n        // the record must be marked after the preceding write, as the metadata\n        // for this record are not yet written\n        int valend = bb.markRecord();\n\n        mapOutputRecordCounter.increment(1);\n        mapOutputByteCounter.increment(\n            distanceTo(keystart, valend, bufvoid));\n\n        // write accounting info\n        kvmeta.put(kvindex + INDEX, kvindex);\n        kvmeta.put(kvindex + PARTITION, partition);\n        kvmeta.put(kvindex + KEYSTART, keystart);\n        kvmeta.put(kvindex + VALSTART, valstart);\n        // advance kvindex\n        kvindex = (kvindex - NMETA + kvmeta.capacity()) % kvmeta.capacity();\n      } catch (MapBufferTooSmallException e) {\n        LOG.info(\"Record too large for in-memory buffer: \" + e.getMessage());\n        spillSingleRecord(key, value, partition);\n        mapOutputRecordCounter.increment(1);\n        return;\n      } catch (IOException e) {\n        LOG.error(\"IOException during collect: \" + e.getMessage(), e);\n        throw new IOException(\"Error during collect\", e);\n      }\n    }"
        }
    },
    {
        "filename": "MAPREDUCE-4300.json",
        "creation_time": "2012-05-31T20:44:00.000+0000",
        "bug_report": {
            "Title": "OOM in AM can turn it into a zombie.",
            "Description": "The application is experiencing multiple instances of `java.lang.OutOfMemoryError: Java heap space`, which is causing four threads in the Application Master (AM) to terminate unexpectedly. This issue appears to be related to memory management within the Hadoop framework, particularly in the context of data processing and resource allocation. The stack trace indicates that the errors are occurring in various components, including the `ResponseProcessor`, `DefaultSpeculator`, and `Timer for 'MRAppMaster' metrics system`. The application may be processing excessive data or may not have sufficient heap size allocated, leading to these memory errors.",
            "StackTrace": [
                "Exception in thread \"ResponseProcessor for block BP-1114822160-<IP>-1322528669066:blk_-6528896407411719649_34227308\" java.lang.OutOfMemoryError: Java heap space",
                "at com.google.protobuf.CodedInputStream.<init>(CodedInputStream.java:538)",
                "at com.google.protobuf.CodedInputStream.newInstance(CodedInputStream.java:55)",
                "at com.google.protobuf.AbstractMessageLite$Builder.mergeFrom(AbstractMessageLite.java:201)",
                "at com.google.protobuf.AbstractMessage$Builder.mergeFrom(AbstractMessage.java:738)",
                "at org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$PipelineAckProto.parseFrom(DataTransferProtos.java:7287)",
                "at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:95)",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:656)",
                "Exception in thread \"DefaultSpeculator background processing\" java.lang.OutOfMemoryError: Java heap space",
                "at java.util.HashMap.resize(HashMap.java:462)",
                "at java.util.HashMap.addEntry(HashMap.java:755)",
                "at java.util.HashMap.put(HashMap.java:385)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.getTasks(JobImpl.java:632)",
                "at org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator.maybeScheduleASpeculation(DefaultSpeculator.java:465)",
                "at org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator.maybeScheduleAMapSpeculation(DefaultSpeculator.java:433)",
                "at org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator.computeSpeculations(DefaultSpeculator.java:509)",
                "at org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator.access$100(DefaultSpeculator.java:56)",
                "at org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator$1.run(DefaultSpeculator.java:176)",
                "at java.lang.Thread.run(Thread.java:619)",
                "Exception in thread \"Timer for 'MRAppMaster' metrics system\" java.lang.OutOfMemoryError: Java heap space",
                "Exception in thread \"Socket Reader #4 for port 50500\" java.lang.OutOfMemoryError: Java heap space"
            ],
            "RootCause": "The root cause of the issue is insufficient Java heap space allocated for the Application Master, leading to multiple OutOfMemoryError exceptions across different threads. This can be attributed to either a lack of memory allocation in the Hadoop configuration or excessive data processing demands on the system.",
            "StepsToReproduce": [
                "Run a Hadoop job that processes a large dataset.",
                "Monitor the Application Master for memory usage.",
                "Observe the logs for any OutOfMemoryError messages."
            ],
            "ExpectedBehavior": "The Application Master should handle the job without running out of memory, processing data efficiently and without terminating threads unexpectedly.",
            "ObservedBehavior": "The Application Master terminates multiple threads due to OutOfMemoryError, leading to a failure in job execution and potential zombie processes.",
            "Suggestions": "Increase the heap size allocated to the Application Master in the Hadoop configuration. Review the job's data processing requirements to ensure they are within the memory limits. Consider optimizing the job to reduce memory consumption.",
            "problem_location": {
                "files": [
                    "org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.java",
                    "org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator.java"
                ],
                "classes": [
                    "org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl",
                    "org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator"
                ],
                "methods": [
                    "JobImpl.getTasks",
                    "DefaultSpeculator.maybeScheduleASpeculation"
                ]
            },
            "possible_fix": "Increase the heap size in the Hadoop configuration by modifying the `yarn.nodemanager.resource.memory-mb` and `yarn.scheduler.maximum-allocation-mb` settings. Example: Set `yarn.nodemanager.resource.memory-mb` to a higher value, such as 4096 MB."
        },
        "possible_fix_code": {
            "JobImpl.getTasks": "  public Map<TaskId,Task> getTasks(TaskType taskType) {\n    Map<TaskId, Task> localTasksCopy = tasks;\n    Map<TaskId, Task> result = new HashMap<TaskId, Task>();\n    Set<TaskId> tasksOfGivenType = null;\n    readLock.lock();\n    try {\n      if (TaskType.MAP == taskType) {\n        tasksOfGivenType = mapTasks;\n      } else {\n        tasksOfGivenType = reduceTasks;\n      }\n      for (TaskId taskID : tasksOfGivenType) {\n        if (localTasksCopy.containsKey(taskID)) {\n          result.put(taskID, localTasksCopy.get(taskID));\n        }\n      }\n      return result;\n    } finally {\n      readLock.unlock();\n    }\n  }"
        }
    },
    {
        "filename": "MAPREDUCE-3241.json",
        "creation_time": "2011-10-21T16:25:33.000+0000",
        "bug_report": {
            "Title": "(Rumen)TraceBuilder throws IllegalArgumentException",
            "Description": "The TraceBuilder encounters an IllegalArgumentException when processing a job history file. The exception indicates that the JobBuilder's process method is unable to handle an event type from the HistoryEvent interface, leading to a failure in generating the expected output, which should include map and reduce task information.",
            "StackTrace": [
                "2011-10-21 22:07:17,268 WARN  rumen.TraceBuilder (TraceBuilder.java:run(272)) - TraceBuilder got an error while processing the [possibly virtual] file job_1319214405771_0002-1319214846458-root-word+count-1319214871038-1-1-SUCCEEDED.jhist within Path hdfs://10.18.52.57:9000/user/root/null/history/done_intermediate/root/job_1319214405771_0002-1319214846458-root-word+count-1319214871038-1-1-SUCCEEDED.jhist",
                "java.lang.IllegalArgumentException: JobBuilder.process(HistoryEvent): unknown event type",
                "at org.apache.hadoop.tools.rumen.JobBuilder.process(JobBuilder.java:165)",
                "at org.apache.hadoop.tools.rumen.TraceBuilder.processJobHistory(TraceBuilder.java:304)",
                "at org.apache.hadoop.tools.rumen.TraceBuilder.run(TraceBuilder.java:258)",
                "at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:69)",
                "at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:83)",
                "at org.apache.hadoop.tools.rumen.TraceBuilder.main(TraceBuilder.java:185)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "at java.lang.reflect.Method.invoke(Method.java:597)",
                "at org.apache.hadoop.util.RunJar.main(RunJar.java:189)"
            ],
            "RootCause": "The IllegalArgumentException is thrown in the JobBuilder.process method due to an unrecognized event type from the HistoryEvent interface. This indicates that the method is not equipped to handle the specific event type being passed to it.",
            "StepsToReproduce": [
                "Run the TraceBuilder with the specified job history file.",
                "Observe the output and the exception thrown."
            ],
            "ExpectedBehavior": "The TraceBuilder should successfully process the job history file and output the map and reduce task information without throwing exceptions.",
            "ObservedBehavior": "The TraceBuilder fails with an IllegalArgumentException, indicating an unknown event type, and does not produce the expected output.",
            "Suggestions": "Investigate the implementations of the HistoryEvent interface to identify the event types that the JobBuilder is expected to handle. Ensure that the JobBuilder's process method can accommodate all relevant event types.",
            "problem_location": {
                "files": [
                    "hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/JobBuilder.java",
                    "hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/TraceBuilder.java"
                ],
                "classes": [
                    "org.apache.hadoop.tools.rumen.JobBuilder",
                    "org.apache.hadoop.tools.rumen.TraceBuilder",
                    "org.apache.hadoop.mapreduce.jobhistory.HistoryEvent"
                ],
                "methods": [
                    "JobBuilder.process",
                    "TraceBuilder.processJobHistory",
                    "TraceBuilder.run"
                ]
            },
            "possible_fix": "Review the HistoryEvent interface and its implementations to ensure that all event types are accounted for in the JobBuilder.process method. Modify the process method to handle the unknown event type appropriately."
        },
        "possible_fix_code": {
            "JobBuilder.process": "public void process(HistoryEvent event) {\n    if (event == null) {\n        throw new IllegalArgumentException(\"Event cannot be null\");\n    }\n    switch (event.getEventType()) {\n        case JOB_STARTED:\n            // Handle job started event\n            break;\n        case JOB_FINISHED:\n            // Handle job finished event\n            break;\n        // Add cases for other known event types\n        default:\n            throw new IllegalArgumentException(\"JobBuilder.process(HistoryEvent): unknown event type\");\n    }\n}"
        }
    },
    {
        "filename": "MAPREDUCE-6002.json",
        "creation_time": "2014-07-24T02:49:25.000+0000",
        "bug_report": {
            "Title": "MR task should prevent report error to AM when process is shutting down",
            "Description": "The issue arises when a preempted MapReduce (MR) task is incorrectly treated as failed due to an IOException caused by a closed filesystem. This occurs during the shutdown process of the filesystem, particularly when the FileSystem instance is still in use, such as when reading split details from HDFS. The shutdown hook in the FileSystem closes all instances, leading to a fatal error being reported to the Application Master (AM). The stack trace indicates that the error is triggered in the `DFSClient.checkOpen` method when an attempt is made to read from a closed filesystem.",
            "StackTrace": [
                "2014-07-22 01:46:19,613 FATAL [IPC Server handler 10 on 56903] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Task: attempt_1405985051088_0018_m_000025_0 - exited : java.io.IOException: Filesystem closed",
                "at org.apache.hadoop.hdfs.DFSClient.checkOpen(DFSClient.java:707)",
                "at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:776)",
                "at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:837)",
                "at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:645)",
                "at java.io.DataInputStream.readByte(DataInputStream.java:265)",
                "at org.apache.hadoop.io.WritableUtils.readVLong(WritableUtils.java:308)",
                "at org.apache.hadoop.io.WritableUtils.readVIntInRange(WritableUtils.java:348)",
                "at org.apache.hadoop.io.Text.readString(Text.java:464)",
                "at org.apache.hadoop.io.Text.readString(Text.java:457)",
                "at org.apache.hadoop.mapred.MapTask.getSplitDetails(MapTask.java:357)",
                "at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:731)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:340)",
                "at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1594)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)"
            ],
            "RootCause": "The root cause of the issue is the premature closure of the filesystem during the shutdown process, which leads to an IOException when the MR task attempts to read from it. This occurs because the task is still trying to access the filesystem while it is being closed, resulting in a failure being reported to the AM instead of a preemption.",
            "StepsToReproduce": [
                "1. Start a MapReduce job that involves reading from HDFS.",
                "2. Trigger a preemption of the MR task while it is still accessing the filesystem.",
                "3. Initiate a shutdown of the filesystem during the read operation.",
                "4. Observe the error reported to the Application Master."
            ],
            "ExpectedBehavior": "The MR task should recognize that it has been preempted and not report any errors to the Application Master during the shutdown process of the filesystem.",
            "ObservedBehavior": "The MR task fails and reports a fatal IOException to the Application Master, indicating that the filesystem has been closed, instead of being marked as preempted.",
            "Suggestions": "To prevent this issue, implement checks to ensure that the filesystem is not closed while it is still in use by the MR task. Additionally, modify the error handling to differentiate between a preemption and a failure, ensuring that preempted tasks do not report errors to the AM.",
            "problem_location": {
                "files": [
                    "TaskAttemptListenerImpl.java",
                    "MapTask.java"
                ],
                "classes": [
                    "org.apache.hadoop.mapred.TaskAttemptListenerImpl",
                    "org.apache.hadoop.mapred.MapTask"
                ],
                "methods": [
                    "TaskAttemptListenerImpl.reportTaskFailed",
                    "MapTask.getSplitDetails"
                ]
            },
            "possible_fix": "Consider adding a state check in the MR task to determine if it is being preempted before attempting to read from the filesystem. If preempted, the task should gracefully handle the situation without reporting an error. Additionally, ensure that the shutdown hook for the filesystem is managed in a way that it does not interfere with ongoing read operations."
        },
        "possible_fix_code": {
            "org.apache.hadoop.mapred.MapTask.getSplitDetails": " private <T> T getSplitDetails(Path file, long offset) \n  throws IOException {\n   FileSystem fs = file.getFileSystem(conf);\n   if (fs.isClosed()) {\n       // Handle the case where the filesystem is closed\n       throw new IOException(\"Filesystem is closed, cannot read split details.\");\n   }\n   FSDataInputStream inFile = fs.open(file);\n   inFile.seek(offset);\n   String className = StringInterner.weakIntern(Text.readString(inFile));\n   Class<T> cls;\n   try {\n     cls = (Class<T>) conf.getClassByName(className);\n   } catch (ClassNotFoundException ce) {\n     IOException wrap = new IOException(\"Split class \" + className + \n                                         \" not found\");\n     wrap.initCause(ce);\n     throw wrap;\n   }\n   SerializationFactory factory = new SerializationFactory(conf);\n   Deserializer<T> deserializer = \n     (Deserializer<T>) factory.getDeserializer(cls);\n   deserializer.open(inFile);\n   T split = deserializer.deserialize(null);\n   long pos = inFile.getPos();\n   getCounters().findCounter(\n       TaskCounter.SPLIT_RAW_BYTES).increment(pos - offset);\n   inFile.close();\n   return split;\n }"
        }
    },
    {
        "filename": "MAPREDUCE-6452.json",
        "creation_time": "2015-08-14T12:22:27.000+0000",
        "bug_report": {
            "Title": "NPE when intermediate encrypt enabled for LocalRunner",
            "Description": "When running a MapReduce job with the local framework and enabling encrypted intermediate data, a NullPointerException (NPE) occurs. This issue arises specifically during the execution of the map task, indicating that an object reference is being used without proper initialization. The stack trace reveals that the error originates from the `CryptoOutputStream` initialization, which is part of the Hadoop framework's handling of encrypted data. The failure consistently occurs, leading to job termination.",
            "StackTrace": [
                "2015-08-14 16:27:25,248 WARN  [Thread-21] mapred.LocalJobRunner (LocalJobRunner.java:run(561)) - job_local473843898_0001",
                "java.lang.Exception: java.lang.NullPointerException",
                "at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:463)",
                "at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:523)",
                "Caused by: java.lang.NullPointerException",
                "at org.apache.hadoop.crypto.CryptoOutputStream.<init>(CryptoOutputStream.java:92)",
                "at org.apache.hadoop.fs.crypto.CryptoFSDataOutputStream.<init>(CryptoFSDataOutputStream.java:31)",
                "at org.apache.hadoop.mapreduce.CryptoUtils.wrapIfNecessary(CryptoUtils.java:112)",
                "at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1611)",
                "at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.flush(MapTask.java:1492)",
                "at org.apache.hadoop.mapred.MapTask$NewOutputCollector.close(MapTask.java:723)",
                "at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:793)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)",
                "at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:244)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:266)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "RootCause": "The root cause of the NullPointerException is the failure to initialize an object reference in the `CryptoOutputStream` class, which is invoked during the map task execution. This indicates that the necessary setup for handling encrypted intermediate data is not being completed successfully.",
            "StepsToReproduce": [
                "Set the property 'mapreduce.framework.name' to 'local'.",
                "Set the property 'mapreduce.job.encrypted-intermediate-data' to 'true'.",
                "Run a MapReduce job using the local framework."
            ],
            "ExpectedBehavior": "The MapReduce job should execute successfully without throwing a NullPointerException, and the intermediate data should be encrypted as specified.",
            "ObservedBehavior": "The MapReduce job fails with a NullPointerException, causing the job to terminate prematurely.",
            "Suggestions": "To resolve this issue, ensure that all necessary objects related to encryption are properly initialized before use. Implement null checks in the relevant classes to prevent the NullPointerException from occurring. Additionally, review the Hadoop documentation for any specific requirements regarding the use of encrypted intermediate data.",
            "problem_location": {
                "files": [
                    "LocalJobRunner.java",
                    "CryptoOutputStream.java",
                    "CryptoFSDataOutputStream.java",
                    "MapTask.java"
                ],
                "classes": [
                    "org.apache.hadoop.mapred.LocalJobRunner",
                    "org.apache.hadoop.crypto.CryptoOutputStream",
                    "org.apache.hadoop.fs.crypto.CryptoFSDataOutputStream",
                    "org.apache.hadoop.mapred.MapTask"
                ],
                "methods": [
                    "LocalJobRunner.run",
                    "CryptoOutputStream.init",
                    "CryptoFSDataOutputStream.init",
                    "MapTask.run"
                ]
            },
            "possible_fix": "Check the initialization of the CryptoOutputStream and related classes. Ensure that all required parameters are passed correctly during instantiation. Consider adding null checks in the constructors of these classes to handle cases where initialization might fail."
        },
        "possible_fix_code": {
            "LocalJobRunner.run": "    public void run() {\n      JobID jobId = profile.getJobID();\n      JobContext jContext = new JobContextImpl(job, jobId);\n      \n      org.apache.hadoop.mapreduce.OutputCommitter outputCommitter = null;\n      try {\n        outputCommitter = createOutputCommitter(conf.getUseNewMapper(), jobId, conf);\n      } catch (Exception e) {\n        LOG.info(\"Failed to createOutputCommitter\", e);\n        return;\n      }\n      \n      try {\n        TaskSplitMetaInfo[] taskSplitMetaInfos = \n          SplitMetaInfoReader.readSplitMetaInfo(jobId, localFs, conf, systemJobDir);\n\n        int numReduceTasks = job.getNumReduceTasks();\n        outputCommitter.setupJob(jContext);\n        status.setSetupProgress(1.0f);\n\n        Map<TaskAttemptID, MapOutputFile> mapOutputFiles =\n            Collections.synchronizedMap(new HashMap<TaskAttemptID, MapOutputFile>());\n        \n        List<RunnableWithThrowable> mapRunnables = getMapTaskRunnables(\n            taskSplitMetaInfos, jobId, mapOutputFiles);\n              \n        initCounters(mapRunnables.size(), numReduceTasks);\n        ExecutorService mapService = createMapExecutor();\n        runTasks(mapRunnables, mapService, \"map\");\n\n        try {\n          if (numReduceTasks > 0) {\n            List<RunnableWithThrowable> reduceRunnables = getReduceTaskRunnables(\n                jobId, mapOutputFiles);\n            ExecutorService reduceService = createReduceExecutor();\n            runTasks(reduceRunnables, reduceService, \"reduce\");\n          }\n        } finally {\n          for (MapOutputFile output : mapOutputFiles.values()) {\n            output.removeAll();\n          }\n        }\n        // delete the temporary directory in output directory\n        outputCommitter.commitJob(jContext);\n        status.setCleanupProgress(1.0f);\n\n        if (killed) {\n          this.status.setRunState(JobStatus.KILLED);\n        } else {\n          this.status.setRunState(JobStatus.SUCCEEDED);\n        }\n\n        JobEndNotifier.localRunnerNotification(job, status);\n      } catch (Throwable t) {\n        try {\n          outputCommitter.abortJob(jContext, \n            org.apache.hadoop.mapreduce.JobStatus.State.FAILED);\n        } catch (IOException ioe) {\n          LOG.info(\"Error cleaning up job:\" + id);\n        }\n        status.setCleanupProgress(1.0f);\n        if (killed) {\n          this.status.setRunState(JobStatus.KILLED);\n        } else {\n          this.status.setRunState(JobStatus.FAILED);\n        }\n        LOG.warn(id, t);\n\n        JobEndNotifier.localRunnerNotification(job, status);\n\n      } finally {\n        try {\n          fs.delete(systemJobFile.getParent(), true);  // delete submit dir\n          localFs.delete(localJobFile, true);              // delete local copy\n          // Cleanup distributed cache\n          localDistributedCacheManager.close();\n        } catch (IOException e) {\n          LOG.warn(\"Error cleaning up \" + id + \": \" + e);\n        }\n      }\n    }"
        }
    },
    {
        "filename": "MAPREDUCE-6649.json",
        "creation_time": "2016-03-07T16:37:13.000+0000",
        "bug_report": {
            "Title": "getFailureInfo not returning any failure info",
            "Description": "The command executed to run a Hadoop job does not yield any failure information when the job fails. This issue is evident from the stack trace, which indicates an `ExitCodeException` with an exit code of 1, suggesting a failure in executing a command within the Hadoop framework. The stack trace shows that the failure occurs in the `Shell.runCommand` method, but the specific cause of the failure is not detailed in the output. In contrast, a different command execution provides detailed failure information, indicating that the lack of failure info may be related to the job configuration or the command options used.",
            "StackTrace": [
                "2016-03-07 10:34:58,112 INFO  [main] mapreduce.Job (Job.java:monitorAndPrintJob(1431)) - Job job_1457364518683_0004 failed with state FAILED due to:",
                "ExitCodeException exitCode=1:",
                "at org.apache.hadoop.util.Shell.runCommand(Shell.java:927)",
                "at org.apache.hadoop.util.Shell.run(Shell.java:838)",
                "at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1117)",
                "at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:227)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:319)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:88)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:266)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "RootCause": "The root cause of the issue appears to be related to the command execution process within the Hadoop framework, specifically an `ExitCodeException` indicating that a command failed to execute properly. The lack of detailed failure information suggests that the job configuration or command options may not be set up to capture or report errors effectively.",
            "StepsToReproduce": [
                "Run the following command to execute a Hadoop job: $HADOOP_PREFIX/bin/hadoop jar $HADOOP_PREFIX/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-${HADOOP_VERSION}-tests.jar sleep -Dmapreduce.jobtracker.split.metainfo.maxsize=10 -Dmapreduce.job.queuename=default -m 1 -r 1 -mt 1 -rt 1",
                "Observe the output for failure information."
            ],
            "ExpectedBehavior": "The system should provide detailed failure information when a job fails, including the reason for failure and any relevant diagnostics.",
            "ObservedBehavior": "The command executed does not return any failure information, making it difficult to diagnose the cause of the job failure.",
            "Suggestions": "Review the command options used in the job execution to ensure they are configured to capture failure information. Consider using different command options or configurations that have previously yielded detailed failure diagnostics.",
            "problem_location": {
                "files": [
                    "Job.java",
                    "Shell.java",
                    "DefaultContainerExecutor.java"
                ],
                "classes": [
                    "org.apache.hadoop.mapreduce.Job",
                    "org.apache.hadoop.util.Shell",
                    "org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor"
                ],
                "methods": [
                    "Job.monitorAndPrintJob",
                    "Shell.runCommand",
                    "DefaultContainerExecutor.launchContainer"
                ]
            },
            "possible_fix": "Investigate the command options and configurations used in the job execution. Ensure that the job is set up to capture and report failure information effectively. If necessary, modify the job configuration to include additional logging or diagnostics."
        },
        "possible_fix_code": {
            "Shell.runCommand": "  private void runCommand() throws IOException { \n    ProcessBuilder builder = new ProcessBuilder(getExecString());\n    Timer timeOutTimer = null;\n    ShellTimeoutTimerTask timeoutTimerTask = null;\n    timedOut.set(false);\n    completed.set(false);\n\n    if (environment != null) {\n      builder.environment().putAll(this.environment);\n    }\n    if (dir != null) {\n      builder.directory(this.dir);\n    }\n\n    builder.redirectErrorStream(redirectErrorStream);\n    \n    if (Shell.WINDOWS) {\n      synchronized (WindowsProcessLaunchLock) {\n        process = builder.start();\n      }\n    } else {\n      process = builder.start();\n    }\n\n    if (timeOutInterval > 0) {\n      timeOutTimer = new Timer(\"Shell command timeout\");\n      timeoutTimerTask = new ShellTimeoutTimerTask(\n          this);\n      timeOutTimer.schedule(timeoutTimerTask, timeOutInterval);\n    }\n    final BufferedReader errReader = \n            new BufferedReader(new InputStreamReader(\n                process.getErrorStream(), Charset.defaultCharset()));\n    BufferedReader inReader =\n            new BufferedReader(new InputStreamReader(\n                process.getInputStream(), Charset.defaultCharset()));\n    final StringBuffer errMsg = new StringBuffer();\n\n    Thread errThread = new Thread() {\n      @Override\n      public void run() {\n        try {\n          String line = errReader.readLine();\n          while((line != null) && !isInterrupted()) {\n            errMsg.append(line);\n            errMsg.append(System.getProperty(\"line.separator\"));\n            line = errReader.readLine();\n          }\n        } catch(IOException ioe) {\n          LOG.warn(\"Error reading the error stream\", ioe);\n        }\n      }\n    };\n    try {\n      errThread.start();\n    } catch (IllegalStateException ise) {\n    } catch (OutOfMemoryError oe) {\n      LOG.error(\"Caught \" + oe + \". One possible reason is that ulimit\"\n          + \" setting of 'max user processes' is too low. If so, do\"\n          + \" 'ulimit -u <largerNum>' and try again.\");\n      throw oe;\n    }\n    try {\n      parseExecResult(inReader);\n      String line = inReader.readLine();\n      while(line != null) { \n        line = inReader.readLine();\n      }\n      exitCode  = process.waitFor();\n      joinThread(errThread);\n      completed.set(true);\n      if (exitCode != 0) {\n        throw new ExitCodeException(exitCode, errMsg.toString());\n      }\n    } catch (InterruptedException ie) {\n      InterruptedIOException iie = new InterruptedIOException(ie.toString());\n      iie.initCause(ie);\n      throw iie;\n    } finally {\n      if (timeOutTimer != null) {\n        timeOutTimer.cancel();\n      }\n      try {\n        InputStream stdout = process.getInputStream();\n        synchronized (stdout) {\n          inReader.close();\n        }\n      } catch (IOException ioe) {\n        LOG.warn(\"Error while closing the input stream\", ioe);\n      }\n      if (!completed.get()) {\n        errThread.interrupt();\n        joinThread(errThread);\n      }\n      try {\n        InputStream stderr = process.getErrorStream();\n        synchronized (stderr) {\n          errReader.close();\n        }\n      } catch (IOException ioe) {\n        LOG.warn(\"Error while closing the error stream\", ioe);\n      }\n      process.destroy();\n      lastTime = Time.monotonicNow();\n    }\n  }"
        }
    },
    {
        "filename": "MAPREDUCE-4048.json",
        "creation_time": "2012-03-21T10:01:09.000+0000",
        "bug_report": {
            "Title": "NullPointerException exception while accessing the Application Master UI",
            "Description": "A NullPointerException is thrown when accessing the Application Master UI, specifically when the system attempts to handle a request for job attempts. The error occurs in the AppController class, particularly in the badRequest method, which is invoked with a null message. This situation arises when the attempts method fails to retrieve necessary parameters, leading to an exception being thrown without a valid message.",
            "StackTrace": [
                "2012-03-21 10:21:31,838 ERROR [2145015588@qtp-957250718-801] org.apache.hadoop.yarn.webapp.Dispatcher: error handling URI: /mapreduce/attempts/job_1332261815858_2_8/m/KILLED",
                "java.lang.reflect.InvocationTargetException",
                "at sun.reflect.GeneratedMethodAccessor50.invoke(Unknown Source)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "at java.lang.reflect.Method.invoke(Method.java:597)",
                "at org.apache.hadoop.yarn.webapp.Dispatcher.service(Dispatcher.java:150)",
                "at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)",
                "at com.google.inject.servlet.ServletDefinition.doService(ServletDefinition.java:263)",
                "at com.google.inject.servlet.ServletDefinition.service(ServletDefinition.java:178)",
                "at com.google.inject.servlet.ManagedServletPipeline.service(ManagedServletPipeline.java:91)",
                "at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:62)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:900)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:834)",
                "at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)",
                "at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410)",
                "at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)",
                "Caused by: java.lang.NullPointerException",
                "at com.google.common.base.Joiner.toString(Joiner.java:317)",
                "at com.google.common.base.Joiner.appendTo(Joiner.java:97)",
                "at com.google.common.base.Joiner.appendTo(Joiner.java:127)",
                "at com.google.common.base.Joiner.join(Joiner.java:158)",
                "at com.google.common.base.Joiner.join(Joiner.java:166)",
                "at org.apache.hadoop.yarn.util.StringHelper.join(StringHelper.java:102)",
                "at org.apache.hadoop.mapreduce.v2.app.webapp.AppController.badRequest(AppController.java:319)",
                "at org.apache.hadoop.mapreduce.v2.app.webapp.AppController.attempts(AppController.java:286)"
            ],
            "RootCause": "The NullPointerException is caused by the badRequest method being called with a null message, which occurs when the attempts method throws an exception due to missing parameters (taskType or attemptState).",
            "StepsToReproduce": [
                "Access the Application Master UI.",
                "Trigger a request for job attempts without providing the required parameters (taskType and attemptState)."
            ],
            "ExpectedBehavior": "The system should return a valid response indicating the status of job attempts without throwing an exception.",
            "ObservedBehavior": "A NullPointerException is thrown, resulting in an error response when attempting to access job attempts.",
            "Suggestions": "Ensure that the attempts method checks for null or empty parameters before calling the badRequest method. Consider adding validation for taskType and attemptState to prevent null messages.",
            "problem_location": {
                "files": [
                    "hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/webapp/AppController.java"
                ],
                "classes": [
                    "org.apache.hadoop.mapreduce.v2.app.webapp.AppController"
                ],
                "methods": [
                    "AppController.badRequest",
                    "AppController.attempts"
                ]
            },
            "possible_fix": "Modify the attempts method to validate the taskType and attemptState parameters before invoking badRequest. For example, add checks to ensure these parameters are not null or empty before proceeding."
        },
        "possible_fix_code": {
            "AppController.attempts": "  public void attempts() {\n    try {\n      requireJob();\n    }\n    catch (Exception e) {\n      renderText(e.getMessage());\n      return;\n    }\n    if (app.getJob() != null) {\n      try {\n        String taskType = $(TASK_TYPE);\n        if (taskType == null || taskType.isEmpty()) {\n          throw new RuntimeException(\"missing task-type.\");\n        }\n        String attemptState = $(ATTEMPT_STATE);\n        if (attemptState == null || attemptState.isEmpty()) {\n          throw new RuntimeException(\"missing attempt-state.\");\n        }\n        setTitle(join(attemptState, \" \",\n            MRApps.taskType(taskType).toString(), \" attempts in \", $(JOB_ID)));\n\n        render(attemptsPage());\n      } catch (Exception e) {\n        badRequest(e.getMessage() != null ? e.getMessage() : \"Unknown error occurred.\");\n      }\n    }\n  }"
        }
    },
    {
        "filename": "MAPREDUCE-5198.json",
        "creation_time": "2013-04-30T22:22:33.000+0000",
        "bug_report": {
            "Title": "Race condition in cleanup during task tracker reinit with LinuxTaskController",
            "Description": "The issue arises when the job tracker is restarted while jobs are still running, prompting the task tracker to reinitialize. During this process, the task tracker encounters a `ClosedChannelException` while attempting to write to a channel that has already been closed, indicating a potential race condition in network communication. Additionally, an `ExitCodeException` is thrown when a command executed by the Hadoop framework fails, suggesting that the cleanup process is not handling errors correctly during reinitialization.",
            "StackTrace": [
                "2013-04-27 20:19:09,627 INFO org.apache.hadoop.mapred.TaskTracker: Good mapred local directories are: /grid/0/hdp/mapred/local,/grid/1/hdp/mapred/local,/grid/2/hdp/mapred/local,/grid/3/hdp/mapred/local,/grid/4/hdp/mapred/local,/grid/5/hdp/mapred/local",
                "2013-04-27 20:19:09,628 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 42075 caught: java.nio.channels.ClosedChannelException",
                "at sun.nio.ch.SocketChannelImpl.ensureWriteOpen(SocketChannelImpl.java:133)",
                "at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:324)",
                "at org.apache.hadoop.ipc.Server.channelWrite(Server.java:1717)",
                "at org.apache.hadoop.ipc.Server.access$2000(Server.java:98)",
                "at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:744)",
                "at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:808)",
                "at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1433)",
                "2013-04-27 20:19:10,414 ERROR org.apache.hadoop.mapred.TaskTracker: Got fatal exception while reinitializing TaskTracker: org.apache.hadoop.util.Shell$ExitCodeException",
                "at org.apache.hadoop.util.Shell.runCommand(Shell.java:255)",
                "at org.apache.hadoop.util.Shell.run(Shell.java:182)",
                "at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:375)",
                "at org.apache.hadoop.mapred.LinuxTaskController.deleteAsUser(LinuxTaskController.java:281)",
                "at org.apache.hadoop.mapred.TaskTracker.deleteUserDirectories(TaskTracker.java:779)",
                "at org.apache.hadoop.mapred.TaskTracker.initialize(TaskTracker.java:816)",
                "at org.apache.hadoop.mapred.TaskTracker.run(TaskTracker.java:2704)",
                "at org.apache.hadoop.mapred.TaskTracker.main(TaskTracker.java:3934)"
            ],
            "RootCause": "The root cause of the issue is a race condition that occurs when the task tracker attempts to write to a closed channel, resulting in a `ClosedChannelException`. This is compounded by an `ExitCodeException` indicating that a command executed during the cleanup process failed, likely due to improper error handling.",
            "StepsToReproduce": [
                "1. Start a job on the Hadoop cluster.",
                "2. While the job is running, restart the job tracker.",
                "3. Observe the task tracker logs for exceptions."
            ],
            "ExpectedBehavior": "The task tracker should handle the reinitialization process gracefully without throwing exceptions, ensuring that all resources are cleaned up properly and that any commands executed during cleanup succeed.",
            "ObservedBehavior": "The task tracker fails to reinitialize, throwing a `ClosedChannelException` and an `ExitCodeException`, indicating that the cleanup process encountered errors.",
            "Suggestions": "Review the error handling in the `channelWrite` method of the `Server` class and the `runCommand` method of the `Shell` class. Implement checks to ensure that the channel is open before writing and handle potential command failures more gracefully.",
            "problem_location": {
                "files": [
                    "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Server.java",
                    "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/Shell.java"
                ],
                "classes": [
                    "org.apache.hadoop.ipc.Server",
                    "org.apache.hadoop.util.Shell"
                ],
                "methods": [
                    "Server.channelWrite",
                    "Shell.runCommand"
                ]
            },
            "possible_fix": "In the `channelWrite` method, add a check to ensure the channel is open before attempting to write. In the `runCommand` method, improve error handling to capture and log specific command failures, and ensure that the cleanup process can handle such failures without crashing."
        },
        "possible_fix_code": {
            "Server.channelWrite": "  private int channelWrite(WritableByteChannel channel, \n                           ByteBuffer buffer) throws IOException {\n    if (!channel.isOpen()) {\n        throw new ClosedChannelException();\n    }\n    int count =  (buffer.remaining() <= NIO_BUFFER_LIMIT) ?\n                 channel.write(buffer) : channelIO(null, channel, buffer);\n    if (count > 0) {\n      rpcMetrics.incrSentBytes(count);\n    }\n    return count;\n  }",
            "Shell.runCommand": "  private void runCommand() throws IOException { \n    ProcessBuilder builder = new ProcessBuilder(getExecString());\n    Timer timeOutTimer = null;\n    ShellTimeoutTimerTask timeoutTimerTask = null;\n    timedOut = new AtomicBoolean(false);\n    completed = new AtomicBoolean(false);\n    \n    if (environment != null) {\n      builder.environment().putAll(this.environment);\n    }\n    if (dir != null) {\n      builder.directory(this.dir);\n    }\n    \n    if (Shell.WINDOWS) {\n      synchronized (WindowsProcessLaunchLock) {\n        process = builder.start();\n      }\n    } else {\n      process = builder.start();\n    }\n\n    if (timeOutInterval > 0) {\n      timeOutTimer = new Timer(\"Shell command timeout\");\n      timeoutTimerTask = new ShellTimeoutTimerTask(\n          this);\n      timeOutTimer.schedule(timeoutTimerTask, timeOutInterval);\n    }\n    final BufferedReader errReader = \n            new BufferedReader(new InputStreamReader(process\n                                                     .getErrorStream()));\n    BufferedReader inReader = \n            new BufferedReader(new InputStreamReader(process\n                                                     .getInputStream()));\n    final StringBuffer errMsg = new StringBuffer();\n    \n    Thread errThread = new Thread() {\n      @Override\n      public void run() {\n        try {\n          String line = errReader.readLine();\n          while((line != null) && !isInterrupted()) {\n            errMsg.append(line);\n            errMsg.append(System.getProperty(\"line.separator\"));\n            line = errReader.readLine();\n          }\n        } catch(IOException ioe) {\n          LOG.warn(\"Error reading the error stream\", ioe);\n        }\n      }\n    };\n    try {\n      errThread.start();\n    } catch (IllegalStateException ise) { }\n    try {\n      parseExecResult(inReader);\n      String line = inReader.readLine();\n      while(line != null) { \n        line = inReader.readLine();\n      }\n      exitCode  = process.waitFor();\n      try {\n        errThread.join();\n      } catch (InterruptedException ie) {\n        LOG.warn(\"Interrupted while reading the error stream\", ie);\n      }\n      completed.set(true);\n      if (exitCode != 0) {\n        LOG.error(\"Command failed with exit code: \" + exitCode + \" and error message: \" + errMsg.toString());\n        throw new ExitCodeException(exitCode, errMsg.toString());\n      }\n    } catch (InterruptedException ie) {\n      throw new IOException(ie.toString());\n    } finally {\n      if (timeOutTimer != null) {\n        timeOutTimer.cancel();\n      }\n      try {\n        inReader.close();\n      } catch (IOException ioe) {\n        LOG.warn(\"Error while closing the input stream\", ioe);\n      }\n      if (!completed.get()) {\n        errThread.interrupt();\n      }\n      try {\n        errReader.close();\n      } catch (IOException ioe) {\n        LOG.warn(\"Error while closing the error stream\", ioe);\n      }\n      process.destroy();\n      lastTime = Time.now();\n    }\n  }"
        }
    },
    {
        "filename": "MAPREDUCE-7028.json",
        "creation_time": "2017-12-20T15:33:20.000+0000",
        "bug_report": {
            "Title": "Concurrent task progress updates causing NPE in Application Master",
            "Description": "The Application Master encounters a NullPointerException (NPE) during concurrent task progress updates in Hadoop MapReduce jobs. The issue arises when multiple updates to the progress of a task attempt occur simultaneously, leading to a failure in the state transition logic of the task attempt. The stack trace indicates that the NPE is thrown in the `TaskAttemptImpl$StatusUpdater.transition` method, which is responsible for managing the status of task attempts. This problem has been observed during large word count runs and can be reproduced by artificially increasing the frequency of task updates.",
            "StackTrace": [
                "2017-12-20 06:49:42,369 INFO [IPC Server handler 9 on 39501] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1513780867907_0001_m_000002_0 is : 0.02677883",
                "2017-12-20 06:49:42,369 INFO [IPC Server handler 13 on 39501] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1513780867907_0001_m_000002_0 is : 0.02677883",
                "2017-12-20 06:49:42,383 FATAL [AsyncDispatcher event handler] org.apache.hadoop.yarn.event.AsyncDispatcher: Error in dispatcher thread",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$StatusUpdater.transition(TaskAttemptImpl.java:2450)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$StatusUpdater.transition(TaskAttemptImpl.java:2433)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:362)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$500(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:487)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:1362)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:154)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:1543)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:1535)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:197)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:126)",
                "at java.lang.Thread.run(Thread.java:748)",
                "2017-12-20 06:49:42,385 INFO [IPC Server handler 13 on 39501] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Progress of TaskAttempt attempt_1513780867907_0001_m_000002_0 is : 0.02677883",
                "2017-12-20 06:49:42,386 INFO [AsyncDispatcher ShutDown handler] org.apache.hadoop.yarn.event.AsyncDispatcher: Exiting, bbye.."
            ],
            "RootCause": "The NullPointerException is likely caused by uninitialized variables or incorrect object references during the state transition of a task attempt in the Hadoop MapReduce framework. This can occur when multiple threads attempt to update the task progress concurrently, leading to race conditions and improper state management.",
            "StepsToReproduce": [
                "Run a large word count job in Hadoop MapReduce.",
                "Increase the frequency of task progress updates artificially.",
                "Observe the logs for NullPointerException in the Application Master."
            ],
            "ExpectedBehavior": "The Application Master should handle concurrent task progress updates without throwing exceptions, ensuring that task statuses are updated correctly and consistently.",
            "ObservedBehavior": "The Application Master throws a NullPointerException when concurrent updates to task progress occur, leading to a failure in processing task attempts.",
            "Suggestions": "Review the state transition logic in the TaskAttemptImpl class to ensure that all necessary objects are properly initialized before use. Implement proper synchronization mechanisms to handle concurrent updates safely.",
            "problem_location": {
                "files": [
                    "TaskAttemptImpl.java",
                    "StateMachineFactory.java"
                ],
                "classes": [
                    "org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl",
                    "org.apache.hadoop.yarn.state.StateMachineFactory"
                ],
                "methods": [
                    "TaskAttemptImpl$StatusUpdater.transition"
                ]
            },
            "possible_fix": "Ensure that all variables used in the transition method are initialized before being accessed. Consider adding synchronization to the method to prevent concurrent access issues."
        },
        "possible_fix_code": {
            "org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$StatusUpdater.transition": "    public void transition(TaskAttemptImpl taskAttempt, \n        TaskAttemptEvent event) {\n      if (taskAttempt == null || event == null) {\n          LOG.error(\"TaskAttempt or event is null, cannot proceed with transition.\");\n          return;\n      }\n      TaskAttemptDiagnosticsUpdateEvent diagEvent =\n          (TaskAttemptDiagnosticsUpdateEvent) event;\n      if (diagEvent == null) {\n          LOG.error(\"Diagnostic event is null, cannot proceed with transition.\");\n          return;\n      }\n      LOG.info(\"Diagnostics report from \" + taskAttempt.attemptId + \": \"\n          + diagEvent.getDiagnosticInfo());\n      taskAttempt.addDiagnosticInfo(diagEvent.getDiagnosticInfo());\n    }"
        }
    },
    {
        "filename": "MAPREDUCE-3790.json",
        "creation_time": "2012-02-02T18:04:23.000+0000",
        "bug_report": {
            "Title": "Broken pipe on streaming job can lead to truncated output for a successful job",
            "Description": "The issue arises when a streaming job does not consume all of its input, leading to a situation where the job is marked as successful despite producing truncated output. This behavior is particularly evident when the job encounters a 'Broken pipe' error, which indicates that one end of the communication channel is closed while the other is still attempting to write data. The logs show multiple IOExceptions, including 'Bad file descriptor' and 'DFSOutputStream is closed', suggesting that the output streams may be prematurely terminated. The current implementation in PipeMapRed.mapRedFinished() does not handle these exceptions properly, allowing the job to complete without ensuring all output has been processed.",
            "StackTrace": [
                "2012-02-02 11:27:25,054 WARN [main] org.apache.hadoop.streaming.PipeMapRed: java.io.IOException: Broken pipe",
                "2012-02-02 11:27:25,056 WARN [Thread-12] org.apache.hadoop.streaming.PipeMapRed: java.io.IOException: Bad file descriptor",
                "2012-02-02 11:27:25,127 WARN [Thread-11] org.apache.hadoop.streaming.PipeMapRed: java.io.IOException: DFSOutputStream is closed"
            ],
            "RootCause": "The root cause of the issue is that the PipeMapRed.mapRedFinished() method does not properly handle IOExceptions, allowing the job to finish without waiting for output threads to complete, which can lead to truncated output.",
            "StepsToReproduce": [
                "Create a streaming job with a non-zero input file that is not fully consumed.",
                "Run the job using the Hadoop streaming command.",
                "Check the output to see if it is truncated despite the job being marked successful."
            ],
            "ExpectedBehavior": "The job should either consume all input and produce complete output or fail if it does not consume all input, ensuring consistency in job success criteria.",
            "ObservedBehavior": "The job is marked successful even when it does not consume all input, resulting in truncated output and multiple IOExceptions in the logs.",
            "Suggestions": "Modify the PipeMapRed.mapRedFinished() method to properly handle IOExceptions by waiting for output threads to complete before marking the job as finished. Additionally, consider implementing a consistent policy for job success based on input consumption.",
            "problem_location": {
                "files": [
                    "PipeMapRed.java"
                ],
                "classes": [
                    "org.apache.hadoop.streaming.PipeMapRed"
                ],
                "methods": [
                    "PipeMapRed.mapRedFinished"
                ]
            },
            "possible_fix": "In PipeMapRed.java, update the mapRedFinished() method to include checks for active output threads and ensure that the job only completes successfully if all output has been processed. This may involve adding synchronization mechanisms or flags to track the state of output processing."
        },
        "possible_fix_code": {
            "PipeMapRed.mapRedFinished": "  public void mapRedFinished() throws IOException {\n    // Check if there are any active output threads\n    if (outerrThreadsThrowable != null) {\n      // If there was an error in the output/error threads, handle it\n      throw new IOException(\"MROutput/MRErrThread failed:\", outerrThreadsThrowable);\n    }\n    // Wait for output threads to complete before marking the job as finished\n    try {\n      // Assuming outputThreads is a list of threads handling output\n      for (Thread outputThread : outputThreads) {\n        if (outputThread.isAlive()) {\n          outputThread.join(); // Wait for the thread to finish\n        }\n      }\n    } catch (InterruptedException e) {\n      Thread.currentThread().interrupt(); // Restore interrupted status\n      throw new IOException(\"Interrupted while waiting for output threads to finish\", e);\n    }\n    // Mark the job as finished only after ensuring all output has been processed\n    // Additional logic to finalize job status can be added here\n  }"
        }
    },
    {
        "filename": "MAPREDUCE-6357.json",
        "creation_time": "2015-05-05T18:11:38.000+0000",
        "bug_report": {
            "Title": "MultipleOutputs.write() API should document that output committing is not utilized when input path is absolute",
            "Description": "The issue arises when using the MultipleOutputs.write() method with an absolute path as the baseOutputPath. In such cases, the output committing mechanism is not engaged, leading to potential failures during task retries or speculative execution in MapReduce jobs. This was highlighted in a user job that encountered an IOException due to an existing file at the specified path in Azure Blob Storage. The warning indicates that the system attempted to create a file that already exists, which is a common issue when the overwrite option is not specified. The documentation should clarify that absolute paths may cause improper execution of tasks on retry or when speculative execution is enabled.",
            "StackTrace": [
                "2015-04-28 23:13:10,452 WARN [main] org.apache.hadoop.mapred.YarnChild: Exception running child : java.io.IOException: File already exists: wasb://full20150320@bgtstoragefull.blob.core.windows.net/user/hadoop/some/path/block-r-00299.bz2",
                "at org.apache.hadoop.fs.azure.NativeAzureFileSystem.create(NativeAzureFileSystem.java:1354)",
                "at org.apache.hadoop.fs.azure.NativeAzureFileSystem.create(NativeAzureFileSystem.java:1195)",
                "at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:908)",
                "at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:889)",
                "at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:786)",
                "at org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:135)",
                "at org.apache.hadoop.mapreduce.lib.output.MultipleOutputs.getRecordWriter(MultipleOutputs.java:475)",
                "at org.apache.hadoop.mapreduce.lib.output.MultipleOutputs.write(MultipleOutputs.java:433)",
                "at com.ancestry.bigtree.hadoop.LevelReducer.processValue(LevelReducer.java:91)",
                "at com.ancestry.bigtree.hadoop.LevelReducer.reduce(LevelReducer.java:69)",
                "at com.ancestry.bigtree.hadoop.LevelReducer.reduce(LevelReducer.java:14)",
                "at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:171)",
                "at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:627)",
                "at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:389)",
                "at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)"
            ],
            "RootCause": "The root cause of the issue is that the MultipleOutputs.write() method does not utilize output committing when the baseOutputPath is an absolute path. This leads to an IOException when attempting to create a file that already exists, as the overwrite option is not specified.",
            "StepsToReproduce": [
                "1. Set up a MapReduce job that uses MultipleOutputs.write() with an absolute path as the baseOutputPath.",
                "2. Run the job and observe the behavior during task retries or speculative execution.",
                "3. Check the logs for any IOException related to file creation."
            ],
            "ExpectedBehavior": "When using MultipleOutputs.write() with an absolute path, the system should either overwrite existing files or handle the situation gracefully without throwing an IOException.",
            "ObservedBehavior": "The job fails with an IOException indicating that a file already exists at the specified path, preventing successful execution of reduce tasks on retry.",
            "Suggestions": "To resolve this issue, ensure that the create method in the FileSystem class is called with the OVERWRITE flag included in the EnumSet<CreateFlag>. This will allow existing files to be truncated and overwritten without causing an exception.",
            "problem_location": {
                "files": [
                    "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FileSystem.java",
                    "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/azure/NativeAzureFileSystem.java",
                    "hadoop-mapreduce/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/output/MultipleOutputs.java"
                ],
                "classes": [
                    "org.apache.hadoop.fs.FileSystem",
                    "org.apache.hadoop.fs.azure.NativeAzureFileSystem",
                    "org.apache.hadoop.mapreduce.lib.output.MultipleOutputs"
                ],
                "methods": [
                    "FileSystem.create",
                    "NativeAzureFileSystem.create",
                    "MultipleOutputs.write"
                ]
            },
            "possible_fix": "Modify the call to the create method in the FileSystem class to include the OVERWRITE flag in the EnumSet<CreateFlag>. Example: EnumSet<CreateFlag> flags = EnumSet.of(CreateFlag.CREATE, CreateFlag.OVERWRITE);"
        },
        "possible_fix_code": {
            "org.apache.hadoop.mapreduce.lib.output.MultipleOutputs.write": "public void write(String baseOutputPath, String key, String value) throws IOException {\n    // Create an EnumSet with the OVERWRITE flag to allow overwriting existing files\n    EnumSet<CreateFlag> flags = EnumSet.of(CreateFlag.CREATE, CreateFlag.OVERWRITE);\n\n    // Assuming we have a FileSystem instance to work with\n    FileSystem fs = FileSystem.get(new Configuration());\n\n    // Create the output path\n    Path outputPath = new Path(baseOutputPath);\n\n    // Use the create method with the flags that include OVERWRITE\n    FSDataOutputStream out = fs.create(outputPath, flags, 4096, (short) 1, 64 * 1024, null);\n\n    // Write the key-value pair to the output stream\n    out.writeBytes(key + \"\\t\" + value + \"\\n\");\n\n    // Close the output stream\n    out.close();\n}"
        }
    },
    {
        "filename": "MAPREDUCE-6091.json",
        "creation_time": "2014-09-16T01:19:58.000+0000",
        "bug_report": {
            "Title": "YARNRunner.getJobStatus() fails with ApplicationNotFoundException if the job rolled off the RM view",
            "Description": "The issue arises when querying the status of a job that has rolled off the Resource Manager (RM) view using the method YARNRunner.getJobStatus(). This results in an ApplicationNotFoundException, indicating that the application with the specified ID no longer exists in the RM. The error message highlights that the application ID 'application_1410289045532_90542' cannot be found, which is likely due to the job being completed and subsequently removed from the RM's records. This behavior was altered by YARN-873, which changed the ClientRMService to throw an ApplicationNotFoundException instead of returning null for unknown application IDs.",
            "StackTrace": [
                "2014-09-15 07:09:51,084 ERROR org.apache.pig.tools.grunt.Grunt: ERROR 6017: JobID: job_1410289045532_90542 Reason: java.io.IOException: org.apache.hadoop.yarn.exceptions.ApplicationNotFoundException: Application with id 'application_1410289045532_90542' doesn't exist in RM.",
                "at org.apache.hadoop.yarn.server.resourcemanager.ClientRMService.getApplicationReport(ClientRMService.java:288)",
                "at org.apache.hadoop.yarn.api.impl.pb.service.ApplicationClientProtocolPBServiceImpl.getApplicationReport(ApplicationClientProtocolPBServiceImpl.java:150)",
                "at org.apache.hadoop.yarn.proto.ApplicationClientProtocol$ApplicationClientProtocolService$2.callBlockingMethod(ApplicationClientProtocol.java:337)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:587)",
                "at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2058)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2054)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1547)",
                "at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2052)",
                "at org.apache.hadoop.mapred.ClientServiceDelegate.invoke(ClientServiceDelegate.java:348)",
                "at org.apache.hadoop.mapred.ClientServiceDelegate.getJobStatus(ClientServiceDelegate.java:419)",
                "at org.apache.hadoop.mapred.YARNRunner.getJobStatus(YARNRunner.java:559)",
                "at org.apache.hadoop.mapreduce.Job$1.run(Job.java:314)",
                "at org.apache.hadoop.mapreduce.Job$1.run(Job.java:311)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:396)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1547)",
                "at org.apache.hadoop.mapreduce.Job.updateStatus(Job.java:311)",
                "at org.apache.hadoop.mapreduce.Job.isComplete(Job.java:599)",
                "at org.apache.hadoop.mapreduce.lib.jobcontrol.ControlledJob.checkRunningState(ControlledJob.java:257)",
                "at org.apache.hadoop.mapreduce.lib.jobcontrol.ControlledJob.checkState(ControlledJob.java:282)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "at java.lang.reflect.Method.invoke(Method.java:597)",
                "at org.apache.pig.backend.hadoop23.PigJobControl.checkState(PigJobControl.java:120)",
                "at org.apache.pig.backend.hadoop23.PigJobControl.run(PigJobControl.java:180)",
                "at java.lang.Thread.run(Thread.java:662)",
                "at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher$1.run(MapReduceLauncher.java:279)"
            ],
            "RootCause": "The ApplicationNotFoundException occurs because the job associated with the application ID has either completed and been removed from the Resource Manager or an incorrect application ID is being referenced. This change in behavior was introduced by YARN-873, which modified how unknown application IDs are handled.",
            "StepsToReproduce": [
                "Submit a job to YARN using the YARNRunner.",
                "Wait for the job to complete.",
                "Attempt to query the job status using YARNRunner.getJobStatus() after the job has rolled off the RM view."
            ],
            "ExpectedBehavior": "The system should be able to retrieve the job status from the job history server even if the job has completed and rolled off the RM view.",
            "ObservedBehavior": "An ApplicationNotFoundException is thrown, indicating that the application ID does not exist in the Resource Manager.",
            "Suggestions": "Consider implementing a fallback mechanism to query the job history server for job status when the application ID is not found in the Resource Manager. Additionally, review the job submission process to ensure that valid application IDs are being used.",
            "problem_location": {
                "files": [
                    "org.apache.hadoop.yarn.server.resourcemanager.ClientRMService",
                    "org.apache.hadoop.yarn.api.impl.pb.service.ApplicationClientProtocolPBServiceImpl",
                    "org.apache.hadoop.mapreduce.Job",
                    "org.apache.hadoop.mapreduce.lib.jobcontrol.ControlledJob"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.ClientRMService",
                    "org.apache.hadoop.yarn.api.impl.pb.service.ApplicationClientProtocolPBServiceImpl",
                    "org.apache.hadoop.mapreduce.Job",
                    "org.apache.hadoop.mapreduce.lib.jobcontrol.ControlledJob"
                ],
                "methods": [
                    "ClientRMService.getApplicationReport",
                    "ApplicationClientProtocolPBServiceImpl.getApplicationReport",
                    "Job.updateStatus",
                    "ControlledJob.checkRunningState",
                    "Job.getJobID",
                    "Job.submit"
                ]
            },
            "possible_fix": "Implement a check in the YARNRunner.getJobStatus() method to query the job history server if the application ID is not found in the Resource Manager. This could involve modifying the ClientRMService to include a fallback mechanism."
        },
        "possible_fix_code": {
            "YARNRunner.getJobStatus": "public JobStatus getJobStatus(String applicationId) throws IOException {\n    try {\n        // Attempt to get the application report from the Resource Manager\n        ApplicationReport report = clientRMService.getApplicationReport(applicationId);\n        return convertToJobStatus(report);\n    } catch (ApplicationNotFoundException e) {\n        // If the application is not found, attempt to query the job history server\n        return queryJobHistoryServer(applicationId);\n    }\n}\n\nprivate JobStatus queryJobHistoryServer(String applicationId) throws IOException {\n    // Logic to query the job history server for the job status\n    // This is a placeholder for the actual implementation\n    // You would typically use a JobHistoryClient to fetch the job status\n    JobHistoryClient jobHistoryClient = new JobHistoryClient();\n    JobHistoryInfo jobHistoryInfo = jobHistoryClient.getJobHistory(applicationId);\n    if (jobHistoryInfo != null) {\n        return convertToJobStatus(jobHistoryInfo);\n    } else {\n        throw new IOException(\"Job history not found for application ID: \" + applicationId);\n    }\n}\n\nprivate JobStatus convertToJobStatus(ApplicationReport report) {\n    // Convert ApplicationReport to JobStatus\n    // Implementation details would depend on the JobStatus class structure\n    return new JobStatus(report.getApplicationId(), report.getYarnApplicationState());\n}\n"
        }
    },
    {
        "filename": "MAPREDUCE-4825.json",
        "creation_time": "2012-11-27T19:01:45.000+0000",
        "bug_report": {
            "Title": "JobImpl.finished doesn't expect ERROR as a final job state",
            "Description": "The issue arises in the Hadoop MapReduce application when the `JobImpl.finished` method is called with an invalid job state of 'ERROR'. This state is not accounted for in the method's switch-case structure, leading to an `IllegalArgumentException`. The error is triggered during the execution of the `testJobError` test case, which causes the `AsyncDispatcher` to exit unexpectedly due to this unhandled state. The stack trace indicates that the error occurs when the job transitions into an unexpected state, which is not properly managed by the job's state machine.",
            "StackTrace": [
                "2012-11-27 18:46:15,240 ERROR [AsyncDispatcher event handler] impl.TaskImpl (TaskImpl.java:internalError(665)) - Invalid event T_SCHEDULE on Task task_0_0000_m_000000",
                "2012-11-27 18:46:15,242 FATAL [AsyncDispatcher event handler] event.AsyncDispatcher (AsyncDispatcher.java:dispatch(132)) - Error in dispatcher thread",
                "java.lang.IllegalArgumentException: Illegal job state: ERROR",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.finished(JobImpl.java:838)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$InternalErrorTransition.transition(JobImpl.java:1622)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$InternalErrorTransition.transition(JobImpl.java:1)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:359)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:299)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$3(StateMachineFactory.java:287)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:445)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:723)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:1)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:974)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:1)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:128)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:77)",
                "at java.lang.Thread.run(Thread.java:662)",
                "2012-11-27 18:46:15,242 INFO  [AsyncDispatcher event handler] event.AsyncDispatcher (AsyncDispatcher.java:dispatch(135)) - Exiting, bbye.."
            ],
            "RootCause": "The root cause of the issue is that the `JobImpl.finished` method does not handle the 'ERROR' state, which leads to an `IllegalArgumentException` being thrown when this state is encountered. This indicates that the job has transitioned into an error state that was not properly managed, likely due to internal errors during job execution.",
            "StepsToReproduce": [
                "Run the test case `TestMRApp.testJobError`.",
                "Observe the console output for errors related to job state transitions.",
                "Check the stack trace for the `IllegalArgumentException` thrown in the `JobImpl.finished` method."
            ],
            "ExpectedBehavior": "The job should handle all possible final states, including ERROR, without throwing an exception. The system should log the error and transition to a safe state or provide a meaningful error message.",
            "ObservedBehavior": "The system throws an `IllegalArgumentException` with the message 'Illegal job state: ERROR', causing the `AsyncDispatcher` to exit unexpectedly.",
            "Suggestions": "Implement handling for the 'ERROR' state in the `JobImpl.finished` method. Consider logging the error and transitioning to a safe state instead of throwing an exception.",
            "problem_location": {
                "files": [
                    "JobImpl.java"
                ],
                "classes": [
                    "org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl"
                ],
                "methods": [
                    "JobImpl.finished",
                    "JobImpl$InternalErrorTransition.transition"
                ]
            },
            "possible_fix": "Modify the `JobImpl.finished` method to include a case for handling the 'ERROR' state. For example:\n\n```java\ncase ERROR:\n    // Handle the error state appropriately\n    log.error(\"Job encountered an error state: \" + finalState);\n    return finalState;\n```"
        },
        "possible_fix_code": {
            "JobImpl.finished": "  JobStateInternal finished(JobStateInternal finalState) {\n    if (getInternalState() == JobStateInternal.RUNNING) {\n      metrics.endRunningJob(this);\n    }\n    if (finishTime == 0) setFinishTime();\n    eventHandler.handle(new JobFinishEvent(jobId));\n\n    switch (finalState) {\n      case KILLED:\n        metrics.killedJob(this);\n        break;\n      case FAILED:\n        metrics.failedJob(this);\n        break;\n      case SUCCEEDED:\n        metrics.completedJob(this);\n        break;\n      case ERROR:\n        // Handle the error state appropriately\n        log.error(\"Job encountered an error state: \" + finalState);\n        return finalState;\n      default:\n        throw new IllegalArgumentException(\"Illegal job state: \" + finalState);\n    }\n    return finalState;\n  }"
        }
    },
    {
        "filename": "MAPREDUCE-3319.json",
        "creation_time": "2011-10-31T21:41:36.000+0000",
        "bug_report": {
            "Title": "ClassCastException in multifilewc due to type mismatch in LongSumReducer",
            "Description": "The multifilewc example in Hadoop version 0.20.205.0 fails with a ClassCastException when processing input files. The error occurs during the reduction phase, where the LongSumReducer attempts to cast an IntWritable to a LongWritable, leading to a type mismatch. This issue arises from the mapper outputting IntWritable values instead of the expected LongWritable values, which the LongSumReducer requires for its operation.",
            "StackTrace": [
                "java.lang.ClassCastException: org.apache.hadoop.io.IntWritable cannot be cast to org.apache.hadoop.io.LongWritable",
                "at org.apache.hadoop.mapred.lib.LongSumReducer.reduce(LongSumReducer.java:44)",
                "at org.apache.hadoop.mapred.Task$OldCombinerRunner.combine(Task.java:1431)",
                "at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1436)",
                "at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.flush(MapTask.java:1298)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:437)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:372)",
                "at org.apache.hadoop.mapred.Child$4.run(Child.java:255)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:396)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1059)",
                "at org.apache.hadoop.mapred.Child.main(Child.java:249)"
            ],
            "RootCause": "The root cause of the issue is a ClassCastException occurring in the LongSumReducer, which expects LongWritable values but is receiving IntWritable values from the mapper output.",
            "StepsToReproduce": [
                "Run the command: /usr/lib/hadoop/bin/hadoop jar /usr/lib/hadoop/hadoop-examples-0.20.205.0.22.jar multifilewc examples/text examples-output/multifilewc",
                "Monitor the job execution and observe the logs for the ClassCastException."
            ],
            "ExpectedBehavior": "The LongSumReducer should successfully sum all LongWritable values associated with a key and output the correct results without any exceptions.",
            "ObservedBehavior": "The job fails with a ClassCastException, indicating that an IntWritable cannot be cast to a LongWritable, preventing the reducer from completing its task.",
            "Suggestions": "Ensure that the mapper outputs LongWritable values instead of IntWritable values when using LongSumReducer. This may involve modifying the mapper logic to produce the correct data type.",
            "problem_location": {
                "files": [
                    "LongSumReducer.java"
                ],
                "classes": [
                    "org.apache.hadoop.mapred.lib.LongSumReducer"
                ],
                "methods": [
                    "LongSumReducer.reduce"
                ]
            },
            "possible_fix": "Modify the mapper to output LongWritable values instead of IntWritable. For example, if the mapper currently uses IntWritable, change it to LongWritable to ensure compatibility with LongSumReducer."
        },
        "possible_fix_code": {
            "reduce": "  public void reduce(K key, Iterator<LongWritable> values,\n                     OutputCollector<K, LongWritable> output,\n                     Reporter reporter)\n    throws IOException {\n\n    // sum all values for this key\n    long sum = 0;\n    while (values.hasNext()) {\n      sum += values.next().get();\n    }\n\n    // output sum\n    output.collect(key, new LongWritable(sum));\n  }"
        }
    },
    {
        "filename": "MAPREDUCE-6361.json",
        "creation_time": "2015-05-11T15:36:10.000+0000",
        "bug_report": {
            "Title": "NPE issue in shuffle caused by concurrent issue between copySucceeded() in one thread and copyFailed() in another thread on the same host",
            "Description": "The bug report highlights a NullPointerException (NPE) occurring during the shuffle phase of a Hadoop MapReduce job, specifically within the `ShuffleSchedulerImpl.copyFailed` method. This issue arises when one thread is executing `copySucceeded()` while another thread is executing `copyFailed()` on the same host, leading to a race condition. The stack trace indicates that the error occurs in the `Shuffle.run()` method, which is part of the reduce task execution. The NPE is likely triggered by uninitialized or improperly handled connection objects during the data fetching process.",
            "StackTrace": [
                "2015-05-08 21:00:00,513 WARN [main] org.apache.hadoop.mapred.YarnChild: Exception running child : org.apache.hadoop.mapreduce.task.reduce.Shuffle$ShuffleError: error in shuffle in fetcher#25",
                "at org.apache.hadoop.mapreduce.task.reduce.Shuffle.run(Shuffle.java:134)",
                "at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:376)",
                "at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)",
                "Caused by: java.lang.NullPointerException",
                "at org.apache.hadoop.mapreduce.task.reduce.ShuffleSchedulerImpl.copyFailed(ShuffleSchedulerImpl.java:267)",
                "at org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyFromHost(Fetcher.java:308)",
                "at org.apache.hadoop.mapreduce.task.reduce.Fetcher.run(Fetcher.java:193)"
            ],
            "RootCause": "The root cause of the NullPointerException is likely due to the `connection` object in the `Fetcher` class not being properly initialized. This can occur if the URL is invalid or if there are issues with SSL configuration in the `openConnection` method.",
            "StepsToReproduce": [
                "Run a Hadoop MapReduce job that involves a shuffle phase.",
                "Simultaneously trigger `copySucceeded()` and `copyFailed()` methods on the same host.",
                "Monitor the logs for any NullPointerExceptions during the shuffle phase."
            ],
            "ExpectedBehavior": "The system should successfully fetch map outputs from the specified host without encountering any NullPointerExceptions, allowing the reduce task to complete successfully.",
            "ObservedBehavior": "The system throws a NullPointerException during the shuffle phase, causing the reduce task to fail and preventing successful data fetching from the host.",
            "Suggestions": "Review the connection handling logic in the `Fetcher` class, particularly in the `openConnection` and `setupConnectionsWithRetry` methods. Ensure that the connection object is properly initialized before use. Consider adding null checks and error handling to prevent uninitialized states.",
            "problem_location": {
                "files": [
                    "ShuffleSchedulerImpl.java",
                    "Fetcher.java"
                ],
                "classes": [
                    "org.apache.hadoop.mapreduce.task.reduce.ShuffleSchedulerImpl",
                    "org.apache.hadoop.mapreduce.task.reduce.Fetcher"
                ],
                "methods": [
                    "ShuffleSchedulerImpl.copyFailed",
                    "Fetcher.copyFromHost",
                    "Fetcher.openShuffleUrl",
                    "Fetcher.setupConnectionsWithRetry",
                    "Fetcher.openConnection"
                ]
            },
            "possible_fix": "In the `openConnection` method, ensure that the URL is valid before attempting to open a connection. Additionally, implement error handling to manage SSL configuration issues. For example, add checks to verify that the `connection` object is not null before using it in subsequent operations."
        },
        "possible_fix_code": {
            "Fetcher.openConnection": "  protected synchronized void openConnection(URL url)\n      throws IOException {\n    if (url == null) {\n      throw new IOException(\"URL cannot be null\");\n    }\n    HttpURLConnection conn = (HttpURLConnection) url.openConnection();\n    if (sslShuffle) {\n      HttpsURLConnection httpsConn = (HttpsURLConnection) conn;\n      try {\n        httpsConn.setSSLSocketFactory(sslFactory.createSSLSocketFactory());\n      } catch (GeneralSecurityException ex) {\n        throw new IOException(ex);\n      }\n      httpsConn.setHostnameVerifier(sslFactory.getHostnameVerifier());\n    }\n    connection = conn;\n  }"
        }
    },
    {
        "filename": "MAPREDUCE-4848.json",
        "creation_time": "2012-12-05T04:01:47.000+0000",
        "bug_report": {
            "Title": "TaskAttemptContext cast error during AM recovery",
            "Description": "During the recovery of an Application Master (AM) in Hadoop's MapReduce framework, a ClassCastException occurs when attempting to recover a task. The error arises from a type mismatch where an instance of `TaskAttemptContextImpl` is incorrectly cast to `TaskAttemptContext`, leading to a failure in the recovery process. This issue was observed after a failed AM attempt, which subsequently caused the recovery service to fail as well.",
            "StackTrace": [
                "2012-12-05 02:33:36,752 FATAL [AsyncDispatcher event handler] org.apache.hadoop.yarn.event.AsyncDispatcher: Error in dispatcher thread",
                "java.lang.ClassCastException: org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl cannot be cast to org.apache.hadoop.mapred.TaskAttemptContext",
                "at org.apache.hadoop.mapred.OutputCommitter.recoverTask(OutputCommitter.java:284)",
                "at org.apache.hadoop.mapreduce.v2.app.recover.RecoveryService$InterceptingEventHandler.handle(RecoveryService.java:361)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$ContainerAssignedTransition.transition(TaskAttemptImpl.java:1211)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$ContainerAssignedTransition.transition(TaskAttemptImpl.java:1177)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:357)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:298)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:443)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:958)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:135)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:926)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:918)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:126)",
                "at org.apache.hadoop.mapreduce.v2.app.recover.RecoveryService$RecoveryDispatcher.realDispatch(RecoveryDispatcher.java:285)",
                "at org.apache.hadoop.mapreduce.v2.app.recover.RecoveryService$RecoveryDispatcher.dispatch(RecoveryDispatcher.java:281)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:75)",
                "at java.lang.Thread.run(Thread.java:619)"
            ],
            "RootCause": "The root cause of the ClassCastException is a type mismatch between the expected `TaskAttemptContext` from the `org.apache.hadoop.mapred` package and the actual instance of `TaskAttemptContextImpl` from the `org.apache.hadoop.mapreduce.task` package being passed to the `recoverTask` method.",
            "StepsToReproduce": [
                "1. Start a MapReduce job that fails during execution.",
                "2. Attempt to recover the Application Master (AM) after the failure.",
                "3. Observe the logs for a ClassCastException related to TaskAttemptContext."
            ],
            "ExpectedBehavior": "The recovery process should successfully cast the task context and recover the task without throwing a ClassCastException.",
            "ObservedBehavior": "The recovery process fails with a ClassCastException, preventing the task from being recovered and causing the AM to exit.",
            "Suggestions": "Ensure that the correct type of `TaskAttemptContext` is passed to the `recoverTask` method. This may involve modifying the instantiation of `TaskAttemptContextImpl` to ensure compatibility with the expected type.",
            "problem_location": {
                "files": [
                    "OutputCommitter.java",
                    "TaskAttemptContext.java",
                    "TaskAttemptContextImpl.java"
                ],
                "classes": [
                    "org.apache.hadoop.mapred.OutputCommitter",
                    "org.apache.hadoop.mapred.TaskAttemptContext",
                    "org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl"
                ],
                "methods": [
                    "OutputCommitter.recoverTask",
                    "TaskAttemptContextImpl.<constructor>"
                ]
            },
            "possible_fix": "Modify the `recoverTask` method in `OutputCommitter` to accept the correct type of `TaskAttemptContext` or ensure that the instance of `TaskAttemptContextImpl` is created in a context that matches the expected type."
        },
        "possible_fix_code": {
            "OutputCommitter.recoverTask": "void recoverTask(org.apache.hadoop.mapreduce.TaskAttemptContext taskContext) throws IOException {\n    if (taskContext instanceof org.apache.hadoop.mapred.TaskAttemptContext) {\n        recoverTask((org.apache.hadoop.mapred.TaskAttemptContext) taskContext);\n    } else {\n        throw new ClassCastException(\"Expected TaskAttemptContext of type org.apache.hadoop.mapred.TaskAttemptContext, but received: \" + taskContext.getClass().getName());\n    }\n}"
        }
    },
    {
        "filename": "MAPREDUCE-3226.json",
        "creation_time": "2011-10-20T06:38:04.000+0000",
        "bug_report": {
            "Title": "Few reduce tasks hanging in a gridmix-run",
            "Description": "During a gridmix run with approximately 1000 jobs, one job is experiencing a hang due to 2-3 reducers that are stuck. The issue arises after these reducers have downloaded all map outputs. The stack trace indicates that the 'EventFetcher' thread is in a TIMED_WAITING state, likely due to a sleep call, while another thread is in a WAITING state, waiting for a lock on the 'EventFetcher' object. This suggests potential synchronization issues within the Hadoop MapReduce framework.",
            "StackTrace": [
                "\"EventFetcher for fetching Map Completion Events\" daemon prio=10 tid=0xa325fc00 nid=0x1ca4 waiting on condition [0xa315c000]",
                "java.lang.Thread.State: TIMED_WAITING (sleeping)",
                "at java.lang.Thread.sleep(Native Method)",
                "at org.apache.hadoop.mapreduce.task.reduce.EventFetcher.run(EventFetcher.java:71)",
                "\"main\" prio=10 tid=0x080ed400 nid=0x1c71 in Object.wait() [0xf73a2000]",
                "java.lang.Thread.State: WAITING (on object monitor)",
                "at java.lang.Object.wait(Native Method)",
                "- waiting on <0xa94b23d8> (a org.apache.hadoop.mapreduce.task.reduce.EventFetcher)",
                "at java.lang.Thread.join(Thread.java:1143)",
                "- locked <0xa94b23d8> (a org.apache.hadoop.mapreduce.task.reduce.EventFetcher)",
                "at java.lang.Thread.join(Thread.java:1196)",
                "at org.apache.hadoop.mapreduce.task.reduce.Shuffle.run(Shuffle.java:135)",
                "at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:367)",
                "at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:147)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:396)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1135)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:142)"
            ],
            "RootCause": "The root cause of the issue appears to be related to the synchronization mechanisms in the 'EventFetcher' and 'Shuffle' classes. The 'EventFetcher' thread is in a TIMED_WAITING state due to a sleep call after attempting to fetch map completion events, while another thread is in a WAITING state, likely waiting for a lock on the 'EventFetcher' object. This indicates that there may be persistent issues with thread management or synchronization in the MapReduce job.",
            "StepsToReproduce": [
                "Run a gridmix job with approximately 1000 jobs.",
                "Monitor the reducer tasks for any hanging states.",
                "Check the stack traces of the hanging reducers."
            ],
            "ExpectedBehavior": "All reducer tasks should complete successfully without hanging, and the job should finish within the expected time frame.",
            "ObservedBehavior": "2-3 reducer tasks are hanging indefinitely after downloading all map outputs, causing the overall job to stall.",
            "Suggestions": "Investigate the synchronization logic in the 'EventFetcher' and 'Shuffle' classes. Consider increasing the timeout values or implementing better error handling to avoid persistent hangs.",
            "problem_location": {
                "files": [
                    "EventFetcher.java",
                    "Shuffle.java"
                ],
                "classes": [
                    "org.apache.hadoop.mapreduce.task.reduce.EventFetcher",
                    "org.apache.hadoop.mapreduce.task.reduce.Shuffle"
                ],
                "methods": [
                    "EventFetcher.run",
                    "Shuffle.run"
                ]
            },
            "possible_fix": "Review the 'EventFetcher.run' method to ensure that the sleep duration and retry logic are appropriate. Additionally, check the 'Shuffle.run' method for any potential deadlocks or synchronization issues that could lead to the observed hanging behavior."
        },
        "possible_fix_code": {
            "EventFetcher.run": "  public void run() {\n    int failures = 0;\n    LOG.info(reduce + \" Thread started: \" + getName());\n    \n    try {\n      while (true) {\n        try {\n          int numNewMaps = getMapCompletionEvents();\n          failures = 0;\n          if (numNewMaps > 0) {\n            LOG.info(reduce + \": \" + \"Got \" + numNewMaps + \" new map-outputs\");\n          }\n          LOG.debug(\"GetMapEventsThread about to sleep for \" + SLEEP_TIME);\n          Thread.sleep(SLEEP_TIME);\n        } catch (IOException ie) {\n          LOG.info(\"Exception in getting events\", ie);\n          // check to see whether to abort\n          if (++failures >= MAX_RETRIES) {\n            throw new IOException(\"too many failures downloading events\", ie);\n          }\n          // sleep for a bit\n          Thread.sleep(RETRY_PERIOD);\n        }\n      }\n    } catch (InterruptedException e) {\n      return;\n    } catch (Throwable t) {\n      exceptionReporter.reportException(t);\n      return;\n    }\n  }",
            "Shuffle.run": "  public RawKeyValueIterator run() throws IOException, InterruptedException {\n    // Start the map-completion events fetcher thread\n    final EventFetcher<K,V> eventFetcher = \n      new EventFetcher<K,V>(reduceId, umbilical, scheduler, this);\n    eventFetcher.start();\n    \n    // Start the map-output fetcher threads\n    final int numFetchers = jobConf.getInt(MRJobConfig.SHUFFLE_PARALLEL_COPIES, 5);\n    Fetcher<K,V>[] fetchers = new Fetcher[numFetchers];\n    for (int i=0; i < numFetchers; ++i) {\n      fetchers[i] = new Fetcher<K,V>(jobConf, reduceId, scheduler, merger, \n                                     reporter, metrics, this, \n                                     reduceTask.getJobTokenSecret());\n      fetchers[i].start();\n    }\n    \n    // Wait for shuffle to complete successfully\n    while (!scheduler.waitUntilDone(PROGRESS_FREQUENCY)) {\n      reporter.progress();\n      \n      synchronized (this) {\n        if (throwable != null) {\n          throw new ShuffleError(\"error in shuffle in \" + throwingThreadName,\n                                 throwable);\n        }\n      }\n    }\n\n    // Stop the event-fetcher thread\n    eventFetcher.interrupt();\n    try {\n      eventFetcher.join();\n    } catch(Throwable t) {\n      LOG.info(\"Failed to stop \" + eventFetcher.getName(), t);\n    }\n    \n    // Stop the map-output fetcher threads\n    for (Fetcher<K,V> fetcher : fetchers) {\n      fetcher.interrupt();\n    }\n    for (Fetcher<K,V> fetcher : fetchers) {\n      fetcher.join();\n    }\n    fetchers = null;\n    \n    // stop the scheduler\n    scheduler.close();\n\n    copyPhase.complete(); // copy is already complete\n    taskStatus.setPhase(TaskStatus.Phase.SORT);\n    reduceTask.statusUpdate(umbilical);\n\n    // Finish the on-going merges...\n    RawKeyValueIterator kvIter = null;\n    try {\n      kvIter = merger.close();\n    } catch (Throwable e) {\n      throw new ShuffleError(\"Error while doing final merge \" , e);\n    }\n\n    // Sanity check\n    synchronized (this) {\n      if (throwable != null) {\n        throw new ShuffleError(\"error in shuffle in \" + throwingThreadName,\n                               throwable);\n      }\n    }\n    \n    return kvIter;\n  }"
        }
    },
    {
        "filename": "MAPREDUCE-3070.json",
        "creation_time": "2011-09-22T14:33:58.000+0000",
        "bug_report": {
            "Title": "NM not able to register with RM after NM restart",
            "Description": "After gracefully stopping the NodeManager (NM) and subsequently restarting it, the NM fails to register with the ResourceManager (RM), resulting in a 'Duplicate registration from the node!' error. This issue arises because the `registerWithRM` method in the `NodeStatusUpdaterImpl` class is invoked multiple times without proper checks to prevent re-registration, leading to a conflict during the registration process.",
            "StackTrace": [
                "2011-09-23 01:50:46,705 FATAL nodemanager.NodeManager (NodeManager.java:main(204)) - Error starting NodeManager",
                "org.apache.hadoop.yarn.YarnException: Failed to Start org.apache.hadoop.yarn.server.nodemanager.NodeManager",
                "at org.apache.hadoop.yarn.service.CompositeService.start(CompositeService.java:78)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.start(NodeManager.java:153)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.main(NodeManager.java:202)",
                "Caused by: org.apache.avro.AvroRuntimeException: org.apache.hadoop.yarn.exceptions.impl.pb.YarnRemoteExceptionPBImpl: Duplicate registration from the node!",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.start(NodeStatusUpdaterImpl.java:141)",
                "at org.apache.hadoop.yarn.service.CompositeService.start(CompositeService.java:68)",
                "... 2 more",
                "Caused by: org.apache.hadoop.yarn.exceptions.impl.pb.YarnRemoteExceptionPBImpl: Duplicate registration from the node!",
                "at org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Invoker.invoke(ProtoOverHadoopRpcEngine.java:142)",
                "at $Proxy13.registerNodeManager(Unknown Source)",
                "at org.apache.hadoop.yarn.server.api.impl.pb.client.ResourceTrackerPBClientImpl.registerNodeManager(ResourceTrackerPBClientImpl.java:59)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.registerWithRM(NodeStatusUpdaterImpl.java:175)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.start(NodeStatusUpdaterImpl.java:137)",
                "... 3 more"
            ],
            "RootCause": "The root cause of the issue is that the `registerWithRM` method in the `NodeStatusUpdaterImpl` class is called multiple times during the startup process without any checks to determine if the NodeManager is already registered with the ResourceManager. This leads to a 'Duplicate registration from the node!' error.",
            "StepsToReproduce": [
                "1. Stop the NodeManager gracefully.",
                "2. Start the NodeManager.",
                "3. Observe the logs for registration errors."
            ],
            "ExpectedBehavior": "The NodeManager should successfully register with the ResourceManager without any errors after a restart.",
            "ObservedBehavior": "The NodeManager fails to register with the ResourceManager, resulting in a 'Duplicate registration from the node!' error.",
            "Suggestions": "Implement a boolean flag in the `NodeStatusUpdaterImpl` class to track the registration status of the NodeManager. This flag should be checked before calling the `registerWithRM` method to prevent duplicate registrations.",
            "problem_location": {
                "files": [
                    "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeStatusUpdaterImpl.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl"
                ],
                "methods": [
                    "NodeStatusUpdaterImpl.start",
                    "NodeStatusUpdaterImpl.registerWithRM"
                ]
            },
            "possible_fix": "Add a boolean flag (e.g., `isRegistered`) in the `NodeStatusUpdaterImpl` class. Set this flag to true after a successful registration in the `registerWithRM` method and check this flag before calling `registerWithRM` in the `start` method."
        },
        "possible_fix_code": {
            "NodeStatusUpdaterImpl.start": "  public void start() {\n    String cmBindAddressStr =\n        getConfig().get(YarnConfiguration.NM_ADDRESS,\n            YarnConfiguration.DEFAULT_NM_ADDRESS);\n    InetSocketAddress cmBindAddress =\n        NetUtils.createSocketAddr(cmBindAddressStr);\n    String httpBindAddressStr =\n      getConfig().get(YarnConfiguration.NM_WEBAPP_ADDRESS,\n          YarnConfiguration.DEFAULT_NM_WEBAPP_ADDRESS);\n    InetSocketAddress httpBindAddress =\n      NetUtils.createSocketAddr(httpBindAddressStr);\n    try {\n      this.hostName = InetAddress.getLocalHost().getHostAddress();\n      this.containerManagerPort = cmBindAddress.getPort();\n      this.httpPort = httpBindAddress.getPort();\n      this.containerManagerBindAddress =\n          this.hostName + \":\" + this.containerManagerPort;\n      LOG.info(\"Configured ContainerManager Address is \"\n          + this.containerManagerBindAddress);\n      // Check if already registered\n      if (!isRegistered) {\n        registerWithRM();\n      }\n      super.start();\n      startStatusUpdater();\n    } catch (Exception e) {\n      throw new AvroRuntimeException(e);\n    }\n  }",
            "NodeStatusUpdaterImpl.registerWithRM": "  private void registerWithRM() throws YarnRemoteException {\n    this.resourceTracker = getRMClient();\n    LOG.info(\"Connected to ResourceManager at \" + this.rmAddress);\n    \n    RegisterNodeManagerRequest request = recordFactory.newRecordInstance(RegisterNodeManagerRequest.class);\n    this.nodeId = Records.newRecord(NodeId.class);\n    this.nodeId.setHost(this.hostName);\n    this.nodeId.setPort(this.containerManagerPort);\n    request.setHttpPort(this.httpPort);\n    request.setResource(this.totalResource);\n    request.setNodeId(this.nodeId);\n    RegistrationResponse regResponse =\n        this.resourceTracker.registerNodeManager(request).getRegistrationResponse();\n    if (UserGroupInformation.isSecurityEnabled()) {\n      this.secretKeyBytes = regResponse.getSecretKey().array();\n    }\n\n    // do this now so that its set before we start heartbeating to RM\n    if (UserGroupInformation.isSecurityEnabled()) {\n      LOG.info(\"Security enabled - updating secret keys now\");\n      this.containerTokenSecretManager.setSecretKey(\n          this.getContainerManagerBindAddress(),\n          this.getRMNMSharedSecret());\n    }\n    LOG.info(\"Registered with ResourceManager as \" + this.containerManagerBindAddress\n        + \" with total resource of \" + this.totalResource);\n    // Set registration flag to true\n    isRegistered = true;\n  }"
        }
    },
    {
        "filename": "MAPREDUCE-3932.json",
        "creation_time": "2012-02-27T22:39:43.000+0000",
        "bug_report": {
            "Title": "MR tasks failing and crashing the AM when available-resources/headRoom becomes zero",
            "Description": "A reported issue where a reduce task is preempted due to zero headRoom, leading to a crash of the Application Master (AM). The logs indicate that the available resources drop to zero, causing invalid state transitions in the YARN framework, specifically `InvalidStateTransitonException` errors when tasks attempt to launch in an invalid state.",
            "StackTrace": [
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: TA_CONTAINER_LAUNCH_FAILED at KILL_TASK_CLEANUP",
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: JOB_COUNTER_UPDATE at ERROR"
            ],
            "RootCause": "The root cause of the issue is the invalid state transitions occurring in the YARN framework due to the available resources (headRoom) dropping to zero. This leads to tasks being preempted and subsequently failing to launch, resulting in exceptions related to invalid state transitions.",
            "StepsToReproduce": [
                "1. Start a MapReduce job with a high number of reduce tasks.",
                "2. Monitor the available resources in the YARN ResourceManager.",
                "3. Allow the available resources to drop to zero.",
                "4. Observe the behavior of the Application Master and the state transitions of the tasks."
            ],
            "ExpectedBehavior": "The Application Master should manage task assignments and resource allocations without crashing, even when resources are low. Tasks should transition through valid states without encountering invalid state transition exceptions.",
            "ObservedBehavior": "The Application Master crashes when the available resources drop to zero, leading to invalid state transitions and exceptions being thrown for task attempts.",
            "Suggestions": "Review the resource allocation logic in the RMContainerAllocator and ensure that tasks are not assigned when resources are insufficient. Consider implementing checks to prevent invalid state transitions when resources are critically low.",
            "problem_location": {
                "files": [
                    "RMContainerAllocator.java",
                    "TaskAttemptImpl.java",
                    "JobImpl.java"
                ],
                "classes": [
                    "org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator",
                    "org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl",
                    "org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl"
                ],
                "methods": [
                    "RMContainerAllocator.schedule",
                    "TaskAttemptImpl.handle",
                    "JobImpl.handle"
                ]
            },
            "possible_fix": "Implement resource checks in the RMContainerAllocator to prevent task assignments when headRoom is zero. Additionally, enhance the state management logic in TaskAttemptImpl and JobImpl to handle low resource scenarios gracefully."
        },
        "possible_fix_code": {
            "RMContainerAllocator.schedule": "public void schedule() {\n    // Check available resources before scheduling tasks\n    if (headRoom <= 0) {\n        LOG.warn(\"Insufficient resources to schedule tasks. Headroom is zero.\");\n        return; // Prevent scheduling if headRoom is zero\n    }\n    // Existing scheduling logic...\n    // Schedule tasks based on available resources\n    // ...\n}"
        }
    },
    {
        "filename": "MAPREDUCE-4062.json",
        "creation_time": "2012-03-23T21:42:23.000+0000",
        "bug_report": {
            "Title": "AM Launcher thread can hang forever",
            "Description": "An issue was identified where the Application Master (AM) launcher thread becomes unresponsive, leading to a halt in launching Application Masters. This situation arises when the Node Manager (NM) node encounters problems, causing the RM to be stuck indefinitely. The current implementation only utilizes a single launcher thread, which exacerbates the issue, as a failure in the NM can lead to prolonged downtime. This bug was observed to persist for approximately 9 hours, indicating a critical need for improvement in thread management and fault tolerance.",
            "StackTrace": [
                "\"pool-1-thread-1\" prio=10 tid=0x000000004343e800 nid=0x3a4c in Object.wait()",
                "[0x000000004fad2000]",
                "java.lang.Thread.State: WAITING (on object monitor)",
                "at java.lang.Object.wait(Native Method)",
                "at java.lang.Object.wait(Object.java:485)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1076)",
                "- locked <0x00002aab05a4f3f0> (a org.apache.hadoop.ipc.Client$Call)",
                "at org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Invoker.invoke(ProtoOverHadoopRpcEngine.java:135)",
                "at $Proxy76.startContainer(Unknown Source)",
                "at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagerPBClientImpl.startContainer(ContainerManagerPBClientImpl.java:87)",
                "at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.launch(AMLauncher.java:118)",
                "at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.run(AMLauncher.java:265)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)",
                "at java.lang.Thread.run(Thread.java:619)"
            ],
            "RootCause": "The root cause of the issue is that the AM launcher thread is in a WAITING state, waiting for a lock or resource that is held by another thread. This is likely due to contention or improper synchronization in the application, particularly in the methods related to inter-thread communication and resource management.",
            "StepsToReproduce": [
                "Start the Resource Manager (RM) with a single Node Manager (NM).",
                "Simulate a failure in the NM node.",
                "Observe the behavior of the AM launcher thread."
            ],
            "ExpectedBehavior": "The AM launcher should be able to handle NM failures gracefully, either by retrying the launch or by reporting the failure without hanging indefinitely.",
            "ObservedBehavior": "The AM launcher thread hangs in a WAITING state for an extended period (up to 9 hours) when the NM node fails, preventing any further application launches.",
            "Suggestions": "Consider implementing a timeout mechanism for the AM launcher thread to prevent indefinite waiting. Additionally, increasing the number of launcher threads could help mitigate the impact of a single NM failure.",
            "problem_location": {
                "files": [
                    "AMLauncher.java",
                    "ContainerManagerPBClientImpl.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher",
                    "org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagerPBClientImpl"
                ],
                "methods": [
                    "AMLauncher.launch",
                    "ContainerManagerPBClientImpl.startContainer"
                ]
            },
            "possible_fix": "Implement a timeout for the wait state in the AMLauncher class. For example, modify the call method in the AMLauncher to include a timeout parameter, allowing the thread to exit the WAITING state after a specified duration."
        },
        "possible_fix_code": {
            "AMLauncher.launch": "  private void launch() throws IOException {\n    connect();\n    ContainerId masterContainerID = application.getMasterContainer().getId();\n    ApplicationSubmissionContext applicationContext =\n      application.getSubmissionContext();\n    LOG.info(\"Setting up container \" + application.getMasterContainer() \n        + \" for AM \" + application.getAppAttemptId());  \n    ContainerLaunchContext launchContext =\n        createAMContainerLaunchContext(applicationContext, masterContainerID);\n    StartContainerRequest request = \n        recordFactory.newRecordInstance(StartContainerRequest.class);\n    request.setContainerLaunchContext(launchContext);\n    // Implementing a timeout mechanism for the startContainer call\n    long timeout = 30000; // 30 seconds timeout\n    long startTime = System.currentTimeMillis();\n    boolean success = false;\n    while (!success && (System.currentTimeMillis() - startTime) < timeout) {\n        try {\n            containerMgrProxy.startContainer(request);\n            success = true;\n        } catch (IOException e) {\n            LOG.warn(\"Failed to start container, retrying...\", e);\n            // Optionally, add a small sleep here to avoid tight loop\n            Thread.sleep(1000);\n        }\n    }\n    if (!success) {\n        throw new IOException(\"Failed to start container after timeout\");\n    }\n    LOG.info(\"Done launching container \" + application.getMasterContainer() \n        + \" for AM \" + application.getAppAttemptId());\n  }"
        }
    },
    {
        "filename": "MAPREDUCE-3066.json",
        "creation_time": "2011-09-21T22:42:00.000+0000",
        "bug_report": {
            "Title": "YARN NM fails to start",
            "Description": "The YARN NodeManager fails to start due to an invalid configuration for the `yarn.resourcemanager.resource-tracker.address`. The error indicates that this configuration is not a valid host:port pair, which is essential for the NodeManager to communicate with the ResourceManager. Additionally, there are `IllegalStateException` messages indicating that the NodeManager is in the wrong state (INITED instead of STARTED) when attempting to stop the service.",
            "StackTrace": [
                "2011-09-21 15:36:33,534 INFO  service.AbstractService (AbstractService.java:stop(71)) - Service:org.apache.hadoop.yarn.server.nodemanager.DeletionService is stopped.",
                "2011-09-21 15:36:33,534 FATAL nodemanager.NodeManager (NodeManager.java:main(204)) - Error starting NodeManager",
                "org.apache.hadoop.yarn.YarnException: Failed to Start org.apache.hadoop.yarn.server.nodemanager.NodeManager",
                "at org.apache.hadoop.yarn.service.CompositeService.start(CompositeService.java:78)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.start(NodeManager.java:153)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.main(NodeManager.java:202)",
                "Caused by: org.apache.avro.AvroRuntimeException: java.lang.RuntimeException: Not a host:port pair: yarn.resourcemanager.resource-tracker.address",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.start(NodeStatusUpdaterImpl.java:141)",
                "at org.apache.hadoop.yarn.service.CompositeService.start(CompositeService.java:68)",
                "... 2 more",
                "Caused by: java.lang.RuntimeException: Not a host:port pair: yarn.resourcemanager.resource-tracker.address",
                "at org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:148)",
                "at org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:132)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.getRMClient(NodeStatusUpdaterImpl.java:154)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.registerWithRM(NodeStatusUpdaterImpl.java:164)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.start(NodeStatusUpdaterImpl.java:137)",
                "... 3 more"
            ],
            "RootCause": "The root cause of the failure is an incorrectly configured `yarn.resourcemanager.resource-tracker.address`, which is not set as a valid host:port pair. This prevents the NodeManager from starting successfully.",
            "StepsToReproduce": [
                "1. Check the configuration file (e.g., yarn-site.xml) for the setting of `yarn.resourcemanager.resource-tracker.address`.",
                "2. Ensure that the value is in the format `hostname:port`.",
                "3. Attempt to start the YARN NodeManager."
            ],
            "ExpectedBehavior": "The NodeManager should start successfully and register with the ResourceManager using the correct host:port configuration.",
            "ObservedBehavior": "The NodeManager fails to start, throwing an error indicating that the `yarn.resourcemanager.resource-tracker.address` is not a valid host:port pair.",
            "Suggestions": "Check the configuration file (yarn-site.xml) for the correct setting of `yarn.resourcemanager.resource-tracker.address` and ensure it follows the correct format. After correcting the configuration, restart the NodeManager.",
            "problem_location": {
                "files": [
                    "yarn-site.xml"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.nodemanager.NodeManager",
                    "org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl"
                ],
                "methods": [
                    "NodeManager.main",
                    "NodeStatusUpdaterImpl.start"
                ]
            },
            "possible_fix": "Ensure the `yarn.resourcemanager.resource-tracker.address` is set correctly in the configuration file (e.g., yarn-site.xml) in the format `hostname:port`. After making the changes, restart the NodeManager."
        },
        "possible_fix_code": {
            "NodeManager.main": "  public static void main(String[] args) {\n    StringUtils.startupShutdownMessage(NodeManager.class, args, LOG);\n    try {\n      NodeManager nodeManager = new NodeManager();\n      Runtime.getRuntime().addShutdownHook(\n          new CompositeServiceShutdownHook(nodeManager));\n      YarnConfiguration conf = new YarnConfiguration();\n      // Ensure the resource-tracker address is set correctly\n      String resourceTrackerAddress = conf.get(YarnConfiguration.RM_RESOURCE_TRACKER_ADDRESS);\n      if (resourceTrackerAddress == null || !resourceTrackerAddress.matches(\"^[^:]+:\\d+$\")) {\n        throw new IllegalArgumentException(\"Invalid configuration for yarn.resourcemanager.resource-tracker.address. It must be in the format hostname:port.\");\n      }\n      nodeManager.init(conf);\n      nodeManager.start();\n    } catch (Throwable t) {\n      LOG.fatal(\"Error starting NodeManager\", t);\n      System.exit(-1);\n    }\n  }"
        }
    },
    {
        "filename": "MAPREDUCE-3123.json",
        "creation_time": "2011-09-29T19:03:45.000+0000",
        "bug_report": {
            "Title": "Symbolic links with special chars causing container/task.sh to fail",
            "Description": "The job fails with a syntax error when special characters are included in symbolic links. The command executed is part of a Hadoop streaming job that attempts to create a symbolic link with special characters in its name. The error occurs in the shell script located at a specific path, indicating that the command being executed is malformed due to unexpected tokens in the script.",
            "StackTrace": [
                "2011-09-27 20:58:48,903 INFO org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor: launchContainer:",
                "[container-executor, hadoopuser, 1, application_1317077272567_0239, container_1317077272567_0239_01_000001, tmp/mapred-local/usercache/hadoopuser/appcache/application_1317077272567_0239/container_1317077272567_0239_01_000001, tmp/mapred-local/nmPrivate/application_1317077272567_0239/container_1317077272567_0239_01_000001.tokens]1109221111-tests.jar:hadoop-mapreduce-p2011-09-27",
                "20:58:48,944 WARN org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor: Exit code from container is : 2",
                "2011-09-27 20:58:48,946 WARN org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor: Exception from container-launch :",
                "org.apache.hadoop.util.Shell$ExitCodeException: /tmp/mapred-local/usercache/hadoopuser/appcache/application_1317077272567_0239/container_1317077272567_0239_01_000001/task.sh: line 26: syntax error near unexpected token `-_+='",
                "/tmp/mapred-local/usercache/hadoopuser/appcache/application_1317077272567_0239/container_1317077272567_0239_01_000001/task.sh: line 26: `ln -sf /tmp/mapred-local/usercache/hadoopqa/filecache/-1888139433818483070/InputDir testlink!@$&*()-_+='kson-jaxrs-1.7.1.jar:/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.7.3/jackson-mapper-asl-1.7.3.jar:.m2/repository/org/codehaus/jackson/jackson-xc/1.7.1/jackson-xc-1.7.1.jar:"
            ],
            "RootCause": "The root cause of the issue is a syntax error in the shell script located at line 26, where the command attempts to create a symbolic link with special characters. The unexpected token `-_+=` indicates that the command is malformed, likely due to improper handling of special characters in the symbolic link name.",
            "StepsToReproduce": [
                "Run the following Hadoop streaming job with a symbolic link containing special characters: hadoop jar hadoop-streaming.jar -Dmapreduce.job.acl-view-job=* -Dmapreduce.job.queuename=queue1 -files file:///homes/user/hadoop/Streaming/data/streaming-980//InputDir#testlink!@$&*()-_+= -input Streaming/streaming-980/input.txt -mapper 'xargs cat' -reducer cat -output Streaming/streaming-980/Output -jobconf mapred.job.name=streamingTest-980 -jobconf mapreduce.job.acl-view-job=*"
            ],
            "ExpectedBehavior": "The job should execute successfully, creating the necessary symbolic links and processing the input files without any syntax errors.",
            "ObservedBehavior": "The job fails with a syntax error in the shell script, preventing the container from launching and resulting in a non-zero exit code.",
            "Suggestions": "Review the values being passed to the script, especially around line 26 where the error occurs. Ensure that the parameters are correctly formatted and do not contain any invalid characters that could lead to a syntax error in the shell.",
            "problem_location": {
                "files": [
                    "task.sh"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor"
                ],
                "methods": [
                    "LinuxContainerExecutor.launchContainer"
                ]
            },
            "possible_fix": "Modify the script to properly handle special characters in symbolic link names. This may involve escaping special characters or validating input to ensure that they do not cause syntax errors."
        },
        "possible_fix_code": {
            "launchContainer": "  public int launchContainer(Container container,\n      Path nmPrivateCotainerScriptPath, Path nmPrivateTokensPath,\n      String user, String appId, Path containerWorkDir) throws IOException {\n\n    ContainerId containerId = container.getContainerID();\n    String containerIdStr = ConverterUtils.toString(containerId);\n    List<String> command = new ArrayList<String>(\n      Arrays.asList(containerExecutorExe, \n                    user, \n                    Integer.toString(Commands.LAUNCH_CONTAINER.getValue()),\n                    appId,\n                    containerIdStr,\n                    containerWorkDir.toString(),\n                    nmPrivateCotainerScriptPath.toUri().getPath().toString(),\n                    nmPrivateTokensPath.toUri().getPath().toString()));\n\n    // Escape special characters in the symbolic link name\n    String escapedLinkName = nmPrivateCotainerScriptPath.getName().replaceAll(\"[!@#$%^&*()+=-]\", \"\\\\$0\");\n    command.add(escapedLinkName);\n\n    String[] commandArray = command.toArray(new String[command.size()]);\n    ShellCommandExecutor shExec = \n        new ShellCommandExecutor(\n            commandArray,\n            null,                                              // NM's cwd\n            container.getLaunchContext().getEnvironment());    // sanitized env\n    launchCommandObjs.put(containerId, shExec);\n    // DEBUG\n    LOG.info(\"launchContainer: \" + Arrays.toString(commandArray));\n    String output = shExec.getOutput();\n    try {\n      shExec.execute();\n      if (LOG.isDebugEnabled()) {\n        logOutput(output);\n      }\n    } catch (ExitCodeException e) {\n      int exitCode = shExec.getExitCode();\n      LOG.warn(\"Exit code from container is : \" + exitCode);\n      // 143 (SIGTERM) and 137 (SIGKILL) exit codes means the container was\n      // terminated/killed forcefully. In all other cases, log the\n      // container-executor's output\n      if (exitCode != 143 && exitCode != 137) {\n        LOG.warn(\"Exception from container-launch : \", e);\n        logOutput(output);\n        String diagnostics = \"Exception from container-launch: \\n\"\n            + StringUtils.stringifyException(e) + \"\\n\" + output;\n        container.handle(new ContainerDiagnosticsUpdateEvent(containerId,\n            diagnostics));\n      } else {\n        container.handle(new ContainerDiagnosticsUpdateEvent(containerId,\n            \"Container killed on request. Exit code is \" + exitCode));\n      }\n      return exitCode;\n    } finally {\n      launchCommandObjs.remove(containerId);\n    }\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Output from LinuxContainerExecutor's launchContainer follows:\");\n      logOutput(output);\n    }\n    return 0;\n  }"
        }
    },
    {
        "filename": "MAPREDUCE-6439.json",
        "creation_time": "2015-07-30T20:31:00.000+0000",
        "bug_report": {
            "Title": "AM may fail instead of retrying if RM shuts down during the allocate call",
            "Description": "The issue arises when the Application Master (AM) encounters a YarnRuntimeException during communication with the Resource Manager (RM). This exception is thrown back to the AM, leading it to mistakenly believe it has exhausted its retry attempts. The stack trace indicates that an InterruptedException occurs within the AsyncDispatcher and ApplicationMasterService, which are critical components in the YARN framework responsible for resource allocation and event handling.",
            "StackTrace": [
                "2015-07-25 20:07:27,346 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Error communicating with RM: java.lang.InterruptedException",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$GenericEventHandler.handle(AsyncDispatcher.java:245)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService.allocate(ApplicationMasterService.java:469)",
                "at org.apache.hadoop.yarn.api.impl.pb.service.ApplicationMasterProtocolPBServiceImpl.allocate(ApplicationMasterProtocolPBServiceImpl.java:60)",
                "at org.apache.hadoop.yarn.proto.ApplicationMasterProtocol$ApplicationMasterProtocolService$2.callBlockingMethod(ApplicationMasterProtocol.java:99)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:587)",
                "at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1642)",
                "at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)",
                "Caused by: java.lang.InterruptedException",
                "at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireInterruptibly(AbstractQueuedSynchronizer.java:1219)",
                "at java.util.concurrent.locks.ReentrantLock.lockInterruptibly(ReentrantLock.java:340)",
                "at java.util.concurrent.LinkedBlockingQueue.put(LinkedBlockingQueue.java:338)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$GenericEventHandler.handle(AsyncDispatcher.java:240)"
            ],
            "RootCause": "The root cause of the issue is the occurrence of a java.lang.InterruptedException during the resource allocation process in the YARN framework. This exception is propagated through the AsyncDispatcher and ApplicationMasterService, leading the AM to incorrectly conclude that it has exhausted its retry attempts.",
            "StepsToReproduce": [
                "1. Start the YARN Resource Manager (RM) and Application Master (AM).",
                "2. Initiate a resource allocation request from the AM to the RM.",
                "3. Simulate a shutdown of the RM during the allocate call.",
                "4. Observe the behavior of the AM and check for YarnRuntimeException in the logs."
            ],
            "ExpectedBehavior": "The Application Master should handle interruptions gracefully and continue retrying the allocation request until the maximum retry limit is reached or a successful allocation occurs.",
            "ObservedBehavior": "The Application Master fails to retry the allocation request and logs a YarnRuntimeException, leading to a premature termination of the heartbeat thread.",
            "Suggestions": "Review the handling of InterruptedException in the AsyncDispatcher and ApplicationMasterService. Ensure that the AM correctly interprets the state of retries and does not terminate prematurely upon receiving an InterruptedException.",
            "problem_location": {
                "files": [
                    "org/apache/hadoop/yarn/event/AsyncDispatcher.java",
                    "org/apache/hadoop/yarn/server/resourcemanager/ApplicationMasterService.java",
                    "org/apache/hadoop/security/UserGroupInformation.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.event.AsyncDispatcher",
                    "org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService",
                    "org.apache.hadoop.security.UserGroupInformation"
                ],
                "methods": [
                    "AsyncDispatcher.handle",
                    "ApplicationMasterService.allocate",
                    "UserGroupInformation.doAs"
                ]
            },
            "possible_fix": "Modify the error handling in the ApplicationMasterService.allocate method to ensure that InterruptedException is handled properly, allowing the AM to continue retrying allocation requests. Consider implementing a backoff strategy for retries to avoid overwhelming the RM."
        },
        "possible_fix_code": {
            "ApplicationMasterService.allocate": "public AllocateResponse allocate(AllocateRequest request) throws YarnRuntimeException {\n    try {\n        // Existing allocation logic\n        // ...\n    } catch (InterruptedException e) {\n        // Log the interruption and handle it gracefully\n        LOG.warn(\"Allocation interrupted, will retry: \" + e.getMessage());\n        // Implement a backoff strategy for retries\n        retryAllocation(request);\n    } catch (Exception e) {\n        // Handle other exceptions\n        throw new YarnRuntimeException(e);\n    }\n    return response;\n}\n\nprivate void retryAllocation(AllocateRequest request) {\n    int retryCount = 0;\n    while (retryCount < MAX_RETRY_ATTEMPTS) {\n        try {\n            // Attempt allocation again\n            AllocateResponse response = allocate(request);\n            if (response != null) {\n                return response;\n            }\n        } catch (InterruptedException e) {\n            LOG.warn(\"Retry interrupted, will attempt again: \" + e.getMessage());\n            // Optionally implement a backoff delay here\n        }\n        retryCount++;\n    }\n    throw new YarnRuntimeException(\"Max retry attempts reached for allocation.\");\n}"
        }
    },
    {
        "filename": "MAPREDUCE-4490.json",
        "creation_time": "2012-07-27T01:22:21.000+0000",
        "bug_report": {
            "Title": "JVM reuse is incompatible with LinuxTaskController (and therefore incompatible with Security)",
            "Description": "The issue arises when using the LinuxTaskController with JVM reuse enabled (mapred.job.reuse.jvm.num.tasks > 1). In scenarios where there are more map tasks than available map slots, the second task in each JVM fails immediately, causing the JVM to exit. This failure occurs because the user log directory for a task attempt is only created during the first invocation of the JVM. As a result, when the second task attempts to write to the log.index file, it fails with an ENOENT error due to the absence of the required directory.",
            "StackTrace": [
                "2012-07-24 14:29:11,914 INFO org.apache.hadoop.mapred.TaskLog: Starting logging for a new task attempt_201207241401_0013_m_000027_0 in the same JVM as that of the first task /var/log/hadoop/mapred/userlogs/job_201207241401_0013/attempt_201207241401_0013_m_000006_0",
                "2012-07-24 14:29:11,915 WARN org.apache.hadoop.mapred.Child: Error running child",
                "ENOENT: No such file or directory",
                "at org.apache.hadoop.io.nativeio.NativeIO.open(Native Method)",
                "at org.apache.hadoop.io.SecureIOUtils.createForWrite(SecureIOUtils.java:161)",
                "at org.apache.hadoop.mapred.TaskLog.writeToIndexFile(TaskLog.java:296)",
                "at org.apache.hadoop.mapred.TaskLog.syncLogs(TaskLog.java:369)",
                "at org.apache.hadoop.mapred.Child.main(Child.java:229)"
            ],
            "RootCause": "The root cause of the issue is that the user log directory (LOG_DIR) is not created for subsequent tasks in the same JVM when using the LinuxTaskController. This leads to a failure when attempting to write the log.index file for the second task, resulting in an ENOENT error.",
            "StepsToReproduce": [
                "Set up a Hadoop cluster with LinuxTaskController.",
                "Configure mapred.job.reuse.jvm.num.tasks to a value greater than 1.",
                "Submit a job with more map tasks than available map slots.",
                "Observe the task failures in the logs."
            ],
            "ExpectedBehavior": "The system should successfully log all task attempts without any errors, creating necessary directories for each task in the same JVM.",
            "ObservedBehavior": "The second task in each JVM fails with an ENOENT error due to the absence of the required user log directory, causing the JVM to exit.",
            "Suggestions": "Modify the LinuxTaskController to ensure that the user log directory is created for each task attempt. This can be achieved by adding a command to initialize the task and create the necessary directories before attempting to write logs.",
            "problem_location": {
                "files": [
                    "hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/TaskLog.java"
                ],
                "classes": [
                    "org.apache.hadoop.mapred.TaskLog"
                ],
                "methods": [
                    "TaskLog.syncLogs",
                    "TaskLog.writeToIndexFile",
                    "TaskLog.getTmpIndexFile",
                    "TaskLog.getAttemptDir",
                    "TaskLog.getJobDir",
                    "TaskLog.getUserLogDir"
                ]
            },
            "possible_fix": "Implement a new command in the task-controller to create attempt directories. Call this command in the LinuxTaskController#createLogDir method to ensure that the necessary directories are created before any logging operations."
        },
        "possible_fix_code": {
            "org.apache.hadoop.mapred.TaskLog.getUserLogDir": "  static File getUserLogDir() {\n    if (!LOG_DIR.exists()) {\n      boolean b = LOG_DIR.mkdirs();\n      if (!b) {\n        LOG.debug(\"mkdirs failed. Ignoring.\");\n        // Throw an IOException if the directory creation fails\n        throw new IOException(\"Failed to create user log directory: \" + LOG_DIR);\n      }\n    }\n    return LOG_DIR;\n  }"
        }
    },
    {
        "filename": "MAPREDUCE-3744.json",
        "creation_time": "2012-01-27T20:31:19.000+0000",
        "bug_report": {
            "Title": "Unable to retrieve application logs via \"yarn logs\" or \"mapred job -logs\"",
            "Description": "The application fails to retrieve logs using the \"yarn logs\" command, resulting in a `FileNotFoundException`. The error indicates that the log file at `/tmp/logs/application_1327694122989_0001` does not exist. This issue may stem from incorrect log directory configuration or problems with the log aggregation process. Additionally, using the \"mapred job -logs\" command leads to warnings about the Job History Server not being configured, despite the process running.",
            "StackTrace": [
                "Exception in thread \"main\" java.io.FileNotFoundException: File /tmp/logs/application_1327694122989_0001 does not exist.",
                "at org.apache.hadoop.fs.Hdfs$DirListingIterator.<init>(Hdfs.java:226)",
                "at org.apache.hadoop.fs.Hdfs$DirListingIterator.<init>(Hdfs.java:217)",
                "at org.apache.hadoop.fs.Hdfs$2.<init>(Hdfs.java:192)",
                "at org.apache.hadoop.fs.Hdfs.listStatusIterator(Hdfs.java:192)",
                "at org.apache.hadoop.fs.FileContext$20.next(FileContext.java:1371)",
                "at org.apache.hadoop.fs.FileContext$20.next(FileContext.java:1)",
                "at org.apache.hadoop.fs.FileContext$FSLinkResolver.resolve(FileContext.java:2319)",
                "at org.apache.hadoop.fs.FileContext.listStatus(FileContext.java:1373)",
                "at org.apache.hadoop.yarn.logaggregation.LogDumper.dumpAllContainersLogs(LogDumper.java:191)",
                "at org.apache.hadoop.yarn.logaggregation.LogDumper.run(LogDumper.java:107)",
                "at org.apache.hadoop.yarn.logaggregation.LogDumper.main(LogDumper.java:226)"
            ],
            "RootCause": "The `FileNotFoundException` occurs because the log directory does not exist, likely due to incorrect configuration settings for `YarnConfiguration.NM_REMOTE_APP_LOG_DIR` and `YarnConfiguration.NM_REMOTE_APP_LOG_DIR_SUFFIX`, issues with the log aggregation process, or the log file being deleted.",
            "StepsToReproduce": [
                "Run the command 'yarn logs -applicationId application_1327694122989_0001'.",
                "Observe the error message indicating that the log file does not exist.",
                "Run the command 'mapred job -logs job_1327694122989_0001'.",
                "Check for warnings about the Job History Server configuration."
            ],
            "ExpectedBehavior": "The application logs should be retrieved successfully without any errors, indicating the log files are accessible.",
            "ObservedBehavior": "The application fails to retrieve logs, resulting in a `FileNotFoundException` and warnings about the Job History Server not being configured.",
            "Suggestions": "Check the configuration settings for `YarnConfiguration.NM_REMOTE_APP_LOG_DIR` and `YarnConfiguration.NM_REMOTE_APP_LOG_DIR_SUFFIX`. Ensure that the log aggregation process is functioning correctly and verify that the log files are being written to the expected location.",
            "problem_location": {
                "files": [
                    "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/logaggregation/LogDumper.java",
                    "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/logaggregation/LogAggregationUtils.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.logaggregation.LogDumper",
                    "org.apache.hadoop.yarn.logaggregation.LogAggregationUtils"
                ],
                "methods": [
                    "LogDumper.dumpAllContainersLogs",
                    "LogAggregationUtils.getRemoteAppLogDir"
                ]
            },
            "possible_fix": "Verify and update the configuration settings for `YarnConfiguration.NM_REMOTE_APP_LOG_DIR` and `YarnConfiguration.NM_REMOTE_APP_LOG_DIR_SUFFIX` to ensure they point to valid directories. Additionally, check the log aggregation process to confirm it is correctly writing logs to the specified location."
        },
        "possible_fix_code": {
            "LogDumper.dumpAllContainersLogs": "  private void dumpAllContainersLogs(ApplicationId appId, String appOwner,\n      DataOutputStream out) throws IOException {\n    Path remoteRootLogDir =\n        new Path(getConf().get(YarnConfiguration.NM_REMOTE_APP_LOG_DIR,\n            YarnConfiguration.DEFAULT_NM_REMOTE_APP_LOG_DIR));\n    String user = appOwner;\n    String logDirSuffix =\n        getConf().get(YarnConfiguration.NM_REMOTE_APP_LOG_DIR_SUFFIX,\n            YarnConfiguration.DEFAULT_NM_REMOTE_APP_LOG_DIR_SUFFIX);\n    // Ensure the remote application log directory exists\n    Path remoteAppLogDir =\n        LogAggregationUtils.getRemoteAppLogDir(remoteRootLogDir, appId, user,\n            logDirSuffix);\n    FileContext fileContext = FileContext.getFileContext();\n    if (!fileContext.util().exists(remoteAppLogDir)) {\n        throw new FileNotFoundException(\"Log directory does not exist: \" + remoteAppLogDir);\n    }\n    RemoteIterator<FileStatus> nodeFiles =\n        fileContext.listStatus(remoteAppLogDir);\n    while (nodeFiles.hasNext()) {\n      FileStatus thisNodeFile = nodeFiles.next();\n      AggregatedLogFormat.LogReader reader =\n          new AggregatedLogFormat.LogReader(getConf(),\n              new Path(remoteAppLogDir, thisNodeFile.getPath().getName()));\n      try {\n\n        DataInputStream valueStream;\n        LogKey key = new LogKey();\n        valueStream = reader.next(key);\n\n        while (valueStream != null) {\n          while (true) {\n            try {\n              LogReader.readAContainerLogsForALogType(valueStream, out);\n            } catch (EOFException eof) {\n              break;\n            }\n          }\n\n          // Next container\n          key = new LogKey();\n          valueStream = reader.next(key);\n        }\n      } finally {\n        reader.close();\n      }\n    }\n  }"
        }
    },
    {
        "filename": "MAPREDUCE-3053.json",
        "creation_time": "2011-09-20T22:32:42.000+0000",
        "bug_report": {
            "Title": "YARN Protobuf RPC Failures in RM",
            "Description": "The application master fails to register with YARN's ResourceManager (RM) due to a NullPointerException. The issue arises during the invocation of the `registerApplicationMaster` method in the `AMRMProtocolPBClientImpl` class, where a null request is being passed to the proxy method. This indicates that the `RegisterApplicationMasterRequest` object is not being properly instantiated or populated with the required fields before the method call.",
            "StackTrace": [
                "Exception in thread \"main\" java.lang.reflect.UndeclaredThrowableException",
                "at org.apache.hadoop.yarn.api.impl.pb.client.AMRMProtocolPBClientImpl.registerApplicationMaster(AMRMProtocolPBClientImpl.java:108)",
                "at kafka.yarn.util.ApplicationMasterHelper.registerWithResourceManager(YarnHelper.scala:48)",
                "at kafka.yarn.ApplicationMaster$.main(ApplicationMaster.scala:32)",
                "at kafka.yarn.ApplicationMaster.main(ApplicationMaster.scala)",
                "Caused by: com.google.protobuf.ServiceException: java.lang.NullPointerException: java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.proto.ClientRMProtocol$ClientRMProtocolService$2.getRequestPrototype(ClientRMProtocol.java:186)",
                "at org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Server.call(ProtoOverHadoopRpcEngine.java:323)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1489)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1485)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:396)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1135)",
                "at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1483)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1084)",
                "at org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Invoker.invoke(ProtoOverHadoopRpcEngine.java:127)"
            ],
            "RootCause": "The root cause of the NullPointerException is the null argument being passed to the `registerApplicationMaster` method in the `AMRMProtocolPBClientImpl`. This suggests that the `RegisterApplicationMasterRequest` object is not being properly constructed or populated with the necessary fields before the method call.",
            "StepsToReproduce": [
                "1. Start the ApplicationMaster with the required parameters.",
                "2. Attempt to register the ApplicationMaster with the ResourceManager.",
                "3. Observe the logs for the NullPointerException."
            ],
            "ExpectedBehavior": "The ApplicationMaster should successfully register with the ResourceManager without any exceptions.",
            "ObservedBehavior": "The ApplicationMaster fails to register, resulting in a NullPointerException being thrown.",
            "Suggestions": "Ensure that the `RegisterApplicationMasterRequest` object is properly instantiated and all required fields (ApplicationAttemptId, host, RPC port, and tracking URL) are set before calling the `registerApplicationMaster` method.",
            "problem_location": {
                "files": [
                    "AMRMProtocolPBClientImpl.java",
                    "ApplicationMasterHelper.scala",
                    "ApplicationMaster.scala"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.api.impl.pb.client.AMRMProtocolPBClientImpl",
                    "kafka.yarn.util.ApplicationMasterHelper",
                    "kafka.yarn.ApplicationMaster"
                ],
                "methods": [
                    "AMRMProtocolPBClientImpl.registerApplicationMaster",
                    "ApplicationMasterHelper.registerWithResourceManager",
                    "ApplicationMaster.main"
                ]
            },
            "possible_fix": "Modify the `registerWithResourceManager` method to ensure that the `RegisterApplicationMasterRequest` is properly constructed and populated with the necessary fields before invoking the `registerApplicationMaster` method."
        },
        "possible_fix_code": {
            "AMRMProtocolPBClientImpl.registerApplicationMaster": "public RegisterApplicationMasterResponse registerApplicationMaster(\n      RegisterApplicationMasterRequest request) throws YarnRemoteException {\n    if (request == null) {\n        throw new IllegalArgumentException(\"RegisterApplicationMasterRequest cannot be null\");\n    }\n    RegisterApplicationMasterRequestProto requestProto = ((RegisterApplicationMasterRequestPBImpl)request).getProto();\n    try {\n      return new RegisterApplicationMasterResponsePBImpl(proxy.registerApplicationMaster(null, requestProto));\n    } catch (ServiceException e) {\n      if (e.getCause() instanceof YarnRemoteException) {\n        throw (YarnRemoteException)e.getCause();\n      } else if (e.getCause() instanceof UndeclaredThrowableException) {\n        throw (UndeclaredThrowableException)e.getCause();\n      } else {\n        throw new UndeclaredThrowableException(e);\n      }\n    }\n  }"
        }
    },
    {
        "filename": "MAPREDUCE-5884.json",
        "creation_time": "2014-04-16T01:07:22.000+0000",
        "bug_report": {
            "Title": "History server uses short user name when canceling tokens",
            "Description": "When a user attempts to cancel a delegation token, an AccessControlException is thrown, indicating that the user is not authorized to perform this action. The issue arises from a mismatch between the full principal name of the token owner and the short user name of the canceller. The `cancelToken` method in `AbstractDelegationTokenSecretManager` checks if the canceller is either the owner or the renewer of the token, leading to the exception when these conditions are not met.",
            "StackTrace": [
                "2014-04-14 20:07:35,744 WARN org.apache.hadoop.security.UserGroupInformation: PriviledgedActionException as:<someuser>/<machine_name>.linkedin.com@<realm>.LINKEDIN.COM (auth:KERBEROS) cause:org.apache.hadoop.security.AccessControlException: <someuser> is not authorized to cancel the token",
                "2014-04-14 20:07:35,744 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 10020, call org.apache.hadoop.mapreduce.v2.api.HSClientProtocolPB.cancelDelegationToken from 172.20.158.61:49042 Call#4 Retry#0: error: org.apache.hadoop.security.AccessControlException: <someuser> is not authorized to cancel the token",
                "org.apache.hadoop.security.AccessControlException: <someuser> is not authorized to cancel the token",
                "at org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.cancelToken(AbstractDelegationTokenSecretManager.java:429)",
                "at org.apache.hadoop.mapreduce.v2.hs.HistoryClientService$HSClientProtocolHandler.cancelDelegationToken(HistoryClientService.java:400)",
                "at org.apache.hadoop.mapreduce.v2.api.impl.pb.service.MRClientProtocolPBServiceImpl.cancelDelegationToken(MRClientProtocolPBServiceImpl.java:286)",
                "at org.apache.hadoop.yarn.proto.MRClientProtocol$MRClientProtocolService$2.callBlockingMethod(MRClientProtocol.java:301)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)",
                "at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1962)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1958)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)",
                "at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1956)"
            ],
            "RootCause": "The root cause of the issue is that the `cancelToken` method in `AbstractDelegationTokenSecretManager` requires the canceller to be either the owner or the renewer of the token. The canceller's short name is being used, which does not match the full principal name of the token owner, resulting in an AccessControlException.",
            "StepsToReproduce": [
                "1. Authenticate as a user with a delegation token.",
                "2. Attempt to cancel the token using a short user name.",
                "3. Observe the AccessControlException thrown."
            ],
            "ExpectedBehavior": "The user should be able to cancel the delegation token if they are the owner or renewer, without encountering an AccessControlException.",
            "ObservedBehavior": "The user receives an AccessControlException stating that they are not authorized to cancel the token, due to a mismatch between the short user name and the full principal name of the token owner.",
            "Suggestions": "Consider modifying the `cancelToken` method to allow comparison with both the short name and the full principal name, or ensure that all callers provide a consistent value for the canceller.",
            "problem_location": {
                "files": [
                    "AbstractDelegationTokenSecretManager.java",
                    "HistoryClientService.java"
                ],
                "classes": [
                    "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager",
                    "org.apache.hadoop.mapreduce.v2.hs.HistoryClientService"
                ],
                "methods": [
                    "AbstractDelegationTokenSecretManager.cancelToken",
                    "HistoryClientService.cancelDelegationToken"
                ]
            },
            "possible_fix": "In the `cancelToken` method, modify the authorization check to compare the canceller against both the short name and the full principal name of the owner. Alternatively, ensure that the canceller is consistently passed as either a short name or a full principal name across all calls."
        },
        "possible_fix_code": {
            "AbstractDelegationTokenSecretManager.cancelToken": "  public synchronized TokenIdent cancelToken(Token<TokenIdent> token,\n      String canceller) throws IOException {\n    ByteArrayInputStream buf = new ByteArrayInputStream(token.getIdentifier());\n    DataInputStream in = new DataInputStream(buf);\n    TokenIdent id = createIdentifier();\n    id.readFields(in);\n    LOG.info(\"Token cancelation requested for identifier: \"+id);\n    \n    if (id.getUser() == null) {\n      throw new InvalidToken(\"Token with no owner\");\n    }\n    String owner = id.getUser().getUserName();\n    Text renewer = id.getRenewer();\n    HadoopKerberosName cancelerKrbName = new HadoopKerberosName(canceller);\n    String cancelerShortName = cancelerKrbName.getShortName();\n    if (!canceller.equals(owner)\n        && (renewer == null || renewer.toString().isEmpty() || !cancelerShortName\n            .equals(renewer.toString()) && !canceller.equals(owner))) {\n      throw new AccessControlException(canceller\n          + \" is not authorized to cancel the token\");\n    }\n    DelegationTokenInformation info = null;\n    info = currentTokens.remove(id);\n    if (info == null) {\n      throw new InvalidToken(\"Token not found\");\n    }\n    removeStoredToken(id);\n    return id;\n  }"
        }
    },
    {
        "filename": "MAPREDUCE-3706.json",
        "creation_time": "2012-01-21T04:01:30.000+0000",
        "bug_report": {
            "Title": "HTTP Circular redirect error on the job attempts page",
            "Description": "When attempting to access the job attempts page for a submitted job, a circular redirect error occurs. The URL accessed is: http://rmhost.domain.com:8088/proxy/application_1326992308313_0004/mapreduce/attempts/job_1326992308313_4_4/m/NEW. This results in an HTTP ERROR 500 due to a CircularRedirectException, indicating that the request is being redirected in a loop. The issue appears to be related to the handling of the tracking URL and the application state in the WebAppProxyServlet.",
            "StackTrace": [
                "HTTP ERROR 500",
                "Problem accessing /proxy/application_1326992308313_0004/mapreduce/attempts/job_1326992308313_4_4/m/NEW. Reason:",
                "Circular redirect to 'http://amhost.domain.com:44869/mapreduce/attempts/job_1326992308313_4_4/m/NEW'",
                "Caused by:",
                "org.apache.commons.httpclient.CircularRedirectException: Circular redirect to 'http://amhost.domain.com:44869/mapreduce/attempts/job_1326992308313_4_4/m/NEW'",
                "at org.apache.commons.httpclient.HttpMethodDirector.processRedirectResponse(HttpMethodDirector.java:638)",
                "at org.apache.commons.httpclient.HttpMethodDirector.executeMethod(HttpMethodDirector.java:179)",
                "at org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:397)",
                "at org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:323)",
                "at org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet.proxyLink(WebAppProxyServlet.java:148)",
                "at org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet.doGet(WebAppProxyServlet.java:269)",
                "at javax.servlet.http.HttpServlet.service(HttpServlet.java:707)",
                "at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)",
                "at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1221)",
                "at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:66)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:900)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:834)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:795)"
            ],
            "RootCause": "The CircularRedirectException is caused by the logic in the WebAppProxyServlet, specifically in the doGet method, where the constructed tracking URI leads to a redirect loop. This can occur if the tracking URL is misconfigured or if the application state leads to repeated redirects.",
            "StepsToReproduce": [
                "Submit a job to the YARN resource manager.",
                "Access the job attempts page using the URL: http://rmhost.domain.com:8088/proxy/application_1326992308313_0004/mapreduce/attempts/job_1326992308313_4_4/m/NEW."
            ],
            "ExpectedBehavior": "The job attempts page should display the relevant information about the job attempts without any errors.",
            "ObservedBehavior": "An HTTP ERROR 500 occurs with a CircularRedirectException, preventing access to the job attempts page.",
            "Suggestions": "Check the configuration of the tracking URL in the ApplicationReport and ensure that it does not lead to a circular reference. Review the logic in the WebAppProxyServlet's doGet method to ensure proper handling of application states and redirects.",
            "problem_location": {
                "files": [
                    "WebAppProxyServlet.java",
                    "ApplicationReport.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet",
                    "org.apache.hadoop.yarn.api.records.ApplicationReport"
                ],
                "methods": [
                    "WebAppProxyServlet.doGet",
                    "WebAppProxyServlet.proxyLink",
                    "ApplicationReport.getOriginalTrackingUrl",
                    "ApplicationReport.setOriginalTrackingUrl"
                ]
            },
            "possible_fix": "Review and modify the logic in the WebAppProxyServlet's doGet method to ensure that the constructed tracking URI does not lead to a circular redirect. Additionally, verify the setting of the original tracking URL in the ApplicationReport to ensure it is correctly formatted."
        },
        "possible_fix_code": {
            "WebAppProxyServlet.doGet": "protected void doGet(HttpServletRequest req, HttpServletResponse resp) \n  throws IOException{\n    try {\n      String userApprovedParamS = \n        req.getParameter(ProxyUriUtils.PROXY_APPROVAL_PARAM);\n      boolean userWasWarned = false;\n      boolean userApproved = \n        (userApprovedParamS != null && Boolean.valueOf(userApprovedParamS));\n      boolean securityEnabled = isSecurityEnabled();\n      final String remoteUser = req.getRemoteUser();\n      final String pathInfo = req.getPathInfo();\n\n      String parts[] = pathInfo.split(\"/\", 3);\n      if(parts.length < 2) {\n        LOG.warn(remoteUser+\" Gave an invalid proxy path \"+pathInfo);\n        notFound(resp, \"Your path appears to be formatted incorrectly.\");\n        return;\n      }\n      //parts[0] is empty because path info always starts with a /\n      String appId = parts[1];\n      String rest = parts.length > 2 ? parts[2] : \"\";\n      ApplicationId id = Apps.toAppID(appId);\n      if(id == null) {\n        LOG.warn(req.getRemoteUser()+\" Attempting to access \"+appId+\n        \" that is invalid\");\n        notFound(resp, appId+\" appears to be formatted incorrectly.\");\n        return;\n      }\n      \n      if(securityEnabled) {\n        String cookieName = getCheckCookieName(id); \n        for(Cookie c: req.getCookies()) {\n          if(cookieName.equals(c.getName())) {\n            userWasWarned = true;\n            userApproved = userApproved || Boolean.valueOf(c.getValue());\n            break;\n          }\n        }\n      }\n      \n      boolean checkUser = securityEnabled && (!userWasWarned || !userApproved);\n\n      ApplicationReport applicationReport = getApplicationReport(id);\n      if(applicationReport == null) {\n        LOG.warn(req.getRemoteUser()+\" Attempting to access \"+id+\n            \" that was not found\");\n        notFound(resp, \"Application \"+appId+\" could not be found, \" +\n        \t\t\"please try the history server\");\n        return;\n      }\n      URI trackingUri = ProxyUriUtils.getUriFromAMUrl(\n          applicationReport.getOriginalTrackingUrl());\n      \n      String runningUser = applicationReport.getUser();\n      if(checkUser && !runningUser.equals(remoteUser)) {\n        LOG.info(\"Asking \"+remoteUser+\" if they want to connect to the \" +\n            \"app master GUI of \"+appId+\" owned by \"+runningUser);\n        warnUserPage(resp, ProxyUriUtils.getPathAndQuery(id, rest, \n            req.getQueryString(), true), runningUser, id);\n        return;\n      }\n      \n      URI toFetch = new URI(req.getScheme(), \n          trackingUri.getAuthority(),\n          StringHelper.ujoin(trackingUri.getPath(), rest), req.getQueryString(),\n          null);\n\n      // Check for circular redirect\n      if (toFetch.toString().equals(req.getRequestURL().toString())) {\n        LOG.warn(\"Circular redirect detected for URL: \" + toFetch);\n        resp.sendError(HttpServletResponse.SC_BAD_REQUEST, \"Circular redirect detected.\");\n        return;\n      }\n\n      LOG.info(req.getRemoteUser()+\" is accessing unchecked \"+toFetch+\n          \" which is the app master GUI of \"+appId+\" owned by \"+runningUser);\n\n      switch(applicationReport.getYarnApplicationState()) {\n      case KILLED:\n      case FINISHED:\n      case FAILED:\n        resp.sendRedirect(resp.encodeRedirectURL(toFetch.toString()));\n        return;\n      }\n      Cookie c = null;\n      if(userWasWarned && userApproved) {\n        c = makeCheckCookie(id, true);\n      }\n      proxyLink(req, resp, toFetch, c);\n\n    } catch(URISyntaxException e) {\n      throw new IOException(e); \n    }\n  }"
        }
    },
    {
        "filename": "MAPREDUCE-5451.json",
        "creation_time": "2013-05-30T00:08:18.000+0000",
        "bug_report": {
            "Title": "MR uses LD_LIBRARY_PATH which doesn't mean anything in Windows",
            "Description": "The MapReduce (MR) framework is incorrectly configured for Windows environments, relying on the LD_LIBRARY_PATH variable to load native libraries. This approach is problematic for two reasons: first, LD_LIBRARY_PATH is not applicable in Windows; second, the configuration incorrectly uses $HADOOP_COMMON_HOME instead of the Windows-specific %HADOOP_COMMON_HOME%. As a result, MR jobs fail to execute properly unless this configuration is overridden, leading to an UnsatisfiedLinkError when attempting to access native methods.",
            "StackTrace": [
                "2013-05-29 13:51:41,049 FATAL [main] org.apache.hadoop.mapred.YarnChild: Error running child : java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z",
                "at org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)",
                "at org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:393)",
                "at org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:928)",
                "at org.apache.hadoop.util.DiskChecker.checkAccessByFileMethods(DiskChecker.java:177)",
                "at org.apache.hadoop.util.DiskChecker.checkDirAccess(DiskChecker.java:164)",
                "at org.apache.hadoop.util.DiskChecker.checkDir(DiskChecker.java:98)",
                "at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.confChanged(LocalDirAllocator.java:288)",
                "at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathToRead(LocalDirAllocator.java:431)",
                "at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathToRead(LocalDirAllocator.java:164)",
                "at org.apache.hadoop.mapred.YarnChild.configureLocalDirs(YarnChild.java:235)",
                "at org.apache.hadoop.mapred.YarnChild.configureTask(YarnChild.java:294)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:143)"
            ],
            "RootCause": "The root cause of the issue is the reliance on the LD_LIBRARY_PATH environment variable for loading native libraries in a Windows environment, which is not supported. This leads to the UnsatisfiedLinkError when the system attempts to access native methods that cannot be found.",
            "StepsToReproduce": [
                "Set up a Hadoop environment on a Windows machine.",
                "Configure the mapreduce.admin.user.env setting to include LD_LIBRARY_PATH.",
                "Run a MapReduce job that requires access to native libraries."
            ],
            "ExpectedBehavior": "The MapReduce job should execute successfully without errors related to native library loading, allowing access to all required native methods.",
            "ObservedBehavior": "The MapReduce job fails with an UnsatisfiedLinkError, indicating that the native method cannot be found due to incorrect library path configuration.",
            "Suggestions": "To resolve this issue, the configuration should be modified to use the PATH variable instead of LD_LIBRARY_PATH for Windows environments. Additionally, ensure that the HADOOP_HOME and PATH environment variables are correctly set to include the directory where the native libraries are located.",
            "problem_location": {
                "files": [
                    "NativeIO.java",
                    "DiskChecker.java",
                    "LocalDirAllocator.java",
                    "YarnChild.java"
                ],
                "classes": [
                    "org.apache.hadoop.io.nativeio.NativeIO",
                    "org.apache.hadoop.util.DiskChecker",
                    "org.apache.hadoop.fs.LocalDirAllocator",
                    "org.apache.hadoop.mapred.YarnChild"
                ],
                "methods": [
                    "NativeIO.access0",
                    "DiskChecker.checkAccessByFileMethods",
                    "LocalDirAllocator.getLocalPathToRead",
                    "YarnChild.configureLocalDirs"
                ]
            },
            "possible_fix": "Modify the MapReduce configuration to use the PATH variable for native library loading on Windows. Update the relevant code sections to check the operating system and set the library path accordingly."
        },
        "possible_fix_code": {
            "YarnChild.configureLocalDirs": "  private static void configureLocalDirs(Task task, JobConf job) throws IOException {\n    String[] localSysDirs = StringUtils.getTrimmedStrings(\n        System.getenv(Environment.LOCAL_DIRS.name()));\n    job.setStrings(MRConfig.LOCAL_DIR, localSysDirs);\n    LOG.info(MRConfig.LOCAL_DIR + \" for child: \" + job.get(MRConfig.LOCAL_DIR));\n    LocalDirAllocator lDirAlloc = new LocalDirAllocator(MRConfig.LOCAL_DIR);\n    Path workDir = null;\n    // First, try to find the JOB_LOCAL_DIR on this host.\n    try {\n      workDir = lDirAlloc.getLocalPathToRead(\"work\", job);\n    } catch (DiskErrorException e) {\n      // DiskErrorException means dir not found. If not found, it will\n      // be created below.\n    }\n    if (workDir == null) {\n      // JOB_LOCAL_DIR doesn't exist on this host -- Create it.\n      workDir = lDirAlloc.getLocalPathForWrite(\"work\", job);\n      FileSystem lfs = FileSystem.getLocal(job).getRaw();\n      boolean madeDir = false;\n      try {\n        madeDir = lfs.mkdirs(workDir);\n      } catch (FileAlreadyExistsException e) {\n        // Since all tasks will be running in their own JVM, the race condition\n        // exists where multiple tasks could be trying to create this directory\n        // at the same time. If this task loses the race, it's okay because\n        // the directory already exists.\n        madeDir = true;\n        workDir = lDirAlloc.getLocalPathToRead(\"work\", job);\n      }\n      if (!madeDir) {\n          throw new IOException(\"Mkdirs failed to create \"\n              + workDir.toString());\n      }\n    }\n    // Set the library path for Windows\n    if (System.getProperty(\"os.name\").toLowerCase().contains(\"win\")) {\n        String hadoopCommonHome = System.getenv(\"HADOOP_COMMON_HOME\");\n        if (hadoopCommonHome != null) {\n            System.setProperty(\"java.library.path\", hadoopCommonHome);\n        }\n    }\n    job.set(MRJobConfig.JOB_LOCAL_DIR,workDir.toString());\n  }"
        }
    },
    {
        "filename": "MAPREDUCE-2463.json",
        "creation_time": "2011-05-02T06:35:07.000+0000",
        "bug_report": {
            "Title": "Job History files are not moving to done folder when job history location is HDFS",
            "Description": "When the configuration for 'mapreduce.jobtracker.jobhistory.location' is set to an HDFS path, the system fails to move completed job history files to the 'done' folder. This issue arises either during the initialization of the Job Tracker or after job completion, leading to an exception indicating a mismatch between the expected file system and the actual file system being used.",
            "StackTrace": [
                "2011-04-29 15:27:27,813 ERROR org.apache.hadoop.mapreduce.jobhistory.JobHistory: Unable to move history file to DONE folder.",
                "java.lang.IllegalArgumentException: Wrong FS: hdfs://10.18.52.146:9000/history/job_201104291518_0001_root, expected: file:///",
                "at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:402)",
                "at org.apache.hadoop.fs.RawLocalFileSystem.pathToFile(RawLocalFileSystem.java:58)",
                "at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:419)",
                "at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:294)",
                "at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:215)",
                "at org.apache.hadoop.fs.FileSystem.copyFromLocalFile(FileSystem.java:1516)",
                "at org.apache.hadoop.fs.FileSystem.copyFromLocalFile(FileSystem.java:1492)",
                "at org.apache.hadoop.fs.FileSystem.moveFromLocalFile(FileSystem.java:1482)",
                "at org.apache.hadoop.mapreduce.jobhistory.JobHistory.moveToDoneNow(JobHistory.java:348)",
                "at org.apache.hadoop.mapreduce.jobhistory.JobHistory.access$200(JobHistory.java:61)",
                "at org.apache.hadoop.mapreduce.jobhistory.JobHistory$1.run(JobHistory.java:439)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)",
                "at java.lang.Thread.run(Thread.java:619)"
            ],
            "RootCause": "The root cause of the issue is an IllegalArgumentException triggered by a mismatch between the expected file system (local) and the actual file system (HDFS) being used. The Job History component is attempting to move files to a local file system path while the job history location is configured to use HDFS.",
            "StepsToReproduce": [
                "Set the configuration 'mapreduce.jobtracker.jobhistory.location' to an HDFS path.",
                "Run a job that generates job history files.",
                "Check the 'done' folder for the job history files after job completion."
            ],
            "ExpectedBehavior": "The job history files should be successfully moved to the 'done' folder in the specified HDFS location after job completion.",
            "ObservedBehavior": "The job history files are not moved to the 'done' folder, and an error is logged indicating a file system mismatch.",
            "Suggestions": "Verify the configuration for 'mapreduce.jobtracker.jobhistory.location' to ensure it is correctly set for HDFS usage. Additionally, check the implementation of the Job History component to ensure it can handle HDFS paths appropriately.",
            "problem_location": {
                "files": [
                    "JobHistory.java"
                ],
                "classes": [
                    "org.apache.hadoop.mapreduce.jobhistory.JobHistory"
                ],
                "methods": [
                    "JobHistory.moveToDoneNow"
                ]
            },
            "possible_fix": "Modify the JobHistory class to correctly handle HDFS paths when moving job history files. Ensure that the file system type is checked and that appropriate methods for HDFS are used for file operations."
        },
        "possible_fix_code": {
            "JobHistory.moveToDoneNow": "public void moveToDoneNow() throws IOException {\n    // Get the current file system\n    FileSystem fs = FileSystem.get(getConf());\n    Path donePath = new Path(getConf().get(\"mapreduce.jobtracker.jobhistory.done.location\"));\n\n    // Check if the done path is in HDFS\n    if (!donePath.toString().startsWith(\"hdfs://\")) {\n        throw new IllegalArgumentException(\"Expected HDFS path for done location, but got: \" + donePath);\n    }\n\n    // Move the job history file to the done folder\n    Path historyFilePath = new Path(getConf().get(\"mapreduce.jobtracker.jobhistory.location\"));\n    if (fs.exists(historyFilePath)) {\n        fs.rename(historyFilePath, new Path(donePath, historyFilePath.getName()));\n    } else {\n        LOG.error(\"History file does not exist: \" + historyFilePath);\n    }\n}"
        }
    },
    {
        "filename": "MAPREDUCE-3531.json",
        "creation_time": "2011-12-12T05:42:24.000+0000",
        "bug_report": {
            "Title": "java.lang.IllegalArgumentException: Invalid key to HMAC computation in NODE_UPDATE causing RM to stop scheduling",
            "Description": "This bug report addresses an issue encountered in a Hadoop cluster where an `IllegalArgumentException` is thrown due to an invalid key used in HMAC computation during a `NODE_UPDATE` event. The problem arises when the ResourceManager (RM) fails to allocate resources for a submitted job, leading to the job not running. The stack trace indicates that the error occurs in the `SecretManager.createPassword` method, which is responsible for creating a password using a secret key. The root cause appears to be related to the key generation process in the `SecretManager` class.",
            "StackTrace": [
                "2011-12-01 11:56:25,200 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: nodeUpdate: <NMHost>:48490 clusterResources: memory: 3225600",
                "2011-12-01 11:56:25,202 ERROR org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Error in handling event type NODE_UPDATE to the scheduler",
                "java.lang.IllegalArgumentException: Invalid key to HMAC computation",
                "at org.apache.hadoop.security.token.SecretManager.createPassword(SecretManager.java:141)",
                "at org.apache.hadoop.yarn.server.security.ContainerTokenSecretManager.createPassword(ContainerTokenSecretManager.java:61)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.createContainer(LeafQueue.java:1108)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.getContainer(LeafQueue.java:1091)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainer(LeafQueue.java:1137)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignNodeLocalContainers(LeafQueue.java:1001)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainersOnNode(LeafQueue.java:973)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainers(LeafQueue.java:760)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.assignContainersToChildQueues(ParentQueue.java:583)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.assignContainers(ParentQueue.java:513)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.nodeUpdate(CapacityScheduler.java:569)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:611)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:77)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:294)",
                "at java.lang.Thread.run(Thread.java:619)",
                "Caused by: java.security.InvalidKeyException: Secret key expected",
                "at com.sun.crypto.provider.HmacCore.a(DashoA13*..)",
                "at com.sun.crypto.provider.HmacSHA1.engineInit(DashoA13*..)",
                "at javax.crypto.Mac.init(DashoA13*..)",
                "at org.apache.hadoop.security.token.SecretManager.createPassword(SecretManager.java:139)",
                "... 14 more"
            ],
            "RootCause": "The root cause of the issue is an `IllegalArgumentException` thrown in the `SecretManager.createPassword` method due to an invalid key being passed for HMAC computation. This is likely caused by an improperly generated or initialized secret key in the `SecretManager` class.",
            "StepsToReproduce": [
                "Start a Hadoop cluster with a capacity scheduler.",
                "Submit a large sleep job to the ResourceManager.",
                "Monitor the ResourceManager logs for errors related to NODE_UPDATE."
            ],
            "ExpectedBehavior": "The ResourceManager should successfully allocate resources for the submitted job without throwing any exceptions.",
            "ObservedBehavior": "The ResourceManager fails to allocate resources for the job, resulting in an `IllegalArgumentException` related to an invalid key for HMAC computation.",
            "Suggestions": "Check the initialization of the `keyGen` instance in the `SecretManager` class to ensure it is properly configured. Verify that the generated key meets the requirements for HMAC-SHA1. Investigate potential concurrency issues that may affect key generation.",
            "problem_location": {
                "files": [
                    "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/token/SecretManager.java"
                ],
                "classes": [
                    "org.apache.hadoop.security.token.SecretManager"
                ],
                "methods": [
                    "SecretManager.createPassword",
                    "SecretManager.generateSecret"
                ]
            },
            "possible_fix": "Ensure that the `keyGen` instance in the `SecretManager` is properly initialized with the correct algorithm and key size. Additionally, verify that the generated key is valid for HMAC-SHA1 and address any concurrency issues that may arise during key generation."
        },
        "possible_fix_code": {
            "SecretManager.generateSecret": "  protected SecretKey generateSecret() {\n    SecretKey key;\n    try {\n      synchronized (keyGen) {\n        keyGen.init(256); // Ensure keyGen is initialized with the correct key size\n        key = keyGen.generateKey();\n      }\n    } catch (Exception e) {\n      throw new IllegalStateException(\"Failed to generate secret key\", e);\n    }\n    return key;\n  }"
        }
    },
    {
        "filename": "MAPREDUCE-3931.json",
        "creation_time": "2012-02-27T22:30:59.000+0000",
        "bug_report": {
            "Title": "MR tasks failing due to changing timestamps on Resources to download",
            "Description": "During gridmix runs, tasks are intermittently failing due to an IOException related to a resource in HDFS. The error indicates that the resource 'job.jar' has changed on the source filesystem, leading to a mismatch between the expected and actual file sizes. This issue arises when the file is modified or replaced while a job is attempting to access it, causing the task to fail.",
            "StackTrace": [
                "java.io.IOException: Resource hdfs://hostname.com:8020/user/hadoop15/.staging/job_1330116323296_0140/job.jar changed on src filesystem (expected 2971811411, was 1330116705875)",
                "at org.apache.hadoop.yarn.util.FSDownload.copy(FSDownload.java:90)",
                "at org.apache.hadoop.yarn.util.FSDownload.access$000(FSDownload.java:49)",
                "at org.apache.hadoop.yarn.util.FSDownload$1.run(FSDownload.java:157)",
                "at org.apache.hadoop.yarn.util.FSDownload$1.run(FSDownload.java:155)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:396)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1177)",
                "at org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:153)",
                "at org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:49)",
                "at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:138)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)",
                "at java.lang.Thread.run(Thread.java:619)"
            ],
            "RootCause": "The root cause of the issue is a mismatch in the expected and actual sizes of the resource 'job.jar' in HDFS, which occurs when the file is modified or replaced while a job is trying to access it.",
            "StepsToReproduce": [
                "1. Start a gridmix run that utilizes the 'job.jar' resource.",
                "2. While the job is running, modify or replace the 'job.jar' file in HDFS.",
                "3. Observe the task failures due to IOException related to the resource size mismatch."
            ],
            "ExpectedBehavior": "The job should successfully access the 'job.jar' resource without any IOException, leading to successful task execution.",
            "ObservedBehavior": "Tasks fail with an IOException indicating that the resource 'job.jar' has changed on the source filesystem, resulting in a size mismatch.",
            "Suggestions": "1. Ensure that the 'job.jar' file is not modified or replaced while jobs are running. 2. Implement a retry mechanism for job submissions to handle transient file access issues. 3. Enable detailed logging to monitor changes to the file and capture the context of the error.",
            "problem_location": {
                "files": [
                    "FSDownload.java",
                    "ResourceLocalizationService.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.util.FSDownload",
                    "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService"
                ],
                "methods": [
                    "FSDownload.copy",
                    "ResourceLocalizationService.heartbeat"
                ]
            },
            "possible_fix": "To prevent this issue, ensure that the 'job.jar' file is stable during job execution. Consider implementing a locking mechanism or a versioning system to manage file access."
        },
        "possible_fix_code": {
            "FSDownload.copy": "  private Path copy(Path sCopy, Path dstdir) throws IOException {\n    FileSystem sourceFs = sCopy.getFileSystem(conf);\n    Path dCopy = new Path(dstdir, sCopy.getName() + \".tmp\");\n    FileStatus sStat = sourceFs.getFileStatus(sCopy);\n    if (sStat.getModificationTime() != resource.getTimestamp()) {\n      LOG.warn(\"Resource \" + sCopy + \" changed on src filesystem (expected \" + resource.getTimestamp() + \", was \" + sStat.getModificationTime() + \"). Retrying...\");\n      // Implementing a retry mechanism\n      for (int attempt = 0; attempt < 3; attempt++) {\n        try {\n          Thread.sleep(1000); // Wait before retrying\n          sStat = sourceFs.getFileStatus(sCopy);\n          if (sStat.getModificationTime() == resource.getTimestamp()) {\n            break; // Exit loop if the file is stable\n          }\n        } catch (InterruptedException e) {\n          Thread.currentThread().interrupt(); // Restore interrupted status\n          throw new IOException(\"Thread interrupted during retry\", e);\n        }\n      }\n      // Final check after retries\n      if (sStat.getModificationTime() != resource.getTimestamp()) {\n        throw new IOException(\"Resource \" + sCopy + \" changed on src filesystem (expected \" + resource.getTimestamp() + \", was \" + sStat.getModificationTime() + \");\");\n      }\n    }\n\n    sourceFs.copyToLocalFile(sCopy, dCopy);\n    return dCopy;\n  }"
        }
    },
    {
        "filename": "MAPREDUCE-4467.json",
        "creation_time": "2012-07-20T18:20:20.000+0000",
        "bug_report": {
            "Title": "IndexCache failures due to missing synchronization",
            "Description": "The `TestMRJobs.testSleepJob` fails intermittently due to a synchronization error in the `IndexCache` class. The root of the issue lies in the improper use of the `wait()` method on the `info` object within the `getIndexInformation` and `readIndexFileToCache` methods. The `IllegalMonitorStateException` is thrown when a thread attempts to call `wait()` without holding the intrinsic lock of the object. This problem arose after the removal of the `synchronized` keyword in a related change (MAPREDUCE-4384), which has led to inconsistent access to shared resources.",
            "StackTrace": [
                "2012-07-20 19:32:34,627 ERROR [New I/O server worker #2-1] mapred.ShuffleHandler (ShuffleHandler.java:exceptionCaught(528)) - Shuffle error:",
                "java.lang.IllegalMonitorStateException",
                "at java.lang.Object.wait(Native Method)",
                "at org.apache.hadoop.mapred.IndexCache.getIndexInformation(IndexCache.java:74)",
                "at org.apache.hadoop.mapred.ShuffleHandler$Shuffle.sendMapOutput(ShuffleHandler.java:471)",
                "at org.apache.hadoop.mapred.ShuffleHandler$Shuffle.messageReceived(ShuffleHandler.java:397)",
                "at org.jboss.netty.handler.stream.ChunkedWriteHandler.handleUpstream(ChunkedWriteHandler.java:148)",
                "at org.jboss.netty.handler.codec.http.HttpChunkAggregator.messageReceived(HttpChunkAggregator.java:116)",
                "at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302)",
                "at org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndfireMessageReceived(ReplayingDecoder.java:522)",
                "at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:506)",
                "at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:443)",
                "at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:274)",
                "at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:261)",
                "at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:349)",
                "at org.jboss.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:280)",
                "at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:200)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)",
                "at java.lang.Thread.run(Thread.java:662)"
            ],
            "RootCause": "The `IllegalMonitorStateException` occurs because the `wait()` method is called on the `info` object without holding its intrinsic lock. This is due to the absence of a `synchronized` block around the `wait()` calls in both the `getIndexInformation` and `readIndexFileToCache` methods.",
            "StepsToReproduce": [
                "Run the `TestMRJobs.testSleepJob` test case.",
                "Observe the intermittent failures due to synchronization issues in the `IndexCache` class."
            ],
            "ExpectedBehavior": "The `testSleepJob` should complete successfully without throwing an `IllegalMonitorStateException`, indicating that the synchronization around the `info` object is correctly implemented.",
            "ObservedBehavior": "The test fails randomly with an `IllegalMonitorStateException`, indicating that the thread is attempting to wait on the `info` object without holding the necessary lock.",
            "Suggestions": "Wrap the `wait()` calls in both the `getIndexInformation` and `readIndexFileToCache` methods within a `synchronized` block that locks on the `info` object to ensure proper synchronization.",
            "problem_location": {
                "files": [
                    "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/IndexCache.java"
                ],
                "classes": [
                    "org.apache.hadoop.mapred.IndexCache"
                ],
                "methods": [
                    "IndexCache.getIndexInformation",
                    "IndexCache.readIndexFileToCache"
                ]
            },
            "possible_fix": "Enclose the `wait()` calls in `getIndexInformation` and `readIndexFileToCache` methods within a `synchronized(info)` block to ensure that the thread holds the lock on `info` before calling `wait()`. Example fix for `getIndexInformation`:\n\n```java\nwhile (isUnderConstruction(info)) {\n    synchronized(info) {\n        info.wait(200);\n    }\n}\n```"
        },
        "possible_fix_code": {
            "IndexCache.getIndexInformation": "  public IndexRecord getIndexInformation(String mapId, int reduce,\n                                         Path fileName, String expectedIndexOwner)\n    throws IOException {\n\n    IndexInformation info = cache.get(mapId);\n\n    if (info == null) {\n      info = readIndexFileToCache(fileName, mapId, expectedIndexOwner);\n    } else {\n      while (isUnderConstruction(info)) {\n        synchronized(info) {\n          try {\n            // In case the entry is ready after the above check but\n            // before the following wait, we do timed wait.\n            info.wait(200);\n          } catch (InterruptedException e) {\n            throw new IOException(\"Interrupted waiting for construction\", e);\n          }\n        }\n      }\n      LOG.debug(\"IndexCache HIT: MapId \" + mapId + \" found\");\n    }\n\n    if (info.mapSpillRecord.size() == 0 ||\n        info.mapSpillRecord.size() <= reduce) {\n      throw new IOException(\"Invalid request \" +\n        \" Map Id = \" + mapId + \" Reducer = \" + reduce +\n        \" Index Info Length = \" + info.mapSpillRecord.size());\n    }\n    return info.mapSpillRecord.getIndex(reduce);\n  }",
            "IndexCache.readIndexFileToCache": "  private IndexInformation readIndexFileToCache(Path indexFileName,\n                                                String mapId,\n                                                String expectedIndexOwner)\n    throws IOException {\n    IndexInformation info;\n    IndexInformation newInd = new IndexInformation();\n    if ((info = cache.putIfAbsent(mapId, newInd)) != null) {\n      while (isUnderConstruction(info)) {\n        synchronized(info) {\n          try {\n            // In case the entry is ready after the above check but\n            // before the following wait, we do timed wait.\n            info.wait(200);\n          } catch (InterruptedException e) {\n            throw new IOException(\"Interrupted waiting for construction\", e);\n          }\n        }\n      }\n      LOG.debug(\"IndexCache HIT: MapId \" + mapId + \" found\");\n      return info;\n    }\n    LOG.debug(\"IndexCache MISS: MapId \" + mapId + \" not found\") ;\n    SpillRecord tmp = null;\n    try { \n      tmp = new SpillRecord(indexFileName, conf, expectedIndexOwner);\n    } catch (Throwable e) { \n      tmp = new SpillRecord(0);\n      cache.remove(mapId);\n      throw new IOException(\"Error Reading IndexFile\", e);\n    } finally { \n      synchronized (newInd) { \n        newInd.mapSpillRecord = tmp;\n        newInd.notifyAll();\n      } \n    } \n    queue.add(mapId);\n    \n    if (totalMemoryUsed.addAndGet(newInd.getSize()) > totalMemoryAllowed) {\n      freeIndexInformation();\n    }\n    return newInd;\n  }"
        }
    },
    {
        "filename": "MAPREDUCE-3463.json",
        "creation_time": "2011-11-23T13:45:46.000+0000",
        "bug_report": {
            "Title": "Second AM fails to recover properly when first AM is killed with java.lang.IllegalArgumentException causing lost job",
            "Description": "The issue arises in a Hadoop YARN environment where the second Application Master (AM) fails to recover after the first AM is forcefully terminated. The configuration 'yarn.resourcemanager.am.max-retries' is set to 5, and the job was initially running successfully until the first AM was killed. Upon termination, the client encounters an 'Unknown job' error and an 'IllegalArgumentException' related to an invalid NodeId format. The logs indicate that the second AM was launched but subsequently shut down due to the invalid NodeId.",
            "StackTrace": [
                "org.apache.hadoop.yarn.exceptions.impl.pb.YarnRemoteExceptionPBImpl: Unknown job job_1322040898409_0005",
                "java.lang.IllegalArgumentException: Invalid NodeId [<NMHostName>]. Expected host:port",
                "at org.apache.hadoop.yarn.util.ConverterUtils.toNodeId(ConverterUtils.java:144)",
                "at org.apache.hadoop.mapreduce.v2.app.recover.RecoveryService$InterceptingEventHandler.sendAssignedEvent(RecoveryService.java:410)",
                "at org.apache.hadoop.mapreduce.v2.app.recover.RecoveryService$InterceptingEventHandler.handle(RecoveryService.java:314)"
            ],
            "RootCause": "The root cause of the issue is the invalid NodeId format being passed to the RecoveryService, which is expected to be in the format 'host:port'. This leads to the failure of the second AM to recover properly after the first AM is killed.",
            "StepsToReproduce": [
                "Set 'yarn.resourcemanager.am.max-retries' to 5 in 'yarn-site.xml'.",
                "Start a 4 Node YARN cluster.",
                "Run a job (e.g., Randowriter/Sort/Sort-validate) successfully.",
                "While the job is running (approximately 50% complete), kill the first Application Master using 'kill -9'.",
                "Observe the client logs for errors related to the job and AM recovery."
            ],
            "ExpectedBehavior": "The second Application Master should successfully take over and continue the job execution without errors, even after the first AM is killed.",
            "ObservedBehavior": "The client fails with an 'Unknown job' error and the second AM shuts down due to an 'IllegalArgumentException' regarding an invalid NodeId format.",
            "Suggestions": "Verify the job ID to ensure it is still active in the YARN ResourceManager. Check the configuration files for YARN to ensure that all nodes are correctly defined with the proper host and port format. Review the application code to ensure that NodeIds are being generated and passed correctly.",
            "problem_location": {
                "files": [
                    "RecoveryService.java",
                    "ConverterUtils.java"
                ],
                "classes": [
                    "org.apache.hadoop.mapreduce.v2.app.recover.RecoveryService",
                    "org.apache.hadoop.yarn.util.ConverterUtils"
                ],
                "methods": [
                    "RecoveryService.sendAssignedEvent",
                    "ConverterUtils.toNodeId"
                ]
            },
            "possible_fix": "Ensure that the NodeId is generated in the correct format 'host:port' before being passed to the RecoveryService. This may involve modifying the code in the relevant methods to validate and format the NodeId correctly."
        },
        "possible_fix_code": {
            "RecoveryService.sendAssignedEvent": "    private void sendAssignedEvent(TaskAttemptId yarnAttemptID,\n        TaskAttemptInfo attemptInfo) {\n      LOG.info(\"Sending assigned event to \" + yarnAttemptID);\n      ContainerId cId = attemptInfo.getContainerId();\n\n      // Validate and format the NodeId before using it\n      String hostname = attemptInfo.getHostname();\n      if (hostname == null || hostname.isEmpty()) {\n          throw new IllegalArgumentException(\"Invalid NodeId: Hostname is null or empty\");\n      }\n      NodeId nodeId = ConverterUtils.toNodeId(attemptInfo.getHostname());\n\n      // Resource/Priority/ApplicationACLs are only needed while launching the\n      // container on an NM, these are already completed tasks, so setting them\n      // to null\n      Container container = BuilderUtils.newContainer(cId, nodeId,\n          attemptInfo.getTrackerName() + \":\" + attemptInfo.getHttpPort(),\n          null, null, null);\n      actualHandler.handle(new TaskAttemptContainerAssignedEvent(yarnAttemptID,\n          container, null));\n    }"
        }
    },
    {
        "filename": "MAPREDUCE-4164.json",
        "creation_time": "2012-04-17T21:30:13.000+0000",
        "bug_report": {
            "Title": "Hadoop 22 Exception thrown after task completion causes its reexecution",
            "Description": "The issue arises when a Hadoop task completes, but an IOException occurs due to a ClosedByInterruptException. This exception indicates that a thread performing I/O operations was interrupted, leading to the closure of the communication channel. The stack trace shows that this happens during a status update call from the TaskReporter, which is part of the Hadoop IPC client. The problem seems to stem from the handling of interruptions during I/O operations, particularly in the Client and Connection classes.",
            "StackTrace": [
                "2012-02-28 19:17:08,504 INFO org.apache.hadoop.mapred.Merger: Down to the last merge-pass, with 3 segments left of total size: 1969310 bytes",
                "2012-02-28 19:17:08,694 INFO org.apache.hadoop.mapred.Task: Task:attempt_201202272306_0794_m_000094_0 is done. And is in the process of commiting",
                "2012-02-28 19:18:08,774 INFO org.apache.hadoop.mapred.Task: Communication exception: java.io.IOException: Call to /127.0.0.1:35400 failed on local exception: java.nio.channels.ClosedByInterruptException",
                "at org.apache.hadoop.ipc.Client.wrapException(Client.java:1094)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1062)",
                "at org.apache.hadoop.ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:198)",
                "at $Proxy0.statusUpdate(Unknown Source)",
                "at org.apache.hadoop.mapred.Task$TaskReporter.run(Task.java:650)",
                "at java.lang.Thread.run(Thread.java:662)",
                "Caused by: java.nio.channels.ClosedByInterruptException",
                "at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:184)",
                "at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:341)",
                "at org.apache.hadoop.net.SocketOutputStream$Writer.performIO(SocketOutputStream.java:60)",
                "at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)",
                "at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:151)",
                "at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:112)",
                "at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65)",
                "at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:123)",
                "at java.io.DataOutputStream.flush(DataOutputStream.java:106)",
                "at org.apache.hadoop.ipc.Client$Connection.sendParam(Client.java:769)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1040)"
            ],
            "RootCause": "The root cause of the issue is a ClosedByInterruptException that occurs when a thread performing I/O operations is interrupted. This leads to the closure of the communication channel during a status update call in the Hadoop IPC client.",
            "StepsToReproduce": [
                "1. Start a Hadoop task that performs I/O operations.",
                "2. Ensure that the task completes successfully.",
                "3. Interrupt the thread performing the I/O operation during the status update phase."
            ],
            "ExpectedBehavior": "The task should complete without any exceptions, and the status update should be sent successfully without interruptions.",
            "ObservedBehavior": "An IOException occurs due to a ClosedByInterruptException, causing the task to be re-executed.",
            "Suggestions": "Consider implementing better handling of thread interruptions in the Client and Connection classes to prevent ClosedByInterruptException during I/O operations. Ensure that I/O operations can gracefully handle interruptions.",
            "problem_location": {
                "files": [
                    "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Client.java",
                    "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Connection.java"
                ],
                "classes": [
                    "org.apache.hadoop.ipc.Client",
                    "org.apache.hadoop.ipc.Connection"
                ],
                "methods": [
                    "Client.call",
                    "Client.sendParam",
                    "Connection.sendParam",
                    "Client.stop"
                ]
            },
            "possible_fix": "To prevent the ClosedByInterruptException, modify the Client.call and Connection.sendParam methods to include explicit handling for InterruptedException. This may involve checking for interruptions before performing I/O operations and ensuring that the operations can be safely retried or aborted."
        },
        "possible_fix_code": {
            "org.apache.hadoop.ipc.Client.call": "  public Writable[] call(Writable[] params, InetSocketAddress[] addresses,\n      Class<?> protocol, UserGroupInformation ticket, Configuration conf)\n      throws IOException, InterruptedException {\n    if (addresses.length == 0) return new Writable[0];\n\n    ParallelResults results = new ParallelResults(params.length);\n    synchronized (results) {\n      for (int i = 0; i < params.length; i++) {\n        ParallelCall call = new ParallelCall(params[i], results, i);\n        try {\n          ConnectionId remoteId = ConnectionId.getConnectionId(addresses[i],\n              protocol, ticket, 0, conf);\n          Connection connection = getConnection(remoteId, call);\n          connection.sendParam(call);             // send each parameter\n        } catch (IOException e) {\n          // log errors\n          LOG.info(\"Calling \"+addresses[i]+\" caught: \" + \n                   e.getMessage(),e);\n          results.size--;                         //  wait for one fewer result\n        }\n      }\n      while (results.count != results.size) {\n        try {\n          results.wait();                    // wait for all results\n        } catch (InterruptedException e) {\n          // Handle interruption gracefully\n          Thread.currentThread().interrupt(); // Restore interrupted status\n          throw new InterruptedException(\"Call interrupted while waiting for results\");\n        }\n      }\n\n      return results.values;\n    }\n  }",
            "org.apache.hadoop.ipc.Client.sendParam": "    public void sendParam(Call call) {\n      if (shouldCloseConnection.get()) {\n        return;\n      }\n\n      DataOutputBuffer d=null;\n      try {\n        synchronized (this.out) {\n          if (LOG.isDebugEnabled())\n            LOG.debug(getName() + \" sending #\" + call.id);\n          \n          //for serializing the\n          //data to be written\n          d = new DataOutputBuffer();\n          d.writeInt(0); // placeholder for data length\n          RpcPayloadHeader header = new RpcPayloadHeader(\n              call.rpcKind, RpcPayloadOperation.RPC_FINAL_PAYLOAD, call.id);\n          header.write(d);\n          call.rpcRequest.write(d);\n          byte[] data = d.getData();\n          int dataLength = d.getLength() - 4;\n          data[0] = (byte)((dataLength >>> 24) & 0xff);\n          data[1] = (byte)((dataLength >>> 16) & 0xff);\n          data[2] = (byte)((dataLength >>> 8) & 0xff);\n          data[3] = (byte)(dataLength & 0xff);\n          out.write(data, 0, dataLength + 4);//write the data\n          out.flush();\n        }\n      } catch(IOException e) {\n        markClosed(e);\n      } finally {\n        //the buffer is just an in-memory buffer, but it is still polite to\n        // close early\n        IOUtils.closeStream(d);\n      }\n    }"
        }
    },
    {
        "filename": "MAPREDUCE-5260.json",
        "creation_time": "2013-05-20T05:43:32.000+0000",
        "bug_report": {
            "Title": "Job failed because of JvmManager running into inconsistent state",
            "Description": "Jobs in the cluster are failing due to a `NullPointerException` occurring in the `JvmManager` class, specifically within the `getDetails` method. This exception is causing the `TaskRunner` to fail in executing tasks, leading to inconsistent states in the job management. The stack trace indicates that the error arises during the JVM initialization process, where the `JvmManager` attempts to reap JVMs and launch new ones, but encounters null references.",
            "StackTrace": [
                "java.lang.Throwable: Child Error",
                "\tat org.apache.hadoop.mapred.TaskRunner.run(TaskRunner.java:271)",
                "Caused by: java.lang.NullPointerException",
                "\tat org.apache.hadoop.mapred.JvmManager$JvmManagerForType.getDetails(JvmManager.java:402)",
                "\tat org.apache.hadoop.mapred.JvmManager$JvmManagerForType.reapJvm(JvmManager.java:387)",
                "\tat org.apache.hadoop.mapred.JvmManager$JvmManagerForType.access$000(JvmManager.java:192)",
                "\tat org.apache.hadoop.mapred.JvmManager.launchJvm(JvmManager.java:125)",
                "\tat org.apache.hadoop.mapred.TaskRunner.launchJvmAndWait(TaskRunner.java:292)",
                "\tat org.apache.hadoop.mapred.TaskRunner.run(TaskRunner.java:251)"
            ],
            "RootCause": "The root cause of the issue is a `NullPointerException` in the `JvmManager` class, specifically when attempting to access details of a JVM that may not have been properly initialized or is in an inconsistent state.",
            "StepsToReproduce": [
                "1. Deploy a job in the Hadoop cluster.",
                "2. Monitor the job execution and initialization of tasks.",
                "3. Observe the logs for any `NullPointerException` related to `JvmManager`."
            ],
            "ExpectedBehavior": "The job should initialize tasks successfully without encountering any exceptions, and the `JvmManager` should manage JVM instances without errors.",
            "ObservedBehavior": "Jobs fail to initialize tasks due to a `NullPointerException` in the `JvmManager`, leading to inconsistent states and task failures.",
            "Suggestions": "To mitigate this issue, check for null references in the `JvmManager` class, particularly in the `getDetails` and `reapJvm` methods. Implement logging to identify which variables are null before they are accessed. Ensure that all necessary configurations for the Hadoop environment are correctly set.",
            "problem_location": {
                "files": [
                    "JvmManager.java",
                    "TaskRunner.java"
                ],
                "classes": [
                    "org.apache.hadoop.mapred.JvmManager",
                    "org.apache.hadoop.mapred.TaskRunner"
                ],
                "methods": [
                    "JvmManager.getDetails",
                    "JvmManager.reapJvm",
                    "TaskRunner.run"
                ]
            },
            "possible_fix": "Review the `JvmManager` class to ensure that all variables are properly initialized before use. Consider adding null checks in the `getDetails` and `reapJvm` methods to prevent the `NullPointerException` from occurring."
        },
        "possible_fix_code": {
            "JvmManager.getDetails": "public class JvmManager {\n    // Other methods and fields...\n\n    public JvmDetails getDetails() {\n        // Assuming there's a field that holds JVM details\n        if (jvmDetails == null) {\n            // Log a warning or throw an exception\n            System.err.println(\"Warning: JVM details are not initialized.\");\n            return null; // or throw new IllegalStateException(\"JVM details not initialized\");\n        }\n        return jvmDetails;\n    }\n\n    // Other methods...\n}\n",
            "JvmManager.reapJvm": "public class JvmManager {\n    // Other methods and fields...\n\n    public void reapJvm() {\n        // Assuming there's a field that holds a list of JVMs\n        if (jvmList == null || jvmList.isEmpty()) {\n            // Log a warning or handle the empty state\n            System.err.println(\"Warning: No JVMs to reap.\");\n            return;\n        }\n        // Logic to reap JVMs...\n    }\n\n    // Other methods...\n}\n"
        }
    },
    {
        "filename": "MAPREDUCE-4774.json",
        "creation_time": "2012-11-06T11:28:43.000+0000",
        "bug_report": {
            "Title": "JobImpl does not handle asynchronous task events in FAILED state",
            "Description": "The test `org.apache.hadoop.mapred.TestClusterMRNotification.testMR` frequently fails in the mapred build, indicating issues with job status notifications received through HTTP Servlet. The test runs three jobs: successful, killed, and failed, expecting the servlet to receive notifications in a specific order. However, the actual notifications differ from the expected ones, particularly when a job fails. The root cause appears to be an invalid job state transition when a task fails, leading to an `InvalidStateTransitionException` during event processing.",
            "StackTrace": [
                "2012-11-06 12:22:02,335 ERROR [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: Can't handle this event at current state",
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: JOB_TASK_ATTEMPT_COMPLETED at FAILED",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:309)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$3(StateMachineFactory.java:290)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:454)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:716)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:1)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:917)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:1)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:130)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:79)",
                "at java.lang.Thread.run(Thread.java:662)"
            ],
            "RootCause": "The root cause of the issue is an invalid state transition where the event `JOB_TASK_ATTEMPT_COMPLETED` is processed while the job is in the `FAILED` state, which is not allowed according to the event transition map.",
            "StepsToReproduce": [
                "Run the test `org.apache.hadoop.mapred.TestClusterMRNotification.testMR`.",
                "Ensure that the test executes three jobs: one successful, one killed, and one that fails.",
                "Observe the notifications received by the servlet and compare them with the expected notifications."
            ],
            "ExpectedBehavior": "The servlet should receive notifications in the expected order for each job state: success, killed, and failed, without any invalid transitions.",
            "ObservedBehavior": "The servlet receives notifications that do not match the expected order, particularly receiving an 'ERROR' status instead of 'FAILED' for the job that fails.",
            "Suggestions": "Review the state transition logic in the JobImpl class to ensure that the event `JOB_TASK_ATTEMPT_COMPLETED` is not processed when the job is in the `FAILED` state. Implement checks to prevent invalid transitions.",
            "problem_location": {
                "files": [
                    "JobImpl.java",
                    "TaskImpl.java"
                ],
                "classes": [
                    "org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl",
                    "org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl"
                ],
                "methods": [
                    "JobImpl.handle",
                    "TaskImpl.handleTaskAttemptCompletion"
                ]
            },
            "possible_fix": "Add a condition in the `JobImpl.handle` method to check the current state of the job before processing the `JOB_TASK_ATTEMPT_COMPLETED` event. If the job is in the `FAILED` state, skip processing this event."
        },
        "possible_fix_code": {
            "JobImpl.handle": "  public void handle(JobEvent event) {\n    LOG.debug(\"Processing \" + event.getJobId() + \" of type \" + event.getType());\n    try {\n      writeLock.lock();\n      JobStateInternal oldState = getInternalState();\n      // Check if the job is in FAILED state before processing the event\n      if (oldState == JobStateInternal.FAILED && event.getType() == JobEventType.JOB_TASK_ATTEMPT_COMPLETED) {\n        LOG.warn(\"Skipping JOB_TASK_ATTEMPT_COMPLETED event for job in FAILED state: \" + this.jobId);\n        return; // Skip processing this event\n      }\n      try {\n         getStateMachine().doTransition(event.getType(), event);\n      } catch (InvalidStateTransitonException e) {\n        LOG.error(\"Can't handle this event at current state\", e);\n        addDiagnostic(\"Invalid event \" + event.getType() + \n            \" on Job \" + this.jobId);\n        eventHandler.handle(new JobEvent(this.jobId,\n            JobEventType.INTERNAL_ERROR));\n      }\n      //notify the eventhandler of state change\n      if (oldState != getInternalState()) {\n        LOG.info(jobId + \" Job Transitioned from \" + oldState + \" to \" +\n                 getInternalState());\n      }\n    } finally {\n      writeLock.unlock();\n    }\n  }"
        }
    },
    {
        "filename": "MAPREDUCE-3005.json",
        "creation_time": "2011-09-14T04:29:08.000+0000",
        "bug_report": {
            "Title": "MR app hangs because of a NPE in ResourceManager",
            "Description": "The application experiences intermittent hangs due to a NullPointerException (NPE) occurring in the ResourceManager component of the Hadoop YARN framework. This issue has been observed during sort runs on a large cluster, specifically in two out of five attempts. The stack trace indicates that the NPE arises when handling NODE_UPDATE events, particularly within the AppSchedulingInfo class methods responsible for resource allocation.",
            "StackTrace": [
                "2011-09-12 15:02:33,715 ERROR org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Error in handling event type NODE_UPDATE to the scheduler",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.allocateNodeLocal(AppSchedulingInfo.java:244)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.allocate(AppSchedulingInfo.java:206)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApp.allocate(SchedulerApp.java:230)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainer(LeafQueue.java:1120)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignNodeLocalContainers(LeafQueue.java:961)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainersOnNode(LeafQueue.java:933)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainers(LeafQueue.java:725)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.assignContainersToChildQueues(ParentQueue.java:577)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.assignContainers(ParentQueue.java:509)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.nodeUpdate(CapacityScheduler.java:579)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:620)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:75)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:266)",
                "at java.lang.Thread.run(Thread.java:619)"
            ],
            "RootCause": "The NullPointerException is likely caused by uninitialized or null values in the requests map or parameters passed to the methods in the AppSchedulingInfo class. Specifically, methods such as allocateNodeLocal, allocate, and getResourceRequest may return null or operate on null values, leading to the exception.",
            "StepsToReproduce": [
                "Run the MR application on a large cluster.",
                "Execute sort runs multiple times (at least five) to observe the intermittent hanging behavior.",
                "Monitor the logs for NODE_UPDATE events and check for NullPointerExceptions in the ResourceManager."
            ],
            "ExpectedBehavior": "The application should handle resource allocation without encountering NullPointerExceptions, allowing it to run smoothly without hanging.",
            "ObservedBehavior": "The application hangs intermittently due to a NullPointerException in the ResourceManager, specifically during the handling of NODE_UPDATE events.",
            "Suggestions": "Ensure proper initialization of the requests map and validate that all parameters passed to the methods in AppSchedulingInfo are non-null. Review the commented-out code in the allocate method to address potential issues with resource allocation.",
            "problem_location": {
                "files": [
                    "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/AppSchedulingInfo.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo",
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.LeafQueue"
                ],
                "methods": [
                    "AppSchedulingInfo.allocateNodeLocal",
                    "AppSchedulingInfo.allocate",
                    "AppSchedulingInfo.getResourceRequest",
                    "AppSchedulingInfo.getResource"
                ]
            },
            "possible_fix": "Review and modify the allocateNodeLocal and allocate methods to ensure that the requests map is properly initialized and that null checks are implemented before accessing its elements. For example, add checks to ensure that nodeLocalRequest and other parameters are not null before proceeding with their usage."
        },
        "possible_fix_code": {
            "AppSchedulingInfo.allocateNodeLocal": "  synchronized private void allocateNodeLocal(SchedulerNode node, Priority priority,\n      ResourceRequest nodeLocalRequest, Container container) {\n    if (nodeLocalRequest == null || this.requests.get(priority) == null) {\n        // Handle the null case appropriately, possibly logging an error or throwing an exception\n        LOG.error(\"Node local request or requests map is null for priority: \" + priority);\n        return;\n    }\n\n    // Update consumption and track allocations\n    allocate(container);\n\n    // Update future requirements\n    nodeLocalRequest.setNumContainers(nodeLocalRequest.getNumContainers() - 1);\n    if (nodeLocalRequest.getNumContainers() == 0) {\n      this.requests.get(priority).remove(node.getHostName());\n    }\n\n    ResourceRequest rackLocalRequest = requests.get(priority).get(\n        node.getRackName());\n    if (rackLocalRequest != null) {\n        rackLocalRequest.setNumContainers(rackLocalRequest.getNumContainers() - 1);\n        if (rackLocalRequest.getNumContainers() == 0) {\n          this.requests.get(priority).remove(node.getRackName());\n        }\n    }\n\n    // Do not remove ANY\n    ResourceRequest offSwitchRequest = requests.get(priority).get(\n        RMNode.ANY);\n    if (offSwitchRequest != null) {\n        offSwitchRequest.setNumContainers(offSwitchRequest.getNumContainers() - 1);\n    }\n  }"
        }
    },
    {
        "filename": "MAPREDUCE-6895.json",
        "creation_time": "2017-05-25T21:07:48.000+0000",
        "bug_report": {
            "Title": "Job end notification not sent due to YarnRuntimeException",
            "Description": "The issue arises when the MRAppMaster attempts to stop, leading to a YarnRuntimeException caused by a ClosedChannelException. This occurs during the handling of job history events in the Hadoop MapReduce framework, specifically within the JobHistoryEventHandler class. The stack trace indicates that the exception is thrown when the event writer is accessed after it has been closed, which prevents the job end notification from being sent.",
            "StackTrace": [
                "2017-05-24 12:14:02,165 WARN [Thread-693] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: Graceful stop failed",
                "org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.nio.channels.ClosedChannelException",
                "at org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.handleEvent(JobHistoryEventHandler.java:531)",
                "at org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.serviceStop(JobHistoryEventHandler.java:360)",
                "at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:221)",
                "at org.apache.hadoop.service.ServiceOperations.stop(ServiceOperations.java:52)",
                "at org.apache.hadoop.service.ServiceOperations.stopQuietly(ServiceOperations.java:80)",
                "at org.apache.hadoop.service.CompositeService.stop(CompositeService.java:157)",
                "at org.apache.hadoop.service.CompositeService.serviceStop(CompositeService.java:131)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.serviceStop(MRAppMaster.java:1476)",
                "at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:221)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.stop(MRAppMaster.java:1090)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.shutDownJob(MRAppMaster.java:554)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobFinishEventHandler$1.run(MRAppMaster.java:605)",
                "Caused by: java.nio.channels.ClosedChannelException",
                "at org.apache.hadoop.hdfs.DFSOutputStream.checkClosed(DFSOutputStream.java:1528)",
                "at org.apache.hadoop.fs.FSOutputSummer.write(FSOutputSummer.java:98)",
                "at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:58)",
                "at java.io.DataOutputStream.write(DataOutputStream.java:107)",
                "at org.codehaus.jackson.impl.Utf8Generator._flushBuffer(Utf8Generator.java:1754)",
                "at org.codehaus.jackson.impl.Utf8Generator.flush(Utf8Generator.java:1088)",
                "at org.apache.avro.io.JsonEncoder.flush(JsonEncoder.java:67)",
                "at org.apache.hadoop.mapreduce.jobhistory.EventWriter.write(EventWriter.java:67)",
                "at org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler$MetaInfo.writeEvent(JobHistoryEventHandler.java:886)",
                "at org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.handleEvent(JobHistoryEventHandler.java:520)"
            ],
            "RootCause": "The root cause of the issue is a YarnRuntimeException triggered by a ClosedChannelException. This occurs when the JobHistoryEventHandler attempts to write to an event writer that has already been closed, likely due to improper synchronization during job state transitions.",
            "StepsToReproduce": [
                "Submit a job to the Hadoop MapReduce framework.",
                "Allow the job to run until completion.",
                "Observe the job's shutdown process, particularly the stopping of the MRAppMaster."
            ],
            "ExpectedBehavior": "Upon job completion, the MRAppMaster should gracefully stop and send a job end notification without encountering any exceptions.",
            "ObservedBehavior": "The MRAppMaster fails to stop gracefully, resulting in a YarnRuntimeException and preventing the job end notification from being sent.",
            "Suggestions": "Implement proper synchronization mechanisms to manage the state of the event writer, ensuring it is not closed while still in use. Additionally, enhance error handling in the closeEventWriter and processDoneFiles methods to prevent race conditions.",
            "problem_location": {
                "files": [
                    "org/apache/hadoop/mapreduce/jobhistory/JobHistoryEventHandler.java"
                ],
                "classes": [
                    "org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler"
                ],
                "methods": [
                    "JobHistoryEventHandler.handleEvent",
                    "JobHistoryEventHandler.closeEventWriter",
                    "JobHistoryEventHandler.processDoneFiles"
                ]
            },
            "possible_fix": "Review the synchronization logic in the JobHistoryEventHandler class, particularly in the handleEvent, closeEventWriter, and processDoneFiles methods. Ensure that the event writer is only closed when it is safe to do so, and consider implementing a state check before writing events."
        },
        "possible_fix_code": {
            "JobHistoryEventHandler.handleEvent": "  public void handleEvent(JobHistoryEvent event) {\n    synchronized (lock) {\n\n      // If this is JobSubmitted Event, setup the writer\n      if (event.getHistoryEvent().getEventType() == EventType.AM_STARTED) {\n        try {\n          AMStartedEvent amStartedEvent =\n              (AMStartedEvent) event.getHistoryEvent();\n          setupEventWriter(event.getJobID(), amStartedEvent);\n        } catch (IOException ioe) {\n          LOG.error(\"Error JobHistoryEventHandler in handleEvent: \" + event,\n              ioe);\n          throw new YarnRuntimeException(ioe);\n        }\n      }\n\n      // For all events\n      // (1) Write it out\n      // (2) Process it for JobSummary\n      // (3) Process it for ATS (if enabled)\n      MetaInfo mi = fileMap.get(event.getJobID());\n      try {\n        HistoryEvent historyEvent = event.getHistoryEvent();\n        if (!(historyEvent instanceof NormalizedResourceEvent)) {\n          if (mi.isWriterActive()) { // Check if writer is active before writing\n            mi.writeEvent(historyEvent);\n          } else {\n            LOG.warn(\"Attempted to write to a closed event writer for JobID: \" + event.getJobID());\n          }\n        }\n        processEventForJobSummary(event.getHistoryEvent(), mi.getJobSummary(),\n            event.getJobID());\n        if (timelineV2Client != null) {\n          processEventForNewTimelineService(historyEvent, event.getJobID(),\n              event.getTimestamp());\n        } else if (timelineClient != null) {\n          processEventForTimelineServer(historyEvent, event.getJobID(),\n              event.getTimestamp());\n        }\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"In HistoryEventHandler \"\n              + event.getHistoryEvent().getEventType());\n        }\n      } catch (IOException e) {\n        LOG.error(\"Error writing History Event: \" + event.getHistoryEvent(),\n            e);\n        throw new YarnRuntimeException(e);\n      }\n\n      if (event.getHistoryEvent().getEventType() == EventType.JOB_SUBMITTED) {\n        JobSubmittedEvent jobSubmittedEvent =\n            (JobSubmittedEvent) event.getHistoryEvent();\n        mi.getJobIndexInfo().setSubmitTime(jobSubmittedEvent.getSubmitTime());\n        mi.getJobIndexInfo().setQueueName(jobSubmittedEvent.getJobQueueName());\n      }\n      //initialize the launchTime in the JobIndexInfo of MetaInfo\n      if(event.getHistoryEvent().getEventType() == EventType.JOB_INITED ){\n        JobInitedEvent jie = (JobInitedEvent) event.getHistoryEvent();\n        mi.getJobIndexInfo().setJobStartTime(jie.getLaunchTime());\n      }\n      \n      if (event.getHistoryEvent().getEventType() == EventType.JOB_QUEUE_CHANGED) {\n        JobQueueChangeEvent jQueueEvent =\n            (JobQueueChangeEvent) event.getHistoryEvent();\n        mi.getJobIndexInfo().setQueueName(jQueueEvent.getJobQueueName());\n      }\n\n      // If this is JobFinishedEvent, close the writer and setup the job-index\n      if (event.getHistoryEvent().getEventType() == EventType.JOB_FINISHED) {\n        try {\n          JobFinishedEvent jFinishedEvent =\n              (JobFinishedEvent) event.getHistoryEvent();\n          mi.getJobIndexInfo().setFinishTime(jFinishedEvent.getFinishTime());\n          mi.getJobIndexInfo().setNumMaps(jFinishedEvent.getFinishedMaps());\n          mi.getJobIndexInfo().setNumReduces(\n              jFinishedEvent.getFinishedReduces());\n          mi.getJobIndexInfo().setJobStatus(JobState.SUCCEEDED.toString());\n          closeEventWriter(event.getJobID());\n          processDoneFiles(event.getJobID());\n        } catch (IOException e) {\n          throw new YarnRuntimeException(e);\n        }\n      }\n      // In case of JOB_ERROR, only process all the Done files(e.g. job\n      // summary, job history file etc.) if it is last AM retry.\n      if (event.getHistoryEvent().getEventType() == EventType.JOB_ERROR) {\n        try {\n          JobUnsuccessfulCompletionEvent jucEvent =\n              (JobUnsuccessfulCompletionEvent) event.getHistoryEvent();\n          mi.getJobIndexInfo().setFinishTime(jucEvent.getFinishTime());\n          mi.getJobIndexInfo().setNumMaps(jucEvent.getFinishedMaps());\n          mi.getJobIndexInfo().setNumReduces(jucEvent.getFinishedReduces());\n          mi.getJobIndexInfo().setJobStatus(jucEvent.getStatus());\n          closeEventWriter(event.getJobID());\n          if(context.isLastAMRetry())\n            processDoneFiles(event.getJobID());\n        } catch (IOException e) {\n          throw new YarnRuntimeException(e);\n        }\n      }\n\n      if (event.getHistoryEvent().getEventType() == EventType.JOB_FAILED\n          || event.getHistoryEvent().getEventType() == EventType.JOB_KILLED) {\n        try {\n          JobUnsuccessfulCompletionEvent jucEvent = \n              (JobUnsuccessfulCompletionEvent) event\n              .getHistoryEvent();\n          mi.getJobIndexInfo().setFinishTime(jucEvent.getFinishTime());\n          mi.getJobIndexInfo().setNumMaps(jucEvent.getFinishedMaps());\n          mi.getJobIndexInfo().setNumReduces(jucEvent.getFinishedReduces());\n          mi.getJobIndexInfo().setJobStatus(jucEvent.getStatus());\n          closeEventWriter(event.getJobID());\n          processDoneFiles(event.getJobID());\n        } catch (IOException e) {\n          throw new YarnRuntimeException(e);\n        }\n      }\n    }\n  }"
        }
    },
    {
        "filename": "MAPREDUCE-4451.json",
        "creation_time": "2012-07-17T10:34:13.000+0000",
        "bug_report": {
            "Title": "FairScheduler fails to initialize job with Kerberos authentication configured",
            "Description": "The FairScheduler in Hadoop 1.0.3 encounters an issue during job initialization when Kerberos authentication is configured. The error indicates that the job initialization fails due to the absence of a valid Kerberos ticket-granting ticket (TGT). This is a common problem when the Kerberos setup is not properly configured or when the user has not obtained a valid TGT. The stack trace reveals that the failure occurs during the RPC call path, specifically when the JobTracker attempts to generate and store security tokens in HDFS.",
            "StackTrace": [
                "2012-07-17 15:15:09,220 ERROR org.apache.hadoop.mapred.JobTracker: Job initialization failed:",
                "java.io.IOException: Call to /192.168.7.80:8020 failed on local exception: java.io.IOException: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]",
                "at org.apache.hadoop.ipc.Client.wrapException(Client.java:1129)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1097)",
                "at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)",
                "at $Proxy7.getProtocolVersion(Unknown Source)",
                "at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:411)",
                "at org.apache.hadoop.hdfs.DFSClient.createRPCNamenode(DFSClient.java:125)",
                "at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:329)",
                "at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:294)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:100)",
                "at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:1411)",
                "at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:66)",
                "at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:1429)",
                "at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:254)",
                "at org.apache.hadoop.fs.Path.getFileSystem(Path.java:187)",
                "at org.apache.hadoop.security.Credentials.writeTokenStorageFile(Credentials.java:169)",
                "at org.apache.hadoop.mapred.JobInProgress.generateAndStoreTokens(JobInProgress.java:3558)",
                "at org.apache.hadoop.mapred.JobInProgress.initTasks(JobInProgress.java:696)",
                "at org.apache.hadoop.mapred.JobTracker.initJob(JobTracker.java:3911)",
                "at org.apache.hadoop.mapred.FairScheduler$JobInitializer$InitJob.run(FairScheduler.java:301)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)",
                "at java.lang.Thread.run(Thread.java:662)"
            ],
            "RootCause": "The root cause of the issue is the failure to obtain a valid Kerberos ticket-granting ticket (TGT) during the job initialization process. The JobTracker is attempting to use a UserGroupInformation (UGI) instance that does not have a valid TGT, leading to authentication failure.",
            "StepsToReproduce": [
                "Configure Kerberos authentication in Hadoop 1.0.3.",
                "Submit a job using FairScheduler.",
                "Observe the job initialization process."
            ],
            "ExpectedBehavior": "The job should initialize successfully, and the FairScheduler should be able to generate and store security tokens in HDFS without authentication errors.",
            "ObservedBehavior": "The job fails to initialize, and an error is logged indicating that no valid credentials were provided for Kerberos authentication.",
            "Suggestions": "1. Ensure that a valid Kerberos ticket-granting ticket (TGT) is obtained by running `kinit <username>`. 2. If using a keytab file, verify that it is correctly specified and accessible. 3. Check Hadoop configuration files (e.g., `core-site.xml`, `hdfs-site.xml`) for proper Kerberos settings. 4. Ensure necessary environment variables (like `KRB5_CONFIG`) are set correctly. 5. Confirm that the service principal for Hadoop services is correctly configured.",
            "problem_location": {
                "files": [
                    "JobTracker.java",
                    "JobInProgress.java",
                    "FairScheduler.java"
                ],
                "classes": [
                    "org.apache.hadoop.mapred.JobTracker",
                    "org.apache.hadoop.mapred.JobInProgress",
                    "org.apache.hadoop.mapred.FairScheduler"
                ],
                "methods": [
                    "JobTracker.initJob",
                    "JobInProgress.generateAndStoreTokens",
                    "FairScheduler$JobInitializer$InitJob.run"
                ]
            },
            "possible_fix": "To resolve the issue, ensure that the JobTracker uses a UserGroupInformation instance that has a valid TGT. This may involve modifying the authentication flow to ensure that the correct UGI is used during the job initialization process."
        },
        "possible_fix_code": {
            "JobTracker.initJob": "public void initJob(JobInProgress job) throws IOException {\n    // Ensure that the UserGroupInformation has a valid TGT before proceeding\n    UserGroupInformation ugi = UserGroupInformation.getLoginUser();\n    if (ugi == null || !ugi.hasKerberosCredentials()) {\n        throw new IOException(\"UserGroupInformation does not have valid Kerberos credentials.\");\n    }\n\n    // Proceed with job initialization\n    try {\n        // Existing job initialization logic\n        job.initTasks();\n        generateAndStoreTokens(job);\n    } catch (IOException e) {\n        LOG.error(\"Job initialization failed:\", e);\n        throw e;\n    }\n}"
        }
    },
    {
        "filename": "MAPREDUCE-4144.json",
        "creation_time": "2012-04-12T16:28:30.000+0000",
        "bug_report": {
            "Title": "ResourceManager NPE while handling NODE_UPDATE",
            "Description": "The ResourceManager in our YARN cluster has experienced multiple crashes due to a NullPointerException (NPE) when processing NODE_UPDATE events. The stack trace indicates that the NPE occurs in the `allocateNodeLocal` method of the `AppSchedulingInfo` class, which is invoked during the scheduling process. This issue appears to be related to uninitialized or improperly handled objects, particularly concerning the `SchedulerNode`, `SchedulerApp`, or `ResourceRequest` being `null` during resource allocation.",
            "StackTrace": [
                "2012-04-12 02:09:01,672 FATAL org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Error in handling event type NODE_UPDATE to the scheduler",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.allocateNodeLocal(AppSchedulingInfo.java:261)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.allocate(AppSchedulingInfo.java:223)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApp.allocate(SchedulerApp.java:246)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainer(LeafQueue.java:1229)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignNodeLocalContainers(LeafQueue.java:1078)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainersOnNode(LeafQueue.java:1048)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignReservedContainer(LeafQueue.java:859)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainers(LeafQueue.java:756)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.nodeUpdate(CapacityScheduler.java:573)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:622)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:78)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:302)",
                "at java.lang.Thread.run(Thread.java:619)"
            ],
            "RootCause": "The NullPointerException is likely caused by uninitialized or improperly handled objects in the scheduling process of the YARN ResourceManager, particularly related to `SchedulerNode`, `SchedulerApp`, or `ResourceRequest` being `null`.",
            "StepsToReproduce": [
                "Trigger a NODE_UPDATE event in the YARN ResourceManager.",
                "Monitor the ResourceManager logs for any NullPointerExceptions.",
                "Observe the conditions under which the NPE occurs, particularly focusing on the state of the SchedulerNode and associated applications."
            ],
            "ExpectedBehavior": "The ResourceManager should successfully handle NODE_UPDATE events without throwing exceptions, properly allocating resources to applications based on their requests.",
            "ObservedBehavior": "The ResourceManager crashes with a NullPointerException when attempting to process NODE_UPDATE events, leading to service interruptions.",
            "Suggestions": "Review the initialization of SchedulerNode and SchedulerApp objects to ensure they are not null when passed to the scheduling methods. Implement null checks in the `assignContainers` and `assignNodeLocalContainers` methods to prevent NPEs.",
            "problem_location": {
                "files": [
                    "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/AppSchedulingInfo.java",
                    "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
                    "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo",
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue",
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler"
                ],
                "methods": [
                    "AppSchedulingInfo.allocateNodeLocal",
                    "LeafQueue.assignNodeLocalContainers",
                    "LeafQueue.assignContainers",
                    "CapacityScheduler.nodeUpdate"
                ]
            },
            "possible_fix": "Add null checks in the `assignContainers` and `assignNodeLocalContainers` methods to handle cases where `SchedulerNode`, `SchedulerApp`, or `ResourceRequest` may be null. For example, in `assignNodeLocalContainers`, check if `request` is null before proceeding with allocation logic."
        },
        "possible_fix_code": {
            "AppSchedulingInfo.allocateNodeLocal": "  synchronized private void allocateNodeLocal(SchedulerNode node, Priority priority,\n      ResourceRequest nodeLocalRequest, Container container) {\n    if (node == null || priority == null || nodeLocalRequest == null || container == null) {\n        LOG.error(\"One of the parameters is null: node=\" + node + \", priority=\" + priority + \", nodeLocalRequest=\" + nodeLocalRequest + \", container=\" + container);\n        return;\n    }\n    // Update consumption and track allocations\n    allocate(container);\n\n    // Update future requirements\n    nodeLocalRequest.setNumContainers(nodeLocalRequest.getNumContainers() - 1);\n    if (nodeLocalRequest.getNumContainers() == 0) {\n      this.requests.get(priority).remove(node.getHostName());\n    }\n\n    ResourceRequest rackLocalRequest = requests.get(priority).get(\n        node.getRackName());\n    if (rackLocalRequest != null) {\n        rackLocalRequest.setNumContainers(rackLocalRequest.getNumContainers() - 1);\n        if (rackLocalRequest.getNumContainers() == 0) {\n          this.requests.get(priority).remove(node.getRackName());\n        }\n    }\n\n    decrementOutstanding(requests.get(priority).get(RMNode.ANY));\n  }"
        }
    },
    {
        "filename": "MAPREDUCE-5924.json",
        "creation_time": "2014-06-13T05:02:09.000+0000",
        "bug_report": {
            "Title": "Windows: Sort Job failed due to 'Invalid event: TA_COMMIT_PENDING at COMMIT_PENDING'",
            "Description": "The Sort job processing over 1GB of data encountered a failure due to an invalid state transition in the Hadoop YARN application. The error log indicates that the event `TA_COMMIT_PENDING` was not valid in the current state `COMMIT_PENDING`, leading to an `InvalidStateTransitonException`. This suggests a mismanagement of state transitions within the job's execution flow.",
            "StackTrace": [
                "2014-06-09 09:15:38,746 INFO [Socket Reader #1 for port 63415] SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for job_1402304714683_0002 (auth:SIMPLE)",
                "2014-06-09 09:15:38,750 INFO [IPC Server handler 13 on 63415] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Commit-pending state update from attempt_1402304714683_0002_r_000015_1000",
                "2014-06-09 09:15:38,751 ERROR [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: Can't handle this event at current state for attempt_1402304714683_0002_r_000015_1000",
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: TA_COMMIT_PENDING at COMMIT_PENDING",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:1058)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:145)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:1271)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:1263)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:173)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:106)",
                "at java.lang.Thread.run(Thread.java:722)",
                "2014-06-09 09:15:38,753 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: job_1402304714683_0002 Job Transitioned from RUNNING to ERROR"
            ],
            "RootCause": "The root cause of the issue is an invalid state transition in the Hadoop YARN state machine, where the event `TA_COMMIT_PENDING` is not permissible in the `COMMIT_PENDING` state. This indicates a flaw in the state management logic of the job execution.",
            "StepsToReproduce": [
                "1. Submit a Sort job with over 1GB of data to the Hadoop YARN cluster.",
                "2. Monitor the job execution and observe the state transitions.",
                "3. Check the logs for any state transition errors, particularly related to `TA_COMMIT_PENDING`."
            ],
            "ExpectedBehavior": "The Sort job should successfully process the data and transition through the appropriate states without encountering invalid state transitions.",
            "ObservedBehavior": "The Sort job fails with an error indicating an invalid event `TA_COMMIT_PENDING` while in the `COMMIT_PENDING` state, leading to a job state of ERROR.",
            "Suggestions": "Review the state transition logic in the Hadoop YARN codebase to ensure that the event `TA_COMMIT_PENDING` is only triggered in valid states. Consider adding validation checks before state transitions occur.",
            "problem_location": {
                "files": [
                    "StateMachineFactory.java",
                    "TaskAttemptImpl.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.state.StateMachineFactory",
                    "org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl"
                ],
                "methods": [
                    "StateMachineFactory.doTransition",
                    "TaskAttemptImpl.handle"
                ]
            },
            "possible_fix": "Investigate the state transition definitions in the Hadoop YARN codebase. Ensure that the `doTransition` method in the `StateMachineFactory` class correctly handles the `TA_COMMIT_PENDING` event only when in a valid state. Implement necessary checks to prevent invalid transitions."
        },
        "possible_fix_code": {
            "TaskAttemptImpl.handle": "  public void handle(TaskAttemptEvent event) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Processing \" + event.getTaskAttemptID() + \" of type \" + event.getType());\n    }\n    writeLock.lock();\n    try {\n      final TaskAttemptStateInternal oldState = getInternalState();\n      // Check if the event is valid for the current state\n      if (!isValidEventForCurrentState(event.getType())) {\n        LOG.error(\"Invalid event \" + event.getType() + \" for current state \" + oldState);\n        return; // Early exit to prevent invalid state transition\n      }\n      try {\n        stateMachine.doTransition(event.getType(), event);\n      } catch (InvalidStateTransitonException e) {\n        LOG.error(\"Can't handle this event at current state for \" + this.attemptId, e);\n        eventHandler.handle(new JobDiagnosticsUpdateEvent(this.attemptId.getTaskId().getJobId(), \"Invalid event \" + event.getType() + \" on TaskAttempt \" + this.attemptId));\n        eventHandler.handle(new JobEvent(this.attemptId.getTaskId().getJobId(), JobEventType.INTERNAL_ERROR));\n      }\n      if (oldState != getInternalState()) {\n          LOG.info(attemptId + \" TaskAttempt Transitioned from \" + oldState + \" to \" + getInternalState());\n      }\n    } finally {\n      writeLock.unlock();\n    }\n  }\n\n  private boolean isValidEventForCurrentState(TaskAttemptEventType eventType) {\n    // Implement logic to check if the eventType is valid for the current state\n    // This is a placeholder for the actual validation logic\n    return true; // Replace with actual validation\n  }"
        }
    },
    {
        "filename": "MAPREDUCE-3649.json",
        "creation_time": "2012-01-09T23:53:06.000+0000",
        "bug_report": {
            "Title": "Job End notification gives an error on calling back.",
            "Description": "The application encounters an `UnknownServiceException` when attempting to send a job end notification to a specified URL. This issue arises during the job completion process in a Hadoop MapReduce application, specifically within the `JobEndNotifier` class. The stack trace indicates that the error occurs due to a missing `Content-Type` header in the server's response, which is essential for the `URLConnection` to process the response correctly.",
            "StackTrace": [
                "2012-01-09 23:45:41,732 WARN [AsyncDispatcher event handler] org.mortbay.log: Job end notification to http://HOST:11000/oozie/v0/callback?id=0000000-120109234442311-oozie-oozi-W@mr-node&status=SUCCEEDED& failed",
                "java.net.UnknownServiceException: no content-type",
                "at java.net.URLConnection.getContentHandler(URLConnection.java:1192)",
                "at java.net.URLConnection.getContent(URLConnection.java:689)",
                "at org.apache.hadoop.mapreduce.v2.app.JobEndNotifier.notifyURLOnce(JobEndNotifier.java:95)",
                "at org.apache.hadoop.mapreduce.v2.app.JobEndNotifier.notify(JobEndNotifier.java:139)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobFinishEventHandler.handle(MRAppMaster.java:388)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobFinishEventHandler.handle(MRAppMaster.java:375)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:125)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:82)"
            ],
            "RootCause": "The root cause of the issue is the `UnknownServiceException` triggered by the absence of a `Content-Type` header in the server's response when the `notifyURLOnce` method in the `JobEndNotifier` class attempts to read the content from the URL.",
            "StepsToReproduce": [
                "1. Submit a job to the Hadoop MapReduce application.",
                "2. Wait for the job to complete.",
                "3. Observe the job end notification process that attempts to notify the specified URL."
            ],
            "ExpectedBehavior": "The job end notification should successfully send a request to the specified URL and receive a valid response with a `Content-Type` header, indicating the job has completed successfully.",
            "ObservedBehavior": "The job end notification fails with an `UnknownServiceException`, indicating that the server response does not include a `Content-Type` header.",
            "Suggestions": "Ensure that the server being notified is correctly configured to return a `Content-Type` header in its response. Additionally, review the configuration settings in the Hadoop application for any relevant keys related to URL notifications.",
            "problem_location": {
                "files": [
                    "hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/JobEndNotifier.java"
                ],
                "classes": [
                    "org.apache.hadoop.mapreduce.v2.app.JobEndNotifier"
                ],
                "methods": [
                    "JobEndNotifier.notifyURLOnce"
                ]
            },
            "possible_fix": "Modify the server configuration to ensure it returns a valid `Content-Type` header in the response. Additionally, consider adding error handling in the `notifyURLOnce` method to manage cases where the `Content-Type` is missing."
        },
        "possible_fix_code": {
            "JobEndNotifier.notifyURLOnce": "  protected boolean notifyURLOnce() {\n    boolean success = false;\n    try {\n      Log.info(\"Job end notification trying \" + urlToNotify);\n      URLConnection conn = urlToNotify.openConnection();\n      conn.setConnectTimeout(5*1000);\n      conn.setReadTimeout(5*1000);\n      conn.setAllowUserInteraction(false);\n\n      // Check for Content-Type header\n      String contentType = conn.getContentType();\n      if (contentType == null || contentType.isEmpty()) {\n        Log.warn(\"No Content-Type header received from the server.\");\n        return false; // Early exit if Content-Type is missing\n      }\n\n      InputStream is = conn.getInputStream();\n      conn.getContent();\n      is.close();\n      success = true;\n      Log.info(\"Job end notification to \" + urlToNotify + \" succeeded\");\n    } catch(IOException ioe) {\n      Log.warn(\"Job end notification to \" + urlToNotify + \" failed\", ioe);\n    }\n    return success;\n  }"
        }
    },
    {
        "filename": "MAPREDUCE-7077.json",
        "creation_time": "2018-04-11T18:35:34.000+0000",
        "bug_report": {
            "Title": "Pipe mapreduce job fails with Permission denied for jobTokenPassword",
            "Description": "The issue arises when launching a word count example using Hadoop pipes, which results in a failure due to a permission error. The application attempts to write to a specific file in the Hadoop YARN local user cache but encounters a `FileNotFoundException` with a 'Permission denied' message. This indicates that the user running the job lacks the necessary permissions to create or write to the specified file path.",
            "StackTrace": [
                "2018-02-02 02:40:51,071 ERROR [IPC Server handler 16 on 43391] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Task: attempt_1517534613368_0041_r_000000_2 - exited : java.io.FileNotFoundException: /grid/0/hadoop/yarn/local/usercache/hrt_qa/appcache/application_1517534613368_0041/jobTokenPassword (Permission denied)",
                "at java.io.FileOutputStream.open0(Native Method)",
                "at java.io.FileOutputStream.open(FileOutputStream.java:270)",
                "at java.io.FileOutputStream.<init>(FileOutputStream.java:213)",
                "at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:236)",
                "at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:219)",
                "at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:318)",
                "at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:307)",
                "at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:338)",
                "at org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:401)",
                "at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:464)",
                "at org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:443)",
                "at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1169)",
                "at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1149)",
                "at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1038)",
                "at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1026)",
                "at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:703)",
                "at org.apache.hadoop.mapred.pipes.Application.writePasswordToLocalFile(Application.java:173)",
                "at org.apache.hadoop.mapred.pipes.Application.<init>(Application.java:109)",
                "at org.apache.hadoop.mapred.pipes.PipesReducer.startApplication(PipesReducer.java:87)",
                "at org.apache.hadoop.mapred.pipes.PipesReducer.reduce(PipesReducer.java:65)",
                "at org.apache.hadoop.mapred.pipes.PipesReducer.reduce(PipesReducer.java:38)",
                "at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:445)",
                "at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:393)",
                "at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:174)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1965)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:168)"
            ],
            "RootCause": "The root cause of the issue is a lack of necessary permissions for the user running the Hadoop job to write to the specified file path in the YARN local user cache. This is confirmed by the `FileNotFoundException` indicating 'Permission denied'.",
            "StepsToReproduce": [
                "Launch the word count example using the following command:",
                "/usr/hdp/current/hadoop-client/bin/hadoop pipes \"-Dhadoop.pipes.java.recordreader=true\" \"-Dhadoop.pipes.java.recordwriter=true\" -input pipeInput -output pipeOutput -program bin/wordcount",
                "Observe the error message in the logs indicating a permission issue."
            ],
            "ExpectedBehavior": "The application should successfully create and write to the jobTokenPassword file in the specified directory without encountering permission errors.",
            "ObservedBehavior": "The application fails with a `FileNotFoundException` due to 'Permission denied' when attempting to write to the jobTokenPassword file.",
            "Suggestions": "Check the permissions of the directory `/grid/0/hadoop/yarn/local/usercache/hrt_qa/appcache/application_1517534613368_0041/` to ensure that the user has write access. If permissions are insufficient, use the `setPermission` method in the Hadoop `FileSystem` class to grant the necessary access.",
            "problem_location": {
                "files": [
                    "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystem"
                ],
                "classes": [
                    "org.apache.hadoop.fs.FileSystem"
                ],
                "methods": [
                    "getFileStatus",
                    "access",
                    "setPermission"
                ]
            },
            "possible_fix": "To resolve the permission issue, check the current permissions of the directory and use the `setPermission` method to grant write access to the user running the Hadoop job. Example code to set permissions: `fileSystem.setPermission(new Path(\"/grid/0/hadoop/yarn/local/usercache/hrt_qa/appcache/application_1517534613368_0041/\"), new FsPermission(FsAction.READ_WRITE));`"
        },
        "possible_fix_code": {
            "setPermission": "  public void setPermission(Path p, FsPermission permission) throws IOException {\n      // Check if the path exists\n      FileStatus fileStatus = getFileStatus(p);\n      if (fileStatus == null) {\n          throw new FileNotFoundException(\"Path does not exist: \" + p);\n      }\n      // Set the new permissions\n      // Assuming permission is a valid FsPermission object\n      // This is a placeholder for the actual implementation that sets permissions\n      // In a real implementation, you would interact with the underlying filesystem\n      // to change the permissions of the specified path.\n      // Example: nativeSetPermission(p, permission);\n  }"
        }
    },
    {
        "filename": "MAPREDUCE-4102.json",
        "creation_time": "2012-04-03T19:51:52.000+0000",
        "bug_report": {
            "Title": "Job counters not available in Jobhistory webui for killed jobs",
            "Description": "When running a job (e.g., a simple wordcount or sleep) and killing it before completion, accessing the 'Counters' link in the job history web UI results in a '500 error'. The job history log indicates a NullPointerException during the construction of the CountersBlock class, which is likely due to an invalid or uninitialized job ID being passed to the AppContext.",
            "StackTrace": [
                "Caused by: com.google.inject.ProvisionException: Guice provision errors:",
                "java.lang.reflect.InvocationTargetException",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "at java.lang.reflect.Method.invoke(Method.java:597)",
                "Caused by: java.lang.NullPointerException at org.apache.hadoop.mapreduce.counters.AbstractCounters.incrAllCounters(AbstractCounters.java:328)",
                "at org.apache.hadoop.mapreduce.v2.app.webapp.CountersBlock.getCounters(CountersBlock.java:188)",
                "at org.apache.hadoop.mapreduce.v2.app.webapp.CountersBlock.<init>(CountersBlock.java:57)"
            ],
            "RootCause": "The NullPointerException in the CountersBlock constructor is likely due to an invalid or uninitialized job ID being passed to the AppContext, resulting in ctx.getJob(jobID) returning null.",
            "StepsToReproduce": [
                "Run a simple wordcount or sleep job.",
                "Kill the job before it finishes.",
                "Access the job history web UI.",
                "Click on the 'Counters' link for the killed job."
            ],
            "ExpectedBehavior": "The job history web UI should display the counters for the killed job without errors.",
            "ObservedBehavior": "The job history web UI displays a '500 error' when attempting to access the counters for the killed job.",
            "Suggestions": "Ensure that the job ID being passed to CountersBlock is valid and corresponds to an existing job in the AppContext. Verify that the job is properly initialized and registered within the AppContext before the CountersBlock is constructed.",
            "problem_location": {
                "files": [
                    "CountersBlock.java",
                    "AbstractCounters.java",
                    "AppContext.java"
                ],
                "classes": [
                    "org.apache.hadoop.mapreduce.v2.app.webapp.CountersBlock",
                    "org.apache.hadoop.mapreduce.counters.AbstractCounters",
                    "org.apache.hadoop.mapreduce.v2.app.AppContext"
                ],
                "methods": [
                    "CountersBlock.init",
                    "AbstractCounters.incrAllCounters",
                    "AppContext.getJob"
                ]
            },
            "possible_fix": "Check the initialization of the job in the AppContext. Ensure that the job ID is valid and that the job is registered before the CountersBlock is constructed. If necessary, add null checks in the CountersBlock constructor to handle cases where the job may not be found."
        },
        "possible_fix_code": {
            "org.apache.hadoop.mapreduce.v2.app.webapp.CountersBlock.getCounters": "  private void getCounters(AppContext ctx) {\n    JobId jobID = null;\n    TaskId taskID = null;\n    String tid = $(TASK_ID);\n    if (!tid.isEmpty()) {\n      taskID = MRApps.toTaskID(tid);\n      jobID = taskID.getJobId();\n    } else {\n      String jid = $(JOB_ID);\n      if (jid != null && !jid.isEmpty()) {\n        jobID = MRApps.toJobID(jid);\n      }\n    }\n    if (jobID == null) {\n      return;\n    }\n    job = ctx.getJob(jobID);\n    if (job == null) {\n      // Log a warning or handle the case where the job is not found\n      System.err.println(\"Warning: Job not found for jobID: \" + jobID);\n      return;\n    }\n    if (taskID != null) {\n      task = job.getTask(taskID);\n      if (task == null) {\n        // Log a warning or handle the case where the task is not found\n        System.err.println(\"Warning: Task not found for taskID: \" + taskID);\n        return;\n      }\n      total = task.getCounters();\n      return;\n    }\n    // Get all types of counters\n    Map<TaskId, Task> tasks = job.getTasks();\n    total = job.getAllCounters();\n    map = new Counters();\n    reduce = new Counters();\n    for (Task t : tasks.values()) {\n      Counters counters = t.getCounters();\n      switch (t.getType()) {\n        case MAP:     map.incrAllCounters(counters);     break;\n        case REDUCE:  reduce.incrAllCounters(counters);  break;\n      }\n    }\n  }"
        }
    },
    {
        "filename": "MAPREDUCE-6353.json",
        "creation_time": "2015-05-03T14:10:44.000+0000",
        "bug_report": {
            "Title": "Divide by zero error in MR AM when calculating available containers",
            "Description": "An ArithmeticException occurs in the Hadoop MapReduce framework when attempting to calculate available containers with zero CPU vcores. The error arises from the method `computeAvailableContainers` in `ResourceCalculatorUtils`, which performs division without checking if the divisor is zero. This leads to a division by zero error when either the required memory or virtual cores is zero.",
            "StackTrace": [
                "2015-04-30 06:41:06,954 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM.",
                "java.lang.ArithmeticException: / by zero",
                "at org.apache.hadoop.mapreduce.v2.app.rm.ResourceCalculatorUtils.computeAvailableContainers(ResourceCalculatorUtils.java:38)",
                "at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests.assign(RMContainerAllocator.java:947)",
                "at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests.access$200(RMContainerAllocator.java:840)",
                "at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.heartbeat(RMContainerAllocator.java:247)",
                "at org.apache.hadoop.mapreduce.v2.app.rm.RMCommunicator$1.run(RMCommunicator.java:282)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "RootCause": "The root cause of the ArithmeticException is the lack of checks in the `computeAvailableContainers` method to ensure that the required resources (memory and virtual cores) are greater than zero before performing division.",
            "StepsToReproduce": [
                "Run a sleep job with zero CPU vcores.",
                "Monitor the logs for any errors related to RMContainerAllocator."
            ],
            "ExpectedBehavior": "The system should handle cases where required resources are zero by throwing an appropriate exception, rather than causing a division by zero error.",
            "ObservedBehavior": "An ArithmeticException is thrown, indicating a division by zero error in the logs.",
            "Suggestions": "Modify the `computeAvailableContainers` method to include checks for zero before performing any division. If the required resources are zero or negative, throw an IllegalArgumentException.",
            "problem_location": {
                "files": [
                    "hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/ResourceCalculatorUtils.java"
                ],
                "classes": [
                    "org.apache.hadoop.mapreduce.v2.app.rm.ResourceCalculatorUtils"
                ],
                "methods": [
                    "computeAvailableContainers"
                ]
            },
            "possible_fix": "Modify the `computeAvailableContainers` method as follows:\n\n```java\npublic static int computeAvailableContainers(Resource available,\n    Resource required, EnumSet<SchedulerResourceTypes> resourceTypes) {\n  if (required.getMemory() <= 0 || required.getVirtualCores() <= 0) {\n    throw new IllegalArgumentException(\"Required resources must be greater than zero.\");\n  }\n  \n  if (resourceTypes.contains(SchedulerResourceTypes.CPU)) {\n    return Math.min(available.getMemory() / required.getMemory(),\n      available.getVirtualCores() / required.getVirtualCores());\n  }\n  \n  return available.getMemory() / required.getMemory();\n}\n```"
        },
        "possible_fix_code": {
            "computeAvailableContainers": "public static int computeAvailableContainers(Resource available,\n    Resource required, EnumSet<SchedulerResourceTypes> resourceTypes) {\n  if (required.getMemory() <= 0 || required.getVirtualCores() <= 0) {\n    throw new IllegalArgumentException(\"Required resources must be greater than zero.\");\n  }\n  \n  if (resourceTypes.contains(SchedulerResourceTypes.CPU)) {\n    return Math.min(available.getMemory() / required.getMemory(),\n      available.getVirtualCores() / required.getVirtualCores());\n  }\n  \n  return available.getMemory() / required.getMemory();\n}"
        }
    },
    {
        "filename": "MAPREDUCE-4913.json",
        "creation_time": "2013-01-05T02:41:54.000+0000",
        "bug_report": {
            "Title": "TestMRAppMaster#testMRAppMasterMissingStaging occasionally exits",
            "Description": "The test case `testMRAppMasterMissingStaging` intermittently causes the JVM to exit unexpectedly due to an error in the `AsyncDispatcher`. The error log indicates that there is no registered handler for the `EventType` class `AM_STARTED`, which leads to the dispatcher being unable to process the event, resulting in a fatal error and subsequent JVM exit. This behavior disrupts the build process, as the test process exits without proper unregistration from the surefire plugin, causing it to be treated as a build error instead of a test failure.",
            "StackTrace": [
                "2013-01-05 02:14:54,682 FATAL [AsyncDispatcher event handler] event.AsyncDispatcher (AsyncDispatcher.java:dispatch(137)) - Error in dispatcher thread",
                "java.lang.Exception: No handler for registered for class org.apache.hadoop.mapreduce.jobhistory.EventType, cannot deliver EventType: AM_STARTED",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:132)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:77)",
                "at java.lang.Thread.run(Thread.java:662)",
                "2013-01-05 02:14:54,682 INFO  [AsyncDispatcher event handler] event.AsyncDispatcher (AsyncDispatcher.java:dispatch(140)) - Exiting, bbye.."
            ],
            "RootCause": "The root cause of the issue is the absence of a registered event handler for the `AM_STARTED` event type in the Hadoop application, which prevents the `AsyncDispatcher` from processing the event correctly.",
            "StepsToReproduce": [
                "Run the test case `testMRAppMasterMissingStaging` in the Hadoop MapReduce framework.",
                "Observe the logs for any fatal errors related to event dispatching.",
                "Note the JVM exit and the error message indicating no handler for `AM_STARTED`."
            ],
            "ExpectedBehavior": "The application should successfully handle the `AM_STARTED` event without exiting the JVM, allowing the test to complete and report results correctly.",
            "ObservedBehavior": "The JVM exits unexpectedly with a fatal error due to the lack of a registered handler for the `AM_STARTED` event, causing the test to fail and the build process to be interrupted.",
            "Suggestions": "To resolve this issue, ensure that the necessary event handlers for the Application Master events are implemented and registered correctly in the application code. Additionally, review the configuration files to confirm that all required components are set up properly.",
            "problem_location": {
                "files": [
                    "AsyncDispatcher.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.event.AsyncDispatcher"
                ],
                "methods": [
                    "AsyncDispatcher.dispatch"
                ]
            },
            "possible_fix": "Implement the missing event handler for the `AM_STARTED` event type in the application code and ensure it is registered with the `AsyncDispatcher`. Review the event handling setup in the configuration files to ensure all necessary components are included."
        },
        "possible_fix_code": {
            "AsyncDispatcher.dispatch": "public void dispatch(Event<?> event) {\n    if (event.getType() == EventType.AM_STARTED) {\n        handleAMStartedEvent((AMStartedEvent) event);\n    } else {\n        // Handle other event types\n        EventHandler<?> handler = getHandler(event.getType());\n        if (handler != null) {\n            handler.handle(event);\n        } else {\n            LOG.error(\"No handler registered for class \" + event.getType() + \", cannot deliver EventType: \" + event);\n        }\n    }\n}\n\nprivate void handleAMStartedEvent(AMStartedEvent event) {\n    // Implement the logic to handle the AM_STARTED event\n    LOG.info(\"Handling AM_STARTED event: \" + event);\n    // Additional processing for AM_STARTED event\n}"
        }
    },
    {
        "filename": "MAPREDUCE-6492.json",
        "creation_time": "2015-09-28T10:16:44.000+0000",
        "bug_report": {
            "Title": "AsyncDispatcher exit with NPE on TaskAttemptImpl#sendJHStartEventForAssignedFailTask",
            "Description": "A NullPointerException (NPE) occurs in the Hadoop MapReduce application when the method `sendJHStartEventForAssignedFailTask` is invoked during the deallocation of a task attempt that has not been assigned a container. This happens specifically when the task attempt is in the `TaskAttemptStateInternal.UNASSIGNED` state, leading to an attempt to access the HTTP address of a null container object.",
            "StackTrace": [
                "2015-09-28 18:01:48,656 FATAL [AsyncDispatcher event handler] org.apache.hadoop.yarn.event.AsyncDispatcher: Error in dispatcher thread",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.sendJHStartEventForAssignedFailTask(TaskAttemptImpl.java:1494)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.access$2900(TaskAttemptImpl.java:147)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$DeallocateContainerTransition.transition(TaskAttemptImpl.java:1700)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$DeallocateContainerTransition.transition(TaskAttemptImpl.java:1686)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:362)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$3(StateMachineFactory.java:290)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:1190)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:146)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:1415)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:1407)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:183)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:109)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "RootCause": "The NPE is caused by the `container` field being null when `sendJHStartEventForAssignedFailTask` is called. This occurs because the task attempt is in the `UNASSIGNED` state, indicating that it has not yet been assigned a container.",
            "StepsToReproduce": [
                "1. Start a Hadoop MapReduce job that includes task attempts.",
                "2. Ensure that a task attempt transitions to the `UNASSIGNED` state.",
                "3. Trigger the deallocation of the task attempt before it has been assigned a container.",
                "4. Observe the logs for the NPE in the `sendJHStartEventForAssignedFailTask` method."
            ],
            "ExpectedBehavior": "The system should handle the deallocation of task attempts gracefully without throwing a NullPointerException, even if the task attempt is in the `UNASSIGNED` state.",
            "ObservedBehavior": "A NullPointerException is thrown when attempting to access the HTTP address of a null container in the `sendJHStartEventForAssignedFailTask` method.",
            "Suggestions": "Implement a null check for the `container` object in the `sendJHStartEventForAssignedFailTask` method to prevent the NPE. Additionally, ensure that the task attempt's state is properly managed to avoid invoking this method when the container is not assigned.",
            "problem_location": {
                "files": [
                    "TaskAttemptImpl.java"
                ],
                "classes": [
                    "org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl"
                ],
                "methods": [
                    "sendJHStartEventForAssignedFailTask",
                    "DeallocateContainerTransition.transition"
                ]
            },
            "possible_fix": "Add a null check in the `sendJHStartEventForAssignedFailTask` method:\n\n```java\nprivate static void sendJHStartEventForAssignedFailTask(TaskAttemptImpl taskAttempt) {\n    if (taskAttempt.container == null) {\n        // Log a warning or handle the case where the container is not assigned\n        return;\n    }\n    // Existing code follows...\n}\n```"
        },
        "possible_fix_code": {
            "sendJHStartEventForAssignedFailTask": "  private static void\n      sendJHStartEventForAssignedFailTask(TaskAttemptImpl taskAttempt) {\n    if (taskAttempt.container == null) {\n        // Log a warning or handle the case where the container is not assigned\n        LOG.warn(\"Container is null, cannot send job history start event.\");\n        return;\n    }\n    TaskAttemptContainerLaunchedEvent event;\n    taskAttempt.launchTime = taskAttempt.clock.getTime();\n\n    InetSocketAddress nodeHttpInetAddr =\n        NetUtils.createSocketAddr(taskAttempt.container.getNodeHttpAddress());\n    taskAttempt.trackerName = nodeHttpInetAddr.getHostName();\n    taskAttempt.httpPort = nodeHttpInetAddr.getPort();\n    taskAttempt.sendLaunchedEvents();\n  }"
        }
    },
    {
        "filename": "MAPREDUCE-5744.json",
        "creation_time": "2014-02-06T19:05:01.000+0000",
        "bug_report": {
            "Title": "Job hangs because RMContainerAllocator$AssignedRequests.preemptReduce() violates the comparator contract",
            "Description": "The application encounters a hang during task assignment due to an exception thrown in the `preemptReduce` method of the `RMContainerAllocator` class. The method attempts to sort a list of `TaskAttemptId` objects based on their progress using a custom comparator. However, this comparator violates the general contract of the `Comparator` interface, specifically the transitive property, leading to an `IllegalArgumentException` when the sorting operation is executed.",
            "StackTrace": [
                "2014-02-06 16:43:45,183 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM.",
                "java.lang.IllegalArgumentException: Comparison method violates its general contract!",
                "at java.util.TimSort.mergeLo(TimSort.java:747)",
                "at java.util.TimSort.mergeAt(TimSort.java:483)",
                "at java.util.TimSort.mergeCollapse(TimSort.java:408)",
                "at java.util.TimSort.sort(TimSort.java:214)",
                "at java.util.TimSort.sort(TimSort.java:173)",
                "at java.util.Arrays.sort(Arrays.java:659)",
                "at java.util.Collections.sort(Collections.java:217)",
                "at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$AssignedRequests.preemptReduce(RMContainerAllocator.java:1106)",
                "at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.preemptReducesIfNeeded(RMContainerAllocator.java:416)",
                "at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.heartbeat(RMContainerAllocator.java:230)",
                "at org.apache.hadoop.mapreduce.v2.app.rm.RMCommunicator$1.run(RMCommunicator.java:252)",
                "at java.lang.Thread.run(Thread.java:744)"
            ],
            "RootCause": "The root cause of the issue is a custom comparator defined in the `preemptReduce` method that does not adhere to the comparator contract. Specifically, it fails to return `0` when two progress values are equal, which violates the transitive property required for comparators.",
            "StepsToReproduce": [
                "1. Trigger the task assignment process in the application.",
                "2. Ensure that the `preemptReduce` method is invoked during the allocation of resources.",
                "3. Observe the logs for the `IllegalArgumentException` indicating a violation of the comparator contract."
            ],
            "ExpectedBehavior": "The system should successfully sort the list of `TaskAttemptId` objects based on their progress and proceed with task assignment without throwing exceptions.",
            "ObservedBehavior": "The system hangs and throws an `IllegalArgumentException` due to the comparator in the `preemptReduce` method violating its contract, preventing tasks from being assigned.",
            "Suggestions": "Modify the comparator in the `preemptReduce` method to ensure it adheres to the comparator contract. Specifically, it should return `0` when the progress values are equal and consistently return negative, zero, or positive values based on the comparison.",
            "problem_location": {
                "files": [
                    "RMContainerAllocator.java"
                ],
                "classes": [
                    "org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator"
                ],
                "methods": [
                    "RMContainerAllocator.preemptReduce"
                ]
            },
            "possible_fix": "Replace the comparator in the `preemptReduce` method with the following corrected version:\n\n```java\nCollections.sort(reduceList,\n    new Comparator<TaskAttemptId>() {\n        @Override\n        public int compare(TaskAttemptId o1, TaskAttemptId o2) {\n            float progress1 = getJob().getTask(o1.getTaskId()).getAttempt(o1).getProgress();\n            float progress2 = getJob().getTask(o2.getTaskId()).getAttempt(o2).getProgress();\n            return Float.compare(progress1, progress2); // This handles all cases correctly\n        }\n    });\n```"
        },
        "possible_fix_code": {
            "preemptReduce": "void preemptReduce(int toPreempt) {\n      List<TaskAttemptId> reduceList = new ArrayList<TaskAttemptId>(reduces.keySet());\n      //sort reduces on progress\n      Collections.sort(reduceList,\n          new Comparator<TaskAttemptId>() {\n        @Override\n        public int compare(TaskAttemptId o1, TaskAttemptId o2) {\n          float progress1 = getJob().getTask(o1.getTaskId()).getAttempt(o1).getProgress();\n          float progress2 = getJob().getTask(o2.getTaskId()).getAttempt(o2).getProgress();\n          return Float.compare(progress1, progress2); // This handles all cases correctly\n        }\n      });\n      \n      for (int i = 0; i < toPreempt && reduceList.size() > 0; i++) {\n        TaskAttemptId id = reduceList.remove(0);//remove the one on top\n        LOG.info(\"Preempting \" + id);\n        preemptionWaitingReduces.add(id);\n        eventHandler.handle(new TaskAttemptEvent(id, TaskAttemptEventType.TA_KILL));\n      }\n    }"
        }
    },
    {
        "filename": "MAPREDUCE-3583.json",
        "creation_time": "2011-12-20T15:07:55.000+0000",
        "bug_report": {
            "Title": "ProcfsBasedProcessTree#constructProcessInfo() may throw NumberFormatException",
            "Description": "The HBase PreCommit builds are encountering a NumberFormatException due to an attempt to parse a string representation of a process ID that exceeds the maximum value for a Java Long. The specific string causing the issue is '18446743988060683582', which is derived from the process information read from the /proc filesystem. This issue arises in the `constructProcessInfo` method of the `ProcfsBasedProcessTree` class when it tries to parse this value into a Long, leading to the exception.",
            "StackTrace": [
                "java.lang.NumberFormatException: For input string: \"18446743988060683582\"",
                "at java.lang.NumberFormatException.forInputString(NumberFormatException.java:48)",
                "at java.lang.Long.parseLong(Long.java:422)",
                "at java.lang.Long.parseLong(Long.java:468)",
                "at org.apache.hadoop.util.ProcfsBasedProcessTree.constructProcessInfo(ProcfsBasedProcessTree.java:413)",
                "at org.apache.hadoop.util.ProcfsBasedProcessTree.getProcessTree(ProcfsBasedProcessTree.java:148)",
                "at org.apache.hadoop.util.LinuxResourceCalculatorPlugin.getProcResourceValues(LinuxResourceCalculatorPlugin.java:401)",
                "at org.apache.hadoop.mapred.Task.initialize(Task.java:536)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:353)",
                "at org.apache.hadoop.mapred.Child$4.run(Child.java:255)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:396)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1083)",
                "at org.apache.hadoop.mapred.Child.main(Child.java:249)"
            ],
            "RootCause": "The NumberFormatException is caused by attempting to parse a string ('18446743988060683582') into a Long, which exceeds the maximum value for a Long in Java (9223372036854775807). This occurs in the `constructProcessInfo` method when reading process information from the /proc filesystem.",
            "StepsToReproduce": [
                "Run HBase PreCommit builds.",
                "Ensure that the process information being read from the /proc filesystem includes a ppid value larger than 9223372036854775807.",
                "Observe the logs for NumberFormatException."
            ],
            "ExpectedBehavior": "The system should correctly parse process IDs and not throw a NumberFormatException when handling large integers.",
            "ObservedBehavior": "The system throws a NumberFormatException when attempting to parse a process ID that exceeds the maximum value for a Long.",
            "Suggestions": "Change the data structure used to store process information from using Long to a more suitable type that can handle larger integers, such as BigInteger, or modify the parsing logic to avoid parsing excessively large values.",
            "problem_location": {
                "files": [
                    "hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/util/ProcfsBasedProcessTree.java"
                ],
                "classes": [
                    "org.apache.hadoop.mapreduce.util.ProcfsBasedProcessTree"
                ],
                "methods": [
                    "constructProcessInfo",
                    "getProcessTree"
                ]
            },
            "possible_fix": "Modify the `constructProcessInfo` method to use BigInteger for parsing the ppid and other large values instead of Long. This will prevent the NumberFormatException from occurring when encountering large integers."
        },
        "possible_fix_code": {
            "constructProcessInfo": "  private static ProcessInfo constructProcessInfo(ProcessInfo pinfo, \n                                                    String procfsDir) {\n    ProcessInfo ret = null;\n    // Read \"procfsDir/<pid>/stat\" file - typically /proc/<pid>/stat\n    BufferedReader in = null;\n    FileReader fReader = null;\n    try {\n      File pidDir = new File(procfsDir, String.valueOf(pinfo.getPid()));\n      fReader = new FileReader(new File(pidDir, PROCFS_STAT_FILE));\n      in = new BufferedReader(fReader);\n    } catch (FileNotFoundException f) {\n      // The process vanished in the interim!\n      LOG.warn(\"The process \" + pinfo.getPid() +\n          \" may have finished in the interim.\");\n      return ret;\n    }\n\n    ret = pinfo;\n    try {\n      String str = in.readLine(); // only one line\n      Matcher m = PROCFS_STAT_FILE_FORMAT.matcher(str);\n      boolean mat = m.find();\n      if (mat) {\n        // Set (name) (ppid) (pgrpId) (session) (utime) (stime) (vsize) (rss)\n        pinfo.updateProcessInfo(m.group(2), Integer.parseInt(m.group(3)),\n                Integer.parseInt(m.group(4)), Integer.parseInt(m.group(5)),\n                new java.math.BigInteger(m.group(7)), new java.math.BigInteger(m.group(8)),\n                new java.math.BigInteger(m.group(10)), new java.math.BigInteger(m.group(11)));\n      } else {\n        LOG.warn(\"Unexpected: procfs stat file is not in the expected format\"\n            + \" for process with pid \" + pinfo.getPid());\n        ret = null;\n      }\n    } catch (IOException io) {\n      LOG.warn(\"Error reading the stream \" + io);\n      ret = null;\n    } finally {\n      // Close the streams\n      try {\n        fReader.close();\n        try {\n          in.close();\n        } catch (IOException i) {\n          LOG.warn(\"Error closing the stream \" + in);\n        }\n      } catch (IOException i) {\n        LOG.warn(\"Error closing the stream \" + fReader);\n      }\n    }\n\n    return ret;\n  }"
        }
    },
    {
        "filename": "MAPREDUCE-2238.json",
        "creation_time": "2011-01-03T22:57:22.000+0000",
        "bug_report": {
            "Title": "Undeletable Build Directories",
            "Description": "The Hudson job for the Hadoop MapReduce project is failing due to an inability to delete a specific build directory. This issue arises during the checkout process, where a test is attempting to change permissions on the build directory, preventing the cleanup operation from succeeding. The failure is logged as an IOException, indicating that the system cannot delete a user log file within the specified directory.",
            "StackTrace": [
                "hudson.util.IOException2: remote file operation failed: /grid/0/hudson/hudson-slave/workspace/Hadoop-Mapreduce-trunk at hudson.remoting.Channel@2545938c:hadoop7",
                "at hudson.FilePath.act(FilePath.java:749)",
                "at hudson.FilePath.act(FilePath.java:735)",
                "at hudson.scm.SubversionSCM.checkout(SubversionSCM.java:589)",
                "at hudson.scm.SubversionSCM.checkout(SubversionSCM.java:537)",
                "at hudson.model.AbstractProject.checkout(AbstractProject.java:1116)",
                "at hudson.model.AbstractBuild$AbstractRunner.checkout(AbstractBuild.java:479)",
                "at hudson.model.AbstractBuild$AbstractRunner.run(AbstractBuild.java:411)",
                "at hudson.model.Run.run(Run.java:1324)",
                "at hudson.model.FreeStyleBuild.run(FreeStyleBuild.java:46)",
                "at hudson.model.ResourceController.execute(ResourceController.java:88)",
                "at hudson.model.Executor.run(Executor.java:139)",
                "Caused by: java.io.IOException: Unable to delete /grid/0/hudson/hudson-slave/workspace/Hadoop-Mapreduce-trunk/trunk/build/test/logs/userlogs/job_20101230131139886_0001/attempt_20101230131139886_0001_m_000000_0"
            ],
            "RootCause": "The root cause of the issue is an IOException that occurs when the system attempts to delete a user log file during the checkout process. This is likely due to permission issues or the file being locked by another process.",
            "StepsToReproduce": [
                "Trigger a build for the Hadoop MapReduce project in Hudson.",
                "Observe the console output for errors related to file operations.",
                "Check the specified build directory for the presence of undeletable files."
            ],
            "ExpectedBehavior": "The build directory should be cleaned successfully before a new build starts, allowing the checkout process to complete without errors.",
            "ObservedBehavior": "The build fails with an IOException indicating that a specific user log file cannot be deleted, preventing the cleanup of the build directory.",
            "Suggestions": "Investigate the permissions of the build directory and the user log files. Ensure that no processes are locking the files during the cleanup phase. Consider implementing error handling to manage such exceptions gracefully.",
            "problem_location": {
                "files": [
                    "/grid/0/hudson/hudson-slave/workspace/Hadoop-Mapreduce-trunk/trunk/build/test/logs/userlogs/"
                ],
                "classes": [
                    "hudson.FilePath",
                    "hudson.scm.SubversionSCM",
                    "hudson.model.AbstractProject"
                ],
                "methods": [
                    "FilePath.act",
                    "SubversionSCM.checkout",
                    "AbstractProject.checkout"
                ]
            },
            "possible_fix": "Review and modify the permissions of the build directory and user log files. Implement checks to ensure that files are not locked before attempting deletion."
        },
        "possible_fix_code": {
            "FilePath.act": "public void act(FileCallable<?> callable) throws IOException, InterruptedException {\n    // Check if the file is locked before attempting to delete\n    if (isLocked()) {\n        throw new IOException(\"File is locked and cannot be deleted.\");\n    }\n    // Attempt to delete the file or directory\n    try {\n        // Perform the deletion operation\n        delete();\n    } catch (IOException e) {\n        // Log the error and rethrow the exception\n        System.err.println(\"Error deleting file: \" + e.getMessage());\n        throw new IOException(\"Unable to delete file: \" + getRemote(), e);\n    }\n}"
        }
    },
    {
        "filename": "MAPREDUCE-6410.json",
        "creation_time": "2015-06-04T06:34:33.000+0000",
        "bug_report": {
            "Title": "Aggregated Logs Deletion fails after refreshing Log Retention Settings in secure cluster",
            "Description": "The issue arises when attempting to delete aggregated logs after executing the command to refresh log retention settings in a secure Hadoop cluster. A GSSException is thrown, indicating a failure in Kerberos authentication due to the absence of a valid Kerberos ticket-granting ticket (TGT). This problem is reproducible under specific conditions, as detailed in the steps below.",
            "StackTrace": [
                "2015-06-04 14:14:40,070 | ERROR | Timer-3 | Error reading root log dir this deletion attempt is being aborted | AggregatedLogDeletionService.java:127",
                "java.io.IOException: Failed on local exception: java.io.IOException: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]",
                "at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:764)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1414)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1363)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)",
                "at com.sun.proxy.$Proxy9.getListing(Unknown Source)",
                "at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getListing(ClientNamenodeProtocolTranslatorPB.java:519)",
                "at sun.reflect.GeneratedMethodAccessor16.invoke(Unknown Source)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)",
                "at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)",
                "at com.sun.proxy.$Proxy10.getListing(Unknown Source)",
                "at org.apache.hadoop.hdfs.DFSClient.listPaths(DFSClient.java:1767)",
                "at org.apache.hadoop.hdfs.DFSClient.listPaths(DFSClient.java:1750)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.listStatusInternal(DistributedFileSystem.java:691)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.access$600(DistributedFileSystem.java:102)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$15.doCall(DistributedFileSystem.java:753)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$15.doCall(DistributedFileSystem.java:749)",
                "at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.listStatus(DistributedFileSystem.java:749)",
                "at org.apache.hadoop.yarn.logaggregation.AggregatedLogDeletionService$LogDeletionTask.run(AggregatedLogDeletionService.java:68)",
                "at java.util.TimerThread.mainLoop(Timer.java:555)",
                "at java.util.TimerThread.run(Timer.java:505)"
            ],
            "RootCause": "The root cause of the issue is a failure in Kerberos authentication, specifically due to the absence of a valid Kerberos ticket-granting ticket (TGT) when attempting to delete aggregated logs after refreshing log retention settings.",
            "StepsToReproduce": [
                "1. Start the history server in a secure cluster.",
                "2. Perform log deletion, which works as expected.",
                "3. Execute the command 'mapred hsadmin -refreshLogRetentionSettings' to refresh the configuration value.",
                "4. Attempt to delete logs again, which fails with a GSSException."
            ],
            "ExpectedBehavior": "The log deletion should succeed without any exceptions after refreshing the log retention settings.",
            "ObservedBehavior": "After refreshing the log retention settings, all subsequent attempts to delete logs fail with a GSSException indicating no valid credentials were provided.",
            "Suggestions": "To resolve this issue, ensure that the Kerberos client is properly installed and configured. Verify that the user has obtained a valid Kerberos ticket using the 'kinit' command, and check that the Kerberos configuration file (krb5.conf) and Hadoop configuration files (core-site.xml and hdfs-site.xml) are correctly set up for Kerberos authentication.",
            "problem_location": {
                "files": [
                    "AggregatedLogDeletionService.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.logaggregation.AggregatedLogDeletionService"
                ],
                "methods": [
                    "AggregatedLogDeletionService.run"
                ]
            },
            "possible_fix": "Ensure that the user has a valid Kerberos ticket before attempting log deletion. This can be done by running 'kinit' to obtain a TGT. Additionally, review the Kerberos and Hadoop configuration files for correctness."
        },
        "possible_fix_code": {
            "AggregatedLogDeletionService.run": "public void run() {\n    try {\n        // Check if the user has a valid Kerberos ticket before proceeding\n        UserGroupInformation loginUser = UserGroupInformation.getLoginUser();\n        if (loginUser == null || !loginUser.hasKerberosCredentials()) {\n            throw new IOException(\"No valid Kerberos credentials available. Please run 'kinit' to obtain a TGT.\");\n        }\n\n        // Proceed with log deletion logic\n        // ... (existing log deletion code)\n\n    } catch (IOException e) {\n        LOG.error(\"Error during log deletion: \" + e.getMessage(), e);\n        // Handle the exception appropriately\n    }\n}"
        }
    },
    {
        "filename": "MAPREDUCE-6693.json",
        "creation_time": "2016-05-10T11:30:38.000+0000",
        "bug_report": {
            "Title": "ArrayIndexOutOfBoundsException occurs when the length of the job name is equal to mapreduce.jobhistory.jobname.limit",
            "Description": "An ArrayIndexOutOfBoundsException is thrown in the Hadoop MapReduce framework when the job name exceeds the specified limit defined by the configuration parameter `mapreduce.jobhistory.jobname.limit`. This issue arises specifically in the `trimURLEncodedString` method when processing job names that are exactly at the limit, leading to an attempt to access an array index that does not exist.",
            "StackTrace": [
                "2016-05-10 06:51:00,674 DEBUG [Thread-73] org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: Interrupting Event Handling thread",
                "2016-05-10 06:51:00,674 DEBUG [Thread-73] org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: Waiting for Event Handling thread to complete",
                "2016-05-10 06:51:00,674 ERROR [eventHandlingThread] org.apache.hadoop.yarn.YarnUncaughtExceptionHandler: Thread Thread[eventHandlingThread,5,main] threw an Exception.",
                "java.lang.ArrayIndexOutOfBoundsException: 50",
                "at org.apache.hadoop.mapreduce.v2.jobhistory.FileNameIndexUtils.trimURLEncodedString(FileNameIndexUtils.java:326)",
                "at org.apache.hadoop.mapreduce.v2.jobhistory.FileNameIndexUtils.getDoneFileName(FileNameIndexUtils.java:86)",
                "at org.apache.hadoop.mapreduce.v2.jobhistory.JobHistoryEventHandler.processDoneFiles(JobHistoryEventHandler.java:1147)",
                "at org.apache.hadoop.mapreduce.v2.jobhistory.JobHistoryEventHandler.handleEvent(JobHistoryEventHandler.java:635)",
                "at org.apache.hadoop.mapreduce.v2.jobhistory.JobHistoryEventHandler$1.run(JobHistoryEventHandler.java:341)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "RootCause": "The ArrayIndexOutOfBoundsException is caused by the `jobNameLimit` parameter being set incorrectly or the `encodedString` being shorter than expected in the `trimURLEncodedString` method, which is called by `getDoneFileName`. Specifically, when the job name length is equal to the limit, the method attempts to access an index that exceeds the bounds of the array.",
            "StepsToReproduce": [
                "Set the job name to a string with a length equal to the value of `mapreduce.jobhistory.jobname.limit`.",
                "Submit a job to the Hadoop MapReduce framework.",
                "Monitor the job history and observe the logs for the ArrayIndexOutOfBoundsException."
            ],
            "ExpectedBehavior": "The job history entry should be created successfully without any exceptions, and the job name should be properly recorded in the history.",
            "ObservedBehavior": "An ArrayIndexOutOfBoundsException is thrown, preventing the job history entry from being created when the job name length is equal to the specified limit.",
            "Suggestions": "Review the implementation of the `trimURLEncodedString` method to ensure it correctly handles cases where the job name length is equal to the limit. Consider adding validation to prevent job names from exceeding the limit or adjust the logic to handle edge cases.",
            "problem_location": {
                "files": [
                    "hadoop-mapreduce-client-common/src/main/java/org/apache/hadoop/mapreduce/v2/jobhistory/FileNameIndexUtils.java"
                ],
                "classes": [
                    "org.apache.hadoop.mapreduce.v2.jobhistory.FileNameIndexUtils"
                ],
                "methods": [
                    "trimURLEncodedString",
                    "getDoneFileName"
                ]
            },
            "possible_fix": "In the `trimURLEncodedString` method, add a check to ensure that the index does not exceed the length of the encoded string before accessing it. For example, modify the loop condition to ensure that `index + increase <= encodedString.length()`."
        },
        "possible_fix_code": {
            "trimURLEncodedString": "  private static String trimURLEncodedString(\n      String encodedString, int limitLength) {\n    assert(limitLength >= 0) : \"limitLength should be positive integer\";\n\n    if (encodedString.length() < limitLength) {\n      return encodedString;\n    }\n\n    int index = 0;\n    int increase = 0;\n    byte[] strBytes = encodedString.getBytes(UTF_8);\n\n    // calculate effective character length based on UTF-8 specification.\n    // The size of a character coded in UTF-8 should be 4-byte at most.\n    // See RFC3629\n    while (true) {\n      if (index >= strBytes.length) {\n        break; // Prevent accessing out of bounds\n      }\n      byte b = strBytes[index];\n      if (b == '%') {\n        byte minuend1 = strBytes[index + 1];\n        byte subtrahend1 = (byte)(Character.isDigit(\n            minuend1) ? '0' : 'A' - 10);\n        byte minuend2 = strBytes[index + 2];\n        byte subtrahend2 = (byte)(Character.isDigit(\n            minuend2) ? '0' : 'A' - 10);\n        int initialHex =\n            ((Character.toUpperCase(minuend1) - subtrahend1) << 4) +\n            (Character.toUpperCase(minuend2) - subtrahend2);\n\n        if (0x00 <= initialHex && initialHex <= 0x7F) {\n          // For 1-byte UTF-8 characters\n          increase = 3;\n        } else if (0xC2 <= initialHex && initialHex <= 0xDF) {\n          // For 2-byte UTF-8 characters\n          increase = 6;\n        } else if (0xE0 <= initialHex && initialHex <= 0xEF) {\n          // For 3-byte UTF-8 characters\n          increase = 9;\n        } else {\n          // For 4-byte UTF-8 characters\n          increase = 12;\n        }\n      } else {\n        increase = 1;\n      }\n      if (index + increase > limitLength) {\n        break;\n      } else {\n        index += increase;\n      }\n    }\n\n    return encodedString.substring(0, index);\n  }"
        }
    },
    {
        "filename": "MAPREDUCE-5912.json",
        "creation_time": "2014-06-04T15:37:03.000+0000",
        "bug_report": {
            "Title": "Task.calculateOutputSize does not handle Windows files after MAPREDUCE-5196",
            "Description": "The method `Task.calculateOutputSize` is failing to correctly handle file paths on Windows systems after the changes introduced in MAPREDUCE-5196. The issue arises when the method attempts to retrieve the output file size using a local file system path, which is not compatible with the expected HDFS path format. This results in an `IllegalArgumentException` indicating that the provided pathname is not a valid DFS filename.",
            "StackTrace": [
                "2014-06-02 00:14:53,891 WARN [main] org.apache.hadoop.mapred.YarnChild: Exception running child : java.lang.IllegalArgumentException: Pathname /c:/Hadoop/Data/Hadoop/local/usercache/HadoopUser/appcache/application_1401693085139_0001/output/attempt_1401693085139_0001_m_000000_0/file.out from c:/Hadoop/Data/Hadoop/local/usercache/HadoopUser/appcache/application_1401693085139_0001/output/attempt_1401693085139_0001_m_000000_0/file.out is not a valid DFS filename.",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:187)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.access$000(DistributedFileSystem.java:101)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$17.doCall(DistributedFileSystem.java:1024)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$17.doCall(DistributedFileSystem.java:1020)",
                "at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1020)",
                "at org.apache.hadoop.mapred.Task.calculateOutputSize(Task.java:1124)",
                "at org.apache.hadoop.mapred.Task.sendLastUpdate(Task.java:1102)",
                "at org.apache.hadoop.mapred.Task.done(Task.java:1048)"
            ],
            "RootCause": "The root cause of the issue is that the `calculateOutputSize` method is using a local file system path instead of an HDFS path, leading to an `IllegalArgumentException` when trying to access the file status.",
            "StepsToReproduce": [
                "Set up a Hadoop environment on a Windows machine.",
                "Run a MapReduce job that outputs files to a local directory.",
                "Observe the logs for warnings or errors related to file path handling."
            ],
            "ExpectedBehavior": "The method should correctly retrieve the output file size from the local file system without throwing an exception, regardless of the operating system.",
            "ObservedBehavior": "The method throws an `IllegalArgumentException` indicating that the provided pathname is not a valid DFS filename, causing the job to fail.",
            "Suggestions": "To resolve the issue, ensure that the Hadoop configuration is set to use the correct file system URI. Use the method `Configuration.set(\"fs.defaultFS\", \"hdfs://namenode:port/\")` to specify the correct file system URI.",
            "problem_location": {
                "files": [
                    "Task.java"
                ],
                "classes": [
                    "org.apache.hadoop.mapred.Task"
                ],
                "methods": [
                    "Task.calculateOutputSize"
                ]
            },
            "possible_fix": "Modify the `calculateOutputSize` method to ensure it retrieves the correct file system based on the configuration. This can be done by using the `FileSystem.get(Configuration conf)` method to obtain the appropriate file system instance before accessing the file status."
        },
        "possible_fix_code": {
            "Task.calculateOutputSize": "public long calculateOutputSize() throws IOException {\n    // Get the Hadoop configuration\n    Configuration conf = new Configuration();\n    // Set the default file system URI if not already set\n    if (conf.get(\"fs.defaultFS\") == null) {\n        conf.set(\"fs.defaultFS\", \"hdfs://namenode:port/\");\n    }\n    // Get the appropriate FileSystem instance based on the configuration\n    FileSystem fs = FileSystem.get(conf);\n    // Assuming outputPath is the path to the output file\n    Path outputPath = new Path(outputFilePath);\n    // Retrieve the file status from the FileSystem\n    FileStatus fileStatus = fs.getFileStatus(outputPath);\n    // Return the size of the output file\n    return fileStatus.getLen();\n}"
        }
    },
    {
        "filename": "MAPREDUCE-5952.json",
        "creation_time": "2014-06-30T23:08:52.000+0000",
        "bug_report": {
            "Title": "LocalContainerLauncher#renameMapOutputForReduce incorrectly assumes a single dir for mapOutIndex",
            "Description": "The method `renameMapOutputForReduce` in the `LocalContainerLauncher` class is failing due to an incorrect assumption about the output directory structure. The javadoc comment misleadingly suggests that there is a single map output directory, while the actual behavior depends on the `LOCAL_DIRS` configuration. This discrepancy leads to a `FileNotFoundException` when the expected output file is not found at the specified path.",
            "StackTrace": [
                "2014-06-30 14:48:35,574 WARN [uber-SubtaskRunner] org.apache.hadoop.mapred.LocalContainerLauncher: Exception running local (uberized) 'child' : java.io.FileNotFoundException: File /Users/gshegalov/workspace/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/target/org.apache.hadoop.mapreduce.v2.TestMRJobs/org.apache.hadoop.mapreduce.v2.TestMRJobs-localDir-nm-2_3/usercache/gshegalov/appcache/application_1404164272885_0001/output/file.out.index does not exist",
                "at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:517)",
                "at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:726)",
                "at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:507)",
                "at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:337)",
                "at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:289)",
                "at org.apache.hadoop.fs.RawLocalFileSystem.rename(RawLocalFileSystem.java:334)",
                "at org.apache.hadoop.fs.ChecksumFileSystem.rename(ChecksumFileSystem.java:504)",
                "at org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler.renameMapOutputForReduce(LocalContainerLauncher.java:471)",
                "at org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler.runSubtask(LocalContainerLauncher.java:370)",
                "at org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler.runTask(LocalContainerLauncher.java:292)",
                "at org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler.access$200(LocalContainerLauncher.java:178)",
                "at org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler$1.run(LocalContainerLauncher.java:221)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:439)",
                "at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:138)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)",
                "at java.lang.Thread.run(Thread.java:695)"
            ],
            "RootCause": "The root cause of the issue is that the output path for the job is not set correctly, leading to the `FileNotFoundException`. The method `checkOutputSpecs` in the `FileOutputFormat` class ensures that the output directory is specified and does not already exist. If this check fails, the expected output files will not be created, resulting in the observed error.",
            "StepsToReproduce": [
                "Configure a Hadoop job with a map task that outputs to a directory specified by LOCAL_DIRS.",
                "Run the job and ensure that the output path is not set correctly in the job configuration.",
                "Observe the logs for a `FileNotFoundException` related to the output file."
            ],
            "ExpectedBehavior": "The job should successfully create the output files in the specified output directory without any exceptions.",
            "ObservedBehavior": "The job fails with a `FileNotFoundException`, indicating that the expected output file does not exist at the specified path.",
            "Suggestions": "Verify that the output path is set correctly in the job configuration using the `setOutputPath` method. Ensure that the job completes successfully and generates the expected output files.",
            "problem_location": {
                "files": [
                    "hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/LocalContainerLauncher.java",
                    "hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/FileOutputFormat.java"
                ],
                "classes": [
                    "org.apache.hadoop.mapred.LocalContainerLauncher",
                    "org.apache.hadoop.mapred.FileOutputFormat"
                ],
                "methods": [
                    "LocalContainerLauncher.renameMapOutputForReduce",
                    "FileOutputFormat.checkOutputSpecs",
                    "FileOutputFormat.setOutputPath"
                ]
            },
            "possible_fix": "Ensure that the output path is set correctly in the job configuration before executing the job. This can be done by calling `setOutputPath(job, outputPath)` with a valid `Path` object representing the desired output directory."
        },
        "possible_fix_code": {
            "LocalContainerLauncher.renameMapOutputForReduce": "public void renameMapOutputForReduce(JobConf job, Path outputPath) throws IOException {\n    // Ensure that the output path is set correctly before renaming\n    if (outputPath == null) {\n        throw new IOException(\"Output path is not set.\");\n    }\n\n    // Set the output path in the job configuration\n    FileOutputFormat.setOutputPath(job, outputPath);\n\n    // Proceed with renaming the map output files\n    Path mapOutputDir = FileOutputFormat.getOutputPath(job);\n    if (mapOutputDir == null) {\n        throw new IOException(\"Map output directory is not set.\");\n    }\n\n    // Logic to rename the output files from map to reduce\n    // Assuming we have a method to get the map output index file\n    Path mapOutputIndexFile = new Path(mapOutputDir, \"file.out.index\");\n    if (!FileSystem.get(job).exists(mapOutputIndexFile)) {\n        throw new FileNotFoundException(\"Expected map output index file does not exist: \" + mapOutputIndexFile);\n    }\n\n    // Rename logic here (not shown in the original code)\n    // Example: FileUtil.rename(mapOutputIndexFile, new Path(outputPath, \"newFileName.index\"));\n}"
        }
    },
    {
        "filename": "MAPREDUCE-3306.json",
        "creation_time": "2011-10-28T16:59:34.000+0000",
        "bug_report": {
            "Title": "Cannot run apps after MAPREDUCE-2989",
            "Description": "The application fails to run jobs due to a `NoSuchElementException` being thrown in the NodeManager logs. This exception occurs when the application attempts to access an element from an iterator that does not exist, specifically within the context of the YARN framework. The stack trace indicates that the error arises during the application state transition process, particularly in the `ApplicationImpl` class, which handles application events and state changes.",
            "StackTrace": [
                "2011-10-28 21:40:21,263 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.application.Application: Processing application_1319818154209_0001 of type APPLICATION_INITED",
                "2011-10-28 21:40:21,264 FATAL org.apache.hadoop.yarn.event.AsyncDispatcher: Error in dispatcher thread. Exiting..",
                "java.util.NoSuchElementException",
                "at java.util.HashMap$HashIterator.nextEntry(HashMap.java:796)",
                "at java.util.HashMap$ValueIterator.next(HashMap.java:822)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl$AppInitDoneTransition.transition(ApplicationImpl.java:251)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl$AppInitDoneTransition.transition(ApplicationImpl.java:245)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:357)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:298)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:443)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.handle(ApplicationImpl.java:385)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.handle(ApplicationImpl.java:58)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher.handle(ContainerManagerImpl.java:407)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher.handle(ContainerManagerImpl.java:399)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:116)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:75)",
                "at java.lang.Thread.run(Thread.java:662)"
            ],
            "RootCause": "The root cause of the issue is a `NoSuchElementException` thrown when the application attempts to access an element from an iterator of a `HashMap` without checking if any elements are available. This indicates a flaw in the state transition logic or event handling within the `ApplicationImpl` class.",
            "StepsToReproduce": [
                "Attempt to run a job in the YARN framework after the changes introduced in MAPREDUCE-2989.",
                "Check the NodeManager logs for any fatal errors or exceptions."
            ],
            "ExpectedBehavior": "The application should process jobs without throwing exceptions, and the state transitions should handle events correctly without attempting to access non-existent elements.",
            "ObservedBehavior": "The application fails to run jobs, and the NodeManager logs show a `NoSuchElementException`, causing the dispatcher thread to exit.",
            "Suggestions": "Review the state transition logic in the `ApplicationImpl` class to ensure that checks are in place to prevent accessing elements from iterators when none are available. Implement checks using `hasNext()` before calling `next()` on iterators.",
            "problem_location": {
                "files": [
                    "ApplicationImpl.java",
                    "ContainerManagerImpl.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl",
                    "org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl"
                ],
                "methods": [
                    "ApplicationImpl.AppInitDoneTransition.transition",
                    "ContainerManagerImpl.ApplicationEventDispatcher.handle"
                ]
            },
            "possible_fix": "Add checks to ensure that the iterator has elements before calling `next()`. For example, modify the transition method in `ApplicationImpl` to include a check for `hasNext()` before accessing the iterator."
        },
        "possible_fix_code": {
            "ApplicationImpl.handle": "  public void handle(ApplicationEvent event) {\n\n    this.writeLock.lock();\n\n    try {\n      ApplicationId applicationID = event.getApplicationID();\n      LOG.info(\"Processing \" + applicationID + \" of type \" + event.getType());\n\n      ApplicationState oldState = stateMachine.getCurrentState();\n      ApplicationState newState = null;\n      try {\n        // queue event requesting init of the same app\n        newState = stateMachine.doTransition(event.getType(), event);\n      } catch (InvalidStateTransitonException e) {\n        LOG.warn(\"Can't handle this event at current state\", e);\n      }\n      if (oldState != newState) {\n        LOG.info(\"Application \" + applicationID + \" transitioned from \"\n            + oldState + \" to \" + newState);\n      }\n    } finally {\n      this.writeLock.unlock();\n    }\n  }"
        }
    },
    {
        "filename": "MAPREDUCE-6554.json",
        "creation_time": "2015-11-21T07:34:31.000+0000",
        "bug_report": {
            "Title": "MRAppMaster service start failing with NPE in MRAppMaster#parsePreviousJobHistory",
            "Description": "The MRAppMaster service fails to start due to a NullPointerException (NPE) occurring in the method `parsePreviousJobHistory`. This issue arises when the MRAppMaster attempts to recover a previous job history file after being preempted. The stack trace indicates that the NPE is triggered when a null value is passed to the `StringReader` constructor, which is part of the schema parsing process in the Apache Avro library.",
            "StackTrace": [
                "2015-11-21 13:52:27,722 INFO [main] org.apache.hadoop.service.AbstractService: Service org.apache.hadoop.mapreduce.v2.app.MRAppMaster failed in state STARTED; cause: java.lang.NullPointerException",
                "java.lang.NullPointerException",
                "at java.io.StringReader.<init>(StringReader.java:50)",
                "at org.apache.avro.Schema$Parser.parse(Schema.java:917)",
                "at org.apache.avro.Schema.parse(Schema.java:966)",
                "at org.apache.hadoop.mapreduce.jobhistory.EventReader.<init>(EventReader.java:75)",
                "at org.apache.hadoop.mapreduce.jobhistory.JobHistoryParser.parse(JobHistoryParser.java:139)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.parsePreviousJobHistory(MRAppMaster.java:1256)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.processRecovery(MRAppMaster.java:1225)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.serviceStart(MRAppMaster.java:1087)",
                "at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$4.run(MRAppMaster.java:1570)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1673)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.initAndStartAppMaster(MRAppMaster.java:1566)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.main(MRAppMaster.java:1499)",
                "2015-11-21 13:52:27,725 INFO [main] org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: Stopping JobHistoryEventHandler. Size of the outstanding queue size is 0"
            ],
            "RootCause": "The NullPointerException occurs because a null value is being passed to the StringReader constructor during the schema parsing process. This is likely due to a failure in retrieving or initializing the schema from the job history.",
            "StepsToReproduce": [
                "Create a scenario where the MRAppMaster is preempted.",
                "Attempt to launch the MRAppMaster again to recover the previous job history."
            ],
            "ExpectedBehavior": "The MRAppMaster should successfully start and recover the previous job history without throwing any exceptions.",
            "ObservedBehavior": "The MRAppMaster fails to start, throwing a NullPointerException during the recovery process.",
            "Suggestions": "Check for null values before passing them to constructors or methods. Ensure that the schema is properly initialized and retrieved before parsing.",
            "problem_location": {
                "files": [
                    "MRAppMaster.java",
                    "JobHistoryParser.java",
                    "EventReader.java"
                ],
                "classes": [
                    "org.apache.hadoop.mapreduce.v2.app.MRAppMaster",
                    "org.apache.hadoop.mapreduce.jobhistory.JobHistoryParser",
                    "org.apache.hadoop.mapreduce.jobhistory.EventReader"
                ],
                "methods": [
                    "MRAppMaster.parsePreviousJobHistory",
                    "JobHistoryParser.parse",
                    "EventReader.init"
                ]
            },
            "possible_fix": "Add null checks before the StringReader initialization in the schema parsing logic. Ensure that the schema is correctly loaded from the job history."
        },
        "possible_fix_code": {
            "MRAppMaster.parsePreviousJobHistory": "  private void parsePreviousJobHistory() throws IOException {\n    FSDataInputStream in = getPreviousJobHistoryStream(getConfig(),\n        appAttemptID);\n    JobHistoryParser parser = new JobHistoryParser(in);\n    JobInfo jobInfo = parser.parse();\n    Exception parseException = parser.getParseException();\n    if (parseException != null) {\n      LOG.info(\"Got an error parsing job-history file\" +\n          \", ignoring incomplete events.\", parseException);\n    }\n    Map<org.apache.hadoop.mapreduce.TaskID, TaskInfo> taskInfos = jobInfo\n        .getAllTasks();\n    for (TaskInfo taskInfo : taskInfos.values()) {\n      if (TaskState.SUCCEEDED.toString().equals(taskInfo.getTaskStatus())) {\n        Iterator<Entry<TaskAttemptID, TaskAttemptInfo>> taskAttemptIterator =\n            taskInfo.getAllTaskAttempts().entrySet().iterator();\n        while (taskAttemptIterator.hasNext()) {\n          Map.Entry<TaskAttemptID, TaskAttemptInfo> currentEntry = taskAttemptIterator.next();\n          if (!jobInfo.getAllCompletedTaskAttempts().containsKey(currentEntry.getKey())) {\n            taskAttemptIterator.remove();\n          }\n        }\n        completedTasksFromPreviousRun\n            .put(TypeConverter.toYarn(taskInfo.getTaskId()), taskInfo);\n        LOG.info(\"Read from history task \"\n            + TypeConverter.toYarn(taskInfo.getTaskId()));\n      }\n    }\n    LOG.info(\"Read completed tasks from history \"\n        + completedTasksFromPreviousRun.size());\n    recoveredJobStartTime = jobInfo.getLaunchTime();\n\n    // recover AMInfos\n    List<JobHistoryParser.AMInfo> jhAmInfoList = jobInfo.getAMInfos();\n    if (jhAmInfoList != null) {\n      for (JobHistoryParser.AMInfo jhAmInfo : jhAmInfoList) {\n        AMInfo amInfo = MRBuilderUtils.newAMInfo(jhAmInfo.getAppAttemptId(),\n            jhAmInfo.getStartTime(), jhAmInfo.getContainerId(),\n            jhAmInfo.getNodeManagerHost(), jhAmInfo.getNodeManagerPort(),\n            jhAmInfo.getNodeManagerHttpPort());\n        amInfos.add(amInfo);\n      }\n    }\n  }"
        }
    },
    {
        "filename": "MAPREDUCE-4457.json",
        "creation_time": "2012-07-18T21:38:17.000+0000",
        "bug_report": {
            "Title": "mr job invalid transition TA_TOO_MANY_FETCH_FAILURE at FAILED",
            "Description": "The job encountered an invalid state transition in a Hadoop YARN application, specifically when a task attempt transitioned to the ERROR state due to too many fetch failures. The logs indicate multiple transitions of task attempts from SUCCEEDED to FAILED, and an error occurred when handling the event 'TA_TOO_MANY_FETCH_FAILURE' while in the FAILED state. This suggests a misconfiguration in the state machine that does not account for this event in the current state.",
            "StackTrace": [
                "org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1342238829791_2501_m_007743_0 TaskAttempt Transitioned from SUCCEEDED to FAILED",
                "org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1342238829791_2501_m_008850_0 TaskAttempt Transitioned from SUCCEEDED to FAILED",
                "org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: attempt_1342238829791_2501_m_017344_1000 TaskAttempt Transitioned from RUNNING to SUCCESS_CONTAINER_CLEANUP",
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: TA_TOO_MANY_FETCH_FAILURE at FAILED"
            ],
            "RootCause": "The root cause of the issue is that the state machine does not have a defined transition for the event 'TA_TOO_MANY_FETCH_FAILURE' when the task attempt is in the 'FAILED' state, leading to an InvalidStateTransitonException.",
            "StepsToReproduce": [
                "Submit a job that is expected to encounter multiple fetch failures.",
                "Monitor the state transitions of task attempts in the YARN application.",
                "Observe the logs for any InvalidStateTransitonException related to 'TA_TOO_MANY_FETCH_FAILURE'."
            ],
            "ExpectedBehavior": "The system should handle the 'TA_TOO_MANY_FETCH_FAILURE' event gracefully, allowing for a valid state transition or recovery process without throwing an exception.",
            "ObservedBehavior": "The system throws an InvalidStateTransitonException when attempting to handle the 'TA_TOO_MANY_FETCH_FAILURE' event while in the 'FAILED' state, resulting in an error state for the job.",
            "Suggestions": "Review the state machine configuration to ensure that a transition is defined for the 'TA_TOO_MANY_FETCH_FAILURE' event when in the 'FAILED' state. Consider adding a transition that allows for recovery or reprocessing of the task attempt.",
            "problem_location": {
                "files": [
                    "hadoop-mapreduce-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.state.StateMachineFactory.java",
                    "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.state.StateMachineFactory",
                    "org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl"
                ],
                "methods": [
                    "StateMachineFactory.doTransition",
                    "TaskAttemptImpl.handle"
                ]
            },
            "possible_fix": "Add a transition in the StateMachineFactory for the 'TA_TOO_MANY_FETCH_FAILURE' event from the 'FAILED' state to a suitable state, such as 'RETRY' or 'FAILED_RECOVERABLE', to allow the system to handle this event appropriately."
        },
        "possible_fix_code": {
            "StateMachineFactory.addTransition": "  public StateMachineFactory\n             <OPERAND, STATE, EVENTTYPE, EVENT>\n          addTransition(STATE preState, STATE postState, EVENTTYPE eventType) {\n    return addTransition(preState, postState, eventType, null);\n  }\n\n  // Adding a transition for TA_TOO_MANY_FETCH_FAILURE from FAILED to RETRY\n  public void defineTransitions() {\n      addTransition(FAILED, RETRY, TA_TOO_MANY_FETCH_FAILURE);\n  }"
        }
    },
    {
        "filename": "MAPREDUCE-2716.json",
        "creation_time": "2011-07-20T21:13:36.000+0000",
        "bug_report": {
            "Title": "MR279: MRReliabilityTest job fails because of missing job-file.",
            "Description": "The MRReliabilityTest job fails due to a missing job file, which is expected to be located at a specific HDFS path. The absence of this job file leads to an IllegalArgumentException when the system attempts to create a Path object from an empty string. This issue arises because the jobFile variable in the JobStatus class is either uninitialized or incorrectly set to an empty string, as indicated by the stack trace originating from the JobConf constructor.",
            "StackTrace": [
                "java.lang.IllegalArgumentException: Can not create a Path from an empty string",
                "at org.apache.hadoop.fs.Path.checkPathArg(Path.java:88)",
                "at org.apache.hadoop.fs.Path.<init>(Path.java:96)",
                "at org.apache.hadoop.mapred.JobConf.<init>(JobConf.java:445)",
                "at org.apache.hadoop.mapreduce.Cluster.getJobs(Cluster.java:104)",
                "at org.apache.hadoop.mapreduce.Cluster.getAllJobs(Cluster.java:218)",
                "at org.apache.hadoop.mapred.JobClient.getAllJobs(JobClient.java:757)",
                "at org.apache.hadoop.mapred.JobClient.jobsToComplete(JobClient.java:741)",
                "at org.apache.hadoop.mapred.ReliabilityTest.runTest(ReliabilityTest.java:219)",
                "at org.apache.hadoop.mapred.ReliabilityTest.runSleepJobTest(ReliabilityTest.java:133)",
                "at org.apache.hadoop.mapred.ReliabilityTest.run(ReliabilityTest.java:116)",
                "at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:69)",
                "at org.apache.hadoop.mapred.ReliabilityTest.main(ReliabilityTest.java:504)"
            ],
            "RootCause": "The root cause of the issue is that the jobFile variable in the JobStatus class is likely uninitialized or set to an empty string, which results in an IllegalArgumentException when the system attempts to create a Path object from this empty string.",
            "StepsToReproduce": [
                "1. Submit a job using the MRReliabilityTest.",
                "2. Ensure that the job file is not present at the expected HDFS path.",
                "3. Observe the error message indicating that a Path cannot be created from an empty string."
            ],
            "ExpectedBehavior": "The job should successfully locate the job file and proceed without errors.",
            "ObservedBehavior": "The job fails with an IllegalArgumentException due to the absence of the job file, leading to an attempt to create a Path from an empty string.",
            "Suggestions": "Investigate the initialization of the jobFile variable in the JobStatus class. Ensure that it is properly set during job creation. Consider adding validation to check if the jobFile is empty before attempting to create a Path.",
            "problem_location": {
                "files": [
                    "JobStatus.java",
                    "JobConf.java"
                ],
                "classes": [
                    "org.apache.hadoop.mapred.JobStatus",
                    "org.apache.hadoop.mapred.JobConf"
                ],
                "methods": [
                    "JobStatus.getJobFile",
                    "JobConf.init"
                ]
            },
            "possible_fix": "In the JobStatus class, ensure that the jobFile variable is initialized correctly in the constructor. If the jobFile is not set, throw a more descriptive exception or set a default value to prevent the IllegalArgumentException."
        },
        "possible_fix_code": {
            "org.apache.hadoop.mapred.JobStatus.<init>": "  /**\n   * Create a job status object for a given jobid.\n   * @param jobid The jobid of the job\n   * @param mapProgress The progress made on the maps\n   * @param reduceProgress The progress made on the reduces\n   * @param cleanupProgress The progress made on cleanup\n   * @param runState The current state of the job\n   * @param user userid of the person who submitted the job.\n   * @param jobName user-specified job name.\n   * @param jobFile job configuration file. \n   * @param trackingUrl link to the web-ui for details of the job.\n   */\n  public JobStatus(JobID jobid, float mapProgress, float reduceProgress,\n                   float cleanupProgress, int runState, \n                   String user, String jobName, \n                   String jobFile, String trackingUrl) {\n    if (jobFile == null || jobFile.isEmpty()) {\n        throw new IllegalArgumentException(\"Job file cannot be null or empty\");\n    }\n    this.jobFile = jobFile;\n    // Initialize other fields as necessary\n  }"
        }
    },
    {
        "filename": "MAPREDUCE-4748.json",
        "creation_time": "2012-10-24T21:53:19.000+0000",
        "bug_report": {
            "Title": "Invalid event: T_ATTEMPT_SUCCEEDED at SUCCEEDED",
            "Description": "An error occurred while executing a large Pig script, resulting in an InvalidStateTransitionException. The error indicates that the system received an event (`T_ATTEMPT_SUCCEEDED`) while already in the `SUCCEEDED` state, which violates the state machine's transition rules. This issue may stem from race conditions or flaws in the event handling logic within the Hadoop YARN framework.",
            "StackTrace": [
                "2012-10-23 22:45:24,986 ERROR [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl: Can't handle this event at current state for task_1350837501057_21978_m_040453",
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: T_ATTEMPT_SUCCEEDED at SUCCEEDED",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:301)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:443)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl.handle(TaskImpl.java:604)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl.handle(TaskImpl.java:89)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskEventDispatcher.handle(MRAppMaster.java:914)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskEventDispatcher.handle(MRAppMaster.java:908)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:126)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:75)",
                "at java.lang.Thread.run(Thread.java:619)"
            ],
            "RootCause": "The root cause of the issue is an invalid state transition in the YARN application, where the event `T_ATTEMPT_SUCCEEDED` is processed while the task is already in the `SUCCEEDED` state, leading to an InvalidStateTransitionException.",
            "StepsToReproduce": [
                "Run a large Pig script with speculative execution enabled.",
                "Monitor the task states and events during execution.",
                "Observe the logs for any InvalidStateTransitionException related to task state handling."
            ],
            "ExpectedBehavior": "The system should correctly handle task events and transitions without encountering invalid state transitions, ensuring that events are processed only when the task is in a valid state.",
            "ObservedBehavior": "The system throws an InvalidStateTransitionException when attempting to process the `T_ATTEMPT_SUCCEEDED` event while already in the `SUCCEEDED` state.",
            "Suggestions": "1. Review the event handling logic to ensure it checks the current state before processing events. 2. Investigate potential race conditions that may lead to multiple success events being processed. 3. Verify the configuration of the state machine to ensure it correctly handles transitions for the `SUCCEEDED` state.",
            "problem_location": {
                "files": [
                    "TaskImpl.java",
                    "MRAppMaster.java"
                ],
                "classes": [
                    "org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl",
                    "org.apache.hadoop.mapreduce.v2.app.MRAppMaster"
                ],
                "methods": [
                    "TaskImpl.handle",
                    "MRAppMaster$TaskEventDispatcher.handle"
                ]
            },
            "possible_fix": "Implement checks in the event handling logic of TaskImpl to prevent processing of success events when the task is already in the SUCCEEDED state. Additionally, ensure that the state machine is properly configured to handle transitions."
        },
        "possible_fix_code": {
            "TaskImpl.handle": "  public void handle(TaskEvent event) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Processing \" + event.getTaskID() + \" of type \" + event.getType());\n    }\n    try {\n      writeLock.lock();\n      TaskStateInternal oldState = getInternalState();\n      // Check if the event is T_ATTEMPT_SUCCEEDED and the current state is SUCCEEDED\n      if (event.getType() == TaskEventType.T_ATTEMPT_SUCCEEDED && oldState == TaskStateInternal.SUCCEEDED) {\n        LOG.warn(\"Ignoring T_ATTEMPT_SUCCEEDED event for task \" + this.taskId + \" as it is already in SUCCEEDED state.\");\n        return; // Prevent processing this event\n      }\n      try {\n        stateMachine.doTransition(event.getType(), event);\n      } catch (InvalidStateTransitonException e) {\n        LOG.error(\"Can't handle this event at current state for \" + this.taskId, e);\n        internalError(event.getType());\n      }\n      if (oldState != getInternalState()) {\n        LOG.info(taskId + \" Task Transitioned from \" + oldState + \" to \" + getInternalState());\n      }\n    } finally {\n      writeLock.unlock();\n    }\n  }"
        }
    },
    {
        "filename": "MAPREDUCE-3062.json",
        "creation_time": "2011-09-21T17:31:02.000+0000",
        "bug_report": {
            "Title": "YARN NM/RM fail to start",
            "Description": "The YARN ResourceManager fails to start due to a misconfiguration of the `yarn.resourcemanager.admin.address` property. The error message indicates that the value provided for this property is not in the expected 'host:port' format, leading to a RuntimeException during the initialization of the ResourceManager.",
            "StackTrace": [
                "2011-09-21 10:21:41,932 FATAL resourcemanager.ResourceManager (ResourceManager.java:main(502)) - Error starting ResourceManager",
                "java.lang.RuntimeException: Not a host:port pair: yarn.resourcemanager.admin.address",
                "\tat org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:148)",
                "\tat org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:132)",
                "\tat org.apache.hadoop.yarn.server.resourcemanager.AdminService.init(AdminService.java:88)",
                "\tat org.apache.hadoop.yarn.service.CompositeService.init(CompositeService.java:58)",
                "\tat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.init(ResourceManager.java:191)",
                "\tat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:497)"
            ],
            "RootCause": "The root cause of the issue is the incorrect format of the `yarn.resourcemanager.admin.address` configuration property, which is expected to be in the 'host:port' format but is not correctly set.",
            "StepsToReproduce": [
                "1. Open the YARN configuration file (e.g., yarn-site.xml).",
                "2. Check the value of the `yarn.resourcemanager.admin.address` property.",
                "3. Attempt to start the YARN ResourceManager."
            ],
            "ExpectedBehavior": "The YARN ResourceManager should start successfully without any fatal errors, indicating that the configuration is correct.",
            "ObservedBehavior": "The YARN ResourceManager fails to start, throwing a RuntimeException due to an invalid configuration for the `yarn.resourcemanager.admin.address` property.",
            "Suggestions": "Ensure that the `yarn.resourcemanager.admin.address` property is set correctly in the configuration file, following the 'host:port' format. Refer to YARN documentation for examples of valid configurations.",
            "problem_location": {
                "files": [
                    "yarn-site.xml"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.ResourceManager",
                    "org.apache.hadoop.yarn.server.resourcemanager.AdminService"
                ],
                "methods": [
                    "AdminService.init",
                    "ResourceManager.init"
                ]
            },
            "possible_fix": "Update the `yarn.resourcemanager.admin.address` property in the yarn-site.xml file to a valid 'host:port' format, for example: 'localhost:8080'."
        },
        "possible_fix_code": {
            "AdminService.init": "  public void init(Configuration conf) {\n    super.init(conf);\n    String bindAddress =\n      conf.get(YarnConfiguration.RM_ADMIN_ADDRESS,\n          YarnConfiguration.RM_ADMIN_ADDRESS);\n    // Validate the bindAddress format\n    if (!isValidHostPort(bindAddress)) {\n        throw new RuntimeException(\"Not a host:port pair: \" + bindAddress);\n    }\n    masterServiceAddress =  NetUtils.createSocketAddr(bindAddress);\n    adminAcl = \n      new AccessControlList(\n          conf.get(YarnConfiguration.RM_ADMIN_ACL, YarnConfiguration.DEFAULT_RM_ADMIN_ACL));\n  }\n\n  private boolean isValidHostPort(String address) {\n      return address != null && address.matches(\"^[^:]+:\\d+$\");\n  }"
        }
    },
    {
        "filename": "MAPREDUCE-5724.json",
        "creation_time": "2014-01-15T00:58:12.000+0000",
        "bug_report": {
            "Title": "JobHistoryServer does not start if HDFS is not running",
            "Description": "The Job History Server (JHS) fails to start when the Hadoop Distributed File System (HDFS) is not operational. The error log indicates a `YarnRuntimeException` caused by an inability to create a 'done' directory in HDFS due to a connection refusal when attempting to reach the HDFS namenode at `localhost:8020`. This suggests that the HDFS service is either not running or misconfigured.",
            "StackTrace": [
                "org.apache.hadoop.yarn.exceptions.YarnRuntimeException: Error creating done directory: [hdfs://localhost:8020/tmp/hadoop-yarn/staging/history/done]",
                "at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.serviceInit(HistoryFileManager.java:505)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)",
                "at org.apache.hadoop.mapreduce.v2.hs.JobHistory.serviceInit(JobHistory.java:94)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)",
                "at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:108)",
                "at org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer.serviceInit(JobHistoryServer.java:143)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)",
                "at org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer.launchJobHistoryServer(JobHistoryServer.java:207)",
                "at org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer.main(JobHistoryServer.java:217)",
                "Caused by: java.net.ConnectException: Call From dontknow.local/172.20.10.4 to localhost:8020 failed on connection exception: java.net.ConnectException: Connection refused"
            ],
            "RootCause": "The root cause of the issue is a `Connection refused` error when the Job History Server attempts to connect to the HDFS namenode at `localhost:8020`. This indicates that the HDFS service is not running or is misconfigured.",
            "StepsToReproduce": [
                "Ensure that HDFS is not running.",
                "Attempt to start the Job History Server.",
                "Observe the error logs for connection issues."
            ],
            "ExpectedBehavior": "The Job History Server should start successfully and be able to create necessary directories in HDFS without any connection issues.",
            "ObservedBehavior": "The Job History Server fails to start, logging a `YarnRuntimeException` due to an inability to create the 'done' directory in HDFS, resulting from a connection refusal to the namenode.",
            "Suggestions": "1. Verify that the HDFS namenode is running. 2. Check the configuration files `core-site.xml` and `hdfs-site.xml` to ensure the namenode is set to listen on `localhost:8020`. 3. Investigate any network issues or firewall settings that may be blocking access to the namenode.",
            "problem_location": {
                "files": [
                    "org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.java"
                ],
                "classes": [
                    "org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager",
                    "org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer"
                ],
                "methods": [
                    "HistoryFileManager.serviceInit",
                    "HistoryFileManager.mkdir"
                ]
            },
            "possible_fix": "Ensure that the HDFS namenode is running and accessible at `localhost:8020`. If necessary, modify the configuration files to correct any misconfigurations."
        },
        "possible_fix_code": {
            "HistoryFileManager.serviceInit": "  protected void serviceInit(Configuration conf) throws Exception {\n    this.conf = conf;\n\n    int serialNumberLowDigits = 3;\n    serialNumberFormat = (\"%0\" + (JobHistoryUtils.SERIAL_NUMBER_DIRECTORY_DIGITS + serialNumberLowDigits) + \"d\");\n\n    String doneDirPrefix = JobHistoryUtils.getConfiguredHistoryServerDoneDirPrefix(conf);\n    try {\n      doneDirPrefixPath = FileContext.getFileContext(conf).makeQualified(new Path(doneDirPrefix));\n      doneDirFc = FileContext.getFileContext(doneDirPrefixPath.toUri(), conf);\n      doneDirFc.setUMask(JobHistoryUtils.HISTORY_DONE_DIR_UMASK);\n      mkdir(doneDirFc, doneDirPrefixPath, new FsPermission(JobHistoryUtils.HISTORY_DONE_DIR_PERMISSION));\n    } catch (IOException e) {\n      LOG.error(\"Failed to create done directory: \" + doneDirPrefixPath, e);\n      throw new YarnRuntimeException(\"Error creating done directory: [\" + doneDirPrefixPath + \"]\", e);\n    }\n\n    String intermediateDoneDirPrefix = JobHistoryUtils.getConfiguredHistoryIntermediateDoneDirPrefix(conf);\n    try {\n      intermediateDoneDirPath = FileContext.getFileContext(conf).makeQualified(new Path(intermediateDoneDirPrefix));\n      intermediateDoneDirFc = FileContext.getFileContext(intermediateDoneDirPath.toUri(), conf);\n      mkdir(intermediateDoneDirFc, intermediateDoneDirPath, new FsPermission(JobHistoryUtils.HISTORY_INTERMEDIATE_DONE_DIR_PERMISSIONS.toShort()));\n    } catch (IOException e) {\n      LOG.error(\"Failed to create intermediate done directory: \" + intermediateDoneDirPath, e);\n      throw new YarnRuntimeException(\"Error creating intermediate done directory: [\" + intermediateDoneDirPath + \"]\", e);\n    }\n\n    this.aclsMgr = new JobACLsManager(conf);\n\n    maxHistoryAge = conf.getLong(JHAdminConfig.MR_HISTORY_MAX_AGE_MS, JHAdminConfig.DEFAULT_MR_HISTORY_MAX_AGE);\n    jobListCache = createJobListCache();\n\n    serialNumberIndex = new SerialNumberIndex(conf.getInt(JHAdminConfig.MR_HISTORY_DATESTRING_CACHE_SIZE, JHAdminConfig.DEFAULT_MR_HISTORY_DATESTRING_CACHE_SIZE));\n\n    int numMoveThreads = conf.getInt(JHAdminConfig.MR_HISTORY_MOVE_THREAD_COUNT, JHAdminConfig.DEFAULT_MR_HISTORY_MOVE_THREAD_COUNT);\n    ThreadFactory tf = new ThreadFactoryBuilder().setNameFormat(\"MoveIntermediateToDone Thread #%d\").build();\n    moveToDoneExecutor = new ThreadPoolExecutor(numMoveThreads, numMoveThreads, 1, TimeUnit.HOURS, new LinkedBlockingQueue<Runnable>(), tf);\n\n    super.serviceInit(conf);\n  }"
        }
    },
    {
        "filename": "MAPREDUCE-5358.json",
        "creation_time": "2013-06-28T05:40:45.000+0000",
        "bug_report": {
            "Title": "MRAppMaster throws invalid transitions for JobImpl",
            "Description": "The MRAppMaster in the Hadoop YARN framework is encountering invalid state transitions for job events. Specifically, the events `JOB_TASK_ATTEMPT_COMPLETED` and `JOB_MAP_TASK_RESCHEDULED` are being triggered while the job is in the `SUCCEEDED` state, which violates the state machine's rules. This leads to `InvalidStateTransitonException` errors, indicating that the system cannot handle these events in the current state. The issue arises from the improper handling of job events in the `JobImpl` class, which is responsible for managing job states and transitions.",
            "StackTrace": [
                "2013-06-26 11:39:50,128 ERROR [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: Can't handle this event at current state",
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: JOB_TASK_ATTEMPT_COMPLETED at SUCCEEDED",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:301)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:443)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:720)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:119)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:962)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:958)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:128)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:77)",
                "at java.lang.Thread.run(Thread.java:662)",
                "2013-06-26 11:39:50,129 ERROR [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: Can't handle this event at current state",
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: JOB_MAP_TASK_RESCHEDULED at SUCCEEDED",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:301)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:443)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:720)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:119)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:962)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:958)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:128)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:77)",
                "at java.lang.Thread.run(Thread.java:662)"
            ],
            "RootCause": "The root cause of the issue is that the job is incorrectly processing events that are not valid in the `SUCCEEDED` state. The state machine for job events does not allow transitions for `JOB_TASK_ATTEMPT_COMPLETED` and `JOB_MAP_TASK_RESCHEDULED` when the job has already succeeded.",
            "StepsToReproduce": [
                "Submit a job to the Hadoop YARN framework.",
                "Ensure that the job completes successfully.",
                "Trigger events `JOB_TASK_ATTEMPT_COMPLETED` and `JOB_MAP_TASK_RESCHEDULED` after the job has reached the `SUCCEEDED` state."
            ],
            "ExpectedBehavior": "The system should not process job events that are invalid for the current state. Specifically, no events should be handled once the job has succeeded.",
            "ObservedBehavior": "The system throws `InvalidStateTransitonException` errors when attempting to handle job events that are not allowed in the `SUCCEEDED` state.",
            "Suggestions": "Review the event handling logic in the `JobImpl` class to ensure that it correctly checks the job state before processing events. Implement state validation checks to prevent invalid transitions.",
            "problem_location": {
                "files": [
                    "org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.java",
                    "org.apache.hadoop.mapreduce.v2.app.MRAppMaster.java"
                ],
                "classes": [
                    "org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl",
                    "org.apache.hadoop.mapreduce.v2.app.MRAppMaster"
                ],
                "methods": [
                    "JobImpl.handle",
                    "MRAppMaster$JobEventDispatcher.handle"
                ]
            },
            "possible_fix": "Add state validation checks in the `handle` method of `JobImpl` to ensure that events like `JOB_TASK_ATTEMPT_COMPLETED` and `JOB_MAP_TASK_RESCHEDULED` are not processed when the job is in the `SUCCEEDED` state."
        },
        "possible_fix_code": {
            "JobImpl.handle": "public void handle(JobEvent event) {\n    JobState currentState = this.getState();\n    JobEventType eventType = event.getType();\n\n    // Check if the job is in the SUCCEEDED state\n    if (currentState == JobState.SUCCEEDED) {\n        // Do not process events that are invalid in the SUCCEEDED state\n        if (eventType == JobEventType.JOB_TASK_ATTEMPT_COMPLETED || eventType == JobEventType.JOB_MAP_TASK_RESCHEDULED) {\n            // Log a warning or ignore the event\n            LOG.warn(\"Ignoring event \" + eventType + \" in SUCCEEDED state.\");\n            return;\n        }\n    }\n\n    // Handle other events normally\n    // ... existing event handling logic ...\n}"
        }
    },
    {
        "filename": "MAPREDUCE-5837.json",
        "creation_time": "2014-04-15T20:24:58.000+0000",
        "bug_report": {
            "Title": "MRAppMaster fails when checking on uber mode",
            "Description": "The MRAppMaster encounters a failure when determining if a job should run in uber mode due to a missing Scala library in the classpath. The issue arises in the `isChainJob` method of the `JobImpl` class, where it attempts to load the class specified in the configuration using `Class.forName()`. If the class is not found, a `NoClassDefFoundError` is thrown, indicating that the required Scala library is not available. This is particularly problematic for jobs that depend on Scala, such as those utilizing `scala.Function1`.",
            "StackTrace": [
                "2014-04-15 11:52:55,877 FATAL [main] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: Error starting MRAppMaster",
                "java.lang.NoClassDefFoundError: scala/Function1",
                "at java.lang.Class.forName0(Native Method)",
                "at java.lang.Class.forName(Class.java:190)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.isChainJob(JobImpl.java:1282)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.makeUberDecision(JobImpl.java:1224)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.access$3700(JobImpl.java:136)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$InitTransition.transition(JobImpl.java:1425)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$InitTransition.transition(JobImpl.java:1363)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$MultipleInternalArc.doTransition(StateMachineFactory.java:385)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:976)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:135)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:1263)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.serviceStart(MRAppMaster.java:1063)",
                "at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$1.run(MRAppMaster.java:1480)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1606)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.initAndStartAppMaster(MRAppMaster.java:1476)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.main(MRAppMaster.java:1409)",
                "Caused by: java.lang.ClassNotFoundException: scala.Function1",
                "at java.net.URLClassLoader$1.run(URLClassLoader.java:366)",
                "at java.net.URLClassLoader$1.run(URLClassLoader.java:355)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at java.net.URLClassLoader.findClass(URLClassLoader.java:354)",
                "at java.lang.ClassLoader.loadClass(ClassLoader.java:425)",
                "at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)",
                "at java.lang.ClassLoader.loadClass(ClassLoader.java:358)"
            ],
            "RootCause": "The root cause of the issue is a `NoClassDefFoundError` for `scala.Function1`, which indicates that the Scala library is not included in the classpath when the MRAppMaster is executed. This is a dependency management issue that prevents the application from recognizing Scala classes.",
            "StepsToReproduce": [
                "Configure a Hadoop job that requires Scala dependencies.",
                "Ensure that the Scala library is not included in the classpath.",
                "Submit the job to the MRAppMaster in uber mode."
            ],
            "ExpectedBehavior": "The MRAppMaster should successfully determine if the job can run in uber mode without throwing any errors related to missing classes.",
            "ObservedBehavior": "The MRAppMaster fails to start and throws a `NoClassDefFoundError` for `scala.Function1`, indicating that the Scala library is missing from the classpath.",
            "Suggestions": "Ensure that all required Scala libraries are included in the classpath when submitting the job. This may involve updating the build configuration (e.g., Maven or Gradle) to include Scala dependencies.",
            "problem_location": {
                "files": [
                    "hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/JobImpl.java",
                    "hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/MRAppMaster.java"
                ],
                "classes": [
                    "org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl",
                    "org.apache.hadoop.mapreduce.v2.app.MRAppMaster"
                ],
                "methods": [
                    "JobImpl.isChainJob",
                    "MRAppMaster.initAndStartAppMaster"
                ]
            },
            "possible_fix": "Modify the job submission process to include the Scala library in the classpath. This can be done by updating the build configuration to ensure that Scala dependencies are packaged correctly."
        },
        "possible_fix_code": {
            "JobImpl.isChainJob": "  private boolean isChainJob(Configuration conf) {\n    boolean isChainJob = false;\n    try {\n      String mapClassName = conf.get(MRJobConfig.MAP_CLASS_ATTR);\n      if (mapClassName != null) {\n        Class<?> mapClass = Class.forName(mapClassName);\n        if (ChainMapper.class.isAssignableFrom(mapClass))\n          isChainJob = true;\n      }\n    } catch (ClassNotFoundException cnfe) {\n      // don't care; assume it's not derived from ChainMapper\n    } catch (NoClassDefFoundError ncdfe) {\n      // Log the error and handle the missing Scala library case\n      LOG.error(\"Scala library is missing from the classpath: scala.Function1\", ncdfe);\n    }\n    try {\n      String reduceClassName = conf.get(MRJobConfig.REDUCE_CLASS_ATTR);\n      if (reduceClassName != null) {\n        Class<?> reduceClass = Class.forName(reduceClassName);\n        if (ChainReducer.class.isAssignableFrom(reduceClass))\n          isChainJob = true;\n      }\n    } catch (ClassNotFoundException cnfe) {\n      // don't care; assume it's not derived from ChainReducer\n    } catch (NoClassDefFoundError ncdfe) {\n      // Log the error and handle the missing Scala library case\n      LOG.error(\"Scala library is missing from the classpath: scala.Function1\", ncdfe);\n    }\n    return isChainJob;\n  }"
        }
    },
    {
        "filename": "MAPREDUCE-3058.json",
        "creation_time": "2011-09-21T12:53:39.000+0000",
        "bug_report": {
            "Title": "Sometimes task keeps on running while its Syslog says that it is shutdown",
            "Description": "During the execution of GridMixV3, a job was observed to be stuck for over 15 hours. Upon inspecting the job page, it was noted that one of the reducer tasks was not completing. The syslog for this reducer indicated a series of errors related to data transfer, specifically an `EOFException` which suggests that the data stream was interrupted or incomplete. The logs show that the task was expected to stop within 20 seconds, but it remained active for an extended period, indicating a potential issue with the HDFS data nodes or network connectivity.",
            "StackTrace": [
                "java.io.EOFException: Premature EOF: no length prefix available",
                "at org.apache.hadoop.hdfs.protocol.HdfsProtoUtil.vintPrefixed(HdfsProtoUtil.java:158)",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.transfer(DFSOutputStream.java:860)",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:838)",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:929)",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:740)",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:415)"
            ],
            "RootCause": "The root cause of the issue appears to be an `EOFException` occurring during data transfer in the HDFS context, likely due to network issues, data node failures, or data corruption.",
            "StepsToReproduce": [
                "Run GridMixV3 with a job that includes multiple reducers.",
                "Monitor the job page for any reducers that appear to be stuck.",
                "Check the syslog for the reducer to identify any exceptions or errors."
            ],
            "ExpectedBehavior": "The reducer task should complete successfully within a reasonable time frame, and the system should not report it as running after it has been shut down.",
            "ObservedBehavior": "The reducer task was reported as running for over 15 hours despite syslog entries indicating that it should have stopped within 20 seconds.",
            "Suggestions": "Investigate the health of the HDFS data nodes and check for any network interruptions. Review the HDFS configuration settings and ensure that all data nodes are operational. Additionally, check for any data corruption in the files being processed.",
            "problem_location": {
                "files": [
                    "HdfsProtoUtil.java",
                    "DFSOutputStream.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.protocol.HdfsProtoUtil",
                    "org.apache.hadoop.hdfs.DFSOutputStream"
                ],
                "methods": [
                    "vintPrefixed",
                    "transfer"
                ]
            },
            "possible_fix": "Ensure that all data nodes are healthy and operational. Check network connectivity between the client and data nodes. Review and correct any configuration issues in the HDFS setup."
        },
        "possible_fix_code": {
            "transfer": "public void transfer() throws IOException {\n    // Ensure that the data node is healthy before proceeding with the transfer\n    if (!isDataNodeHealthy()) {\n        throw new IOException(\"Data node is not healthy, aborting transfer.\");\n    }\n\n    // Attempt to transfer data, handling potential EOFException\n    try {\n        // Existing data transfer logic here\n        // ...\n    } catch (EOFException e) {\n        // Log the error and handle the premature EOF\n        LOG.error(\"EOFException during data transfer: \" + e.getMessage());\n        // Implement retry logic or failover to another data node if necessary\n        handleDataNodeError();\n    }\n}\n\nprivate boolean isDataNodeHealthy() {\n    // Logic to check the health of the data node\n    // This could involve checking the status of the data node, network connectivity, etc.\n    return true; // Placeholder for actual health check logic\n}\n\nprivate void handleDataNodeError() {\n    // Logic to handle data node errors, such as retrying the transfer or switching to another data node\n    // This could involve updating the pipeline or notifying the user\n}"
        }
    },
    {
        "filename": "MAPREDUCE-5088.json",
        "creation_time": "2013-03-15T17:55:08.000+0000",
        "bug_report": {
            "Title": "MR Client gets a renewer token exception while Oozie is submitting a job",
            "Description": "After the fix for HADOOP-9299, an exception occurs in Oozie while trying to submit a job. The error is related to the `renewer` field in the delegation token request, which is missing and leads to an `UninitializedMessageException`.",
            "StackTrace": [
                "org.apache.oozie.action.ActionExecutorException: UninitializedMessageException: Message missing required fields: renewer",
                "at org.apache.oozie.action.ActionExecutor.convertException(ActionExecutor.java:401)",
                "at org.apache.oozie.action.hadoop.JavaActionExecutor.submitLauncher(JavaActionExecutor.java:738)",
                "at org.apache.oozie.action.hadoop.JavaActionExecutor.start(JavaActionExecutor.java:889)",
                "at org.apache.oozie.command.wf.ActionStartXCommand.execute(ActionStartXCommand.java:211)",
                "at org.apache.oozie.command.wf.ActionStartXCommand.execute(ActionStartXCommand.java:59)",
                "at org.apache.oozie.command.XCommand.call(XCommand.java:277)",
                "at org.apache.oozie.service.CallableQueueService$CompositeCallable.call(CallableQueueService.java:326)",
                "at org.apache.oozie.service.CallableQueueService$CompositeCallable.call(CallableQueueService.java:255)",
                "at org.apache.oozie.service.CallableQueueService$CallableWrapper.run(CallableQueueService.java:175)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)",
                "at java.lang.Thread.run(Thread.java:662)",
                "Caused by: com.google.protobuf.UninitializedMessageException: Message missing required fields: renewer",
                "at com.google.protobuf.AbstractMessage$Builder.newUninitializedMessageException(AbstractMessage.java:605)",
                "at org.apache.hadoop.security.proto.SecurityProtos$GetDelegationTokenRequestProto$Builder.build(SecurityProtos.java:973)",
                "at org.apache.hadoop.mapreduce.v2.api.protocolrecords.impl.pb.GetDelegationTokenRequestPBImpl.mergeLocalToProto(GetDelegationTokenRequestPBImpl.java:84)",
                "at org.apache.hadoop.mapreduce.v2.api.protocolrecords.impl.pb.GetDelegationTokenRequestPBImpl.getProto(GetDelegationTokenRequestPBImpl.java:67)",
                "at org.apache.hadoop.mapreduce.v2.api.impl.pb.client.MRClientProtocolPBClientImpl.getDelegationToken(MRClientProtocolPBClientImpl.java:200)",
                "at org.apache.hadoop.mapred.YARNRunner.getDelegationTokenFromHS(YARNRunner.java:194)",
                "at org.apache.hadoop.mapred.YARNRunner.submitJob(YARNRunner.java:273)",
                "at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:392)",
                "at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1218)",
                "at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1215)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:396)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1439)",
                "at org.apache.hadoop.mapreduce.Job.submit(Job.java:1215)",
                "at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:581)",
                "at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:576)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:396)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1439)",
                "at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:576)",
                "at org.apache.oozie.action.hadoop.JavaActionExecutor.submitLauncher(JavaActionExecutor.java:723)"
            ],
            "RootCause": "The `renewer` field for delegation tokens is not being set correctly during job submission due to issues in user authentication or context initialization.",
            "StepsToReproduce": [
                "Authenticate a user using Kerberos with a keytab file.",
                "Submit a job to Oozie that requires delegation tokens.",
                "Observe the logs for the `UninitializedMessageException` related to the `renewer` field."
            ],
            "ExpectedBehavior": "The job should submit successfully without any exceptions related to missing fields in the delegation token request.",
            "ObservedBehavior": "An `UninitializedMessageException` is thrown indicating that the `renewer` field is missing when submitting a job.",
            "Suggestions": "Ensure that the user is authenticated correctly and that the user context is established before job submission.",
            "problem_location": {
                "files": [
                    "org/apache/oozie/action/hadoop/JavaActionExecutor.java",
                    "org/apache/hadoop/mapred/YARNRunner.java",
                    "org/apache/hadoop/mapred/JobSubmitter.java",
                    "org/apache/hadoop/security/UserGroupInformation.java"
                ],
                "classes": [
                    "org.apache.oozie.action.ActionExecutor",
                    "org.apache.hadoop.mapreduce.JobSubmitter",
                    "org.apache.hadoop.security.UserGroupInformation"
                ],
                "methods": [
                    "ActionExecutor.convertException",
                    "JavaActionExecutor.submitLauncher",
                    "YARNRunner.getDelegationTokenFromHS",
                    "JobSubmitter.submitJobInternal",
                    "UserGroupInformation.getCurrentUser",
                    "UserGroupInformation.getLoginUser"
                ]
            },
            "possible_fix": "Ensure that the user is authenticated properly using the `loginUserFromKeytab` method or another appropriate method. Verify that the user context is initialized correctly before job submission, which includes setting the `renewer` field for delegation tokens."
        },
        "possible_fix_code": {
            "YARNRunner.getDelegationTokenFromHS": "  Token<?> getDelegationTokenFromHS(MRClientProtocol hsProxy)\n      throws IOException, InterruptedException {\n    GetDelegationTokenRequest request = recordFactory\n      .newRecordInstance(GetDelegationTokenRequest.class);\n    String renewer = Master.getMasterPrincipal(conf);\n    if (renewer == null || renewer.isEmpty()) {\n        throw new IOException(\"Renewer cannot be null or empty\");\n    }\n    request.setRenewer(renewer);\n    DelegationToken mrDelegationToken = hsProxy.getDelegationToken(request)\n      .getDelegationToken();\n    return ProtoUtils.convertFromProtoFormat(mrDelegationToken,\n                                             hsProxy.getConnectAddress());\n  }"
        }
    }
]