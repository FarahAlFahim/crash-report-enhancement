[
    {
        "filename": "YARN-5918.json",
        "creation_time": "2016-11-20T14:19:00.000+0000",
        "bug_report": {
            "Title": "Handle Opportunistic scheduling allocate request failure when NM is lost",
            "Description": "The bug report addresses a failure in the allocation request during opportunistic container allocation when the NodeManager (NM) is lost. The stack trace indicates a `NullPointerException` occurring in the `OpportunisticContainerAllocatorAMService` class, specifically in the `convertToRemoteNode`, `convertToRemoteNodes`, and `getLeastLoadedNodes` methods. This issue arises when the system attempts to access a null reference, likely due to the NodeManager's unavailability.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.OpportunisticContainerAllocatorAMService.convertToRemoteNode(OpportunisticContainerAllocatorAMService.java:420)",
                "at org.apache.hadoop.yarn.server.resourcemanager.OpportunisticContainerAllocatorAMService.convertToRemoteNodes(OpportunisticContainerAllocatorAMService.java:412)",
                "at org.apache.hadoop.yarn.server.resourcemanager.OpportunisticContainerAllocatorAMService.getLeastLoadedNodes(OpportunisticContainerAllocatorAMService.java:402)",
                "at org.apache.hadoop.yarn.server.resourcemanager.OpportunisticContainerAllocatorAMService.allocate(OpportunisticContainerAllocatorAMService.java:236)"
            ],
            "RootCause": "The `NullPointerException` is likely caused by the `convertToRemoteNode` method attempting to access a node's HTTP address using a null reference. This can occur if `rmContext.getScheduler()` or `getNode(nodeId)` returns null, particularly when the NodeManager is lost.",
            "StepsToReproduce": [
                "1. Start the YARN ResourceManager and NodeManager.",
                "2. Submit an application that requires opportunistic container allocation.",
                "3. Forcefully stop the NodeManager while the application is running.",
                "4. Observe the logs for allocation request failures."
            ],
            "ExpectedBehavior": "The system should gracefully handle the loss of the NodeManager and either retry the allocation or report a meaningful error without throwing a `NullPointerException`.",
            "ObservedBehavior": "The system throws a `NullPointerException` in the `OpportunisticContainerAllocatorAMService` when attempting to allocate resources after the NodeManager has been lost.",
            "Suggestions": "Investigate the `NodeMonitor` class to ensure it properly handles scenarios where nodes are unavailable. Implement null checks in the `convertToRemoteNode` and `getLeastLoadedNodes` methods to prevent dereferencing null objects.",
            "problem_location": {
                "files": [
                    "OpportunisticContainerAllocatorAMService.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.OpportunisticContainerAllocatorAMService"
                ],
                "methods": [
                    "convertToRemoteNode",
                    "convertToRemoteNodes",
                    "getLeastLoadedNodes"
                ]
            },
            "possible_fix": "Add null checks in the `convertToRemoteNode` method to verify that both `rmContext.getScheduler()` and `getNode(nodeId)` return non-null values before proceeding. Additionally, ensure that `getLeastLoadedNodes` checks the validity of the nodes returned by `nodeMonitor.selectLeastLoadedNodes(this.k)`."
        },
        "possible_fix_code": {
            "convertToRemoteNode": "  private RemoteNode convertToRemoteNode(NodeId nodeId) {\n    if (rmContext.getScheduler() == null) {\n        throw new IllegalStateException(\"Scheduler is not available.\");\n    }\n    Node node = ((AbstractYarnScheduler)rmContext.getScheduler()).getNode(nodeId);\n    if (node == null) {\n        throw new IllegalArgumentException(\"Node not found for NodeId: \" + nodeId);\n    }\n    return RemoteNode.newInstance(nodeId, node.getHttpAddress());\n  }",
            "convertToRemoteNodes": "  private List<RemoteNode> convertToRemoteNodes(List<NodeId> nodeIds) {\n    ArrayList<RemoteNode> retNodes = new ArrayList<>();\n    for (NodeId nId : nodeIds) {\n      retNodes.add(convertToRemoteNode(nId));\n    }\n    return retNodes;\n  }",
            "getLeastLoadedNodes": "  private synchronized List<RemoteNode> getLeastLoadedNodes() {\n    long currTime = System.currentTimeMillis();\n    if ((currTime - lastCacheUpdateTime > cacheRefreshInterval) || cachedNodes == null) {\n      List<NodeId> selectedNodes = this.nodeMonitor.selectLeastLoadedNodes(this.k);\n      if (selectedNodes == null) {\n          throw new IllegalStateException(\"No nodes selected by NodeMonitor.\");\n      }\n      cachedNodes = convertToRemoteNodes(selectedNodes);\n      lastCacheUpdateTime = currTime;\n    }\n    return cachedNodes;\n  }"
        }
    },
    {
        "filename": "YARN-8629.json",
        "creation_time": "2018-08-07T00:14:14.000+0000",
        "bug_report": {
            "Title": "Container cleanup fails while trying to delete Cgroups",
            "Description": "The application fails to launch a container successfully, leading to a failure in the cleanup process. The error message indicates a `FileNotFoundException` when attempting to read the cgroup tasks file, which is expected to be located at `/sys/fs/cgroup/cpu,cpuacct/hadoop-yarn-tmp-cxx/container_e02_1533336898541_0010_20_000002/tasks`. This issue arises from the cgroup directory not being created or initialized properly, which is critical for managing resources in a YARN environment.",
            "StackTrace": [
                "2018-08-06 03:28:20,351 WARN  resources.CGroupsHandlerImpl (CGroupsHandlerImpl.java:checkAndDeleteCgroup(523)) - Failed to read cgroup tasks file.",
                "java.io.FileNotFoundException: /sys/fs/cgroup/cpu,cpuacct/hadoop-yarn-tmp-cxx/container_e02_1533336898541_0010_20_000002/tasks (No such file or directory)",
                "at java.io.FileInputStream.open0(Native Method)",
                "at java.io.FileInputStream.open(FileInputStream.java:195)",
                "at java.io.FileInputStream.<init>(FileInputStream.java:138)",
                "at java.io.FileInputStream.<init>(FileInputStream.java:93)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsHandlerImpl.checkAndDeleteCgroup(CGroupsHandlerImpl.java:507)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsHandlerImpl.deleteCGroup(CGroupsHandlerImpl.java:542)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsCpuResourceHandlerImpl.postComplete(CGroupsCpuResourceHandlerImpl.java:238)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.ResourceHandlerChain.postComplete(ResourceHandlerChain.java:111)",
                "at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.postComplete(LinuxContainerExecutor.java:964)",
                "at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.reapContainer(LinuxContainerExecutor.java:787)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.cleanupContainer(ContainerLaunch.java:821)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher.handle(ContainersLauncher.java:161)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher.handle(ContainersLauncher.java:57)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:197)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:126)",
                "at java.lang.Thread.run(Thread.java:748)",
                "2018-08-06 03:28:20,372 WARN  resources.CGroupsHandlerImpl (CGroupsHandlerImpl.java:checkAndDeleteCgroup(523)) - Failed to read cgroup tasks file."
            ],
            "RootCause": "The `FileNotFoundException` occurs because the cgroup directory was not created successfully, which is necessary for the cleanup process. This can happen due to failures in the `createCGroup` method or issues during the initialization of the cgroup hierarchy.",
            "StepsToReproduce": [
                "Attempt to launch a container in the YARN environment.",
                "Ensure that the container fails to launch.",
                "Observe the cleanup process and check for warnings related to cgroup tasks file."
            ],
            "ExpectedBehavior": "The container should launch successfully, and upon failure, the cleanup process should be able to access the cgroup tasks file without encountering a `FileNotFoundException`.",
            "ObservedBehavior": "The cleanup process fails with a `FileNotFoundException`, indicating that the expected cgroup tasks file does not exist.",
            "Suggestions": "Review the YARN configuration settings related to cgroups to ensure they are correct. Check the logs for any errors during the initialization of cgroups. Ensure that the necessary permissions are set for creating and accessing cgroup directories.",
            "problem_location": {
                "files": [
                    "CGroupsHandlerImpl.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsHandlerImpl"
                ],
                "methods": [
                    "checkAndDeleteCgroup",
                    "createCGroup",
                    "initializeCGroupController",
                    "initializePreMountedCGroupController"
                ]
            },
            "possible_fix": "Ensure that the `createCGroup` method is called successfully to create the necessary cgroup directory. Additionally, verify that the `initializePreMountedCGroupController` method is functioning correctly to set up the cgroup hierarchy."
        },
        "possible_fix_code": {
            "checkAndDeleteCgroup": "  private boolean checkAndDeleteCgroup(File cgf) throws InterruptedException {\n    boolean deleted = false;\n    try {\n      // Ensure the cgroup directory exists before attempting to read the tasks file\n      if (!cgf.exists() || !cgf.isDirectory()) {\n        LOG.warn(\"Cgroup directory does not exist: \" + cgf);\n        return deleted;\n      }\n\n      try (FileInputStream in = new FileInputStream(cgf + \"/tasks\")) {\n        if (in.read() == -1) {\n          Thread.sleep(deleteCGroupDelay);\n          deleted = cgf.delete();\n          if (!deleted) {\n            LOG.warn(\"Failed attempt to delete cgroup: \" + cgf);\n          }\n        } else {\n          logLineFromTasksFile(cgf);\n        }\n      } catch (IOException e) {\n        LOG.warn(\"Failed to read cgroup tasks file. \", e);\n      }\n    } catch (Exception e) {\n      LOG.error(\"An unexpected error occurred while checking and deleting cgroup: \", e);\n    }\n    return deleted;\n  }"
        }
    },
    {
        "filename": "YARN-4431.json",
        "creation_time": "2015-12-07T18:31:36.000+0000",
        "bug_report": {
            "Title": "Unnecessary unRegistration of NodeManager on connection failure to ResourceManager",
            "Description": "The NodeManager (NM) attempts to connect to the ResourceManager (RM) at an invalid address (0.0.0.0:8031), leading to repeated connection failures. The NM's NodeStatusUpdaterImpl retries the connection according to a defined policy. After exhausting the retry attempts, it triggers a shutdown event, which subsequently calls the unRegisterNM() method. This behavior is unnecessary when the NM is shutting down due to connection issues, as it leads to redundant attempts to unregister the NM from the RM.",
            "StackTrace": [
                "java.net.ConnectException: Call From jduMBP.local/10.200.10.53 to 0.0.0.0:8031 failed on connection exception: java.net.ConnectException: Connection refused",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1452)",
                "at org.apache.hadoop.yarn.server.api.impl.pb.client.ResourceTrackerPBClientImpl.unRegisterNodeManager(ResourceTrackerPBClientImpl.java:98)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.unRegisterNM(NodeStatusUpdaterImpl.java:267)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.serviceStop(NodeStatusUpdaterImpl.java:245)"
            ],
            "RootCause": "The NM is attempting to connect to an invalid address (0.0.0.0:8031), which results in a ConnectException. This is compounded by the NM's logic to unregister itself upon shutdown, which is unnecessary in this context.",
            "StepsToReproduce": [
                "1. Configure the NodeManager to point to an invalid ResourceManager address (0.0.0.0:8031).",
                "2. Start the NodeManager.",
                "3. Observe the logs for connection attempts and subsequent unregistration attempts."
            ],
            "ExpectedBehavior": "The NodeManager should handle connection failures gracefully without attempting to unregister itself if it is shutting down due to connection issues.",
            "ObservedBehavior": "The NodeManager repeatedly attempts to connect to an invalid address and unnecessarily calls unRegisterNM() during shutdown, leading to redundant operations.",
            "Suggestions": "Modify the NodeStatusUpdaterImpl to skip the unRegisterNM() call when the shutdown is due to connection issues. This can prevent unnecessary retries and improve efficiency.",
            "problem_location": {
                "files": [
                    "NodeStatusUpdaterImpl.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl"
                ],
                "methods": [
                    "unRegisterNM",
                    "serviceStop"
                ]
            },
            "possible_fix": "In the NodeStatusUpdaterImpl class, modify the serviceStop() method to check the reason for shutdown and skip the unRegisterNM() call if it is due to connection issues."
        },
        "possible_fix_code": {
            "serviceStop": "  protected void serviceStop() throws Exception {\n    // Check if the shutdown is due to connection issues\n    if (this.registeredWithRM && !this.isStopped\n        && !isNMUnderSupervisionWithRecoveryEnabled()\n        && !context.getDecommissioned() && !isShuttingDownDueToConnectionIssues()) {\n      unRegisterNM();\n    }\n    // Interrupt the updater.\n    this.isStopped = true;\n    stopRMProxy();\n    super.serviceStop();\n  }\n\n  private boolean isShuttingDownDueToConnectionIssues() {\n    // Logic to determine if the shutdown is due to connection issues\n    // This could involve checking the last connection attempt status\n    // or any other relevant flags that indicate a connection failure.\n    return /* condition to check for connection issues */;\n  }"
        }
    },
    {
        "filename": "YARN-2273.json",
        "creation_time": "2014-07-10T18:38:53.000+0000",
        "bug_report": {
            "Title": "NPE in ContinuousScheduling thread when we lose a node",
            "Description": "A NullPointerException (NPE) occurs in the ContinuousScheduling thread of the Apache Hadoop YARN framework when a DataNode (DN) experiences memory errors and subsequently reboots and rejoins the cluster. The issue arises after the ResourceManager (RM) attempts to manage resources for a node that has gone offline, leading to a failure in the NodeAvailableResourceComparator's compare method. This results in the RM being unable to assign containers, crippling YARN's functionality.",
            "StackTrace": [
                "2014-07-09 21:47:36,571 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler: Application attempt appattempt_1404858438119_4352_000001 released container container_1404858438119_4352_01_000004 on node: host: node-A16-R09-19.hadoop.dfw.wordpress.com:8041 #containers=0 available=<memory:8192, vCores:8> used=<memory:0, vCores:0> with event: KILL",
                "2014-07-09 21:47:36,571 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler: Removed node node-A16-R09-19.hadoop.dfw.wordpress.com:8041 cluster capacity: <memory:335872, vCores:328>",
                "2014-07-09 21:47:36,571 ERROR org.apache.hadoop.yarn.YarnUncaughtExceptionHandler: Thread Thread[ContinuousScheduling,5,main] threw an Exception.",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler$NodeAvailableResourceComparator.compare(FairScheduler.java:1044)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler$NodeAvailableResourceComparator.compare(FairScheduler.java:1040)",
                "at java.util.TimSort.countRunAndMakeAscending(TimSort.java:329)",
                "at java.util.TimSort.sort(TimSort.java:203)",
                "at java.util.TimSort.sort(TimSort.java:173)",
                "at java.util.Arrays.sort(Arrays.java:659)",
                "at java.util.Collections.sort(Collections.java:217)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.continuousScheduling(FairScheduler.java:1012)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.access$600(FairScheduler.java:124)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler$2.run(FairScheduler.java:1306)",
                "at java.lang.Thread.run(Thread.java:744)"
            ],
            "RootCause": "The NullPointerException is caused by the NodeAvailableResourceComparator's compare method attempting to access an object that is null, likely due to the state of the node being improperly handled after it has been removed from the cluster.",
            "StepsToReproduce": [
                "1. Start a YARN cluster with multiple DataNodes.",
                "2. Simulate a memory error on one of the DataNodes, causing it to reboot.",
                "3. Allow the DataNode to rejoin the cluster.",
                "4. Observe the ResourceManager's logs for the NullPointerException in the ContinuousScheduling thread."
            ],
            "ExpectedBehavior": "The ResourceManager should gracefully handle the removal and rejoining of DataNodes, ensuring that resource comparisons do not result in NullPointerExceptions.",
            "ObservedBehavior": "The ResourceManager throws a NullPointerException in the ContinuousScheduling thread, leading to an inability to assign containers and progress jobs.",
            "Suggestions": "Ensure that all objects being compared in the NodeAvailableResourceComparator are properly initialized and that the state of nodes is accurately tracked to prevent null references.",
            "problem_location": {
                "files": [
                    "FairScheduler.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler"
                ],
                "methods": [
                    "NodeAvailableResourceComparator.compare"
                ]
            },
            "possible_fix": "Review the implementation of the NodeAvailableResourceComparator to ensure that it checks for null values before attempting to compare resources. Additionally, implement better state management for nodes in the cluster."
        },
        "possible_fix_code": {
            "NodeAvailableResourceComparator.compare": "    public int compare(NodeId n1, NodeId n2) {\n        // Check if either node is null before proceeding with comparison\n        if (n1 == null || n2 == null) {\n            throw new IllegalArgumentException(\"NodeId cannot be null\");\n        }\n        // Ensure that the nodes exist in the nodes map\n        FSSchedulerNode node1 = nodes.get(n1);\n        FSSchedulerNode node2 = nodes.get(n2);\n        if (node1 == null || node2 == null) {\n            throw new IllegalArgumentException(\"One of the nodes is not present in the scheduler\");\n        }\n        return RESOURCE_CALCULATOR.compare(clusterResource,\n                node2.getAvailableResource(),\n                node1.getAvailableResource());\n    }"
        }
    },
    {
        "filename": "YARN-2834.json",
        "creation_time": "2014-11-09T06:07:01.000+0000",
        "bug_report": {
            "Title": "Resource manager crashed with Null Pointer Exception",
            "Description": "The Resource Manager encountered a Null Pointer Exception during the restart process, specifically within the CapacityScheduler class. The exception occurred when the system attempted to add an application attempt, indicating that certain expected objects were not initialized properly. The stack trace reveals that the issue arises in the addApplicationAttempt method, which is responsible for managing application attempts in the scheduler. This failure leads to the Resource Manager being unable to recover from its previous state, resulting in a crash.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.addApplicationAttempt(CapacityScheduler.java:734)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:1089)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:114)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl$AttemptRecoveredTransition.transition(RMAppAttemptImpl.java:1041)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl$AttemptRecoveredTransition.transition(RMAppAttemptImpl.java:1005)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$MultipleInternalArc.doTransition(StateMachineFactory.java:385)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:757)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:106)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.recoverAppAttempts(RMAppImpl.java:821)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.access$1900(RMAppImpl.java:101)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl$RMAppRecoveredTransition.transition(RMAppImpl.java:843)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl$RMAppRecoveredTransition.transition(RMAppImpl.java:826)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$MultipleInternalArc.doTransition(StateMachineFactory.java:385)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.handle(RMAppImpl.java:701)",
                "at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.recoverApplication(RMAppManager.java:312)",
                "at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.recover(RMAppManager.java:413)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.recover(ResourceManager.java:1207)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:590)",
                "at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager.java:1014)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1051)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1047)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.transitionToActive(ResourceManager.java:1047)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:1091)",
                "at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1226)"
            ],
            "RootCause": "The Null Pointer Exception in the addApplicationAttempt method is likely caused by either the application not being found in the applications map (indicating it was not added successfully) or the queue associated with the application being null due to improper configuration or initialization.",
            "StepsToReproduce": [
                "Restart the Resource Manager.",
                "Submit an application to the CapacityScheduler.",
                "Observe the logs for the Null Pointer Exception."
            ],
            "ExpectedBehavior": "The Resource Manager should successfully recover from its previous state and handle application attempts without encountering exceptions.",
            "ObservedBehavior": "The Resource Manager crashes with a Null Pointer Exception during the recovery process, preventing it from functioning correctly.",
            "Suggestions": "Check the initialization of the applications map and ensure that all applications are properly added before attempting to recover. Verify that the queues are correctly configured and initialized.",
            "problem_location": {
                "files": [
                    "CapacityScheduler.java",
                    "FiCaSchedulerApp.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler",
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp"
                ],
                "methods": [
                    "CapacityScheduler.addApplicationAttempt",
                    "FiCaSchedulerApp.getQueue"
                ]
            },
            "possible_fix": "Ensure that the applications map is populated correctly in the addApplication method before calling addApplicationAttempt. Additionally, validate that the queue associated with each application is not null."
        },
        "possible_fix_code": {
            "CapacityScheduler.addApplicationAttempt": "  private synchronized void addApplicationAttempt(\n      ApplicationAttemptId applicationAttemptId,\n      boolean transferStateFromPreviousAttempt,\n      boolean isAttemptRecovering) {\n    SchedulerApplication<FiCaSchedulerApp> application =\n        applications.get(applicationAttemptId.getApplicationId());\n    if (application == null) {\n        LOG.error(\"Application not found for ID: \" + applicationAttemptId.getApplicationId());\n        return; // Early exit if application is not found\n    }\n    CSQueue queue = (CSQueue) application.getQueue();\n    if (queue == null) {\n        LOG.error(\"Queue is null for application: \" + application.getApplicationId());\n        return; // Early exit if queue is null\n    }\n\n    FiCaSchedulerApp attempt =\n        new FiCaSchedulerApp(applicationAttemptId, application.getUser(),\n          queue, queue.getActiveUsersManager(), rmContext);\n    if (transferStateFromPreviousAttempt) {\n      attempt.transferStateFromPreviousAttempt(application\n        .getCurrentAppAttempt());\n    }\n    application.setCurrentAppAttempt(attempt);\n\n    queue.submitApplicationAttempt(attempt, application.getUser());\n    LOG.info(\"Added Application Attempt \" + applicationAttemptId\n        + \" to scheduler from user \" + application.getUser() + \" in queue \"\n        + queue.getQueueName());\n    if (isAttemptRecovering) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(applicationAttemptId\n            + \" is recovering. Skipping notifying ATTEMPT_ADDED\");\n      }\n    } else {\n      rmContext.getDispatcher().getEventHandler().handle(\n        new RMAppAttemptEvent(applicationAttemptId,\n            RMAppAttemptEventType.ATTEMPT_ADDED));\n    }\n  }"
        }
    },
    {
        "filename": "YARN-370.json",
        "creation_time": "2013-02-01T04:02:58.000+0000",
        "bug_report": {
            "Title": "CapacityScheduler app submission fails when min alloc size not multiple of AM size",
            "Description": "The issue arises when attempting to submit an application using the CapacityScheduler in a Hadoop YARN environment. The application is configured with a minimum allocation size of 1G and an Application Master (AM) size of 1.5G. The error indicates a failure to launch the application due to a mismatch in expected and actual resource allocations, specifically an unauthorized request to start a container. The expected resources were 2048 MB of memory, but only 1536 MB was found, leading to the failure of the application launch.",
            "StackTrace": [
                "Application application_1359688216672_0001 failed 1 times due to Error launching appattempt_1359688216672_0001_000001.",
                "Got exception: RemoteTrace: at LocalTrace: org.apache.hadoop.yarn.exceptions.impl.pb.YarnRemoteExceptionPBImpl: RemoteTrace: at LocalTrace: org.apache.hadoop.yarn.exceptions.impl.pb.YarnRemoteExceptionPBImpl: Unauthorized request to start container.",
                "Expected resource <memory:2048, vCores:1> but found <memory:1536, vCores:1> at org.apache.hadoop.yarn.factories.impl.pb.YarnRemoteExceptionFactoryPBImpl.createYarnRemoteException(YarnRemoteExceptionFactoryPBImpl.java:39)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.authorizeRequest(ContainerManagerImpl.java:383)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.startContainer(ContainerManagerImpl.java:400)",
                "at org.apache.hadoop.yarn.api.impl.pb.service.ContainerManagerPBServiceImpl.startContainer(ContainerManagerPBServiceImpl.java:68)",
                "at org.apache.hadoop.yarn.proto.ContainerManager$ContainerManagerService$2.callBlockingMethod(ContainerManager.java:83)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:454)",
                "at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1014)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1735)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1441)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1729)",
                "at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)",
                "at java.lang.reflect.Constructor.newInstance(Constructor.java:525)",
                "at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:90)",
                "at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:57)",
                "at org.apache.hadoop.yarn.exceptions.impl.pb.YarnRemoteExceptionPBImpl.unwrapAndThrowException(YarnRemoteExceptionPBImpl.java:123)",
                "at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagerPBClientImpl.startContainer(ContainerManagerPBClientImpl.java:109)",
                "at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.launch(AMLauncher.java:111)",
                "at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.run(AMLauncher.java:255)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)",
                "at java.lang.Thread.run(Thread.java:722)"
            ],
            "RootCause": "The root cause of the issue is a mismatch between the requested resources (2048 MB) and the available resources (1536 MB) for the application, leading to an unauthorized request error when attempting to start the container.",
            "StepsToReproduce": [
                "1. Configure the CapacityScheduler with a minimum allocation size of 1G.",
                "2. Set the Application Master size to 1.5G.",
                "3. Submit an application without specifying a resource calculator (defaulting to DefaultResourceCalculator).",
                "4. Observe the error during the application launch."
            ],
            "ExpectedBehavior": "The application should launch successfully if the requested resources match the available resources in the YARN cluster.",
            "ObservedBehavior": "The application fails to launch due to an unauthorized request error, indicating a resource mismatch.",
            "Suggestions": "1. Ensure that the requested resources in the application configuration match the cluster's resource settings. 2. Adjust the YARN ResourceManager settings to allow for the requested memory and CPU configurations. 3. Verify user permissions to ensure they have access to the requested resources.",
            "problem_location": {
                "files": [
                    "ContainerManagerImpl.java",
                    "AMLauncher.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl",
                    "org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher"
                ],
                "methods": [
                    "ContainerManagerImpl.authorizeRequest",
                    "AMLauncher.launch"
                ]
            },
            "possible_fix": "Adjust the YARN configuration settings to ensure that the container memory limits are set to accommodate the requested resources. For example, update 'yarn.nodemanager.resource.memory-mb' and 'yarn.scheduler.maximum-allocation-mb' to values that allow for the requested 2048 MB."
        },
        "possible_fix_code": {
            "ContainerManagerImpl.authorizeRequest": "public void authorizeRequest(ResourceRequest request) throws YarnException {\n    // Check if the requested resources are within the allowed limits\n    Resource availableResources = getAvailableResources();\n    Resource requestedResources = request.getResource();\n\n    // Ensure that the requested memory is not greater than the available memory\n    if (requestedResources.getMemory() > availableResources.getMemory()) {\n        throw new YarnException(\"Unauthorized request: requested memory exceeds available memory.\");\n    }\n\n    // Ensure that the requested resources are in line with the configured limits\n    if (requestedResources.getMemory() > getMaxAllocation().getMemory()) {\n        throw new YarnException(\"Unauthorized request: requested memory exceeds maximum allocation.\");\n    }\n\n    // Additional checks can be added here as needed\n    // If all checks pass, proceed with the authorization\n    // ... (existing authorization logic)\n}"
        }
    },
    {
        "filename": "YARN-3675.json",
        "creation_time": "2015-05-18T22:38:39.000+0000",
        "bug_report": {
            "Title": "FairScheduler: RM quits when node removal races with continuous scheduling on the same node",
            "Description": "The issue arises in the Apache Hadoop YARN Fair Scheduler when continuous scheduling is performed on a node that is in the process of being removed. This race condition leads to a `NullPointerException` during the handling of application attempt removal events. The stack trace indicates that the exception occurs in the `unreserve` method of the `FSAppAttempt` class, which is called by the `completedContainer` method of the `FairScheduler` class. The root cause is linked to the state of the `node` and `rmContainer` objects, which may not be properly initialized or may be `null` at the time of invocation.",
            "StackTrace": [
                "12:28:53.782 AM FATAL org.apache.hadoop.yarn.server.resourcemanager.ResourceManager",
                "Error in handling event type APP_ATTEMPT_REMOVED to the scheduler",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt.unreserve(FSAppAttempt.java:469)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.completedContainer(FairScheduler.java:815)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.removeApplicationAttempt(FairScheduler.java:763)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:1217)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:111)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:684)",
                "at java.lang.Thread.run(Thread.java:745)",
                "12:28:53.783 AM INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager Exiting, bbye.."
            ],
            "RootCause": "The `NullPointerException` occurs in the `unreserve` method of the `FSAppAttempt` class when either the `node` is `null` (if `getFSSchedulerNode(container.getNodeId())` fails to find the node) or the `rmContainer` is `null` (if it is not properly initialized). Additionally, if `rmContainer` is valid but does not have an associated container, it can also lead to a `NullPointerException`.",
            "StepsToReproduce": [
                "1. Start the YARN ResourceManager with continuous scheduling enabled.",
                "2. Remove a node while it is still being scheduled for tasks.",
                "3. Observe the ResourceManager logs for the `NullPointerException`."
            ],
            "ExpectedBehavior": "The ResourceManager should handle the removal of nodes gracefully without throwing exceptions, allowing for proper scheduling and resource management.",
            "ObservedBehavior": "The ResourceManager crashes with a `NullPointerException` when attempting to unreserve resources on a node that is being removed.",
            "Suggestions": "Implement checks in the `unreserve` method to ensure that both `node` and `rmContainer` are not `null` before proceeding with resource unreservation. Additionally, consider adding synchronization mechanisms to prevent race conditions during node removal.",
            "problem_location": {
                "files": [
                    "hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSAppAttempt.java",
                    "hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt",
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler"
                ],
                "methods": [
                    "FSAppAttempt.unreserve",
                    "FairScheduler.completedContainer",
                    "FairScheduler.getFSSchedulerNode"
                ]
            },
            "possible_fix": "In the `unreserve` method of `FSAppAttempt`, add null checks for `node` and `rmContainer` before proceeding with the unreservation logic. For example:\n\n```java\nif (node == null || rmContainer == null) {\n    LOG.warn(\"Node or RMContainer is null, skipping unreserve.\");\n    return;\n}\n```"
        },
        "possible_fix_code": {
            "FSAppAttempt.unreserve": "  public void unreserve(Priority priority, FSSchedulerNode node) {\n    if (node == null) {\n        LOG.warn(\"Node is null, skipping unreserve.\");\n        return;\n    }\n    RMContainer rmContainer = node.getReservedContainer();\n    if (rmContainer == null) {\n        LOG.warn(\"RMContainer is null, skipping unreserve.\");\n        return;\n    }\n    unreserveInternal(priority, node);\n    node.unreserveResource(this);\n    getMetrics().unreserveResource(\n        getUser(), rmContainer.getContainer().getResource());\n  }"
        }
    },
    {
        "filename": "YARN-4763.json",
        "creation_time": "2016-03-04T10:03:56.000+0000",
        "bug_report": {
            "Title": "RMApps Page crashes with NPE",
            "Description": "The RMApps page crashes due to a NullPointerException (NPE) occurring in the `renderData` method of the `RMAppsBlock` class. The stack trace indicates that the exception is thrown when attempting to render application data, specifically when accessing the `appReports` variable. This variable is critical for generating the application data table displayed on the RMApps page.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.webapp.RMAppsBlock.renderData(RMAppsBlock.java:100)",
                "at org.apache.hadoop.yarn.server.webapp.AppsBlock.render(AppsBlock.java:140)",
                "at org.apache.hadoop.yarn.webapp.view.HtmlBlock.render(HtmlBlock.java:69)",
                "at org.apache.hadoop.yarn.webapp.view.HtmlBlock.renderPartial(HtmlBlock.java:79)",
                "at org.apache.hadoop.yarn.webapp.View.render(View.java:235)",
                "at org.apache.hadoop.yarn.webapp.view.HtmlPage$Page.subView(HtmlPage.java:49)",
                "at org.apache.hadoop.yarn.webapp.hamlet.HamletImpl$EImp._v(HamletImpl.java:117)",
                "at org.apache.hadoop.yarn.webapp.view.TwoColumnLayout.render(TwoColumnLayout.java:71)",
                "at org.apache.hadoop.yarn.webapp.view.HtmlPage.render(HtmlPage.java:82)",
                "at org.apache.hadoop.yarn.webapp.Dispatcher.render(Dispatcher.java:197)",
                "at org.apache.hadoop.yarn.webapp.Dispatcher.service(Dispatcher.java:156)"
            ],
            "RootCause": "The NullPointerException is likely caused by the `appReports` variable being uninitialized or null when accessed in the `renderData` method. This can occur if the method `rm.getRMContext().getRMApps()` returns null, or if the application reports are not properly populated before rendering.",
            "StepsToReproduce": [
                "1. Access the RMApps page in the application.",
                "2. Observe the application state being NEW.",
                "3. Trigger the rendering of application data."
            ],
            "ExpectedBehavior": "The RMApps page should display a table of application data without crashing, showing the relevant application details such as ID, user, name, type, and state.",
            "ObservedBehavior": "The RMApps page crashes with a NullPointerException, preventing the display of application data.",
            "Suggestions": "Investigate the initialization of the `appReports` variable in the `RMAppsBlock` class. Ensure that it is properly populated before the `renderData` method is called. Additionally, check the initialization of the `rm` context to prevent null references.",
            "problem_location": {
                "files": [
                    "RMAppsBlock.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.webapp.RMAppsBlock"
                ],
                "methods": [
                    "renderData"
                ]
            },
            "possible_fix": "Ensure that `appReports` is initialized before it is accessed in the `renderData` method. This may involve checking the logic that populates `appReports` and ensuring that the `rm` context is correctly set up."
        },
        "possible_fix_code": {
            "renderData": "protected void renderData(Block html) {\n    TBODY<TABLE<Hamlet>> tbody =\n        html.table(\"#apps\").thead().tr().th(\".id\", \"ID\").th(\".user\", \"User\")\n          .th(\".name\", \"Name\").th(\".type\", \"Application Type\")\n          .th(\".queue\", \"Queue\").th(\".priority\", \"Application Priority\")\n          .th(\".starttime\", \"StartTime\")\n          .th(\".finishtime\", \"FinishTime\").th(\".state\", \"State\")\n          .th(\".finalstatus\", \"FinalStatus\")\n          .th(\".runningcontainer\", \"Running Containers\")\n          .th(\".allocatedCpu\", \"Allocated CPU VCores\")\n          .th(\".allocatedMemory\", \"Allocated Memory MB\")\n          .th(\".queuePercentage\", \"% of Queue\")\n          .th(\".clusterPercentage\", \"% of Cluster\")\n          .th(\".progress\", \"Progress\")\n          .th(\".ui\", \"Tracking UI\")\n          .th(\".blacklisted\", \"Blacklisted Nodes\")._()\n          ._().tbody();\n\n    StringBuilder appsTableData = new StringBuilder(\"[\\n\");\n    if (appReports == null) {\n        // Handle the case where appReports is null\n        appsTableData.append(\"No application reports available\\n\");\n    } else {\n        for (ApplicationReport appReport : appReports) {\n            // TODO: remove the following condition. It is still here because\n            // the history side implementation of ApplicationBaseProtocol\n            // hasn't filtering capability (YARN-1819).\n            if (!reqAppStates.isEmpty()\n                && !reqAppStates.contains(appReport.getYarnApplicationState())) {\n                continue;\n            }\n\n            AppInfo app = new AppInfo(appReport);\n            ApplicationAttemptId appAttemptId =\n                ConverterUtils.toApplicationAttemptId(app.getCurrentAppAttemptId());\n            String queuePercent = \"N/A\";\n            String clusterPercent = \"N/A\";\n            if(appReport.getApplicationResourceUsageReport() != null) {\n                queuePercent = String.format(\"%.1f\",\n                    appReport.getApplicationResourceUsageReport()\n                        .getQueueUsagePercentage());\n                clusterPercent = String.format(\"%.1f\",\n                    appReport.getApplicationResourceUsageReport().getClusterUsagePercentage());\n            }\n\n            String blacklistedNodesCount = \"N/A\";\n            Set<String> nodes = rm.getRMContext().getRMApps()\n                .get(appAttemptId.getApplicationId()).getAppAttempts()\n                .get(appAttemptId).getBlacklistedNodes();\n            if (nodes != null) {\n                blacklistedNodesCount = String.valueOf(nodes.size());\n            }\n            String percent = StringUtils.format(\"%.1f\", app.getProgress());\n            appsTableData\n                .append(\"[\\\"<a href='\")\n                .append(url(\"app\", app.getAppId()))\n                .append(\"'>\")\n                .append(app.getAppId())\n                .append(\"</a>\\\",\\\"\")\n                .append(\n                    StringEscapeUtils.escapeJavaScript(\n                        StringEscapeUtils.escapeHtml(app.getUser())))\n                .append(\"\\\",\\\"\")\n                .append(\n                    StringEscapeUtils.escapeJavaScript(\n                        StringEscapeUtils.escapeHtml(app.getName())))\n                .append(\"\\\",\\\"\")\n                .append(\n                    StringEscapeUtils.escapeJavaScript(StringEscapeUtils.escapeHtml(app\n                        .getType())))\n                .append(\"\\\",\\\"\")\n                .append(\n                    StringEscapeUtils.escapeJavaScript(StringEscapeUtils.escapeHtml(app\n                        .getQueue()))).append(\"\\\",\\\"\").append(String\n                        .valueOf(app.getPriority()))\n                .append(\"\\\",\\\"\").append(app.getStartedTime())\n                .append(\"\\\",\\\"\").append(app.getFinishedTime())\n                .append(\"\\\",\\\"\")\n                .append(app.getAppState() == null ? UNAVAILABLE : app.getAppState())\n                .append(\"\\\",\\\"\")\n                .append(app.getFinalAppStatus())\n                .append(\"\\\",\\\"\")\n                .append(app.getRunningContainers() == -1 ? \"N/A\" : String\n                    .valueOf(app.getRunningContainers()))\n                .append(\"\\\",\\\"\")\n                .append(app.getAllocatedCpuVcores() == -1 ? \"N/A\" : String\n                    .valueOf(app.getAllocatedCpuVcores()))\n                .append(\"\\\",\\\"\")\n                .append(app.getAllocatedMemoryMB() == -1 ? \"N/A\" :\n                    String.valueOf(app.getAllocatedMemoryMB()))\n                .append(\"\\\",\\\"\")\n                .append(queuePercent)\n                .append(\"\\\",\\\"\")\n                .append(clusterPercent)\n                .append(\"\\\",\\\"\")\n                // Progress bar\n                .append(\"<br title='\").append(percent).append(\"'> <div class='\")\n                .append(C_PROGRESSBAR).append(\"' title='\").append(join(percent, '%'))\n                .append(\"'> \").append(\"<div class='\").append(C_PROGRESSBAR_VALUE)\n                .append(\"' style='\").append(join(\"width:\", percent, '%'))\n                .append(\"'> </div> </div>\").append(\"\\\",\\\"<a \");\n\n            String trackingURL =\n                app.getTrackingUrl() == null\n                    || app.getTrackingUrl().equals(UNAVAILABLE)\n                    || app.getAppState() == YarnApplicationState.NEW ? null : app\n                    .getTrackingUrl();\n\n            String trackingUI =\n                app.getTrackingUrl() == null\n                    || app.getTrackingUrl().equals(UNAVAILABLE)\n                    || app.getAppState() == YarnApplicationState.NEW ? \"Unassigned\"\n                    : app.getAppState() == YarnApplicationState.FINISHED\n                    || app.getAppState() == YarnApplicationState.FAILED\n                    || app.getAppState() == YarnApplicationState.KILLED ? \"History\"\n                    : \"ApplicationMaster\";\n            appsTableData.append(trackingURL == null ? \"#\" : \"href='\" + trackingURL)\n                .append(\"'>\").append(trackingUI).append(\"</a>\\\",\").append(\"\\\"\")\n                .append(blacklistedNodesCount).append(\"\\\"],\\n\");\n        }\n    }\n    if (appsTableData.charAt(appsTableData.length() - 2) == ',') {\n        appsTableData.delete(appsTableData.length() - 2,\n            appsTableData.length() - 1);\n    }\n    appsTableData.append(\"]\");\n    html.script().$type(\"text/javascript\")\n        ._(\"var appsTableData=\" + appsTableData)._();\n\n    tbody._()._();\n  }"
        }
    },
    {
        "filename": "YARN-8202.json",
        "creation_time": "2018-04-24T15:52:00.000+0000",
        "bug_report": {
            "Title": "DefaultAMSProcessor should properly check units of requested custom resource types against minimum/maximum allocation",
            "Description": "When executing a YARN job with specific resource allocation arguments, an exception is thrown indicating an invalid resource request. The job hangs due to the resource request for 'resource1' being either less than 0 or exceeding the maximum allowed allocation. The issue arises from the `validateResourceRequest` method in the `SchedulerUtils` class, which fails to account for resource units correctly. This bug manifests when requesting 500M of 'resource1' while the maximum allowed is 5G, leading to an `InvalidResourceRequestException`.",
            "StackTrace": [
                "org.apache.hadoop.yarn.exceptions.InvalidResourceRequestException: Invalid resource request, requested resource type=[resource1] < 0 or greater than maximum allowed allocation. Requested resource=<memory:200, vCores:1, resource1: 500M>, maximum allowed allocation=<memory:6144, vCores:8, resource1: 5G>",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils.validateResourceRequest(SchedulerUtils.java:286)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils.normalizeAndValidateRequest(SchedulerUtils.java:242)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils.normalizeAndvalidateRequest(SchedulerUtils.java:258)",
                "at org.apache.hadoop.yarn.server.resourcemanager.RMServerUtils.normalizeAndValidateRequests(RMServerUtils.java:249)",
                "at org.apache.hadoop.yarn.server.resourcemanager.DefaultAMSProcessor.allocate(DefaultAMSProcessor.java:230)"
            ],
            "RootCause": "The root cause of the issue is that the `validateResourceRequest` method in the `SchedulerUtils` class does not properly validate the units of the requested resources against the maximum allowed allocation, leading to an invalid resource request exception.",
            "StepsToReproduce": [
                "1. Execute a YARN job with the following arguments: -Dmapreduce.map.resource.memory-mb=200 -Dmapreduce.map.resource.resource1=500M 1 1000.",
                "2. Ensure that there is one node with 5GB of resource1 available.",
                "3. Observe the exception thrown and the job hanging."
            ],
            "ExpectedBehavior": "The job should successfully allocate resources and complete without throwing an exception, as the requested resources are within the allowed limits.",
            "ObservedBehavior": "The job hangs and throws an `InvalidResourceRequestException`, indicating that the requested resource type 'resource1' is invalid due to exceeding the maximum allowed allocation.",
            "Suggestions": "Review the `validateResourceRequest` method in the `SchedulerUtils` class to ensure it correctly handles resource units during validation. Consider adding unit checks for resource types to prevent invalid requests.",
            "problem_location": {
                "files": [
                    "SchedulerUtils.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils"
                ],
                "methods": [
                    "validateResourceRequest"
                ]
            },
            "possible_fix": "Modify the `validateResourceRequest` method to include checks for resource units, ensuring that the requested resources are validated against the maximum allowed allocation correctly. For example, ensure that the method can handle different units (e.g., MB, GB) appropriately."
        },
        "possible_fix_code": {
            "validateResourceRequest": "  private static void validateResourceRequest(ResourceRequest resReq,\n      Resource maximumResource, QueueInfo queueInfo, RMContext rmContext)\n      throws InvalidResourceRequestException {\n    Resource requestedResource = resReq.getCapability();\n    for (int i = 0; i < ResourceUtils.getNumberOfKnownResourceTypes(); i++) {\n      ResourceInformation reqRI = requestedResource.getResourceInformation(i);\n      ResourceInformation maxRI = maximumResource.getResourceInformation(i);\n      // Convert requested resource value to a consistent unit (e.g., MB)\n      long requestedValue = convertToMB(reqRI);\n      long maxValue = convertToMB(maxRI);\n      if (requestedValue < 0 || requestedValue > maxValue) {\n        throw new InvalidResourceRequestException(\n            \"Invalid resource request, requested resource type=[\" + reqRI\n                .getName()\n                + \"] < 0 or greater than maximum allowed allocation. Requested \"\n                + \"resource=\" + requestedResource\n                + \", maximum allowed allocation=\" + maximumResource\n                + \", please note that maximum allowed allocation is calculated \"\n                + \"by scheduler based on maximum resource of registered \"\n                + \"NodeManagers, which might be less than configured \"\n                + \"maximum allocation=\" + ResourceUtils\n                .getResourceTypesMaximumAllocation());\n      }\n    }\n    String labelExp = resReq.getNodeLabelExpression();\n    // we don\\'t allow specify label expression other than resourceName=ANY now\n    if (!ResourceRequest.ANY.equals(resReq.getResourceName())\n        && labelExp != null && !labelExp.trim().isEmpty()) {\n      throw new InvalidLabelResourceRequestException(\n          \"Invalid resource request, queue=\" + queueInfo.getQueueName()\n              + \" specified node label expression in a \"\n              + \"resource request has resource name = \"\n              + resReq.getResourceName());\n    }\n\n    // we don\\'t allow specify label expression with more than one node labels now\n    if (labelExp != null && labelExp.contains(\"&&\")) {\n      throw new InvalidLabelResourceRequestException(\n          \"Invalid resource request, queue=\" + queueInfo.getQueueName()\n              + \" specified more than one node label \"\n              + \"in a node label expression, node label expression = \"\n              + labelExp);\n    }\n\n    if (labelExp != null && !labelExp.trim().isEmpty() && queueInfo != null) {\n      if (!checkQueueLabelExpression(queueInfo.getAccessibleNodeLabels(),\n          labelExp, rmContext)) {\n        throw new InvalidLabelResourceRequestException(\n            \"Invalid resource request\" + \", queue=\" + queueInfo.getQueueName()\n                + \" doesn\\'t have permission to access all labels \"\n                + \"in resource request. labelExpression of resource request=\"\n                + labelExp + \". Queue labels=\"\n                + (queueInfo.getAccessibleNodeLabels() == null ? \"\"\n                    : StringUtils.join(\n                        queueInfo.getAccessibleNodeLabels().iterator(), \",\")));\n      } else {\n        checkQueueLabelInLabelManager(labelExp, rmContext);\n      }\n    }\n  }\n\n  private static long convertToMB(ResourceInformation resourceInfo) {\n    // Implement conversion logic based on resourceInfo's unit\n    // For example, if resourceInfo.getName() is \"resource1\" and its value is in GB, convert it to MB\n    // This is a placeholder for the actual conversion logic\n    return resourceInfo.getValue(); // Adjust this line based on actual unit handling\n  }"
        }
    },
    {
        "filename": "YARN-7118.json",
        "creation_time": "2017-08-29T12:04:01.000+0000",
        "bug_report": {
            "Title": "AHS REST API can return NullPointerException",
            "Description": "The ApplicationHistoryService REST API is encountering a NullPointerException when attempting to retrieve application history. This issue arises specifically in the `getApps` method of the `WebServices` class, which is part of the Hadoop YARN framework. The stack trace indicates that the exception is thrown when the method attempts to access application reports without proper validation of input parameters, particularly when the request lacks a valid remote user or when certain internal variables are not initialized.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.webapp.WebServices.getApps(WebServices.java:191)",
                "at org.apache.hadoop.yarn.server.applicationhistoryservice.webapp.AHSWebServices.getApps(AHSWebServices.java:96)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:497)",
                "at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)",
                "at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$TypeOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:185)"
            ],
            "RootCause": "The NullPointerException is likely caused by the `callerUGI` variable being null when the request does not contain a valid remote user. Additionally, the `appBaseProt` variable may not be properly initialized, leading to further null references when attempting to retrieve application reports.",
            "StepsToReproduce": [
                "Send a request to the AHS REST API without a valid remote user.",
                "Example request: curl --negotiate -u: 'http://<ATS IP>:8188/ws/v1/applicationhistory/apps?queue=test'"
            ],
            "ExpectedBehavior": "The API should return a valid response containing application history without throwing an exception, even if the user is not authenticated.",
            "ObservedBehavior": "The API returns a NullPointerException, indicating that it cannot process the request due to missing user information or uninitialized variables.",
            "Suggestions": "Ensure that the request always includes a valid remote user. Additionally, implement null checks for `callerUGI` and `appBaseProt` before they are used in the `getApps` method to prevent the NullPointerException.",
            "problem_location": {
                "files": [
                    "WebServices.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.webapp.WebServices"
                ],
                "methods": [
                    "getApps",
                    "getUser"
                ]
            },
            "possible_fix": "Add null checks for `callerUGI` and `appBaseProt` in the `getApps` method. For example:\n\nif (callerUGI == null) {\n    // Handle the case where the user is not authenticated\n}\n\nif (appBaseProt == null) {\n    // Handle the case where appBaseProt is not initialized\n}"
        },
        "possible_fix_code": {
            "getApps": "  public AppsInfo getApps(HttpServletRequest req, HttpServletResponse res,\n      String stateQuery, Set<String> statesQuery, String finalStatusQuery,\n      String userQuery, String queueQuery, String count, String startedBegin,\n      String startedEnd, String finishBegin, String finishEnd,\n      Set<String> applicationTypes) {\n    UserGroupInformation callerUGI = getUser(req);\n    boolean checkEnd = false;\n    boolean checkAppTypes = false;\n    boolean checkAppStates = false;\n    long countNum = Long.MAX_VALUE;\n\n    // set values suitable in case both of begin/end not specified\n    long sBegin = 0;\n    long sEnd = Long.MAX_VALUE;\n    long fBegin = 0;\n    long fEnd = Long.MAX_VALUE;\n\n    if (count != null && !count.isEmpty()) {\n      countNum = Long.parseLong(count);\n      if (countNum <= 0) {\n        throw new BadRequestException(\"limit value must be greater then 0\");\n      }\n    }\n\n    if (startedBegin != null && !startedBegin.isEmpty()) {\n      sBegin = Long.parseLong(startedBegin);\n      if (sBegin < 0) {\n        throw new BadRequestException(\"startedTimeBegin must be greater than 0\");\n      }\n    }\n    if (startedEnd != null && !startedEnd.isEmpty()) {\n      sEnd = Long.parseLong(startedEnd);\n      if (sEnd < 0) {\n        throw new BadRequestException(\"startedTimeEnd must be greater than 0\");\n      }\n    }\n    if (sBegin > sEnd) {\n      throw new BadRequestException(\n        \"startedTimeEnd must be greater than startTimeBegin\");\n    }\n\n    if (finishBegin != null && !finishBegin.isEmpty()) {\n      checkEnd = true;\n      fBegin = Long.parseLong(finishBegin);\n      if (fBegin < 0) {\n        throw new BadRequestException(\"finishTimeBegin must be greater than 0\");\n      }\n    }\n    if (finishEnd != null && !finishEnd.isEmpty()) {\n      checkEnd = true;\n      fEnd = Long.parseLong(finishEnd);\n      if (fEnd < 0) {\n        throw new BadRequestException(\"finishTimeEnd must be greater than 0\");\n      }\n    }\n    if (fBegin > fEnd) {\n      throw new BadRequestException(\n        \"finishTimeEnd must be greater than finishTimeBegin\");\n    }\n\n    Set<String> appTypes = parseQueries(applicationTypes, false);\n    if (!appTypes.isEmpty()) {\n      checkAppTypes = true;\n    }\n\n    // stateQuery is deprecated.\n    if (stateQuery != null && !stateQuery.isEmpty()) {\n      statesQuery.add(stateQuery);\n    }\n    Set<String> appStates = parseQueries(statesQuery, true);\n    if (!appStates.isEmpty()) {\n      checkAppStates = true;\n    }\n\n    AppsInfo allApps = new AppsInfo();\n    Collection<ApplicationReport> appReports = null;\n    final GetApplicationsRequest request =\n        GetApplicationsRequest.newInstance();\n    request.setLimit(countNum);\n    request.setStartRange(new LongRange(sBegin, sEnd));\n    try {\n      if (callerUGI == null) {\n        // Handle the case where the user is not authenticated\n        return allApps; // Return empty AppsInfo if callerUGI is null\n      } else {\n        appReports = callerUGI.doAs(\n            new PrivilegedExceptionAction<Collection<ApplicationReport>> () {\n          @Override\n          public Collection<ApplicationReport> run() throws Exception {\n            return appBaseProt.getApplications(request).getApplicationList();\n          }\n        });\n      }\n    } catch (Exception e) {\n      rewrapAndThrowException(e);\n    }\n    if (appReports == null) {\n      return allApps;\n    }\n    for (ApplicationReport appReport : appReports) {\n\n      if (checkAppStates &&\n          !appStates.contains(StringUtils.toLowerCase(\n              appReport.getYarnApplicationState().toString()))) {\n        continue;\n      }\n      if (finalStatusQuery != null && !finalStatusQuery.isEmpty()) {\n        FinalApplicationStatus.valueOf(finalStatusQuery);\n        if (!appReport.getFinalApplicationStatus().toString()\n          .equalsIgnoreCase(finalStatusQuery)) {\n          continue;\n        }\n      }\n      if (userQuery != null && !userQuery.isEmpty()) {\n        if (!appReport.getUser().equals(userQuery)) {\n          continue;\n        }\n      }\n      if (queueQuery != null && !queueQuery.isEmpty()) {\n        if (!appReport.getQueue().equals(queueQuery)) {\n          continue;\n        }\n      }\n      if (checkAppTypes &&\n          !appTypes.contains(\n              StringUtils.toLowerCase(appReport.getApplicationType().trim()))) {\n        continue;\n      }\n\n      if (checkEnd\n          && (appReport.getFinishTime() < fBegin || appReport.getFinishTime() > fEnd)) {\n        continue;\n      }\n      AppInfo app = new AppInfo(appReport);\n\n      allApps.add(app);\n    }\n    return allApps;\n  }"
        }
    },
    {
        "filename": "YARN-4743.json",
        "creation_time": "2016-02-27T09:12:28.000+0000",
        "bug_report": {
            "Title": "FairSharePolicy breaks TimSort assumption",
            "Description": "The issue arises in the Apache Hadoop YARN ResourceManager when handling NODE_UPDATE events, leading to an IllegalArgumentException due to a comparator violation in the FairSharePolicy. The stack trace indicates that the error occurs during the sorting process in TimSort, specifically when the FairShareComparator is used in the FSLeafQueue's assignContainer method. The comparator fails to maintain transitivity, particularly when both memorySize and weight are zero, resulting in a NaN value.",
            "StackTrace": [
                "2016-02-26 14:08:50,821 FATAL org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Error in handling event type NODE_UPDATE to the scheduler",
                "java.lang.IllegalArgumentException: Comparison method violates its general contract!",
                "at java.util.TimSort.mergeHi(TimSort.java:868)",
                "at java.util.TimSort.mergeAt(TimSort.java:485)",
                "at java.util.TimSort.mergeCollapse(TimSort.java:410)",
                "at java.util.TimSort.sort(TimSort.java:214)",
                "at java.util.TimSort.sort(TimSort.java:173)",
                "at java.util.Arrays.sort(Arrays.java:659)",
                "at java.util.Collections.sort(Collections.java:217)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue.assignContainer(FSLeafQueue.java:316)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSParentQueue.assignContainer(FSParentQueue.java:240)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.attemptScheduling(FairScheduler.java:1091)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.nodeUpdate(FairScheduler.java:989)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:1185)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:112)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:684)",
                "at java.lang.Thread.run(Thread.java:745)",
                "2016-02-26 14:08:50,822 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Exiting, bbye.."
            ],
            "RootCause": "The root cause of the issue is a comparator used in the FairSharePolicy that does not adhere to the required contract for comparison methods, particularly in the context of sorting operations. This inconsistency leads to an IllegalArgumentException during the sorting process in TimSort.",
            "StepsToReproduce": [
                "Trigger a NODE_UPDATE event in the YARN ResourceManager.",
                "Ensure that the memorySize and weight parameters are both set to zero.",
                "Observe the logs for the IllegalArgumentException related to the comparator."
            ],
            "ExpectedBehavior": "The system should handle NODE_UPDATE events without throwing exceptions, and the sorting should be performed correctly without violating the comparator's contract.",
            "ObservedBehavior": "An IllegalArgumentException is thrown, indicating that the comparison method violates its general contract, leading to a failure in handling NODE_UPDATE events.",
            "Suggestions": "Review the implementation of the FairShareComparator to ensure it adheres to the comparison contract. Consider adding checks for edge cases where memorySize and weight are both zero to prevent NaN values. Additionally, implement logging to trace the behavior of the comparator during sorting.",
            "problem_location": {
                "files": [
                    "FairSharePolicy.java",
                    "FSLeafQueue.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue",
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler"
                ],
                "methods": [
                    "FSLeafQueue.assignContainer",
                    "FairScheduler.nodeUpdate"
                ]
            },
            "possible_fix": "Modify the FairShareComparator to handle cases where memorySize and weight are both zero, ensuring it does not return NaN. For example, add a condition to return a consistent value when both parameters are zero."
        },
        "possible_fix_code": {
            "FSLeafQueue.assignContainer": "  public Resource assignContainer(FSSchedulerNode node) {\n    Resource assigned = Resources.none();\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Node \" + node.getNodeName() + \" offered to queue: \" +\n          getName() + \" fairShare: \" + getFairShare());\n    }\n\n    if (!assignContainerPreCheck(node)) {\n      return assigned;\n    }\n\n    // Apps that have resource demands.\n    TreeSet<FSAppAttempt> pendingForResourceApps =\n        new TreeSet<FSAppAttempt>(policy.getComparator());\n    readLock.lock();\n    try {\n      for (FSAppAttempt app : runnableApps) {\n        Resource pending = app.getAppAttemptResourceUsage().getPending();\n        if (!pending.equals(Resources.none())) {\n          pendingForResourceApps.add(app);\n        }\n      }\n    } finally {\n      readLock.unlock();\n    }\n    for (FSAppAttempt sched : pendingForResourceApps) {\n      if (SchedulerAppUtils.isBlacklisted(sched, node, LOG)) {\n        continue;\n      }\n      assigned = sched.assignContainer(node);\n      if (!assigned.equals(Resources.none())) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Assigned container in queue:\" + getName() + \" \" +\n              \"container:\" + assigned);\n        }\n        break;\n      }\n    }\n    return assigned;\n  }\n\n  private Comparator<FSAppAttempt> getComparator() {\n    return new Comparator<FSAppAttempt>() {\n      @Override\n      public int compare(FSAppAttempt a1, FSAppAttempt a2) {\n        int memorySize1 = a1.getAppAttemptResourceUsage().getMemorySize();\n        int memorySize2 = a2.getAppAttemptResourceUsage().getMemorySize();\n        int weight1 = a1.getWeight();\n        int weight2 = a2.getWeight();\n\n        // Handle edge case where both memorySize and weight are zero\n        if (memorySize1 == 0 && weight1 == 0 && memorySize2 == 0 && weight2 == 0) {\n          return 0; // Treat as equal\n        }\n\n        // Normal comparison logic\n        if (memorySize1 != memorySize2) {\n          return Integer.compare(memorySize1, memorySize2);\n        } else {\n          return Integer.compare(weight1, weight2);\n        }\n      }\n    };\n  }"
        }
    },
    {
        "filename": "YARN-2414.json",
        "creation_time": "2014-08-12T23:48:48.000+0000",
        "bug_report": {
            "Title": "RM web UI: app page will crash if app is failed before any attempt has been created",
            "Description": "The application page in the RM web UI crashes when an application fails before any attempts have been created. The stack trace indicates that a NullPointerException occurs in the AppBlock.render method, specifically when trying to access properties of an ApplicationReport that may not be properly initialized. This issue arises when the application is not registered correctly or when the application report is null, leading to attempts to access null properties.",
            "StackTrace": [
                "2014-08-12 16:45:13,573 ERROR org.apache.hadoop.yarn.webapp.Dispatcher: error handling URI: /cluster/app/application_1407887030038_0001",
                "java.lang.reflect.InvocationTargetException",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "at java.lang.reflect.Method.invoke(Method.java:597)",
                "at org.apache.hadoop.yarn.webapp.Dispatcher.service(Dispatcher.java:153)",
                "at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)",
                "at com.google.inject.servlet.ServletDefinition.doService(ServletDefinition.java:263)",
                "at com.google.inject.servlet.ServletDefinition.service(ServletDefinition.java:178)",
                "at com.google.inject.servlet.ManagedServletPipeline.service(ManagedServletPipeline.java:91)",
                "at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:62)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:900)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:834)",
                "at org.apache.hadoop.yarn.server.resourcemanager.webapp.RMWebAppFilter.doFilter(RMWebAppFilter.java:84)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:795)",
                "at com.google.inject.servlet.FilterDefinition.doFilter(FilterDefinition.java:163)",
                "at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:58)",
                "at com.google.inject.servlet.ManagedFilterPipeline.dispatch(ManagedFilterPipeline.java:118)",
                "at com.google.inject.servlet.GuiceFilter.doFilter(GuiceFilter.java:113)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "at org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter.doFilter(StaticUserWebFilter.java:109)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:460)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "at org.apache.hadoop.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1191)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)",
                "at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)",
                "at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)",
                "at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)",
                "at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)",
                "at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)",
                "at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)",
                "at org.mortbay.jetty.Server.handle(Server.java:326)",
                "at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)",
                "at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)",
                "at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)",
                "at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)",
                "at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)",
                "at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410)",
                "at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)",
                "Caused by: java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.webapp.AppBlock.render(AppBlock.java:116)",
                "at org.apache.hadoop.yarn.webapp.view.HtmlBlock.render(HtmlBlock.java:67)",
                "at org.apache.hadoop.yarn.webapp.view.HtmlBlock.renderPartial(HtmlBlock.java:77)",
                "at org.apache.hadoop.yarn.webapp.View.render(View.java:235)",
                "at org.apache.hadoop.yarn.webapp.view.HtmlPage$Page.subView(HtmlPage.java:49)",
                "at org.apache.hadoop.yarn.webapp.hamlet.HamletImpl$EImp._v(HamletImpl.java:117)",
                "at org.apache.hadoop.yarn.webapp.hamlet.Hamlet$TD._(Hamlet.java:845)",
                "at org.apache.hadoop.yarn.webapp.view.TwoColumnLayout.render(TwoColumnLayout.java:56)",
                "at org.apache.hadoop.yarn.webapp.view.HtmlPage.render(HtmlPage.java:82)",
                "at org.apache.hadoop.yarn.webapp.Controller.render(Controller.java:212)",
                "at org.apache.hadoop.yarn.server.resourcemanager.webapp.RmController.app(RmController.java:55)"
            ],
            "RootCause": "The root cause of the crash is a NullPointerException in the AppBlock.render method, which occurs when the application report is null. This can happen if the application is not properly registered with the ResourceManager or if it fails before any attempts are created, leading to attempts to access properties of a null ApplicationReport.",
            "StepsToReproduce": [
                "1. Submit an application to the YARN ResourceManager.",
                "2. Ensure that the application fails before any attempts are created.",
                "3. Access the application page in the RM web UI."
            ],
            "ExpectedBehavior": "The application page should display the application status and details without crashing, even if the application has failed.",
            "ObservedBehavior": "The application page crashes with a NullPointerException when trying to render details of an application that has failed before any attempts were created.",
            "Suggestions": "Ensure that the application is properly registered with the ResourceManager and that the application report is not null before attempting to access its properties in the AppBlock.render method. Implement null checks for the application report and its properties.",
            "problem_location": {
                "files": [
                    "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-common.src.main.java.org.apache.hadoop.yarn.server.webapp.AppBlock.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.webapp.AppBlock"
                ],
                "methods": [
                    "render"
                ]
            },
            "possible_fix": "Add null checks in the AppBlock.render method to handle cases where the application report is null. For example, before accessing properties of the application report, check if appReport is null and return an appropriate error message instead of proceeding with rendering."
        },
        "possible_fix_code": {
            "render": "  protected void render(Block html) {\n    String aid = $(APPLICATION_ID);\n    if (aid.isEmpty()) {\n      puts(\"Bad request: requires Application ID\");\n      return;\n    }\n\n    ApplicationId appID = null;\n    try {\n      appID = Apps.toAppID(aid);\n    } catch (Exception e) {\n      puts(\"Invalid Application ID: \" + aid);\n      return;\n    }\n\n    ApplicationReport appReport;\n    try {\n      appReport = appContext.getApplication(appID);\n    } catch (IOException e) {\n      String message = \"Failed to read the application \" + appID + \".\";\n      LOG.error(message, e);\n      html.p()._(message)._();\n      return;\n    }\n    if (appReport == null) {\n      puts(\"Application not found: \" + aid);\n      return;\n    }\n\n    // Check for null properties in appReport before accessing them\n    String user = appReport.getUser() != null ? appReport.getUser() : \"N/A\";\n    String name = appReport.getName() != null ? appReport.getName() : \"N/A\";\n    String type = appReport.getApplicationType() != null ? appReport.getApplicationType() : \"N/A\";\n    YarnApplicationState state = appReport.getYarnApplicationState() != null ? appReport.getYarnApplicationState() : YarnApplicationState.UNDEFINED;\n    FinalApplicationStatus finalStatus = appReport.getFinalApplicationStatus() != null ? appReport.getFinalApplicationStatus() : FinalApplicationStatus.UNDEFINED;\n    long startTime = appReport.getStartTime();\n    long finishTime = appReport.getFinishTime();\n    String trackingUrl = appReport.getTrackingUrl() != null ? appReport.getTrackingUrl() : \"#\";\n    String diagnostics = appReport.getDiagnostics() != null ? appReport.getDiagnostics() : \"N/A\";\n\n    AppInfo app = new AppInfo(appReport);\n\n    setTitle(join(\"Application \", aid));\n\n    info(\"Application Overview\")\n      ._(\"User:\", user)\n      ._(\"Name:\", name)\n      ._(\"Application Type:\", type)\n      ._(\"State:\", state)\n      ._(\"FinalStatus:\", finalStatus)\n      ._(\"Started:\", Times.format(startTime))\n      ._(\n        \"Elapsed:\",\n        StringUtils.formatTime(Times.elapsed(startTime, finishTime)))\n      ._(\"Tracking URL:\", trackingUrl, \"History\")\n      ._(\"Diagnostics:\", diagnostics);\n\n    html._(InfoBlock.class);\n\n    Collection<ApplicationAttemptReport> attempts;\n    try {\n      attempts = appContext.getApplicationAttempts(appID).values();\n    } catch (IOException e) {\n      String message =\n          \"Failed to read the attempts of the application \" + appID + \".\";\n      LOG.error(message, e);\n      html.p()._(message)._();\n      return;\n    }\n\n    // Application Attempt Table\n    TBODY<TABLE<Hamlet>> tbody =\n        html.table(\"#attempts\").thead().tr().th(\".id\", \"Attempt ID\")\n          .th(\".started\", \"Started\").th(\".node\", \"Node\").th(\".logs\", \"Logs\")\n          ._()._().tbody();\n\n    StringBuilder attemptsTableData = new StringBuilder(\"[\\n\");\n    for (ApplicationAttemptReport appAttemptReport : attempts) {\n      AppAttemptInfo appAttempt = new AppAttemptInfo(appAttemptReport);\n      ContainerReport containerReport;\n      try {\n        containerReport =\n            appContext.getAMContainer(appAttemptReport\n              .getApplicationAttemptId());\n      } catch (IOException e) {\n        String message =\n            \"Failed to read the AM container of the application attempt \"\n                + appAttemptReport.getApplicationAttemptId() + \".\";\n        LOG.error(message, e);\n        html.p()._(message)._();\n        return;\n      }\n      long startTimeAttempt = Long.MAX_VALUE;\n      String logsLink = null;\n      if (containerReport != null) {\n        ContainerInfo container = new ContainerInfo(containerReport);\n        startTimeAttempt = container.getStartedTime();\n        logsLink = containerReport.getLogUrl();\n      }\n      String nodeLink = null;\n      if (appAttempt.getHost() != null && appAttempt.getRpcPort() >= 0\n          && appAttempt.getRpcPort() < 65536) {\n        nodeLink = appAttempt.getHost() + \":\" + appAttempt.getRpcPort();\n      }\n      // AppAttemptID numerical value parsed by parseHadoopID in\n      // yarn.dt.plugins.js\n      attemptsTableData\n        .append(\"[\\\"<a href='\")\n        .append(url(\"appattempt\", appAttempt.getAppAttemptId()))\n        .append(\"'>\")\n        .append(appAttempt.getAppAttemptId())\n        .append(\"</a>\\\",\\\"\")\n        .append(startTimeAttempt)\n        .append(\"\\\",\\\"<a href='\")\n        .append(\n          nodeLink == null ? \"#\" : url(\"//\", nodeLink))\n        .append(\"'>\")\n        .append(\n          nodeLink == null ? \"N/A\" : StringEscapeUtils\n            .escapeJavaScript(StringEscapeUtils.escapeHtml(nodeLink)))\n        .append(\"</a>\\\",\\\"<a href='\")\n        .append(logsLink == null ? \"#\" : logsLink).append(\"'>\")\n        .append(logsLink == null ? \"N/A\" : \"Logs\").append(\"</a>\\\"],\\n\");\n    }\n    if (attemptsTableData.charAt(attemptsTableData.length() - 2) == ',') {\n      attemptsTableData.delete(attemptsTableData.length() - 2,\n        attemptsTableData.length() - 1);\n    }\n    attemptsTableData.append(\"]\");\n    html.script().$type(\"text/javascript\")\n      ._(\"var attemptsTableData=\" + attemptsTableData)._();\n\n    tbody._()._();\n  }"
        }
    },
    {
        "filename": "YARN-3878.json",
        "creation_time": "2015-07-02T00:20:59.000+0000",
        "bug_report": {
            "Title": "AsyncDispatcher can hang while stopping if it is configured for draining events on stop",
            "Description": "The issue arises when the ResourceManager (RM) is stopped while attempting to post an RMStateStore event to the AsyncDispatcher. This leads to an InterruptedException being thrown. As the RM is stopped, the AsyncDispatcher is also stopped, and during the serviceStop method, it checks if all events have been drained. However, this condition never becomes true, causing the AsyncDispatcher to wait indefinitely for the event queue to drain until the JVM exits. The logs indicate that the AsyncDispatcher is continuously waiting for the event queue to drain, which is not happening due to the interruption.",
            "StackTrace": [
                "2015-06-27 20:08:35,922 DEBUG [main] service.AbstractService (AbstractService.java:enterState(452)) - Service: Dispatcher entered state STOPPED",
                "2015-06-27 20:08:35,923 WARN  [AsyncDispatcher event handler] event.AsyncDispatcher (AsyncDispatcher.java:handle(247)) - AsyncDispatcher thread interrupted",
                "java.lang.InterruptedException",
                "at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireInterruptibly(AbstractQueuedSynchronizer.java:1219)",
                "at java.util.concurrent.locks.ReentrantLock.lockInterruptibly(ReentrantLock.java:340)",
                "at java.util.concurrent.LinkedBlockingQueue.put(LinkedBlockingQueue.java:338)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$GenericEventHandler.handle(AsyncDispatcher.java:244)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore.updateApplicationAttemptState(RMStateStore.java:652)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.rememberTargetTransitionsAndStoreState(RMAppAttemptImpl.java:1173)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.access$3300(RMAppAttemptImpl.java:109)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl$ContainerFinishedTransition.transition(RMAppAttemptImpl.java:1650)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl$ContainerFinishedTransition.transition(RMAppAttemptImpl.java:1619)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$MultipleInternalArc.doTransition(StateMachineFactory.java:385)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:786)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:108)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:838)"
            ],
            "RootCause": "The root cause of the issue is the interruption of the event handling thread during the shutdown process of the AsyncDispatcher. This interruption leads to an InterruptedException, which prevents the event queue from draining, causing the AsyncDispatcher to hang indefinitely.",
            "StepsToReproduce": [
                "1. Start the ResourceManager.",
                "2. Configure the AsyncDispatcher to drain events on stop.",
                "3. Attempt to stop the ResourceManager while it is processing events."
            ],
            "ExpectedBehavior": "The AsyncDispatcher should successfully drain all events and stop without hanging, allowing the ResourceManager to shut down cleanly.",
            "ObservedBehavior": "The AsyncDispatcher hangs indefinitely while waiting for the event queue to drain, resulting in a failure to stop the ResourceManager cleanly.",
            "Suggestions": "Consider modifying the serviceStop method to handle the InterruptedException more effectively, ensuring that the event queue can drain properly even when interruptions occur. Additionally, review the logic that checks for drained events to ensure it can exit gracefully.",
            "problem_location": {
                "files": [
                    "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/event/AsyncDispatcher.java",
                    "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/recovery/RMStateStore.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.event.AsyncDispatcher",
                    "org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore"
                ],
                "methods": [
                    "AsyncDispatcher.serviceStop",
                    "AsyncDispatcher.handle"
                ]
            },
            "possible_fix": "In the serviceStop method of AsyncDispatcher, ensure that the event handling thread is allowed to complete its processing even if it is interrupted. This may involve adding checks to handle the InterruptedException more gracefully and ensuring that the drained condition can be met."
        },
        "possible_fix_code": {
            "AsyncDispatcher.serviceStop": "  protected void serviceStop() throws Exception {\n    if (drainEventsOnStop) {\n      blockNewEvents = true;\n      LOG.info(\"AsyncDispatcher is draining to stop, ignoring any new events.\");\n      synchronized (waitForDrained) {\n        while (!drained && eventHandlingThread.isAlive()) {\n          waitForDrained.wait(1000);\n          LOG.info(\"Waiting for AsyncDispatcher to drain. Thread state is :\" +\n              eventHandlingThread.getState());\n        }\n      }\n    }\n    stopped = true;\n    if (eventHandlingThread != null) {\n      eventHandlingThread.interrupt();\n      try {\n        eventHandlingThread.join();\n      } catch (InterruptedException ie) {\n        LOG.warn(\"Interrupted Exception while stopping, attempting to drain events again.\", ie);\n        // Attempt to drain events again after interruption\n        synchronized (waitForDrained) {\n          while (!drained && eventHandlingThread.isAlive()) {\n            waitForDrained.wait(1000);\n            LOG.info(\"Retrying to wait for AsyncDispatcher to drain. Thread state is :\" +\n                eventHandlingThread.getState());\n          }\n        }\n      }\n    }\n\n    // stop all the components\n    super.serviceStop();\n  }"
        }
    },
    {
        "filename": "YARN-6683.json",
        "creation_time": "2017-06-02T00:29:13.000+0000",
        "bug_report": {
            "Title": "Invalid event: COLLECTOR_UPDATE at KILLED",
            "Description": "An `InvalidStateTransitionException` is thrown when attempting to handle a `COLLECTOR_UPDATE` event while the application is in the `KILLED` state. The `RMAppImpl.handle` method processes events related to the application state, but the state machine configuration does not allow the `COLLECTOR_UPDATE` event in the `KILLED` state. This leads to the error logged in the system.",
            "StackTrace": [
                "2017-06-01 20:01:22,686 ERROR rmapp.RMAppImpl (RMAppImpl.java:handle(905)) - Can't handle this event at current state",
                "org.apache.hadoop.yarn.state.InvalidStateTransitionException: Invalid event: COLLECTOR_UPDATE at KILLED",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.handle(RMAppImpl.java:903)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.handle(RMAppImpl.java:118)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher.handle(ResourceManager.java:904)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher.handle(ResourceManager.java:888)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:201)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:127)"
            ],
            "RootCause": "The `COLLECTOR_UPDATE` event is not a valid event for the `KILLED` state in the YARN application state machine. The state machine configuration explicitly defines the valid transitions for the `KILLED` state, which do not include `COLLECTOR_UPDATE`.",
            "StepsToReproduce": [
                "1. Submit an application to the YARN ResourceManager.",
                "2. Transition the application to the KILLED state.",
                "3. Attempt to send a COLLECTOR_UPDATE event to the application."
            ],
            "ExpectedBehavior": "The application should not process any events that are invalid for its current state, and no exceptions should be thrown.",
            "ObservedBehavior": "An `InvalidStateTransitionException` is thrown, indicating that the `COLLECTOR_UPDATE` event cannot be handled in the `KILLED` state.",
            "Suggestions": "To avoid this exception, ensure that the `COLLECTOR_UPDATE` event is not sent when the application is in the `KILLED` state. Alternatively, consider updating the collector address directly through a method of `RMApp` instead of sending an event.",
            "problem_location": {
                "files": [
                    "RMAppImpl.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl",
                    "org.apache.hadoop.yarn.state.StateMachineFactory"
                ],
                "methods": [
                    "RMAppImpl.handle",
                    "StateMachineFactory.doTransition"
                ]
            },
            "possible_fix": "Modify the application logic to check the current state before sending the `COLLECTOR_UPDATE` event. Alternatively, implement a direct method call to update the collector address without triggering state transitions."
        },
        "possible_fix_code": {
            "RMAppImpl.handle": "  public void handle(RMAppEvent event) {\n\n    this.writeLock.lock();\n\n    try {\n      ApplicationId appID = event.getApplicationId();\n      LOG.debug(\"Processing event for \" + appID + \" of type \"\n          + event.getType());\n      final RMAppState oldState = getState();\n\n      // Check if the event is COLLECTOR_UPDATE and the state is KILLED\n      if (event.getType() == RMAppEventType.COLLECTOR_UPDATE && oldState == RMAppState.KILLED) {\n        LOG.warn(\"Ignoring COLLECTOR_UPDATE event for application in KILLED state.\");\n        return; // Ignore the event\n      }\n\n      try {\n        /* keep the master in sync with the state machine */\n        this.stateMachine.doTransition(event.getType(), event);\n      } catch (InvalidStateTransitonException e) {\n        LOG.error(\"Can't handle this event at current state\", e);\n        /* TODO fail the application on the failed transition */\n      }\n\n      if (oldState != getState()) {\n        LOG.info(appID + \" State change from \" + oldState + \" to \"\n            + getState());\n      }\n    } finally {\n      this.writeLock.unlock();\n    }\n  }"
        }
    },
    {
        "filename": "YARN-2910.json",
        "creation_time": "2014-11-27T06:19:00.000+0000",
        "bug_report": {
            "Title": "FSLeafQueue can throw ConcurrentModificationException",
            "Description": "The `FSLeafQueue` class is experiencing a `ConcurrentModificationException` due to concurrent modifications of the `runnableApps` and `nonRunnableApps` lists while they are being iterated over in the `getResourceUsage` method. This issue arises in multi-threaded environments where methods like `updateRunnability` may modify these lists while `getResourceUsage` is executing, leading to inconsistencies and exceptions.",
            "StackTrace": [
                "2014-11-12 02:29:01,169 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM.",
                "java.util.ConcurrentModificationException: java.util.ConcurrentModificationException",
                "at java.util.ArrayList$Itr.checkForComodification(ArrayList.java:859)",
                "at java.util.ArrayList$Itr.next(ArrayList.java:831)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue.getResourceUsage(FSLeafQueue.java:147)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt.getHeadroom(FSAppAttempt.java:180)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.allocate(FairScheduler.java:923)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService.allocate(ApplicationMasterService.java:516)"
            ],
            "RootCause": "The `ConcurrentModificationException` occurs in the `getResourceUsage` method of the `FSLeafQueue` class when the `runnableApps` and `nonRunnableApps` lists are modified concurrently, likely due to the `updateRunnability` method being called while `getResourceUsage` is iterating over these lists.",
            "StepsToReproduce": [
                "1. Start the application with multiple threads accessing the FSLeafQueue.",
                "2. Trigger the `getResourceUsage` method while concurrently calling `updateRunnability` or other methods that modify the `runnableApps` and `nonRunnableApps` lists.",
                "3. Observe the `ConcurrentModificationException` in the logs."
            ],
            "ExpectedBehavior": "The system should be able to retrieve resource usage without throwing a `ConcurrentModificationException`, ensuring that the lists are accessed safely.",
            "ObservedBehavior": "The system throws a `ConcurrentModificationException` when attempting to iterate over the `runnableApps` and `nonRunnableApps` lists in the `getResourceUsage` method.",
            "Suggestions": "To resolve this issue, consider using a thread-safe collection such as `CopyOnWriteArrayList` for `runnableApps` and `nonRunnableApps` to prevent concurrent modification exceptions during iteration.",
            "problem_location": {
                "files": [
                    "hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSLeafQueue.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue"
                ],
                "methods": [
                    "getResourceUsage",
                    "updateRunnability"
                ]
            },
            "possible_fix": "Replace the `ArrayList` used for `runnableApps` and `nonRunnableApps` with `CopyOnWriteArrayList` to ensure thread-safe iteration. Example modification:\n\n```java\nprivate List<FSAppAttempt> runnableApps = new CopyOnWriteArrayList<>();\nprivate List<FSAppAttempt> nonRunnableApps = new CopyOnWriteArrayList<>();\n```"
        },
        "possible_fix_code": {
            "getResourceUsage": "public Resource getResourceUsage() {\n    Resource usage = Resources.createResource(0);\n    // Use a synchronized block to ensure thread safety during iteration\n    synchronized (this) {\n        for (FSAppAttempt app : runnableApps) {\n            Resources.addTo(usage, app.getResourceUsage());\n        }\n        for (FSAppAttempt app : nonRunnableApps) {\n            Resources.addTo(usage, app.getResourceUsage());\n        }\n    }\n    return usage;\n}"
        }
    },
    {
        "filename": "YARN-192.json",
        "creation_time": "2012-11-01T05:00:41.000+0000",
        "bug_report": {
            "Title": "Node update causes NPE in the fair scheduler",
            "Description": "A NullPointerException (NPE) occurs in the `unreserve` method of the `FSSchedulerApp` class when attempting to unreserve a container from a node that is not recognized by the scheduler. This issue arises due to a mismatch between the ResourceManager's understanding of reserved applications and the scheduler's state. The stack trace indicates that the exception is thrown when the method tries to access a reserved container that does not exist, leading to a failure in the scheduling process.",
            "StackTrace": [
                "2012-10-29 22:30:52,901 FATAL org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Error in handling event type NODE_UPDATE to the scheduler",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSSchedulerApp.unreserve(FSSchedulerApp.java:356)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AppSchedulable.unreserve(AppSchedulable.java:214)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AppSchedulable.assignContainer(AppSchedulable.java:266)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AppSchedulable.assignContainer(AppSchedulable.java:330)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueueSchedulable.assignContainer(FSQueueSchedulable.java:161)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.nodeUpdate(FairScheduler.java:759)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:836)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:1)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:329)",
                "at java.lang.Thread.run(Thread.java:662)"
            ],
            "RootCause": "The root cause of the NullPointerException is that the `unreserve` method attempts to access a reserved container from the `reservedContainers` map using a priority that does not exist, resulting in a null reference. Specifically, if `this.reservedContainers.get(priority)` returns null, any subsequent calls to methods on the resulting null object will throw a NullPointerException.",
            "StepsToReproduce": [
                "1. Start the ResourceManager and FairScheduler.",
                "2. Reserve a container for a specific node and priority.",
                "3. Update the node state in the scheduler with a NodeId that is not recognized.",
                "4. Trigger the unreserve operation for the same NodeId."
            ],
            "ExpectedBehavior": "The system should handle the unreserve operation gracefully without throwing a NullPointerException, even if the NodeId is not recognized by the scheduler.",
            "ObservedBehavior": "A NullPointerException is thrown in the `unreserve` method, causing the scheduler to fail to process the NODE_UPDATE event.",
            "Suggestions": "Modify the `unreserve` method to include null checks for `reservedContainers` and `reservedContainer` before accessing their properties or methods. This will prevent the NullPointerException from occurring.",
            "problem_location": {
                "files": [
                    "FSSchedulerApp.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSSchedulerApp"
                ],
                "methods": [
                    "FSSchedulerApp.unreserve"
                ]
            },
            "possible_fix": "Modify the `unreserve` method as follows:\n\n```java\npublic synchronized void unreserve(FSSchedulerNode node, Priority priority) {\n    Map<NodeId, RMContainer> reservedContainers = this.reservedContainers.get(priority);\n    if (reservedContainers == null) {\n        LOG.warn(\"No reserved containers for priority: \" + priority);\n        return; // Exit early if there are no reserved containers for this priority\n    }\n    RMContainer reservedContainer = reservedContainers.remove(node.getNodeID());\n    if (reservedContainer == null) {\n        LOG.warn(\"No reserved container found for node: \" + node.getNodeID());\n        return; // Exit early if there is no reserved container for this node\n    }\n    if (reservedContainers.isEmpty()) {\n        this.reservedContainers.remove(priority);\n    }\n    // Reset the re-reservation count\n    resetReReservations(priority);\n    Resource resource = reservedContainer.getContainer().getResource();\n    Resources.subtractFrom(currentReservation, resource);\n    LOG.info(\"Application \" + getApplicationId() + \" unreserved \" + \" on node \" + node + \", currently has \" + reservedContainers.size() + \" at priority \" + priority + \"; currentReservation \" + currentReservation);\n}\n```"
        },
        "possible_fix_code": {
            "FSSchedulerApp.unreserve": "public synchronized void unreserve(FSSchedulerNode node, Priority priority) {\n    Map<NodeId, RMContainer> reservedContainers = this.reservedContainers.get(priority);\n    // Check if reservedContainers is null\n    if (reservedContainers == null) {\n        LOG.warn(\"No reserved containers for priority: \" + priority);\n        return; // Exit early if there are no reserved containers for this priority\n    }\n    RMContainer reservedContainer = reservedContainers.remove(node.getNodeID());\n    // Check if reservedContainer is null\n    if (reservedContainer == null) {\n        LOG.warn(\"No reserved container found for node: \" + node.getNodeID());\n        return; // Exit early if there is no reserved container for this node\n    }\n    if (reservedContainers.isEmpty()) {\n        this.reservedContainers.remove(priority);\n    }\n    // Reset the re-reservation count\n    resetReReservations(priority);\n    Resource resource = reservedContainer.getContainer().getResource();\n    Resources.subtractFrom(currentReservation, resource);\n    LOG.info(\"Application \" + getApplicationId() + \" unreserved \" + \" on node \" + node + \", currently has \" + reservedContainers.size() + \" at priority \" + priority + \"; currentReservation \" + currentReservation);\n}"
        }
    },
    {
        "filename": "YARN-4581.json",
        "creation_time": "2016-01-12T03:37:40.000+0000",
        "bug_report": {
            "Title": "AHS writer thread leak makes RM crash while RM is recovering",
            "Description": "The issue arises when the ApplicationHistoryWriter is enabled, leading to multiple errors related to file handling and thread management in a Hadoop environment. The primary error is an IOException indicating that the output file is not at zero offset, which suggests improper file initialization or a failure to close previous write operations. This is compounded by an OutOfMemoryError, which occurs when the ResourceManager (RM) is unable to create new native threads, likely due to resource constraints. After several failovers, a significant number of HDFS client threads are leaked, causing the RM to crash.",
            "StackTrace": [
                "2016-01-08 03:13:03,441 ERROR org.apache.hadoop.yarn.server.applicationhistoryservice.FileSystemApplicationHistoryStore: Error when opening history file of application application_1451878591907_0197",
                "java.io.IOException: Output file not at zero offset.",
                "at org.apache.hadoop.io.file.tfile.BCFile$Writer.<init>(BCFile.java:288)",
                "at org.apache.hadoop.io.file.tfile.TFile$Writer.<init>(TFile.java:288)",
                "at org.apache.hadoop.yarn.server.applicationhistoryservice.FileSystemApplicationHistoryStore$HistoryFileWriter.<init>(FileSystemApplicationHistoryStore.java:728)",
                "at org.apache.hadoop.yarn.server.applicationhistoryservice.FileSystemApplicationHistoryStore.applicationStarted(FileSystemApplicationHistoryStore.java:418)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ahs.RMApplicationHistoryWriter.handleWritingApplicationHistoryEvent(RMApplicationHistoryWriter.java:140)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ahs.RMApplicationHistoryWriter$ForwardingEventHandler.handle(RMApplicationHistoryWriter.java:297)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ahs.RMApplicationHistoryWriter$ForwardingEventHandler.handle(RMApplicationHistoryWriter.java:292)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:191)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:124)",
                "at java.lang.Thread.run(Thread.java:745)",
                "2016-01-08 03:13:08,335 FATAL org.apache.hadoop.yarn.event.AsyncDispatcher: Error in dispatcher thread",
                "java.lang.OutOfMemoryError: unable to create new native thread",
                "at java.lang.Thread.start0(Native Method)",
                "at java.lang.Thread.start(Thread.java:714)",
                "at org.apache.hadoop.hdfs.DFSOutputStream.start(DFSOutputStream.java:2033)",
                "at org.apache.hadoop.hdfs.DFSOutputStream.newStreamForAppend(DFSOutputStream.java:1652)",
                "at org.apache.hadoop.hdfs.DFSClient.callAppend(DFSClient.java:1573)",
                "at org.apache.hadoop.hdfs.DFSClient.append(DFSClient.java:1603)",
                "at org.apache.hadoop.hdfs.DFSClient.append(DFSClient.java:1591)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:328)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:324)",
                "at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.append(DistributedFileSystem.java:324)",
                "at org.apache.hadoop.fs.FileSystem.append(FileSystem.java:1161)",
                "at org.apache.hadoop.yarn.server.applicationhistoryservice.FileSystemApplicationHistoryStore$HistoryFileWriter.<init>(FileSystemApplicationHistoryStore.java:723)",
                "at org.apache.hadoop.yarn.server.applicationhistoryservice.FileSystemApplicationHistoryStore.applicationStarted(FileSystemApplicationHistoryStore.java:418)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ahs.RMApplicationHistoryWriter.handleWritingApplicationHistoryEvent(RMApplicationHistoryWriter.java:140)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ahs.RMApplicationHistoryWriter$ForwardingEventHandler.handle(RMApplicationHistoryWriter.java:297)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ahs.RMApplicationHistoryWriter$ForwardingEventHandler.handle(RMApplicationHistoryWriter.java:292)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:191)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:124)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "RootCause": "The root cause of the issue is twofold: an IOException due to improper file initialization or a failure to close previous write operations, and an OutOfMemoryError caused by exceeding the system's thread creation limits, likely due to resource constraints.",
            "StepsToReproduce": [
                "Enable ApplicationHistoryWriter in the Hadoop configuration.",
                "Run multiple applications that generate application history events.",
                "Monitor the ResourceManager for errors and thread usage."
            ],
            "ExpectedBehavior": "The ResourceManager should handle application history events without errors, maintaining a stable number of threads and not exceeding memory limits.",
            "ObservedBehavior": "The ResourceManager crashes due to an OutOfMemoryError after generating thousands of errors related to file handling, leading to a significant number of leaked HDFS client threads.",
            "Suggestions": "To resolve the IOException, ensure that files are properly initialized and closed before writing. For the OutOfMemoryError, check and increase the operating system's thread limits and allocate more memory to the JVM by adjusting the -Xmx and -Xms parameters in the Hadoop configuration.",
            "problem_location": {
                "files": [
                    "BCFile.java",
                    "TFile.java",
                    "FileSystemApplicationHistoryStore.java"
                ],
                "classes": [
                    "org.apache.hadoop.io.file.tfile.BCFile",
                    "org.apache.hadoop.io.file.tfile.TFile",
                    "org.apache.hadoop.yarn.server.applicationhistoryservice.FileSystemApplicationHistoryStore"
                ],
                "methods": [
                    "BCFile.init",
                    "TFile.init",
                    "FileSystemApplicationHistoryStore.init"
                ]
            },
            "possible_fix": "Ensure proper file initialization and closure in the FileSystemApplicationHistoryStore class. Increase the thread limits in the operating system and adjust JVM memory settings to prevent OutOfMemoryError."
        },
        "possible_fix_code": {
            "FileSystemApplicationHistoryStore.applicationStarted": "  public void applicationStarted(ApplicationStartData appStart)\n      throws IOException {\n    HistoryFileWriter hfWriter =\n        outstandingWriters.get(appStart.getApplicationId());\n    if (hfWriter == null) {\n      Path applicationHistoryFile =\n          new Path(rootDirPath, appStart.getApplicationId().toString());\n      try {\n        hfWriter = new HistoryFileWriter(applicationHistoryFile);\n        LOG.info(\"Opened history file of application \"\n            + appStart.getApplicationId());\n      } catch (IOException e) {\n        LOG.error(\"Error when opening history file of application \"\n            + appStart.getApplicationId(), e);\n        throw e;\n      }\n      outstandingWriters.put(appStart.getApplicationId(), hfWriter);\n    } else {\n      throw new IOException(\"History file of application \"\n          + appStart.getApplicationId() + \" is already opened\");\n    }\n    assert appStart instanceof ApplicationStartDataPBImpl;\n    try {\n      hfWriter.writeHistoryData(new HistoryDataKey(appStart.getApplicationId()\n        .toString(), START_DATA_SUFFIX),\n        ((ApplicationStartDataPBImpl) appStart).getProto().toByteArray());\n      LOG.info(\"Start information of application \"\n          + appStart.getApplicationId() + \" is written\");\n    } catch (IOException e) {\n      LOG.error(\"Error when writing start information of application \"\n          + appStart.getApplicationId(), e);\n      throw e;\n    } finally {\n      // Ensure the HistoryFileWriter is closed to prevent leaks\n      if (hfWriter != null) {\n        hfWriter.close();\n      }\n    }\n  }"
        }
    },
    {
        "filename": "YARN-7786.json",
        "creation_time": "2018-01-22T14:29:46.000+0000",
        "bug_report": {
            "Title": "NullPointerException while launching ApplicationMaster",
            "Description": "A NullPointerException occurs when attempting to launch the ApplicationMaster after sending a kill command to the job. The stack trace indicates that the exception is thrown in the `setupTokens` method of the `AMLauncher` class, specifically when accessing potentially null objects such as `application`, `masterContainer`, or `rmContext`. This issue arises during the initialization of the ApplicationMaster, where critical context objects may not be properly set up.",
            "StackTrace": [
                "2017-11-25 21:27:25,333 INFO org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher: Error launching appattempt_1511616410268_0001_000001. Got exception: java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.setupTokens(AMLauncher.java:205)",
                "at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.createAMContainerLaunchContext(AMLauncher.java:193)",
                "at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.launch(AMLauncher.java:112)",
                "at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.run(AMLauncher.java:304)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "RootCause": "The NullPointerException is likely caused by one of the following being null: `application`, `masterContainer`, or `rmContext`. Specifically, if `application` is null, it will lead to a NullPointerException when accessing its methods in the `setupTokens` method.",
            "StepsToReproduce": [
                "1. Send a kill command to the job.",
                "2. Attempt to launch the ApplicationMaster."
            ],
            "ExpectedBehavior": "The ApplicationMaster should launch successfully without throwing a NullPointerException.",
            "ObservedBehavior": "A NullPointerException is thrown, preventing the ApplicationMaster from launching.",
            "Suggestions": "Check the initialization of the `application`, `masterContainer`, and `rmContext` objects in the `AMLauncher` constructor. Ensure that these objects are not null before they are accessed in the `setupTokens` method.",
            "problem_location": {
                "files": [
                    "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/amlaucher/AMLauncher.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher"
                ],
                "methods": [
                    "AMLauncher.setupTokens",
                    "AMLauncher.createAMContainerLaunchContext",
                    "AMLauncher.launch"
                ]
            },
            "possible_fix": "Ensure that the `application`, `masterContainer`, and `rmContext` fields are properly initialized in the `AMLauncher` constructor. Add null checks before accessing their methods in the `setupTokens` method."
        },
        "possible_fix_code": {
            "AMLauncher.setupTokens": "  protected void setupTokens(\n      ContainerLaunchContext container, ContainerId containerID)\n      throws IOException {\n    if (application == null || rmContext == null) {\n        throw new IOException(\"Application or RMContext is not initialized.\");\n    }\n    Map<String, String> environment = container.getEnvironment();\n    if (environment == null) {\n        throw new IOException(\"Container environment is null.\");\n    }\n    environment.put(ApplicationConstants.APPLICATION_WEB_PROXY_BASE_ENV,\n        application.getWebProxyBase());\n    // Set AppSubmitTime to be consumable by the AM.\n    ApplicationId applicationId =\n        application.getAppAttemptId().getApplicationId();\n    if (rmContext.getRMApps() == null || rmContext.getRMApps().get(applicationId) == null) {\n        throw new IOException(\"RMApps is not initialized or does not contain the application.\");\n    }\n    environment.put(\n        ApplicationConstants.APP_SUBMIT_TIME_ENV,\n        String.valueOf(rmContext.getRMApps()\n            .get(applicationId)\n            .getSubmitTime()));\n\n    Credentials credentials = new Credentials();\n    DataInputByteBuffer dibb = new DataInputByteBuffer();\n    ByteBuffer tokens = container.getTokens();\n    if (tokens != null) {\n      // TODO: Don't do this kind of checks everywhere.\n      dibb.reset(tokens);\n      credentials.readTokenStorageStream(dibb);\n      tokens.rewind();\n    }\n\n    // Add AMRMToken\n    Token<AMRMTokenIdentifier> amrmToken = createAndSetAMRMToken();\n    if (amrmToken != null) {\n      credentials.addToken(amrmToken.getService(), amrmToken);\n    }\n    DataOutputBuffer dob = new DataOutputBuffer();\n    credentials.writeTokenStorageToStream(dob);\n    container.setTokens(ByteBuffer.wrap(dob.getData(), 0, dob.getLength()));\n  }"
        }
    },
    {
        "filename": "YARN-8035.json",
        "creation_time": "2018-03-16T12:02:04.000+0000",
        "bug_report": {
            "Title": "Uncaught exception in ContainersMonitorImpl during relaunch due to the process ID changing",
            "Description": "During a container relaunch event, the container ID is reused while a new process is spawned. The {{ContainersMonitorImpl}} attempts to monitor the new process by obtaining its PID and initializing the process tree monitoring. However, if the previous container failed, the original PID may still be registered, leading to a {{MetricsException}} when trying to register the same tag {{ContainerPid}} again. This issue arises from the inability of the {{MetricsRegistry}} to handle duplicate tag registrations, as indicated by the stack trace.",
            "StackTrace": [
                "2018-03-16 11:59:02,563 WARN org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl: Uncaught exception in ContainersMonitorImpl while monitoring resource of container_1521201379995_0001_01_000002",
                "org.apache.hadoop.metrics2.MetricsException: Tag ContainerPid already exists!",
                "at org.apache.hadoop.metrics2.lib.MetricsRegistry.checkTagName(MetricsRegistry.java:433)",
                "at org.apache.hadoop.metrics2.lib.MetricsRegistry.tag(MetricsRegistry.java:394)",
                "at org.apache.hadoop.metrics2.lib.MetricsRegistry.tag(MetricsRegistry.java:400)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainerMetrics.recordProcessId(ContainerMetrics.java:277)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl$MonitoringThread.initializeProcessTrees(ContainersMonitorImpl.java:559)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl$MonitoringThread.run(ContainersMonitorImpl.java:448)"
            ],
            "RootCause": "The root cause of the issue is the attempt to register a tag named {{ContainerPid}} in the {{MetricsRegistry}} that has already been registered, leading to a {{MetricsException}}.",
            "StepsToReproduce": [
                "1. Start a container and allow it to run.",
                "2. Force a relaunch of the container, ensuring the original process fails.",
                "3. Observe the logs for the {{MetricsException}} related to the {{ContainerPid}} tag."
            ],
            "ExpectedBehavior": "The system should successfully register the new PID for the container without throwing a {{MetricsException}}.",
            "ObservedBehavior": "An uncaught {{MetricsException}} is thrown indicating that the tag {{ContainerPid}} already exists when attempting to register it again.",
            "Suggestions": "To resolve this issue, ensure that the metric registration logic checks if the {{ContainerPid}} tag already exists before attempting to register it again. Additionally, consider synchronizing the registration process if it occurs in a multi-threaded context to prevent concurrent registrations.",
            "problem_location": {
                "files": [
                    "ContainersMonitorImpl.java",
                    "ContainerMetrics.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl",
                    "org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainerMetrics"
                ],
                "methods": [
                    "ContainersMonitorImpl$MonitoringThread.initializeProcessTrees",
                    "ContainerMetrics.recordProcessId"
                ]
            },
            "possible_fix": "Add a check in the {{recordProcessId}} method of {{ContainerMetrics}} to verify if the {{ContainerPid}} tag is already registered before attempting to register it again. This can be done by modifying the logic to include a condition that checks for existing tags."
        },
        "possible_fix_code": {
            "ContainerMetrics.recordProcessId": "  public void recordProcessId(String processId) {\n    // Check if the tag already exists before attempting to register it\n    try {\n        registry.tag(PROCESSID_INFO, processId);\n    } catch (MetricsException e) {\n        // Log the exception and handle it gracefully\n        LOG.warn(\"Failed to register process ID: \" + processId + \", tag may already exist.\", e);\n    }\n  }"
        }
    },
    {
        "filename": "YARN-4152.json",
        "creation_time": "2015-09-12T15:02:22.000+0000",
        "bug_report": {
            "Title": "NM crash with NPE when LogAggregationService#stopContainer called for absent container",
            "Description": "The NodeManager (NM) crashes due to a NullPointerException (NPE) when the LogAggregationService's stopContainer method is invoked for a container that does not exist. This issue arises during the log aggregation process when an application with multiple containers is terminated unexpectedly. The logs indicate that a KILL_CONTAINER event is sent to a non-existent container, leading to the NPE in the stopContainer method.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService.stopContainer(LogAggregationService.java:422)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService.handle(LogAggregationService.java:456)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService.handle(LogAggregationService.java:68)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:183)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:109)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "RootCause": "The NullPointerException occurs because the stopContainer method is called with a containerId that does not exist in the context. Specifically, the context.getContainers() call returns null for the absent container, leading to the NPE when attempting to access its properties.",
            "StepsToReproduce": [
                "1. Run a Pi job with 500 containers.",
                "2. Kill the application while it is running.",
                "3. Observe the logs for the KILL_CONTAINER event sent to an absent container."
            ],
            "ExpectedBehavior": "The system should gracefully handle the termination of containers, ensuring that no NullPointerException occurs when attempting to stop a non-existent container.",
            "ObservedBehavior": "The system crashes with a NullPointerException when the LogAggregationService attempts to stop a container that has already been removed from the context.",
            "Suggestions": "Implement a null check for the context.getContainers().get(containerId) call in the stopContainer method to prevent the NPE. Additionally, ensure that the event handling logic correctly verifies the existence of the container before attempting to stop it.",
            "problem_location": {
                "files": [
                    "LogAggregationService.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService"
                ],
                "methods": [
                    "stopContainer",
                    "handle"
                ]
            },
            "possible_fix": "In the stopContainer method, add a check to ensure that the context and the containerId are valid before proceeding with the log aggregation logic:\n\n```java\nif (context == null || !context.getContainers().containsKey(containerId)) {\n    LOG.warn(\"Container not found: \" + containerId);\n    return;\n}\n```"
        },
        "possible_fix_code": {
            "stopContainer": "  private void stopContainer(ContainerId containerId, int exitCode) {\n\n    // A container is complete. Put this containers' logs up for aggregation if\n    // this containers' logs are needed.\n\n    if (context == null || !context.getContainers().containsKey(containerId)) {\n        LOG.warn(\"Container not found: \" + containerId);\n        return;\n    }\n\n    AppLogAggregator aggregator = this.appLogAggregators.get(\n        containerId.getApplicationAttemptId().getApplicationId());\n    if (aggregator == null) {\n      LOG.warn(\"Log aggregation is not initialized for \" + containerId\n          + \", did it fail to start?\");\n      return;\n    }\n    ContainerType containerType = context.getContainers().get(\n        containerId).getContainerTokenIdentifier().getContainerType();\n    aggregator.startContainerLogAggregation(\n        new ContainerLogContext(containerId, containerType, exitCode));\n  }"
        }
    },
    {
        "filename": "YARN-3697.json",
        "creation_time": "2015-05-21T18:05:38.000+0000",
        "bug_report": {
            "Title": "FairScheduler: ContinuousSchedulingThread can fail to shutdown",
            "Description": "The ContinuousSchedulingThread in the FairScheduler may fail to shut down properly due to an unhandled InterruptedException during the scheduling attempt. The issue arises when the thread is interrupted while it is waiting to acquire a lock or while processing events, leading to a situation where the thread does not exit as expected. The stack trace indicates that the InterruptedException occurs in the AsyncDispatcher and RMContainerImpl classes, which are part of the YARN framework's event handling and resource management.",
            "StackTrace": [
                "2015-05-17 23:30:43,065 WARN  [FairSchedulerContinuousScheduling] event.AsyncDispatcher (AsyncDispatcher.java:handle(247)) - AsyncDispatcher thread interrupted",
                "java.lang.InterruptedException",
                "at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireInterruptibly(AbstractQueuedSynchronizer.java:1219)",
                "at java.util.concurrent.locks.ReentrantLock.lockInterruptibly(ReentrantLock.java:340)",
                "at java.util.concurrent.LinkedBlockingQueue.put(LinkedBlockingQueue.java:338)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$GenericEventHandler.handle(AsyncDispatcher.java:244)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl$ContainerStartedTransition.transition(RMContainerImpl.java:467)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl$ContainerStartedTransition.transition(RMContainerImpl.java:462)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:362)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl.handle(RMContainerImpl.java:387)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl.handle(RMContainerImpl.java:58)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt.allocate(FSAppAttempt.java:357)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt.assignContainer(FSAppAttempt.java:516)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt.assignContainer(FSAppAttempt.java:649)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt.assignContainer(FSAppAttempt.java:803)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue.assignContainer(FSLeafQueue.java:334)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSParentQueue.assignContainer(FSParentQueue.java:173)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.attemptScheduling(FairScheduler.java:1082)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.continuousSchedulingAttempt(FairScheduler.java:1014)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler$ContinuousSchedulingThread.run(FairScheduler.java:285)"
            ],
            "RootCause": "The root cause of the issue is that the ContinuousSchedulingThread does not handle InterruptedException properly, leading to a failure in shutting down the thread when an interruption occurs. The AsyncDispatcher's createThread method logs the interruption but does not ensure that the ContinuousSchedulingThread exits cleanly.",
            "StepsToReproduce": [
                "Start the FairScheduler with a configured ContinuousSchedulingThread.",
                "Trigger a stop command to shut down the scheduler.",
                "Observe the logs for any InterruptedException warnings.",
                "Check if the ContinuousSchedulingThread remains active despite the stop command."
            ],
            "ExpectedBehavior": "The ContinuousSchedulingThread should shut down cleanly and exit when a stop command is issued, without leaving any active threads.",
            "ObservedBehavior": "The ContinuousSchedulingThread sometimes remains active and does not shut down properly, leading to potential resource leaks and scheduling issues.",
            "Suggestions": "Implement proper handling of InterruptedException in the ContinuousSchedulingThread to ensure it exits cleanly when interrupted. Consider adding a catch block for InterruptedException in the continuousSchedulingAttempt method.",
            "problem_location": {
                "files": [
                    "hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java",
                    "hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/event/AsyncDispatcher.java",
                    "hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmcontainer/RMContainerImpl.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler",
                    "org.apache.hadoop.yarn.event.AsyncDispatcher",
                    "org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl"
                ],
                "methods": [
                    "FairScheduler.continuousSchedulingAttempt",
                    "AsyncDispatcher.createThread",
                    "RMContainerImpl.handle"
                ]
            },
            "possible_fix": "Add a catch block for InterruptedException in the continuousSchedulingAttempt method of FairScheduler to ensure that the thread exits cleanly. Example modification:\n\ntry {\n    // existing code\n} catch (InterruptedException e) {\n    LOG.warn(\"ContinuousSchedulingThread interrupted, shutting down.\", e);\n    return;\n}"
        },
        "possible_fix_code": {
            "FairScheduler.continuousSchedulingAttempt": "public void continuousSchedulingAttempt() {\n    try {\n        // existing scheduling logic\n    } catch (InterruptedException e) {\n        LOG.warn(\"ContinuousSchedulingThread interrupted, shutting down.\", e);\n        return;\n    }\n}"
        }
    },
    {
        "filename": "YARN-2340.json",
        "creation_time": "2014-07-23T15:18:38.000+0000",
        "bug_report": {
            "Title": "NPE thrown when RM restart after queue is STOPPED. Thereafter RM cannot recover applications and remains in standby",
            "Description": "When the ResourceManager (RM) is restarted after the queue state has been set to STOPPED, the standby RM fails to transition to active status. This failure is accompanied by a NullPointerException (NPE) in the `CapacityScheduler` class, specifically during the handling of the `APP_ATTEMPT_ADDED` event. The stack trace indicates that the NPE occurs in the `addApplicationAttempt` method, which is called by the `handle` method of the `CapacityScheduler` when processing application attempt events.",
            "StackTrace": [
                "2014-07-23 18:43:24,432 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1406116264351_0014_000002 State change from NEW to SUBMITTED",
                "2014-07-23 18:43:24,433 FATAL org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Error in handling event type APP_ATTEMPT_ADDED to the scheduler",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.addApplicationAttempt(CapacityScheduler.java:568)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:916)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:101)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:602)",
                "at java.lang.Thread.run(Thread.java:662)",
                "2014-07-23 18:43:24,434 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Exiting, bbye.."
            ],
            "RootCause": "The NullPointerException in the `addApplicationAttempt` method of the `CapacityScheduler` class is likely caused by a null application object when attempting to retrieve the application from the `applications` map using the application ID. This can occur if the application ID is not found in the map, which may happen if the application was not properly registered or if the queue state was not correctly handled during the RM restart.",
            "StepsToReproduce": [
                "1. Start a job in the ResourceManager (RM).",
                "2. Change the queue state to STOPPED.",
                "3. Restart the ResourceManager.",
                "4. Observe the logs for the NullPointerException."
            ],
            "ExpectedBehavior": "The ResourceManager should successfully transition to active status and recover applications without throwing any exceptions.",
            "ObservedBehavior": "The ResourceManager fails to transition to active status and throws a NullPointerException, preventing it from recovering applications.",
            "Suggestions": "Ensure that the application ID is valid and that the `applications` map is populated correctly before calling `addApplicationAttempt`. Implement checks to handle cases where the application ID may not be found in the map.",
            "problem_location": {
                "files": [
                    "CapacityScheduler.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler"
                ],
                "methods": [
                    "addApplicationAttempt",
                    "handle"
                ]
            },
            "possible_fix": "Add null checks for the application object and the queue object in the `addApplicationAttempt` method. For example:\n\n```java\nif (application == null) {\n    LOG.error(\"Application not found for ID: \" + applicationAttemptId.getApplicationId());\n    return;\n}\nif (queue == null) {\n    LOG.error(\"Queue is null for application: \" + application.getUser());\n    return;\n}\n```"
        },
        "possible_fix_code": {
            "addApplicationAttempt": "  private synchronized void addApplicationAttempt(\n      ApplicationAttemptId applicationAttemptId,\n      boolean transferStateFromPreviousAttempt,\n      boolean isAttemptRecovering) {\n    SchedulerApplication<FiCaSchedulerApp> application =\n        applications.get(applicationAttemptId.getApplicationId());\n    if (application == null) {\n        LOG.error(\"Application not found for ID: \" + applicationAttemptId.getApplicationId());\n        return;\n    }\n    CSQueue queue = (CSQueue) application.getQueue();\n    if (queue == null) {\n        LOG.error(\"Queue is null for application: \" + application.getUser());\n        return;\n    }\n\n    FiCaSchedulerApp attempt =\n        new FiCaSchedulerApp(applicationAttemptId, application.getUser(),\n          queue, queue.getActiveUsersManager(), rmContext);\n    if (transferStateFromPreviousAttempt) {\n      attempt.transferStateFromPreviousAttempt(application\n        .getCurrentAppAttempt());\n    }\n    application.setCurrentAppAttempt(attempt);\n\n    queue.submitApplicationAttempt(attempt, application.getUser());\n    LOG.info(\"Added Application Attempt \" + applicationAttemptId\n        + \" to scheduler from user \" + application.getUser() + \" in queue \"\n        + queue.getQueueName());\n    if (isAttemptRecovering) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(applicationAttemptId\n            + \" is recovering. Skipping notifying ATTEMPT_ADDED\");\n      }\n    } else {\n      rmContext.getDispatcher().getEventHandler().handle(\n        new RMAppAttemptEvent(applicationAttemptId,\n            RMAppAttemptEventType.ATTEMPT_ADDED));\n    }\n  }"
        }
    },
    {
        "filename": "YARN-8022.json",
        "creation_time": "2018-03-10T19:29:27.000+0000",
        "bug_report": {
            "Title": "ResourceManager UI cluster/app/<app-id> page fails to render",
            "Description": "The ResourceManager UI fails to render the application details for the specified application ID, displaying the message 'Failed to read the attempts of the application'. The issue is traced back to a NullPointerException occurring in the AppBlock class during the rendering process, specifically when attempting to access application metrics and attempts. This is likely due to an invalid or missing application ID, which results in the application object being null.",
            "StackTrace": [
                "org.apache.hadoop.yarn.server.webapp.AppBlock: Failed to read the attempts of the application application_1520597233415_0002.",
                "java.lang.NullPointerException",
                " at org.apache.hadoop.yarn.server.webapp.AppBlock$3.run(AppBlock.java:283)",
                " at org.apache.hadoop.yarn.server.webapp.AppBlock$3.run(AppBlock.java:280)",
                " at java.security.AccessController.doPrivileged(Native Method)",
                " at javax.security.auth.Subject.doAs(Subject.java:422)",
                " at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1682)",
                " at org.apache.hadoop.yarn.server.webapp.AppBlock.render(AppBlock.java:279)",
                " at org.apache.hadoop.yarn.server.resourcemanager.webapp.RMAppBlock.render(RMAppBlock.java:71)",
                " at org.apache.hadoop.yarn.webapp.view.HtmlBlock.render(HtmlBlock.java:69)",
                " at org.apache.hadoop.yarn.webapp.view.HtmlBlock.renderPartial(HtmlBlock.java:79)",
                " at org.apache.hadoop.yarn.webapp.View.render(View.java:235)",
                " at org.apache.hadoop.yarn.webapp.view.HtmlPage$Page.subView(HtmlPage.java:49)",
                " at org.apache.hadoop.yarn.webapp.hamlet2.HamletImpl$EImp._v(HamletImpl.java:117)",
                " at org.apache.hadoop.yarn.webapp.hamlet2.Hamlet$TD.__(Hamlet.java:848)",
                " at org.apache.hadoop.yarn.webapp.view.TwoColumnLayout.render(TwoColumnLayout.java:71)",
                " at org.apache.hadoop.yarn.webapp.view.HtmlPage.render(HtmlPage.java:82)",
                " at org.apache.hadoop.yarn.webapp.Controller.render(Controller.java:212)",
                " at org.apache.hadoop.yarn.server.resourcemanager.webapp.RmController.app(RmController.java:54)"
            ],
            "RootCause": "The NullPointerException is caused by an invalid or missing application ID, leading to attempts to access properties of a null application object in the YARN web application. Specifically, the application object is not found in the ResourceManager context, resulting in null dereferences when accessing application metrics and attempts.",
            "StepsToReproduce": [
                "Navigate to the ResourceManager UI.",
                "Access the cluster/app/<app-id> page.",
                "Observe the error message displayed."
            ],
            "ExpectedBehavior": "The application details should render correctly, displaying the application metrics and attempts without any errors.",
            "ObservedBehavior": "The page displays an error message stating 'Failed to read the attempts of the application' and does not render the application details.",
            "Suggestions": "Implement validation checks for the application ID before attempting to access application metrics. Ensure that the application exists in the ResourceManager context to prevent null dereferences.",
            "problem_location": {
                "files": [
                    "RMAppBlock.java",
                    "AppBlock.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.webapp.RMAppBlock",
                    "org.apache.hadoop.yarn.server.webapp.AppBlock"
                ],
                "methods": [
                    "RMAppBlock.createApplicationMetricsTable",
                    "RMAppBlock.generateApplicationTable",
                    "AppBlock.render"
                ]
            },
            "possible_fix": "Add null checks for the application object in the RMAppBlock methods, particularly in createApplicationMetricsTable and generateApplicationTable, to handle cases where the application ID is invalid or not found."
        },
        "possible_fix_code": {
            "RMAppBlock.createApplicationMetricsTable": "  @Override\n  protected void createApplicationMetricsTable(Block html){\n    RMApp rmApp = this.rm.getRMContext().getRMApps().get(appID);\n    if (rmApp == null) {\n      html.p().__(\"Application not found: \" + appID).__();\n      return;\n    }\n    RMAppMetrics appMetrics = rmApp.getRMAppMetrics();\n    // Get attempt metrics and fields, it is possible currentAttempt of RMApp is\n    // null. In that case, we will assume resource preempted and number of Non\n    // AM container preempted on that attempt is 0\n    RMAppAttemptMetrics attemptMetrics;\n    if (rmApp.getCurrentAppAttempt() == null) {\n      attemptMetrics = null;\n    } else {\n      attemptMetrics = rmApp.getCurrentAppAttempt().getRMAppAttemptMetrics();\n    }\n    Resource attemptResourcePreempted =\n        attemptMetrics == null ? Resources.none() : attemptMetrics\n          .getResourcePreempted();\n    int attemptNumNonAMContainerPreempted =\n        attemptMetrics == null ? 0 : attemptMetrics\n          .getNumNonAMContainersPreempted();\n    DIV<Hamlet> pdiv = html.\n        __(InfoBlock.class).\n        div(_INFO_WRAP);\n    info(\"Application Overview\").clear();\n    info(\"Application Metrics\")\n        .__(\"Total Resource Preempted:\",\n          appMetrics == null ? \"N/A\" : appMetrics.getResourcePreempted())\n        .__(\"Total Number of Non-AM Containers Preempted:\",\n          appMetrics == null ? \"N/A\"\n              : appMetrics.getNumNonAMContainersPreempted())\n        .__(\"Total Number of AM Containers Preempted:\",\n          appMetrics == null ? \"N/A\"\n              : appMetrics.getNumAMContainersPreempted())\n        .__(\"Resource Preempted from Current Attempt:\",\n          attemptResourcePreempted)\n        .__(\"Number of Non-AM Containers Preempted from Current Attempt:\",\n          attemptNumNonAMContainerPreempted)\n        .__(\"Aggregate Resource Allocation:\", appMetrics == null ? \"N/A\" :\n            StringHelper\n                .getResourceSecondsString(appMetrics.getResourceSecondsMap()))\n        .__(\"Aggregate Preempted Resource Allocation:\",\n            appMetrics == null ? \"N/A\" : StringHelper.getResourceSecondsString(\n                appMetrics.getPreemptedResourceSecondsMap()));\n\n    pdiv.__();\n  }"
        }
    },
    {
        "filename": "YARN-3793.json",
        "creation_time": "2015-06-10T20:52:38.000+0000",
        "bug_report": {
            "Title": "Several NPEs when deleting local files on NM recovery",
            "Description": "When the NodeManager (NM) work-preserving restart is enabled, multiple NullPointerExceptions (NPEs) occur during the recovery process. These exceptions are linked to attempts to delete sub-directories, which may not be correctly tracked, leading to potential resource leaks. The logs indicate that the deletion process fails due to null paths being passed to the deletion methods, specifically in the `fixRelativePart` method of the `FileContext` class. This report aims to investigate the root cause and implement necessary fixes.",
            "StackTrace": [
                "2015-05-18 07:06:10,225 INFO org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor: Deleting absolute path : null",
                "2015-05-18 07:06:10,224 ERROR org.apache.hadoop.yarn.server.nodemanager.DeletionService: Exception during execution of task in DeletionService",
                "java.lang.NullPointerException",
                "        at org.apache.hadoop.fs.FileContext.fixRelativePart(FileContext.java:274)",
                "        at org.apache.hadoop.fs.FileContext.delete(FileContext.java:755)",
                "        at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.deleteAsUser(DefaultContainerExecutor.java:458)",
                "        at org.apache.hadoop.yarn.server.nodemanager.DeletionService$FileDeletionTask.run(DeletionService.java:293)"
            ],
            "RootCause": "The NullPointerException occurs because the `subDir` in the `DeletionAsUserContext` can be null. This leads to a call to `lfs.delete(null, true)` in the `deleteAsUser` method, which subsequently calls `fixRelativePart` with a null path, resulting in the NPE.",
            "StepsToReproduce": [
                "Enable NM work-preserving restart.",
                "Attempt to delete local files or directories that may not be properly tracked.",
                "Monitor logs for NPE occurrences during the deletion process."
            ],
            "ExpectedBehavior": "The system should successfully delete specified local files or directories without throwing any exceptions, ensuring proper resource management.",
            "ObservedBehavior": "The system throws NullPointerExceptions when attempting to delete local files, indicating that null paths are being passed to the deletion methods.",
            "Suggestions": "Ensure that the `subDir` is properly set in the `DeletionAsUserContext` before invoking the `deleteAsUser` method. Implement checks to prevent null values from being passed to the deletion methods.",
            "problem_location": {
                "files": [
                    "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.java",
                    "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileContext.java",
                    "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.executor.DeletionAsUserContext.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor",
                    "org.apache.hadoop.fs.FileContext",
                    "org.apache.hadoop.yarn.server.nodemanager.executor.DeletionAsUserContext"
                ],
                "methods": [
                    "DefaultContainerExecutor.deleteAsUser",
                    "FileContext.delete",
                    "FileContext.fixRelativePart"
                ]
            },
            "possible_fix": "Modify the `DeletionAsUserContext` builder to ensure that `subDir` is explicitly set before creating an instance. Additionally, add null checks in the `deleteAsUser` method to handle cases where `subDir` might be null."
        },
        "possible_fix_code": {
            "DefaultContainerExecutor.deleteAsUser": "  public void deleteAsUser(DeletionAsUserContext ctx)\n      throws IOException, InterruptedException {\n    Path subDir = ctx.getSubDir();\n    List<Path> baseDirs = ctx.getBasedirs();\n\n    // Check if subDir is null and handle it appropriately\n    if (subDir == null) {\n      LOG.warn(\"subDir is null, cannot proceed with deletion.\");\n      return;\n    }\n\n    if (baseDirs == null || baseDirs.size() == 0) {\n      LOG.info(\"Deleting absolute path : \" + subDir);\n      if (!lfs.delete(subDir, true)) {\n        // Maybe retry\n        LOG.warn(\"delete returned false for path: [\" + subDir + \"]\");\n      }\n      return;\n    }\n    for (Path baseDir : baseDirs) {\n      Path del = new Path(baseDir, subDir);\n      LOG.info(\"Deleting path : \" + del);\n      if (!lfs.delete(del, true)) {\n        LOG.warn(\"delete returned false for path: [\" + del + \"]\");\n      }\n    }\n  }"
        }
    },
    {
        "filename": "YARN-6102.json",
        "creation_time": "2017-01-17T09:36:29.000+0000",
        "bug_report": {
            "Title": "RMActiveService context to be updated with new RMContext on failover",
            "Description": "The bug occurs in the Hadoop YARN framework when a failover happens during the processing of a node heartbeat. Specifically, the error arises because the dispatcher is reset before the new event handlers are registered, leading to a situation where the dispatcher cannot handle the `STATUS_UPDATE` event for `RMNodeEventType`. This issue was reproduced in both test and production environments, indicating a critical flaw in the event handling mechanism during failover scenarios.",
            "StackTrace": [
                "2017-01-17 16:42:17,911 FATAL [AsyncDispatcher event handler] event.AsyncDispatcher (AsyncDispatcher.java:dispatch(200)) - Error in dispatcher thread",
                "java.lang.Exception: No handler for registered for class org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeEventType",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:196)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:120)",
                "at java.lang.Thread.run(Thread.java:745)",
                "2017-01-17 16:42:17,914 INFO  [AsyncDispatcher ShutDown handler] event.AsyncDispatcher (AsyncDispatcher.java:run(303)) - Exiting, bbye.."
            ],
            "RootCause": "The root cause of the issue is the improper ordering of operations during the failover process. When a node heartbeat is sent, if a failover occurs, the dispatcher is reset before the new event handlers are registered. This results in the dispatcher being unable to handle the `STATUS_UPDATE` event, leading to the exception indicating that no handler is registered for `RMNodeEventType`.",
            "StepsToReproduce": [
                "1. Start the ResourceManager and ensure it is running.",
                "2. Simulate a failover while sending a node heartbeat to the ResourceTrackerService.",
                "3. Observe the logs for the error indicating no handler registered for `RMNodeEventType`."
            ],
            "ExpectedBehavior": "The ResourceManager should successfully handle the `STATUS_UPDATE` event for `RMNodeEventType` without throwing an exception, even during a failover.",
            "ObservedBehavior": "The ResourceManager throws an exception indicating that there is no handler registered for `RMNodeEventType`, causing the dispatcher to fail.",
            "Suggestions": "Ensure that event handlers for `RMNodeEventType` are registered before resetting the dispatcher during the failover process. This may involve modifying the order of operations in the `ResourceTrackerService.nodeHeartbeat` method and the `ResourceManager.reinitialize` method.",
            "problem_location": {
                "files": [
                    "AsyncDispatcher.java",
                    "ResourceTrackerService.java",
                    "ResourceManager.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.event.AsyncDispatcher",
                    "org.apache.hadoop.yarn.server.resourcemanager.ResourceTrackerService",
                    "org.apache.hadoop.yarn.server.resourcemanager.ResourceManager"
                ],
                "methods": [
                    "AsyncDispatcher.dispatch",
                    "ResourceTrackerService.nodeHeartbeat",
                    "ResourceManager.reinitialize"
                ]
            },
            "possible_fix": "Modify the `ResourceTrackerService.nodeHeartbeat` method to ensure that the dispatcher is not reset until after the new event handlers are registered. This may involve reordering the calls to `resetDispatcher()` and `createAndInitActiveServices()`."
        },
        "possible_fix_code": {
            "ResourceTrackerService.nodeHeartbeat": "public void nodeHeartbeat(NodeHeartbeatRequest request) {\n    // Ensure that event handlers are registered before resetting the dispatcher\n    registerEventHandlers();\n\n    // Proceed with the existing logic\n    if (isFailover) {\n        // Handle failover logic\n        resetDispatcher();\n    }\n    // Other existing logic for processing the heartbeat\n}"
        }
    },
    {
        "filename": "YARN-8409.json",
        "creation_time": "2018-06-08T20:36:32.000+0000",
        "bug_report": {
            "Title": "ActiveStandbyElectorBasedElectorService is failing with NPE",
            "Description": "In a ResourceManager High Availability (RM-HA) environment, when the ZooKeeper leader is killed and a ResourceManager failover is performed, the active ResourceManager sometimes encounters a NullPointerException (NPE) and fails to start successfully. The stack trace indicates that the failure occurs during the initialization of the ActiveStandbyElectorBasedElectorService, specifically when attempting to connect to ZooKeeper, which is also experiencing a connection refusal error.",
            "StackTrace": [
                "2018-06-08 10:31:03,007 INFO client.ZooKeeperSaslClient (ZooKeeperSaslClient.java:run(289)) - Client will use GSSAPI as SASL mechanism.",
                "2018-06-08 10:31:03,008 INFO zookeeper.ClientCnxn (ClientCnxn.java:logStartConnect(1019)) - Opening socket connection to server xxx/xxx:2181. Will attempt to SASL-authenticate using Login Context section 'Client'",
                "2018-06-08 10:31:03,009 WARN zookeeper.ClientCnxn (ClientCnxn.java:run(1146)) - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect",
                "java.net.ConnectException: Connection refused",
                "at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)",
                "at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)",
                "at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:361)",
                "at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1125)",
                "2018-06-08 10:31:03,344 INFO service.AbstractService (AbstractService.java:noteFailure(267)) - Service org.apache.hadoop.yarn.server.resourcemanager.ActiveStandbyElectorBasedElectorService failed in state INITED",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.ha.ActiveStandbyElector$3.run(ActiveStandbyElector.java:1033)",
                "at org.apache.hadoop.ha.ActiveStandbyElector$3.run(ActiveStandbyElector.java:1030)",
                "at org.apache.hadoop.ha.ActiveStandbyElector.zkDoWithRetries(ActiveStandbyElector.java:1095)",
                "at org.apache.hadoop.ha.ActiveStandbyElector.zkDoWithRetries(ActiveStandbyElector.java:1087)",
                "at org.apache.hadoop.ha.ActiveStandbyElector.createWithRetries(ActiveStandbyElector.java:1030)",
                "at org.apache.hadoop.ha.ActiveStandbyElector.ensureParentZNode(ActiveStandbyElector.java:347)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ActiveStandbyElectorBasedElectorService.serviceInit(ActiveStandbyElectorBasedElectorService.java:110)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)",
                "at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:108)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceInit(ResourceManager.java:336)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1479)",
                "2018-06-08 10:31:03,345 INFO ha.ActiveStandbyElector (ActiveStandbyElector.java:quitElection(409)) - Yielding from election"
            ],
            "RootCause": "The root cause of the issue is twofold: a connection refusal when attempting to connect to the ZooKeeper server, which prevents the ActiveStandbyElectorBasedElectorService from initializing properly, and a NullPointerException occurring in the ActiveStandbyElector class due to an uninitialized object reference.",
            "StepsToReproduce": [
                "1. Set up a ResourceManager High Availability (RM-HA) environment.",
                "2. Kill the ZooKeeper leader node.",
                "3. Perform a ResourceManager failover.",
                "4. Observe the logs for any NullPointerException or connection errors."
            ],
            "ExpectedBehavior": "The ActiveStandbyElectorBasedElectorService should initialize successfully and the active ResourceManager should come up without any exceptions.",
            "ObservedBehavior": "The active ResourceManager fails to initialize and throws a NullPointerException, along with a connection refused error when attempting to connect to ZooKeeper.",
            "Suggestions": "Ensure that the ZooKeeper server is running and accessible. Check network configurations, including firewalls, to confirm that the necessary ports are open. Review the code for potential uninitialized variables that could lead to the NullPointerException.",
            "problem_location": {
                "files": [
                    "ActiveStandbyElector.java",
                    "ActiveStandbyElectorBasedElectorService.java"
                ],
                "classes": [
                    "org.apache.hadoop.ha.ActiveStandbyElector",
                    "org.apache.hadoop.yarn.server.resourcemanager.ActiveStandbyElectorBasedElectorService"
                ],
                "methods": [
                    "ActiveStandbyElector.zkDoWithRetries",
                    "ActiveStandbyElector.ensureParentZNode",
                    "ActiveStandbyElectorBasedElectorService.serviceInit"
                ]
            },
            "possible_fix": "Check for uninitialized variables in the ActiveStandbyElector class, particularly in the run methods. Implement null checks before using objects that could potentially be null. Additionally, ensure that the ZooKeeper service is properly configured and running."
        },
        "possible_fix_code": {
            "ActiveStandbyElector.zkDoWithRetries": "  private <T> T zkDoWithRetries(ZKAction<T> action, Code retryCode)\n      throws KeeperException, InterruptedException {\n    int retry = 0;\n    while (true) {\n      try {\n        return action.run();\n      } catch (KeeperException ke) {\n        if ((shouldRetry(ke.code()) || shouldRetry(ke.code(), retryCode))\n            && ++retry < maxRetryNum) {\n          continue;\n        }\n        throw ke;\n      } catch (NullPointerException npe) {\n        LOG.error(\"NullPointerException occurred while executing zkDoWithRetries: \" + npe.getMessage());\n        throw new KeeperException(\"A null reference was encountered during ZooKeeper operation.\", npe);\n      }\n    }\n  }"
        }
    },
    {
        "filename": "YARN-8223.json",
        "creation_time": "2018-04-27T11:49:02.000+0000",
        "bug_report": {
            "Title": "ClassNotFoundException when auxiliary service is loaded from HDFS",
            "Description": "The application encounters a ClassNotFoundException when attempting to load an auxiliary service from HDFS. This issue arises specifically when the classpath is not set correctly, leading to the failure of the class loader to locate the required class `org.apache.auxtest.AuxServiceFromLocal`. In contrast, loading the same service from a local path works without issues, indicating a discrepancy in classpath configuration between local and HDFS environments.",
            "StackTrace": [
                "java.lang.ClassNotFoundException: org.apache.auxtest.AuxServiceFromLocal",
                "at java.net.URLClassLoader.findClass(URLClassLoader.java:381)",
                "at java.lang.ClassLoader.loadClass(ClassLoader.java:424)",
                "at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:338)",
                "at java.lang.ClassLoader.loadClass(ClassLoader.java:357)",
                "at org.apache.hadoop.util.ApplicationClassLoader.loadClass(ApplicationClassLoader.java:189)",
                "at org.apache.hadoop.util.ApplicationClassLoader.loadClass(ApplicationClassLoader.java:157)",
                "at java.lang.Class.forName0(Native Method)",
                "at java.lang.Class.forName(Class.java:348)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxiliaryServiceWithCustomClassLoader.getInstance(AuxiliaryServiceWithCustomClassLoader.java:169)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices.serviceInit(AuxServices.java:249)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)",
                "at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:108)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.serviceInit(ContainerManagerImpl.java:316)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)",
                "at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:108)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.serviceInit(NodeManager.java:472)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.initAndStartNodeManager(NodeManager.java:918)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.main(NodeManager.java:979)"
            ],
            "RootCause": "The ClassNotFoundException is caused by the absence of the class `org.apache.auxtest.AuxServiceFromLocal` in the classpath when loading the auxiliary service from HDFS. The classpath is empty in the HDFS case, which prevents the class loader from finding the required class.",
            "StepsToReproduce": [
                "1. Configure the application to load an auxiliary service from HDFS.",
                "2. Start the application and monitor the logs.",
                "3. Observe the ClassNotFoundException in the logs indicating the missing class."
            ],
            "ExpectedBehavior": "The auxiliary service should load successfully from HDFS without any ClassNotFoundException, similar to the behavior observed when loading from a local path.",
            "ObservedBehavior": "The application throws a ClassNotFoundException for `org.apache.auxtest.AuxServiceFromLocal` when attempting to load the auxiliary service from HDFS, indicating that the classpath is not set correctly.",
            "Suggestions": "Ensure that the class `org.apache.auxtest.AuxServiceFromLocal` is included in the application's classpath when loading from HDFS. Verify the configuration settings for auxiliary services to ensure the correct classpath is specified.",
            "problem_location": {
                "files": [
                    "AuxiliaryServiceWithCustomClassLoader.java",
                    "AuxServices.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxiliaryServiceWithCustomClassLoader",
                    "org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices"
                ],
                "methods": [
                    "AuxiliaryServiceWithCustomClassLoader.getInstance",
                    "AuxiliaryServiceWithCustomClassLoader.createAuxServiceClassLoader",
                    "AuxiliaryServiceWithCustomClassLoader.serviceInit"
                ]
            },
            "possible_fix": "Modify the configuration to include the correct path to the `org.apache.auxtest.AuxServiceFromLocal` class in the appClasspath parameter. Ensure that the service initialization method sets the class loader correctly to avoid ClassNotFoundException."
        },
        "possible_fix_code": {
            "AuxiliaryServiceWithCustomClassLoader.serviceInit": "  protected void serviceInit(Configuration conf) throws Exception {\n    // We pass a shared configuration as part of serviceInit call.\n    // To avoid the scenario that we could get a ClassNotFoundException\n    // when we use customClassLoader to load the class, we create a copy\n    // of the configuration.\n    Configuration config = new Configuration(conf);\n    // reset the service configuration\n    setConfig(config);\n    // Ensure the appClasspath includes the path to org.apache.auxtest.AuxServiceFromLocal\n    String appClassPath = config.get(YarnConfiguration.NM_AUX_SERVICES_APP_CLASSPATH);\n    if (appClassPath == null || appClassPath.isEmpty()) {\n        throw new IOException(\"Application classpath is not set. Please ensure it includes the path to org.apache.auxtest.AuxServiceFromLocal.\");\n    }\n    config.setClassLoader(customClassLoader);\n    ClassLoader original = Thread.currentThread().getContextClassLoader();\n    Thread.currentThread().setContextClassLoader(customClassLoader);\n    try {\n      wrapped.init(config);\n    } finally {\n      Thread.currentThread().setContextClassLoader(original);\n    }\n  }"
        }
    },
    {
        "filename": "YARN-8331.json",
        "creation_time": "2018-05-21T05:19:35.000+0000",
        "bug_report": {
            "Title": "Race condition in NM container launched after done",
            "Description": "A race condition occurs during the container launch process in the YARN framework. Specifically, when a container is in the SCHEDULED state, a kill event is sent, transitioning the state to KILLING and then to DONE. Subsequently, a CONTAINER_LAUNCHED event is sent, which is invalid for a container that is already in the DONE state. This leads to an InvalidStateTransitionException, indicating that the container is not able to handle further events after it has completed execution.",
            "StackTrace": [
                "org.apache.hadoop.yarn.state.InvalidStateTransitionException: Invalid event: CONTAINER_LAUNCHED at DONE",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$500(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:487)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl.handle(ContainerImpl.java:2104)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl.handle(ContainerImpl.java:104)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ContainerEventDispatcher.handle(ContainerManagerImpl.java:1525)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ContainerEventDispatcher.handle(ContainerManagerImpl.java:1518)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:197)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:126)",
                "at java.lang.Thread.run(Thread.java:748)"
            ],
            "RootCause": "The root cause of the issue is that the CONTAINER_LAUNCHED event is being sent to a container that is already in the DONE state, which is not a valid state transition according to the YARN state management system.",
            "StepsToReproduce": [
                "1. Launch a container in the YARN framework.",
                "2. Send a kill event to the container while it is in the SCHEDULED state.",
                "3. Observe the state transition from SCHEDULED to KILLING and then to DONE.",
                "4. Trigger the CONTAINER_LAUNCHED event for the same container.",
                "5. Check for the InvalidStateTransitionException in the logs."
            ],
            "ExpectedBehavior": "The container should not receive any events after it has transitioned to the DONE state, and no exceptions should be thrown.",
            "ObservedBehavior": "An InvalidStateTransitionException is thrown when the CONTAINER_LAUNCHED event is sent to a container that is already in the DONE state.",
            "Suggestions": "Review the event handling logic in the ContainerImpl class to ensure that events are not sent to containers that are in the DONE state. Implement checks to prevent invalid state transitions.",
            "problem_location": {
                "files": [
                    "ContainerImpl.java",
                    "StateMachineFactory.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl",
                    "org.apache.hadoop.yarn.state.StateMachineFactory"
                ],
                "methods": [
                    "ContainerImpl.handle",
                    "StateMachineFactory.doTransition"
                ]
            },
            "possible_fix": "Add a condition in the handle method of ContainerImpl to check if the container is in the DONE state before processing any further events. If it is, ignore the event or log a warning."
        },
        "possible_fix_code": {
            "ContainerImpl.handle": "  public void handle(ContainerEvent event) {\n    try {\n      this.writeLock.lock();\n\n      ContainerId containerID = event.getContainerID();\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Processing \" + containerID + \" of type \" + event.getType());\n      }\n      ContainerState oldState = stateMachine.getCurrentState();\n\n      // Check if the container is in DONE state before processing further events\n      if (oldState == ContainerState.COMPLETE) {\n        LOG.warn(\"Ignoring event for container \" + containerID + \" as it is already in DONE state.\");\n        return;\n      }\n\n      ContainerState newState = null;\n      try {\n        newState = stateMachine.doTransition(event.getType(), event);\n      } catch (InvalidStateTransitionException e) {\n        LOG.warn(\"Can't handle this event at current state: Current: [\" + oldState + \"], eventType: [\" + event.getType() + \"], container: [\" + containerID + \"]\", e);\n      }\n      if (newState != null && oldState != newState) {\n        LOG.info(\"Container \" + containerID + \" transitioned from \" + oldState + \" to \" + newState);\n      }\n    } finally {\n      this.writeLock.unlock();\n    }\n  }"
        }
    },
    {
        "filename": "YARN-2931.json",
        "creation_time": "2014-12-08T21:09:13.000+0000",
        "bug_report": {
            "Title": "PublicLocalizer may fail until directory is initialized by LocalizeRunner",
            "Description": "The issue arises when the data directory is cleaned up and the NodeManager (NM) is started with an existing recovery state. Due to YARN-90, the local directories are not recreated, leading to a failure in the PublicLocalizer until the getInitializedLocalDirs method is invoked by a LocalizeRunner for private localization. The error is primarily a FileNotFoundException indicating that the expected directory for file caching does not exist.",
            "StackTrace": [
                "2014-12-02 22:57:32,629 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService: Failed to download rsrc { { hdfs:/<blah machine>:8020/tmp/hive-hive/hive_2014-12-02_22-56-58_741_2045919883676051996-3/-mr-10004/8060c9dd-54b6-42fc-9d77-34b655fa5e82/reduce.xml, 1417589819618, FILE, null },pending,[(container_1417589109512_0001_02_000003)],119413444132127,DOWNLOADING}",
                "java.io.FileNotFoundException: File /data/yarn/nm/filecache does not exist",
                "at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:524)",
                "at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:737)",
                "at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:514)",
                "at org.apache.hadoop.fs.FileSystem.primitiveMkdir(FileSystem.java:1051)",
                "at org.apache.hadoop.fs.DelegateToFileSystem.mkdir(DelegateToFileSystem.java:162)",
                "at org.apache.hadoop.fs.FilterFs.mkdir(FilterFs.java:197)",
                "at org.apache.hadoop.fs.FileContext$4.next(FileContext.java:724)",
                "at org.apache.hadoop.fs.FileContext$4.next(FileContext.java:720)",
                "at org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)",
                "at org.apache.hadoop.fs.FileContext.mkdir(FileContext.java:720)",
                "at org.apache.hadoop.yarn.util.FSDownload.createDir(FSDownload.java:104)",
                "at org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:351)",
                "at org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:60)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:262)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:262)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:745)",
                "2014-12-02 22:57:32,629 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container: Container container_1417589109512_0001_02_000003 transitioned from LOCALIZING to LOCALIZATION_FAILED"
            ],
            "RootCause": "The root cause of the issue is that the directory '/data/yarn/nm/filecache' does not exist, leading to a FileNotFoundException when the system attempts to access it. This failure is likely due to the mkdirs method in the FileSystem class not being called successfully, possibly because of permission issues or an incorrect path.",
            "StepsToReproduce": [
                "Clean up the data directory.",
                "Start the NodeManager with an existing recovery state.",
                "Observe the logs for any FileNotFoundException related to '/data/yarn/nm/filecache'."
            ],
            "ExpectedBehavior": "The system should recreate the necessary local directories automatically, allowing the PublicLocalizer to function without errors.",
            "ObservedBehavior": "The PublicLocalizer fails to download resources due to a missing directory, resulting in a FileNotFoundException.",
            "Suggestions": "Verify that the path '/data/yarn/nm/filecache' is correct and properly formatted. Ensure that the necessary permissions are granted to create directories in the specified location. Check if the parent directories exist and create them if necessary.",
            "problem_location": {
                "files": [
                    "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystem.java"
                ],
                "classes": [
                    "org.apache.hadoop.fs.FileSystem"
                ],
                "methods": [
                    "mkdirs",
                    "checkPath",
                    "getFileStatus"
                ]
            },
            "possible_fix": "Ensure that the mkdirs method is called to create the '/data/yarn/nm/filecache' directory, and check for any permission issues that may prevent its creation. If necessary, modify the code to handle directory creation more robustly."
        },
        "possible_fix_code": {
            "mkdirs": "  public abstract boolean mkdirs(Path f, FsPermission permission) throws IOException {\n    // Check if the parent directory exists\n    Path parent = f.getParent();\n    if (parent != null && !getFileStatus(parent).isDirectory()) {\n        // If the parent directory does not exist, create it\n        mkdirs(parent, permission);\n    }\n    // Attempt to create the directory\n    if (!exists(f)) {\n        // If the directory does not exist, create it\n        return createNewDirectory(f, permission);\n    }\n    return true;\n  }\n\n  private boolean createNewDirectory(Path f, FsPermission permission) throws IOException {\n    // Logic to create the directory with the specified permissions\n    // This is a placeholder for the actual implementation\n    // Ensure that the necessary permissions are checked and handled\n    // Return true if the directory was created successfully, false otherwise\n    return true; // Placeholder return value\n  }"
        }
    },
    {
        "filename": "YARN-6837.json",
        "creation_time": "2017-07-18T11:17:55.000+0000",
        "bug_report": {
            "Title": "Null LocalResource visibility or resource type can crash the NodeManager",
            "Description": "When creating a YARN application, if a LocalResource is instantiated without setting its visibility, it can lead to a NullPointerException in the NodeManager. This occurs specifically in the `addResources` method of the `ResourceSet` class when it attempts to process resources with null visibility. The issue manifests as NodeManager crashes when it encounters a LocalResource with null visibility during resource addition, leading to a cascading failure in container management.",
            "StackTrace": [
                "2017-07-18 17:54:09,292 FATAL org.apache.hadoop.yarn.event.AsyncDispatcher: Error in dispatcher thread",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceSet.addResources(ResourceSet.java:84)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl$RequestResourcesTransition.transition(ContainerImpl.java:868)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl$RequestResourcesTransition.transition(ContainerImpl.java:819)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$MultipleInternalArc.doTransition(StateMachineFactory.java:385)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl.handle(ContainerImpl.java:1684)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl.handle(ContainerImpl.java:96)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ContainerEventDispatcher.handle(ContainerManagerImpl.java:1418)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ContainerEventDispatcher.handle(ContainerManagerImpl.java:1411)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:197)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:126)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "RootCause": "The root cause of the issue is that the `addResources` method in the `ResourceSet` class does not handle cases where the visibility of a LocalResource is null. This leads to a NullPointerException when the method attempts to categorize resources based on their visibility.",
            "StepsToReproduce": [
                "Create a LocalResource without setting its visibility.",
                "Submit a YARN application that uses this LocalResource.",
                "Observe the NodeManager logs for NullPointerException."
            ],
            "ExpectedBehavior": "The NodeManager should handle LocalResources with null visibility gracefully, without crashing.",
            "ObservedBehavior": "The NodeManager crashes with a NullPointerException when processing a LocalResource with null visibility.",
            "Suggestions": "Ensure that the visibility of LocalResource is set to a valid value (e.g., PUBLIC, PRIVATE, APPLICATION) before submission. Implement checks in the `addResources` method to handle null visibility cases appropriately.",
            "problem_location": {
                "files": [
                    "ResourceSet.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceSet"
                ],
                "methods": [
                    "addResources"
                ]
            },
            "possible_fix": "In the `addResources` method, add a check for null visibility before processing resources. For example:\n\nif (resource.getVisibility() == null) {\n    throw new IllegalArgumentException(\"Resource visibility cannot be null\");\n}"
        },
        "possible_fix_code": {
            "addResources": "  public Map<LocalResourceVisibility, Collection<LocalResourceRequest>>\n      addResources(Map<String, LocalResource> localResourceMap)\n      throws URISyntaxException {\n    if (localResourceMap == null || localResourceMap.isEmpty()) {\n      return null;\n    }\n    Map<LocalResourceRequest, Set<String>> allResources = new HashMap<>();\n    List<LocalResourceRequest> publicList = new ArrayList<>();\n    List<LocalResourceRequest> privateList = new ArrayList<>();\n    List<LocalResourceRequest> appList = new ArrayList<>();\n\n    for (Map.Entry<String, LocalResource> rsrc : localResourceMap.entrySet()) {\n      LocalResource resource = rsrc.getValue();\n      if (resource.getVisibility() == null) {\n          throw new IllegalArgumentException(\"Resource visibility cannot be null\");\n      }\n      LocalResourceRequest req = new LocalResourceRequest(resource);\n      allResources.putIfAbsent(req, new HashSet<>());\n      allResources.get(req).add(rsrc.getKey());\n      storeSharedCacheUploadPolicy(req,\n          resource.getShouldBeUploadedToSharedCache());\n      switch (resource.getVisibility()) {\n      case PUBLIC:\n        publicList.add(req);\n        break;\n      case PRIVATE:\n        privateList.add(req);\n        break;\n      case APPLICATION:\n        appList.add(req);\n        break;\n      default:\n        break;\n      }\n    }\n    Map<LocalResourceVisibility, Collection<LocalResourceRequest>> req =\n        new LinkedHashMap<>();\n    if (!publicList.isEmpty()) {\n      publicRsrcs.addAll(publicList);\n      req.put(LocalResourceVisibility.PUBLIC, publicList);\n    }\n    if (!privateList.isEmpty()) {\n      privateRsrcs.addAll(privateList);\n      req.put(LocalResourceVisibility.PRIVATE, privateList);\n    }\n    if (!appList.isEmpty()) {\n      appRsrcs.addAll(appList);\n      req.put(LocalResourceVisibility.APPLICATION, appList);\n    }\n    if (!allResources.isEmpty()) {\n      this.pendingResources.putAll(allResources);\n    }\n    return req;\n  }"
        }
    },
    {
        "filename": "YARN-4762.json",
        "creation_time": "2016-03-04T02:24:47.000+0000",
        "bug_report": {
            "Title": "NMs failing on DelegatingLinuxContainerRuntime init with LCE on",
            "Description": "The Node Managers (NMs) are crashing during initialization due to a failure in the Linux container executor setup. The logs indicate a `YarnRuntimeException` caused by an `IOException` when attempting to initialize the Linux container runtime. This issue is likely related to misconfiguration or permission problems in the Hadoop YARN environment, particularly concerning the Linux container executor.",
            "StackTrace": [
                "org.apache.hadoop.yarn.server.nodemanager.NodeManager.serviceInit(NodeManager.java:240)",
                "org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)",
                "org.apache.hadoop.yarn.server.nodemanager.NodeManager.initAndStartNodeManager(NodeManager.java:539)",
                "org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.init(LinuxContainerExecutor.java:207)",
                "java.io.IOException: Failed to initialize linux container runtime(s)!"
            ],
            "RootCause": "The root cause of the issue is a failure in the `init` method of the `LinuxContainerExecutor`, which is unable to initialize the Linux container runtime due to potential misconfiguration or permission issues.",
            "StepsToReproduce": [
                "Start the Node Manager with the Linux Container Executor enabled.",
                "Check the logs for any initialization errors related to the container executor."
            ],
            "ExpectedBehavior": "The Node Manager should initialize successfully without any exceptions, allowing it to manage containers as expected.",
            "ObservedBehavior": "The Node Manager fails to start, logging a `YarnRuntimeException` indicating that the container executor could not be initialized.",
            "Suggestions": "Check the configuration settings in `yarn-site.xml` for the following properties: `yarn.nodemanager.container-executor.class`, `yarn.nodemanager.linux-container-executor.group`, and ensure that the paths to the container executor executable are correct. Verify that the Node Manager has the necessary permissions to access these resources.",
            "problem_location": {
                "files": [
                    "yarn-site.xml",
                    "NodeManager.java",
                    "LinuxContainerExecutor.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.nodemanager.NodeManager",
                    "org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor"
                ],
                "methods": [
                    "LinuxContainerExecutor.init"
                ]
            },
            "possible_fix": "Ensure that the `yarn.nodemanager.container-executor.class` is set to `org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor` in `yarn-site.xml`. Verify that the executor has the correct permissions and that the path to the container executor executable is valid."
        },
        "possible_fix_code": {
            "LinuxContainerExecutor.init": "  public void init() throws IOException {\n    Configuration conf = super.getConf();\n\n    // Send command to executor which will just start up,\n    // verify configuration/permissions and exit\n    try {\n      PrivilegedOperation checkSetupOp = new PrivilegedOperation(\n          PrivilegedOperation.OperationType.CHECK_SETUP, (String) null);\n      PrivilegedOperationExecutor privilegedOperationExecutor =\n          PrivilegedOperationExecutor.getInstance(conf);\n\n      privilegedOperationExecutor.executePrivilegedOperation(checkSetupOp,\n          false);\n    } catch (PrivilegedOperationException e) {\n      int exitCode = e.getExitCode();\n      LOG.warn(\"Exit code from container executor initialization is : \"\n          + exitCode, e);\n\n      throw new IOException(\"Linux container executor not configured properly\"\n          + \" (error=\" + exitCode + \")\", e);\n    }\n\n    try {\n      resourceHandlerChain = ResourceHandlerModule\n          .getConfiguredResourceHandlerChain(conf);\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Resource handler chain enabled = \" + (resourceHandlerChain\n            == null));\n      }\n      if (resourceHandlerChain != null) {\n        LOG.debug(\"Bootstrapping resource handler chain\");\n        resourceHandlerChain.bootstrap(conf);\n      }\n    } catch (ResourceHandlerException e) {\n      LOG.error(\"Failed to bootstrap configured resource subsystems! \", e);\n      throw new IOException(\n          \"Failed to bootstrap configured resource subsystems!\");\n    }\n\n    try {\n      if (linuxContainerRuntime == null) {\n        LinuxContainerRuntime runtime = new DelegatingLinuxContainerRuntime();\n\n        runtime.initialize(conf);\n        this.linuxContainerRuntime = runtime;\n      }\n    } catch (ContainerExecutionException e) {\n      throw new IOException(\"Failed to initialize linux container runtime(s)!\", e);\n    }\n\n    resourcesHandler.init(this);\n  }"
        }
    },
    {
        "filename": "YARN-2823.json",
        "creation_time": "2014-11-06T21:38:47.000+0000",
        "bug_report": {
            "Title": "NullPointerException in RM HA enabled 3-node cluster",
            "Description": "In a 3-node cluster with ResourceManager High Availability (HA) enabled, a NullPointerException (NPE) occurs during the handling of application attempts. The issue arises when the ResourceManager attempts to process an event of type APP_ATTEMPT_ADDED, leading to a failure in the scheduler. The stack trace indicates that the exception is thrown in the `transferStateFromPreviousAttempt` method of the `SchedulerApplicationAttempt` class, suggesting that a null reference is being accessed.",
            "StackTrace": [
                "2014-09-16 01:36:28,037 FATAL resourcemanager.ResourceManager (ResourceManager.java:run(612)) - Error in handling event type APP_ATTEMPT_ADDED to the scheduler",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt.transferStateFromPreviousAttempt(SchedulerApplicationAttempt.java:530)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.addApplicationAttempt(CapacityScheduler.java:678)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:1015)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:98)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:603)",
                "at java.lang.Thread.run(Thread.java:744)"
            ],
            "RootCause": "The NullPointerException is likely caused by a null reference in the `transferStateFromPreviousAttempt` method of the `SchedulerApplicationAttempt` class. This occurs when the `appAttempt` parameter is null or when any of the methods called on it return null, leading to an attempt to access properties of a null object.",
            "StepsToReproduce": [
                "Set up a 3-node cluster with ResourceManager HA enabled using Ambari.",
                "Install HBase using Slider.",
                "Allow the ResourceManagers to go down and attempt to bring them back up.",
                "Monitor the ResourceManager logs for NullPointerException related to APP_ATTEMPT_ADDED events."
            ],
            "ExpectedBehavior": "The ResourceManager should handle application attempts without throwing exceptions, successfully transferring state from previous attempts when applicable.",
            "ObservedBehavior": "The ResourceManager throws a NullPointerException when handling APP_ATTEMPT_ADDED events, causing it to exit unexpectedly.",
            "Suggestions": "Implement null checks in the `transferStateFromPreviousAttempt` method to ensure that the `appAttempt` parameter and its properties are not null before accessing them. Additionally, verify that the `ApplicationAttemptId` passed to the `AppAttemptAddedSchedulerEvent` constructor is not null.",
            "problem_location": {
                "files": [
                    "SchedulerApplicationAttempt.java",
                    "CapacityScheduler.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt",
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler"
                ],
                "methods": [
                    "SchedulerApplicationAttempt.transferStateFromPreviousAttempt",
                    "CapacityScheduler.addApplicationAttempt",
                    "CapacityScheduler.handle"
                ]
            },
            "possible_fix": "Add null checks in the `transferStateFromPreviousAttempt` method to ensure that the `appAttempt` parameter is not null before accessing its properties. For example:\n\npublic synchronized void transferStateFromPreviousAttempt(SchedulerApplicationAttempt appAttempt) {\n    if (appAttempt == null) {\n        throw new IllegalArgumentException(\"appAttempt cannot be null\");\n    }\n    this.liveContainers = appAttempt.getLiveContainersMap();\n    this.currentConsumption = appAttempt.getCurrentConsumption();\n    this.resourceLimit = appAttempt.getResourceLimit();\n    this.lastScheduledContainer = appAttempt.getLastScheduledContainer();\n    this.appSchedulingInfo.transferStateFromPreviousAppSchedulingInfo(appAttempt.appSchedulingInfo);\n}"
        },
        "possible_fix_code": {
            "SchedulerApplicationAttempt.transferStateFromPreviousAttempt": "  public synchronized void transferStateFromPreviousAttempt(\n      SchedulerApplicationAttempt appAttempt) {\n    if (appAttempt == null) {\n        throw new IllegalArgumentException(\"appAttempt cannot be null\");\n    }\n    this.liveContainers = appAttempt.getLiveContainersMap();\n    this.currentConsumption = appAttempt.getCurrentConsumption();\n    this.resourceLimit = appAttempt.getResourceLimit();\n    this.lastScheduledContainer = appAttempt.getLastScheduledContainer();\n    this.appSchedulingInfo\n      .transferStateFromPreviousAppSchedulingInfo(appAttempt.appSchedulingInfo);\n  }"
        }
    },
    {
        "filename": "YARN-5098.json",
        "creation_time": "2016-05-17T00:43:08.000+0000",
        "bug_report": {
            "Title": "Yarn Application Log Aggregation Fails Due to NM Unable to Retrieve HDFS Delegation Token",
            "Description": "In a high-availability (HA) cluster environment, the Yarn application logs for a long-running application could not be aggregated due to a failure in the NodeManager's communication with HDFS. The error indicates that the HDFS delegation token cannot be found in the cache, which suggests potential issues with token expiration, invalidation, or client misconfiguration. The stack trace shows that the error originates from the `SaslRpcClient` during the connection setup, indicating a failure in establishing a secure connection to HDFS.",
            "StackTrace": [
                "2016-05-16 18:18:28,533 INFO  logaggregation.AppLogAggregatorImpl (AppLogAggregatorImpl.java:finishLogAggregation(555)) - Application just finished : application_1463170334122_0002",
                "2016-05-16 18:18:28,545 WARN  ipc.Client (Client.java:run(705)) - Exception encountered while connecting to the server :",
                "org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.token.SecretManager$InvalidToken): token (HDFS_DELEGATION_TOKEN token 171 for hrt_qa) can't be found in cache",
                "at org.apache.hadoop.security.SaslRpcClient.saslConnect(SaslRpcClient.java:375)",
                "at org.apache.hadoop.ipc.Client$Connection.setupSaslConnection(Client.java:583)",
                "at org.apache.hadoop.ipc.Client$Connection.access$1900(Client.java:398)",
                "at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:752)",
                "at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:748)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1719)",
                "at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:747)",
                "at org.apache.hadoop.ipc.Client$Connection.access$3100(Client.java:398)",
                "at org.apache.hadoop.ipc.Client.getConnection(Client.java:1597)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1439)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1386)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:240)",
                "at com.sun.proxy.$Proxy83.getServerDefaults(Unknown Source)",
                "at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getServerDefaults(ClientNamenodeProtocolTranslatorPB.java:282)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:498)",
                "at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:256)",
                "at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)",
                "at com.sun.proxy.$Proxy84.getServerDefaults(Unknown Source)",
                "at org.apache.hadoop.hdfs.DFSClient.getServerDefaults(DFSClient.java:1018)",
                "at org.apache.hadoop.fs.Hdfs.getServerDefaults(Hdfs.java:156)",
                "at org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:550)",
                "at org.apache.hadoop.fs.FileContext$3.next(FileContext.java:687)"
            ],
            "RootCause": "The root cause of the issue is that the HDFS delegation token cannot be found in the cache, likely due to expiration, invalidation, or misconfiguration of the client settings related to token management.",
            "StepsToReproduce": [
                "Deploy a Yarn application in a high-availability cluster environment.",
                "Run a long-running application that requires log aggregation.",
                "Monitor the NodeManager logs for errors related to HDFS delegation tokens."
            ],
            "ExpectedBehavior": "The NodeManager should successfully retrieve the HDFS delegation token and aggregate the application logs without any errors.",
            "ObservedBehavior": "The NodeManager fails to retrieve the HDFS delegation token, resulting in an inability to aggregate logs, accompanied by a warning about the token not being found in the cache.",
            "Suggestions": "To resolve this issue, check the following: 1) Ensure that the delegation token is renewed before expiration. 2) Verify that the token has not been invalidated by the SecretManager. 3) Confirm that the client is properly configured for token management. 4) Investigate any potential cache issues that may prevent token retrieval. 5) Ensure that the user has the necessary permissions to access the token.",
            "problem_location": {
                "files": [
                    "AppLogAggregatorImpl.java",
                    "Client.java"
                ],
                "classes": [
                    "org.apache.hadoop.security.token.SecretManager",
                    "org.apache.hadoop.ipc.Client"
                ],
                "methods": [
                    "AppLogAggregatorImpl.finishLogAggregation",
                    "Client.run"
                ]
            },
            "possible_fix": "Review and update the token management configuration in the Hadoop setup. Ensure that the delegation tokens are properly cached and renewed as needed. Additionally, check the permissions and network configurations to ensure seamless communication between the NodeManager and HDFS."
        },
        "possible_fix_code": {
            "Client.run": "          public void run() {\n            try {\n              synchronized (Connection.this.out) {\n                if (shouldCloseConnection.get()) {\n                  return;\n                }\n                \n                if (LOG.isDebugEnabled())\n                  LOG.debug(getName() + \" sending #\" + call.id);\n         \n                byte[] data = d.getData();\n                int totalLength = d.getLength();\n                out.writeInt(totalLength); // Total Length\n                out.write(data, 0, totalLength);// RpcRequestHeader + RpcRequest\n                out.flush();\n              }\n            } catch (IOException e) {\n              // exception at this point would leave the connection in an\n              // unrecoverable state (eg half a call left on the wire).\n              // So, close the connection, killing any outstanding calls\n              markClosed(e);\n            } finally {\n              //the buffer is just an in-memory buffer, but it is still polite to\n              // close early\n              IOUtils.closeStream(d);\n            }\n          }"
        }
    },
    {
        "filename": "YARN-3971.json",
        "creation_time": "2015-07-24T10:17:05.000+0000",
        "bug_report": {
            "Title": "Skip RMNodeLabelsManager#checkRemoveFromClusterNodeLabelsOfQueue on nodelabel recovery",
            "Description": "The issue arises during the recovery process of node labels in the ResourceManager when attempting to remove a label that is still in use by a queue. Specifically, the label 'x' cannot be removed because it is associated with queue 'a1'. The error occurs in the `FileSystemNodeLabelsStore#recover` method, which triggers the `RMNodeLabelsManager#checkRemoveFromClusterNodeLabelsOfQueue` method to validate the removal of labels. This validation fails, resulting in an IOException.",
            "StackTrace": [
                "java.io.IOException: Cannot remove label=x, because queue=a1 is using this label. Please remove label on queue before remove the label",
                "at org.apache.hadoop.yarn.server.resourcemanager.nodelabels.RMNodeLabelsManager.checkRemoveFromClusterNodeLabelsOfQueue(RMNodeLabelsManager.java:104)",
                "at org.apache.hadoop.yarn.server.resourcemanager.nodelabels.RMNodeLabelsManager.removeFromClusterNodeLabels(RMNodeLabelsManager.java:118)",
                "at org.apache.hadoop.yarn.nodelabels.FileSystemNodeLabelsStore.recover(FileSystemNodeLabelsStore.java:221)",
                "at org.apache.hadoop.yarn.nodelabels.CommonNodeLabelsManager.initNodeLabelStore(CommonNodeLabelsManager.java:232)",
                "at org.apache.hadoop.yarn.nodelabels.CommonNodeLabelsManager.serviceStart(CommonNodeLabelsManager.java:245)"
            ],
            "RootCause": "The root cause of the issue is that the label 'x' is currently in use by queue 'a1', preventing its removal from the cluster. The `checkRemoveFromClusterNodeLabelsOfQueue` method enforces this constraint by throwing an IOException when an attempt is made to remove a label that is still associated with any queue.",
            "StepsToReproduce": [
                "Create label x,y",
                "Delete label x,y",
                "Create label x,y and add capacity scheduler XML for labels x and y",
                "Restart ResourceManager"
            ],
            "ExpectedBehavior": "The label 'x' should be successfully removed from the cluster if it is not in use by any queues.",
            "ObservedBehavior": "An IOException is thrown indicating that the label 'x' cannot be removed because it is still in use by queue 'a1'.",
            "Suggestions": "To resolve this issue, first remove the label 'x' from any queues that are using it, specifically queue 'a1'. After ensuring that the label is no longer in use, attempt to remove it from the cluster again.",
            "problem_location": {
                "files": [
                    "hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/nodelabels/RMNodeLabelsManager.java",
                    "hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/nodelabels/FileSystemNodeLabelsStore.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.nodelabels.RMNodeLabelsManager",
                    "org.apache.hadoop.yarn.nodelabels.FileSystemNodeLabelsStore"
                ],
                "methods": [
                    "RMNodeLabelsManager.checkRemoveFromClusterNodeLabelsOfQueue",
                    "RMNodeLabelsManager.removeFromClusterNodeLabels",
                    "FileSystemNodeLabelsStore.recover"
                ]
            },
            "possible_fix": "1. Identify the queue(s) using label 'x' by calling `getAccessibleNodeLabels()` on the relevant queue instances. 2. Remove label 'x' from those queues. 3. Once the label is no longer in use, call `removeFromClusterNodeLabels` in the `RMNodeLabelsManager` to successfully remove the label from the cluster."
        },
        "possible_fix_code": {
            "RMNodeLabelsManager.removeFromClusterNodeLabels": "  public void removeFromClusterNodeLabels(Collection<String> labelsToRemove)\n      throws IOException {\n    try {\n      writeLock.lock();\n\n      // Identify queues using the labels to remove\n      for (String label : labelsToRemove) {\n        label = normalizeLabel(label);\n        for (Entry<String, Queue> entry : queueCollections.entrySet()) {\n          String queueName = entry.getKey();\n          Set<String> queueLabels = entry.getValue().getAccessibleNodeLabels();\n          if (queueLabels.contains(label)) {\n            // Remove label from the queue before proceeding\n            entry.getValue().removeLabel(label);\n          }\n        }\n      }\n\n      // Now check if any labels are still in use\n      checkRemoveFromClusterNodeLabelsOfQueue(labelsToRemove);\n\n      // copy before NMs\n      Map<String, Host> before = cloneNodeMap();\n\n      super.removeFromClusterNodeLabels(labelsToRemove);\n\n      updateResourceMappings(before, nodeCollections);\n    } finally {\n      writeLock.unlock();\n    }\n  }"
        }
    },
    {
        "filename": "YARN-6948.json",
        "creation_time": "2017-08-04T08:23:46.000+0000",
        "bug_report": {
            "Title": "Invalid event: ATTEMPT_ADDED at FINAL_SAVING",
            "Description": "The application encountered an InvalidStateTransitionException when processing an ATTEMPT_ADDED event while in the FINAL_SAVING state. This indicates that the state machine governing application attempts in the Hadoop YARN framework does not permit this transition. The error occurs in the RMAppAttemptImpl class, specifically within the handle method, which is responsible for managing state transitions based on incoming events.",
            "StackTrace": [
                "2017-08-03 01:35:20,485 ERROR org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Can't handle this event at current state",
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: ATTEMPT_ADDED at FINAL_SAVING",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:757)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:106)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:834)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:815)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:173)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:106)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "RootCause": "The root cause of the issue is that the ATTEMPT_ADDED event is being processed while the application attempt is in the FINAL_SAVING state, which is not a valid transition according to the state machine defined in the YARN framework.",
            "StepsToReproduce": [
                "1. Start a job in the YARN framework.",
                "2. Send a kill command to the running job.",
                "3. Check the logs for any exceptions related to state transitions."
            ],
            "ExpectedBehavior": "The system should handle the ATTEMPT_ADDED event appropriately without throwing an InvalidStateTransitionException, allowing for valid state transitions as defined in the state machine.",
            "ObservedBehavior": "An InvalidStateTransitionException is thrown, indicating that the ATTEMPT_ADDED event cannot be processed while in the FINAL_SAVING state.",
            "Suggestions": "Review the state machine configuration for RMAppAttemptState to identify valid transitions from the FINAL_SAVING state. Ensure that the application logic does not attempt to add an attempt when in this state.",
            "problem_location": {
                "files": [
                    "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.java",
                    "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptState.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl",
                    "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptState"
                ],
                "methods": [
                    "RMAppAttemptImpl.handle"
                ]
            },
            "possible_fix": "To fix the issue, ensure that the application does not attempt to add an attempt when it is in the FINAL_SAVING state. This may involve adding checks in the handle method of RMAppAttemptImpl to validate the current state before processing the ATTEMPT_ADDED event."
        },
        "possible_fix_code": {
            "RMAppAttemptImpl.handle": "public void handle(RMAppAttemptEvent event) {\n\n    this.writeLock.lock();\n\n    try {\n      ApplicationAttemptId appAttemptID = event.getApplicationAttemptId();\n      LOG.debug(\"Processing event for \" + appAttemptID + \" of type \" + event.getType());\n      final RMAppAttemptState oldState = getAppAttemptState();\n\n      // Check if the current state is FINAL_SAVING before processing ATTEMPT_ADDED event\n      if (event.getType() == RMAppAttemptEventType.ATTEMPT_ADDED && oldState == RMAppAttemptState.FINAL_SAVING) {\n          LOG.error(\"Cannot process ATTEMPT_ADDED event in FINAL_SAVING state\");\n          return; // Early exit to prevent InvalidStateTransitionException\n      }\n\n      try {\n        /* keep the master in sync with the state machine */\n        this.stateMachine.doTransition(event.getType(), event);\n      } catch (InvalidStateTransitionException e) {\n        LOG.error(\"Can't handle this event at current state\", e);\n        /* TODO fail the application on the failed transition */\n      }\n\n      // Log at INFO if we're not recovering or not in a terminal state.\n      // Log at DEBUG otherwise.\n      if ((oldState != getAppAttemptState()) &&\n          ((recoveredFinalState == null) ||\n            (event.getType() != RMAppAttemptEventType.RECOVER))) {\n        LOG.info(String.format(STATE_CHANGE_MESSAGE, appAttemptID, oldState,\n            getAppAttemptState(), event.getType()));\n      } else if ((oldState != getAppAttemptState()) && LOG.isDebugEnabled()) {\n        LOG.debug(String.format(STATE_CHANGE_MESSAGE, appAttemptID, oldState,\n            getAppAttemptState(), event.getType()));\n      }\n    } finally {\n      this.writeLock.unlock();\n    }\n  }"
        }
    },
    {
        "filename": "YARN-1409.json",
        "creation_time": "2013-11-13T11:25:56.000+0000",
        "bug_report": {
            "Title": "NonAggregatingLogHandler can throw RejectedExecutionException",
            "Description": "The issue arises when the NonAggregatingLogHandler attempts to handle APPLICATION_FINISHED events after the ScheduledThreadPoolExecutor has been instructed to shut down. This leads to a RejectedExecutionException, as the executor is no longer accepting new tasks. The stack trace indicates that the error occurs during the dispatching of events in the AsyncDispatcher, specifically when trying to execute a task that has been rejected due to the executor's shutdown state.",
            "StackTrace": [
                "2013-11-13 10:53:06,970 FATAL [AsyncDispatcher event handler] event.AsyncDispatcher (AsyncDispatcher.java:dispatch(166)) - Error in dispatcher thread",
                "java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@d51df63 rejected from java.util.concurrent.ScheduledThreadPoolExecutor@7a20e369[Shutting down, pool size = 4, active threads = 0, queued tasks = 7, completed tasks = 0]",
                "at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2048)",
                "at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:821)",
                "at java.util.concurrent.ScheduledThreadPoolExecutor.delayedExecute(ScheduledThreadPoolExecutor.java:325)",
                "at java.util.concurrent.ScheduledThreadPoolExecutor.schedule(ScheduledThreadPoolExecutor.java:530)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.NonAggregatingLogHandler.handle(NonAggregatingLogHandler.java:121)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.NonAggregatingLogHandler.handle(NonAggregatingLogHandler.java:49)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:159)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:95)",
                "at java.lang.Thread.run(Thread.java:724)"
            ],
            "RootCause": "The root cause of the RejectedExecutionException is that the ScheduledThreadPoolExecutor is in the process of shutting down when NonAggregatingLogHandler attempts to submit a new task. This occurs after the serviceStop() method is called, which leads to the executor rejecting new tasks.",
            "StepsToReproduce": [
                "1. Trigger an APPLICATION_FINISHED event in the system.",
                "2. Ensure that the NonAggregatingLogHandler's serviceStop() method is called, which initiates the shutdown of the ScheduledThreadPoolExecutor.",
                "3. Observe the logs for the RejectedExecutionException when the handler attempts to process the event."
            ],
            "ExpectedBehavior": "The NonAggregatingLogHandler should be able to handle APPLICATION_FINISHED events without throwing a RejectedExecutionException, even if the executor is shutting down.",
            "ObservedBehavior": "The system throws a RejectedExecutionException when the NonAggregatingLogHandler attempts to handle events after the executor has been shut down.",
            "Suggestions": "To resolve this issue, ensure that the NonAggregatingLogHandler does not attempt to handle events after the executor has been instructed to shut down. This can be achieved by checking the state of the executor before submitting tasks or by implementing a mechanism to gracefully handle shutdown scenarios.",
            "problem_location": {
                "files": [
                    "NonAggregatingLogHandler.java",
                    "AsyncDispatcher.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.NonAggregatingLogHandler",
                    "org.apache.hadoop.yarn.event.AsyncDispatcher"
                ],
                "methods": [
                    "NonAggregatingLogHandler.handle",
                    "AsyncDispatcher.dispatch"
                ]
            },
            "possible_fix": "Modify the NonAggregatingLogHandler to check if the ScheduledThreadPoolExecutor is shutting down before attempting to handle new tasks. This can prevent the RejectedExecutionException from occurring."
        },
        "possible_fix_code": {
            "NonAggregatingLogHandler.handle": "  public void handle(LogHandlerEvent event) {\n    switch (event.getType()) {\n      case APPLICATION_STARTED:\n        LogHandlerAppStartedEvent appStartedEvent =\n            (LogHandlerAppStartedEvent) event;\n        this.appOwners.put(appStartedEvent.getApplicationId(),\n            appStartedEvent.getUser());\n        this.dispatcher.getEventHandler().handle(\n            new ApplicationEvent(appStartedEvent.getApplicationId(),\n                ApplicationEventType.APPLICATION_LOG_HANDLING_INITED));\n        break;\n      case CONTAINER_FINISHED:\n        // Ignore\n        break;\n      case APPLICATION_FINISHED:\n        if (!sched.isShutdown()) { // Check if the scheduler is shutting down\n          LogHandlerAppFinishedEvent appFinishedEvent =\n              (LogHandlerAppFinishedEvent) event;\n          // Schedule - so that logs are available on the UI till they're deleted.\n          LOG.info(\"Scheduling Log Deletion for application: \"\n              + appFinishedEvent.getApplicationId() + \", with delay of \"\n              + this.deleteDelaySeconds + \" seconds\");\n          sched.schedule(\n              new LogDeleterRunnable(appOwners.remove(appFinishedEvent\n                  .getApplicationId()), appFinishedEvent.getApplicationId()),\n              this.deleteDelaySeconds, TimeUnit.SECONDS);\n        } else {\n          LOG.warn(\"Scheduler is shutting down, ignoring APPLICATION_FINISHED event.\");\n        }\n        break;\n      default:\n        ; // Ignore\n    }\n  }"
        }
    },
    {
        "filename": "YARN-5545.json",
        "creation_time": "2016-08-21T12:57:35.000+0000",
        "bug_report": {
            "Title": "Fix issues related to Max App in capacity scheduler",
            "Description": "This bug report addresses multiple issues related to the maximum applications allowed in the Capacity Scheduler of YARN. The primary concern is that when the capacity configuration for the default partition is set to zero, applications cannot be submitted to the `root.default` queue, even though other partitions have available capacity. This leads to an `AccessControlException` when attempting to submit applications, indicating a configuration issue with the queue settings.",
            "StackTrace": [
                "2016-08-21 18:21:31,375 INFO mapreduce.JobSubmitter: Cleaning up the staging area /tmp/hadoop-yarn/staging/root/.staging/job_1471670113386_0001",
                "java.io.IOException: org.apache.hadoop.yarn.exceptions.YarnException: Failed to submit application_1471670113386_0001 to YARN : org.apache.hadoop.security.AccessControlException: Queue root.default already has 0 applications, cannot accept submission of application: application_1471670113386_0001",
                "at org.apache.hadoop.mapred.YARNRunner.submitJob(YARNRunner.java:316)",
                "at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:255)",
                "at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1344)",
                "at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1341)",
                "Caused by: org.apache.hadoop.yarn.exceptions.YarnException: Failed to submit application_1471670113386_0001 to YARN : org.apache.hadoop.security.AccessControlException: Queue root.default already has 0 applications, cannot accept submission of application: application_1471670113386_0001",
                "at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.submitApplication(YarnClientImpl.java:286)",
                "at org.apache.hadoop.mapred.ResourceMgrDelegate.submitApplication(ResourceMgrDelegate.java:296)",
                "at org.apache.hadoop.mapred.YARNRunner.submitJob(YARNRunner.java:301)"
            ],
            "RootCause": "The root cause of the issue is that the `root.default` queue is configured with a capacity of zero, which prevents any applications from being submitted to it. This is compounded by the fact that the access control settings do not allow submissions when the application count is zero.",
            "StepsToReproduce": [
                "Configure the capacity scheduler with the following settings:",
                "yarn.scheduler.capacity.root.default.capacity=0",
                "yarn.scheduler.capacity.root.queue1.accessible-node-labels.labelx.capacity=50",
                "yarn.scheduler.capacity.root.default.accessible-node-labels.labelx.capacity=50",
                "Submit an application using the command:",
                "./yarn jar ../share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0-alpha2-SNAPSHOT-tests.jar sleep -Dmapreduce.job.node-label-expression=labelx -Dmapreduce.job.queuename=default -m 1 -r 1 -mt 10000000 -rt 1"
            ],
            "ExpectedBehavior": "The application should be successfully submitted to the `root.default` queue or another available queue without encountering an `AccessControlException`.",
            "ObservedBehavior": "The application submission fails with an `AccessControlException`, indicating that the `root.default` queue cannot accept submissions because it already has zero applications.",
            "Suggestions": "To resolve this issue, check the queue configuration for `root.default` to ensure it is set to accept applications. Verify the access control permissions for the user or service account attempting to submit the application. Review the YARN Resource Manager logs for additional context and ensure the cluster is healthy.",
            "problem_location": {
                "files": [
                    "capacity-scheduler.xml"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.conf.YarnConfiguration"
                ],
                "methods": []
            },
            "possible_fix": "Adjust the configuration for the `root.default` queue in the `capacity-scheduler.xml` file to allow for application submissions. Ensure that the queue has a non-zero capacity and that the access control settings permit submissions."
        },
        "possible_fix_code": {
            "YarnClientImpl.submitApplication": "  public ApplicationId\n      submitApplication(ApplicationSubmissionContext appContext)\n          throws YarnException, IOException {\n    ApplicationId applicationId = appContext.getApplicationId();\n    if (applicationId == null) {\n      throw new ApplicationIdNotProvidedException(\n          \"ApplicationId is not provided in ApplicationSubmissionContext\");\n    }\n    SubmitApplicationRequest request =\n        Records.newRecord(SubmitApplicationRequest.class);\n    request.setApplicationSubmissionContext(appContext);\n\n    // Automatically add the timeline DT into the CLC\n    // Only when the security and the timeline service are both enabled\n    if (isSecurityEnabled() && timelineServiceEnabled) {\n      addTimelineDelegationToken(appContext.getAMContainerSpec());\n    }\n\n    // Check if the queue has a non-zero capacity before submission\n    YarnConfiguration conf = new YarnConfiguration();\n    int capacity = conf.getInt(\"yarn.scheduler.capacity.root.default.capacity\", 0);\n    if (capacity <= 0) {\n      throw new YarnException(\"Queue root.default has zero capacity, cannot accept submissions.\");\n    }\n\n    //TODO: YARN-1763:Handle RM failovers during the submitApplication call.\n    rmClient.submitApplication(request);\n\n    int pollCount = 0;\n    long startTime = System.currentTimeMillis();\n    EnumSet<YarnApplicationState> waitingStates = \n                                 EnumSet.of(YarnApplicationState.NEW,\n                                 YarnApplicationState.NEW_SAVING,\n                                 YarnApplicationState.SUBMITTED);\n    EnumSet<YarnApplicationState> failToSubmitStates = \n                                  EnumSet.of(YarnApplicationState.FAILED,\n                                  YarnApplicationState.KILLED);\t\t\n    while (true) {\n      try {\n        ApplicationReport appReport = getApplicationReport(applicationId);\n        YarnApplicationState state = appReport.getYarnApplicationState();\n        if (!waitingStates.contains(state)) {\n          if(failToSubmitStates.contains(state)) {\n            throw new YarnException(\"Failed to submit \" + applicationId + \n                \" to YARN : \" + appReport.getDiagnostics());\n          }\n          LOG.info(\"Submitted application \" + applicationId);\n          break;\n        }\n\n        long elapsedMillis = System.currentTimeMillis() - startTime;\n        if (enforceAsyncAPITimeout() &&\n            elapsedMillis >= asyncApiPollTimeoutMillis) {\n          throw new YarnException(\"Timed out while waiting for application \" +\n              applicationId + \" to be submitted successfully\");\n        }\n\n        // Notify the client through the log every 10 poll, in case the client\n        // is blocked here too long.\n        if (++pollCount % 10 == 0) {\n          LOG.info(\"Application submission is not finished, \" +\n              \"submitted application \" + applicationId +\n              \" is still in \" + state);\n        }\n        try {\n          Thread.sleep(submitPollIntervalMillis);\n        } catch (InterruptedException ie) {\n          LOG.error(\"Interrupted while waiting for application \"\n              + applicationId\n              + \" to be successfully submitted.\");\n        }\n      } catch (ApplicationNotFoundException ex) {\n        // FailOver or RM restart happens before RMStateStore saves\n        // ApplicationState\n        LOG.info(\"Re-submit application \" + applicationId + \"with the \" +\n            \"same ApplicationSubmissionContext\");\n        rmClient.submitApplication(request);\n      }\n    }\n\n    return applicationId;\n  }"
        }
    },
    {
        "filename": "YARN-301.json",
        "creation_time": "2013-01-01T05:40:18.000+0000",
        "bug_report": {
            "Title": "Fair scheduler throws ConcurrentModificationException when iterating over app's priorities",
            "Description": "In a test cluster, the FairScheduler encounters a ConcurrentModificationException, leading to a ResourceManager crash. The stack trace indicates that the exception occurs during the processing of node updates, specifically when iterating over collections in the AppSchedulable and FairScheduler classes. This issue is likely due to concurrent modifications of shared collections while they are being accessed by multiple threads.",
            "StackTrace": [
                "2012-12-30 17:14:17,171 FATAL org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Error in handling event type NODE_UPDATE to the scheduler",
                "java.util.ConcurrentModificationException",
                "at java.util.TreeMap$PrivateEntryIterator.nextEntry(TreeMap.java:1100)",
                "at java.util.TreeMap$KeyIterator.next(TreeMap.java:1154)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AppSchedulable.assignContainer(AppSchedulable.java:297)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue.assignContainer(FSLeafQueue.java:181)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.nodeUpdate(FairScheduler.java:780)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:842)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:98)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:340)",
                "at java.lang.Thread.run(Thread.java:662)"
            ],
            "RootCause": "The ConcurrentModificationException is likely caused by concurrent modifications to collections in the FairScheduler and AppSchedulable classes during container assignment and node updates. Specifically, the nodeUpdate method in FairScheduler and the assignContainer method in AppSchedulable are involved in accessing shared resources without adequate synchronization.",
            "StepsToReproduce": [
                "Set up a test cluster with the FairScheduler.",
                "Trigger a node update event while multiple applications are requesting resources.",
                "Observe the ResourceManager logs for ConcurrentModificationException."
            ],
            "ExpectedBehavior": "The FairScheduler should handle node updates and resource assignments without throwing exceptions, ensuring that all applications receive their requested resources in a synchronized manner.",
            "ObservedBehavior": "The FairScheduler throws a ConcurrentModificationException, causing the ResourceManager to crash and fail to process node updates correctly.",
            "Suggestions": "Implement proper synchronization mechanisms when accessing shared collections in the FairScheduler and AppSchedulable classes. Consider using concurrent collections like CopyOnWriteArrayList for collections that are frequently modified during iteration.",
            "problem_location": {
                "files": [
                    "FairScheduler.java",
                    "AppSchedulable.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler",
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AppSchedulable"
                ],
                "methods": [
                    "FairScheduler.nodeUpdate",
                    "AppSchedulable.assignContainer"
                ]
            },
            "possible_fix": "In the FairScheduler class, ensure that the nodeUpdate method uses synchronized blocks or concurrent collections when iterating over shared resources. For example, replace the TreeMap with a CopyOnWriteArrayList where applicable to prevent ConcurrentModificationException."
        },
        "possible_fix_code": {
            "FairScheduler.nodeUpdate": "  private synchronized void nodeUpdate(RMNode nm,\n      List<ContainerStatus> newlyLaunchedContainers,\n      List<ContainerStatus> completedContainers) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"nodeUpdate: \" + nm + \" cluster capacity: \" + clusterCapacity);\n    }\n    eventLog.log(\"HEARTBEAT\", nm.getHostName());\n    FSSchedulerNode node = nodes.get(nm.getNodeID());\n\n    // Processing the newly launched containers\n    for (ContainerStatus launchedContainer : newlyLaunchedContainers) {\n      containerLaunchedOnNode(launchedContainer.getContainerId(), node);\n    }\n\n    // Process completed containers\n    for (ContainerStatus completedContainer : completedContainers) {\n      ContainerId containerId = completedContainer.getContainerId();\n      LOG.debug(\"Container FINISHED: \" + containerId);\n      completedContainer(getRMContainer(containerId),\n          completedContainer, RMContainerEventType.FINISHED);\n    }\n\n    // Assign new containers...\n    // 1. Check for reserved applications\n    // 2. Schedule if there are no reservations\n\n    // If we have have an application that has reserved a resource on this node\n    // already, we try to complete the reservation.\n    RMContainer reservedContainer = node.getReservedContainer();\n    if (reservedContainer != null) {\n      FSSchedulerApp reservedApplication =\n          applications.get(reservedContainer.getApplicationAttemptId());\n\n      // Try to fulfill the reservation\n      LOG.info(\"Trying to fulfill reservation for application \" +\n          reservedApplication.getApplicationId() + \" on node: \" + nm);\n\n      FSLeafQueue queue = queueMgr.getLeafQueue(reservedApplication.getQueueName());\n      queue.assignContainer(node, true);\n    }\n\n    // Otherwise, schedule at queue which is furthest below fair share\n    else {\n      int assignedContainers = 0;\n      while (true) {\n        // At most one task is scheduled each iteration of this loop\n        List<FSLeafQueue> scheds = new CopyOnWriteArrayList<FSLeafQueue>(\n            queueMgr.getLeafQueues());\n        Collections.sort(scheds, new SchedulingAlgorithms.FairShareComparator());\n        boolean assignedContainer = false;\n        for (FSLeafQueue sched : scheds) {\n          Resource assigned = sched.assignContainer(node, false);\n          if (Resources.greaterThan(assigned, Resources.none()) ||\n              node.getReservedContainer() != null) {\n            eventLog.log(\"ASSIGN\", nm.getHostName(), assigned);\n            assignedContainers++;\n            assignedContainer = true;\n            break;\n          }\n        }\n        if (!assignedContainer) { break; }\n        if (!assignMultiple) { break; }\n        if ((assignedContainers >= maxAssign) && (maxAssign > 0)) { break; }\n      }\n    }\n  }"
        }
    },
    {
        "filename": "YARN-7942.json",
        "creation_time": "2018-02-16T19:09:39.000+0000",
        "bug_report": {
            "Title": "Yarn ServiceClient does not delete znode from secure ZooKeeper",
            "Description": "The Yarn ServiceClient is unable to delete a znode from the secure ZooKeeper registry, resulting in a `NoPathPermissionsException`. Despite having the `sasl:rm:cdrwa` permission set on the ZK node, the ResourceManager (RM) fails to remove the node, indicating a permissions issue. The error log shows that the operation fails due to insufficient authorization, specifically a `NoAuthException` for the path `/registry/users/hbase/services/yarn-service/hbase-app-test`. The destroy operation appears to succeed, but the underlying issue is related to access control.",
            "StackTrace": [
                "2018-02-16 15:49:29,691 WARN  client.ServiceClient (ServiceClient.java:actionDestroy(470)) - Error deleting registry entry /users/hbase/services/yarn-service/hbase-app-test",
                "org.apache.hadoop.registry.client.exceptions.NoPathPermissionsException: `/registry/users/hbase/services/yarn-service/hbase-app-test': Not authorized to access path; ACLs: [null ACL]: KeeperErrorCode = NoAuth for /registry/users/hbase/services/yarn-service/hbase-app-test",
                "Caused by: org.apache.zookeeper.KeeperException$NoAuthException: KeeperErrorCode = NoAuth for /registry/users/hbase/services/yarn-service/hbase-app-test"
            ],
            "RootCause": "The root cause of the issue is a lack of proper authorization for the user or service attempting to delete the znode in ZooKeeper. The `NoAuthException` indicates that the authentication credentials do not have the necessary permissions to access or modify the specified path.",
            "StepsToReproduce": [
                "Attempt to delete the znode at `/registry/users/hbase/services/yarn-service/hbase-app-test` using the Yarn ServiceClient.",
                "Ensure that the user has the `sasl:rm:cdrwa` permission set on the ZK node.",
                "Observe the error message in the logs indicating a `NoPathPermissionsException`."
            ],
            "ExpectedBehavior": "The znode at `/registry/users/hbase/services/yarn-service/hbase-app-test` should be successfully deleted without any permission errors.",
            "ObservedBehavior": "The deletion of the znode fails with a `NoPathPermissionsException`, indicating that the user is not authorized to access the specified path.",
            "Suggestions": "Verify the authentication credentials being used to access the ZooKeeper registry. Check the Access Control Lists (ACLs) for the specified path to ensure that the user or service has the necessary permissions. If the permissions are not set correctly, update the ACLs to grant the required access.",
            "problem_location": {
                "files": [
                    "ServiceClient.java",
                    "CuratorService.java",
                    "RegistryOperationsService.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.service.client.ServiceClient",
                    "org.apache.hadoop.registry.client.impl.zk.CuratorService",
                    "org.apache.hadoop.registry.client.impl.zk.RegistryOperationsService"
                ],
                "methods": [
                    "ServiceClient.actionDestroy",
                    "CuratorService.zkDelete",
                    "RegistryOperationsService.delete"
                ]
            },
            "possible_fix": "To resolve the `NoPathPermissionsException`, ensure that the user has the correct permissions set in ZooKeeper. This may involve using the `UserGroupInformation.getCurrentUser()` method to verify the current user's permissions and updating the ACLs in the ZooKeeper configuration to grant the required access."
        },
        "possible_fix_code": {
            "ServiceClient.actionDestroy": "public void actionDestroy(String path) throws Exception {\n    UserGroupInformation currentUser = UserGroupInformation.getCurrentUser();\n    // Log the current user for debugging purposes\n    System.out.println(\"Current user: \" + currentUser);\n    // Check if the user has the necessary permissions\n    if (!hasPermission(currentUser, path)) {\n        throw new NoPathPermissionsException(\"User does not have permission to delete the path: \" + path);\n    }\n    // Proceed with the deletion if permissions are valid\n    try {\n        curatorService.zkDelete(path);\n    } catch (KeeperException.NoAuthException e) {\n        throw new NoPathPermissionsException(\"Not authorized to access path: \" + path, e);\n    }\n}\n\nprivate boolean hasPermission(UserGroupInformation user, String path) {\n    // Implement logic to check if the user has the required permissions for the given path\n    // This is a placeholder for actual permission checking logic\n    return true; // Assume user has permission for now\n}"
        }
    },
    {
        "filename": "YARN-7692.json",
        "creation_time": "2017-12-29T06:00:34.000+0000",
        "bug_report": {
            "Title": "Skip validating priority acls while recovering applications",
            "Description": "The issue arises when attempting to recover applications in a YARN cluster where ACLs are enabled. In a test scenario, a cluster is created without ACLs, and jobs are submitted by a user ('user_a'). After enabling ACLs and creating a priority ACL that excludes 'user_a', subsequent job submissions by 'user_a' are correctly rejected due to lack of permissions. However, during the recovery of previously submitted applications, the Resource Manager crashes due to an AccessControlException, indicating that the user 'hrt_qa' does not have the necessary permissions to submit or update an application. This behavior suggests a flaw in the recovery process that fails to account for user permissions.",
            "StackTrace": [
                "2017-12-27 10:52:30,064 INFO  conf.Configuration (Configuration.java:getConfResourceAsInputStream(2659)) - found resource yarn-site.xml at file:/etc/hadoop/3.0.0.0-636/0/yarn-site.xml",
                "2017-12-27 10:52:30,065 INFO  scheduler.AbstractYarnScheduler (AbstractYarnScheduler.java:setClusterMaxPriority(911)) - Updated the cluste max priority to maxClusterLevelAppPriority = 10",
                "2017-12-27 10:52:30,066 INFO  resourcemanager.ResourceManager (ResourceManager.java:transitionToActive(1177)) - Transitioning to active state",
                "2017-12-27 10:52:30,097 INFO  resourcemanager.ResourceManager (ResourceManager.java:serviceStart(765)) - Recovery started",
                "2017-12-27 10:52:30,102 INFO  recovery.RMStateStore (RMStateStore.java:checkVersion(747)) - Loaded RM state version info 1.5",
                "2017-12-27 10:52:30,375 INFO  security.RMDelegationTokenSecretManager (RMDelegationTokenSecretManager.java:recover(196)) - recovering RMDelegationTokenSecretManager.",
                "2017-12-27 10:52:30,380 INFO  resourcemanager.RMAppManager (RMAppManager.java:recover(561)) - Recovering 51 applications",
                "2017-12-27 10:52:30,432 INFO  resourcemanager.RMAppManager (RMAppManager.java:recover(571)) - Successfully recovered 0 out of 51 applications",
                "2017-12-27 10:52:30,432 ERROR resourcemanager.ResourceManager (ResourceManager.java:serviceStart(776)) - Failed to load/recover state",
                "org.apache.hadoop.yarn.exceptions.YarnException: org.apache.hadoop.security.AccessControlException: User hrt_qa (auth:SIMPLE) does not have permission to submit/update application_1514268754125_0001 for 0",
                "Caused by: org.apache.hadoop.security.AccessControlException: User hrt_qa (auth:SIMPLE) does not have permission to submit/update application_1514268754125_0001 for 0"
            ],
            "RootCause": "The root cause of the issue is an AccessControlException triggered during the recovery of applications in YARN. The method `checkAndGetApplicationPriority` in the `CapacityScheduler` class fails to validate user permissions correctly, leading to the Resource Manager crashing when it attempts to recover applications submitted by users without the necessary permissions.",
            "StepsToReproduce": [
                "1. Create a YARN cluster without any ACLs.",
                "2. Submit jobs using a user, e.g., 'user_a'.",
                "3. Enable ACLs and create a priority ACL entry that excludes 'user_a'.",
                "4. Submit a job with 'user_a' and observe the rejection due to lack of permissions.",
                "5. Restart the Resource Manager and observe the failure during application recovery."
            ],
            "ExpectedBehavior": "The Resource Manager should successfully recover all applications without crashing, even if some applications are submitted by users without the necessary permissions.",
            "ObservedBehavior": "The Resource Manager crashes with an AccessControlException when attempting to recover applications submitted by users who do not have the required permissions.",
            "Suggestions": "Review and update the YARN ACL configurations to ensure that user permissions are correctly set. Additionally, modify the recovery logic in the Resource Manager to handle permission checks more gracefully, preventing crashes.",
            "problem_location": {
                "files": [
                    "CapacityScheduler.java",
                    "RMAppManager.java",
                    "ResourceManager.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler",
                    "org.apache.hadoop.yarn.server.resourcemanager.RMAppManager",
                    "org.apache.hadoop.yarn.server.resourcemanager.ResourceManager"
                ],
                "methods": [
                    "CapacityScheduler.checkAndGetApplicationPriority",
                    "RMAppManager.recoverApplication",
                    "ResourceManager.recover"
                ]
            },
            "possible_fix": "In the `checkAndGetApplicationPriority` method, ensure that the permission checks are robust and handle cases where users lack permissions without causing the Resource Manager to crash. Consider adding exception handling that allows the recovery process to continue even if some applications cannot be recovered due to permission issues."
        },
        "possible_fix_code": {
            "checkAndGetApplicationPriority": "  public Priority checkAndGetApplicationPriority(\n      Priority priorityRequestedByApp, UserGroupInformation user,\n      String queueName, ApplicationId applicationId) throws YarnException {\n    try {\n      readLock.lock();\n      Priority appPriority = priorityRequestedByApp;\n\n      // Verify the scenario where priority is null from submissionContext.\n      if (null == appPriority) {\n        // Verify whether submitted user has any default priority set. If so,\n        // user's default priority will get precedence over queue default.\n        // for updateApplicationPriority call flow, this check is done in\n        // CientRMService itself.\n        appPriority = this.appPriorityACLManager.getDefaultPriority(queueName,\n            user);\n\n        // Get the default priority for the Queue. If Queue is non-existent,\n        // then use default priority. Do it only if user doesn't have any default.\n        if (null == appPriority) {\n          appPriority = this.queueManager.getDefaultPriorityForQueue(queueName);\n        }\n\n        LOG.info(\n            \"Application '\" + applicationId + \"' is submitted without priority \"\n                + \"hence considering default queue/cluster priority: \"\n                + appPriority.getPriority());\n      }\n\n      // Verify whether submitted priority is lesser than max priority\n      // in the cluster. If it is out of found, defining a max cap.\n      if (appPriority.getPriority() > getMaxClusterLevelAppPriority()\n          .getPriority()) {\n        appPriority = Priority\n            .newInstance(getMaxClusterLevelAppPriority().getPriority());\n      }\n\n      // Lets check for ACLs here.\n      if (!appPriorityACLManager.checkAccess(user, queueName, appPriority)) {\n        LOG.warn(\"User \" + user + \" does not have permission to submit/update \"\n            + applicationId + \" for \" + appPriority + \". Skipping this application.\");\n        return null; // Skip this application instead of throwing an exception\n      }\n\n      LOG.info(\"Priority '\" + appPriority.getPriority() + \"' is acceptable in queue : \" + queueName + \" for application: \"\n          + applicationId);\n\n      return appPriority;\n    } finally {\n      readLock.unlock();\n    }\n  }"
        }
    },
    {
        "filename": "YARN-3917.json",
        "creation_time": "2015-07-11T00:41:28.000+0000",
        "bug_report": {
            "Title": "getResourceCalculatorPlugin for the default should intercept all exceptions",
            "Description": "The issue arises when the default resource calculator plugin is instantiated without a user-defined configuration. An `UnsupportedOperationException` is thrown due to the inability to determine the operating system, which disrupts the initialization process of the YARN NodeManager. This exception occurs in the `SysInfo.newInstance()` method, which is responsible for OS detection. The failure to handle this exception properly leads to the service not starting as expected.",
            "StackTrace": [
                "2015-07-10 08:16:18,445 INFO org.apache.hadoop.service.AbstractService: Service containers-monitor failed in state INITED; cause: java.lang.UnsupportedOperationException: Could not determine OS",
                "java.lang.UnsupportedOperationException: Could not determine OS",
                "at org.apache.hadoop.util.SysInfo.newInstance(SysInfo.java:43)",
                "at org.apache.hadoop.yarn.util.ResourceCalculatorPlugin.<init>(ResourceCalculatorPlugin.java:37)",
                "at org.apache.hadoop.yarn.util.ResourceCalculatorPlugin.getResourceCalculatorPlugin(ResourceCalculatorPlugin.java:160)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl.serviceInit(ContainersMonitorImpl.java:108)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)",
                "at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:107)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.serviceInit(ContainerManagerImpl.java:249)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)",
                "at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:107)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.serviceInit(NodeManager.java:312)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.initAndStartNodeManager(NodeManager.java:547)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.main(NodeManager.java:595)"
            ],
            "RootCause": "The root cause of the issue is the `UnsupportedOperationException` thrown by the `SysInfo.newInstance()` method, which fails to determine the operating system. This failure occurs during the initialization of the `ResourceCalculatorPlugin`, which is critical for the YARN NodeManager's operation.",
            "StepsToReproduce": [
                "1. Ensure that no specific resource calculator plugin is configured by the user.",
                "2. Start the YARN NodeManager service.",
                "3. Observe the logs for the `UnsupportedOperationException` related to OS detection."
            ],
            "ExpectedBehavior": "The YARN NodeManager should initialize successfully without throwing exceptions, allowing it to manage container resources effectively.",
            "ObservedBehavior": "The YARN NodeManager fails to initialize, throwing an `UnsupportedOperationException` due to the inability to determine the operating system.",
            "Suggestions": "To mitigate this issue, ensure that the environment is correctly configured to allow OS detection. This may involve checking system properties or ensuring that the necessary libraries for OS detection are included. Additionally, consider implementing exception handling in the `ResourceCalculatorPlugin` to gracefully handle such failures.",
            "problem_location": {
                "files": [
                    "SysInfo.java",
                    "ResourceCalculatorPlugin.java",
                    "NodeManager.java"
                ],
                "classes": [
                    "org.apache.hadoop.util.SysInfo",
                    "org.apache.hadoop.yarn.util.ResourceCalculatorPlugin",
                    "org.apache.hadoop.yarn.server.nodemanager.NodeManager"
                ],
                "methods": [
                    "SysInfo.newInstance",
                    "ResourceCalculatorPlugin.init",
                    "NodeManager.serviceInit"
                ]
            },
            "possible_fix": "Implement a fallback mechanism in the `ResourceCalculatorPlugin` to handle the `UnsupportedOperationException` gracefully. For example, if the OS cannot be determined, default to a safe configuration that allows the NodeManager to start without crashing."
        },
        "possible_fix_code": {
            "ResourceCalculatorPlugin.getResourceCalculatorPlugin": "  public static ResourceCalculatorPlugin getResourceCalculatorPlugin(\n      Class<? extends ResourceCalculatorPlugin> clazz, Configuration conf) {\n\n    if (clazz != null) {\n      return ReflectionUtils.newInstance(clazz, conf);\n    }\n    try {\n      return new ResourceCalculatorPlugin();\n    } catch (SecurityException e) {\n      return null;\n    } catch (UnsupportedOperationException e) {\n      // Log the exception and return a default implementation or handle gracefully\n      LOG.warn(\"UnsupportedOperationException caught: \" + e.getMessage());\n      // Return a default or fallback ResourceCalculatorPlugin\n      return new DefaultResourceCalculatorPlugin(); // Assuming DefaultResourceCalculatorPlugin is a safe fallback\n    }\n  }"
        }
    },
    {
        "filename": "YARN-3537.json",
        "creation_time": "2015-04-23T11:34:23.000+0000",
        "bug_report": {
            "Title": "NPE when NodeManager.serviceInit fails and stopRecoveryStore invoked",
            "Description": "The NodeManager service fails to initialize properly, leading to a NullPointerException (NPE) when the stopRecoveryStore method is invoked. This issue arises when the nmStore is not initialized due to recovery being disabled in the configuration. The stack trace indicates that the NPE occurs at NodeManager.java:181 during the execution of stopRecoveryStore, which is called from serviceStop.",
            "StackTrace": [
                "2015-04-23 19:30:34,961 INFO  [main] service.AbstractService (AbstractService.java:noteFailure(272)) - Service NodeManager failed in state STOPPED; cause: java.lang.NullPointerException",
                "java.lang.NullPointerException",
                "\tat org.apache.hadoop.yarn.server.nodemanager.NodeManager.stopRecoveryStore(NodeManager.java:181)",
                "\tat org.apache.hadoop.yarn.server.nodemanager.NodeManager.serviceStop(NodeManager.java:326)",
                "\tat org.apache.hadoop.service.AbstractService.stop(AbstractService.java:221)",
                "\tat org.apache.hadoop.yarn.server.nodemanager.TestNodeManagerShutdown.tearDown(TestNodeManagerShutdown.java:106)"
            ],
            "RootCause": "The root cause of the NullPointerException is that the nmStore is not initialized when recovery is disabled in the configuration. The initAndStartRecoveryStore method sets nmStore to null if recovery is not enabled, which leads to the NPE when stopRecoveryStore is called.",
            "StepsToReproduce": [
                "1. Ensure that the NodeManager is configured with recovery disabled (YarnConfiguration.NM_RECOVERY_ENABLED set to false).",
                "2. Start the NodeManager service.",
                "3. Trigger a failure in the service initialization (e.g., by simulating an error during serviceInit).",
                "4. Observe the logs for the NullPointerException when stopRecoveryStore is invoked."
            ],
            "ExpectedBehavior": "The NodeManager should initialize successfully and handle the stopRecoveryStore method without throwing a NullPointerException, regardless of the recovery configuration.",
            "ObservedBehavior": "A NullPointerException is thrown when the stopRecoveryStore method is called, indicating that nmStore is null due to recovery being disabled.",
            "Suggestions": "To resolve this issue, ensure that recovery is enabled in the configuration by setting YarnConfiguration.NM_RECOVERY_ENABLED to true. Alternatively, add a null check in the stopRecoveryStore method to handle cases where nmStore is not initialized.",
            "problem_location": {
                "files": [
                    "NodeManager.java",
                    "YarnConfiguration.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.nodemanager.NodeManager",
                    "org.apache.hadoop.yarn.conf.YarnConfiguration"
                ],
                "methods": [
                    "NodeManager.stopRecoveryStore",
                    "NodeManager.serviceInit",
                    "NodeManager.initAndStartRecoveryStore"
                ]
            },
            "possible_fix": "In the NodeManager class, modify the stopRecoveryStore method to include a null check for nmStore before calling nmStore.stop(). Additionally, ensure that the YarnConfiguration.NM_RECOVERY_ENABLED is set to true in the configuration to prevent nmStore from being null."
        },
        "possible_fix_code": {
            "NodeManager.stopRecoveryStore": "  private void stopRecoveryStore() throws IOException {\n    if (nmStore != null) {\n      nmStore.stop();\n    } else {\n      LOG.warn(\"nmStore is null, cannot stop recovery store.\");\n    }\n    if (null != context) {\n      if (context.getDecommissioned() && nmStore.canRecover()) {\n        LOG.info(\"Removing state store due to decommission\");\n        Configuration conf = getConfig();\n        Path recoveryRoot =\n            new Path(conf.get(YarnConfiguration.NM_RECOVERY_DIR));\n        LOG.info(\"Removing state store at \" + recoveryRoot\n            + \" due to decommission\");\n        FileSystem recoveryFs = FileSystem.getLocal(conf);\n        if (!recoveryFs.delete(recoveryRoot, true)) {\n          LOG.warn(\"Unable to delete \" + recoveryRoot);\n        }\n      }\n    }\n  }"
        }
    },
    {
        "filename": "YARN-7962.json",
        "creation_time": "2018-02-22T22:32:20.000+0000",
        "bug_report": {
            "Title": "Race Condition When Stopping DelegationTokenRenewer causes RM crash during failover",
            "Description": "The issue arises when the `DelegationTokenRenewer` attempts to process a renewal event after the `ThreadPoolExecutor` has been shut down. The `serviceStop` method does not properly update the `isServiceStarted` flag to false before shutting down the executor, leading to a `RejectedExecutionException` when new tasks are submitted. This occurs during the failover process of the ResourceManager, causing a crash.",
            "StackTrace": [
                "2018-02-21 11:18:16,253  FATAL org.apache.hadoop.yarn.event.AsyncDispatcher: Error in dispatcher thread",
                "java.util.concurrent.RejectedExecutionException: Task org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$DelegationTokenRenewerRunnable@39bddaf2 rejected from java.util.concurrent.ThreadPoolExecutor@5f71637b[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 15487]",
                "at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2048)",
                "at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:821)",
                "at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1372)",
                "at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.processDelegationTokenRenewerEvent(DelegationTokenRenewer.java:196)",
                "at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.applicationFinished(DelegationTokenRenewer.java:734)",
                "at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.finishApplication(RMAppManager.java:199)",
                "at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.handle(RMAppManager.java:424)",
                "at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.handle(RMAppManager.java:65)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:177)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:109)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "RootCause": "The root cause of the issue is that the `serviceStop` method does not set the `isServiceStarted` flag to false before shutting down the `ThreadPoolExecutor`. This leads to a race condition where tasks are still being submitted to a terminated executor, resulting in a `RejectedExecutionException`.",
            "StepsToReproduce": [
                "Start the ResourceManager with the DelegationTokenRenewer enabled.",
                "Trigger a failover event while the system is processing delegation token renewer events.",
                "Observe the logs for a `RejectedExecutionException` indicating that tasks are being rejected."
            ],
            "ExpectedBehavior": "The ResourceManager should gracefully handle the stopping of the DelegationTokenRenewer without rejecting tasks or crashing.",
            "ObservedBehavior": "The ResourceManager crashes with a `RejectedExecutionException` when attempting to process a renewal event after the `ThreadPoolExecutor` has been shut down.",
            "Suggestions": "To resolve this issue, ensure that the `serviceStop` method acquires the `serviceStateLock` and sets `isServiceStarted` to false before shutting down the `renewerService`. Additionally, consider implementing logging around the executor lifecycle to monitor its state.",
            "problem_location": {
                "files": [
                    "DelegationTokenRenewer.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer"
                ],
                "methods": [
                    "processDelegationTokenRenewerEvent",
                    "serviceStop"
                ]
            },
            "possible_fix": "In the `serviceStop` method, add the following lines before shutting down the `renewerService`:\n\n```java\nserviceStateLock.writeLock().lock();\ntry {\n    isServiceStarted = false;\n} finally {\n    serviceStateLock.writeLock().unlock();\n}\n```"
        },
        "possible_fix_code": {
            "serviceStop": "public void serviceStop() {\n    serviceStateLock.writeLock().lock();\n    try {\n        isServiceStarted = false;\n    } finally {\n        serviceStateLock.writeLock().unlock();\n    }\n    if (renewerService != null) {\n        renewerService.shutdown();\n    }\n}"
        }
    },
    {
        "filename": "YARN-8357.json",
        "creation_time": "2018-05-24T16:46:57.000+0000",
        "bug_report": {
            "Title": "Yarn Service: NPE when service is saved first and then started.",
            "Description": "A NullPointerException (NPE) occurs in the Yarn service client when attempting to start a service that has been saved but not properly initialized. The issue arises from the `ServiceClient.actionStart` method, specifically when it calls `getStatus` and other related methods that may return null values. The stack trace indicates that the NPE is thrown at line 974 of `ServiceClient.java`, where the service state is accessed without ensuring that the service object is not null.",
            "StackTrace": [
                "2018-05-24 04:39:22,911 INFO client.ServiceClient (ServiceClient.java:getStatus(1203)) - Service test1 does not have an application ID",
                "2018-05-24 04:39:22,911 ERROR webapp.ApiServer (ApiServer.java:updateService(480)) - Error while performing operation for app: test1",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.service.client.ServiceClient.actionStart(ServiceClient.java:974)",
                "at org.apache.hadoop.yarn.service.webapp.ApiServer$7.run(ApiServer.java:650)",
                "at org.apache.hadoop.yarn.service.webapp.ApiServer$7.run(ApiServer.java:644)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1687)",
                "at org.apache.hadoop.yarn.service.webapp.ApiServer.startService(ApiServer.java:644)",
                "at org.apache.hadoop.yarn.service.webapp.ApiServer.updateService(ApiServer.java:449)"
            ],
            "RootCause": "The root cause of the NullPointerException is likely due to uninitialized objects or methods returning null values in the `getStatus`, `checkAppExistOnHdfs`, `loadService`, and `createAMProxy` methods. Specifically, the `getStatus` method may return a service object that is null if the application ID is not found, leading to a failure when accessing its state.",
            "StepsToReproduce": [
                "1. Save a service using the Yarn service client.",
                "2. Attempt to start the saved service without ensuring that it has a valid application ID.",
                "3. Observe the NullPointerException in the logs."
            ],
            "ExpectedBehavior": "The service should start successfully without throwing a NullPointerException, provided that it has been saved correctly and has a valid application ID.",
            "ObservedBehavior": "A NullPointerException is thrown when attempting to start the service, indicating that the service object is null or its state cannot be accessed.",
            "Suggestions": "Ensure that all objects are properly initialized before they are used. Add null checks in the `getStatus`, `checkAppExistOnHdfs`, `loadService`, and `createAMProxy` methods to prevent the NPE.",
            "problem_location": {
                "files": [
                    "ServiceClient.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.service.client.ServiceClient"
                ],
                "methods": [
                    "ServiceClient.actionStart",
                    "ServiceClient.getStatus",
                    "ServiceClient.checkAppExistOnHdfs",
                    "ServiceApiUtil.loadService",
                    "ServiceClient.createAMProxy"
                ]
            },
            "possible_fix": "Add null checks in the `getStatus` method to handle cases where the application ID is not found. For example, before accessing the state of the service, check if the service object is null and handle it appropriately."
        },
        "possible_fix_code": {
            "ServiceClient.actionStart": "  public int actionStart(String serviceName) throws YarnException, IOException {\n    ServiceApiUtil.validateNameFormat(serviceName, getConfig());\n    Service liveService = getStatus(serviceName);\n    if (liveService == null ||\n        !liveService.getState().equals(ServiceState.UPGRADING)) {\n      Path appDir = checkAppExistOnHdfs(serviceName);\n      Service service = ServiceApiUtil.loadService(fs, serviceName);\n      ServiceApiUtil.validateAndResolveService(service, fs, getConfig());\n      // see if it is actually running and bail out;\n      verifyNoLiveAppInRM(serviceName, \"start\");\n      ApplicationId appId = submitApp(service);\n      service.setId(appId.toString());\n      // write app definition on to hdfs\n      Path appJson = ServiceApiUtil.writeAppDefinition(fs, appDir, service);\n      LOG.info(\"Persisted service \" + service.getName() + \" at \" + appJson);\n      return 0;\n    } else {\n      LOG.info(\"Finalize service {} upgrade\", serviceName);\n      ApplicationReport appReport =\n          yarnClient.getApplicationReport(getAppId(serviceName));\n      if (appReport == null || StringUtils.isEmpty(appReport.getHost())) {\n        throw new YarnException(serviceName + \" AM hostname is empty\");\n      }\n      ClientAMProtocol proxy = createAMProxy(serviceName, appReport);\n\n      RestartServiceRequestProto.Builder requestBuilder =\n          RestartServiceRequestProto.newBuilder();\n      proxy.restart(requestBuilder.build());\n      return 0;\n    }\n  }"
        }
    },
    {
        "filename": "YARN-6534.json",
        "creation_time": "2017-04-26T21:43:52.000+0000",
        "bug_report": {
            "Title": "ResourceManager failed due to TimelineClient trying to initialize SSLFactory even when HTTPS is not enabled",
            "Description": "In a non-secured cluster, the ResourceManager fails consistently because the TimelineServiceV1Publisher attempts to initialize the TimelineClient with SSLFactory without verifying if HTTPS is enabled. This leads to a FileNotFoundException for the KeyStore file '/etc/security/clientKeys/all.jks', which is required for SSL configuration. The stack trace indicates that the failure occurs during the initialization of various Hadoop services, particularly in the TimelineClientImpl and related classes.",
            "StackTrace": [
                "2017-04-26 21:09:10,683 FATAL resourcemanager.ResourceManager (ResourceManager.java:main(1457)) - Error starting ResourceManager",
                "org.apache.hadoop.service.ServiceStateException: java.io.FileNotFoundException: /etc/security/clientKeys/all.jks (No such file or directory)",
                "at org.apache.hadoop.service.ServiceStateException.convert(ServiceStateException.java:59)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:172)",
                "at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:107)",
                "at org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl.serviceInit(TimelineClientImpl.java:131)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)",
                "at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:107)",
                "at org.apache.hadoop.yarn.server.resourcemanager.metrics.AbstractSystemMetricsPublisher.serviceInit(AbstractSystemMetricsPublisher.java:59)",
                "at org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher.serviceInit(TimelineServiceV1Publisher.java:67)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)",
                "at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:107)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceInit(ResourceManager.java:344)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1453)",
                "Caused by: java.io.FileNotFoundException: /etc/security/clientKeys/all.jks (No such file or directory)",
                "at java.io.FileInputStream.open0(Native Method)",
                "at java.io.FileInputStream.open(FileInputStream.java:195)",
                "at java.io.FileInputStream.<init>(FileInputStream.java:138)",
                "at org.apache.hadoop.security.ssl.ReloadingX509TrustManager.loadTrustManager(ReloadingX509TrustManager.java:168)",
                "at org.apache.hadoop.security.ssl.ReloadingX509TrustManager.<init>(ReloadingX509TrustManager.java:86)",
                "at org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory.init(FileBasedKeyStoresFactory.java:219)",
                "at org.apache.hadoop.security.ssl.SSLFactory.init(SSLFactory.java:179)",
                "at org.apache.hadoop.yarn.client.api.impl.TimelineConnector.getSSLFactory(TimelineConnector.java:176)",
                "at org.apache.hadoop.yarn.client.api.impl.TimelineConnector.serviceInit(TimelineConnector.java:106)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)"
            ],
            "RootCause": "The ResourceManager fails to start because it attempts to initialize SSL components without verifying if SSL is required, leading to a missing KeyStore file error.",
            "StepsToReproduce": [
                "Set up a non-secured Hadoop cluster.",
                "Attempt to start the ResourceManager.",
                "Observe the logs for the FileNotFoundException related to '/etc/security/clientKeys/all.jks'."
            ],
            "ExpectedBehavior": "The ResourceManager should start successfully without attempting to initialize SSL components when HTTPS is not enabled.",
            "ObservedBehavior": "The ResourceManager fails to start due to a FileNotFoundException for the missing KeyStore file.",
            "Suggestions": "1. Check the Hadoop configuration for SSL settings and ensure that SSL is only initialized when HTTPS is enabled. 2. If SSL is required, create the KeyStore file at the specified location or update the configuration to point to the correct KeyStore file. 3. Ensure that the Hadoop process has the necessary permissions to access the KeyStore file.",
            "problem_location": {
                "files": [
                    "ResourceManager.java",
                    "TimelineClientImpl.java",
                    "FileBasedKeyStoresFactory.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.ResourceManager",
                    "org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl",
                    "org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory"
                ],
                "methods": [
                    "ResourceManager.serviceInit",
                    "TimelineClientImpl.serviceInit",
                    "FileBasedKeyStoresFactory.init"
                ]
            },
            "possible_fix": "Update the Hadoop configuration to check if SSL is required before initializing SSL components. If SSL is needed, ensure the KeyStore file '/etc/security/clientKeys/all.jks' exists or point to the correct KeyStore location."
        },
        "possible_fix_code": {
            "FileBasedKeyStoresFactory.init": "  public void init(SSLFactory.Mode mode)\n    throws IOException, GeneralSecurityException {\n\n    boolean requireClientCert =\n      conf.getBoolean(SSLFactory.SSL_REQUIRE_CLIENT_CERT_KEY,\n          SSLFactory.SSL_REQUIRE_CLIENT_CERT_DEFAULT);\n\n    // certificate store\n    String keystoreType =\n      conf.get(resolvePropertyName(mode, SSL_KEYSTORE_TYPE_TPL_KEY),\n               DEFAULT_KEYSTORE_TYPE);\n    KeyStore keystore = KeyStore.getInstance(keystoreType);\n    String keystoreKeyPassword = null;\n\n    // Check if SSL is required before initializing SSL components\n    if (requireClientCert || mode == SSLFactory.Mode.SERVER) {\n      String locationProperty =\n        resolvePropertyName(mode, SSL_KEYSTORE_LOCATION_TPL_KEY);\n      String keystoreLocation = conf.get(locationProperty, \"\");\n      if (keystoreLocation.isEmpty()) {\n        throw new GeneralSecurityException(\"The property '\" + locationProperty +\n          \"' has not been set in the ssl configuration file.\");\n      }\n\n      // Ensure the KeyStore file exists before attempting to load it\n      File keystoreFile = new File(keystoreLocation);\n      if (!keystoreFile.exists()) {\n        throw new FileNotFoundException(\"KeyStore file not found: \" + keystoreLocation);\n      }\n\n      String passwordProperty =\n        resolvePropertyName(mode, SSL_KEYSTORE_PASSWORD_TPL_KEY);\n      String keystorePassword = getPassword(conf, passwordProperty, \"\");\n      if (keystorePassword.isEmpty()) {\n        throw new GeneralSecurityException(\"The property '\" + passwordProperty +\n          \"' has not been set in the ssl configuration file.\");\n      }\n      String keyPasswordProperty =\n        resolvePropertyName(mode, SSL_KEYSTORE_KEYPASSWORD_TPL_KEY);\n      keystoreKeyPassword = getPassword(\n          conf, keyPasswordProperty, keystorePassword);\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(mode.toString() + \" KeyStore: \" + keystoreLocation);\n      }\n\n      InputStream is = new FileInputStream(keystoreLocation);\n      try {\n        keystore.load(is, keystorePassword.toCharArray());\n      } finally {\n        is.close();\n      }\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(mode.toString() + \" Loaded KeyStore: \" + keystoreLocation);\n      }\n    } else {\n      keystore.load(null, null);\n    }\n    KeyManagerFactory keyMgrFactory = KeyManagerFactory\n        .getInstance(SSLFactory.SSLCERTIFICATE);\n      \n    keyMgrFactory.init(keystore, (keystoreKeyPassword != null) ?\n                                 keystoreKeyPassword.toCharArray() : null);\n    keyManagers = keyMgrFactory.getKeyManagers();\n\n    //trust store\n    String truststoreType =\n      conf.get(resolvePropertyName(mode, SSL_TRUSTSTORE_TYPE_TPL_KEY),\n               DEFAULT_KEYSTORE_TYPE);\n\n    String locationProperty =\n      resolvePropertyName(mode, SSL_TRUSTSTORE_LOCATION_TPL_KEY);\n    String truststoreLocation = conf.get(locationProperty, \"\");\n    if (!truststoreLocation.isEmpty()) {\n      String passwordProperty = resolvePropertyName(mode,\n          SSL_TRUSTSTORE_PASSWORD_TPL_KEY);\n      String truststorePassword = getPassword(conf, passwordProperty, \"\");\n      if (truststorePassword.isEmpty()) {\n        truststorePassword = null;\n      }\n      long truststoreReloadInterval =\n          conf.getLong(\n              resolvePropertyName(mode, SSL_TRUSTSTORE_RELOAD_INTERVAL_TPL_KEY),\n              DEFAULT_SSL_TRUSTSTORE_RELOAD_INTERVAL);\n\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(mode.toString() + \" TrustStore: \" + truststoreLocation);\n      }\n\n      trustManager = new ReloadingX509TrustManager(truststoreType,\n          truststoreLocation,\n          truststorePassword,\n          truststoreReloadInterval);\n      trustManager.init();\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(mode.toString() + \" Loaded TrustStore: \" + truststoreLocation);\n      }\n      trustManagers = new TrustManager[]{trustManager};\n    } else {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"The property '\" + locationProperty + \"' has not been set, \" +\n            \"no TrustStore will be loaded\");\n      }\n      trustManagers = null;\n    }\n  }"
        }
    },
    {
        "filename": "YARN-4227.json",
        "creation_time": "2015-10-06T04:59:10.000+0000",
        "bug_report": {
            "Title": "Ignore expired containers from removed nodes in FairScheduler",
            "Description": "The issue arises when a node is removed from the FairScheduler before the expired container event is processed. This leads to a NullPointerException in the FairScheduler's completedContainer method, causing the ResourceManager (RM) to exit unexpectedly. The stack trace indicates that the exception occurs when the method attempts to access properties of a null RMContainer, which should have been handled properly to avoid such a crash.",
            "StackTrace": [
                "2015-10-04 21:14:01,063 INFO org.apache.hadoop.yarn.util.AbstractLivelinessMonitor: Expired:container_1436927988321_1307950_01_000012 Timed out after 600 secs",
                "2015-10-04 21:14:01,063 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1436927988321_1307950_01_000012 Container Transitioned from ACQUIRED to EXPIRED",
                "2015-10-04 21:14:01,063 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSSchedulerApp: Completed container: container_1436927988321_1307950_01_000012 in state: EXPIRED event:EXPIRE",
                "2015-10-04 21:14:01,063 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=system_op\tOPERATION=AM Released Container\tTARGET=SchedulerApp\tRESULT=SUCCESS\tAPPID=application_1436927988321_1307950\tCONTAINERID=container_1436927988321_1307950_01_000012",
                "2015-10-04 21:14:01,063 FATAL org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Error in handling event type CONTAINER_EXPIRED to the scheduler",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.completedContainer(FairScheduler.java:849)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:1273)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:122)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:585)",
                "at java.lang.Thread.run(Thread.java:745)",
                "2015-10-04 21:14:01,063 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Exiting, bbye.."
            ],
            "RootCause": "The root cause of the issue is a NullPointerException in the completedContainer method of the FairScheduler class. This occurs when the method attempts to access the rmContainer object, which is null due to the node being removed before the expired container event is processed.",
            "StepsToReproduce": [
                "1. Submit a job to the FairScheduler.",
                "2. Allow a container to expire by not processing it within the timeout period.",
                "3. Remove the node that the expired container was allocated to.",
                "4. Observe the ResourceManager exiting with a NullPointerException."
            ],
            "ExpectedBehavior": "The ResourceManager should handle the expired container event gracefully without crashing, even if the node has been removed.",
            "ObservedBehavior": "The ResourceManager crashes with a NullPointerException when trying to process an expired container event for a node that has already been removed.",
            "Suggestions": "Implement a null check for the rmContainer in the completedContainer method to prevent the NullPointerException. Additionally, ensure that the scheduler can handle the removal of nodes while processing container events.",
            "problem_location": {
                "files": [
                    "FairScheduler.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler"
                ],
                "methods": [
                    "completedContainer"
                ]
            },
            "possible_fix": "In the completedContainer method, add a check to ensure rmContainer is not null before proceeding with its properties. For example:\n\nprotected synchronized void completedContainer(RMContainer rmContainer,\n    ContainerStatus containerStatus, RMContainerEventType event) {\n    if (rmContainer == null) {\n        LOG.info(\"Container \" + containerStatus.getContainerId() + \" completed with event \" + event);\n        return;\n    }\n    // existing logic follows...\n}"
        },
        "possible_fix_code": {
            "completedContainer": "  protected synchronized void completedContainer(RMContainer rmContainer,\n      ContainerStatus containerStatus, RMContainerEventType event) {\n    if (rmContainer == null) {\n      LOG.info(\"Container \" + containerStatus.getContainerId()\n          + \" completed with event \" + event);\n      return;\n    }\n\n    Container container = rmContainer.getContainer();\n\n    // Get the application for the finished container\n    FSAppAttempt application =\n        getCurrentAttemptForContainer(container.getId());\n    ApplicationId appId =\n        container.getId().getApplicationAttemptId().getApplicationId();\n    if (application == null) {\n      LOG.info(\"Container \" + container + \" of\"\n          + \" finished application \" + appId +\n          \" completed with event \" + event);\n      return;\n    }\n\n    // Get the node on which the container was allocated\n    FSSchedulerNode node = getFSSchedulerNode(container.getNodeId());\n\n    if (rmContainer.getState() == RMContainerState.RESERVED) {\n      application.unreserve(rmContainer.getReservedPriority(), node);\n    } else {\n      application.containerCompleted(rmContainer, containerStatus, event);\n      node.releaseContainer(container);\n      updateRootQueueMetrics();\n    }\n\n    LOG.info(\"Application attempt \" + application.getApplicationAttemptId()\n        + \" released container \" + container.getId() + \" on node: \" + node\n        + \" with event: \" + event);\n  }"
        }
    },
    {
        "filename": "YARN-2649.json",
        "creation_time": "2014-10-06T22:57:46.000+0000",
        "bug_report": {
            "Title": "Flaky test TestAMRMRPCNodeUpdates",
            "Description": "The test 'TestAMRMRPCNodeUpdates' intermittently fails due to an assertion error indicating that the application attempt state is not transitioning as expected. Specifically, the test expects the application attempt to be in the 'ALLOCATED' state, but it remains in the 'SCHEDULED' state. This issue arises when the SchedulerEventType.NODE_UPDATE is processed before the RMAppAttemptEvent.ATTEMPT_ADDED event, which can occur because the test only waits for the RMAppState.ACCEPTED state before the NodeManager sends a heartbeat. The failure can be reproduced using a custom AsyncDispatcher with CountDownLatch, which may lead to timing issues in event processing.",
            "StackTrace": [
                "junit.framework.AssertionFailedError: AppAttempt state is not correct (timedout) expected:<ALLOCATED> but was:<SCHEDULED>",
                "at junit.framework.Assert.fail(Assert.java:50)",
                "at junit.framework.Assert.failNotEquals(Assert.java:287)",
                "at junit.framework.Assert.assertEquals(Assert.java:67)",
                "at org.apache.hadoop.yarn.server.resourcemanager.MockAM.waitForState(MockAM.java:82)",
                "at org.apache.hadoop.yarn.server.resourcemanager.MockRM.sendAMLaunched(MockRM.java:382)",
                "at org.apache.hadoop.yarn.server.resourcemanager.applicationsmanager.TestAMRMRPCNodeUpdates.testAMRMUnusableNodes(TestAMRMRPCNodeUpdates.java:125)"
            ],
            "RootCause": "The root cause of the issue is a timing problem in the event processing order within the YARN ResourceManager. The NODE_UPDATE event is being processed before the ATTEMPT_ADDED event, which prevents the application attempt from transitioning to the expected 'ALLOCATED' state.",
            "StepsToReproduce": [
                "Run the test 'TestAMRMRPCNodeUpdates'.",
                "Ensure that the test environment is set up to use a custom AsyncDispatcher with CountDownLatch.",
                "Observe the timing of event processing to confirm that NODE_UPDATE is processed before ATTEMPT_ADDED."
            ],
            "ExpectedBehavior": "The application attempt should transition to the 'ALLOCATED' state after being in the 'SUBMITTED' state, allowing the test to pass without assertion failures.",
            "ObservedBehavior": "The application attempt remains in the 'SCHEDULED' state instead of transitioning to 'ALLOCATED', leading to a test failure.",
            "Suggestions": "Review the event handling logic in the YARN ResourceManager to ensure that events are processed in the correct order. Consider adding synchronization mechanisms to prevent NODE_UPDATE from being processed before ATTEMPT_ADDED.",
            "problem_location": {
                "files": [
                    "MockAM.java",
                    "MockRM.java",
                    "TestAMRMRPCNodeUpdates.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.MockAM",
                    "org.apache.hadoop.yarn.server.resourcemanager.MockRM",
                    "org.apache.hadoop.yarn.server.resourcemanager.applicationsmanager.TestAMRMRPCNodeUpdates"
                ],
                "methods": [
                    "MockAM.waitForState",
                    "MockRM.sendAMLaunched",
                    "TestAMRMRPCNodeUpdates.testAMRMUnusableNodes"
                ]
            },
            "possible_fix": "Investigate the event dispatching mechanism in the YARN ResourceManager. Ensure that the state transition logic correctly handles the order of events. If necessary, implement a fix to enforce the correct sequence of event processing."
        },
        "possible_fix_code": {
            "MockAM.waitForState": "public void waitForState(RMAppState expectedState) throws InterruptedException {\n    long timeout = System.currentTimeMillis() + 10000; // 10 seconds timeout\n    while (System.currentTimeMillis() < timeout) {\n        if (this.state == expectedState) {\n            return;\n        }\n        // Add a small sleep to prevent busy waiting\n        Thread.sleep(100);\n    }\n    // If we reach here, we timed out\n    throw new AssertionFailedError(\"AppAttempt state is not correct (timedout) expected:<\" + expectedState + \"> but was:<\" + this.state + \">\");\n}"
        }
    },
    {
        "filename": "YARN-4288.json",
        "creation_time": "2015-10-22T12:30:16.000+0000",
        "bug_report": {
            "Title": "NodeManager restart should keep retrying to register to RM while connection exception happens during RM failed over.",
            "Description": "When the NodeManager (NM) is restarted, the NodeStatusUpdaterImpl attempts to register with the ResourceManager (RM) via RPC. However, if the RM is also restarting, this can lead to connection issues, specifically 'Connection reset by peer'. The stack trace indicates that the NM fails to register due to an IOException caused by the RM being unavailable. The error occurs in the `rebootNodeStatusUpdaterAndRegisterWithRM` method, which is responsible for handling the registration process. The current implementation does not adequately handle retries during connection failures, leading to a complete failure of the NM restart process.",
            "StackTrace": [
                "2015-08-17 14:35:59,434 ERROR nodemanager.NodeStatusUpdaterImpl (NodeStatusUpdaterImpl.java:rebootNodeStatusUpdaterAndRegisterWithRM(222)) - Unexpected error rebooting NodeStatusUpdater",
                "java.io.IOException: Failed on local exception: java.io.IOException: Connection reset by peer; Host Details : local host is: \"172.27.62.28\"; destination host is: \"172.27.62.57\":8025;",
                "at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:772)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1473)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1400)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)",
                "at com.sun.proxy.$Proxy36.registerNodeManager(Unknown Source)",
                "at org.apache.hadoop.yarn.server.api.impl.pb.client.ResourceTrackerPBClientImpl.registerNodeManager(ResourceTrackerPBClientImpl.java:68)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)",
                "at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)",
                "at com.sun.proxy.$Proxy37.registerNodeManager(Unknown Source)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.registerWithRM(NodeStatusUpdaterImpl.java:257)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.rebootNodeStatusUpdaterAndRegisterWithRM(NodeStatusUpdaterImpl.java:215)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager$2.run(NodeManager.java:304)"
            ],
            "RootCause": "The root cause of the issue is that the NodeStatusUpdaterImpl does not retry the registration with the ResourceManager when a connection exception occurs, specifically during RM failover. The connection reset by peer indicates that the RM was not available to accept the registration request.",
            "StepsToReproduce": [
                "1. Restart the NodeManager while the ResourceManager is also restarting.",
                "2. Observe the logs for connection errors during the registration attempt.",
                "3. Note that the NodeManager fails to register and does not retry the connection."
            ],
            "ExpectedBehavior": "The NodeManager should continuously attempt to register with the ResourceManager until a successful connection is established, even if the RM is temporarily unavailable.",
            "ObservedBehavior": "The NodeManager fails to register with the ResourceManager and does not retry after encountering a connection reset exception, leading to a failure in the NM restart process.",
            "Suggestions": "Implement a retry mechanism in the `rebootNodeStatusUpdaterAndRegisterWithRM` method to handle connection exceptions gracefully. This could involve using exponential backoff for retries to avoid overwhelming the RM during its restart.",
            "problem_location": {
                "files": [
                    "NodeStatusUpdaterImpl.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl"
                ],
                "methods": [
                    "NodeStatusUpdaterImpl.rebootNodeStatusUpdaterAndRegisterWithRM",
                    "NodeStatusUpdaterImpl.registerWithRM"
                ]
            },
            "possible_fix": "Modify the `rebootNodeStatusUpdaterAndRegisterWithRM` method to include a retry loop that attempts to register with the RM multiple times upon encountering a connection exception. For example:\n\n```java\nint retries = 5;\nwhile (retries > 0) {\n    try {\n        registerWithRM();\n        break; // Exit loop on success\n    } catch (IOException e) {\n        retries--;\n        if (retries == 0) {\n            throw e; // Rethrow after max retries\n        }\n        Thread.sleep(1000); // Wait before retrying\n    }\n}\n```"
        },
        "possible_fix_code": {
            "rebootNodeStatusUpdaterAndRegisterWithRM": "public void rebootNodeStatusUpdaterAndRegisterWithRM() {\n    int retries = 5;\n    while (retries > 0) {\n        try {\n            registerWithRM();\n            break; // Exit loop on success\n        } catch (IOException e) {\n            retries--;\n            if (retries == 0) {\n                throw e; // Rethrow after max retries\n            }\n            try {\n                Thread.sleep(1000); // Wait before retrying\n            } catch (InterruptedException ie) {\n                Thread.currentThread().interrupt(); // Restore interrupted status\n            }\n        }\n    }\n}"
        }
    },
    {
        "filename": "YARN-1032.json",
        "creation_time": "2013-08-05T21:10:46.000+0000",
        "bug_report": {
            "Title": "NPE in RackResolve",
            "Description": "A NullPointerException (NPE) was encountered in the RackResolver class of the Hadoop YARN framework, specifically in the coreResolve method. This issue arises when the dnsToSwitchMapping object is not properly initialized, leading to an attempt to resolve a hostname to a rack name using a null reference. The stack trace indicates that the exception is caught in the RMContainerAllocator, which is responsible for managing container allocations based on locality. The root cause appears to be related to the configuration of the network topology mapping, specifically the CommonConfigurationKeysPublic.NET_TOPOLOGY_NODE_SWITCH_MAPPING_IMPL_KEY, which may not be set correctly in the configuration files.",
            "StackTrace": [
                "2013-08-01 07:11:37,708 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM.",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.util.RackResolver.coreResolve(RackResolver.java:99)",
                "at org.apache.hadoop.yarn.util.RackResolver.resolve(RackResolver.java:92)",
                "at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests.assignMapsWithLocality(RMContainerAllocator.java:1039)",
                "at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests.assignContainers(RMContainerAllocator.java:925)",
                "at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests.assign(RMContainerAllocator.java:861)",
                "at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests.access$400(RMContainerAllocator.java:681)",
                "at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.heartbeat(RMContainerAllocator.java:219)",
                "at org.apache.hadoop.mapreduce.v2.app.rm.RMCommunicator$1.run(RMCommunicator.java:243)",
                "at java.lang.Thread.run(Thread.java:722)"
            ],
            "RootCause": "The NullPointerException in the coreResolve method is caused by the dnsToSwitchMapping being null. This occurs if the configuration key CommonConfigurationKeysPublic.NET_TOPOLOGY_NODE_SWITCH_MAPPING_IMPL_KEY is not set correctly in the configuration files, leading to the dnsToSwitchMapping not being initialized in the init method of the RackResolver class.",
            "StepsToReproduce": [
                "Ensure that the Hadoop YARN configuration files (yarn-site.xml or yarn-default.xml) are present.",
                "Check the configuration for the key CommonConfigurationKeysPublic.NET_TOPOLOGY_NODE_SWITCH_MAPPING_IMPL_KEY.",
                "Run a job that requires rack locality to trigger the RackResolver."
            ],
            "ExpectedBehavior": "The RackResolver should successfully resolve hostnames to rack names without throwing a NullPointerException, allowing the RMContainerAllocator to allocate containers based on locality.",
            "ObservedBehavior": "A NullPointerException is thrown in the coreResolve method of the RackResolver class, preventing the successful allocation of containers based on locality.",
            "Suggestions": "Verify that the configuration for CommonConfigurationKeysPublic.NET_TOPOLOGY_NODE_SWITCH_MAPPING_IMPL_KEY is set correctly in the yarn-site.xml or yarn-default.xml files. Ensure it points to a valid implementation of DNSToSwitchMapping.",
            "problem_location": {
                "files": [
                    "RackResolver.java",
                    "RMContainerAllocator.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.util.RackResolver",
                    "org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator"
                ],
                "methods": [
                    "RackResolver.coreResolve",
                    "RackResolver.init",
                    "RMContainerAllocator.heartbeat"
                ]
            },
            "possible_fix": "Ensure that the configuration key CommonConfigurationKeysPublic.NET_TOPOLOGY_NODE_SWITCH_MAPPING_IMPL_KEY is set in the configuration files (yarn-site.xml or yarn-default.xml) to a valid DNSToSwitchMapping implementation. Example: <property><name>yarn.net.topology.node.switch.mapping.impl</name><value>org.apache.hadoop.yarn.util.DNSToSwitchMapping</value></property>"
        },
        "possible_fix_code": {
            "RackResolver.coreResolve": "  private static Node coreResolve(String hostName) {\n    List <String> tmpList = new ArrayList<String>(1);\n    tmpList.add(hostName);\n    List <String> rNameList = dnsToSwitchMapping.resolve(tmpList);\n    if (rNameList == null || rNameList.isEmpty()) {\n        LOG.error(\"No rack name found for hostname: \" + hostName);\n        return null; // or handle this case appropriately\n    }\n    String rName = rNameList.get(0);\n    LOG.info(\"Resolved \" + hostName + \" to \" + rName);\n    return new NodeBase(hostName, rName);\n  }"
        }
    },
    {
        "filename": "YARN-5837.json",
        "creation_time": "2016-11-04T16:06:59.000+0000",
        "bug_report": {
            "Title": "NPE when getting node status of a decommissioned node after an RM restart",
            "Description": "After decommissioning a node in a YARN cluster, querying its status post ResourceManager (RM) restart leads to a NullPointerException (NPE). The issue arises when the node's ID is represented as '-1' after the RM restart, causing the `printNodeStatus` method in the `NodeCLI` class to fail when it attempts to access properties of a `NodeReport` that does not exist.",
            "StackTrace": [
                "Exception in thread \"main\" java.lang.NullPointerException",
                "\tat org.apache.hadoop.yarn.client.cli.NodeCLI.printNodeStatus(NodeCLI.java:296)",
                "\tat org.apache.hadoop.yarn.client.cli.NodeCLI.run(NodeCLI.java:116)",
                "\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)",
                "\tat org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:90)",
                "\tat org.apache.hadoop.yarn.client.cli.NodeCLI.main(NodeCLI.java:63)"
            ],
            "RootCause": "The `NullPointerException` occurs in the `printNodeStatus` method when the `client.getNodeReports()` returns an empty list or when properties accessed on the `NodeReport` objects are `null`. Specifically, the node ID '-1' does not correspond to any valid node report, leading to attempts to access properties of a null object.",
            "StepsToReproduce": [
                "1. Decommission a node in the YARN cluster.",
                "2. Run the command `bin/yarn node -status <node-id>` to check the status of the decommissioned node.",
                "3. Restart the ResourceManager.",
                "4. Run the command `bin/yarn node -status <node-id>` again, where <node-id> is the ID of the decommissioned node (e.g., '192.168.1.69:-1')."
            ],
            "ExpectedBehavior": "The system should return a valid status report for the decommissioned node, indicating its state without throwing an exception.",
            "ObservedBehavior": "The system throws a NullPointerException when attempting to retrieve the status of the decommissioned node after the ResourceManager restart.",
            "Suggestions": "Implement null checks in the `printNodeStatus` method to handle cases where the node report is not found or when properties of the node report are null. This will prevent the NullPointerException from occurring.",
            "problem_location": {
                "files": [
                    "NodeCLI.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.client.cli.NodeCLI"
                ],
                "methods": [
                    "printNodeStatus"
                ]
            },
            "possible_fix": "In the `printNodeStatus` method, add checks to ensure that `nodesReport` is not empty and that properties accessed on `nodeReport` are not null before attempting to use them. For example:\n\n```java\nif (nodesReport == null || nodesReport.isEmpty()) {\n    nodeReportStr.print(\"No node reports available.\");\n    return;\n}\n```\n\nAdditionally, check each property accessed on `nodeReport` for null before using it."
        },
        "possible_fix_code": {
            "printNodeStatus": "  private void printNodeStatus(String nodeIdStr) throws YarnException,\n      IOException {\n    NodeId nodeId = NodeId.fromString(nodeIdStr);\n    List<NodeReport> nodesReport = client.getNodeReports();\n    // Use PrintWriter.println, which uses correct platform line ending.\n    ByteArrayOutputStream baos = new ByteArrayOutputStream();\n    PrintWriter nodeReportStr = new PrintWriter(\n        new OutputStreamWriter(baos, Charset.forName(\"UTF-8\")));\n    NodeReport nodeReport = null;\n\n    // Check if nodesReport is null or empty\n    if (nodesReport == null || nodesReport.isEmpty()) {\n        nodeReportStr.print(\"No node reports available.\");\n        nodeReportStr.close();\n        sysout.println(baos.toString(\"UTF-8\"));\n        return;\n    }\n\n    for (NodeReport report : nodesReport) {\n      if (!report.getNodeId().equals(nodeId)) {\n        continue;\n      }\n      nodeReport = report;\n      nodeReportStr.println(\"Node Report : \");\n      nodeReportStr.print(\"\\tNode-Id : \");\n      nodeReportStr.println(nodeReport.getNodeId());\n      nodeReportStr.print(\"\\tRack : \");\n      nodeReportStr.println(nodeReport.getRackName());\n      nodeReportStr.print(\"\\tNode-State : \");\n      nodeReportStr.println(nodeReport.getNodeState());\n      nodeReportStr.print(\"\\tNode-Http-Address : \");\n      nodeReportStr.println(nodeReport.getHttpAddress());\n      nodeReportStr.print(\"\\tLast-Health-Update : \");\n      nodeReportStr.println(DateFormatUtils.format(\n          new Date(nodeReport.getLastHealthReportTime()),\n            \"E dd/MMM/yy hh:mm:ss:SSzz\"));\n      nodeReportStr.print(\"\\tHealth-Report : \");\n      nodeReportStr\n          .println(nodeReport.getHealthReport());\n      nodeReportStr.print(\"\\tContainers : \");\n      nodeReportStr.println(nodeReport.getNumContainers());\n      nodeReportStr.print(\"\\tMemory-Used : \");\n      nodeReportStr.println((nodeReport.getUsed() == null) ? \"0MB\"\n          : (nodeReport.getUsed().getMemorySize() + \"MB\"));\n      nodeReportStr.print(\"\\tMemory-Capacity : \");\n      nodeReportStr.println(nodeReport.getCapability().getMemorySize() + \"MB\");\n      nodeReportStr.print(\"\\tCPU-Used : \");\n      nodeReportStr.println((nodeReport.getUsed() == null) ? \"0 vcores\"\n          : (nodeReport.getUsed().getVirtualCores() + \" vcores\"));\n      nodeReportStr.print(\"\\tCPU-Capacity : \");\n      nodeReportStr.println(nodeReport.getCapability().getVirtualCores() + \" vcores\");\n      nodeReportStr.print(\"\\tNode-Labels : \");\n      \n      // Create a List for node labels since we need it get sorted\n      List<String> nodeLabelsList =\n          new ArrayList<String>(report.getNodeLabels());\n      Collections.sort(nodeLabelsList);\n      nodeReportStr.println(StringUtils.join(nodeLabelsList.iterator(), \",\"));\n\n      nodeReportStr.print(\"\\tResource Utilization by Node : \");\n      if (nodeReport.getNodeUtilization() != null) {\n        nodeReportStr.print(\"PMem:\" + nodeReport.getNodeUtilization().getPhysicalMemory() + \" MB, VMem:\" + nodeReport.getNodeUtilization().getVirtualMemory() + \" MB, VCores:\" + nodeReport.getNodeUtilization().getCPU());\n      }\n      nodeReportStr.println();\n\n      nodeReportStr.print(\"\\tResource Utilization by Containers : \");\n      if (nodeReport.getAggregatedContainersUtilization() != null) {\n        nodeReportStr.print(\"PMem:\" + nodeReport.getAggregatedContainersUtilization().getPhysicalMemory() + \" MB, VMem:\" + nodeReport.getAggregatedContainersUtilization().getVirtualMemory() + \" MB, VCores:\" + nodeReport.getAggregatedContainersUtilization().getCPU());\n      }\n      nodeReportStr.println();\n    }\n\n    if (nodeReport == null) {\n      nodeReportStr.print(\"Could not find the node report for node id : \" + nodeIdStr);\n    }\n    nodeReportStr.close();\n    sysout.println(baos.toString(\"UTF-8\"));\n  }"
        }
    },
    {
        "filename": "YARN-6827.json",
        "creation_time": "2017-07-15T05:14:25.000+0000",
        "bug_report": {
            "Title": "[ATS1/1.5] NPE exception while publishing recovering applications into ATS during RM restart.",
            "Description": "During the recovery of applications in the ResourceManager (RM), a NullPointerException (NPE) is thrown when attempting to publish entities to the Application Timeline Service (ATS). The stack trace indicates that the exception occurs in the `putEntities` method of the `TimelineClientImpl` class. This issue arises because the ATS services are not fully initialized before the active services attempt to publish application data, leading to a null reference for the `timelineWriter`.",
            "StackTrace": [
                "017-07-13 14:08:12,476 ERROR org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher: Error when publishing entity [YARN_APPLICATION,application_1499929227397_0001]",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl.putEntities(TimelineClientImpl.java:178)",
                "at org.apache.hadoop.yarn.server.resourcemanager.metrics.TimelineServiceV1Publisher.putEntity(TimelineServiceV1Publisher.java:368)"
            ],
            "RootCause": "The root cause of the NPE is that the `timelineWriter` is not initialized before the `putEntities` method is called. This occurs because the `serviceStart` method, which initializes the `timelineWriter`, is not invoked before the `putEntities` method is called during the application recovery process.",
            "StepsToReproduce": [
                "1. Start the ResourceManager in a non-HA configuration.",
                "2. Trigger the recovery of applications that need to publish to ATS.",
                "3. Observe the logs for the NPE in the `putEntities` method."
            ],
            "ExpectedBehavior": "The application should successfully publish its entities to the ATS without throwing a NullPointerException, indicating that the `timelineWriter` is properly initialized.",
            "ObservedBehavior": "A NullPointerException is thrown when attempting to publish entities to the ATS, indicating that the `timelineWriter` is null.",
            "Suggestions": "Ensure that the `serviceStart` method is called before any attempts to publish entities to the ATS. This can be achieved by reviewing the initialization sequence of the ResourceManager and ensuring that ATS services are fully started before any publishing actions occur.",
            "problem_location": {
                "files": [
                    "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/client/api/impl/TimelineClientImpl.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl"
                ],
                "methods": [
                    "TimelineClientImpl.putEntities",
                    "TimelineClientImpl.serviceStart"
                ]
            },
            "possible_fix": "To fix the issue, ensure that the `serviceStart` method is called in the correct order during the ResourceManager's startup sequence. This may involve modifying the initialization logic to guarantee that the `timelineWriter` is set before any calls to `putEntities` are made."
        },
        "possible_fix_code": {
            "org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl.putEntities": "  public TimelinePutResponse putEntities(ApplicationAttemptId appAttemptId,\n      TimelineEntityGroupId groupId, TimelineEntity... entities)\n      throws IOException, YarnException {\n    // Ensure that the timelineWriter is initialized before proceeding\n    if (this.timelineWriter == null) {\n      throw new YarnException(\"TimelineWriter is not initialized. Ensure that serviceStart() is called before putEntities().\");\n    }\n\n    if (Float.compare(this.timelineServiceVersion, 1.5f) != 0) {\n      throw new YarnException(\n        \"This API is not supported under current Timeline Service Version: \"\n            + timelineServiceVersion);\n    }\n\n    return timelineWriter.putEntities(appAttemptId, groupId, entities);\n  }"
        }
    },
    {
        "filename": "YARN-3832.json",
        "creation_time": "2015-06-19T13:31:18.000+0000",
        "bug_report": {
            "Title": "Resource Localization fails on a cluster due to existing cache directories",
            "Description": "Resource localization fails on a cluster when attempting to rename a directory that is not empty, resulting in an IOException. This issue was observed in the Hadoop 2.7.0 release, which was previously addressed in version 2.6.0 (YARN-2624). The error occurs during the application attempt, leading to multiple failures due to the AM Container exiting with an exit code of -1000. The specific error message indicates that the rename operation cannot overwrite a non-empty destination directory.",
            "StackTrace": [
                "Application application_1434703279149_0057 failed 2 times due to AM Container for appattempt_1434703279149_0057_000002 exited with exitCode: -1000",
                "java.io.IOException: Rename cannot overwrite non empty destination directory /opt/hdfsdata/HA/nmlocal/usercache/root/filecache/39",
                "at org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:735)",
                "at org.apache.hadoop.fs.FilterFs.renameInternal(FilterFs.java:244)",
                "at org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:678)",
                "at org.apache.hadoop.fs.FileContext.rename(FileContext.java:958)",
                "at org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:366)",
                "at org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:62)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:266)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:266)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "RootCause": "The root cause of the issue is an attempt to rename a directory in Hadoop's file system where the destination directory is not empty, leading to an IOException. This is confirmed by the stack trace and the analysis of the rename method in the FileContext class.",
            "StepsToReproduce": [
                "Deploy a Hadoop cluster with version 2.7.0.",
                "Attempt to run an application that requires resource localization.",
                "Ensure that the destination directory for file caching is not empty.",
                "Observe the application failure due to the IOException related to the rename operation."
            ],
            "ExpectedBehavior": "The application should successfully localize resources without encountering an IOException, allowing the rename operation to complete without issues.",
            "ObservedBehavior": "The application fails with an IOException indicating that the rename operation cannot overwrite a non-empty destination directory, causing the application to fail multiple times.",
            "Suggestions": "To resolve this issue, either clear the contents of the destination directory before attempting the rename operation or choose a different destination path that does not conflict with existing files or directories.",
            "problem_location": {
                "files": [
                    "FileContext.java"
                ],
                "classes": [
                    "org.apache.hadoop.fs.FileContext"
                ],
                "methods": [
                    "FileContext.rename"
                ]
            },
            "possible_fix": "Before invoking the rename method, check if the destination directory is empty. If it is not, either delete its contents or select a different destination path. This can be implemented by adding a check in the application logic prior to the rename operation."
        },
        "possible_fix_code": {
            "org.apache.hadoop.fs.FileContext.rename": "  public void rename(final Path src, final Path dst,\n      final Options.Rename... options) throws AccessControlException,\n      FileAlreadyExistsException, FileNotFoundException,\n      ParentNotDirectoryException, UnsupportedFileSystemException,\n      IOException {\n    final Path absSrc = fixRelativePart(src);\n    final Path absDst = fixRelativePart(dst);\n    AbstractFileSystem srcFS = getFSofPath(absSrc);\n    AbstractFileSystem dstFS = getFSofPath(absDst);\n    if(!srcFS.getUri().equals(dstFS.getUri())) {\n      throw new IOException(\"Renames across AbstractFileSystems not supported\");\n    }\n    // Check if the destination directory exists and is not empty\n    if (dstFS.exists(absDst) && dstFS.isDirectory(absDst) && dstFS.listStatus(absDst).length > 0) {\n      throw new IOException(\"Destination directory is not empty: \" + absDst);\n    }\n    try {\n      srcFS.rename(absSrc, absDst, options);\n    } catch (UnresolvedLinkException e) {\n      /* We do not know whether the source or the destination path\n       * was unresolved. Resolve the source path up until the final\n       * path component, then fully resolve the destination. \n       */\n      final Path source = resolveIntermediate(absSrc);    \n      new FSLinkResolver<Void>() {\n        @Override\n        public Void next(final AbstractFileSystem fs, final Path p) \n          throws IOException, UnresolvedLinkException {\n          fs.rename(source, p, options);\n          return null;\n        }\n      }.resolve(this, absDst);\n    }\n  }"
        }
    },
    {
        "filename": "YARN-2409.json",
        "creation_time": "2014-08-12T10:53:06.000+0000",
        "bug_report": {
            "Title": "Active to StandBy transition does not stop rmDispatcher that causes 1 AsyncDispatcher thread leak.",
            "Description": "The issue arises during the transition of an application from the Active to StandBy state in the Hadoop YARN framework. The application attempts to process events such as `STATUS_UPDATE` and `CONTAINER_ALLOCATED` while in the `LAUNCHED` state, which is not permitted according to the state machine's rules. This results in an `InvalidStateTransitonException`, indicating that the application is trying to handle events that are invalid for its current state. The failure to properly manage these state transitions leads to a leak of an AsyncDispatcher thread.",
            "StackTrace": [
                "at java.lang.Thread.run(Thread.java:662)",
                "2014-08-12 07:03:00,839 ERROR org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Can't handle this event at current state",
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: STATUS_UPDATE at LAUNCHED",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:697)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:105)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:779)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:760)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:173)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:106)",
                "at java.lang.Thread.run(Thread.java:662)"
            ],
            "RootCause": "The root cause of the issue is that the application is attempting to process invalid events (`STATUS_UPDATE` and `CONTAINER_ALLOCATED`) while in the `LAUNCHED` state. According to the state machine defined in the `RMAppAttemptImpl` class, these events are not valid in this state, leading to an `InvalidStateTransitonException` being thrown.",
            "StepsToReproduce": [
                "1. Start an application in the YARN framework.",
                "2. Transition the application to the Active state.",
                "3. Attempt to transition the application to StandBy while it is still processing events.",
                "4. Observe the logs for the `InvalidStateTransitonException` related to `STATUS_UPDATE` and `CONTAINER_ALLOCATED` events."
            ],
            "ExpectedBehavior": "The application should successfully transition from Active to StandBy without attempting to process invalid events, thereby preventing any exceptions and ensuring that all threads are properly managed.",
            "ObservedBehavior": "The application throws an `InvalidStateTransitonException` when trying to handle `STATUS_UPDATE` and `CONTAINER_ALLOCATED` events while in the `LAUNCHED` state, leading to a leak of an AsyncDispatcher thread.",
            "Suggestions": "To resolve this issue, the application logic should be modified to check the current state before dispatching events. Specifically, ensure that `STATUS_UPDATE` and `CONTAINER_ALLOCATED` events are not sent while the application is in the `LAUNCHED` state.",
            "problem_location": {
                "files": [
                    "hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/RMAppAttemptImpl.java",
                    "hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/state/StateMachineFactory.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl",
                    "org.apache.hadoop.yarn.state.StateMachineFactory"
                ],
                "methods": [
                    "RMAppAttemptImpl.handle",
                    "StateMachineFactory.doTransition"
                ]
            },
            "possible_fix": "Modify the event handling logic in the `RMAppAttemptImpl` class to prevent the dispatching of `STATUS_UPDATE` and `CONTAINER_ALLOCATED` events when the application is in the `LAUNCHED` state. This can be achieved by adding a state check before processing these events."
        },
        "possible_fix_code": {
            "RMAppAttemptImpl.handle": "  @Override\n  public void handle(RMAppAttemptEvent event) {\n\n    this.writeLock.lock();\n\n    try {\n      ApplicationAttemptId appAttemptID = event.getApplicationAttemptId();\n      LOG.debug(\"Processing event for \" + appAttemptID + \" of type \" + event.getType());\n      final RMAppAttemptState oldState = getAppAttemptState();\n\n      // Check if the current state is LAUNCHED before processing STATUS_UPDATE or CONTAINER_ALLOCATED events\n      if (oldState == RMAppAttemptState.LAUNCHED && (event.getType() == RMAppAttemptEventType.STATUS_UPDATE || event.getType() == RMAppAttemptEventType.CONTAINER_ALLOCATED)) {\n        LOG.error(\"Cannot handle event \" + event.getType() + \" in current state: \" + oldState);\n        return; // Prevent processing invalid events\n      }\n\n      try {\n        /* keep the master in sync with the state machine */\n        this.stateMachine.doTransition(event.getType(), event);\n      } catch (InvalidStateTransitonException e) {\n        LOG.error(\"Can't handle this event at current state\", e);\n        /* TODO fail the application on the failed transition */\n      }\n\n      if (oldState != getAppAttemptState()) {\n        LOG.info(appAttemptID + \" State change from \" + oldState + \" to \" + getAppAttemptState());\n      }\n    } finally {\n      this.writeLock.unlock();\n    }\n  }"
        }
    },
    {
        "filename": "YARN-8116.json",
        "creation_time": "2018-04-04T15:30:52.000+0000",
        "bug_report": {
            "Title": "Nodemanager fails with NumberFormatException: For input string: \"\"",
            "Description": "The NodeManager fails to start due to a NumberFormatException caused by an attempt to parse an empty string as a long integer. This issue arises during the loading of container states in the Hadoop YARN NodeManager, specifically in the `loadContainerState` method of the `NMLeveldbStateStoreService` class. The error occurs when the method tries to parse values retrieved from the database without checking if they are empty.",
            "StackTrace": [
                "java.lang.NumberFormatException: For input string: \"\"",
                "at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)",
                "at java.lang.Long.parseLong(Long.java:601)",
                "at org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService.loadContainerState(NMLeveldbStateStoreService.java:350)",
                "at org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService.loadContainersState(NMLeveldbStateStoreService.java:253)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.recover(ContainerManagerImpl.java:365)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.serviceInit(ContainerManagerImpl.java:316)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)",
                "at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:108)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.serviceInit(NodeManager.java:464)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:164)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.initAndStartNodeManager(NodeManager.java:899)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.main(NodeManager.java:960)"
            ],
            "RootCause": "The root cause of the issue is that the `loadContainerState` method in the `NMLeveldbStateStoreService` class attempts to parse an empty string as a long integer when loading container states from the database. This occurs specifically in the lines where it calls `Long.parseLong(asString(entry.getValue()))` without checking if the string is empty.",
            "StepsToReproduce": [
                "1) Update nodemanager debug delay config:",
                "<property>",
                "  <name>yarn.nodemanager.delete.debug-delay-sec</name>",
                "  <value>350</value>",
                "</property>",
                "2) Launch distributed shell application multiple times:",
                "/usr/hdp/current/hadoop-yarn-client/bin/yarn jar hadoop-yarn-applications-distributedshell-*.jar -shell_command \"sleep 120\" -num_containers 1 -shell_env YARN_CONTAINER_RUNTIME_TYPE=docker -shell_env YARN_CONTAINER_RUNTIME_DOCKER_IMAGE=centos/httpd-24-centos7:latest -shell_env YARN_CONTAINER_RUNTIME_DOCKER_DELAYED_REMOVAL=true -jar hadoop-yarn-applications-distributedshell-*.jar",
                "3) Restart NodeManager."
            ],
            "ExpectedBehavior": "The NodeManager should start successfully without throwing any exceptions, allowing it to manage container states and execute distributed applications.",
            "ObservedBehavior": "The NodeManager fails to start and logs a NumberFormatException due to an attempt to parse an empty string as a long integer.",
            "Suggestions": "Modify the `loadContainerState` method to check if the string returned by `asString(entry.getValue())` is empty before attempting to parse it with `Long.parseLong()`. This will prevent the NumberFormatException from occurring.",
            "problem_location": {
                "files": [
                    "NMLeveldbStateStoreService.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.nodemanager.recovery.NMLeveldbStateStoreService"
                ],
                "methods": [
                    "loadContainerState"
                ]
            },
            "possible_fix": "In the `loadContainerState` method, add a check before parsing the value: \n\nif (!asString(entry.getValue()).isEmpty()) {\n    rcs.setStartTime(Long.parseLong(asString(entry.getValue())));\n} else {\n    // Handle the empty case appropriately, e.g., log a warning or set a default value.\n}"
        },
        "possible_fix_code": {
            "loadContainerState": "  private RecoveredContainerState loadContainerState(ContainerId containerId,\n      LeveldbIterator iter, String keyPrefix) throws IOException {\n    RecoveredContainerState rcs = new RecoveredContainerState();\n    rcs.status = RecoveredContainerStatus.REQUESTED;\n    while (iter.hasNext()) {\n      Entry<byte[],byte[]> entry = iter.peekNext();\n      String key = asString(entry.getKey());\n      if (!key.startsWith(keyPrefix)) {\n        break;\n      }\n      iter.next();\n\n      String suffix = key.substring(keyPrefix.length()-1);  // start with \"/\"\n      if (suffix.equals(CONTAINER_REQUEST_KEY_SUFFIX)) {\n        rcs.startRequest = new StartContainerRequestPBImpl(\n            StartContainerRequestProto.parseFrom(entry.getValue()));\n      } else if (suffix.equals(CONTAINER_VERSION_KEY_SUFFIX)) {\n        rcs.version = Integer.parseInt(asString(entry.getValue()));\n      } else if (suffix.equals(CONTAINER_START_TIME_KEY_SUFFIX)) {\n        String startTimeValue = asString(entry.getValue());\n        if (!startTimeValue.isEmpty()) {\n          rcs.setStartTime(Long.parseLong(startTimeValue));\n        } else {\n          // Handle the empty case appropriately, e.g., log a warning or set a default value.\n          LOG.warn(\"Start time value is empty for container \" + containerId);\n        }\n      } else if (suffix.equals(CONTAINER_DIAGS_KEY_SUFFIX)) {\n        rcs.diagnostics = asString(entry.getValue());\n      } else if (suffix.equals(CONTAINER_QUEUED_KEY_SUFFIX)) {\n        if (rcs.status == RecoveredContainerStatus.REQUESTED) {\n          rcs.status = RecoveredContainerStatus.QUEUED;\n        }\n      } else if (suffix.equals(CONTAINER_PAUSED_KEY_SUFFIX)) {\n        if ((rcs.status == RecoveredContainerStatus.LAUNCHED)\n            ||(rcs.status == RecoveredContainerStatus.QUEUED)\n            ||(rcs.status == RecoveredContainerStatus.REQUESTED)) {\n          rcs.status = RecoveredContainerStatus.PAUSED;\n        }\n      } else if (suffix.equals(CONTAINER_LAUNCHED_KEY_SUFFIX)) {\n        if ((rcs.status == RecoveredContainerStatus.REQUESTED)\n            || (rcs.status == RecoveredContainerStatus.QUEUED)\n            ||(rcs.status == RecoveredContainerStatus.PAUSED)) {\n          rcs.status = RecoveredContainerStatus.LAUNCHED;\n        }\n      } else if (suffix.equals(CONTAINER_KILLED_KEY_SUFFIX)) {\n        rcs.killed = true;\n      } else if (suffix.equals(CONTAINER_EXIT_CODE_KEY_SUFFIX)) {\n        rcs.status = RecoveredContainerStatus.COMPLETED;\n        rcs.exitCode = Integer.parseInt(asString(entry.getValue()));\n      } else if (suffix.equals(CONTAINER_UPDATE_TOKEN_SUFFIX)) {\n        ContainerTokenIdentifierProto tokenIdentifierProto =\n            ContainerTokenIdentifierProto.parseFrom(entry.getValue());\n        Token currentToken = rcs.getStartRequest().getContainerToken();\n        Token updatedToken = Token\n            .newInstance(tokenIdentifierProto.toByteArray(),\n                ContainerTokenIdentifier.KIND.toString(),\n                currentToken.getPassword().array(), currentToken.getService());\n        rcs.startRequest.setContainerToken(updatedToken);\n        rcs.capability = new ResourcePBImpl(tokenIdentifierProto.getResource());\n        rcs.version = tokenIdentifierProto.getVersion();\n      } else if (suffix.equals(CONTAINER_REMAIN_RETRIES_KEY_SUFFIX)) {\n        rcs.setRemainingRetryAttempts(\n            Integer.parseInt(asString(entry.getValue())));\n      } else if (suffix.equals(CONTAINER_RESTART_TIMES_SUFFIX)) {\n        String value = asString(entry.getValue());\n        // parse the string format of List<Long>, e.g. [34, 21, 22]\n        String[] unparsedRestartTimes =\n            value.substring(1, value.length() - 1).split(\", \");\n        List<Long> restartTimes = new ArrayList<>();\n        for (String restartTime : unparsedRestartTimes) {\n          restartTimes.add(Long.parseLong(restartTime));\n        }\n        rcs.setRestartTimes(restartTimes);\n      } else if (suffix.equals(CONTAINER_WORK_DIR_KEY_SUFFIX)) {\n        rcs.setWorkDir(asString(entry.getValue()));\n      } else if (suffix.equals(CONTAINER_LOG_DIR_KEY_SUFFIX)) {\n        rcs.setLogDir(asString(entry.getValue()));\n      } else if (suffix.startsWith(CONTAINER_ASSIGNED_RESOURCES_KEY_SUFFIX)) {\n        String resourceType = suffix.substring(\n            CONTAINER_ASSIGNED_RESOURCES_KEY_SUFFIX.length());\n        ResourceMappings.AssignedResources assignedResources =\n            ResourceMappings.AssignedResources.fromBytes(entry.getValue());\n        rcs.getResourceMappings().addAssignedResources(resourceType,\n            assignedResources);\n      } else {\n        LOG.warn(\"the container \" + containerId\n            + \" will be killed because of the unknown key \" + key\n            + \" during recovery.\");\n        containerUnknownKeySuffixes.put(containerId, suffix);\n        rcs.setRecoveryType(RecoveredContainerType.KILL);\n      }\n    }\n    return rcs;\n  }"
        }
    },
    {
        "filename": "YARN-8403.json",
        "creation_time": "2018-06-06T22:34:42.000+0000",
        "bug_report": {
            "Title": "Nodemanager logs failed to download file with INFO level",
            "Description": "The Nodemanager is encountering issues while attempting to download resources for container execution, resulting in logs being generated at INFO and WARN levels instead of ERROR. The primary error is a `FileNotFoundException` caused by permission issues when trying to write to the directory `/grid/0/hadoop/yarn/local/filecache/28_tmp/InputDir/`. This indicates that the user running the YARN application lacks the necessary permissions to access the specified file path, leading to failures in resource localization.",
            "StackTrace": [
                "2018-06-06 03:10:40,077 INFO  localizer.ResourceLocalizationService (ResourceLocalizationService.java:writeCredentials(1312)) - Writing credentials to the nmPrivate file /grid/0/hadoop/yarn/local/nmPrivate/container_e02_1528246317583_0048_01_000001.tokens",
                "2018-06-06 03:10:40,087 INFO  localizer.ResourceLocalizationService (ResourceLocalizationService.java:run(975)) - Failed to download resource { { hdfs://mycluster.example.com:8020/user/hrt_qa/Streaming/InputDir, 1528254452720, FILE, null },pending,[(container_e02_1528246317583_0048_01_000001)],6074418082915225,DOWNLOADING}",
                "org.apache.hadoop.yarn.exceptions.YarnException: Download and unpack failed",
                "Caused by: java.io.FileNotFoundException: /grid/0/hadoop/yarn/local/filecache/28_tmp/InputDir/input1.txt (Permission denied)",
                "2018-06-06 03:10:41,547 WARN  privileged.PrivilegedOperationExecutor (PrivilegedOperationExecutor.java:executePrivilegedOperation(182)) - IOException executing command:",
                "java.io.InterruptedIOException: java.lang.InterruptedException",
                "2018-06-06 03:10:41,548 WARN  nodemanager.LinuxContainerExecutor (LinuxContainerExecutor.java:startLocalizer(407)) - Exit code from container container_e02_1528246317583_0048_01_000001 startLocalizer is : -1",
                "2018-06-06 03:10:41,548 INFO  localizer.ResourceLocalizationService (ResourceLocalizationService.java:run(1249)) - Localizer failed for container_e02_1528246317583_0048_01_000001"
            ],
            "RootCause": "The root cause of the issue is a `FileNotFoundException` due to permission denied errors when attempting to write to the directory `/grid/0/hadoop/yarn/local/filecache/28_tmp/InputDir/`. The user running the YARN application does not have the necessary write permissions for this directory.",
            "StepsToReproduce": [
                "Run a YARN application that requires downloading resources to the specified directory.",
                "Ensure that the user running the application does not have write permissions to `/grid/0/hadoop/yarn/local/filecache/28_tmp/InputDir/`.",
                "Check the Nodemanager logs for INFO and WARN level messages related to resource localization."
            ],
            "ExpectedBehavior": "The Nodemanager should successfully download and unpack the required resources without any permission errors, and relevant logs should be recorded at the ERROR level for any failures.",
            "ObservedBehavior": "The Nodemanager logs show INFO and WARN level messages indicating failures in downloading resources due to permission issues, specifically a `FileNotFoundException`.",
            "Suggestions": "To resolve the permission issue, ensure that the user running the YARN application has the necessary write permissions to the directory `/grid/0/hadoop/yarn/local/filecache/28_tmp/InputDir/`. This can be done by checking the current permissions and modifying them using the `setPermission` method in the Hadoop `FileSystem` class.",
            "problem_location": {
                "files": [
                    "ResourceLocalizationService.java",
                    "LinuxContainerExecutor.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.util.FSDownload",
                    "org.apache.hadoop.fs.FileSystem"
                ],
                "methods": [
                    "FSDownload.downloadAndUnpack",
                    "FileSystem.create",
                    "FileSystem.setPermission"
                ]
            },
            "possible_fix": "Check the current permissions of the directory using the `getFileStatus` method. If the permissions are inadequate, use the `setPermission` method to update them, allowing write access for the user or group running the YARN application."
        },
        "possible_fix_code": {
            "org.apache.hadoop.yarn.util.FSDownload.downloadAndUnpack": "public void downloadAndUnpack(Path src, Path dest) throws IOException {\n    // Check current permissions of the destination directory\n    FileStatus status = fileSystem.getFileStatus(dest);\n    FsPermission currentPermission = status.getPermission();\n\n    // If the current permissions do not allow write access, set new permissions\n    if (!currentPermission.getUserAction().implies(FsAction.WRITE)) {\n        // Set permissions to allow write access for the user\n        FsPermission newPermission = new FsPermission(currentPermission.toShort() | 0002); // Add write permission\n        fileSystem.setPermission(dest, newPermission);\n    }\n\n    // Proceed with the download and unpacking process\n    FSDataOutputStream out = fileSystem.create(dest, new FsPermission(FsAction.ALL));\n    // Logic to download and unpack the resource\n    // ...\n}"
        }
    },
    {
        "filename": "YARN-1458.json",
        "creation_time": "2013-11-29T03:31:39.000+0000",
        "bug_report": {
            "Title": "FairScheduler: Zero weight can lead to livelock",
            "Description": "The ResourceManager's SchedulerEventDispatcher is experiencing a livelock condition when clients submit a large number of jobs. This issue manifests as a BLOCKED thread waiting for a lock on the FairScheduler, which is held by another RUNNABLE thread. The problem is difficult to reproduce consistently, requiring extended testing periods to observe the behavior. The stack trace indicates contention in the FairScheduler's methods, particularly during application removal and weight calculations.",
            "StackTrace": [
                "java.lang.Thread.State: BLOCKED (on object monitor)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.removeApplication(FairScheduler.java:671)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:1023)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:112)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:440)",
                "at java.lang.Thread.run(Thread.java:744)",
                "java.lang.Thread.State: RUNNABLE",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.getAppWeight(FairScheduler.java:545)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AppSchedulable.getWeights(AppSchedulable.java:129)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.policies.ComputeFairShares.computeShare(ComputeFairShares.java:143)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.policies.ComputeFairShares.resourceUsedWithWeightToResourceRatio(ComputeFairShares.java:131)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.policies.ComputeFairShares.computeShares(ComputeFairShares.java:102)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.policies.FairSharePolicy.computeShares(FairSharePolicy.java:119)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue.recomputeShares(FSLeafQueue.java:100)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSParentQueue.recomputeShares(FSParentQueue.java:62)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.update(FairScheduler.java:282)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler$UpdateThread.run(FairScheduler.java:255)",
                "at java.lang.Thread.run(Thread.java:744)"
            ],
            "RootCause": "The root cause of the livelock is contention for the FairScheduler lock, where one thread is blocked waiting for the lock while another thread is actively holding it. This situation arises when multiple threads attempt to access shared resources managed by the FairScheduler, particularly during application removal and weight calculations.",
            "StepsToReproduce": [
                "1. Set up a test cluster with the FairScheduler.",
                "2. Submit a large number of jobs simultaneously to the ResourceManager.",
                "3. Monitor the thread states using jstack to observe the BLOCKED and RUNNABLE states."
            ],
            "ExpectedBehavior": "In a properly functioning system, the FairScheduler should handle job submissions without causing any threads to become blocked indefinitely, allowing for smooth scheduling and resource allocation.",
            "ObservedBehavior": "The system experiences a livelock condition where the EventProcessor thread is blocked while waiting for a lock held by another thread, leading to delays in job processing and resource allocation.",
            "Suggestions": "Consider implementing a timeout mechanism for acquiring locks in the FairScheduler to prevent indefinite blocking. Additionally, review the locking strategy to minimize contention between threads.",
            "problem_location": {
                "files": [
                    "FairScheduler.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler"
                ],
                "methods": [
                    "FairScheduler.removeApplication",
                    "FairScheduler.getAppWeight",
                    "FairScheduler.update"
                ]
            },
            "possible_fix": "Implement a timeout for lock acquisition in the FairScheduler methods to avoid indefinite blocking. Review and optimize the locking strategy to reduce contention among threads."
        },
        "possible_fix_code": {
            "FairScheduler.removeApplication": "  private synchronized void removeApplication(\n      ApplicationAttemptId applicationAttemptId,\n      RMAppAttemptState rmAppAttemptFinalState) {\n    LOG.info(\"Application \" + applicationAttemptId + \" is done.\" +\n        \" finalState=\" + rmAppAttemptFinalState);\n\n    FSSchedulerApp application = applications.get(applicationAttemptId);\n\n    if (application == null) {\n      LOG.info(\"Unknown application \" + applicationAttemptId + \" has completed!\");\n      return;\n    }\n\n    // Release all the running containers\n    for (RMContainer rmContainer : application.getLiveContainers()) {\n      completedContainer(rmContainer,\n          SchedulerUtils.createAbnormalContainerStatus(\n              rmContainer.getContainerId(),\n              SchedulerUtils.COMPLETED_APPLICATION),\n              RMContainerEventType.KILL);\n    }\n\n    // Release all reserved containers\n    for (RMContainer rmContainer : application.getReservedContainers()) {\n      completedContainer(rmContainer,\n          SchedulerUtils.createAbnormalContainerStatus(\n              rmContainer.getContainerId(),\n              \"Application Complete\"),\n          RMContainerEventType.KILL);\n    }\n\n    // Clean up pending requests, metrics etc.\n    application.stop(rmAppAttemptFinalState);\n\n    // Inform the queue\n    FSLeafQueue queue = queueMgr.getLeafQueue(application.getQueue()\n        .getQueueName(), false);\n    queue.removeApp(application);\n\n    // Remove from our data-structure\n    applications.remove(applicationAttemptId);\n\n    // Implementing a timeout mechanism for lock acquisition\n    try {\n        if (!tryLock(LOCK_TIMEOUT)) {\n            LOG.warn(\"Could not acquire lock within timeout, skipping removal.\");\n            return;\n        }\n        // Proceed with application removal logic\n    } finally {\n        unlock();\n    }\n  }"
        }
    },
    {
        "filename": "YARN-8209.json",
        "creation_time": "2018-04-26T00:22:23.000+0000",
        "bug_report": {
            "Title": "NPE in DeletionService",
            "Description": "A NullPointerException (NPE) is occurring in the DeletionService of a Hadoop YARN application, specifically within the DockerClient's method 'writeCommandToTempFile'. This method is invoked by the DockerCommandExecutor when attempting to execute a Docker command. The NPE arises when either the NodeManager context (nmContext) or the Container object passed to the method is null, leading to an inability to write the Docker command to a temporary file.",
            "StackTrace": [
                "2018-04-25 23:38:41,039 WARN concurrent.ExecutorHelper (ExecutorHelper.java:logThrowableFromAfterExecute(63)) - Caught exception in thread DeletionService #1:",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerClient.writeCommandToTempFile(DockerClient.java:109)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerCommandExecutor.executeDockerCommand(DockerCommandExecutor.java:85)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerCommandExecutor.executeStatusCommand(DockerCommandExecutor.java:192)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerCommandExecutor.getContainerStatus(DockerCommandExecutor.java:128)",
                "at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.removeDockerContainer(LinuxContainerExecutor.java:935)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.deletion.task.DockerContainerDeletionTask.run(DockerContainerDeletionTask.java:61)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:266)",
                "at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)",
                "at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)",
                "at java.lang.Thread.run(Thread.java:748)"
            ],
            "RootCause": "The root cause of the NullPointerException is that either the NodeManager context (nmContext) or the Container object being passed to the 'writeCommandToTempFile' method is null. This leads to a failure when the method attempts to access methods on these null references.",
            "StepsToReproduce": [
                "1. Start the Hadoop YARN application with Docker support.",
                "2. Trigger a deletion task for a Docker container.",
                "3. Monitor the logs for the DeletionService thread."
            ],
            "ExpectedBehavior": "The system should successfully execute the Docker command and write it to a temporary file without throwing a NullPointerException.",
            "ObservedBehavior": "A NullPointerException is thrown, causing the DeletionService to fail when attempting to write a Docker command to a temporary file.",
            "Suggestions": "Ensure that the NodeManager context (nmContext) and the Container object are properly initialized before invoking the 'writeCommandToTempFile' method. Add null checks and handle the cases where these objects may not be available.",
            "problem_location": {
                "files": [
                    "DockerClient.java",
                    "DockerCommandExecutor.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerClient",
                    "org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerCommandExecutor"
                ],
                "methods": [
                    "DockerClient.writeCommandToTempFile",
                    "DockerCommandExecutor.executeDockerCommand"
                ]
            },
            "possible_fix": "In the 'writeCommandToTempFile' method, add checks to ensure that nmContext and the Container object are not null before proceeding with the file writing logic. For example:\n\nif (nmContext == null || container == null) {\n    throw new ContainerExecutionException(\"nmContext or container is null\");\n}"
        },
        "possible_fix_code": {
            "DockerClient.writeCommandToTempFile": "  public String writeCommandToTempFile(DockerCommand cmd, Container container,\n      Context nmContext) throws ContainerExecutionException {\n    if (nmContext == null || container == null) {\n        throw new ContainerExecutionException(\"nmContext or container is null\");\n    }\n    ContainerId containerId = container.getContainerId();\n    String filePrefix = containerId.toString();\n    ApplicationId appId = containerId.getApplicationAttemptId()\n        .getApplicationId();\n    File dockerCommandFile;\n    String cmdDir = null;\n\n    if(nmContext.getLocalDirsHandler() == null) {\n      throw new ContainerExecutionException(\n          \"Unable to write temporary docker command\");\n    }\n\n    try {\n      cmdDir = nmContext.getLocalDirsHandler().getLocalPathForWrite(\n          ResourceLocalizationService.NM_PRIVATE_DIR + Path.SEPARATOR +\n          appId + Path.SEPARATOR + filePrefix + Path.SEPARATOR).toString();\n\n      dockerCommandFile = File.createTempFile(TMP_FILE_PREFIX + filePrefix,\n          TMP_FILE_SUFFIX, new File(cmdDir));\n\n      Writer writer = new OutputStreamWriter(\n          new FileOutputStream(dockerCommandFile.toString()), \"UTF-8\");\n      PrintWriter printWriter = new PrintWriter(writer);\n      printWriter.println(\"[docker-command-execution]\");\n      for (Map.Entry<String, List<String>> entry :\n          cmd.getDockerCommandWithArguments().entrySet()) {\n        if (entry.getKey().contains(\"=\")) {\n          throw new ContainerExecutionException(\n              \"'=' found in entry for docker command file, key = \" + entry\n                  .getKey() + \"; value = \" + entry.getValue());\n        }\n        if (entry.getValue().contains(\"\\n\")) {\n          throw new ContainerExecutionException(\n              \"'\\\\n' found in entry for docker command file, key = \" + entry\n                  .getKey() + \"; value = \" + entry.getValue());\n        }\n        printWriter.println(\"  \" + entry.getKey() + \"=\" + StringUtils\n            .join(\",\", entry.getValue()));\n      }\n      printWriter.close();\n\n      return dockerCommandFile.toString();\n    } catch (IOException e) {\n      LOG.warn(\"Unable to write docker command to \" + cmdDir);\n      throw new ContainerExecutionException(e);\n    }\n  }"
        }
    },
    {
        "filename": "YARN-3804.json",
        "creation_time": "2015-06-15T08:54:42.000+0000",
        "bug_report": {
            "Title": "Both RM are on Standby state when kerberos user not in yarn.admin.acl",
            "Description": "The issue arises when both Resource Managers (RMs) remain in a standby state indefinitely due to a permissions issue with the user 'yarn'. When the cluster is configured in secure mode and the 'yarn.admin.acl' is set to a value that does not include the 'yarn' user, attempts to transition the RMs to an active state fail. The stack trace indicates that the 'refreshAdminAcls' method cannot be executed due to an AccessControlException, leading to a ServiceFailedException.",
            "StackTrace": [
                "2015-06-15 12:20:21,556 WARN org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=yarn OPERATION=refreshAdminAcls TARGET=AdminService RESULT=FAILURE DESCRIPTION=Unauthorized userPERMISSIONS=",
                "2015-06-15 12:20:21,556 WARN org.apache.hadoop.ha.ActiveStandbyElector: Exception handling the winning of election",
                "org.apache.hadoop.ha.ServiceFailedException: RM could not transition to Active",
                "at org.apache.hadoop.yarn.server.resourcemanager.EmbeddedElectorService.becomeActive(EmbeddedElectorService.java:128)",
                "at org.apache.hadoop.ha.ActiveStandbyElector.becomeActive(ActiveStandbyElector.java:824)",
                "at org.apache.hadoop.ha.ActiveStandbyElector.processResult(ActiveStandbyElector.java:420)",
                "at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:645)",
                "at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:518)",
                "Caused by: org.apache.hadoop.ha.ServiceFailedException: Can not execute refreshAdminAcls",
                "at org.apache.hadoop.yarn.server.resourcemanager.AdminService.transitionToActive(AdminService.java:297)",
                "at org.apache.hadoop.yarn.server.resourcemanager.EmbeddedElectorService.becomeActive(EmbeddedElectorService.java:126)",
                "... 4 more",
                "Caused by: org.apache.hadoop.yarn.exceptions.YarnException: org.apache.hadoop.security.AccessControlException: User yarn doesn't have permission to call 'refreshAdminAcls'",
                "at org.apache.hadoop.yarn.ipc.RPCUtil.getRemoteException(RPCUtil.java:38)",
                "at org.apache.hadoop.yarn.server.resourcemanager.AdminService.checkAcls(AdminService.java:230)",
                "at org.apache.hadoop.yarn.server.resourcemanager.AdminService.refreshAdminAcls(AdminService.java:465)",
                "at org.apache.hadoop.yarn.server.resourcemanager.AdminService.transitionToActive(AdminService.java:295)",
                "... 5 more",
                "Caused by: org.apache.hadoop.security.AccessControlException: User yarn doesn't have permission to call 'refreshAdminAcls'",
                "at org.apache.hadoop.yarn.server.resourcemanager.RMServerUtils.verifyAdminAccess(RMServerUtils.java:182)",
                "at org.apache.hadoop.yarn.server.resourcemanager.RMServerUtils.verifyAdminAccess(RMServerUtils.java:148)",
                "at org.apache.hadoop.yarn.server.resourcemanager.AdminService.checkAccess(AdminService.java:223)",
                "at org.apache.hadoop.yarn.server.resourcemanager.AdminService.checkAccess(AdminService.java:228)",
                "... 7 more"
            ],
            "RootCause": "The root cause of the issue is that the user 'yarn' does not have the necessary permissions to call the 'refreshAdminAcls' method, as indicated by the AccessControlException in the stack trace. This prevents the Resource Managers from transitioning to an active state.",
            "StepsToReproduce": [
                "1. Configure the cluster in secure mode.",
                "2. Set yarn.admin.acl to a value that does not include the 'yarn' user (e.g., yarn.admin.acl=dsperf).",
                "3. Set yarn.resourcemanager.principal to 'yarn'.",
                "4. Start both Resource Managers."
            ],
            "ExpectedBehavior": "The Resource Managers should transition to an active state after a few retries or upon the first attempt, even if the user does not have the necessary permissions.",
            "ObservedBehavior": "Both Resource Managers remain in a standby state indefinitely, failing to transition to active due to permission issues.",
            "Suggestions": "Check the 'yarn-site.xml' configuration file to ensure that the 'yarn' user is included in the 'yarn.admin.acl' property. If not, add the 'yarn' user to the admin ACLs to resolve the permission issue.",
            "problem_location": {
                "files": [
                    "yarn-site.xml"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.AdminService",
                    "org.apache.hadoop.yarn.server.resourcemanager.RMServerUtils",
                    "org.apache.hadoop.yarn.security.YarnAuthorizationProvider"
                ],
                "methods": [
                    "AdminService.checkAccess",
                    "RMServerUtils.verifyAdminAccess",
                    "YarnAuthorizationProvider.isAdmin"
                ]
            },
            "possible_fix": "Ensure that the 'yarn' user is included in the 'yarn.admin.acl' property in the 'yarn-site.xml' configuration file. Example: yarn.admin.acl=dsperf,yarn."
        },
        "possible_fix_code": {
            "AdminService.checkAccess": "  private UserGroupInformation checkAccess(String method) throws IOException {\n    // Ensure that the 'yarn' user is included in the admin ACLs\n    if (!isUserInAdminAcl(UserGroupInformation.getCurrentUser())) {\n        throw new AccessControlException(\"User 'yarn' does not have permission to call 'refreshAdminAcls'\");\n    }\n    return RMServerUtils.verifyAdminAccess(authorizer, method, LOG);\n  }\n\n  private boolean isUserInAdminAcl(UserGroupInformation user) {\n      // Logic to check if the user is in the admin ACLs\n      // This is a placeholder for the actual implementation that checks the yarn.admin.acl configuration\n      return true; // Replace with actual check against the yarn.admin.acl\n  }"
        }
    },
    {
        "filename": "YARN-1839.json",
        "creation_time": "2014-03-14T23:52:29.000+0000",
        "bug_report": {
            "Title": "Capacity scheduler preempts an AM out. AM attempt 2 fails to launch task container with SecretManager$InvalidToken: No NMToken sent",
            "Description": "In a single-node cluster with capacity scheduler preemption enabled, running two MR sleep jobs leads to an issue where the Application Master (AM) for the first job fails to launch a task container after being preempted. The error occurs when the AM attempts to retrieve a Node Manager (NM) token, resulting in an 'InvalidToken' exception. This issue is likely related to the handling of NM tokens during the preemption process.",
            "StackTrace": [
                "2014-03-13 20:13:50,254 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: Diagnostics report from attempt_1394741557066_0001_m_000000_1009: Container launch failed for container_1394741557066_0001_02_000021 : org.apache.hadoop.security.token.SecretManager$InvalidToken: No NMToken sent for <host>:45454",
                "at org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy$ContainerManagementProtocolProxyData.newProxy(ContainerManagementProtocolProxy.java:206)",
                "at org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy$ContainerManagementProtocolProxyData.<init>(ContainerManagementProtocolProxy.java:196)",
                "at org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy.getProxy(ContainerManagementProtocolProxy.java:117)",
                "at org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl.getCMProxy(ContainerLauncherImpl.java:403)",
                "at org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl$Container.launch(ContainerLauncherImpl.java:138)",
                "at org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl$EventProcessor.run(ContainerLauncherImpl.java:369)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:722)"
            ],
            "RootCause": "The root cause of the issue is that the NMToken is not being sent or is invalid when the Application Master attempts to launch a task container after being preempted. This is likely due to misconfiguration, network issues, or problems with the token generation process.",
            "StepsToReproduce": [
                "1. Set up a single-node cluster with capacity scheduler preemption enabled.",
                "2. Run an MR sleep job as application 1, consuming the entire cluster resources.",
                "3. Run another MR sleep job as application 2.",
                "4. Allow application 2 to preempt application 1.",
                "5. Wait for application 2 to finish.",
                "6. Observe the logs of application 1's AM attempt 2 for errors."
            ],
            "ExpectedBehavior": "The Application Master for application 1 should successfully launch the task container after being preempted, utilizing a valid NMToken.",
            "ObservedBehavior": "The Application Master for application 1 fails to launch the task container, resulting in an 'InvalidToken' error due to the absence of a valid NMToken.",
            "Suggestions": "1. Check the YARN configuration files (e.g., yarn-site.xml) for correct settings related to security and token management. 2. Verify network connectivity between the ResourceManager and NodeManagers. 3. Ensure that the NMToken has not expired and refresh or regenerate it if necessary. 4. Review logs of the NodeManager and ResourceManager for additional error messages. 5. Restart YARN services to resolve transient issues.",
            "problem_location": {
                "files": [
                    "ContainerManagementProtocolProxy.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.client.api.impl.ContainerManagementProtocolProxy"
                ],
                "methods": [
                    "getProxy"
                ]
            },
            "possible_fix": "Ensure that the NMToken is correctly generated and sent during the preemption process. Review the implementation of the getProxy method in ContainerManagementProtocolProxy to ensure it handles token updates correctly."
        },
        "possible_fix_code": {
            "getProxy": "  public synchronized ContainerManagementProtocolProxyData getProxy(\n      String containerManagerBindAddr, ContainerId containerId)\n      throws InvalidToken {\n    \n    // This get call will update the map which is working as LRU cache.\n    ContainerManagementProtocolProxyData proxy =\n        cmProxy.get(containerManagerBindAddr);\n\n    // Ensure that the NMToken is valid and refresh if necessary\n    NMToken nmToken = nmTokenCache.getToken(containerManagerBindAddr);\n    if (nmToken == null) {\n        throw new InvalidToken(\"No NMToken found for: \" + containerManagerBindAddr);\n    }\n\n    while (proxy != null\n        && !proxy.token.getIdentifier().equals(nmToken.getIdentifier())) {\n      LOG.info(\"Refreshing proxy as NMToken got updated for node : \"\n          + containerManagerBindAddr);\n      // Token is updated. check if anyone has already tried closing it.\n      if (!proxy.scheduledForClose) {\n        // try closing the proxy. Here if someone is already using it\n        // then we might not close it. In which case we will wait.\n        removeProxy(proxy);\n      } else {\n        try {\n          this.wait();\n        } catch (InterruptedException e) {\n          e.printStackTrace();\n        }\n      }\n      if (proxy.activeCallers < 0) {\n        proxy = cmProxy.get(containerManagerBindAddr);\n      }\n    }\n    \n    if (proxy == null) {\n      proxy =\n          new ContainerManagementProtocolProxyData(rpc, containerManagerBindAddr,\n              containerId, nmToken);\n      if (cmProxy.size() > maxConnectedNMs) {\n        // Number of existing proxy exceed the limit.\n        String cmAddr = cmProxy.keySet().iterator().next();\n        removeProxy(cmProxy.get(cmAddr));\n      }\n      \n      cmProxy.put(containerManagerBindAddr, proxy);\n    }\n    // This is to track active users of this proxy.\n    proxy.activeCallers++;\n    updateLRUCache(containerManagerBindAddr);\n    \n    return proxy;\n  }"
        }
    },
    {
        "filename": "YARN-6714.json",
        "creation_time": "2017-06-15T09:56:15.000+0000",
        "bug_report": {
            "Title": "IllegalStateException while handling APP_ATTEMPT_REMOVED event when async-scheduling enabled in CapacityScheduler",
            "Description": "In the async-scheduling mode of the CapacityScheduler, an IllegalStateException is thrown during the handling of the APP_ATTEMPT_REMOVED event. This occurs after an Application Master (AM) failover when all reserved containers are unreserved. If an outdated reserve proposal for a failed application attempt is committed, it leads to a mismatch between the appAttemptId of the current application and the one being unreserved, resulting in a crash of the ResourceManager (RM). The error log indicates that the system is trying to unreserve resources for an application attempt that is not currently reserved, which violates the resource management logic.",
            "StackTrace": [
                "2017-06-08 11:02:24,339 FATAL [ResourceManager Event Processor] org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Error in handling event type APP_ATTEMPT_REMOVED to the scheduler",
                "java.lang.IllegalStateException: Trying to unreserve for application appattempt_1495188831758_0121_000002 when currently reserved for application application_1495188831758_0121 on node host: node1:45454 #containers=2 available=... used=...",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode.unreserveResource(FiCaSchedulerNode.java:123)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp.unreserve(FiCaSchedulerApp.java:845)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.completedContainer(LeafQueue.java:1787)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.completedContainerInternal(CapacityScheduler.java:1957)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler.completedContainer(AbstractYarnScheduler.java:586)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.doneApplicationAttempt(CapacityScheduler.java:966)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:1740)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:152)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:822)",
                "at java.lang.Thread.run(Thread.java:834)"
            ],
            "RootCause": "The root cause of the IllegalStateException is the attempt to unreserve resources for an application attempt that is not currently reserved. This occurs due to a flaw in the resource management logic, particularly in the lifecycle management of application attempts and their interaction with resource reservations.",
            "StepsToReproduce": [
                "Enable async-scheduling in the CapacityScheduler.",
                "Submit an application and allow it to reserve containers.",
                "Trigger a failover of the Application Master (AM).",
                "Unreserve all reserved containers for the application.",
                "Attempt to commit an outdated reserve proposal for the failed application attempt."
            ],
            "ExpectedBehavior": "The system should correctly handle the unreservation of resources without throwing an IllegalStateException, ensuring that only the current application attempt can unreserve its resources.",
            "ObservedBehavior": "An IllegalStateException is thrown, causing the ResourceManager to crash, indicating that the system is trying to unreserve resources for an application attempt that is not currently reserved.",
            "Suggestions": "Implement a check in the CapacityScheduler to ensure that the application attempt state is validated before committing any reserve proposals. This can be achieved by acquiring a write lock in the relevant methods to prevent race conditions.",
            "problem_location": {
                "files": [
                    "FiCaSchedulerNode.java",
                    "LeafQueue.java",
                    "CapacityScheduler.java",
                    "AbstractYarnScheduler.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode",
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue",
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler",
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler"
                ],
                "methods": [
                    "FiCaSchedulerNode.unreserveResource",
                    "FiCaSchedulerApp.unreserve",
                    "LeafQueue.completedContainer",
                    "CapacityScheduler.completedContainerInternal",
                    "AbstractYarnScheduler.completedContainer"
                ]
            },
            "possible_fix": "Add a validation check in the unreserveResource method of FiCaSchedulerNode to ensure that the application attempting to unreserve is the same as the one currently holding the reservation. Additionally, implement locking mechanisms in the doneApplicationAttempt and tryCommit methods to prevent outdated proposals from being committed."
        },
        "possible_fix_code": {
            "FiCaSchedulerNode.unreserveResource": "  public synchronized void unreserveResource(\n      SchedulerApplicationAttempt application) {\n    // adding NP checks as this can now be called for preemption\n    if (getReservedContainer() != null\n        && getReservedContainer().getContainer() != null\n        && getReservedContainer().getContainer().getId() != null\n        && getReservedContainer().getContainer().getId()\n          .getApplicationAttemptId() != null) {\n\n      // Cannot unreserve for wrong application...\n      ApplicationAttemptId reservedApplication =\n          getReservedContainer().getContainer().getId()\n            .getApplicationAttemptId();\n      if (!reservedApplication.equals(\n          application.getApplicationAttemptId())) {\n        throw new IllegalStateException(\"Trying to unreserve \" +\n            \" for application \" + application.getApplicationAttemptId() +\n            \" when currently reserved \" +\n            \" for application \" + reservedApplication.getApplicationId() +\n            \" on node \" + this);\n      }\n    }\n    // New validation check to ensure the application is still valid\n    if (!isApplicationValid(application)) {\n        throw new IllegalStateException(\"Cannot unreserve for an invalid application attempt: \" + application.getApplicationAttemptId());\n    }\n    setReservedContainer(null);\n  }\n\n  private boolean isApplicationValid(SchedulerApplicationAttempt application) {\n      // Implement logic to check if the application is still valid\n      // This could involve checking the application state or other criteria\n      return true; // Placeholder for actual validation logic\n  }"
        }
    },
    {
        "filename": "YARN-3351.json",
        "creation_time": "2015-03-16T14:19:59.000+0000",
        "bug_report": {
            "Title": "AppMaster tracking URL is broken in HA",
            "Description": "After the implementation of YARN-2713, the AppMaster link fails to function correctly in High Availability (HA) mode. The issue manifests when attempting to access the tracking URL for a long-running job while the Resource Manager (RM) is in HA mode, specifically when the first RM is not active. The error logged indicates a failure to bind a socket, which is critical for establishing the connection to the tracking URL.",
            "StackTrace": [
                "2015-02-05 20:47:43,478 WARN org.mortbay.log: /proxy/application_1423182188062_0002/: java.net.BindException: Cannot assign requested address",
                "java.net.BindException: Cannot assign requested address",
                "at java.net.PlainSocketImpl.socketBind(Native Method)",
                "at java.net.AbstractPlainSocketImpl.bind(AbstractPlainSocketImpl.java:376)",
                "at java.net.Socket.bind(Socket.java:631)",
                "at java.net.Socket.<init>(Socket.java:423)",
                "at java.net.Socket.<init>(Socket.java:280)",
                "at org.apache.commons.httpclient.protocol.DefaultProtocolSocketFactory.createSocket(DefaultProtocolSocketFactory.java:80)",
                "at org.apache.commons.httpclient.protocol.DefaultProtocolSocketFactory.createSocket(DefaultProtocolSocketFactory.java:122)",
                "at org.apache.commons.httpclient.HttpConnection.open(HttpConnection.java:707)",
                "at org.apache.commons.httpclient.HttpMethodDirector.executeWithRetry(HttpMethodDirector.java:387)",
                "at org.apache.commons.httpclient.HttpMethodDirector.executeMethod(HttpMethodDirector.java:171)",
                "at org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:397)",
                "at org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:346)",
                "at org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet.proxyLink(WebAppProxyServlet.java:188)",
                "at org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet.doGet(WebAppProxyServlet.java:345)",
                "at javax.servlet.http.HttpServlet.service(HttpServlet.java:707)",
                "at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)",
                "at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1221)"
            ],
            "RootCause": "The root cause of the issue is a `java.net.BindException`, which indicates that the application is unable to bind to the requested address. This can occur due to several reasons, including the address being already in use, the address being invalid, or insufficient permissions to bind to the address.",
            "StepsToReproduce": [
                "Set up Resource Manager (RM) High Availability (HA) with the first RM not active.",
                "Run a long sleep job.",
                "Attempt to view the tracking URL on the RM applications page."
            ],
            "ExpectedBehavior": "The tracking URL should be accessible and display the job's status and details without any binding errors.",
            "ObservedBehavior": "The application fails to bind to the requested address, resulting in a `java.net.BindException`, and the tracking URL is not accessible.",
            "Suggestions": "To resolve the `java.net.BindException`, check for the following: ensure that the address is valid and not already in use, verify that the application has the necessary permissions to bind to the address, and review firewall settings that may be blocking the binding.",
            "problem_location": {
                "files": [
                    "WebAppProxyServlet.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet"
                ],
                "methods": [
                    "proxyLink",
                    "doGet"
                ]
            },
            "possible_fix": "Investigate the network configuration and ensure that the address being used for binding is valid and available. If necessary, modify the application to use a different address or port that is not in use."
        },
        "possible_fix_code": {
            "proxyLink": "  private static void proxyLink(HttpServletRequest req, \n      HttpServletResponse resp, URI link, Cookie c, String proxyHost)\n      throws IOException {\n    DefaultHttpClient client = new DefaultHttpClient();\n    client\n        .getParams()\n        .setParameter(ClientPNames.COOKIE_POLICY,\n            CookiePolicy.BROWSER_COMPATIBILITY)\n        .setBooleanParameter(ClientPNames.ALLOW_CIRCULAR_REDIRECTS, true);\n    // Make sure we send the request from the proxy address in the config\n    // since that is what the AM filter checks against. IP aliasing or\n    // similar could cause issues otherwise.\n    InetAddress localAddress = InetAddress.getByName(proxyHost);\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"local InetAddress for proxy host: {}\", localAddress);\n    }\n    client.getParams()\n        .setParameter(ConnRoutePNames.LOCAL_ADDRESS, localAddress);\n    HttpGet httpGet = new HttpGet(link);\n    @SuppressWarnings(\"unchecked\")\n    Enumeration<String> names = req.getHeaderNames();\n    while(names.hasMoreElements()) {\n      String name = names.nextElement();\n      if(passThroughHeaders.contains(name)) {\n        String value = req.getHeader(name);\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"REQ HEADER: {} : {}\", name, value);\n        }\n        httpGet.setHeader(name, value);\n      }\n    }\n\n    String user = req.getRemoteUser();\n    if (user != null && !user.isEmpty()) {\n      httpGet.setHeader(\"Cookie\",\n          PROXY_USER_COOKIE_NAME + \"=\" + URLEncoder.encode(user, \"ASCII\"));\n    }\n    OutputStream out = resp.getOutputStream();\n    try {\n      HttpResponse httpResp = client.execute(httpGet);\n      resp.setStatus(httpResp.getStatusLine().getStatusCode());\n      for (Header header : httpResp.getAllHeaders()) {\n        resp.setHeader(header.getName(), header.getValue());\n      }\n      if (c != null) {\n        resp.addCookie(c);\n      }\n      InputStream in = httpResp.getEntity().getContent();\n      if (in != null) {\n        IOUtils.copyBytes(in, out, 4096, true);\n      }\n    } catch (BindException e) {\n      LOG.error(\"BindException occurred: {}\", e.getMessage());\n      throw new IOException(\"Failed to bind to the requested address. Please check the network configuration.\", e);\n    } finally {\n      httpGet.releaseConnection();\n    }\n  }"
        }
    },
    {
        "filename": "YARN-2813.json",
        "creation_time": "2014-11-05T22:29:46.000+0000",
        "bug_report": {
            "Title": "NPE from MemoryTimelineStore.getDomains",
            "Description": "A NullPointerException occurs when invoking the getDomains method in the TimelineWebServices class, which is part of the Apache Hadoop YARN framework. The exception is propagated from the MemoryTimelineStore.getDomains method, indicating an attempt to access an object or variable that has not been initialized.",
            "StackTrace": [
                "2014-11-04 20:50:05,146 WARN org.apache.hadoop.yarn.webapp.GenericExceptionHandler: INTERNAL_SERVER_ERROR",
                "javax.ws.rs.WebApplicationException: java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.timeline.webapp.TimelineWebServices.getDomains(TimelineWebServices.java:356)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at com.sun.jersey.spi.container.JavaMethodInvokerFactory$1.invoke(JavaMethodInvokerFactory.java:60)",
                "at com.sun.jersey.server.impl.model.method.dispatch.AbstractResourceMethodDispatchProvider$TypeOutInvoker._dispatch(AbstractResourceMethodDispatchProvider.java:185)",
                "at com.sun.jersey.server.impl.model.method.dispatch.ResourceJavaMethodDispatcher.dispatch(ResourceJavaMethodDispatcher.java:75)",
                "at com.sun.jersey.server.impl.uri.rules.HttpMethodRule.accept(HttpMethodRule.accept(HttpMethodRule.java:288)",
                "at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.accept(RightHandPathRule.java:147)",
                "at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.accept(ResourceClassRule.java:108)",
                "at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.accept(RightHandPathRule.java:147)",
                "at com.sun.jersey.server.impl.uri.rules.ResourceClassRule.accept(ResourceClassRule.accept(ResourceClassRule.java:108)",
                "at com.sun.jersey.server.impl.uri.rules.RightHandPathRule.accept(RightHandPathRule.accept(RightHandPathRule.java:147)",
                "at com.sun.jersey.server.impl.uri.rules.RootResourceClassesRule.accept(RootResourceClassesRule.accept(RootResourceClassesRule.java:84)",
                "at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1469)",
                "at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1400)",
                "at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1349)",
                "at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1339)",
                "at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:416)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:537)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:886)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:834)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:795)",
                "at com.google.inject.servlet.FilterDefinition.doFilter(FilterDefinition.java:163)",
                "at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:58)",
                "at com.google.inject.servlet.ManagedFilterPipeline.dispatch(ManagedFilterPipeline.java:118)",
                "at com.google.inject.servlet.GuiceFilter.doFilter(GuiceFilter.java:113)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "at org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter.doFilter(StaticUserWebFilter.java:96)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:572)",
                "at org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter.doFilter(DelegationTokenAuthenticationFilter.java:269)",
                "at org.apache.hadoop.security.authentication.server.AuthenticationFilter.doFilter(AuthenticationFilter.java:542)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "at org.apache.hadoop.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1204)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "at org.apache.hadoop.http.NoCacheFilter.doFilter(NoCacheFilter.java:45)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)",
                "at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)",
                "at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)",
                "at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)",
                "at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)",
                "at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)",
                "at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)",
                "at org.mortbay.jetty.Server.handle(Server.java:326)",
                "at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)",
                "at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)",
                "at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)",
                "at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)",
                "at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)",
                "at org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228)",
                "at org.mortbay.jetty.security.SslSocketConnector$SslConnection.run(SslSocketConnector.java:713)",
                "at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)",
                "Caused by: java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.timeline.MemoryTimelineStore.getDomains(MemoryTimelineStore.java:244)",
                "at org.apache.hadoop.yarn.server.timeline.TimelineDataManager.getDomains(TimelineDataManager.java:383)",
                "at org.apache.hadoop.yarn.server.timeline.webapp.TimelineWebServices.getDomains(TimelineWebServices.java:353)",
                "... 54 more"
            ],
            "RootCause": "The NullPointerException is likely caused by the domainsByOwner map not containing the specified owner, resulting in a null value when attempting to iterate over it.",
            "StepsToReproduce": [
                "Invoke the getDomains method on the TimelineWebServices class without a valid owner parameter.",
                "Ensure that the domainsByOwner map in MemoryTimelineStore is not populated with the specified owner."
            ],
            "ExpectedBehavior": "The getDomains method should return a list of TimelineDomain objects associated with the specified owner, or an empty list if the owner does not exist.",
            "ObservedBehavior": "A NullPointerException is thrown when attempting to access the domainsByOwner map for an owner that does not exist.",
            "Suggestions": "Implement a check in the getDomains method to verify if the domainsByOwner map contains the specified owner before attempting to access it.",
            "problem_location": {
                "files": [
                    "MemoryTimelineStore.java",
                    "TimelineWebServices.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.timeline.MemoryTimelineStore",
                    "org.apache.hadoop.yarn.server.timeline.webapp.TimelineWebServices"
                ],
                "methods": [
                    "MemoryTimelineStore.getDomains",
                    "TimelineWebServices.getDomains"
                ]
            },
            "possible_fix": "In the MemoryTimelineStore.getDomains method, add a check to see if the domainsByOwner map contains the specified owner. If not, return an empty list or throw a more informative exception."
        },
        "possible_fix_code": {
            "MemoryTimelineStore.getDomains": "  @Override\n  public synchronized TimelineDomains getDomains(String owner)\n      throws IOException {\n    List<TimelineDomain> domains = new ArrayList<TimelineDomain>();\n    // Check if the owner exists in the domainsByOwner map\n    if (!domainsByOwner.containsKey(owner)) {\n      return new TimelineDomains(); // Return an empty list if the owner does not exist\n    }\n    for (TimelineDomain domain : domainsByOwner.get(owner)) {\n      TimelineDomain domainToReturn = createTimelineDomain(\n          domain.getId(),\n          domain.getDescription(),\n          domain.getOwner(),\n          domain.getReaders(),\n          domain.getWriters(),\n          domain.getCreatedTime(),\n          domain.getModifiedTime());\n      domains.add(domainToReturn);\n    }\n    Collections.sort(domains, new Comparator<TimelineDomain>() {\n      @Override\n      public int compare(\n          TimelineDomain domain1, TimelineDomain domain2) {\n         int result = domain2.getCreatedTime().compareTo(\n             domain1.getCreatedTime());\n         if (result == 0) {\n           return domain2.getModifiedTime().compareTo(\n               domain1.getModifiedTime());\n         } else {\n           return result;\n         }\n      }\n    });\n    TimelineDomains domainsToReturn = new TimelineDomains();\n    domainsToReturn.addDomains(domains);\n    return domainsToReturn;\n  }"
        }
    },
    {
        "filename": "YARN-1550.json",
        "creation_time": "2013-12-30T03:58:32.000+0000",
        "bug_report": {
            "Title": "NPE in FairSchedulerAppsBlock#render",
            "Description": "The application encounters a NullPointerException (NPE) in the `FairSchedulerAppsBlock.render` method when attempting to render the application data on the scheduler page. This issue arises after submitting an application, leading to a 500 error on the web interface. The root cause appears to be related to uninitialized or improperly initialized components within the `FairSchedulerAppsBlock` class, specifically the `apps` map and `fsinfo` object.",
            "StackTrace": [
                "2013-12-30 11:51:43,795 ERROR org.apache.hadoop.yarn.webapp.Dispatcher: error handling URI: /cluster/scheduler",
                "java.lang.reflect.InvocationTargetException",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "Caused by: java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.webapp.FairSchedulerAppsBlock.render(FairSchedulerAppsBlock.java:96)",
                "at org.apache.hadoop.yarn.webapp.view.HtmlBlock.render(HtmlBlock.java:66)",
                "at org.apache.hadoop.yarn.webapp.view.HtmlBlock.renderPartial(HtmlBlock.java:76)"
            ],
            "RootCause": "The NullPointerException in the `FairSchedulerAppsBlock.render` method is likely caused by the `apps` map or `fsinfo` being null, which indicates improper initialization of the `RMContext` or `ResourceManager`. Specifically, if `rmContext.getRMApps()` returns null, the `apps` map will not be populated, leading to the NPE when the method attempts to iterate over it.",
            "StepsToReproduce": [
                "1. Debug at RMAppManager#submitApplication after the code that checks for existing applications.",
                "2. Submit an application using the command: hadoop jar ~/hadoop-current/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.0.0-ydh2.2.0-tests.jar sleep -Dhadoop.job.ugi=test2,#111111 -Dmapreduce.job.queuename=p1 -m 1 -mt 1 -r 1",
                "3. Navigate to the page: http://ip:50030/cluster/scheduler and observe the 500 ERROR."
            ],
            "ExpectedBehavior": "The application data should be rendered correctly on the scheduler page without any errors, displaying the list of applications and their statuses.",
            "ObservedBehavior": "A 500 error is displayed on the scheduler page, indicating that a NullPointerException occurred during the rendering process.",
            "Suggestions": "Check the initialization of the `RMContext` and ensure that it is properly set up to provide a valid `RMApps` map. Additionally, verify that the `ResourceManager` is correctly initialized to return a valid `FairScheduler` instance.",
            "problem_location": {
                "files": [
                    "FairSchedulerAppsBlock.java",
                    "RMContext.java",
                    "ResourceManager.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.webapp.FairSchedulerAppsBlock",
                    "org.apache.hadoop.yarn.server.resourcemanager.RMContext",
                    "org.apache.hadoop.yarn.server.resourcemanager.ResourceManager"
                ],
                "methods": [
                    "FairSchedulerAppsBlock.render",
                    "RMContext.getRMApps",
                    "ResourceManager.getResourceScheduler"
                ]
            },
            "possible_fix": "Ensure that the `RMContext` is properly initialized in the `ResourceManager`'s `serviceInit` method. Specifically, check that `rmContext.getRMApps()` does not return null and that the `FairScheduler` is correctly instantiated and assigned to the `fsinfo` variable."
        },
        "possible_fix_code": {
            "FairSchedulerAppsBlock.render": "  @Override public void render(Block html) {\n    TBODY<TABLE<Hamlet>> tbody = html.\n      table(\"#apps\").\n        thead().\n          tr().\n            th(\".id\", \"ID\").\n            th(\".user\", \"User\").\n            th(\".name\", \"Name\").\n            th(\".queue\", \"Queue\").\n            th(\".fairshare\", \"Fair Share\").\n            th(\".starttime\", \"StartTime\").\n            th(\".finishtime\", \"FinishTime\").\n            th(\".state\", \"State\").\n            th(\".finalstatus\", \"FinalStatus\").\n            th(\".progress\", \"Progress\").\n            th(\".ui\", \"Tracking UI\")._()._().\n        tbody();\n    Collection<YarnApplicationState> reqAppStates = null;\n    String reqStateString = $(APP_STATE);\n    if (reqStateString != null && !reqStateString.isEmpty()) {\n      String[] appStateStrings = reqStateString.split(\",\");\n      reqAppStates = new HashSet<YarnApplicationState>(appStateStrings.length);\n      for(String stateString : appStateStrings) {\n        reqAppStates.add(YarnApplicationState.valueOf(stateString));\n      }\n    }\n    StringBuilder appsTableData = new StringBuilder(\"[\\n\");\n    if (apps != null) {\n      for (RMApp app : apps.values()) {\n        if (reqAppStates != null && !reqAppStates.contains(app.getState())) {\n          continue;\n        }\n        AppInfo appInfo = new AppInfo(app, true);\n        String percent = String.format(\"%.1f\", appInfo.getProgress());\n        ApplicationAttemptId attemptId = app.getCurrentAppAttempt() != null ? app.getCurrentAppAttempt().getAppAttemptId() : null;\n        int fairShare = (attemptId != null && fsinfo != null) ? fsinfo.getAppFairShare(attemptId) : 0;\n        appsTableData.append(\"[\\\"<a href='\")\n          .append(url(\"app\", appInfo.getAppId())).append(\"'>\")\n          .append(appInfo.getAppId()).append(\"</a>\\\",\\\"\")\n          .append(StringEscapeUtils.escapeJavaScript(StringEscapeUtils.escapeHtml(\n            appInfo.getUser()))).append(\"\\\",\\\"\")\n          .append(StringEscapeUtils.escapeJavaScript(StringEscapeUtils.escapeHtml(\n            appInfo.getName()))).append(\"\\\",\\\"\")\n          .append(StringEscapeUtils.escapeJavaScript(StringEscapeUtils.escapeHtml(\n            appInfo.getQueue()))).append(\"\\\",\\\"\")\n          .append(fairShare).append(\"\\\",\\\"\")\n          .append(appInfo.getStartTime()).append(\"\\\",\\\"\")\n          .append(appInfo.getFinishTime()).append(\"\\\",\\\"\")\n          .append(appInfo.getState()).append(\"\\\",\\\"\")\n          .append(appInfo.getFinalStatus()).append(\"\\\",\\\"\")\n          .append(\"<br title='\").append(percent)\n          .append(\"'> <div class='\").append(C_PROGRESSBAR).append(\"' title='\")\n          .append(join(percent, '%')).append(\"'> <div class='\")\n          .append(C_PROGRESSBAR_VALUE).append(\"' style='\")\n          .append(join(\"width:\", percent, '%')).append(\"'> </div> </div>\")\n          .append(\"\\\",\\\"<a href='\");\n\n        String trackingURL =\n          !appInfo.isTrackingUrlReady()? \"#\" : appInfo.getTrackingUrlPretty();\n        appsTableData.append(trackingURL).append(\"'>\")\n          .append(appInfo.getTrackingUI()).append(\"</a>\\\"],\\n\");\n      }\n    }\n    if(appsTableData.charAt(appsTableData.length() - 2) == ',') {\n      appsTableData.delete(appsTableData.length()-2, appsTableData.length()-1);\n    }\n    appsTableData.append(\"]\");\n    html.script().$type(\"text/javascript\").\n    _(\"var appsTableData=\" + appsTableData)._();\n\n    tbody._()._();\n  }"
        }
    },
    {
        "filename": "YARN-5006.json",
        "creation_time": "2016-04-28T08:26:38.000+0000",
        "bug_report": {
            "Title": "ResourceManager quit due to ApplicationStateData exceeding the limit size of znode in ZooKeeper",
            "Description": "When a client submits a job that adds 10,000 files into the DistributedCache, the ResourceManager attempts to store the ApplicationStateData in ZooKeeper. However, the size of the ApplicationStateData exceeds the maximum allowed size for a znode in ZooKeeper, leading to a failure in storing the application state and causing the ResourceManager to exit unexpectedly. The issue is compounded by a ConnectionLossException, indicating that the ResourceManager lost connection to the ZooKeeper server during this operation.",
            "StackTrace": [
                "2016-04-20 11:26:35,732 ERROR org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore AsyncDispatcher event handler: Error storing app: application_1461061795989_17671",
                "org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss",
                "at org.apache.zookeeper.KeeperException.create(KeeperException.java:99)",
                "at org.apache.zookeeper.ZooKeeper.multiInternal(ZooKeeper.java:931)",
                "at org.apache.zookeeper.ZooKeeper.multi(ZooKeeper.java:911)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore$4.run(ZKRMStateStore.java:936)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore$4.run(ZKRMStateStore.java:933)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore.storeApplicationStateInternal(ZKRMStateStore.java:626)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$StoreAppTransition.transition(RMStateStore.java:138)"
            ],
            "RootCause": "The ResourceManager is unable to store the ApplicationStateData in ZooKeeper due to exceeding the maximum znode size limit, which triggers a ConnectionLossException when the connection to the ZooKeeper server is lost during the operation.",
            "StepsToReproduce": [
                "1. Submit a job that adds 10,000 files to the DistributedCache.",
                "2. Monitor the ResourceManager logs for errors related to ApplicationStateData storage.",
                "3. Observe the ResourceManager exiting with a ConnectionLossException."
            ],
            "ExpectedBehavior": "The ResourceManager should successfully store the ApplicationStateData in ZooKeeper without exceeding the znode size limit, allowing the application to continue running without interruption.",
            "ObservedBehavior": "The ResourceManager fails to store the ApplicationStateData due to exceeding the znode size limit, resulting in a ConnectionLossException and causing the ResourceManager to exit.",
            "Suggestions": "To mitigate this issue, consider reducing the size of the ApplicationStateData being stored or increasing the znode size limit in ZooKeeper if possible. Additionally, ensure that the ZooKeeper server is stable and properly configured to handle the expected load.",
            "problem_location": {
                "files": [
                    "RMStateStore.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore"
                ],
                "methods": [
                    "storeApplicationStateInternal",
                    "transition"
                ]
            },
            "possible_fix": "Review the implementation of the ApplicationStateData to ensure it does not exceed the znode size limit. If necessary, refactor the data structure to store less information or split the data across multiple znodes."
        },
        "possible_fix_code": {
            "storeApplicationStateInternal": "  public synchronized void storeApplicationStateInternal(ApplicationId appId,\n      ApplicationStateData appStateDataPB) throws Exception {\n    String nodeCreatePath = getNodePath(rmAppRoot, appId.toString());\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Storing info for app: \" + appId + \" at: \" + nodeCreatePath);\n    }\n    byte[] appStateData = appStateDataPB.getProto().toByteArray();\n\n    // Check the size of the application state data before storing\n    if (appStateData.length > MAX_ZNODE_SIZE) {\n      LOG.error(\"ApplicationStateData exceeds the maximum znode size limit.\");\n      throw new Exception(\"ApplicationStateData exceeds the maximum znode size limit.\");\n    }\n\n    safeCreate(nodeCreatePath, appStateData, zkAcl,\n        CreateMode.PERSISTENT);\n  }"
        }
    },
    {
        "filename": "YARN-5728.json",
        "creation_time": "2016-10-13T05:16:28.000+0000",
        "bug_report": {
            "Title": "TestMiniYarnClusterNodeUtilization.testUpdateNodeUtilization timeout",
            "Description": "The test `TestMiniYARNClusterNodeUtilization.testUpdateNodeUtilization` is failing due to a timeout after 60 seconds. This issue appears to be related to the YARN framework's node heartbeat mechanism, where the test is unable to receive a response in the expected timeframe. The stack trace indicates that the timeout occurs during a sleep operation, suggesting that the test is waiting for a node heartbeat that is not being received.",
            "StackTrace": [
                "java.lang.Exception: test timed out after 60000 milliseconds",
                "at java.lang.Thread.sleep(Native Method)",
                "at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.processWaitTimeAndRetryInfo(RetryInvocationHandler.java:130)",
                "at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:107)",
                "at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:335)",
                "at com.sun.proxy.$Proxy85.nodeHeartbeat(Unknown Source)",
                "at org.apache.hadoop.yarn.server.TestMiniYarnClusterNodeUtilization.testUpdateNodeUtilization(TestMiniYarnClusterNodeUtilization.java:113)"
            ],
            "RootCause": "The test is timing out because it is waiting for a node heartbeat response that is not being received within the expected timeframe, likely due to network issues, overloaded nodes, or misconfigurations in the YARN setup.",
            "StepsToReproduce": [
                "Run the test `TestMiniYARNClusterNodeUtilization.testUpdateNodeUtilization` in the YARN framework.",
                "Observe the timeout after 60 seconds."
            ],
            "ExpectedBehavior": "The test should complete successfully within the allotted time, receiving timely node heartbeat responses.",
            "ObservedBehavior": "The test fails with a timeout exception after 60 seconds, indicating that it did not receive the expected node heartbeat response.",
            "Suggestions": "Investigate the YARN configuration for potential misconfigurations, check the network connectivity between nodes, and ensure that nodes are not overloaded. Consider increasing the timeout duration as a temporary workaround.",
            "problem_location": {
                "files": [
                    "TestMiniYarnClusterNodeUtilization.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.TestMiniYarnClusterNodeUtilization"
                ],
                "methods": [
                    "testUpdateNodeUtilization"
                ]
            },
            "possible_fix": "Review the YARN node heartbeat configuration and ensure that nodes are properly configured to respond in a timely manner. If necessary, adjust the retry and timeout settings in the YARN configuration."
        },
        "possible_fix_code": {
            "testUpdateNodeUtilization": "public void testUpdateNodeUtilization() throws Exception {\n    // Increase the timeout duration to allow more time for node heartbeats\n    final long timeout = 120000; // 120 seconds\n    // Set up the test environment and start the YARN cluster\n    MiniYARNCluster miniCluster = new MiniYARNCluster(\"TestMiniYARNClusterNodeUtilization\");\n    miniCluster.init(new YarnConfiguration());\n    miniCluster.start();\n\n    try {\n        // Simulate node utilization update\n        NodeHeartbeatResponse response = miniCluster.getResourceManager().getNodeHeartbeatService().nodeHeartbeat(...);\n        // Validate the response\n        assertNotNull(response);\n        // Additional assertions can be added here\n    } finally {\n        miniCluster.stop();\n    }\n}"
        }
    },
    {
        "filename": "YARN-2805.json",
        "creation_time": "2014-11-04T20:37:09.000+0000",
        "bug_report": {
            "Title": "RM2 in HA setup tries to login using the RM1's kerberos principal",
            "Description": "The ResourceManager (RM) in a Hadoop YARN high-availability (HA) setup is encountering a login failure due to issues with the Kerberos authentication process. The error indicates that RM2 is attempting to log in using the Kerberos principal associated with RM1, which is not valid in this context. The stack trace reveals that the login process fails when trying to access the keytab file, leading to a YarnRuntimeException.",
            "StackTrace": [
                "2014-11-04 08:41:08,705 INFO  resourcemanager.ResourceManager (SignalLogger.java:register(91)) - registered UNIX signal handlers for [TERM, HUP, INT]",
                "2014-11-04 08:41:10,636 INFO  service.AbstractService (AbstractService.java:noteFailure(272)) - Service ResourceManager failed in state INITED; cause: org.apache.hadoop.yarn.exceptions.YarnRuntimeException: Failed to login",
                "org.apache.hadoop.yarn.exceptions.YarnRuntimeException: Failed to login",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceInit(ResourceManager.java:211)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:1229)",
                "Caused by: java.io.IOException: Login failure for rm/IP@EXAMPLE.COM from keytab /etc/security/keytabs/rm.service.keytab: javax.security.auth.login.LoginException: Unable to obtain password from user",
                "at org.apache.hadoop.security.UserGroupInformation.loginUserFromKeytab(UserGroupInformation.java:935)"
            ],
            "RootCause": "The root cause of the issue is the failure of the ResourceManager to log in due to an inability to obtain a password from the specified keytab file. This is likely caused by incorrect permissions on the keytab file, an invalid keytab, or misconfiguration in the Kerberos setup.",
            "StepsToReproduce": [
                "Set up a Hadoop YARN high-availability environment with multiple ResourceManagers.",
                "Ensure that RM2 is configured to use the keytab file located at /etc/security/keytabs/rm.service.keytab.",
                "Attempt to start RM2 and observe the login failure in the logs."
            ],
            "ExpectedBehavior": "The ResourceManager should successfully log in using its own Kerberos principal and keytab file, allowing it to initialize and operate without errors.",
            "ObservedBehavior": "The ResourceManager fails to log in, resulting in a YarnRuntimeException and preventing it from starting up properly.",
            "Suggestions": "Check the permissions of the keytab file to ensure that the user running the ResourceManager has read access. Verify that the keytab file is valid and contains the correct principal for RM2. Additionally, review the Kerberos configuration and JAAS setup to ensure they are correctly defined.",
            "problem_location": {
                "files": [
                    "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/UserGroupInformation.java"
                ],
                "classes": [
                    "org.apache.hadoop.security.UserGroupInformation"
                ],
                "methods": [
                    "UserGroupInformation.loginUserFromKeytab",
                    "UserGroupInformation.newLoginContext"
                ]
            },
            "possible_fix": "Ensure that the keytab file at /etc/security/keytabs/rm.service.keytab has the correct permissions (e.g., 400) and contains the correct principal for RM2. Validate the Kerberos configuration (krb5.conf) and ensure that the JAAS configuration is set up to use the HadoopLoginModule correctly."
        },
        "possible_fix_code": {
            "UserGroupInformation.loginUserFromKeytab": "  static void loginUserFromKeytab(String user,\n                                  String path\n                                  ) throws IOException {\n    if (!isSecurityEnabled())\n      return;\n\n    keytabFile = path;\n    keytabPrincipal = user;\n    Subject subject = new Subject();\n    LoginContext login; \n    long start = 0;\n    try {\n      // Ensure the keytab file has the correct permissions and is valid\n      if (!isKeytabValid(path)) {\n        throw new IOException(\"Invalid keytab file: \" + path);\n      }\n      login = newLoginContext(HadoopConfiguration.KEYTAB_KERBEROS_CONFIG_NAME,\n            subject, new HadoopConfiguration());\n      start = Time.now();\n      login.login();\n      metrics.loginSuccess.add(Time.now() - start);\n      loginUser = new UserGroupInformation(subject);\n      loginUser.setLogin(login);\n      loginUser.setAuthenticationMethod(AuthenticationMethod.KERBEROS);\n    } catch (LoginException le) {\n      if (start > 0) {\n        metrics.loginFailure.add(Time.now() - start);\n      }\n      throw new IOException(\"Login failure for \" + user + \" from keytab \" + \n                            path + \": \" + le, le);\n    }\n    LOG.info(\"Login successful for user \" + keytabPrincipal\n        + \" using keytab file \" + keytabFile);\n  }\n\n  private static boolean isKeytabValid(String path) {\n    // Implement logic to check keytab file permissions and validity\n    File keytabFile = new File(path);\n    return keytabFile.exists() && keytabFile.canRead();\n  }"
        }
    },
    {
        "filename": "YARN-4744.json",
        "creation_time": "2016-02-29T10:08:57.000+0000",
        "bug_report": {
            "Title": "Too many signal to container failure in case of LCE",
            "Description": "The issue arises when running a MapReduce application (terasort/teragen) in a secure HA cluster with LCE enabled. The server is started with the dsperf user, and upon submission of the application, multiple signals are sent to the container, leading to failures. The logs indicate that a privileged operation to signal the container failed, resulting in an ExitCodeException with exit code 9, which typically signifies that the process was killed, possibly due to an out-of-memory (OOM) condition or manual termination.",
            "StackTrace": [
                "2014-03-02 09:20:43,081 WARN org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DefaultLinuxContainerRuntime: Signal container failed. Exception:",
                "org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationException: ExitCodeException exitCode=9:",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationExecutor.executePrivilegedOperation(PrivilegedOperationExecutor.java:173)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DefaultLinuxContainerRuntime.signalContainer(DefaultLinuxContainerRuntime.java:132)",
                "at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.signalContainer(LinuxContainerExecutor.java:513)"
            ],
            "RootCause": "The root cause of the issue is a failure in executing a privileged operation to signal a container, which resulted in an ExitCodeException with exit code 9. This indicates that the process was likely killed due to an out-of-memory condition or manual termination.",
            "StepsToReproduce": [
                "Install HA cluster in secure mode.",
                "Enable LCE with cgroups.",
                "Start the server with the dsperf user.",
                "Submit a MapReduce application (terasort/teragen) with the user yarn/dsperf."
            ],
            "ExpectedBehavior": "The container should successfully execute the MapReduce application without sending excessive signals or encountering failures.",
            "ObservedBehavior": "The application fails with multiple signals sent to the container, resulting in a failure to execute the privileged operation and an exit code of 9.",
            "Suggestions": "Investigate the resource allocation for the container to ensure it is not exceeding memory limits. Check the logs for any indications of manual termination or misconfiguration. Adjust the container's memory settings if necessary.",
            "problem_location": {
                "files": [
                    "PrivilegedOperationExecutor.java",
                    "DefaultLinuxContainerRuntime.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationExecutor",
                    "org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DefaultLinuxContainerRuntime"
                ],
                "methods": [
                    "PrivilegedOperationExecutor.executePrivilegedOperation",
                    "DefaultLinuxContainerRuntime.signalContainer"
                ]
            },
            "possible_fix": "Review and potentially increase the memory allocation for the container. Ensure that the environment is correctly configured to prevent OOM conditions. Consider adding logging around the resource usage to better diagnose the issue."
        },
        "possible_fix_code": {
            "DefaultLinuxContainerRuntime.signalContainer": "  public void signalContainer(ContainerRuntimeContext ctx)\n      throws ContainerExecutionException {\n    Container container = ctx.getContainer();\n    PrivilegedOperation signalOp = new PrivilegedOperation(\n        PrivilegedOperation.OperationType.SIGNAL_CONTAINER, (String) null);\n\n    signalOp.appendArgs(ctx.getExecutionAttribute(RUN_AS_USER),\n        ctx.getExecutionAttribute(USER),\n        Integer.toString(PrivilegedOperation.RunAsUserCommand\n            .SIGNAL_CONTAINER.getValue()),\n        ctx.getExecutionAttribute(PID),\n        Integer.toString(ctx.getExecutionAttribute(SIGNAL).getValue()));\n\n    try {\n      PrivilegedOperationExecutor executor = PrivilegedOperationExecutor\n          .getInstance(conf);\n\n      executor.executePrivilegedOperation(null,\n          signalOp, null, container.getLaunchContext().getEnvironment(),\n          false);\n    } catch (PrivilegedOperationException e) {\n      LOG.warn(\"Signal container failed. Exception: \", e);\n\n      // Log additional information about the container's resource usage\n      LOG.warn(\"Container resource usage: \" + container.getResourceUsage());\n\n      throw new ContainerExecutionException(\"Signal container failed\", e\n          .getExitCode(), e.getOutput(), e.getErrorOutput());\n    }\n  }"
        }
    },
    {
        "filename": "YARN-1752.json",
        "creation_time": "2014-02-22T05:51:42.000+0000",
        "bug_report": {
            "Title": "Unexpected Unregistered event at Attempt Launched state",
            "Description": "The application is encountering an error during state transition management in YARN. The error log indicates an attempt to transition from the LAUNCHED state to an UNREGISTERED state, which is not a valid operation. This is causing an `InvalidStateTransitonException`, suggesting that the application is trying to unregister while still in the launched state. The issue may stem from misconfiguration or an unexpected event in the application lifecycle.",
            "StackTrace": [
                "2014-02-21 14:56:03,453 ERROR org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Can't handle this event at current state",
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: UNREGISTERED at LAUNCHED",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:647)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:103)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:733)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:714)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:173)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:106)",
                "at java.lang.Thread.run(Thread.java:695)"
            ],
            "RootCause": "The root cause of the issue is an invalid state transition from LAUNCHED to UNREGISTERED, which is not permitted in the YARN state management framework. This indicates that the application is attempting to unregister while it is still in the launched state, likely due to a misconfiguration or an unexpected event.",
            "StepsToReproduce": [
                "Deploy a YARN application and allow it to reach the LAUNCHED state.",
                "Trigger an event that attempts to unregister the application while it is still in the LAUNCHED state."
            ],
            "ExpectedBehavior": "The application should transition through valid states without encountering an InvalidStateTransitonException. Unregistration should only occur from a state that allows it, such as FINISHED or FAILED.",
            "ObservedBehavior": "The application throws an InvalidStateTransitonException when trying to unregister from the LAUNCHED state, indicating an invalid state transition.",
            "Suggestions": "Review the application configuration to ensure that state transitions are correctly defined and handled. Investigate the event triggering the unregistration to ensure it is appropriate for the current state.",
            "problem_location": {
                "files": [
                    "RMAppAttemptImpl.java",
                    "ResourceManager.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl",
                    "org.apache.hadoop.yarn.server.resourcemanager.ResourceManager"
                ],
                "methods": [
                    "RMAppAttemptImpl.handle",
                    "ResourceManager.ApplicationAttemptEventDispatcher.handle"
                ]
            },
            "possible_fix": "Ensure that the application does not attempt to unregister while in the LAUNCHED state. This may involve adding checks in the RMAppAttemptImpl.handle method to validate the current state before processing unregistration events."
        },
        "possible_fix_code": {
            "RMAppAttemptImpl.handle": "  public void handle(RMAppAttemptEvent event) {\n\n    this.writeLock.lock();\n\n    try {\n      ApplicationAttemptId appAttemptID = event.getApplicationAttemptId();\n      LOG.debug(\"Processing event for \" + appAttemptID + \" of type \" + event.getType());\n      final RMAppAttemptState oldState = getAppAttemptState();\n\n      // Check if the current state is LAUNCHED and the event type is UNREGISTERED\n      if (oldState == RMAppAttemptState.LAUNCHED && event.getType() == RMAppAttemptEventType.UNREGISTERED) {\n        LOG.error(\"Cannot unregister while in LAUNCHED state\");\n        return; // Prevent the invalid state transition\n      }\n\n      try {\n        /* keep the master in sync with the state machine */\n        this.stateMachine.doTransition(event.getType(), event);\n      } catch (InvalidStateTransitonException e) {\n        LOG.error(\"Can't handle this event at current state\", e);\n        /* TODO fail the application on the failed transition */\n      }\n\n      if (oldState != getAppAttemptState()) {\n        LOG.info(appAttemptID + \" State change from \" + oldState + \" to \" + getAppAttemptState());\n      }\n    } finally {\n      this.writeLock.unlock();\n    }\n  }"
        }
    },
    {
        "filename": "YARN-6629.json",
        "creation_time": "2017-05-22T08:31:16.000+0000",
        "bug_report": {
            "Title": "NPE occurred when container allocation proposal is applied but its resource requests are removed before",
            "Description": "A NullPointerException (NPE) is thrown when the application master (AM) attempts to allocate resources after a resource request has been removed. The error occurs in the AppSchedulingInfo.allocate method when trying to access a key in the schedulerKeyToPlacementSets map that does not exist, leading to a null reference.",
            "StackTrace": [
                "FATAL event.EventDispatcher (EventDispatcher.java:run(75)) - Error in handling event type NODE_UPDATE to the Event Dispatcher",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.allocate(AppSchedulingInfo.java:446)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp.apply(FiCaSchedulerApp.java:516)",
                "at org.apache.hadoop.yarn.client.TestNegativePendingResource$1.answer(TestNegativePendingResource.java:225)",
                "at org.mockito.internal.stubbing.StubbedInvocationMatcher.answer(StubbedInvocationMatcher.java:31)",
                "at org.mockito.internal.MockHandler.handle(MockHandler.java:97)",
                "at org.mockito.internal.creation.MethodInterceptorFilter.intercept(MethodInterceptorFilter.java:47)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp$$EnhancerByMockitoWithCGLIB$$29eb8afc.apply(<generated>)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.tryCommit(CapacityScheduler.java:2396)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.submitResourceCommitRequest(CapacityScheduler.java:2281)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.allocateOrReserveNewContainers(CapacityScheduler.java:1247)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.allocateContainerOnSingleNode(CapacityScheduler.java:1236)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.allocateContainersToNode(CapacityScheduler.java:1325)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.allocateContainersToNode(CapacityScheduler.java:1112)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.nodeUpdate(CapacityScheduler.java:987)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:1367)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:1437)",
                "at org.apache.hadoop.yarn.event.EventDispatcher$EventProcessor.run(EventDispatcher.java:66)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "RootCause": "The NPE occurs because the schedulerKey used to access the schedulerKeyToPlacementSets map is not present, likely due to the resource request being removed before the allocation attempt. This indicates a flaw in the resource request lifecycle management.",
            "StepsToReproduce": [
                "1. Start the Application Master (AM) and request 1 container with schedulerRequestKey#1.",
                "2. The scheduler allocates 1 container for this request and accepts the proposal.",
                "3. Remove the resource request associated with schedulerRequestKey#1.",
                "4. Attempt to apply the proposal, which triggers the allocation process."
            ],
            "ExpectedBehavior": "The system should handle the removal of resource requests gracefully, ensuring that no allocation attempts are made for removed requests.",
            "ObservedBehavior": "A NullPointerException is thrown when the system attempts to allocate resources for a removed request, indicating that the schedulerKey is not found in the schedulerKeyToPlacementSets map.",
            "Suggestions": "Implement checks to ensure that allocation attempts are only made for valid and existing schedulerKeys. Consider adding logging to track the lifecycle of resource requests.",
            "problem_location": {
                "files": [
                    "AppSchedulingInfo.java",
                    "FiCaSchedulerApp.java",
                    "CapacityScheduler.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo",
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp",
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler"
                ],
                "methods": [
                    "AppSchedulingInfo.allocate",
                    "FiCaSchedulerApp.apply",
                    "CapacityScheduler.tryCommit"
                ]
            },
            "possible_fix": "Add a null check for the schedulerKey before attempting to access schedulerKeyToPlacementSets in the AppSchedulingInfo.allocate method. Ensure that the resource request lifecycle is managed correctly to prevent removal before allocation."
        },
        "possible_fix_code": {
            "AppSchedulingInfo.allocate": "  public List<ResourceRequest> allocate(NodeType type,\n      SchedulerNode node, SchedulerRequestKey schedulerKey,\n      Container containerAllocated) {\n    try {\n      writeLock.lock();\n\n      if (null != containerAllocated) {\n        updateMetricsForAllocatedContainer(type, containerAllocated);\n      }\n\n      // Check if the schedulerKey exists in the map before accessing it\n      SchedulingPlacementSet<SchedulerNode> placementSet = schedulerKeyToPlacementSets.get(schedulerKey);\n      if (placementSet == null) {\n        LOG.warn(\"No placement set found for schedulerKey: \" + schedulerKey);\n        return new ArrayList<>(); // Return an empty list or handle as appropriate\n      }\n\n      return placementSet.allocate(\n          schedulerKey, type, node);\n    } finally {\n      writeLock.unlock();\n    }\n  }"
        }
    },
    {
        "filename": "YARN-3493.json",
        "creation_time": "2015-04-15T22:03:19.000+0000",
        "bug_report": {
            "Title": "RM fails to come up with error 'Failed to load/recover state' when memory settings are changed",
            "Description": "The ResourceManager (RM) fails to start after modifying memory settings in the yarn-site.xml configuration file. The issue arises when the following steps are performed: 1. Change yarn.nodemanager.resource.memory-mb and yarn.scheduler.maximum-allocation-mb to 4000 in yarn-site.xml. 2. Start a random text writer job with mapreduce.map.memory.mb=4000 in the background and wait for the job to reach the running state. 3. Restore yarn-site.xml to set yarn.scheduler.maximum-allocation-mb back to 2048 before the job completes. 4. Restart the ResourceManager. Upon restart, the RM fails to come up, throwing an InvalidResourceRequestException due to a memory request exceeding the maximum configured limit.",
            "StackTrace": [
                "org.apache.hadoop.yarn.exceptions.InvalidResourceRequestException: Invalid resource request, requested memory < 0, or requested memory > max configured, requestedMemory=3072, maxMemory=2048",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils.validateResourceRequest(SchedulerUtils.java:204)",
                "at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.validateAndCreateResourceRequest(RMAppManager.java:385)",
                "at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.createAndPopulateNewRMApp(RMAppManager.java:328)",
                "at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.recoverApplication(RMAppManager.java:317)",
                "at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.recover(RMAppManager.java:422)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.recover(ResourceManager.java:1187)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:574)",
                "at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager.java:994)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1035)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1031)"
            ],
            "RootCause": "The root cause of the issue is an InvalidResourceRequestException thrown by the SchedulerUtils.validateResourceRequest method. This occurs because the requested memory (3072 MB) exceeds the maximum configured memory limit (2048 MB) after the yarn.scheduler.maximum-allocation-mb is restored to 2048.",
            "StepsToReproduce": [
                "1. Change yarn.nodemanager.resource.memory-mb and yarn.scheduler.maximum-allocation-mb to 4000 in yarn-site.xml.",
                "2. Start a random text writer job with mapreduce.map.memory.mb=4000 in the background and wait for the job to reach the running state.",
                "3. Restore yarn-site.xml to set yarn.scheduler.maximum-allocation-mb back to 2048 before the job completes.",
                "4. Restart the ResourceManager."
            ],
            "ExpectedBehavior": "The ResourceManager should start successfully without any errors, allowing the application to run with the specified memory settings.",
            "ObservedBehavior": "The ResourceManager fails to start, throwing an InvalidResourceRequestException indicating that the requested memory exceeds the maximum configured memory.",
            "Suggestions": "To resolve this issue, either adjust the requested memory in the application to be within the configured limits (e.g., set it to 2048 MB or less) or increase the maximum memory allocation in the yarn-site.xml configuration file by setting the properties yarn.scheduler.maximum-allocation-mb and yarn.nodemanager.resource.memory-mb to higher values.",
            "problem_location": {
                "files": [
                    "yarn-site.xml",
                    "SchedulerUtils.java",
                    "RMAppManager.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils",
                    "org.apache.hadoop.yarn.server.resourcemanager.RMAppManager"
                ],
                "methods": [
                    "SchedulerUtils.validateResourceRequest",
                    "RMAppManager.validateAndCreateResourceRequest"
                ]
            },
            "possible_fix": "Increase the maximum memory allocation in yarn-site.xml by setting yarn.scheduler.maximum-allocation-mb to a value greater than 3072 MB, such as 4096 MB, and ensure that yarn.nodemanager.resource.memory-mb is also set appropriately."
        },
        "possible_fix_code": {
            "SchedulerUtils.validateResourceRequest": "  public static void validateResourceRequest(ResourceRequest resReq,\n      Resource maximumResource, String queueName, YarnScheduler scheduler)\n      throws InvalidResourceRequestException {\n    if (resReq.getCapability().getMemory() < 0 ||\n        resReq.getCapability().getMemory() > maximumResource.getMemory()) {\n      throw new InvalidResourceRequestException(\"Invalid resource request\"\n          + \", requested memory < 0\"\n          + \", or requested memory > max configured\"\n          + \", requestedMemory=\" + resReq.getCapability().getMemory()\n          + \", maxMemory=\" + maximumResource.getMemory());\n    }\n    if (resReq.getCapability().getVirtualCores() < 0 ||\n        resReq.getCapability().getVirtualCores() >\n        maximumResource.getVirtualCores()) {\n      throw new InvalidResourceRequestException(\"Invalid resource request\"\n          + \", requested virtual cores < 0\"\n          + \", or requested virtual cores > max configured\"\n          + \", requestedVirtualCores=\"\n          + resReq.getCapability().getVirtualCores()\n          + \", maxVirtualCores=\" + maximumResource.getVirtualCores());\n    }\n    \n    // Get queue from scheduler\n    QueueInfo queueInfo = null;\n    try {\n      queueInfo = scheduler.getQueueInfo(queueName, false, false);\n    } catch (IOException e) {\n      // it is possible queue cannot get when queue mapping is set, just ignore\n      // the queueInfo here, and move forward\n    }\n\n    // check labels in the resource request.\n    String labelExp = resReq.getNodeLabelExpression();\n    \n    // if queue has default label expression, and RR doesnt have, use the\n    // default label expression of queue\n    if (labelExp == null && queueInfo != null\n        && ResourceRequest.ANY.equals(resReq.getResourceName())) {\n      labelExp = queueInfo.getDefaultNodeLabelExpression();\n    }\n    \n    // If labelExp still equals to null, set it to be NO_LABEL\n    resReq\n        .setNodeLabelExpression(labelExp == null ? RMNodeLabelsManager.NO_LABEL\n            : labelExp);\n    \n    // we dont allow specify label expression other than resourceName=ANY now\n    if (!ResourceRequest.ANY.equals(resReq.getResourceName())\n        && labelExp != null && !labelExp.trim().isEmpty()) {\n      throw new InvalidResourceRequestException(\n          \"Invalid resource request, queue=\" + queueInfo.getQueueName()\n              + \" specified node label expression in a \"\n              + \"resource request has resource name = \"\n              + resReq.getResourceName());\n    }\n    \n    // we dont allow specify label expression with more than one node labels now\n    if (labelExp != null && labelExp.contains(\"&&\")) {\n      throw new InvalidResourceRequestException(\n          \"Invalid resource request, queue=\" + queueInfo.getQueueName()\n              + \" specified more than one node label \"\n              + \"in a node label expression, node label expression = \"\n              + labelExp);\n    }\n    \n    if (labelExp != null && !labelExp.trim().isEmpty() && queueInfo != null) {\n      if (!checkQueueLabelExpression(queueInfo.getAccessibleNodeLabels(),\n          labelExp)) {\n        throw new InvalidResourceRequestException(\"Invalid resource request\"\n            + \", queue=\"\n            + queueInfo.getQueueName()\n            + \" doesnt have permission to access all labels \"\n            + \"in resource request. labelExpression of resource request=\"\n            + labelExp\n            + \". Queue labels=\"\n            + (queueInfo.getAccessibleNodeLabels() == null ? \"\" : StringUtils.join(queueInfo\n                .getAccessibleNodeLabels().iterator(), \",\")));\n      }\n    }\n  }"
        }
    },
    {
        "filename": "YARN-7645.json",
        "creation_time": "2017-12-12T21:19:53.000+0000",
        "bug_report": {
            "Title": "TestContainerResourceUsage#testUsageAfterAMRestartWithMultipleContainers is flakey with FairScheduler",
            "Description": "The test case {{TestContainerResourceUsage#testUsageAfterAMRestartWithMultipleContainers}} is exhibiting flakiness when executed with the {{FairScheduler}}. The failure occurs due to an assertion error indicating that the expected state of a resource is 'ALLOCATED', but the actual state is 'SCHEDULED'. This discrepancy suggests a potential issue with the resource state transitions during the Application Master (AM) restart process in the YARN framework.",
            "StackTrace": [
                "java.lang.AssertionError: Attempt state is not correct (timeout). expected:<ALLOCATED> but was:<SCHEDULED>",
                "at org.apache.hadoop.yarn.server.resourcemanager.TestContainerResourceUsage.amRestartTests(TestContainerResourceUsage.java:275)",
                "at org.apache.hadoop.yarn.server.resourcemanager.TestContainerResourceUsage.testUsageAfterAMRestartWithMultipleContainers(TestContainerResourceUsage.java:254)"
            ],
            "RootCause": "The root cause of the issue appears to be related to incorrect handling of resource state transitions during the Application Master restart process, leading to a mismatch between the expected and actual resource states.",
            "StepsToReproduce": [
                "Run the test case TestContainerResourceUsage#testUsageAfterAMRestartWithMultipleContainers using FairScheduler.",
                "Observe the assertion failure indicating the state mismatch."
            ],
            "ExpectedBehavior": "The resource should transition to the 'ALLOCATED' state after the Application Master restart, indicating successful resource allocation.",
            "ObservedBehavior": "The resource remains in the 'SCHEDULED' state, indicating that it has not been allocated as expected.",
            "Suggestions": "Review the resource state transition logic in the YARN framework, particularly during the Application Master restart process. Look for potential race conditions or logic errors that could prevent the resource from transitioning to the 'ALLOCATED' state.",
            "problem_location": {
                "files": [
                    "TestContainerResourceUsage.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.TestContainerResourceUsage"
                ],
                "methods": [
                    "amRestartTests",
                    "testUsageAfterAMRestartWithMultipleContainers"
                ]
            },
            "possible_fix": "Investigate the resource allocation logic in the YARN framework, especially around the Application Master lifecycle. Ensure that state transitions are correctly implemented and consider adding synchronization mechanisms to prevent race conditions."
        },
        "possible_fix_code": {
            "amRestartTests": "public void amRestartTests() throws Exception {\n    // Simulate the Application Master restart process\n    // Ensure that resources are correctly transitioned to ALLOCATED state\n    Resource resource = allocateResource(); // Method to allocate resource\n    assertEquals(\"Resource should be ALLOCATED after AM restart\", ResourceState.ALLOCATED, resource.getState());\n    // Additional logic to handle state transitions and ensure correctness\n    // ... (other necessary code to manage resource states)\n}"
        }
    },
    {
        "filename": "YARN-6054.json",
        "creation_time": "2017-01-04T20:58:59.000+0000",
        "bug_report": {
            "Title": "TimelineServer fails to start when some LevelDb state files are missing.",
            "Description": "The TimelineServer fails to initialize due to missing LevelDB state files, resulting in a service state exception. The error log indicates that 9 files are missing, which leads to a corruption error in the LevelDB database used by the Application History Server. This issue prevents the server from starting, indicating a lack of graceful degradation in the event of missing state files.",
            "StackTrace": [
                "2016-11-21 20:46:43,134 INFO org.apache.hadoop.service.AbstractService: Service org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryServer failed in state INITED; cause: org.apache.hadoop.service.ServiceStateException: org.fusesource.leveldbjni.internal.NativeDB$DBException: Corruption: 9 missing files; e.g.: <levelDbStorePath>/timelineserver/leveldb-timeline-store.ldb/127897.sst",
                "org.apache.hadoop.service.ServiceStateException: org.fusesource.leveldbjni.internal.NativeDB$DBException: Corruption: 9 missing files; e.g.: <levelDbStorePath>/timelineserver/leveldb-timeline-store.ldb/127897.sst",
                "at org.apache.hadoop.service.ServiceStateException.convert(ServiceStateException.java:59)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:172)",
                "at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:107)",
                "at org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryServer.serviceInit(ApplicationHistoryServer.java:104)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)",
                "at org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryServer.launchAppHistoryServer(ApplicationHistoryServer.java:172)",
                "at org.apache.hadoop.yarn.server.applicationhistoryservice.ApplicationHistoryServer.main(ApplicationHistoryServer.java:182)",
                "Caused by: org.fusesource.leveldbjni.internal.NativeDB$DBException: Corruption: 9 missing files; e.g.: <levelDbStorePath>/timelineserver/leveldb-timeline-store.ldb/127897.sst",
                "at org.fusesource.leveldbjni.internal.NativeDB.checkStatus(NativeDB.java:200)",
                "at org.fusesource.leveldbjni.internal.NativeDB.open(NativeDB.java:218)",
                "at org.fusesource.leveldbjni.JniDBFactory.open(JniDBFactory.java:168)",
                "at org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore.serviceInit(LeveldbTimelineStore.java:229)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)",
                "... 5 more",
                "2016-11-21 20:46:43,136 INFO org.apache.hadoop.util.ExitUtil: Exiting with status -1"
            ],
            "RootCause": "The root cause of the issue is the corruption of the LevelDB database due to 9 missing state files, which prevents the TimelineServer from starting. The service fails during the initialization phase, as indicated by the stack trace.",
            "StepsToReproduce": [
                "1. Ensure that the TimelineServer is configured to use LevelDB for state management.",
                "2. Delete or move some of the LevelDB state files from the expected directory.",
                "3. Attempt to start the TimelineServer."
            ],
            "ExpectedBehavior": "The TimelineServer should start successfully, even if some state files are missing, by implementing graceful degradation or recovery mechanisms.",
            "ObservedBehavior": "The TimelineServer fails to start and logs an error indicating that it cannot initialize due to missing LevelDB state files, resulting in a service state exception.",
            "Suggestions": "To address the issue, consider implementing a recovery mechanism in the TimelineServer that allows it to start even when some state files are missing. This could involve using methods like `getDbIterator` to inspect the database and `deleteKeysWithPrefix` to clean up corrupted entries. Additionally, ensure that backup and restore procedures are in place for the LevelDB database.",
            "problem_location": {
                "files": [
                    "hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/timeline/LeveldbTimelineStore.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore"
                ],
                "methods": [
                    "serviceInit",
                    "handleError",
                    "deleteKeysWithPrefix",
                    "getDbIterator"
                ]
            },
            "possible_fix": "Implement a recovery mechanism in the `serviceInit` method of the `LeveldbTimelineStore` class to handle missing state files gracefully. This could include inspecting the database with `getDbIterator`, cleaning up corrupted entries with `deleteKeysWithPrefix`, and providing fallback options for the TimelineServer to start."
        },
        "possible_fix_code": {
            "org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore.serviceInit": "  protected void serviceInit(Configuration conf) throws Exception {\n    Preconditions.checkArgument(conf.getLong(\n        YarnConfiguration.TIMELINE_SERVICE_TTL_MS,\n        YarnConfiguration.DEFAULT_TIMELINE_SERVICE_TTL_MS) > 0,\n        \"%s property value should be greater than zero\",\n        YarnConfiguration.TIMELINE_SERVICE_TTL_MS);\n    Preconditions.checkArgument(conf.getLong(\n        YarnConfiguration.TIMELINE_SERVICE_LEVELDB_TTL_INTERVAL_MS,\n        YarnConfiguration.DEFAULT_TIMELINE_SERVICE_LEVELDB_TTL_INTERVAL_MS) > 0,\n        \"%s property value should be greater than zero\",\n        YarnConfiguration.TIMELINE_SERVICE_LEVELDB_TTL_INTERVAL_MS);\n    Preconditions.checkArgument(conf.getLong(\n        YarnConfiguration.TIMELINE_SERVICE_LEVELDB_READ_CACHE_SIZE,\n        YarnConfiguration.DEFAULT_TIMELINE_SERVICE_LEVELDB_READ_CACHE_SIZE) >= 0,\n        \"%s property value should be greater than or equal to zero\",\n        YarnConfiguration.TIMELINE_SERVICE_LEVELDB_READ_CACHE_SIZE);\n    Preconditions.checkArgument(conf.getLong(\n        YarnConfiguration.TIMELINE_SERVICE_LEVELDB_START_TIME_READ_CACHE_SIZE,\n        YarnConfiguration.DEFAULT_TIMELINE_SERVICE_LEVELDB_START_TIME_READ_CACHE_SIZE) > 0,\n        \" %s property value should be greater than zero\",\n        YarnConfiguration.TIMELINE_SERVICE_LEVELDB_START_TIME_READ_CACHE_SIZE);\n    Preconditions.checkArgument(conf.getLong(\n        YarnConfiguration.TIMELINE_SERVICE_LEVELDB_START_TIME_WRITE_CACHE_SIZE,\n        YarnConfiguration.DEFAULT_TIMELINE_SERVICE_LEVELDB_START_TIME_WRITE_CACHE_SIZE) > 0,\n        \"%s property value should be greater than zero\",\n        YarnConfiguration.TIMELINE_SERVICE_LEVELDB_START_TIME_WRITE_CACHE_SIZE);\n\n    Options options = new Options();\n    options.createIfMissing(true);\n    options.cacheSize(conf.getLong(\n        YarnConfiguration.TIMELINE_SERVICE_LEVELDB_READ_CACHE_SIZE,\n        YarnConfiguration.DEFAULT_TIMELINE_SERVICE_LEVELDB_READ_CACHE_SIZE));\n    JniDBFactory factory = new JniDBFactory();\n    Path dbPath = new Path(\n        conf.get(YarnConfiguration.TIMELINE_SERVICE_LEVELDB_PATH), FILENAME);\n    FileSystem localFS = null;\n    try {\n      localFS = FileSystem.getLocal(conf);\n      if (!localFS.exists(dbPath)) {\n        if (!localFS.mkdirs(dbPath)) {\n          throw new IOException(\"Couldn't create directory for leveldb \" +\n              \"timeline store \" + dbPath);\n        }\n        localFS.setPermission(dbPath, LEVELDB_DIR_UMASK);\n      }\n    } finally {\n      IOUtils.cleanup(LOG, localFS);\n    }\n    LOG.info(\"Using leveldb path \" + dbPath);\n    try {\n      db = factory.open(new File(dbPath.toString()), options);\n      checkVersion();\n    } catch (NativeDB.DBException e) {\n      LOG.error(\"Failed to open LevelDB database: \" + e.getMessage());\n      recoverFromCorruption();\n    }\n    startTimeWriteCache =\n        Collections.synchronizedMap(new LRUMap(getStartTimeWriteCacheSize(\n            conf)));\n    startTimeReadCache =\n        Collections.synchronizedMap(new LRUMap(getStartTimeReadCacheSize(\n            conf)));\n\n    if (conf.getBoolean(YarnConfiguration.TIMELINE_SERVICE_TTL_ENABLE, true)) {\n      deletionThread = new EntityDeletionThread(conf);\n      deletionThread.start();\n    }\n\n    super.serviceInit(conf);\n  }\n\n  private void recoverFromCorruption() {\n    LOG.warn(\"Attempting to recover from LevelDB corruption...\");\n    LeveldbIterator iterator = getDbIterator(false);\n    WriteBatch writeBatch = db.createWriteBatch();\n    deleteKeysWithPrefix(writeBatch, new byte[0], iterator);\n    db.write(writeBatch);\n    LOG.info(\"Recovery from LevelDB corruption completed.\");\n  }"
        }
    },
    {
        "filename": "YARN-196.json",
        "creation_time": "2012-01-16T09:52:45.000+0000",
        "bug_report": {
            "Title": "Nodemanager should be more robust in handling connection failure to ResourceManager when a cluster is started",
            "Description": "The Node Manager (NM) fails to start properly when it is initiated before the Resource Manager (RM) is running, resulting in a connection failure. The error log indicates that the NM attempts to register with the RM but encounters a `Connection refused` error, which is a common issue in Hadoop YARN setups. This situation can arise due to the RM not being operational, network issues, or misconfigurations in the system.",
            "StackTrace": [
                "ERROR org.apache.hadoop.yarn.service.CompositeService: Error starting services org.apache.hadoop.yarn.server.nodemanager.NodeManager",
                "org.apache.avro.AvroRuntimeException: java.lang.reflect.UndeclaredThrowableException",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.start(NodeStatusUpdaterImpl.java:149)",
                "at org.apache.hadoop.yarn.service.CompositeService.start(CompositeService.java:68)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.start(NodeManager.java:167)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.main(NodeManager.java:242)",
                "Caused by: java.lang.reflect.UndeclaredThrowableException",
                "at org.apache.hadoop.yarn.server.api.impl.pb.client.ResourceTrackerPBClientImpl.registerNodeManager(ResourceTrackerPBClientImpl.java:66)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.registerWithRM(NodeStatusUpdaterImpl.java:182)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.start(NodeStatusUpdaterImpl.java:145)",
                "Caused by: com.google.protobuf.ServiceException: java.net.ConnectException: Call From HOST-10-18-52-230/10.18.52.230 to HOST-10-18-52-250:8025 failed on connection exception: java.net.ConnectException: Connection refused",
                "Caused by: java.net.ConnectException: Connection refused"
            ],
            "RootCause": "The root cause of the issue is that the Node Manager is attempting to connect to the Resource Manager, which is not running, leading to a `Connection refused` error. This indicates that the NM cannot register itself with the RM, which is essential for its operation.",
            "StepsToReproduce": [
                "Start the Node Manager before starting the Resource Manager.",
                "Observe the logs for any connection errors."
            ],
            "ExpectedBehavior": "The Node Manager should be able to handle the situation gracefully, either by waiting for the Resource Manager to start or by providing a clear error message without shutting down.",
            "ObservedBehavior": "The Node Manager shuts down with a `Connection refused` error when it attempts to register with the Resource Manager, which is not running.",
            "Suggestions": "To mitigate this issue, ensure that the Resource Manager is started before the Node Manager. Additionally, implement error handling in the Node Manager to manage connection failures more gracefully.",
            "problem_location": {
                "files": [
                    "NodeManager.java",
                    "NodeStatusUpdaterImpl.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.nodemanager.NodeManager",
                    "org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl"
                ],
                "methods": [
                    "NodeManager.start",
                    "NodeStatusUpdaterImpl.start"
                ]
            },
            "possible_fix": "Implement a check in the Node Manager's start method to verify if the Resource Manager is running before attempting to register. If the RM is not available, the NM should log a warning and either retry after a delay or exit with a clear error message."
        },
        "possible_fix_code": {
            "NodeManager.start": "public void start() throws Exception {\n    // Check if Resource Manager is running before attempting to register\n    if (!isResourceManagerRunning()) {\n        LOG.warn(\"Resource Manager is not running. Node Manager will not start.\");\n        return; // or throw an exception based on desired behavior\n    }\n    super.start(); // Call the superclass start method to proceed with normal startup\n}\n\nprivate boolean isResourceManagerRunning() {\n    // Implement a check to see if the Resource Manager is reachable\n    try {\n        // Attempt to connect to the Resource Manager's address\n        InetSocketAddress rmAddress = new InetSocketAddress(resourceManagerHost, resourceManagerPort);\n        Socket socket = new Socket();\n        socket.connect(rmAddress, 2000); // 2 seconds timeout\n        socket.close();\n        return true;\n    } catch (IOException e) {\n        LOG.warn(\"Resource Manager is not reachable: \" + e.getMessage());\n        return false;\n    }\n}"
        }
    },
    {
        "filename": "YARN-8508.json",
        "creation_time": "2018-07-09T23:37:49.000+0000",
        "bug_report": {
            "Title": "On NodeManager container gets cleaned up before its pid file is created",
            "Description": "The issue arises when a container is being killed, leading to a failure in GPU resource allocation. The logs indicate that the NodeManager is unable to create a PID file for the container before it is cleaned up, resulting in a `ResourceHandlerException` due to insufficient available GPUs. Specifically, a request for 2 GPUs was made, but only 1 was available, causing the container launch to fail.",
            "StackTrace": [
                "2018-07-06 05:22:26,201 INFO  container.ContainerImpl (ContainerImpl.java:handle(2093)) - Container container_e20_1530854311763_0006_01_000001 transitioned from RUNNING to KILLING",
                "2018-07-06 05:22:31,358 INFO  launcher.ContainerLaunch (ContainerLaunch.java:getContainerPid(1102)) - Could not get pid for container_e20_1530854311763_0006_01_000002. Waited for 5000 ms.",
                "2018-07-06 05:22:31,358 WARN  launcher.ContainerLaunch (ContainerLaunch.java:cleanupContainer(784)) - Container clean up before pid file created container_e20_1530854311763_0006_01_000002",
                "2018-07-06 05:22:39,048 ERROR nodemanager.LinuxContainerExecutor (LinuxContainerExecutor.java:handleLaunchForLaunchType(550)) - ResourceHandlerChain.preStart() failed!",
                "org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.ResourceHandlerException: Failed to find enough GPUs, requestor=container_e20_1530854311763_0007_01_000002, #RequestedGPUs=2, #availableGpus=1"
            ],
            "RootCause": "The root cause of the issue is the failure in GPU resource allocation due to a mismatch between the requested number of GPUs (2) and the available GPUs (1) on the NodeManager. This leads to the container being cleaned up before the PID file can be created.",
            "StepsToReproduce": [
                "1. Submit a container request that requires more GPUs than are available on the NodeManager.",
                "2. Monitor the NodeManager logs for the transition of the container state.",
                "3. Observe the cleanup process and check for warnings about PID file creation."
            ],
            "ExpectedBehavior": "The NodeManager should successfully allocate the requested GPUs for the container, create the PID file, and allow the container to run without premature cleanup.",
            "ObservedBehavior": "The NodeManager fails to allocate the requested GPUs, resulting in a cleanup of the container before the PID file is created, leading to errors in resource allocation.",
            "Suggestions": "1. Check the GPU availability on the NodeManager and ensure it matches the requested resources. \n2. Review the YARN configuration files (e.g., yarn-site.xml) to ensure proper GPU resource settings. \n3. Consider scaling the cluster to provide more GPUs if necessary.",
            "problem_location": {
                "files": [
                    "ContainerImpl.java",
                    "LinuxContainerExecutor.java",
                    "ContainerLaunch.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.gpu.GpuResourceAllocator",
                    "org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor",
                    "org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch"
                ],
                "methods": [
                    "ContainerImpl.handle",
                    "LinuxContainerExecutor.handleLaunchForLaunchType",
                    "ContainerLaunch.cleanupContainer"
                ]
            },
            "possible_fix": "Ensure that the YARN configuration for GPU resources is correctly set up, particularly the properties 'yarn.nodemanager.resource.gpu' and 'yarn.nodemanager.resource-types'. Additionally, modify the container request to align with the available GPU resources."
        },
        "possible_fix_code": {
            "GpuResourceAllocator.internalAssignGpus": "  private synchronized GpuAllocation internalAssignGpus(Container container)\n      throws ResourceHandlerException {\n    Resource requestedResource = container.getResource();\n    ContainerId containerId = container.getContainerId();\n    int numRequestedGpuDevices = getRequestedGpus(requestedResource);\n    // Assign Gpus to container if requested some.\n    if (numRequestedGpuDevices > 0) {\n      if (numRequestedGpuDevices > getAvailableGpus()) {\n        // If there are some devices which are getting released, wait for few\n        // seconds to get it.\n        if (numRequestedGpuDevices <= getReleasingGpus() + getAvailableGpus()) {\n          return null;\n        }\n        // Log a warning if not enough GPUs are available\n        LOG.warn(\"Not enough GPUs available for container \" + containerId + \". Requested: \" + numRequestedGpuDevices + \", Available: \" + getAvailableGpus());\n      }\n\n      if (numRequestedGpuDevices > getAvailableGpus()) {\n        throw new ResourceHandlerException(\n            getResourceHandlerExceptionMessage(numRequestedGpuDevices,\n                containerId));\n      }\n\n      Set<GpuDevice> assignedGpus = new TreeSet<>();\n\n      for (GpuDevice gpu : allowedGpuDevices) {\n        if (!usedDevices.containsKey(gpu)) {\n          usedDevices.put(gpu, containerId);\n          assignedGpus.add(gpu);\n          if (assignedGpus.size() == numRequestedGpuDevices) {\n            break;\n          }\n        }\n      }\n\n      // Record in state store if we allocated anything\n      if (!assignedGpus.isEmpty()) {\n        try {\n          // Update state store.\n          nmContext.getNMStateStore().storeAssignedResources(container, GPU_URI,\n              new ArrayList<>(assignedGpus));\n        } catch (IOException e) {\n          cleanupAssignGpus(containerId);\n          throw new ResourceHandlerException(e);\n        }\n      }\n\n      return new GpuAllocation(assignedGpus,\n          Sets.difference(allowedGpuDevices, assignedGpus));\n    }\n    return new GpuAllocation(null, allowedGpuDevices);\n  }"
        }
    },
    {
        "filename": "YARN-2308.json",
        "creation_time": "2014-07-17T10:01:57.000+0000",
        "bug_report": {
            "Title": "NPE occurred when RM restart after CapacityScheduler queue configuration changed",
            "Description": "A NullPointerException (NPE) was encountered during the restart of the ResourceManager (RM) after modifications were made to the CapacityScheduler queue configuration. The issue arises when the RM attempts to recover historical applications, and if any of the queues associated with these applications have been removed, an NPE is triggered. The stack trace indicates that the exception occurs in the `addApplicationAttempt` method of the `CapacityScheduler` class, specifically when trying to access a queue that no longer exists.",
            "StackTrace": [
                "2014-07-16 07:22:46,957 FATAL org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Error in handling event type APP_ATTEMPT_ADDED to the scheduler",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.addApplicationAttempt(CapacityScheduler.java:566)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:922)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:98)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:594)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.handle(RMAppImpl.java:654)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.handle(RMAppImpl.java:85)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher.handle(ResourceManager.java:698)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher.handle(ResourceManager.java:682)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:173)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:106)",
                "at java.lang.Thread.run(Thread.java:744)"
            ],
            "RootCause": "The NPE is likely caused by the `addApplicationAttempt` method attempting to access a queue that has been removed from the configuration. Specifically, if the application ID does not exist in the `applications` map or if the queue associated with the application is not found in the `queues` map, a NPE will occur.",
            "StepsToReproduce": [
                "1. Modify the CapacityScheduler queue configuration by removing existing queues and adding new ones.",
                "2. Restart the ResourceManager.",
                "3. Observe the logs for a NullPointerException during the recovery of historical applications."
            ],
            "ExpectedBehavior": "The ResourceManager should restart successfully and recover all historical applications without encountering any exceptions.",
            "ObservedBehavior": "The ResourceManager fails to restart due to a NullPointerException when attempting to recover applications associated with removed queues.",
            "Suggestions": "Ensure that the application IDs are correctly registered in the `applications` map and that the queues associated with these applications exist in the `queues` map before attempting to add application attempts. Implement validation checks to handle cases where queues may have been removed.",
            "problem_location": {
                "files": [
                    "hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler"
                ],
                "methods": [
                    "addApplicationAttempt",
                    "getQueue",
                    "addApplication"
                ]
            },
            "possible_fix": "Add validation checks in the `addApplicationAttempt` method to ensure that the application ID exists in the `applications` map and that the queue retrieved by `getQueue()` is not null before proceeding with the application attempt addition."
        },
        "possible_fix_code": {
            "addApplicationAttempt": "  private synchronized void addApplicationAttempt(\n      ApplicationAttemptId applicationAttemptId,\n      boolean transferStateFromPreviousAttempt,\n      boolean isAttemptRecovering) {\n    SchedulerApplication<FiCaSchedulerApp> application =\n        applications.get(applicationAttemptId.getApplicationId());\n    if (application == null) {\n        LOG.error(\"Application not found for ID: \" + applicationAttemptId.getApplicationId());\n        return;\n    }\n    CSQueue queue = (CSQueue) application.getQueue();\n    if (queue == null) {\n        LOG.error(\"Queue not found for application: \" + applicationAttemptId.getApplicationId());\n        return;\n    }\n\n    FiCaSchedulerApp attempt =\n        new FiCaSchedulerApp(applicationAttemptId, application.getUser(),\n          queue, queue.getActiveUsersManager(), rmContext);\n    if (transferStateFromPreviousAttempt) {\n      attempt.transferStateFromPreviousAttempt(application\n        .getCurrentAppAttempt());\n    }\n    application.setCurrentAppAttempt(attempt);\n\n    queue.submitApplicationAttempt(attempt, application.getUser());\n    LOG.info(\"Added Application Attempt \" + applicationAttemptId\n        + \" to scheduler from user \" + application.getUser() + \" in queue \"\n        + queue.getQueueName());\n    if (isAttemptRecovering) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(applicationAttemptId\n            + \" is recovering. Skipping notifying ATTEMPT_ADDED\");\n      }\n    } else {\n      rmContext.getDispatcher().getEventHandler().handle(\n        new RMAppAttemptEvent(applicationAttemptId,\n            RMAppAttemptEventType.ATTEMPT_ADDED));\n    }\n  }"
        }
    },
    {
        "filename": "YARN-933.json",
        "creation_time": "2013-07-17T12:29:28.000+0000",
        "bug_report": {
            "Title": "Potential InvalidStateTransitonException: Invalid event: LAUNCHED at FINAL_SAVING",
            "Description": "The issue arises in the Hadoop YARN framework where an `InvalidStateTransitonException` is thrown due to an invalid event `LAUNCH_FAILED` occurring while the application is already in the `FAILED` state. This situation is triggered after a connection loss occurs during the allocation of an application attempt (AppAttempt_1). The ResourceManager (RM) attempts to retry launching the same application attempt, which is not valid given its current state. The root cause appears to be a `ConnectTimeoutException`, indicating that a connection attempt to a remote host timed out after 20 seconds.",
            "StackTrace": [
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: LAUNCH_FAILED at FAILED",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:630)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:495)"
            ],
            "RootCause": "The root cause of the issue is a `ConnectTimeoutException` that occurs when the application attempts to connect to a remote host but fails due to a timeout. This leads to an invalid state transition when the ResourceManager tries to launch an application attempt that is already in a failed state.",
            "StepsToReproduce": [
                "Step 1: Install a cluster with NodeManager (NM) on 2 machines.",
                "Step 2: Ping from the ResourceManager (RM) machine to NM1 using IP (should succeed) and using hostname (should fail).",
                "Step 3: Execute a job.",
                "Step 4: After AppAttempt_1 is allocated to NM1, simulate a connection loss."
            ],
            "ExpectedBehavior": "The application should handle connection losses gracefully, allowing for valid retries without entering an invalid state.",
            "ObservedBehavior": "After AppAttempt_1 fails, the ResourceManager attempts to launch it again, resulting in an `InvalidStateTransitonException` because the application is already in a failed state.",
            "Suggestions": "1. Check network connectivity between nodes to ensure they can communicate. 2. Increase timeout settings in the YARN configuration to allow for longer connection attempts. 3. Ensure sufficient resources are available on the nodes. 4. Review logs for additional error messages. 5. Restart ResourceManager and NodeManager services if necessary.",
            "problem_location": {
                "files": [
                    "RMAppAttemptImpl.java",
                    "ResourceManager.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl",
                    "org.apache.hadoop.yarn.server.resourcemanager.ResourceManager"
                ],
                "methods": [
                    "RMAppAttemptImpl.handle",
                    "ResourceManager.ApplicationAttemptEventDispatcher.handle"
                ]
            },
            "possible_fix": "Consider implementing a check in the RMAppAttemptImpl class to prevent launching an application attempt that is already in a failed state. Additionally, review and adjust the connection timeout settings in the YARN configuration."
        },
        "possible_fix_code": {
            "RMAppAttemptImpl.handle": "  public void handle(RMAppAttemptEvent event) {\n\n    this.writeLock.lock();\n\n    try {\n      ApplicationAttemptId appAttemptID = event.getApplicationAttemptId();\n      LOG.debug(\"Processing event for \" + appAttemptID + \" of type \" + event.getType());\n      final RMAppAttemptState oldState = getAppAttemptState();\n\n      // Check if the current state is FAILED before processing the event\n      if (oldState == RMAppAttemptState.FAILED) {\n        LOG.warn(\"Ignoring event \" + event.getType() + \" for application attempt \" + appAttemptID + \" as it is already in FAILED state.\");\n        return;\n      }\n\n      try {\n        /* keep the master in sync with the state machine */\n        this.stateMachine.doTransition(event.getType(), event);\n      } catch (InvalidStateTransitonException e) {\n        LOG.error(\"Can't handle this event at current state\", e);\n        /* TODO fail the application on the failed transition */\n      }\n\n      if (oldState != getAppAttemptState()) {\n        LOG.info(appAttemptID + \" State change from \" + oldState + \" to \" + getAppAttemptState());\n      }\n    } finally {\n      this.writeLock.unlock();\n    }\n  }"
        }
    },
    {
        "filename": "YARN-1374.json",
        "creation_time": "2013-10-30T11:49:49.000+0000",
        "bug_report": {
            "Title": "Resource Manager fails to start due to ConcurrentModificationException",
            "Description": "The Resource Manager fails to start due to a ConcurrentModificationException, which occurs when a collection is modified while it is being iterated over. The stack trace indicates that this exception is thrown during the initialization of services in the CompositeService class, specifically in the serviceInit method. The issue arises when the serviceInit method attempts to iterate over a list of services that may be concurrently modified by other threads.",
            "StackTrace": [
                "java.util.ConcurrentModificationException",
                "at java.util.AbstractList$Itr.checkForComodification(AbstractList.java:372)",
                "at java.util.AbstractList$Itr.next(AbstractList.java:343)",
                "at java.util.Collections$UnmodifiableCollection$1.next(Collections.java:1010)",
                "at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:107)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceInit(ResourceManager.java:187)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:944)"
            ],
            "RootCause": "The ConcurrentModificationException is caused by the serviceInit method in CompositeService iterating over a list of services retrieved from getServices(), which returns an unmodifiable view of serviceList. If serviceList is modified by another thread during this iteration, it leads to the exception.",
            "StepsToReproduce": [
                "Start the Resource Manager service.",
                "Ensure that another thread modifies the serviceList while the Resource Manager is initializing."
            ],
            "ExpectedBehavior": "The Resource Manager should start successfully without throwing any exceptions.",
            "ObservedBehavior": "The Resource Manager fails to start and throws a ConcurrentModificationException.",
            "Suggestions": "Ensure that modifications to serviceList are properly synchronized or avoid modifying it while it is being iterated over in the serviceInit method.",
            "problem_location": {
                "files": [
                    "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/service/CompositeService.java",
                    "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceManager.java"
                ],
                "classes": [
                    "org.apache.hadoop.service.CompositeService",
                    "org.apache.hadoop.yarn.server.resourcemanager.ResourceManager"
                ],
                "methods": [
                    "CompositeService.serviceInit",
                    "ResourceManager.serviceInit",
                    "CompositeService.getServices"
                ]
            },
            "possible_fix": "Modify the serviceInit method to ensure that the iteration over the services list is done in a thread-safe manner, possibly by using a synchronized block or a concurrent collection."
        },
        "possible_fix_code": {
            "org.apache.hadoop.service.CompositeService.serviceInit": "  protected void serviceInit(Configuration conf) throws Exception {\n    List<Service> services;\n    synchronized (serviceList) {\n      services = getServices();\n    }\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(getName() + \": initing services, size=\" + services.size());\n    }\n    for (Service service : services) {\n      service.init(conf);\n    }\n    super.serviceInit(conf);\n  }"
        }
    },
    {
        "filename": "YARN-174.json",
        "creation_time": "2012-10-19T17:25:40.000+0000",
        "bug_report": {
            "Title": "TestNodeStatusUpdater is failing in trunk",
            "Description": "The Node Status Updater is encountering a fatal error during the initialization of the NodeManager. The error message indicates that the path specified for the Yarn log directory, `${yarn.log.dir}/userlogs`, is invalid. This is due to the path needing to either have a file scheme (like `file://`) or no scheme at all. The failure occurs in the `LocalDirsHandlerService` class when it attempts to validate the paths, leading to a cascade of failures that ultimately results in the NodeManager calling `System.exit(-1)`, causing the unit test to exit unexpectedly.",
            "StackTrace": [
                "2012-10-19 12:18:23,941 FATAL [Node Status Updater] nodemanager.NodeManager (NodeManager.java:initAndStartNodeManager(277)) - Error starting NodeManager",
                "org.apache.hadoop.yarn.YarnException: ${yarn.log.dir}/userlogs is not a valid path. Path should be with file scheme or without scheme",
                "at org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService.validatePaths(LocalDirsHandlerService.java:321)",
                "at org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService$MonitoringTimerTask.<init>(LocalDirsHandlerService.java:95)",
                "at org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService.init(LocalDirsHandlerService.java:123)",
                "at org.apache.hadoop.yarn.service.CompositeService.init(CompositeService.java:58)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeHealthCheckerService.init(NodeHealthCheckerService.java:48)",
                "at org.apache.hadoop.yarn.service.CompositeService.init(CompositeService.java:58)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.init(NodeManager.java:165)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.initAndStartNodeManager(NodeManager.java:274)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.stateChanged(NodeManager.java:256)",
                "at org.apache.hadoop.yarn.service.AbstractService.changeState(AbstractService.java:163)",
                "at org.apache.hadoop.yarn.service.AbstractService.stop(AbstractService.java:112)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.stop(NodeStatusUpdaterImpl.java:149)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.reboot(NodeStatusUpdaterImpl.java:157)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.access$900(NodeStatusUpdaterImpl.java:63)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl$1.run(NodeStatusUpdaterImpl.java:357)"
            ],
            "RootCause": "The root cause of the issue is that the Yarn log directory path `${yarn.log.dir}/userlogs` is not being resolved correctly, leading to an invalid path error. This indicates a misconfiguration in the Yarn settings, specifically for the `yarn.log.dir` property.",
            "StepsToReproduce": [
                "Attempt to start the NodeManager.",
                "Observe the initialization process and check for errors related to the Yarn log directory."
            ],
            "ExpectedBehavior": "The NodeManager should start successfully without any fatal errors, and the Yarn log directory should be correctly configured to a valid path.",
            "ObservedBehavior": "The NodeManager fails to start, producing a fatal error due to an invalid Yarn log directory path, which leads to the termination of the unit test.",
            "Suggestions": "1. Check the `yarn-site.xml` configuration file for the `yarn.log.dir` property and ensure it is set correctly. It should either be a valid file path without a scheme or with a proper file scheme (e.g., `file:///path/to/logs`). 2. Verify that any environment variables used in the configuration are correctly set. 3. Restart the Yarn services after making any changes to the configuration.",
            "problem_location": {
                "files": [
                    "NodeManager.java",
                    "LocalDirsHandlerService.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService",
                    "org.apache.hadoop.yarn.server.nodemanager.NodeManager"
                ],
                "methods": [
                    "LocalDirsHandlerService.validatePaths",
                    "NodeManager.initAndStartNodeManager"
                ]
            },
            "possible_fix": "Ensure that the `yarn.log.dir` property in the `yarn-site.xml` file is set to a valid path. For example, update it to `file:///var/log/hadoop-yarn/userlogs` or a similar valid path. After making this change, restart the Yarn services to apply the new configuration."
        },
        "possible_fix_code": {
            "NodeManager.initAndStartNodeManager": "  private void initAndStartNodeManager(boolean hasToReboot) {\n    try {\n\n      // Remove the old hook if we are rebooting.\n      if (hasToReboot && null != nodeManagerShutdownHook) {\n        ShutdownHookManager.get().removeShutdownHook(nodeManagerShutdownHook);\n      }\n\n      nodeManagerShutdownHook = new CompositeServiceShutdownHook(this);\n      ShutdownHookManager.get().addShutdownHook(nodeManagerShutdownHook,\n                                                SHUTDOWN_HOOK_PRIORITY);\n\n      YarnConfiguration conf = new YarnConfiguration();\n      // Set the yarn.log.dir to a valid path if not already set\n      String yarnLogDir = conf.get(YarnConfiguration.LOG_DIRS, \"file:///var/log/hadoop-yarn/userlogs\");\n      conf.set(YarnConfiguration.LOG_DIRS, yarnLogDir);\n\n      this.init(conf);\n      this.start();\n    } catch (Throwable t) {\n      LOG.fatal(\"Error starting NodeManager\", t);\n      System.exit(-1);\n    }\n  }"
        }
    },
    {
        "filename": "YARN-6448.json",
        "creation_time": "2017-04-05T18:39:49.000+0000",
        "bug_report": {
            "Title": "Continuous scheduling thread crashes while sorting nodes",
            "Description": "The continuous scheduling thread in the YARN resource manager crashes due to an `IllegalArgumentException` caused by a comparator that violates its general contract during the sorting of nodes. This issue arises when the `sortedNodeList` method in the `ClusterNodeTracker` class attempts to sort nodes using a comparator that may not be consistent with equals or transitive, particularly when the nodes' available resources change during sorting.",
            "StackTrace": [
                "2017-04-04 23:42:26,123 FATAL org.apache.hadoop.yarn.server.resourcemanager.RMCriticalThreadUncaughtExceptionHandler: Critical thread FairSchedulerContinuousScheduling crashed!",
                "java.lang.IllegalArgumentException: Comparison method violates its general contract!",
                "at java.util.TimSort.mergeHi(TimSort.java:899)",
                "at java.util.TimSort.mergeAt(TimSort.java:516)",
                "at java.util.TimSort.mergeForceCollapse(TimSort.java:457)",
                "at java.util.TimSort.sort(TimSort.java:254)",
                "at java.util.Arrays.sort(Arrays.java:1512)",
                "at java.util.ArrayList.sort(ArrayList.java:1454)",
                "at java.util.Collections.sort(Collections.java:175)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.ClusterNodeTracker.sortedNodeList(ClusterNodeTracker.java:306)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.continuousSchedulingAttempt(FairScheduler.java:884)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler$ContinuousSchedulingThread.run(FairScheduler.java:316)"
            ],
            "RootCause": "The root cause of the crash is an `IllegalArgumentException` thrown by the sorting mechanism in the `sortedNodeList` method of the `ClusterNodeTracker` class. This is likely due to the `NodeAvailableResourceComparator` used for sorting nodes, which may not adhere to the comparator contract, especially if the `Resource` objects being compared are improperly initialized or contain invalid values.",
            "StepsToReproduce": [
                "1. Start the YARN resource manager.",
                "2. Trigger continuous scheduling attempts in the FairScheduler.",
                "3. Ensure that nodes' available resources change during the sorting process."
            ],
            "ExpectedBehavior": "The continuous scheduling thread should sort the nodes based on their available resources without throwing any exceptions, allowing for successful scheduling attempts.",
            "ObservedBehavior": "The continuous scheduling thread crashes with an `IllegalArgumentException` indicating that the comparison method violates its general contract.",
            "Suggestions": "Ensure that all `Resource` objects are properly initialized with valid memory sizes and virtual core counts before being passed to the comparator. Additionally, review the implementation of the `NodeAvailableResourceComparator` to confirm it adheres to the comparator contract.",
            "problem_location": {
                "files": [
                    "ClusterNodeTracker.java",
                    "FairScheduler.java",
                    "NodeAvailableResourceComparator.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.ClusterNodeTracker",
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler",
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.NodeAvailableResourceComparator"
                ],
                "methods": [
                    "ClusterNodeTracker.sortedNodeList",
                    "FairScheduler.continuousSchedulingAttempt",
                    "NodeAvailableResourceComparator.compare"
                ]
            },
            "possible_fix": "Validate the initialization of `Resource` objects in the `Resource` class to ensure they are not created with negative memory sizes or uninitialized values. Additionally, consider adding checks in the `compare` method of the `NodeAvailableResourceComparator` to handle invalid `Resource` instances gracefully."
        },
        "possible_fix_code": {
            "ClusterNodeTracker.sortedNodeList": "  public List<N> sortedNodeList(Comparator<N> comparator) {\n    List<N> sortedList = null;\n    readLock.lock();\n    try {\n      sortedList = new ArrayList(nodes.values());\n    } finally {\n      readLock.unlock();\n    }\n    // Validate resources before sorting\n    for (N node : sortedList) {\n      Resource resource = node.getUnallocatedResource();\n      if (resource.getMemorySize() < 0 || resource.getVirtualCores() < 0) {\n        throw new IllegalArgumentException(\"Node resources must be non-negative.\");\n      }\n    }\n    Collections.sort(sortedList, comparator);\n    return sortedList;\n  }"
        }
    },
    {
        "filename": "YARN-4530.json",
        "creation_time": "2015-12-30T15:19:19.000+0000",
        "bug_report": {
            "Title": "LocalizedResource triggers NPE causing NodeManager exit",
            "Description": "In our cluster, the LocalizedResource download process fails, leading to a NullPointerException (NPE) that causes the NodeManager to shut down. The issue arises when a resource in HDFS is modified after it has been scheduled for processing, resulting in an IOException. This is followed by a NPE in the ResourceLocalizationService, indicating potential problems with the initialization of its components.",
            "StackTrace": [
                "java.io.IOException: Resource hdfs://ns3/user/username/projects/user_insight/lookalike/oozie/workflow/lib/unilever_support_udf-0.0.1-SNAPSHOT.jar changed on src filesystem (expected 1451380519452, was 1451380611793)",
                "at org.apache.hadoop.yarn.util.FSDownload.copy(FSDownload.java:176)",
                "at org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:276)",
                "at org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:50)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:262)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:262)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:745)",
                "java.lang.NullPointerException at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService$PublicLocalizer.run(ResourceLocalizationService.java:712)"
            ],
            "RootCause": "The root cause of the issue is an IOException triggered by a mismatch in the expected and actual modification times of a resource in HDFS, followed by a NullPointerException in the ResourceLocalizationService due to improper initialization of its components.",
            "StepsToReproduce": [
                "1. Schedule a resource for download in HDFS.",
                "2. Modify the resource in HDFS after scheduling.",
                "3. Monitor the NodeManager logs for errors."
            ],
            "ExpectedBehavior": "The NodeManager should successfully download the resource without errors, and the localization service should handle resources without throwing exceptions.",
            "ObservedBehavior": "The NodeManager fails to download the resource, resulting in an IOException and subsequently a NullPointerException, causing the NodeManager to shut down.",
            "Suggestions": "Ensure that resources are not modified after they have been scheduled for processing. Additionally, review the initialization process of the ResourceLocalizationService to ensure all components are properly set up.",
            "problem_location": {
                "files": [
                    "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/ResourceLocalizationService.java",
                    "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/FSDownload.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService",
                    "org.apache.hadoop.yarn.util.FSDownload"
                ],
                "methods": [
                    "FSDownload.copy",
                    "ResourceLocalizationService.run",
                    "ResourceLocalizationService.serviceInit"
                ]
            },
            "possible_fix": "Review the 'copy' method in FSDownload to ensure it handles modification time checks correctly. Additionally, verify the 'serviceInit' method in ResourceLocalizationService to ensure all necessary components are initialized properly before the run method is called."
        },
        "possible_fix_code": {
            "FSDownload.copy": "  private Path copy(Path sCopy, Path dstdir) throws IOException {\n    FileSystem sourceFs = sCopy.getFileSystem(conf);\n    Path dCopy = new Path(dstdir, \"tmp_\" + sCopy.getName());\n    FileStatus sStat = sourceFs.getFileStatus(sCopy);\n    if (sStat.getModificationTime() != resource.getTimestamp()) {\n      throw new IOException(\"Resource \" + sCopy +\n          \" changed on src filesystem (expected \" + resource.getTimestamp() +\n          \", was \" + sStat.getModificationTime() + \")\");\n    }\n    if (resource.getVisibility() == LocalResourceVisibility.PUBLIC) {\n      if (!isPublic(sourceFs, sCopy, sStat, statCache)) {\n        throw new IOException(\"Resource \" + sCopy +\n            \" is not publicly accessible and as such cannot be part of the\" +\n            \" public cache.\");\n      }\n    }\n\n    FileUtil.copy(sourceFs, sStat, FileSystem.getLocal(conf), dCopy, false,\n        true, conf);\n    return dCopy;\n  }",
            "ResourceLocalizationService.serviceInit": "  public void serviceInit(Configuration conf) throws Exception {\n    this.validateConf(conf);\n    this.publicRsrc = new LocalResourcesTrackerImpl(null, null, dispatcher,\n        true, conf, stateStore, dirsHandler);\n    this.recordFactory = RecordFactoryProvider.getRecordFactory(conf);\n\n    try {\n      lfs = getLocalFileContext(conf);\n      lfs.setUMask(new FsPermission((short) FsPermission.DEFAULT_UMASK));\n\n      if (!stateStore.canRecover() || stateStore.isNewlyCreated()) {\n        cleanUpLocalDirs(lfs, delService);\n        initializeLocalDirs(lfs);\n        initializeLogDirs(lfs);\n      }\n    } catch (Exception e) {\n      throw new YarnRuntimeException(\n        \"Failed to initialize LocalizationService\", e);\n    }\n\n    cacheTargetSize =\n      conf.getLong(YarnConfiguration.NM_LOCALIZER_CACHE_TARGET_SIZE_MB, YarnConfiguration.DEFAULT_NM_LOCALIZER_CACHE_TARGET_SIZE_MB) << 20;\n    cacheCleanupPeriod =\n      conf.getLong(YarnConfiguration.NM_LOCALIZER_CACHE_CLEANUP_INTERVAL_MS, YarnConfiguration.DEFAULT_NM_LOCALIZER_CACHE_CLEANUP_INTERVAL_MS);\n    localizationServerAddress = conf.getSocketAddr(\n        YarnConfiguration.NM_BIND_HOST,\n        YarnConfiguration.NM_LOCALIZER_ADDRESS,\n        YarnConfiguration.DEFAULT_NM_LOCALIZER_ADDRESS,\n        YarnConfiguration.DEFAULT_NM_LOCALIZER_PORT);\n\n    localizerTracker = createLocalizerTracker(conf);\n    addService(localizerTracker);\n    dispatcher.register(LocalizerEventType.class, localizerTracker);\n    localDirsChangeListener = new DirsChangeListener() {\n      @Override\n      public void onDirsChanged() {\n        checkAndInitializeLocalDirs();\n      }\n    };\n    logDirsChangeListener = new DirsChangeListener() {\n      @Override\n      public void onDirsChanged() {\n        initializeLogDirs(lfs);\n      }\n    };\n    super.serviceInit(conf);\n  }"
        }
    },
    {
        "filename": "YARN-7737.json",
        "creation_time": "2018-01-11T19:35:01.000+0000",
        "bug_report": {
            "Title": "prelaunch.err file not found exception on container failure",
            "Description": "A FileNotFoundException is encountered when a container fails, specifically when attempting to access the prelaunch error log file located at /grid/b/tmp/userlogs/application_1515190594800_1766/container_e39_1515190594800_1766_01_000002/prelaunch.err. The error occurs within the Hadoop Yarn NodeManager's container management system, indicating that the expected log file does not exist at the specified path. This issue may arise from various factors, including incorrect file paths, failure of prior processes to create the file, or permission issues.",
            "StackTrace": [
                "2018-01-11 19:04:08,036 ERROR org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch: Failed to get tail of the container's prelaunch error log file",
                "java.io.FileNotFoundException: File /grid/b/tmp/userlogs/application_1515190594800_1766/container_e39_1515190594800_1766_01_000002/prelaunch.err does not exist",
                "at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:641)",
                "at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:930)",
                "at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:631)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.handleContainerExitWithFailure(ContainerLaunch.java:545)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.handleContainerExitCode(ContainerLaunch.java:511)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:319)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:93)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:266)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "RootCause": "The root cause of the FileNotFoundException is that the prelaunch error log file is not created or is missing at the expected path due to potential issues in the container launch process or prior tasks not completing successfully.",
            "StepsToReproduce": [
                "1. Launch a container in the Hadoop Yarn environment.",
                "2. Simulate a failure in the container.",
                "3. Observe the logs for the FileNotFoundException related to the prelaunch.err file."
            ],
            "ExpectedBehavior": "The prelaunch error log file should be created and accessible at the specified path when a container fails, allowing for proper logging and debugging.",
            "ObservedBehavior": "The system throws a FileNotFoundException indicating that the prelaunch.err file does not exist, preventing access to error logs for the failed container.",
            "Suggestions": "To resolve this issue, verify the following: 1) Ensure that the file path is correct and points to the intended location. 2) Check if the file is supposed to be created by a previous process and confirm that it completed successfully. 3) Review permissions to ensure the application has access to the directory.",
            "problem_location": {
                "files": [
                    "ContainerLaunch.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher"
                ],
                "methods": [
                    "ContainerLaunch.handleContainerExitWithFailure",
                    "ContainerLaunch.call"
                ]
            },
            "possible_fix": "Investigate the container launch process to ensure that the prelaunch.err file is created correctly. This may involve checking the execution of the commands in the container and ensuring that the environment is set up properly."
        },
        "possible_fix_code": {
            "ContainerLaunch.handleContainerExitWithFailure": "  protected void handleContainerExitWithFailure(ContainerId containerID,\n      int ret, Path containerLogDir, StringBuilder diagnosticInfo) {\n    LOG.warn(\"Container launch failed : \" + diagnosticInfo.toString());\n\n    FileSystem fileSystem = null;\n    long tailSizeInBytes =\n        conf.getLong(YarnConfiguration.NM_CONTAINER_STDERR_BYTES,\n            YarnConfiguration.DEFAULT_NM_CONTAINER_STDERR_BYTES);\n\n    // Append container prelaunch stderr to diagnostics\n    try {\n      fileSystem = FileSystem.getLocal(conf).getRaw();\n      Path preLaunchErrorFilePath = new Path(containerLogDir, ContainerLaunch.CONTAINER_PRE_LAUNCH_STDERR);\n      if (fileSystem.exists(preLaunchErrorFilePath)) {\n        FileStatus preLaunchErrorFileStatus = fileSystem.getFileStatus(preLaunchErrorFilePath);\n        Path errorFile = preLaunchErrorFileStatus.getPath();\n        long fileSize = preLaunchErrorFileStatus.getLen();\n\n        diagnosticInfo.append(\"Error file: \")\n            .append(ContainerLaunch.CONTAINER_PRE_LAUNCH_STDERR).append(\".\\n\");\n\n        byte[] tailBuffer = tailFile(errorFile, fileSize, tailSizeInBytes);\n        diagnosticInfo.append(\"Last \").append(tailSizeInBytes)\n            .append(\" bytes of \").append(errorFile.getName()).append(\" :\\n\")\n            .append(new String(tailBuffer, StandardCharsets.UTF_8));\n      } else {\n        LOG.error(\"Prelaunch error log file does not exist: \" + preLaunchErrorFilePath);\n        diagnosticInfo.append(\"Prelaunch error log file does not exist: \").append(preLaunchErrorFilePath.toString()).append(\".\\n\");\n      }\n    } catch (IOException e) {\n      LOG.error(\"Failed to get tail of the container's prelaunch error log file\", e);\n    }\n\n    // Append container stderr to diagnostics\n    String errorFileNamePattern =\n        conf.get(YarnConfiguration.NM_CONTAINER_STDERR_PATTERN,\n            YarnConfiguration.DEFAULT_NM_CONTAINER_STDERR_PATTERN);\n\n    try {\n      if (fileSystem == null) {\n        fileSystem = FileSystem.getLocal(conf).getRaw();\n      }\n      FileStatus[] errorFileStatuses = fileSystem\n          .globStatus(new Path(containerLogDir, errorFileNamePattern));\n      if (errorFileStatuses != null && errorFileStatuses.length != 0) {\n        Path errorFile = errorFileStatuses[0].getPath();\n        long fileSize = errorFileStatuses[0].getLen();\n\n        // if more than one file matches the stderr pattern, take the latest\n        // modified file, and also append the file names in the diagnosticInfo\n        if (errorFileStatuses.length > 1) {\n          String[] errorFileNames = new String[errorFileStatuses.length];\n          long latestModifiedTime = errorFileStatuses[0].getModificationTime();\n          errorFileNames[0] = errorFileStatuses[0].getPath().getName();\n          for (int i = 1; i < errorFileStatuses.length; i++) {\n            errorFileNames[i] = errorFileStatuses[i].getPath().getName();\n            if (errorFileStatuses[i]\n                .getModificationTime() > latestModifiedTime) {\n              latestModifiedTime = errorFileStatuses[i].getModificationTime();\n              errorFile = errorFileStatuses[i].getPath();\n              fileSize = errorFileStatuses[i].getLen();\n            }\n          }\n          diagnosticInfo.append(\"Error files:\")\n              .append(StringUtils.join(\", \", errorFileNames)).append(\".\\n\");\n        }\n\n        byte[] tailBuffer = tailFile(errorFile, fileSize, tailSizeInBytes);\n        String tailBufferMsg = new String(tailBuffer, StandardCharsets.UTF_8);\n        diagnosticInfo.append(\"Last \").append(tailSizeInBytes)\n            .append(\" bytes of \").append(errorFile.getName()).append(\" :\\n\")\n            .append(tailBufferMsg).append(\"\\n\")\n            .append(analysesErrorMsgOfContainerExitWithFailure(tailBufferMsg));\n\n      }\n    } catch (IOException e) {\n      LOG.error(\"Failed to get tail of the container's error log file\", e);\n    }\n    this.dispatcher.getEventHandler()\n        .handle(new ContainerExitEvent(containerID,\n            ContainerEventType.CONTAINER_EXITED_WITH_FAILURE, ret,\n            diagnosticInfo.toString()));\n  }"
        }
    },
    {
        "filename": "YARN-5136.json",
        "creation_time": "2016-05-24T15:34:28.000+0000",
        "bug_report": {
            "Title": "Error in handling event type APP_ATTEMPT_REMOVED to the scheduler",
            "Description": "The application removal process in the YARN scheduler is failing due to an `IllegalStateException`. The error occurs when attempting to remove an application identified by `org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt@ea94c3b` from a queue where it does not exist. This indicates a potential issue with the application's lifecycle management within the scheduler, particularly in the `FSLeafQueue` and `FairScheduler` classes.",
            "StackTrace": [
                "2016-05-24 23:20:47,202 FATAL org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Error in handling event type APP_ATTEMPT_REMOVED to the scheduler",
                "java.lang.IllegalStateException: Given app to remove org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt@ea94c3b does not exist in queue [root.bdp_xx.bdp_mart_xx_formal, demand=<memory:28672000, vCores:14000>, running=<memory:28647424, vCores:13422>, share=<memory:28672000, vCores:0>, w=<memory weight=1.0, cpu weight=1.0>]",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue.removeApp(FSLeafQueue.java:119)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.removeApplicationAttempt(FairScheduler.java:779)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:1231)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:114)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:680)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "RootCause": "The `IllegalStateException` is thrown because the application being removed from the queue does not exist in that queue. This suggests that the application may not have been added correctly or has already been removed, indicating a flaw in the application's lifecycle management.",
            "StepsToReproduce": [
                "1. Submit an application to the YARN scheduler.",
                "2. Attempt to remove the application from the queue before it has been fully added or after it has already been removed.",
                "3. Observe the resulting error in the logs."
            ],
            "ExpectedBehavior": "The application should be successfully removed from the queue without any exceptions, indicating that it was present in the queue at the time of removal.",
            "ObservedBehavior": "An `IllegalStateException` is thrown, indicating that the application does not exist in the queue when attempting to remove it.",
            "Suggestions": "Review the application lifecycle management in the `FairScheduler` and `FSLeafQueue` classes. Ensure that applications are properly tracked and that removal operations only occur for applications that are confirmed to be present in the queue.",
            "problem_location": {
                "files": [
                    "hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FSLeafQueue.java",
                    "hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue",
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler"
                ],
                "methods": [
                    "FSLeafQueue.removeApp",
                    "FairScheduler.removeApplicationAttempt"
                ]
            },
            "possible_fix": "Ensure that the `removeApp` method in `FSLeafQueue` checks for the application's existence in both the runnable and non-runnable lists before attempting to remove it. Additionally, verify that the `addApplicationAttempt` method in `FairScheduler` correctly adds applications to the queue and manages their state throughout their lifecycle."
        },
        "possible_fix_code": {
            "FSLeafQueue.removeApp": "  public boolean removeApp(FSAppAttempt app) {\n    boolean runnable = false;\n\n    // Remove app from runnable/nonRunnable list while holding the write lock\n    writeLock.lock();\n    try {\n      runnable = runnableApps.remove(app);\n      if (!runnable) {\n        // Check if the app exists in nonRunnableApps before attempting to remove\n        if (!removeNonRunnableApp(app)) {\n          throw new IllegalStateException(\"Given app to remove \" + app +\n              \" does not exist in queue \" + this);\n        }\n      }\n    } finally {\n      writeLock.unlock();\n    }\n\n    // Update AM resource usage if needed. If isAMRunning is true, we are not\n    // running an unmanaged AM.\n    if (runnable && app.isAmRunning()) {\n      Resources.subtractFrom(amResourceUsage, app.getAMResource());\n    }\n\n    return runnable;\n  }"
        }
    },
    {
        "filename": "YARN-8211.json",
        "creation_time": "2018-04-26T02:13:22.000+0000",
        "bug_report": {
            "Title": "Yarn registry dns log finds BufferUnderflowException on port ping",
            "Description": "The Yarn registry DNS server is encountering a `BufferUnderflowException` during its operation. This exception is logged when the server attempts to read from a buffer that does not contain enough data. The stack trace indicates that the issue arises in the `RegistryDNS` class, specifically within the `getMessgeLength` and `nioTCPClient` methods. The `nioTCPClient` method reads data from a `SocketChannel` into a `ByteBuffer`, but it does not verify if sufficient data has been read before calling `getMessgeLength`, which expects at least two bytes.",
            "StackTrace": [
                "2018-04-25 01:36:56,139 WARN concurrent.ExecutorHelper (ExecutorHelper.java:logThrowableFromAfterExecute(50)) - Execution exception when running task in RegistryDNS 76",
                "2018-04-25 01:36:56,139 WARN concurrent.ExecutorHelper (ExecutorHelper.java:logThrowableFromAfterExecute(63)) - Caught exception in thread RegistryDNS 76:",
                "java.nio.BufferUnderflowException",
                "at java.nio.Buffer.nextGetIndex(Buffer.java:500)",
                "at java.nio.HeapByteBuffer.get(HeapByteBuffer.java:135)",
                "at org.apache.hadoop.registry.server.dns.RegistryDNS.getMessgeLength(RegistryDNS.java:820)",
                "at org.apache.hadoop.registry.server.dns.RegistryDNS.nioTCPClient(RegistryDNS.java:767)",
                "at org.apache.hadoop.registry.server.dns.RegistryDNS$3.call(RegistryDNS.java:846)",
                "at org.apache.hadoop.registry.server.dns.RegistryDNS$3.call(RegistryDNS.java:843)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:266)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)",
                "at java.lang.Thread.run(Thread.java:748)"
            ],
            "RootCause": "The `BufferUnderflowException` is caused by the `nioTCPClient` method not ensuring that enough data has been read into the `ByteBuffer` before calling `getMessgeLength`, which expects at least two bytes. If fewer than two bytes are available, it leads to the exception.",
            "StepsToReproduce": [
                "Start the Yarn registry DNS server.",
                "Send a request to the server that results in a read operation on the `SocketChannel`.",
                "Monitor the logs for `BufferUnderflowException`."
            ],
            "ExpectedBehavior": "The server should successfully read the incoming message length and process the request without throwing exceptions.",
            "ObservedBehavior": "The server throws a `BufferUnderflowException` when attempting to read from the buffer, indicating insufficient data.",
            "Suggestions": "Implement a check in the `nioTCPClient` method to ensure that at least two bytes are read into the `ByteBuffer` before calling `getMessgeLength`. This can prevent the `BufferUnderflowException` from occurring.",
            "problem_location": {
                "files": [
                    "RegistryDNS.java"
                ],
                "classes": [
                    "org.apache.hadoop.registry.server.dns.RegistryDNS"
                ],
                "methods": [
                    "getMessgeLength",
                    "nioTCPClient"
                ]
            },
            "possible_fix": "Modify the `nioTCPClient` method to check the number of bytes read from the `SocketChannel` before calling `getMessgeLength`. For example:\n\n```java\nint bytesRead = ch.read(buf);\nif (bytesRead < 2) {\n    throw new IOException(\"Not enough data read from channel\");\n}\nbuf.flip();\nint messageLength = getMessgeLength(buf);\n```"
        },
        "possible_fix_code": {
            "nioTCPClient": "  public void nioTCPClient(SocketChannel ch) throws IOException {\n    try {\n      ByteBuffer buf = ByteBuffer.allocate(1024);\n      int bytesRead = ch.read(buf);\n      if (bytesRead < 2) {\n          throw new IOException(\"Not enough data read from channel\");\n      }\n      buf.flip();\n      int messageLength = getMessgeLength(buf);\n\n      byte[] in = new byte[messageLength];\n\n      buf.get(in, 0, messageLength);\n\n      Message query;\n      byte[] response;\n      try {\n        query = new Message(in);\n        LOG.info(\"received TCP query {}\", query.getQuestion());\n        response = generateReply(query, ch.socket());\n        if (response == null) {\n          return;\n        }\n      } catch (IOException e) {\n        response = formErrorMessage(in);\n      }\n\n      ByteBuffer out = ByteBuffer.allocate(response.length + 2);\n      out.clear();\n      byte[] data = new byte[2];\n\n      data[1] = (byte)(response.length & 0xFF);\n      data[0] = (byte)((response.length >> 8) & 0xFF);\n      out.put(data);\n      out.put(response);\n      out.flip();\n\n      while(out.hasRemaining()) {\n        ch.write(out);\n      }\n\n    } catch (IOException e) {\n      throw NetUtils.wrapException(ch.socket().getInetAddress().getHostName(),\n          ch.socket().getPort(),\n          ch.socket().getLocalAddress().getHostName(),\n          ch.socket().getLocalPort(), e);\n    } finally {\n      IOUtils.closeStream(ch);\n    }\n  }"
        }
    },
    {
        "filename": "YARN-2124.json",
        "creation_time": "2014-06-05T07:44:27.000+0000",
        "bug_report": {
            "Title": "ProportionalCapacityPreemptionPolicy cannot work because it's initialized before scheduler initialized",
            "Description": "The issue arises when the ProportionalCapacityPreemptionPolicy is initialized prior to the CapacityScheduler, leading to a NullPointerException (NPE) during the ResourceManager startup. The stack trace indicates that the NPE occurs in the `greaterThan` method of the `Resources` class, which is invoked within the `computeIdealResourceDistribution` method of the ProportionalCapacityPreemptionPolicy. This is due to the ResourceCalculator being null, as it is not yet initialized when the policy is created.",
            "StackTrace": [
                "2014-06-05 11:01:33,201 ERROR org.apache.hadoop.yarn.YarnUncaughtExceptionHandler: Thread Thread[SchedulingMonitor (ProportionalCapacityPreemptionPolicy),5,main] threw an Exception.",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.util.resource.Resources.greaterThan(Resources.java:225)",
                "at org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy.computeIdealResourceDistribution(ProportionalCapacityPreemptionPolicy.java:302)",
                "at org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy.recursivelyComputeIdealAssignment(ProportionalCapacityPreemptionPolicy.java:261)",
                "at org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy.containerBasedPreemptOrKill(ProportionalCapacityPreemptionPolicy.java:198)",
                "at org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy.editSchedule(ProportionalCapacityPreemptionPolicy.java:174)",
                "at org.apache.hadoop.yarn.server.resourcemanager.monitor.SchedulingMonitor.invokePolicy(SchedulingMonitor.java:72)",
                "at org.apache.hadoop.yarn.server.resourcemanager.monitor.SchedulingMonitor$PreemptionChecker.run(SchedulingMonitor.java:82)",
                "at java.lang.Thread.run(Thread.java:744)"
            ],
            "RootCause": "The root cause of the NullPointerException is that the ProportionalCapacityPreemptionPolicy is initialized before the CapacityScheduler, resulting in a null ResourceCalculator when the `greaterThan` method is called.",
            "StepsToReproduce": [
                "Initialize the ProportionalCapacityPreemptionPolicy before the CapacityScheduler.",
                "Start the ResourceManager.",
                "Observe the logs for a NullPointerException."
            ],
            "ExpectedBehavior": "The ProportionalCapacityPreemptionPolicy should function correctly without throwing a NullPointerException, allowing for proper resource preemption management.",
            "ObservedBehavior": "A NullPointerException is thrown during the initialization of the ResourceManager, specifically when the `greaterThan` method is called due to a null ResourceCalculator.",
            "Suggestions": "Ensure that the ProportionalCapacityPreemptionPolicy is initialized only after the CapacityScheduler is fully initialized. Additionally, implement null checks for Resource objects and the ResourceCalculator before invoking the `greaterThan` method.",
            "problem_location": {
                "files": [
                    "ProportionalCapacityPreemptionPolicy.java",
                    "Resources.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy",
                    "org.apache.hadoop.yarn.util.resource.Resources"
                ],
                "methods": [
                    "ProportionalCapacityPreemptionPolicy.computeIdealResourceDistribution",
                    "Resources.greaterThan"
                ]
            },
            "possible_fix": "Modify the initialization sequence to ensure that the CapacityScheduler is initialized before the ProportionalCapacityPreemptionPolicy. Additionally, add null checks in the `computeIdealResourceDistribution` method to verify that ResourceCalculator and Resource objects are not null before calling `greaterThan`."
        },
        "possible_fix_code": {
            "ProportionalCapacityPreemptionPolicy.computeIdealResourceDistribution": "  private void computeIdealResourceDistribution(ResourceCalculator rc,\n      List<TempQueue> queues, Resource totalPreemptionAllowed, Resource tot_guarant) {\n\n    // qAlloc tracks currently active queues (will decrease progressively as\n    // demand is met)\n    List<TempQueue> qAlloc = new ArrayList<TempQueue>(queues);\n    // unassigned tracks how much resources are still to assign, initialized\n    // with the total capacity for this set of queues\n    Resource unassigned = Resources.clone(tot_guarant);\n\n    // group queues based on whether they have non-zero guaranteed capacity\n    Set<TempQueue> nonZeroGuarQueues = new HashSet<TempQueue>();\n    Set<TempQueue> zeroGuarQueues = new HashSet<TempQueue>();\n\n    for (TempQueue q : qAlloc) {\n      if (q.guaranteed == null || Resources.none() == null) {\n        continue; // Skip if guaranteed resource is null\n      }\n      if (Resources.greaterThan(rc, tot_guarant, q.guaranteed, Resources.none())) {\n        nonZeroGuarQueues.add(q);\n      } else {\n        zeroGuarQueues.add(q);\n      }\n    }\n\n    // first compute the allocation as a fixpoint based on guaranteed capacity\n    computeFixpointAllocation(rc, tot_guarant, nonZeroGuarQueues, unassigned,\n        false);\n\n    // if any capacity is left unassigned, distributed among zero-guarantee \n    // queues uniformly (i.e., not based on guaranteed capacity, as this is zero)\n    if (!zeroGuarQueues.isEmpty()\n        && Resources.greaterThan(rc, tot_guarant, unassigned, Resources.none())) {\n      computeFixpointAllocation(rc, tot_guarant, zeroGuarQueues, unassigned,\n          true);\n    }\n    \n    // based on ideal assignment computed above and current assignment we derive\n    // how much preemption is required overall\n    Resource totPreemptionNeeded = Resource.newInstance(0, 0);\n    for (TempQueue t:queues) {\n      if (Resources.greaterThan(rc, tot_guarant, t.current, t.idealAssigned)) {\n        Resources.addTo(totPreemptionNeeded,\n            Resources.subtract(t.current, t.idealAssigned));\n      }\n    }\n\n    // if we need to preempt more than is allowed, compute a factor (0<f<1)\n    // that is used to scale down how much we ask back from each queue\n    float scalingFactor = 1.0F;\n    if (Resources.greaterThan(rc, tot_guarant,\n          totPreemptionNeeded, totalPreemptionAllowed)) {\n       scalingFactor = Resources.divide(rc, tot_guarant,\n           totalPreemptionAllowed, totPreemptionNeeded);\n    }\n\n    // assign to each queue the amount of actual preemption based on local\n    // information of ideal preemption and scaling factor\n    for (TempQueue t : queues) {\n      t.assignPreemption(scalingFactor, rc, tot_guarant);\n    }\n    if (LOG.isDebugEnabled()) {\n      long time = clock.getTime();\n      for (TempQueue t : queues) {\n        LOG.debug(time + \": \" + t);\n      }\n    }\n\n  }"
        }
    },
    {
        "filename": "YARN-8591.json",
        "creation_time": "2018-07-27T05:56:26.000+0000",
        "bug_report": {
            "Title": "[ATSv2] NPE while checking for entity acl in non-secure cluster",
            "Description": "A NullPointerException (NPE) occurs in the `TimelineReaderWebServices` class when handling requests for YARN container entities in a non-secure cluster. The issue arises during the access check for `TimelineEntity` objects, specifically when the `info` field of an entity is null. This leads to an unhandled exception when the application attempts to access user-related information from the entity's `info` field.",
            "StackTrace": [
                "2018-07-27 05:32:03,468 WARN  webapp.GenericExceptionHandler (GenericExceptionHandler.java:toResponse(98)) - INTERNAL_SERVER_ERROR",
                "javax.ws.rs.WebApplicationException: java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices.handleException(TimelineReaderWebServices.java:196)",
                "at org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices.getEntities(TimelineReaderWebServices.java:624)",
                "at org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices.getEntities(TimelineReaderWebServices.java:474)",
                "Caused by: java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices.checkAccess(TimelineReaderWebServices.java:3536)",
                "at org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices.checkAccessForGenericEntities(TimelineReaderWebServices.java:3513)",
                "at org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices.getEntities(TimelineReaderWebServices.java:622)"
            ],
            "RootCause": "The root cause of the NullPointerException is due to the `info` field of a `TimelineEntity` being null when accessed in the `checkAccess` method. This can occur if the `entities` set passed to `checkAccessForGenericEntities` contains entities that do not have their `info` field properly initialized.",
            "StepsToReproduce": [
                "Send a GET request to the timeline service endpoint for YARN container entities.",
                "Ensure that the request parameters lead to a scenario where the `info` field of at least one `TimelineEntity` is null.",
                "Observe the server logs for the NullPointerException."
            ],
            "ExpectedBehavior": "The system should successfully check access permissions for the requested YARN container entities without throwing a NullPointerException.",
            "ObservedBehavior": "The system throws a NullPointerException when attempting to access the `info` field of a `TimelineEntity` that is null, resulting in an internal server error.",
            "Suggestions": "Implement null checks for the `info` field in the `checkAccessForGenericEntities` method to prevent the NullPointerException. Additionally, ensure that the data retrieval process in `getEntities` handles cases where entities might not have the expected structure.",
            "problem_location": {
                "files": [
                    "TimelineReaderWebServices.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices"
                ],
                "methods": [
                    "checkAccess",
                    "checkAccessForGenericEntities",
                    "getEntities"
                ]
            },
            "possible_fix": "Add null checks in the `checkAccessForGenericEntities` method to handle cases where the `info` field of a `TimelineEntity` is null. For example, modify the method to check if `entity.getInfo()` is null before attempting to access its fields."
        },
        "possible_fix_code": {
            "checkAccessForGenericEntities": "  private void checkAccessForGenericEntities(Set<TimelineEntity> entities,\n      UserGroupInformation callerUGI, String entityType) throws Exception {\n    if (entities != null && !entities.isEmpty() && isDisplayEntityPerUserFilterEnabled(\n        getTimelineReaderManager().getConfig())) {\n      TimelineReaderContext timelineReaderContext = null;\n      for (TimelineEntity entity : entities) {\n        if (entity.getInfo() != null) {\n          String uid = (String) entity.getInfo().get(TimelineReaderUtils.FROMID_KEY);\n          if (TimelineEntityType.YARN_APPLICATION.matches(entityType)) {\n            timelineReaderContext =\n                TimelineFromIdConverter.APPLICATION_FROMID.decodeUID(uid);\n          } else {\n            timelineReaderContext =\n                TimelineFromIdConverter.GENERIC_ENTITY_FROMID.decodeUID(uid);\n          }\n          checkAccess(getTimelineReaderManager(), callerUGI,\n              timelineReaderContext.getUserId());\n        } else {\n          // Handle the case where entity info is null, log or throw an exception if necessary\n          LOG.warn(\"Entity info is null for entity: \" + entity);\n        }\n      }\n    }\n  }"
        }
    },
    {
        "filename": "YARN-6649.json",
        "creation_time": "2017-05-25T20:36:08.000+0000",
        "bug_report": {
            "Title": "RollingLevelDBTimelineServer throws RuntimeException if object decoding ever fails",
            "Description": "The issue arises when using the Tez UI, which makes REST API calls to the Timeline Service. Some of these calls return a 500 Internal Server Error due to an inability to encode a class from a specific code (1000). This problem is linked to the YARN-6654 issue, which necessitates handling object decoding more gracefully to prevent internal server errors and instead return partial messages to the client. The stack trace indicates that the error occurs in the `getEntity` methods of both `TimelineWebServices` and `RollingLevelDBTimelineStore`, where the serialization framework (FST) fails to encode a value, likely due to a missing class definition or serialization configuration mismatch.",
            "StackTrace": [
                "2017-05-30 12:47:10,670 WARN org.apache.hadoop.yarn.webapp.GenericExceptionHandler: INTERNAL_SERVER_ERROR",
                "javax.ws.rs.WebApplicationException: java.lang.RuntimeException: java.io.IOException: java.lang.RuntimeException: unable to encodeValue class from code 1000",
                "at org.apache.hadoop.yarn.server.timeline.webapp.TimelineWebServices.getEntity(TimelineWebServices.java:164)",
                "at org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore.getEntity(RollingLevelDBTimelineStore.java:478)",
                "at org.apache.hadoop.yarn.server.timeline.EntityFileTimelineStore.getEntity(EntityFileTimelineStore.java:911)",
                "at org.apache.hadoop.yarn.server.timeline.TimelineDataManager.getEntity(TimelineDataManager.java:202)"
            ],
            "RootCause": "The root cause of the issue is the failure of the FST serialization framework to encode a class identified by code 1000. This could be due to a missing class definition or a mismatch in serialization configurations, leading to the inability to retrieve the expected entity.",
            "StepsToReproduce": [
                "1. Use the Tez UI to make a REST API call to the Timeline Service.",
                "2. Observe the response for a 500 Internal Server Error.",
                "3. Check the server logs for the stack trace related to the error."
            ],
            "ExpectedBehavior": "The system should successfully retrieve the requested timeline entity and return it without errors, or provide a meaningful partial response if the entity cannot be fully retrieved.",
            "ObservedBehavior": "The system returns a 500 Internal Server Error when attempting to retrieve a timeline entity, indicating a failure in the encoding process.",
            "Suggestions": "1. Verify that all classes expected by the FST serialization framework are present and correctly defined. 2. Check the serialization configurations to ensure they are set up correctly and that class mappings are accurate. 3. Review documentation related to the FST serialization framework for guidance on handling encoding issues.",
            "problem_location": {
                "files": [
                    "RollingLevelDBTimelineStore.java",
                    "TimelineWebServices.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore",
                    "org.apache.hadoop.yarn.server.timeline.webapp.TimelineWebServices"
                ],
                "methods": [
                    "RollingLevelDBTimelineStore.getEntity",
                    "TimelineWebServices.getEntity"
                ]
            },
            "possible_fix": "Ensure that the class definitions expected by the FST serialization framework are included in the project. Additionally, review and correct any serialization configuration settings that may be causing the mismatch."
        },
        "possible_fix_code": {
            "RollingLevelDBTimelineStore.getEntity": "  private static TimelineEntity getEntity(String entityId, String entityType,\n      Long startTime, EnumSet<Field> fields, DBIterator iterator,\n      byte[] prefix, int prefixlen) throws IOException {\n    if (fields == null) {\n      fields = EnumSet.allOf(Field.class);\n    }\n\n    TimelineEntity entity = new TimelineEntity();\n    boolean events = false;\n    boolean lastEvent = false;\n    if (fields.contains(Field.EVENTS)) {\n      events = true;\n    } else if (fields.contains(Field.LAST_EVENT_ONLY)) {\n      lastEvent = true;\n    } else {\n      entity.setEvents(null);\n    }\n    boolean relatedEntities = false;\n    if (fields.contains(Field.RELATED_ENTITIES)) {\n      relatedEntities = true;\n    } else {\n      entity.setRelatedEntities(null);\n    }\n    boolean primaryFilters = false;\n    if (fields.contains(Field.PRIMARY_FILTERS)) {\n      primaryFilters = true;\n    } else {\n      entity.setPrimaryFilters(null);\n    }\n    boolean otherInfo = false;\n    if (fields.contains(Field.OTHER_INFO)) {\n      otherInfo = true;\n    } else {\n      entity.setOtherInfo(null);\n    }\n\n    // iterate through the entity's entry, parsing information if it is part\n    // of a requested field\n    for (; iterator.hasNext(); iterator.next()) {\n      byte[] key = iterator.peekNext().getKey();\n      if (!prefixMatches(prefix, prefixlen, key)) {\n        break;\n      }\n      if (key.length == prefixlen) {\n        continue;\n      }\n      if (key[prefixlen] == PRIMARY_FILTERS_COLUMN[0]) {\n        if (primaryFilters) {\n          addPrimaryFilter(entity, key, prefixlen\n              + PRIMARY_FILTERS_COLUMN.length);\n        }\n      } else if (key[prefixlen] == OTHER_INFO_COLUMN[0]) {\n        if (otherInfo) {\n          try {\n            entity.addOtherInfo(\n                parseRemainingKey(key, prefixlen + OTHER_INFO_COLUMN.length),\n                fstConf.asObject(iterator.peekNext().getValue()));\n          } catch (Exception e) {\n            LOG.error(\"Failed to decode other info for entity: \" + entityId, e);\n            entity.addOtherInfo(\n                parseRemainingKey(key, prefixlen + OTHER_INFO_COLUMN.length),\n                null); // Set to null or a default value\n          }\n        }\n      } else if (key[prefixlen] == RELATED_ENTITIES_COLUMN[0]) {\n        if (relatedEntities) {\n          addRelatedEntity(entity, key, prefixlen\n              + RELATED_ENTITIES_COLUMN.length);\n        }\n      } else if (key[prefixlen] == EVENTS_COLUMN[0]) {\n        if (events || (lastEvent && entity.getEvents().size() == 0)) {\n          TimelineEvent event = getEntityEvent(null, key, prefixlen\n              + EVENTS_COLUMN.length, iterator.peekNext().getValue());\n          if (event != null) {\n            entity.addEvent(event);\n          }\n        }\n      } else if (key[prefixlen] == DOMAIN_ID_COLUMN[0]) {\n        byte[] v = iterator.peekNext().getValue();\n        String domainId = new String(v, UTF_8);\n        entity.setDomainId(domainId);\n      } else {\n        LOG.warn(String.format(\"Found unexpected column for entity %s of \"\n            + \"type %s (0x%02x)\", entityId, entityType, key[prefixlen]));\n      }\n    }\n\n    entity.setEntityId(entityId);\n    entity.setEntityType(entityType);\n    entity.setStartTime(startTime);\n\n    return entity;\n  }"
        }
    },
    {
        "filename": "YARN-3742.json",
        "creation_time": "2015-05-29T06:00:38.000+0000",
        "bug_report": {
            "Title": "YARN RM will shut down if ZKClient creation times out",
            "Description": "The ResourceManager (RM) in YARN experiences a fatal shutdown when the ZKClient fails to establish a connection within the specified timeout period. This issue arises during the recovery process, specifically when the RM attempts to interact with Zookeeper for state management. The expected behavior is for the RM to transition to a standby state instead of shutting down completely, allowing another RM to take over its responsibilities. The stack trace indicates that the timeout occurs in the `ZKRMStateStore` class, particularly in the `existsWithRetries` method, which is responsible for checking the existence of nodes in Zookeeper with retry logic.",
            "StackTrace": [
                "2015-04-19 01:22:20,513  FATAL org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Received a org.apache.hadoop.yarn.server.resourcemanager.RMFatalEvent of type STATE_STORE_OP_FAILED. Cause:",
                "java.io.IOException: Wait for ZKClient creation timed out",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore$ZKAction.runWithCheck(ZKRMStateStore.java:1066)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore$ZKAction.runWithRetries(ZKRMStateStore.java:1090)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore.existsWithRetries(ZKRMStateStore.java:996)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore.updateApplicationStateInternal(ZKRMStateStore.java:643)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$UpdateAppTransition.transition(RMStateStore.java:162)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$UpdateAppTransition.transition(RMStateStore.java:147)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:362)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore.handleStoreEvent(RMStateStore.java:806)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$ForwardingEventHandler.handle(RMStateStore.java:879)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore$ForwardingEventHandler.handle(RMStateStore.java:874)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:173)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:106)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "RootCause": "The root cause of the issue is a timeout during the creation of the ZKClient, which is critical for the ResourceManager's recovery process. This timeout can be attributed to misconfiguration in the Zookeeper settings or network issues preventing the RM from connecting to the Zookeeper server.",
            "StepsToReproduce": [
                "1. Configure YARN with Zookeeper settings in the yarn-site.xml file.",
                "2. Simulate a scenario where the Zookeeper server is unreachable or under heavy load.",
                "3. Start the YARN ResourceManager.",
                "4. Observe the logs for the fatal error indicating a timeout during ZKClient creation."
            ],
            "ExpectedBehavior": "The ResourceManager should transition to a standby state instead of shutting down completely when the ZKClient creation times out, allowing another RM to take over its responsibilities.",
            "ObservedBehavior": "The ResourceManager shuts down completely, resulting in a loss of service when the ZKClient fails to connect within the timeout period.",
            "Suggestions": "1. Review and adjust the Zookeeper timeout settings in the yarn-site.xml configuration file, particularly the RM_ZK_TIMEOUT_MS parameter. Consider increasing its value if the Zookeeper server is under heavy load or if there are known network latency issues.\n2. Ensure that the ResourceManager has proper network connectivity to the Zookeeper server and that the Zookeeper ensemble is healthy.",
            "problem_location": {
                "files": [
                    "yarn-site.xml",
                    "ZKRMStateStore.java",
                    "RMStateStore.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore",
                    "org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore"
                ],
                "methods": [
                    "ZKRMStateStore.existsWithRetries",
                    "RMStateStore.handleStoreEvent"
                ]
            },
            "possible_fix": "Increase the timeout value for ZKClient connections in the yarn-site.xml configuration file by modifying the RM_ZK_TIMEOUT_MS parameter. For example, set it to a higher value such as 30000 (30 seconds) to allow more time for the connection to be established."
        },
        "possible_fix_code": {
            "org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore.existsWithRetries": "  private Stat existsWithRetries(\n      final String path, final boolean watch) throws Exception {\n    return new ZKAction<Stat>() {\n      @Override\n      Stat run() throws KeeperException, InterruptedException {\n        // Check if zkClient is initialized and handle timeout\n        if (zkClient == null) {\n          throw new IOException(\"ZKClient is not initialized\");\n        }\n        return zkClient.exists(path, watch);\n      }\n    }.runWithRetries();\n  }"
        }
    },
    {
        "filename": "YARN-4984.json",
        "creation_time": "2016-04-21T19:16:03.000+0000",
        "bug_report": {
            "Title": "LogAggregationService shouldn't swallow exception in handling createAppDir() which causes thread leak.",
            "Description": "The LogAggregationService in Hadoop is encountering an issue where it fails to handle exceptions properly during the creation of application log directories. This is particularly problematic due to the presence of stale applications in the NodeManager (NM) state store, as indicated by YARN-4325. When an application is initiated with an invalid token, the service swallows the exception instead of handling it appropriately, leading to the creation of an aggregator thread for an invalid application. The error message indicates that the token cannot be found in the cache, which typically occurs when the token has expired or was not cached correctly.",
            "StackTrace": [
                "158 2016-04-19 23:38:33,039 ERROR logaggregation.LogAggregationService (LogAggregationService.java:run(300)) - Failed to setup application log directory for application_1448060878692_11842",
                "159 org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.token.SecretManager$InvalidToken): token (HDFS_DELEGATION_TOKEN token 1380589 for hdfswrite) can't be found in cache",
                "160 at org.apache.hadoop.ipc.Client.call(Client.java:1427)",
                "161 at org.apache.hadoop.ipc.Client.call(Client.java:1358)",
                "162 at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:229)",
                "163 at com.sun.proxy.$Proxy13.getFileInfo(Unknown Source)",
                "164 at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:771)",
                "165 at sun.reflect.GeneratedMethodAccessor76.invoke(Unknown Source)",
                "166 at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:252)",
                "167 at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)",
                "168 at com.sun.proxy.$Proxy14.getFileInfo(Unknown Source)",
                "169 at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:2116)",
                "170 at org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:1315)",
                "171 at org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:1311)",
                "172 at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)",
                "173 at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1311)",
                "174 at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService.checkExists(LogAggregationService.java:248)",
                "175 at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService.access$100(LogAggregationService.java:67)",
                "176 at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService$1.run(LogAggregationService.java:276)",
                "177 at java.security.AccessController.doPrivileged(Native Method)",
                "178 at javax.security.auth.Subject.doAs(Subject.java:415)",
                "179 at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1657)",
                "180 at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService.createAppDir(LogAggregationService.java:261)",
                "181 at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService.initAppAggregator(LogAggregationService.java:367)",
                "182 at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService.initApp(LogAggregationService.java:320)",
                "183 at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService.handle(LogAggregationService.java:447)",
                "184 at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService.handle(LogAggregationService.java:67)"
            ],
            "RootCause": "The root cause of the issue is that the LogAggregationService is swallowing exceptions related to invalid Hadoop delegation tokens, which leads to the creation of threads for applications that should not be processed due to token invalidation.",
            "StepsToReproduce": [
                "1. Start the NodeManager with stale applications in the state store.",
                "2. Attempt to initiate an application with an invalid HDFS delegation token.",
                "3. Observe the logs for the error message indicating that the token cannot be found in the cache."
            ],
            "ExpectedBehavior": "The LogAggregationService should properly handle exceptions related to invalid tokens, preventing the creation of aggregator threads for invalid applications.",
            "ObservedBehavior": "The LogAggregationService swallows the exception and continues to create an aggregator thread for the invalid application, leading to resource leaks.",
            "Suggestions": "Implement proper exception handling in the LogAggregationService to ensure that invalid tokens are not ignored, and prevent the creation of threads for invalid applications.",
            "problem_location": {
                "files": [
                    "LogAggregationService.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService"
                ],
                "methods": [
                    "createAppDir",
                    "initAppAggregator",
                    "handle"
                ]
            },
            "possible_fix": "Add a check in the LogAggregationService methods to handle exceptions related to invalid tokens appropriately. For example, modify the createAppDir method to log the error and return early if the token is invalid."
        },
        "possible_fix_code": {
            "createAppDir": "  protected void createAppDir(final String user, final ApplicationId appId,\n      UserGroupInformation userUgi) {\n    try {\n      userUgi.doAs(new PrivilegedExceptionAction<Object>() {\n        @Override\n        public Object run() throws Exception {\n          try {\n            // TODO: Reuse FS for user?\n            FileSystem remoteFS = getFileSystem(getConfig());\n\n            // Only creating directories if they are missing to avoid\n            // unnecessary load on the filesystem from all of the nodes\n            Path appDir = LogAggregationUtils.getRemoteAppLogDir(\n                LogAggregationService.this.remoteRootLogDir, appId, user,\n                LogAggregationService.this.remoteRootLogDirSuffix);\n            appDir = appDir.makeQualified(remoteFS.getUri(),\n                remoteFS.getWorkingDirectory());\n\n            if (!checkExists(remoteFS, appDir, APP_DIR_PERMISSIONS)) {\n              Path suffixDir = LogAggregationUtils.getRemoteLogSuffixedDir(\n                  LogAggregationService.this.remoteRootLogDir, user,\n                  LogAggregationService.this.remoteRootLogDirSuffix);\n              suffixDir = suffixDir.makeQualified(remoteFS.getUri(),\n                  remoteFS.getWorkingDirectory());\n\n              if (!checkExists(remoteFS, suffixDir, APP_DIR_PERMISSIONS)) {\n                Path userDir = LogAggregationUtils.getRemoteLogUserDir(\n                    LogAggregationService.this.remoteRootLogDir, user);\n                userDir = userDir.makeQualified(remoteFS.getUri(),\n                    remoteFS.getWorkingDirectory());\n\n                if (!checkExists(remoteFS, userDir, APP_DIR_PERMISSIONS)) {\n                  createDir(remoteFS, userDir, APP_DIR_PERMISSIONS);\n                }\n\n                createDir(remoteFS, suffixDir, APP_DIR_PERMISSIONS);\n              }\n\n              createDir(remoteFS, appDir, APP_DIR_PERMISSIONS);\n            }\n\n          } catch (IOException e) {\n            LOG.error(\"Failed to setup application log directory for \" + appId, e);\n            // Handle the exception properly to avoid swallowing it\n            throw new YarnRuntimeException(\"Failed to create app directory due to invalid token\", e);\n          }\n          return null;\n        }\n      });\n    } catch (Exception e) {\n      throw new YarnRuntimeException(e);\n    }\n  }"
        }
    },
    {
        "filename": "YARN-4584.json",
        "creation_time": "2016-01-12T09:08:31.000+0000",
        "bug_report": {
            "Title": "RM startup failure when AM attempts greater than max-attempts",
            "Description": "The ResourceManager (RM) fails to restart after an Application Master (AM) is preempted multiple times due to resource limits in the default queue. This issue arises when applications are submitted to three queues with specific memory allocations, leading to a NullPointerException during the recovery process of the RM. The stack trace indicates that the exception occurs in the `RMAppAttemptImpl.recover` method, which is responsible for recovering the state of the application attempts.",
            "StackTrace": [
                "2016-01-12 10:49:04,081 DEBUG org.apache.hadoop.service.AbstractService: noteFailure java.lang.NullPointerException",
                "2016-01-12 10:49:04,081 INFO org.apache.hadoop.service.AbstractService: Service RMActiveServices failed in state STARTED; cause: java.lang.NullPointerException",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.recover(RMAppAttemptImpl.java:887)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.recover(RMAppImpl.java:826)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl$RMAppRecoveredTransition.transition(RMAppImpl.java:953)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl$RMAppRecoveredTransition.transition(RMAppImpl.java:946)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$MultipleInternalArc.doTransition(StateMachineFactory.java:385)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.handle(RMAppImpl.java:786)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppManager.recoverApplication(RMAppManager.java:328)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppManager.recover(RMAppManager.java:464)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.recover(ResourceManager.java:1232)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:594)",
                "at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager.java:1022)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1062)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1058)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1705)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.transitionToActive(ResourceManager.java:1058)",
                "at org.apache.hadoop.yarn.server.resourcemanager.AdminService.transitionToActive(AdminService.java:323)",
                "at org.apache.hadoop.yarn.server.resourcemanager.EmbeddedElectorService.becomeActive(EmbeddedElectorService.java:127)",
                "at org.apache.hadoop.ha.ActiveStandbyElector.becomeActive(ActiveStandbyElector.java:877)",
                "at org.apache.hadoop.ha.ActiveStandbyElector.processResult(ActiveStandbyElector.java:467)",
                "at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:599)",
                "at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498)"
            ],
            "RootCause": "The root cause of the issue is a NullPointerException occurring in the `RMAppAttemptImpl.recover` method. This happens when the application state is not properly initialized or retrieved, leading to an attempt to access a null reference during the recovery process.",
            "StepsToReproduce": [
                "Configure three queues in the cluster with the following memory allocations: 40% for queue 1, 50% for queue 2, and 10% for the default queue.",
                "Submit applications to all three queues with a container size of 1024MB, specifically using a sleep job with 50 containers on each queue.",
                "Allow the AM assigned to the default queue to be preempted multiple times (approximately 20 times).",
                "Restart the ResourceManager after the AM has been preempted."
            ],
            "ExpectedBehavior": "The ResourceManager should successfully restart and recover the application states without throwing any exceptions.",
            "ObservedBehavior": "The ResourceManager fails to restart and throws a NullPointerException during the recovery process.",
            "Suggestions": "Investigate the initialization of application states in the `RMAppAttemptImpl.recover` and `RMAppImpl.recover` methods. Ensure that all necessary states are properly set before recovery is attempted. Consider adding null checks or initializing default values to prevent NullPointerExceptions.",
            "problem_location": {
                "files": [
                    "RMAppAttemptImpl.java",
                    "RMAppImpl.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl",
                    "org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl"
                ],
                "methods": [
                    "RMAppAttemptImpl.recover",
                    "RMAppImpl.recover"
                ]
            },
            "possible_fix": "In the `RMAppAttemptImpl.recover` method, add checks to ensure that `appState` and `attemptState` are not null before accessing their properties. For example:\n\nif (appState != null) {\n    ApplicationAttemptStateData attemptState = appState.getAttempt(getAppAttemptId());\n    if (attemptState != null) {\n        // Proceed with recovery logic\n    } else {\n        // Handle null attempt state\n    }\n} else {\n    // Handle null application state\n}"
        },
        "possible_fix_code": {
            "RMAppAttemptImpl.recover": "  public void recover(RMState state) {\n    ApplicationStateData appState =\n        state.getApplicationState().get(getAppAttemptId().getApplicationId());\n    if (appState != null) {\n        ApplicationAttemptStateData attemptState =\n            appState.getAttempt(getAppAttemptId());\n        if (attemptState != null) {\n            LOG.info(\"Recovering attempt: \" + getAppAttemptId() + \" with final state: \"\n                + attemptState.getState());\n            diagnostics.append(\"Attempt recovered after RM restart\");\n            diagnostics.append(attemptState.getDiagnostics());\n            this.amContainerExitStatus = attemptState.getAMContainerExitStatus();\n            if (amContainerExitStatus == ContainerExitStatus.PREEMPTED) {\n                this.attemptMetrics.setIsPreempted();\n            }\n\n            Credentials credentials = attemptState.getAppAttemptTokens();\n            setMasterContainer(attemptState.getMasterContainer());\n            recoverAppAttemptCredentials(credentials, attemptState.getState());\n            this.recoveredFinalState = attemptState.getState();\n            this.originalTrackingUrl = attemptState.getFinalTrackingUrl();\n            this.finalStatus = attemptState.getFinalApplicationStatus();\n            this.startTime = attemptState.getStartTime();\n            this.finishTime = attemptState.getFinishTime();\n            this.attemptMetrics.updateAggregateAppResourceUsage(\n                attemptState.getMemorySeconds(),attemptState.getVcoreSeconds());\n        } else {\n            // Handle null attempt state\n            LOG.warn(\"Attempt state is null for attempt ID: \" + getAppAttemptId());\n        }\n    } else {\n        // Handle null application state\n        LOG.warn(\"Application state is null for application ID: \" + getAppAttemptId().getApplicationId());\n    }\n  }"
        }
    },
    {
        "filename": "YARN-2846.json",
        "creation_time": "2014-11-11T15:30:08.000+0000",
        "bug_report": {
            "Title": "Incorrect persist exit code for running containers in reacquireContainer() that interrupted by NodeManager restart.",
            "Description": "The issue arises when the NodeManager (NM) is restarted while an Application Master (AM) container is running. During the NM shutdown, the `reacquireContainer()` method in the `ContainerExecutor` class is interrupted, leading to an `IOException`. This exception occurs because the method is waiting for the container process to exit, but the NM stop signal interrupts the sleep operation, causing the exit code to be recorded as LOST. This behavior is problematic as it results in the container being marked as COMPLETE after NM restart, but with an incorrect exit code of LOST (154).",
            "StackTrace": [
                "2014-11-11 00:48:35,214 INFO  monitor.ContainersMonitorImpl (ContainersMonitorImpl.java:run(408)) - Memory usage of ProcessTree 22140 for container-id container_1415666714233_0001_01_000084: 53.8 MB of 512 MB physical memory used; 931.3 MB of 1.0 GB virtual memory used",
                "2014-11-11 00:48:35,223 ERROR nodemanager.NodeManager (SignalLogger.java:handle(60)) - RECEIVED SIGNAL 15: SIGTERM",
                "2014-11-11 00:48:35,299 INFO  mortbay.log (Slf4jLog.java:info(67)) - Stopped HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50060",
                "2014-11-11 00:48:35,337 INFO  containermanager.ContainerManagerImpl (ContainerManagerImpl.java:cleanUpApplicationsOnNMShutDown(512)) - Applications still running : [application_1415666714233_0001]",
                "2014-11-11 00:48:35,406 ERROR launcher.RecoveredContainerLaunch (RecoveredContainerLaunch.java:call(87)) - Unable to recover container container_1415666714233_0001_01_000001",
                "java.io.IOException: Interrupted while waiting for process 20001 to exit",
                "at org.apache.hadoop.yarn.server.nodemanager.ContainerExecutor.reacquireContainer(ContainerExecutor.java:180)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.RecoveredContainerLaunch.call(RecoveredContainerLaunch.java:82)",
                "Caused by: java.lang.InterruptedException: sleep interrupted",
                "at java.lang.Thread.sleep(Native Method)",
                "at org.apache.hadoop.yarn.server.nodemanager.ContainerExecutor.reacquireContainer(ContainerExecutor.java:177)"
            ],
            "RootCause": "The root cause of the issue is an `InterruptedException` that occurs in the `reacquireContainer()` method of the `ContainerExecutor` class when the NodeManager is stopped. This interruption prevents the method from properly waiting for the container process to exit, leading to an incorrect exit code being recorded.",
            "StepsToReproduce": [
                "1. Start a container using the NodeManager.",
                "2. Initiate a restart of the NodeManager while the container is still running.",
                "3. Observe the exit code recorded for the container after the NodeManager restarts."
            ],
            "ExpectedBehavior": "The expected behavior is that the exit code for the container should be accurately recorded based on its actual termination status, rather than being marked as LOST due to an interruption.",
            "ObservedBehavior": "The observed behavior is that the exit code for the container is recorded as LOST (154) after the NodeManager restarts, even though the container was running at the time of the NM shutdown.",
            "Suggestions": "To resolve this issue, the logic in the `reacquireContainer()` method should be modified to prevent recording the exit code of running containers if an interruption is detected during the waiting period.",
            "problem_location": {
                "files": [
                    "ContainerExecutor.java",
                    "RecoveredContainerLaunch.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.nodemanager.ContainerExecutor",
                    "org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.RecoveredContainerLaunch"
                ],
                "methods": [
                    "ContainerExecutor.reacquireContainer",
                    "RecoveredContainerLaunch.call"
                ]
            },
            "possible_fix": "Modify the `reacquireContainer()` method in `ContainerExecutor` to check for interruptions and avoid recording the exit code if an interruption occurs. For example, add a condition to skip the exit code recording logic if an `InterruptedException` is caught."
        },
        "possible_fix_code": {
            "reacquireContainer": "  public int reacquireContainer(String user, ContainerId containerId)\n      throws IOException {\n    Path pidPath = getPidFilePath(containerId);\n    if (pidPath == null) {\n      LOG.warn(containerId + \" is not active, returning terminated error\");\n      return ExitCode.TERMINATED.getExitCode();\n    }\n\n    String pid = null;\n    pid = ProcessIdFileReader.getProcessId(pidPath);\n    if (pid == null) {\n      throw new IOException(\"Unable to determine pid for \" + containerId);\n    }\n\n    LOG.info(\"Reacquiring \" + containerId + \" with pid \" + pid);\n    try {\n      while(isContainerProcessAlive(user, pid)) {\n        Thread.sleep(1000);\n      }\n    } catch (InterruptedException e) {\n      LOG.warn(\"Interrupted while waiting for process \" + pid + \" to exit, skipping exit code recording\");\n      return ExitCode.TERMINATED.getExitCode();\n    }\n\n    // wait for exit code file to appear\n    String exitCodeFile = ContainerLaunch.getExitCodeFile(pidPath.toString());\n    File file = new File(exitCodeFile);\n    final int sleepMsec = 100;\n    int msecLeft = 2000;\n    while (!file.exists() && msecLeft >= 0) {\n      if (!isContainerActive(containerId)) {\n        LOG.info(containerId + \" was deactivated\");\n        return ExitCode.TERMINATED.getExitCode();\n      }\n      try {\n        Thread.sleep(sleepMsec);\n      } catch (InterruptedException e) {\n        LOG.warn(\"Interrupted while waiting for exit code from \" + containerId + \", skipping exit code recording\");\n        return ExitCode.TERMINATED.getExitCode();\n      }\n      msecLeft -= sleepMsec;\n    }\n    if (msecLeft < 0) {\n      throw new IOException(\"Timeout while waiting for exit code from \" + containerId);\n    }\n\n    try {\n      return Integer.parseInt(FileUtils.readFileToString(file).trim());\n    } catch (NumberFormatException e) {\n      throw new IOException(\"Error parsing exit code from pid \" + pid, e);\n    }\n  }"
        }
    },
    {
        "filename": "YARN-7890.json",
        "creation_time": "2018-02-03T21:10:43.000+0000",
        "bug_report": {
            "Title": "NPE during container relaunch",
            "Description": "A NullPointerException (NPE) occurs during the relaunch of a container in the Hadoop YARN NodeManager. The issue arises when the system attempts to create an unmodifiable list from a null reference in the `getFilecacheDirs` method of the `ContainerStartContext` class. This method is invoked during the container launch process, specifically when the container is transitioning from the RELAUNCHING state to RUNNING. The stack trace indicates that the NPE is triggered because the `filecacheDirs` field is not properly initialized before being accessed.",
            "StackTrace": [
                "2018-02-02 21:02:40,026 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl: Container container_e02_1517604848419_0002_01_000004 transitioned from RELAUNCHING to RUNNING",
                "2018-02-02 21:02:40,026 WARN org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerRelaunch: Failed to relaunch container.",
                "java.lang.NullPointerException",
                "at java.util.Collections$UnmodifiableCollection.<init>(Collections.java:1026)",
                "at java.util.Collections$UnmodifiableList.<init>(Collections.java:1302)",
                "at java.util.Collections.unmodifiableList(Collections.java:1287)",
                "at org.apache.hadoop.yarn.server.nodemanager.executor.ContainerStartContext.getFilecacheDirs(ContainerStartContext.java:200)",
                "at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.buildContainerRuntimeContext(LinuxContainerExecutor.java:651)",
                "at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:546)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.launchContainer(ContainerLaunch.java:465)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerRelaunch.call(ContainerRelaunch.java:107)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerRelaunch.call(ContainerRelaunch.java:49)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:266)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)",
                "at java.lang.Thread.run(Thread.java:748)"
            ],
            "RootCause": "The root cause of the NullPointerException is that the `filecacheDirs` field in the `ContainerStartContext` class is null when the `getFilecacheDirs` method is called. This typically occurs if the `setFilecacheDirs` method in the `Builder` class is not invoked before constructing the `ContainerStartContext` instance.",
            "StepsToReproduce": [
                "1. Attempt to relaunch a container in the Hadoop YARN NodeManager.",
                "2. Ensure that the `ContainerStartContext` is created without calling `setFilecacheDirs` on the builder.",
                "3. Observe the logs for the NullPointerException during the relaunch process."
            ],
            "ExpectedBehavior": "The container should relaunch successfully without throwing a NullPointerException, and the `getFilecacheDirs` method should return a valid unmodifiable list of file cache directories.",
            "ObservedBehavior": "The system throws a NullPointerException when attempting to access the file cache directories during the container relaunch process.",
            "Suggestions": "Ensure that the `setFilecacheDirs` method is called with a valid list of directories before invoking the `build` method on the `ContainerStartContext.Builder`. This will prevent the `filecacheDirs` field from being null.",
            "problem_location": {
                "files": [
                    "hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/executor/ContainerStartContext.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.nodemanager.executor.ContainerStartContext",
                    "org.apache.hadoop.yarn.server.nodemanager.executor.ContainerStartContext.Builder"
                ],
                "methods": [
                    "ContainerStartContext.getFilecacheDirs",
                    "ContainerStartContext.Builder.setFilecacheDirs",
                    "ContainerStartContext.Builder.build"
                ]
            },
            "possible_fix": "To fix the issue, ensure that the `setFilecacheDirs` method is called with a non-null list before the `build` method is invoked. Example code modification:\n\n```java\nContainerStartContext.Builder builder = new ContainerStartContext.Builder();\nList<String> fileCacheDirs = new ArrayList<>(); // Populate with valid directories\nbuilder.setFilecacheDirs(fileCacheDirs);\nContainerStartContext context = builder.build();\n```"
        },
        "possible_fix_code": {
            "ContainerStartContext.Builder.build": "    public ContainerStartContext build() {\n      if (this.filecacheDirs == null) {\n          throw new IllegalStateException(\"filecacheDirs must be set before building ContainerStartContext\");\n      }\n      return new ContainerStartContext(this);\n    }"
        }
    },
    {
        "filename": "YARN-139.json",
        "creation_time": "2012-10-01T19:51:20.000+0000",
        "bug_report": {
            "Title": "Interrupted Exception within AsyncDispatcher leads to user confusion",
            "Description": "During the shutdown process of the Hadoop application, an `InterruptedException` is frequently logged, particularly within the `AsyncDispatcher` class. This exception occurs when the dispatcher attempts to stop its event handling thread, which can lead to confusion among users, despite the exception being harmless. The stack trace indicates that the exception arises during the `join()` call after the thread has been interrupted, highlighting a potential area for improvement in the shutdown process to enhance user experience.",
            "StackTrace": [
                "java.lang.InterruptedException",
                "at java.lang.Object.wait(Native Method)",
                "at java.lang.Thread.join(Thread.java:1143)",
                "at java.lang.Thread.join(Thread.java:1196)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.stop(AsyncDispatcher.java:105)",
                "at org.apache.hadoop.yarn.service.CompositeService.stop(CompositeService.java:99)",
                "at org.apache.hadoop.yarn.service.CompositeService.stop(CompositeService.java:89)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobFinishEventHandler.handle(MRAppMaster.java:437)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobFinishEventHandler.handle(MRAppMaster.java:402)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:126)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:75)",
                "at java.lang.Thread.run(Thread.java:619)"
            ],
            "RootCause": "The `InterruptedException` occurs in the `stop()` method of the `AsyncDispatcher` class when it attempts to join the event handling thread after interrupting it. This is part of the service management process in Hadoop, where interruptions can happen during the controlled shutdown of services.",
            "StepsToReproduce": [
                "Start a Hadoop application that utilizes the AsyncDispatcher.",
                "Trigger a shutdown of the application.",
                "Observe the logs for any warnings related to InterruptedExceptions."
            ],
            "ExpectedBehavior": "The application should shut down gracefully without logging any warnings or exceptions that could confuse users.",
            "ObservedBehavior": "During the shutdown process, an `InterruptedException` is logged, indicating that the event handling thread was interrupted, which may lead to user confusion.",
            "Suggestions": "Consider modifying the shutdown process to handle interruptions more gracefully, potentially by suppressing the logging of harmless exceptions or providing clearer messaging in the logs.",
            "problem_location": {
                "files": [
                    "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.event.AsyncDispatcher.java",
                    "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.service.CompositeService.java",
                    "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.event.AsyncDispatcher",
                    "org.apache.hadoop.yarn.service.CompositeService",
                    "org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobFinishEventHandler"
                ],
                "methods": [
                    "AsyncDispatcher.stop",
                    "CompositeService.stop",
                    "MRAppMaster$JobFinishEventHandler.handle"
                ]
            },
            "possible_fix": "In the `AsyncDispatcher.stop()` method, consider adding a check to suppress logging of the `InterruptedException` if it occurs during the shutdown process, or implement a more user-friendly logging mechanism that clarifies the nature of the exception."
        },
        "possible_fix_code": {
            "AsyncDispatcher.stop": "  public void stop() {\n    stopped = true;\n    if (eventHandlingThread != null) {\n      eventHandlingThread.interrupt();\n      try {\n        eventHandlingThread.join();\n      } catch (InterruptedException ie) {\n        // Suppress logging of InterruptedException during shutdown\n      }\n    }\n\n    // stop all the components\n    super.stop();\n  }"
        }
    },
    {
        "filename": "YARN-42.json",
        "creation_time": "2012-05-14T11:38:55.000+0000",
        "bug_report": {
            "Title": "Node Manager throws NPE on startup",
            "Description": "The Node Manager (NM) in Hadoop YARN encounters a NullPointerException (NPE) during startup due to a failure in initializing the LocalizationService. This issue arises when the NM attempts to create a directory at `/mrv2/tmp/nm-local-dir/usercache`, which fails due to an IOException, likely caused by permission issues or the absence of the parent directory. Additionally, the NPE occurs when stopping the NonAggregatingLogHandler, indicating a secondary issue that may need attention.",
            "StackTrace": [
                "2012-05-14 16:32:13,468 FATAL org.apache.hadoop.yarn.server.nodemanager.NodeManager: Error starting NodeManager",
                "org.apache.hadoop.yarn.YarnException: Failed to initialize LocalizationService",
                "\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService.init(ResourceLocalizationService.java:202)",
                "\tat org.apache.hadoop.yarn.service.CompositeService.init(CompositeService.java:58)",
                "\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.init(ContainerManagerImpl.java:183)",
                "\tat org.apache.hadoop.yarn.service.CompositeService.init(CompositeService.java:58)",
                "\tat org.apache.hadoop.yarn.server.nodemanager.NodeManager.init(NodeManager.java:166)",
                "\tat org.apache.hadoop.yarn.server.nodemanager.NodeManager.initAndStartNodeManager(NodeManager.java:268)",
                "\tat org.apache.hadoop.yarn.server.nodemanager.NodeManager.main(NodeManager.java:284)",
                "Caused by: java.io.IOException: mkdir of /mrv2/tmp/nm-local-dir/usercache failed",
                "\tat org.apache.hadoop.fs.FileSystem.primitiveMkdir(FileSystem.java:907)",
                "\tat org.apache.hadoop.fs.DelegateToFileSystem.mkdir(DelegateToFileSystem.java:143)",
                "\tat org.apache.hadoop.fs.FilterFs.mkdir(FilterFs.java:189)",
                "\tat org.apache.hadoop.fs.FileContext$4.next(FileContext.java:706)",
                "\tat org.apache.hadoop.fs.FileContext$4.next(FileContext.java:703)",
                "\tat org.apache.hadoop.fs.FileContext.mkdir(FileContext.java:703)",
                "\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService.init(ResourceLocalizationService.java:188)",
                "... 6 more",
                "2012-05-14 16:32:13,472 INFO org.apache.hadoop.yarn.service.CompositeService: Error stopping org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.NonAggregatingLogHandler",
                "java.lang.NullPointerException",
                "\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.NonAggregatingLogHandler.stop(NonAggregatingLogHandler.java:82)",
                "\tat org.apache.hadoop.yarn.service.CompositeService.stop(CompositeService.java:99)",
                "\tat org.apache.hadoop.yarn.service.CompositeService.stop(CompositeService.java:89)",
                "\tat org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.stop(ContainerManagerImpl.java:266)",
                "\tat org.apache.hadoop.yarn.service.CompositeService.stop(CompositeService.java:99)",
                "\tat org.apache.hadoop.yarn.service.CompositeService.stop(CompositeService.java:89)",
                "\tat org.apache.hadoop.yarn.server.nodemanager.NodeManager.stop(NodeManager.java:182)",
                "\tat org.apache.hadoop.yarn.service.CompositeService$CompositeServiceShutdownHook.run(CompositeService.java:122)",
                "\tat org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)"
            ],
            "RootCause": "The root cause of the issue is an IOException when the Node Manager attempts to create the directory `/mrv2/tmp/nm-local-dir/usercache`, which is likely due to insufficient permissions or the non-existence of the parent directory.",
            "StepsToReproduce": [
                "Start the Node Manager without the necessary permissions on the local directory.",
                "Ensure that the directory `/mrv2/tmp/nm-local-dir/usercache` does not exist or the parent directory is missing."
            ],
            "ExpectedBehavior": "The Node Manager should start successfully without throwing any exceptions, initializing the LocalizationService and creating necessary directories as required.",
            "ObservedBehavior": "The Node Manager fails to start, throwing a NullPointerException and an IOException related to directory creation.",
            "Suggestions": "1. Check and ensure that the user running the Node Manager has the necessary permissions to create directories in the specified path. 2. Verify that the parent directory `/mrv2/tmp/nm-local-dir` exists. 3. Review the YARN configuration files for correct paths. 4. Ensure sufficient disk space is available.",
            "problem_location": {
                "files": [
                    "ResourceLocalizationService.java",
                    "NodeManager.java",
                    "ContainerManagerImpl.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService",
                    "org.apache.hadoop.yarn.server.nodemanager.NodeManager",
                    "org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl"
                ],
                "methods": [
                    "ResourceLocalizationService.init",
                    "NodeManager.init",
                    "ContainerManagerImpl.init"
                ]
            },
            "possible_fix": "Ensure that the directory `/mrv2/tmp/nm-local-dir/usercache` can be created by the Node Manager by adjusting permissions or creating the parent directory manually. Additionally, review the YARN configuration for any misconfigurations."
        },
        "possible_fix_code": {
            "ResourceLocalizationService.init": "protected void init() throws IOException {\n    Path userCachePath = new Path(\"/mrv2/tmp/nm-local-dir/usercache\");\n    // Ensure the parent directory exists\n    Path parentDir = userCachePath.getParent();\n    if (!FileSystem.get(parentDir.getFileSystem().getConf()).exists(parentDir)) {\n        // Attempt to create the parent directory with appropriate permissions\n        if (!FileSystem.get(parentDir.getFileSystem().getConf()).mkdirs(parentDir, new FsPermission(FsAction.ALL, FsAction.NONE, FsAction.NONE))) {\n            throw new IOException(\"Failed to create parent directory: \" + parentDir);\n        }\n    }\n    // Now create the user cache directory\n    if (!FileSystem.get(userCachePath.getFileSystem().getConf()).mkdirs(userCachePath, new FsPermission(FsAction.ALL, FsAction.NONE, FsAction.NONE))) {\n        throw new IOException(\"mkdir of \" + userCachePath + \" failed\");\n    }\n    // Additional initialization logic...\n}"
        }
    },
    {
        "filename": "YARN-7453.json",
        "creation_time": "2017-11-07T09:46:28.000+0000",
        "bug_report": {
            "Title": "Fix issue where RM fails to switch to active after first successful start",
            "Description": "The ResourceManager (RM) fails to transition to the ACTIVE state after its first successful start, resulting in a continuous loop between ACTIVE and STANDBY states. The issue is accompanied by a `NoAuthException` from Apache ZooKeeper, indicating that the RM is attempting to perform an operation without the necessary authentication credentials. This problem arises during the recovery process when the RM tries to load its state from ZooKeeper.",
            "StackTrace": [
                "2017-11-07 15:08:11,664 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Transitioning to active state",
                "2017-11-07 15:08:11,669 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Recovery started",
                "2017-11-07 15:08:11,669 INFO org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore: Loaded RM state version info 1.5",
                "2017-11-07 15:08:11,670 ERROR org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Failed to load/recover state",
                "org.apache.zookeeper.KeeperException$NoAuthException: KeeperErrorCode = NoAuth",
                "at org.apache.zookeeper.KeeperException.create(KeeperException.java:113)",
                "at org.apache.zookeeper.ZooKeeper.multiInternal(ZooKeeper.java:1006)",
                "at org.apache.zookeeper.ZooKeeper.multi(ZooKeeper.java:910)",
                "at org.apache.curator.framework.imps.CuratorTransactionImpl.doOperation(CuratorTransactionImpl.java:159)",
                "at org.apache.curator.framework.imps.CuratorTransactionImpl.access$200(CuratorTransactionImpl.java:44)",
                "at org.apache.curator.framework.imps.CuratorTransactionImpl$2.call(CuratorTransactionImpl.java:129)",
                "at org.apache.curator.framework.imps.CuratorTransactionImpl$2.call(CuratorTransactionImpl.java:125)",
                "at org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:109)",
                "at org.apache.curator.framework.imps.CuratorTransactionImpl.commit(CuratorTransactionImpl.java:122)",
                "at org.apache.hadoop.util.curator.ZKCuratorManager$SafeTransaction.commit(ZKCuratorManager.java:403)",
                "at org.apache.hadoop.util.curator.ZKCuratorManager.safeSetData(ZKCuratorManager.java:372)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore.getAndIncrementEpoch(ZKRMStateStore.java:493)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:771)",
                "at org.apache.hadoop.service.AbstractService.start(AbstractService.java:194)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager.java:1162)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1202)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1198)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1962)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.transitionToActive(ResourceManager.java:1198)",
                "at org.apache.hadoop.yarn.server.resourcemanager.AdminService.transitionToActive(AdminService.java:320)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ActiveStandbyElectorBasedElectorService.becomeActive(ActiveStandbyElectorBasedElectorService.java:144)",
                "at org.apache.hadoop.ha.ActiveStandbyElector.becomeActive(ActiveStandbyElector.java:894)",
                "at org.apache.hadoop.ha.ActiveStandbyElector.processResult(ActiveStandbyElector.java:473)",
                "at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:607)",
                "at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:505)"
            ],
            "RootCause": "The root cause of the issue is a `NoAuthException` from Apache ZooKeeper, indicating that the ResourceManager is attempting to perform an operation without the necessary authentication credentials. This typically occurs when the ZooKeeper server requires authentication, but the client (ResourceManager) has not provided valid credentials.",
            "StepsToReproduce": [
                "Start the ResourceManager for the first time.",
                "Observe the transition from ACTIVE to STANDBY and back to ACTIVE.",
                "Monitor the logs for any authentication-related errors."
            ],
            "ExpectedBehavior": "The ResourceManager should successfully transition to the ACTIVE state after the first successful start without encountering authentication errors.",
            "ObservedBehavior": "The ResourceManager fails to transition to the ACTIVE state and enters a loop between ACTIVE and STANDBY states, logging a `NoAuthException`.",
            "Suggestions": "Ensure that the ResourceManager is configured with the correct authentication credentials for ZooKeeper. Review the ZooKeeper configuration to verify that the authentication mechanism (e.g., SASL, digest) is correctly set up and that the ResourceManager is providing the necessary credentials.",
            "problem_location": {
                "files": [
                    "ResourceManager.java",
                    "ZKRMStateStore.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.ResourceManager",
                    "org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore"
                ],
                "methods": [
                    "ResourceManager.transitionToActive",
                    "ZKRMStateStore.getAndIncrementEpoch"
                ]
            },
            "possible_fix": "Check the ZooKeeper configuration for authentication settings and ensure that the ResourceManager is correctly configured to provide the necessary credentials. This may involve modifying the ResourceManager's configuration files to include authentication details."
        },
        "possible_fix_code": {
            "ResourceManager.transitionToActive": "  synchronized void transitionToActive() throws Exception {\n    if (rmContext.getHAServiceState() == HAServiceProtocol.HAServiceState.ACTIVE) {\n      LOG.info(\"Already in active state\");\n      return;\n    }\n    LOG.info(\"Transitioning to active state\");\n\n    // Ensure that the ResourceManager is configured with the correct authentication credentials\n    UserGroupInformation ugi = UserGroupInformation.getCurrentUser();\n    if (ugi == null) {\n      throw new IOException(\"UserGroupInformation is not set. Please check the configuration.\");\n    }\n\n    this.rmLoginUGI.doAs(new PrivilegedExceptionAction<Void>() {\n      @Override\n      public Void run() throws Exception {\n        try {\n          startActiveServices();\n          return null;\n        } catch (Exception e) {\n          reinitialize(true);\n          throw e;\n        }\n      }\n    });\n\n    rmContext.setHAServiceState(HAServiceProtocol.HAServiceState.ACTIVE);\n    LOG.info(\"Transitioned to active state\");\n  }"
        }
    },
    {
        "filename": "YARN-3369.json",
        "creation_time": "2015-03-18T23:29:06.000+0000",
        "bug_report": {
            "Title": "Missing NullPointer check in AppSchedulingInfo causes RM to die",
            "Description": "The issue arises in the `checkForDeactivation()` method of the `AppSchedulingInfo` class, where a `NullPointerException` is thrown due to the lack of null checks on the result of `getResourceRequest()`. This method can return null if the `requests` map does not contain an entry for the specified priority. The subsequent call to `request.getNumContainers()` on a null reference leads to the ResourceManager crashing. Additionally, the `getPriorities()` method may also return null, which would cause a similar issue when iterating over its results.",
            "StackTrace": [
                "2015-03-17 14:14:04,757 FATAL org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Error in handling event type NODE_UPDATE to the scheduler",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.checkForDeactivation(AppSchedulingInfo.java:383)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.decrementOutstanding(AppSchedulingInfo.java:375)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.allocateOffSwitch(AppSchedulingInfo.java:360)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.allocate(AppSchedulingInfo.java:270)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp.allocate(FiCaSchedulerApp.java:142)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainer(LeafQueue.java:1559)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignOffSwitchContainers(LeafQueue.java:1384)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainersOnNode(LeafQueue.java:1263)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainers(LeafQueue.java:816)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.assignContainersToChildQueues(ParentQueue.java:588)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.assignContainers(ParentQueue.java:449)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.allocateContainersToNode(CapacityScheduler.java:1017)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:1059)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:114)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:739)",
                "at java.lang.Thread.run(Thread.java:722)",
                "2015-03-17 14:14:04,758 INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Exiting, bbye.."
            ],
            "RootCause": "The `NullPointerException` is caused by the `checkForDeactivation()` method attempting to dereference a null `ResourceRequest` object returned by `getResourceRequest()`, which can occur if the `requests` map does not contain an entry for the specified priority.",
            "StepsToReproduce": [
                "Trigger a scheduling event that leads to the invocation of the `checkForDeactivation()` method.",
                "Ensure that the `requests` map does not contain any entries for the priorities being checked.",
                "Observe the resulting `NullPointerException` in the logs."
            ],
            "ExpectedBehavior": "The ResourceManager should handle resource requests gracefully without crashing, even if some requests are missing or uninitialized.",
            "ObservedBehavior": "The ResourceManager crashes with a `NullPointerException`, leading to a failure in handling scheduling events.",
            "Suggestions": "Implement null checks in the `checkForDeactivation()` method to handle cases where `getResourceRequest()` returns null. Additionally, ensure that the `priorities` collection is initialized before use.",
            "problem_location": {
                "files": [
                    "AppSchedulingInfo.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo"
                ],
                "methods": [
                    "checkForDeactivation",
                    "getResourceRequest",
                    "getPriorities"
                ]
            },
            "possible_fix": "Add null checks in the `checkForDeactivation()` method as follows:\n\n```java\nResourceRequest request = getResourceRequest(priority, ResourceRequest.ANY);\nif (request != null && request.getNumContainers() > 0) {\n    deactivate = false;\n    break;\n}\n```\nAlso, ensure that `priorities` is initialized properly in the constructor or relevant initialization method."
        },
        "possible_fix_code": {
            "checkForDeactivation": "  synchronized private void checkForDeactivation() {\n    boolean deactivate = true;\n    Collection<Priority> priorities = getPriorities();\n    if (priorities != null) {\n      for (Priority priority : priorities) {\n        ResourceRequest request = getResourceRequest(priority, ResourceRequest.ANY);\n        if (request != null && request.getNumContainers() > 0) {\n          deactivate = false;\n          break;\n        }\n      }\n    }\n    if (deactivate) {\n      activeUsersManager.deactivateApplication(user, applicationId);\n    }\n  }"
        }
    },
    {
        "filename": "YARN-945.json",
        "creation_time": "2013-07-19T22:59:06.000+0000",
        "bug_report": {
            "Title": "AM register failing after AMRMToken",
            "Description": "The application is encountering an authentication error in a Hadoop environment, specifically indicating that 'SIMPLE authentication is not enabled.' The error occurs when the IPC Server listener attempts to process a request from a client, leading to an AccessControlException. This suggests a misconfiguration in the authentication settings, where the system expects a different authentication method (TOKEN) but is not set up to use SIMPLE authentication.",
            "StackTrace": [
                "509 2013-07-19 15:53:55,569 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 54313: readAndProcess from client 127.0.0.1 threw exception [org.apache.hadoop.security.AccessControlException: SIMPLE authentication is not enabled. Available:[TOKEN]]",
                "510 org.apache.hadoop.security.AccessControlException: SIMPLE authentication is not enabled. Available:[TOKEN]",
                "511   at org.apache.hadoop.ipc.Server$Connection.initializeAuthContext(Server.java:1531)",
                "512   at org.apache.hadoop.ipc.Server$Connection.readAndProcess(Server.java:1482)",
                "513   at org.apache.hadoop.ipc.Server$Listener.doRead(Server.java:788)",
                "514   at org.apache.hadoop.ipc.Server$Listener$Reader.doRunLoop(Server.java:587)",
                "515   at org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:562)"
            ],
            "RootCause": "The root cause of the issue is that the Hadoop configuration is not set to use SIMPLE authentication, which is required for the current operation. The system is expecting a different authentication method (TOKEN) but is not configured to support SIMPLE authentication.",
            "StepsToReproduce": [
                "1. Start the Hadoop application with the current configuration.",
                "2. Attempt to register the Application Master (AM) which triggers the IPC Server listener.",
                "3. Observe the logs for the AccessControlException indicating SIMPLE authentication is not enabled."
            ],
            "ExpectedBehavior": "The Application Master should register successfully without any authentication errors, allowing the IPC Server to process requests from clients.",
            "ObservedBehavior": "The Application Master fails to register, and an AccessControlException is thrown, indicating that SIMPLE authentication is not enabled.",
            "Suggestions": "Check the Hadoop configuration files, specifically `core-site.xml` and `hdfs-site.xml`, for the property `hadoop.security.authentication`. Ensure it is set to 'simple' to enable SIMPLE authentication. If it is set to 'TOKEN' or another method, adjust it accordingly based on your authentication requirements.",
            "problem_location": {
                "files": [
                    "core-site.xml",
                    "hdfs-site.xml"
                ],
                "classes": [
                    "org.apache.hadoop.security.UserGroupInformation"
                ],
                "methods": [
                    "UserGroupInformation.getAuthenticationMethod"
                ]
            },
            "possible_fix": "To fix the issue, modify the Hadoop configuration files to set the property `hadoop.security.authentication` to 'simple'. This can be done by adding or updating the following line in `core-site.xml`:\n\n<property>\n    <name>hadoop.security.authentication</name>\n    <value>simple</value>\n</property>"
        },
        "possible_fix_code": {
            "UserGroupInformation.getAuthenticationMethod": "  public synchronized AuthenticationMethod getAuthenticationMethod() {\n    return user.getAuthenticationMethod();\n  }"
        }
    },
    {
        "filename": "YARN-6072.json",
        "creation_time": "2017-01-08T09:21:12.000+0000",
        "bug_report": {
            "Title": "RM unable to start in secure mode",
            "Description": "The Resource Manager (RM) fails to start in secure mode due to a NullPointerException encountered during the transition to the active state. The issue arises when the AdminService attempts to refresh service ACLs, which is a critical step in the startup process. The stack trace indicates that the refreshServiceAcls method is invoked without proper initialization of its parameters, leading to the failure of the refreshAll method and ultimately preventing the RM from becoming active.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.AdminService.refreshServiceAcls(AdminService.java:569)",
                "at org.apache.hadoop.yarn.server.resourcemanager.AdminService.refreshServiceAcls(AdminService.java:552)",
                "at org.apache.hadoop.yarn.server.resourcemanager.AdminService.refreshAll(AdminService.java:707)",
                "at org.apache.hadoop.yarn.server.resourcemanager.AdminService.transitionToActive(AdminService.java:302)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ActiveStandbyElectorBasedElectorService.becomeActive(ActiveStandbyElectorBasedElectorService.java:142)",
                "at org.apache.hadoop.ha.ActiveStandbyElector.becomeActive(ActiveStandbyElector.java:888)",
                "at org.apache.hadoop.ha.ActiveStandbyElector.processResult(ActiveStandbyElector.java:467)",
                "at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:599)",
                "at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498)"
            ],
            "RootCause": "The root cause of the issue is a NullPointerException in the refreshServiceAcls method of the AdminService class. This occurs because the method is called without the necessary Configuration or PolicyProvider being properly initialized, leading to a failure in the refreshAll method during the transition to active state.",
            "StepsToReproduce": [
                "Start the Resource Manager in secure mode.",
                "Ensure that the necessary configurations for security are set.",
                "Observe the logs for any NullPointerException during the startup process."
            ],
            "ExpectedBehavior": "The Resource Manager should start successfully in secure mode, transitioning to the active state without any exceptions.",
            "ObservedBehavior": "The Resource Manager fails to start in secure mode, throwing a NullPointerException during the refreshServiceAcls method call, which prevents the transition to active state.",
            "Suggestions": "Ensure that the Configuration and PolicyProvider are properly initialized before invoking the refreshServiceAcls method. Review the service initialization sequence to guarantee that all dependencies are set up correctly.",
            "problem_location": {
                "files": [
                    "AdminService.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.AdminService"
                ],
                "methods": [
                    "refreshServiceAcls",
                    "refreshAll",
                    "transitionToActive"
                ]
            },
            "possible_fix": "Modify the service initialization logic to ensure that the Configuration and PolicyProvider are not null before calling refreshServiceAcls. For example, add checks or initialize these parameters in the serviceInit method."
        },
        "possible_fix_code": {
            "refreshServiceAcls": "  private synchronized void refreshServiceAcls(Configuration configuration,\n      PolicyProvider policyProvider) {\n    if (configuration == null || policyProvider == null) {\n        throw new IllegalArgumentException(\"Configuration and PolicyProvider must not be null\");\n    }\n    this.server.refreshServiceAclWithLoadedConfiguration(configuration,\n        policyProvider);\n  }"
        }
    },
    {
        "filename": "YARN-7663.json",
        "creation_time": "2017-12-15T01:52:46.000+0000",
        "bug_report": {
            "Title": "RMAppImpl: Invalid event: START at KILLED",
            "Description": "The application encounters an `InvalidStateTransitionException` when attempting to process a `START` event while in the `KILLED` state. This indicates that the application is trying to transition to a state that is not valid according to the YARN state machine. The stack trace shows that the exception is thrown during the handling of application events in the `RMAppImpl` class, specifically when the state machine attempts to process the event. The issue can be reproduced deterministically by introducing a delay before the `START` event is created.",
            "StackTrace": [
                "org.apache.hadoop.yarn.state.InvalidStateTransitionException: Invalid event: START at KILLED",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.handle(RMAppImpl.java:805)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.handle(RMAppImpl.java:116)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher.handle(ResourceManager.java:901)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher.handle(ResourceManager.java:885)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:184)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:110)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "RootCause": "The root cause of the issue is that the application is attempting to transition to the `START` state from the `KILLED` state, which is not a valid transition according to the YARN state machine rules.",
            "StepsToReproduce": [
                "Send a kill command to the application.",
                "Immediately attempt to send a START event to the application.",
                "Observe the logs for the InvalidStateTransitionException."
            ],
            "ExpectedBehavior": "The application should not process a START event when it is in the KILLED state, and no exception should be thrown.",
            "ObservedBehavior": "An `InvalidStateTransitionException` is thrown, indicating that the START event is invalid for the KILLED state.",
            "Suggestions": "Review the state transition logic in the YARN framework to ensure that events are only processed when valid. Consider implementing checks to prevent sending invalid events from the KILLED state.",
            "problem_location": {
                "files": [
                    "RMAppImpl.java",
                    "ResourceManager.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl",
                    "org.apache.hadoop.yarn.server.resourcemanager.ResourceManager"
                ],
                "methods": [
                    "RMAppImpl.handle",
                    "ResourceManager.ApplicationEventDispatcher.handle"
                ]
            },
            "possible_fix": "Implement a check in the RMAppImpl.handle method to prevent processing of START events when the application is in the KILLED state."
        },
        "possible_fix_code": {
            "RMAppImpl.handle": "  public void handle(RMAppEvent event) {\n\n    this.writeLock.lock();\n\n    try {\n      ApplicationId appID = event.getApplicationId();\n      LOG.debug(\"Processing event for \" + appID + \" of type \"\n          + event.getType());\n      final RMAppState oldState = getState();\n\n      // Check if the current state is KILLED and the event is START\n      if (oldState == RMAppState.KILLED && event.getType() == RMAppEventType.START) {\n        LOG.error(\"App: \" + appID + \" cannot handle START event from KILLED state\");\n        return; // Prevent processing the START event\n      }\n\n      try {\n        /* keep the master in sync with the state machine */\n        this.stateMachine.doTransition(event.getType(), event);\n      } catch (InvalidStateTransitionException e) {\n        LOG.error(\"App: \" + appID\n            + \" cannot handle this event at current state\", e);\n        /* TODO fail the application on the failed transition */\n      }\n\n      // Log at INFO if we are not recovering or not in a terminal state.\n      // Log at DEBUG otherwise.\n      if ((oldState != getState()) &&\n          (((recoveredFinalState == null)) ||\n            (event.getType() != RMAppEventType.RECOVER))) {\n        LOG.info(String.format(STATE_CHANGE_MESSAGE, appID, oldState,\n            getState(), event.getType()));\n      } else if ((oldState != getState()) && LOG.isDebugEnabled()) {\n        LOG.debug(String.format(STATE_CHANGE_MESSAGE, appID, oldState,\n            getState(), event.getType()));\n      }\n    } finally {\n      this.writeLock.unlock();\n    }\n  }"
        }
    },
    {
        "filename": "YARN-5873.json",
        "creation_time": "2016-11-12T09:54:20.000+0000",
        "bug_report": {
            "Title": "RM crashes with NPE if generic application history is enabled",
            "Description": "The ResourceManager (RM) crashes due to a NullPointerException (NPE) when the generic application history feature is enabled. The stack trace indicates that the NPE occurs in the `hashCode` method of the `WritingContainerStartEvent` class, which is invoked during the handling of container start events. This issue arises when the `containerId` field in the `WritingContainerStartEvent` is not properly initialized, leading to attempts to access methods on a null reference.",
            "StackTrace": [
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.ahs.WritingContainerStartEvent.hashCode(WritingContainerStartEvent.java:38)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ahs.RMApplicationHistoryWriter$MultiThreadedDispatcher$CompositEventHandler.handle(RMApplicationHistoryWriter.java:354)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ahs.RMApplicationHistoryWriter.containerStarted(RMApplicationHistoryWriter.java:278)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl.<init>(RMContainerImpl.java:251)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl.<init>(RMContainerImpl.java:217)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl.<init>(RMContainerImpl.java:210)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp.allocate(FiCaSchedulerApp.java:227)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.RegularContainerAllocator.handleNewContainerAllocation(RegularContainerAllocator.java:704)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.RegularContainerAllocator.doAllocation(RegularContainerAllocator.java:746)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.RegularContainerAllocator.allocate(RegularContainerAllocator.java:832)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.allocator.RegularContainerAllocator.assignContainers(RegularContainerAllocator.java:865)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp.assignContainers(FiCaSchedulerApp.java:931)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainers(LeafQueue.java:1044)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.assignContainersToChildQueues(ParentQueue.java:690)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.assignContainers(ParentQueue.java:508)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.allocateOrReserveNewContainers(CapacityScheduler.java:1475)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.allocateContainerOnSingleNode(CapacityScheduler.java:1470)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.allocateContainersToNode(CapacityScheduler.java:1559)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.allocateContainersToNode(CapacityScheduler.java:1346)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.nodeUpdate(CapacityScheduler.java:1221)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:1601)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:149)",
                "at org.apache.hadoop.yarn.event.EventDispatcher$EventProcessor.run(EventDispatcher.java:66)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "RootCause": "The root cause of the NullPointerException is that the `containerId` in the `WritingContainerStartEvent` class is null. This occurs when the `RMContainer` is created without a valid `ContainerId` being set before the `containerStarted` method is called in the `RMApplicationHistoryWriter` class.",
            "StepsToReproduce": [
                "Enable the generic application history feature in the ResourceManager configuration.",
                "Start a new application that requires container allocation.",
                "Observe the ResourceManager logs for the NullPointerException."
            ],
            "ExpectedBehavior": "The ResourceManager should handle container start events without crashing, and the application history should be recorded correctly.",
            "ObservedBehavior": "The ResourceManager crashes with a NullPointerException when attempting to access the `containerId` in the `WritingContainerStartEvent` class.",
            "Suggestions": "Ensure that the `ContainerId` is properly initialized in the `RMContainer` before it is passed to the `WritingContainerStartEvent` constructor. Review the lifecycle of `RMContainer` instances to confirm that `setContainerId` is called appropriately.",
            "problem_location": {
                "files": [
                    "WritingContainerStartEvent.java",
                    "RMApplicationHistoryWriter.java",
                    "RMContainerImpl.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.ahs.WritingContainerStartEvent",
                    "org.apache.hadoop.yarn.server.resourcemanager.ahs.RMApplicationHistoryWriter",
                    "org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl"
                ],
                "methods": [
                    "WritingContainerStartEvent.hashCode",
                    "RMApplicationHistoryWriter.containerStarted",
                    "RMContainerImpl.RMContainerImpl"
                ]
            },
            "possible_fix": "In the `RMContainerImpl` constructor, ensure that the `setContainerId` method is called with a valid `ContainerId` before invoking the `containerStarted` method in the `RMApplicationHistoryWriter`."
        },
        "possible_fix_code": {
            "RMContainerImpl.RMContainerImpl": "  public RMContainerImpl(Container container,\n      ApplicationAttemptId appAttemptId, NodeId nodeId, String user,\n      RMContext rmContext, long creationTime, String nodeLabelExpression) {\n    this(container, appAttemptId, nodeId, user, rmContext, creationTime,\n        nodeLabelExpression, false);\n    // Ensure that the containerId is set before calling containerStarted\n    if (container.getId() == null) {\n        throw new IllegalArgumentException(\"ContainerId must be set before starting the container.\");\n    }\n    this.stateMachine = stateMachineFactory.make(this);\n    this.nodeId = nodeId;\n    this.container = container;\n    this.allocatedSchedulerKey = SchedulerRequestKey.extractFrom(container);\n    this.appAttemptId = appAttemptId;\n    this.user = user;\n    this.creationTime = creationTime;\n    this.rmContext = rmContext;\n    this.eventHandler = rmContext.getDispatcher().getEventHandler();\n    this.containerAllocationExpirer = rmContext.getContainerAllocationExpirer();\n    this.isAMContainer = false;\n    this.resourceRequests = null;\n    this.nodeLabelExpression = nodeLabelExpression;\n    this.lastConfirmedResource = container.getResource();\n    this.isExternallyAllocated = isExternallyAllocated;\n\n    ReentrantReadWriteLock lock = new ReentrantReadWriteLock();\n    this.readLock = lock.readLock();\n    this.writeLock = lock.writeLock();\n\n    saveNonAMContainerMetaInfo = rmContext.getYarnConfiguration().getBoolean(\n       YarnConfiguration.APPLICATION_HISTORY_SAVE_NON_AM_CONTAINER_META_INFO,\n       YarnConfiguration\n                 .DEFAULT_APPLICATION_HISTORY_SAVE_NON_AM_CONTAINER_META_INFO);\n\n    rmContext.getRMApplicationHistoryWriter().containerStarted(this);\n\n    // If saveNonAMContainerMetaInfo is true, store system metrics for all\n    // containers. If false, and if this container is marked as the AM, metrics\n    // will still be published for this container, but that calculation happens\n    // later.\n    if (saveNonAMContainerMetaInfo && null != container.getId()) {\n      rmContext.getSystemMetricsPublisher().containerCreated(\n          this, this.creationTime);\n    }\n  }"
        }
    },
    {
        "filename": "YARN-3227.json",
        "creation_time": "2015-02-19T16:58:01.000+0000",
        "bug_report": {
            "Title": "Timeline renew delegation token fails when RM user's TGT is expired",
            "Description": "The issue arises when the ResourceManager (RM) user's Kerberos Ticket Granting Ticket (TGT) expires, leading to a failure in renewing the delegation token during job submission. The expected behavior is for the RM to automatically re-login and obtain a new TGT to facilitate the renewal process. The stack trace indicates an HTTP 401 Unauthorized error, suggesting that the credentials used for the renewal are invalid or that the user lacks the necessary permissions.",
            "StackTrace": [
                "2015-02-06 18:54:05,617 [DelegationTokenRenewer #25954] WARN security.DelegationTokenRenewer: Unable to add the application to the delegation token renewer.",
                "java.io.IOException: Failed to renew token: Kind: TIMELINE_DELEGATION_TOKEN, Service: timelineserver.example.com:4080, Ident: (owner=user, renewer=rmuser, realUser=oozie, issueDate=1423248845528, maxDate=1423853645528, sequenceNumber=9716, masterKeyId=9)",
                "at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.handleAppSubmitEvent(DelegationTokenRenewer.java:443)",
                "at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.access$800(DelegationTokenRenewer.java:77)",
                "at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$DelegationTokenRenewerRunnable.handleDTRenewerAppSubmitEvent(DelegationTokenRenewer.java:808)",
                "at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$DelegationTokenRenewerRunnable.run(DelegationTokenRenewer.java:789)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:722)",
                "Caused by: java.io.IOException: HTTP status [401], message [Unauthorized]",
                "at org.apache.hadoop.util.HttpExceptionUtils.validateResponse(HttpExceptionUtils.java:169)",
                "at org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator.doDelegationTokenOperation(DelegationTokenAuthenticator.java:286)",
                "at org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator.renewDelegationToken(DelegationTokenAuthenticator.java:211)",
                "at org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL.renewDelegationToken(DelegationTokenAuthenticatedURL.java:414)",
                "at org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl$2.run(TimelineClientImpl.java:374)",
                "at org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl$2.run(TimelineClientImpl.java:360)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1694)",
                "at org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl$4.run(TimelineClientImpl.java:429)",
                "at org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl$TimelineClientConnectionRetry.retryOn(TimelineClientImpl.java:161)",
                "at org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl.operateDelegationToken(TimelineClientImpl.java:444)",
                "at org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl.renewDelegationToken(TimelineClientImpl.java:378)",
                "at org.apache.hadoop.yarn.security.client.TimelineDelegationTokenIdentifier$Renewer.renew(TimelineDelegationTokenIdentifier.java:81)",
                "at org.apache.hadoop.security.token.Token.renew(Token.java:377)",
                "at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$1.run(DelegationTokenRenewer.java:532)",
                "at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer$1.run(DelegationTokenRenewer.java:529)"
            ],
            "RootCause": "The root cause of the issue is that the RM user's TGT has expired, leading to an HTTP 401 Unauthorized error when attempting to renew the delegation token. This indicates that the credentials used for the renewal are invalid or that the user lacks the necessary permissions.",
            "StepsToReproduce": [
                "1. Ensure the RM user's TGT is expired.",
                "2. Attempt to submit a job that requires renewing a delegation token.",
                "3. Observe the failure in the renewal process and the resulting HTTP 401 Unauthorized error."
            ],
            "ExpectedBehavior": "The ResourceManager should automatically re-login to obtain a new TGT and successfully renew the delegation token.",
            "ObservedBehavior": "The renewal process fails with an HTTP 401 Unauthorized error, indicating that the credentials are invalid or the user lacks the necessary permissions.",
            "Suggestions": "Check the user's permissions and ensure that the RM user has the necessary rights to renew the delegation token. Additionally, verify the configuration settings related to security, such as 'hadoop.security.auth_to_local' and 'hadoop.security.authorization'.",
            "problem_location": {
                "files": [
                    "hadoop-common-project.hadoop-common/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/security/DelegationTokenRenewer.java",
                    "hadoop-common-project.hadoop-common/src/main/java/org/apache/hadoop/security/UserGroupInformation.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer",
                    "org.apache.hadoop.security.UserGroupInformation"
                ],
                "methods": [
                    "DelegationTokenRenewer.renew",
                    "UserGroupInformation.getCurrentUser"
                ]
            },
            "possible_fix": "Ensure that the RM user has valid credentials and permissions to renew the delegation token. If the TGT is expired, implement a mechanism to automatically re-login and obtain a new TGT before attempting to renew the token."
        },
        "possible_fix_code": {
            "DelegationTokenRenewer.renew": "    private boolean renew() throws IOException, InterruptedException {\n      final T fs = weakFs.get();\n      final boolean b = fs != null;\n      if (b) {\n        synchronized(fs) {\n          try {\n            long expires = token.renew(fs.getConf());\n            updateRenewalTime(expires - Time.now());\n          } catch (IOException ie) {\n            // Attempt to re-login if the TGT is expired\n            UserGroupInformation currentUser = UserGroupInformation.getCurrentUser();\n            if (currentUser != null) {\n              currentUser.relogin(); // Ensure the user re-logs in to obtain a new TGT\n            }\n            try {\n              Token<?>[] tokens = fs.addDelegationTokens(null, null);\n              if (tokens.length == 0) {\n                throw new IOException(\"addDelegationTokens returned no tokens\");\n              }\n              token = tokens[0];\n              updateRenewalTime(renewCycle);\n              fs.setDelegationToken(token);\n            } catch (IOException ie2) {\n              isValid = false;\n              throw new IOException(\"Can't renew or get new delegation token \", ie);\n            }\n          }\n        }\n      }\n      return b;\n    }"
        }
    },
    {
        "filename": "YARN-4235.json",
        "creation_time": "2015-10-07T19:26:24.000+0000",
        "bug_report": {
            "Title": "FairScheduler PrimaryGroup does not handle empty groups returned for a user",
            "Description": "The application encounters a NullPointerException (NPE) when empty groups are returned for a user, leading to a crash of the ResourceManager. The stack trace indicates that the issue arises from an IndexOutOfBoundsException when attempting to access an index in an empty list. This occurs in the `getQueueForApp` method of the `QueuePlacementRule` class, which is called during the application addition process in the FairScheduler.",
            "StackTrace": [
                "2015-09-22 16:51:52,780  FATAL org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Error in handling event type APP_ADDED to the scheduler",
                "java.lang.IndexOutOfBoundsException: Index: 0",
                "at java.util.Collections$EmptyList.get(Collections.java:3212)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.QueuePlacementRule$PrimaryGroup.getQueueForApp(QueuePlacementRule.java:149)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.QueuePlacementRule.assignAppToQueue(QueuePlacementRule.java:74)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.QueuePlacementPolicy.assignAppToQueue(QueuePlacementPolicy.java:167)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.assignToQueue(FairScheduler.java:689)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.addApplication(FairScheduler.java:595)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:1180)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:111)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:684)",
                "at java.lang.Thread.run(Thread.java:745)",
                "2015-09-22 16:51:52,797  INFO org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Exiting, bbye.."
            ],
            "RootCause": "The root cause of the IndexOutOfBoundsException is that the `assignAppToQueue` method in the `QueuePlacementRule` class returns null, which is not handled properly in the subsequent method calls. This leads to an attempt to access an index in an empty list in the `getQueueForApp` method.",
            "StepsToReproduce": [
                "1. Configure the FairScheduler with a user that has no associated groups.",
                "2. Submit an application to the ResourceManager.",
                "3. Observe the logs for the IndexOutOfBoundsException."
            ],
            "ExpectedBehavior": "The system should handle cases where a user has no associated groups gracefully, without throwing exceptions, and should either assign a default queue or reject the application with a clear error message.",
            "ObservedBehavior": "The system throws an IndexOutOfBoundsException, causing the ResourceManager to crash when it attempts to access an index in an empty list.",
            "Suggestions": "Implement proper handling in the `assignAppToQueue` method to return a valid queue or a default value when no groups are found for a user. Additionally, ensure that the `getQueueForApp` method checks for null or empty lists before attempting to access elements.",
            "problem_location": {
                "files": [
                    "QueuePlacementRule.java",
                    "FairScheduler.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.QueuePlacementRule",
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler"
                ],
                "methods": [
                    "QueuePlacementRule.getQueueForApp",
                    "QueuePlacementRule.assignAppToQueue",
                    "FairScheduler.assignToQueue"
                ]
            },
            "possible_fix": "Modify the `assignAppToQueue` method in `QueuePlacementRule` to handle cases where no groups are returned. For example, return a default queue or throw a custom exception that can be handled upstream. Update the `getQueueForApp` method to check for null or empty lists before accessing elements."
        },
        "possible_fix_code": {
            "QueuePlacementRule.assignAppToQueue": "    public String assignAppToQueue(String requestedQueue, String user,\n        Groups groups, Map<FSQueueType, Set<String>> configuredQueues) {\n      // Check if groups are empty or null\n      if (groups == null || groups.isEmpty()) {\n          // Return a default queue or handle the case appropriately\n          return \"defaultQueue\"; // Assuming 'defaultQueue' is a valid queue name\n      }\n      // Existing logic to determine the queue based on groups\n      // This part should be implemented based on the actual logic needed\n      // For now, we return null to indicate no queue assigned\n      return null;\n    }"
        }
    },
    {
        "filename": "YARN-4833.json",
        "creation_time": "2016-03-17T13:22:23.000+0000",
        "bug_report": {
            "Title": "For Queue AccessControlException client retries multiple times on both RM",
            "Description": "When submitting an application to a YARN queue with ACL enabled, the client encounters an AccessControlException if the submitting user lacks the necessary permissions. The client retries the submission up to a maximum of 10 attempts, leading to excessive retries and potential performance issues. The error log indicates that the user 'hdfs' does not have permission to submit the application to the default queue, which is a common issue related to access control in Hadoop environments.",
            "StackTrace": [
                "org.apache.hadoop.security.AccessControlException: User hdfs does not have permission to submit application_1458273884145_0001 to queue default",
                "at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.createAndPopulateNewRMApp(RMAppManager.java:380)",
                "at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.submitApplication(RMAppManager.java:291)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ClientRMService.submitApplication(ClientRMService.java:618)",
                "at org.apache.hadoop.yarn.api.impl.pb.service.ApplicationClientProtocolPBServiceImpl.submitApplication(ApplicationClientProtocolPBServiceImpl.java:252)",
                "at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.submitApplication(YarnClientImpl.java:261)",
                "at org.apache.hadoop.mapreduce.Job.submit(Job.java:1338)",
                "at org.apache.hadoop.examples.QuasiMonteCarlo.estimatePi(QuasiMonteCarlo.java:306)"
            ],
            "RootCause": "The root cause of the issue is that the user 'hdfs' does not have the required permissions to submit applications to the default YARN queue, resulting in an AccessControlException being thrown during the submission process.",
            "StepsToReproduce": [
                "1. Configure a YARN queue with ACLs enabled.",
                "2. Attempt to submit an application using the user 'hdfs'.",
                "3. Observe the retries and the AccessControlException in the logs."
            ],
            "ExpectedBehavior": "The application should be submitted successfully if the user has the appropriate permissions for the YARN queue.",
            "ObservedBehavior": "The application submission fails with an AccessControlException, and the client retries the submission multiple times until the maximum attempt limit is reached.",
            "Suggestions": "To resolve this issue, ensure that the user 'hdfs' has the necessary permissions in the YARN queue configuration files (e.g., capacity-scheduler.xml or fair-scheduler.xml). Additionally, consider implementing a fallback policy in the RetryPolicy to handle AccessControlExceptions more gracefully.",
            "problem_location": {
                "files": [
                    "capacity-scheduler.xml",
                    "fair-scheduler.xml",
                    "RMAppManager.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.RMAppManager"
                ],
                "methods": [
                    "RMAppManager.submitApplication"
                ]
            },
            "possible_fix": "Modify the YARN queue configuration to include the user 'hdfs' in the ACLs for the default queue. Additionally, wrap the AccessControlException in a YarnException in the RMAppManager#submitApplication method to handle it appropriately."
        },
        "possible_fix_code": {
            "org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.submitApplication": "  protected void submitApplication(\n      ApplicationSubmissionContext submissionContext, long submitTime,\n      String user) throws YarnException, AccessControlException {\n    ApplicationId applicationId = submissionContext.getApplicationId();\n\n    // Check for user permissions before proceeding\n    if (!hasPermission(user, submissionContext.getQueue())) {\n        throw new YarnException(\"User \" + user + \" does not have permission to submit to queue \" + submissionContext.getQueue());\n    }\n\n    RMAppImpl application =\n        createAndPopulateNewRMApp(submissionContext, submitTime, user, false);\n    Credentials credentials = null;\n    try {\n      credentials = parseCredentials(submissionContext);\n      if (UserGroupInformation.isSecurityEnabled()) {\n        this.rmContext.getDelegationTokenRenewer()\n            .addApplicationAsync(applicationId, credentials,\n                submissionContext.getCancelTokensWhenComplete(),\n                application.getUser());\n      } else {\n        // Dispatcher is not yet started at this time, so these START events\n        // enqueued should be guaranteed to be first processed when dispatcher\n        // gets started.\n        this.rmContext.getDispatcher().getEventHandler()\n            .handle(new RMAppEvent(applicationId, RMAppEventType.START));\n      }\n    } catch (Exception e) {\n      LOG.warn(\"Unable to parse credentials.\", e);\n      // Sending APP_REJECTED is fine, since we assume that the\n      // RMApp is in NEW state and thus we haven't yet informed the\n      // scheduler about the existence of the application\n      assert application.getState() == RMAppState.NEW;\n      this.rmContext.getDispatcher().getEventHandler()\n          .handle(new RMAppEvent(applicationId,\n              RMAppEventType.APP_REJECTED, e.getMessage()));\n      throw RPCUtil.getRemoteException(e);\n    }\n  }\n\n  private boolean hasPermission(String user, String queue) {\n      // Implement the logic to check if the user has permission to access the specified queue\n      // This is a placeholder for the actual permission checking logic\n      return true; // Replace with actual permission check\n  }"
        }
    },
    {
        "filename": "YARN-1689.json",
        "creation_time": "2014-02-05T19:16:00.000+0000",
        "bug_report": {
            "Title": "RMAppAttempt is not killed when RMApp is at ACCEPTED",
            "Description": "The ResourceManager (RM) enters an unusable state when running Hive on Tez jobs, leading to no jobs being executed. The logs indicate a `NullPointerException` in the `getTransferredContainers` method of the `AbstractYarnScheduler` class, which suggests that the application may not be properly registered or has been killed. Additionally, an `InvalidStateTransitionException` occurs in the `RMAppImpl` class when an `ATTEMPT_REGISTERED` event is received while the application is in the `KILLED` state, indicating a failure in handling state transitions correctly.",
            "StackTrace": [
                "2014-02-04 20:28:08,553 WARN  ipc.Server (Server.java:run(1978)) - IPC Server handler 0 on 8030, call org.apache.hadoop.yarn.api.ApplicationMasterProtocolPB.registerApplicationMaster from 172.18.145.156:40474 Call#0 Retry#0: error: java.lang.NullPointerException",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler.getTransferredContainers(AbstractYarnScheduler.java:48)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService.registerApplicationMaster(ApplicationMasterService.java:278)",
                "at org.apache.hadoop.yarn.api.impl.pb.service.ApplicationMasterProtocolPBServiceImpl.registerApplicationMaster(ApplicationMasterProtocolPBServiceImpl.java:90)",
                "at org.apache.hadoop.yarn.proto.ApplicationMasterProtocol$ApplicationMasterProtocolService$2.callBlockingMethod(ApplicationMasterProtocol.java:95)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)",
                "at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1962)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1958)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:396)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)",
                "at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1956)",
                "2014-02-04 20:28:08,544 ERROR rmapp.RMAppImpl (RMAppImpl.java:handle(626)) - Can't handle this event at current state",
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: ATTEMPT_REGISTERED at KILLED",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.handle(RMAppImpl.java:624)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.handle(RMAppImpl.java:81)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher.handle(ResourceManager.java:656)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationEventDispatcher.handle(ResourceManager.java:640)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:173)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:106)",
                "at java.lang.Thread.run(Thread.java:662)"
            ],
            "RootCause": "The root cause of the issue is twofold: a `NullPointerException` in the `getTransferredContainers` method due to the application not being found in the scheduler's application map, and an `InvalidStateTransitionException` in the `RMAppImpl` class when an `ATTEMPT_REGISTERED` event is processed while the application is in the `KILLED` state.",
            "StepsToReproduce": [
                "Submit a Hive on Tez job to the YARN ResourceManager.",
                "Monitor the ResourceManager logs for exceptions.",
                "Observe the state of the application and the handling of events."
            ],
            "ExpectedBehavior": "The ResourceManager should correctly handle application state transitions and register application attempts without throwing exceptions.",
            "ObservedBehavior": "The ResourceManager throws a `NullPointerException` and an `InvalidStateTransitionException`, leading to an unusable state where no jobs can run.",
            "Suggestions": "Investigate the application registration process to ensure that applications are properly registered before attempting to retrieve their containers. Additionally, review the state transition logic in the `RMAppImpl` class to prevent invalid transitions.",
            "problem_location": {
                "files": [
                    "AbstractYarnScheduler.java",
                    "RMAppImpl.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler",
                    "org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl"
                ],
                "methods": [
                    "AbstractYarnScheduler.getTransferredContainers",
                    "RMAppImpl.handle"
                ]
            },
            "possible_fix": "In the `getTransferredContainers` method, add a check to ensure that the application is present in the scheduler's application map before attempting to retrieve its containers. In the `handle` method of `RMAppImpl`, ensure that state transitions are validated against the current state to prevent invalid transitions."
        },
        "possible_fix_code": {
            "AbstractYarnScheduler.getTransferredContainers": "  public synchronized List<Container> getTransferredContainers(\n      ApplicationAttemptId currentAttempt) {\n    ApplicationId appId = currentAttempt.getApplicationId();\n    SchedulerApplication app = applications.get(appId);\n    List<Container> containerList = new ArrayList<Container>();\n    RMApp appImpl = this.rmContext.getRMApps().get(appId);\n    if (appImpl == null || appImpl.getApplicationSubmissionContext().getUnmanagedAM()) {\n      return containerList;\n    }\n    Collection<RMContainer> liveContainers =\n        app.getCurrentAppAttempt().getLiveContainers();\n    ContainerId amContainerId =\n        appImpl.getCurrentAppAttempt().getMasterContainer().getId();\n    for (RMContainer rmContainer : liveContainers) {\n      if (!rmContainer.getContainerId().equals(amContainerId)) {\n        containerList.add(rmContainer.getContainer());\n      }\n    }\n    return containerList;\n  }",
            "RMAppImpl.handle": "  public void handle(RMAppEvent event) {\n\n    this.writeLock.lock();\n\n    try {\n      ApplicationId appID = event.getApplicationId();\n      LOG.debug(\"Processing event for \" + appID + \" of type \"\n          + event.getType());\n      final RMAppState oldState = getState();\n      if (oldState == RMAppState.KILLED && event.getType() == RMAppEventType.ATTEMPT_REGISTERED) {\n        LOG.error(\"Cannot register attempt for application in KILLED state\");\n        return;\n      }\n      try {\n        /* keep the master in sync with the state machine */\n        this.stateMachine.doTransition(event.getType(), event);\n      } catch (InvalidStateTransitonException e) {\n        LOG.error(\"Can't handle this event at current state\", e);\n        /* TODO fail the application on the failed transition */\n      }\n\n      if (oldState != getState()) {\n        LOG.info(appID + \" State change from \" + oldState + \" to \"\n            + getState());\n      }\n    } finally {\n      this.writeLock.unlock();\n    }\n  }"
        }
    },
    {
        "filename": "YARN-5594.json",
        "creation_time": "2016-08-30T15:14:19.000+0000",
        "bug_report": {
            "Title": "Handle old RMDelegationToken format when recovering RM",
            "Description": "After upgrading the cluster from version 2.5.1 to 2.7.0, an error occurs during the recovery of the ResourceManager state. The error log indicates a failure to load the state due to an `InvalidProtocolBufferException`, which suggests that the Protocol Buffer message contains an invalid tag (zero). This issue is likely caused by the use of different formats for the RMDelegationToken files across the two Hadoop versions, leading to serialization/deserialization problems.",
            "StackTrace": [
                "2016-08-25 17:20:33,293 ERROR org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Failed to load/recover state",
                "com.google.protobuf.InvalidProtocolBufferException: Protocol message contained an invalid tag (zero).",
                "at com.google.protobuf.InvalidProtocolBufferException.invalidTag(InvalidProtocolBufferException.java:89)",
                "at com.google.protobuf.CodedInputStream.readTag(CodedInputStream.java:108)",
                "at org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$RMDelegationTokenIdentifierDataProto.<init>(YarnServerResourceManagerRecoveryProtos.java:4680)",
                "at org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$RMDelegationTokenIdentifierDataProto.<init>(YarnServerResourceManagerRecoveryProtos.java:4644)",
                "at org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$RMDelegationTokenIdentifierDataProto$1.parsePartialFrom(YarnServerResourceManagerRecoveryProtos.java:4740)",
                "at org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$RMDelegationTokenIdentifierDataProto$1.parsePartialFrom(YarnServerResourceManagerRecoveryProtos.java:4735)",
                "at org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$RMDelegationTokenIdentifierDataProto$Builder.mergeFrom(YarnServerResourceManagerRecoveryProtos.java:5075)",
                "at org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos$RMDelegationTokenIdentifierDataProto$Builder.mergeFrom(YarnServerResourceManagerRecoveryProtos.java:4955)",
                "at com.google.protobuf.AbstractMessage$Builder.mergeFrom(AbstractMessage.java:337)",
                "at com.google.protobuf.AbstractMessage$Builder.mergeFrom(AbstractMessage.java:267)",
                "at com.google.protobuf.AbstractMessageLite$Builder.mergeFrom(AbstractMessageLite.java:210)",
                "at com.google.protobuf.AbstractMessage$Builder.mergeFrom(AbstractMessage.java:904)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.records.RMDelegationTokenIdentifierData.readFields(RMDelegationTokenIdentifierData.java:43)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore.loadRMDTSecretManagerState(FileSystemRMStateStore.java:355)",
                "at org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore.loadState(FileSystemRMStateStore.java:199)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:587)",
                "at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager.java:1007)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1048)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1044)"
            ],
            "RootCause": "The root cause of the issue is a mismatch in the expected format of the RMDelegationToken files between Hadoop versions 2.5.1 and 2.7.0, leading to an `InvalidProtocolBufferException` during the deserialization process.",
            "StepsToReproduce": [
                "Upgrade the Hadoop cluster from version 2.5.1 to 2.7.0.",
                "Attempt to recover the ResourceManager state using the existing RMDelegationToken files located at /var/mapr/cluster/yarn/rm/system/FSRMStateRoot/RMDTSecretManagerRoot/RMDelegationToken*."
            ],
            "ExpectedBehavior": "The ResourceManager should successfully recover its state without errors, loading the RMDelegationToken files correctly.",
            "ObservedBehavior": "The ResourceManager fails to recover its state, throwing an `InvalidProtocolBufferException` due to an invalid tag in the Protocol Buffer message.",
            "Suggestions": "To resolve this issue, implement a fix that handles the old data format during the ResourceManager recovery process, specifically when an `InvalidProtocolBufferException` occurs. This may involve adding checks for the data format and applying necessary transformations before deserialization.",
            "problem_location": {
                "files": [
                    "org/apache/hadoop/yarn/server/resourcemanager/recovery/FileSystemRMStateStore.java",
                    "org/apache/hadoop/yarn/proto/YarnServerResourceManagerRecoveryProtos.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore",
                    "org.apache.hadoop.yarn.proto.YarnServerResourceManagerRecoveryProtos"
                ],
                "methods": [
                    "FileSystemRMStateStore.loadState",
                    "YarnServerResourceManagerRecoveryProtos.RMDelegationTokenIdentifierDataProto.init"
                ]
            },
            "possible_fix": "Implement a check in the `FileSystemRMStateStore.loadState` method to handle different formats of RMDelegationToken files. If an `InvalidProtocolBufferException` is caught, log the error and attempt to read the data using a fallback method that accommodates the old format."
        },
        "possible_fix_code": {
            "FileSystemRMStateStore.loadState": "  public synchronized RMState loadState() throws Exception {\n    RMState rmState = new RMState();\n    // recover DelegationTokenSecretManager\n    try {\n        loadRMDTSecretManagerState(rmState);\n    } catch (InvalidProtocolBufferException e) {\n        LOG.error(\"Failed to load RMDelegationTokenSecretManager state due to invalid protocol buffer format. Attempting to recover using fallback method.\", e);\n        // Implement fallback logic to handle old format\n        recoverOldFormatRMDTSecretManagerState(rmState);\n    }\n    // recover RM applications\n    loadRMAppState(rmState);\n    // recover AMRMTokenSecretManager\n    loadAMRMTokenSecretManagerState(rmState);\n    // recover reservation state\n    loadReservationSystemState(rmState);\n    return rmState;\n  }\n\n  private void recoverOldFormatRMDTSecretManagerState(RMState rmState) throws Exception {\n    // Logic to handle the old RMDelegationToken format\n    // This may involve reading the data in a different way or transforming it\n    // before attempting to deserialize it into the new format.\n    // Placeholder for actual implementation.\n  }"
        }
    },
    {
        "filename": "YARN-7511.json",
        "creation_time": "2017-11-16T11:41:43.000+0000",
        "bug_report": {
            "Title": "NPE in ContainerLocalizer when localization failed for running container",
            "Description": "A NullPointerException (NPE) occurs in the ContainerLocalizer when resource localization fails for a running container. The stack trace indicates that the error arises from the `resourceLocalizationFailed` method in the `ResourceSet` class, specifically when attempting to remove a `LocalResourceRequest` from the `pendingResources` map. This issue is triggered when a `null` request is passed to the method, leading to the NPE during the transition of the container state.",
            "StackTrace": [
                "2017-09-30 20:14:32,839 FATAL [AsyncDispatcher event handler] org.apache.hadoop.yarn.event.AsyncDispatcher: Error in dispatcher thread",
                "java.lang.NullPointerException",
                "at java.util.concurrent.ConcurrentHashMap.replaceNode(ConcurrentHashMap.java:1106)",
                "at java.util.concurrent.ConcurrentHashMap.remove(ConcurrentHashMap.java:1097)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceSet.resourceLocalizationFailed(ResourceSet.java:151)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl$ResourceLocalizationFailedWhileRunningTransition.transition(ContainerImpl.java:821)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl$ResourceLocalizationFailedWhileRunningTransition.transition(ContainerImpl.java:813)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:362)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl.handle(ContainerImpl.java:1335)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl.handle(ContainerImpl.java:95)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ContainerEventDispatcher.handle(ContainerManagerImpl.java:1372)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ContainerEventDispatcher.handle(ContainerManagerImpl.java:1365)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:184)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:110)",
                "at java.lang.Thread.run(Thread.java:834)"
            ],
            "RootCause": "The root cause of the NullPointerException is that a null `LocalResourceRequest` is being passed to the `resourceLocalizationFailed` method in the `ResourceSet` class. This occurs during the transition of the container state when resource localization fails, and the request is not properly initialized.",
            "StepsToReproduce": [
                "1. Start a container and call ContainerManagerImpl#localize for the container.",
                "2. Simulate a failure in the ResourceLocalizationService$LocalizerRunner#run method, which sends out a ContainerResourceFailedEvent with a null LocalResourceRequest.",
                "3. Observe the NPE when the ResourceLocalizationFailedWhileRunningTransition#transition method is invoked, leading to a call to resourceSet.resourceLocalizationFailed(null)."
            ],
            "ExpectedBehavior": "The system should handle resource localization failures gracefully without throwing a NullPointerException. The request parameter should be validated before being passed to the `resourceLocalizationFailed` method.",
            "ObservedBehavior": "A NullPointerException is thrown when the `resourceLocalizationFailed` method is called with a null request, causing the container to fail unexpectedly.",
            "Suggestions": "Implement a null check for the `request` parameter in the `resourceLocalizationFailed` method to prevent the NPE. Ensure that the event triggering this method does not pass a null request.",
            "problem_location": {
                "files": [
                    "ResourceSet.java",
                    "ContainerImpl.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceSet",
                    "org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl"
                ],
                "methods": [
                    "ResourceSet.resourceLocalizationFailed",
                    "ContainerImpl$ResourceLocalizationFailedWhileRunningTransition.transition"
                ]
            },
            "possible_fix": "In the `ResourceSet` class, modify the `resourceLocalizationFailed` method to include a null check for the `request` parameter:\n\npublic void resourceLocalizationFailed(LocalResourceRequest request) {\n    if (request != null) {\n        pendingResources.remove(request);\n        resourcesFailedToBeLocalized.add(request);\n    } else {\n        // Log a warning or handle the null case appropriately\n    }\n}"
        },
        "possible_fix_code": {
            "ResourceSet.resourceLocalizationFailed": "public void resourceLocalizationFailed(LocalResourceRequest request) {\n    if (request != null) {\n        pendingResources.remove(request);\n        resourcesFailedToBeLocalized.add(request);\n    } else {\n        LOG.warn(\"Received null LocalResourceRequest in resourceLocalizationFailed method.\");\n    }\n}"
        }
    },
    {
        "filename": "YARN-6068.json",
        "creation_time": "2017-01-07T03:16:07.000+0000",
        "bug_report": {
            "Title": "Log aggregation get failed when NM restart even with recovery",
            "Description": "The log aggregation process fails to complete when the NodeManager (NM) is restarted, even with recovery mechanisms in place. The issue is indicated by an `InvalidStateTransitionException` that occurs when the application attempts to handle the `APPLICATION_LOG_HANDLING_FAILED` event while in the `RUNNING` state. This suggests that the application is not allowed to process this event in its current state, leading to the failure of log aggregation.",
            "StackTrace": [
                "2017-01-05 19:16:36,352 INFO  logaggregation.AppLogAggregatorImpl (AppLogAggregatorImpl.java:abortLogAggregation(527)) - Aborting log aggregation for application_1483640789847_0001",
                "2017-01-05 19:16:36,352 WARN  logaggregation.AppLogAggregatorImpl (AppLogAggregatorImpl.java:run(399)) - Aggregation did not complete for application application_1483640789847_0001",
                "2017-01-05 19:16:36,353 WARN  application.ApplicationImpl (ApplicationImpl.java:handle(461)) - Can't handle this event at current state",
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: APPLICATION_LOG_HANDLING_FAILED at RUNNING",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.handle(ApplicationImpl.java:459)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.handle(ApplicationImpl.java:64)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher.handle(ContainerManagerImpl.java:1084)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher.handle(ContainerManagerImpl.java:1076)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:184)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:110)",
                "at java.lang.Thread.run(Thread.java:745)",
                "2017-01-05 19:16:36,355 INFO  application.ApplicationImpl (ApplicationImpl.java:handle(464)) - Application application_1483640789847_0001 transitioned from RUNNING to null"
            ],
            "RootCause": "The root cause of the issue is that the `APPLICATION_LOG_HANDLING_FAILED` event is not a valid transition for the application when it is in the `RUNNING` state. This is confirmed by the state transition definitions in the `ApplicationImpl` class, where this event is only valid in the `FINISHING_CONTAINERS_WAIT` and `APPLICATION_RESOURCES_CLEANINGUP` states.",
            "StepsToReproduce": [
                "Start an application in YARN.",
                "Allow the application to reach the RUNNING state.",
                "Restart the NodeManager while the application is running.",
                "Observe the logs for the `APPLICATION_LOG_HANDLING_FAILED` event."
            ],
            "ExpectedBehavior": "The log aggregation should complete successfully, and the application should be able to handle log aggregation events without throwing exceptions.",
            "ObservedBehavior": "The log aggregation fails, and an `InvalidStateTransitionException` is thrown, indicating that the application cannot handle the `APPLICATION_LOG_HANDLING_FAILED` event while in the `RUNNING` state.",
            "Suggestions": "Review the state transition definitions in the `ApplicationImpl` class to ensure that the `APPLICATION_LOG_HANDLING_FAILED` event is handled appropriately in the `RUNNING` state, or modify the application logic to prevent this event from being triggered in that state.",
            "problem_location": {
                "files": [
                    "ApplicationImpl.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl"
                ],
                "methods": [
                    "ApplicationImpl.handle",
                    "StateMachineFactory.doTransition"
                ]
            },
            "possible_fix": "Consider adding a transition for the `APPLICATION_LOG_HANDLING_FAILED` event in the `RUNNING` state or modifying the event handling logic to prevent this event from being processed in the `RUNNING` state."
        },
        "possible_fix_code": {
            "ApplicationImpl.handle": "  @Override\n  public void handle(ApplicationEvent event) {\n\n    this.writeLock.lock();\n\n    try {\n      ApplicationId applicationID = event.getApplicationID();\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\n            \"Processing \" + applicationID + \" of type \" + event.getType());\n      }\n      ApplicationState oldState = stateMachine.getCurrentState();\n      ApplicationState newState = null;\n      try {\n        // Check if the event is APPLICATION_LOG_HANDLING_FAILED\n        if (event.getType() == ApplicationEventType.APPLICATION_LOG_HANDLING_FAILED) {\n          // Allow handling of this event in RUNNING state\n          if (oldState == ApplicationState.RUNNING) {\n            LOG.warn(\"Handling APPLICATION_LOG_HANDLING_FAILED in RUNNING state\");\n            // Handle the log handling failure appropriately\n            // You can add specific logic here if needed\n            return;\n          }\n        }\n        // queue event requesting init of the same app\n        newState = stateMachine.doTransition(event.getType(), event);\n      } catch (InvalidStateTransitionException e) {\n        LOG.warn(\"Can't handle this event at current state\", e);\n      }\n      if (newState != null && oldState != newState) {\n        LOG.info(\"Application \" + applicationID + \" transitioned from \"\n            + oldState + \" to \" + newState);\n      }\n    } finally {\n      this.writeLock.unlock();\n    }\n  }"
        }
    },
    {
        "filename": "YARN-903.json",
        "creation_time": "2013-07-07T08:35:30.000+0000",
        "bug_report": {
            "Title": "DistributedShell throwing Errors in logs after successful completion",
            "Description": "The DistributedShell application runs successfully but logs errors related to container management in the NodeManager and ApplicationMaster. The logs indicate that certain containers are not being recognized by the NodeManager, leading to exceptions when attempting to stop or query their status.",
            "StackTrace": [
                "org.apache.hadoop.yarn.exceptions.YarnException: Container container_1373184544832_0001_01_000002 is not handled by this NodeManager",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.authorizeGetAndStopContainerRequest(ContainerManagerImpl.java:614)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.stopContainer(ContainerManagerImpl.java:538)",
                "at org.apache.hadoop.yarn.api.impl.pb.service.ContainerManagementProtocolPBServiceImpl.stopContainer(ContainerManagementProtocolPBServiceImpl.java:88)"
            ],
            "RootCause": "The root cause of the issue appears to be that the NodeManager is unable to handle certain containers due to misconfiguration or state issues. The logs indicate that containers are not recognized by the NodeManager, leading to authorization failures.",
            "StepsToReproduce": [
                "Run the DistributedShell application using the ApplicationMaster.",
                "Monitor the logs from both the NodeManager and ApplicationMaster during execution.",
                "Observe the error messages related to container management."
            ],
            "ExpectedBehavior": "The application should run without logging errors related to container management, and all containers should be properly handled by the NodeManager.",
            "ObservedBehavior": "The application logs multiple errors indicating that certain containers are not handled by the NodeManager, resulting in authorization failures and exit statuses of -1000.",
            "Suggestions": "Check the configuration of the NodeManager and ensure that the containers are being started correctly. Verify that the application attempts match the containers being managed.",
            "problem_location": {
                "files": [
                    "ContainerManagerImpl.java",
                    "NodeManager.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl",
                    "org.apache.hadoop.yarn.server.nodemanager.NodeManager"
                ],
                "methods": [
                    "ContainerManagerImpl.authorizeGetAndStopContainerRequest",
                    "ContainerManagerImpl.stopContainer"
                ]
            },
            "possible_fix": "Review and update the configuration settings for the NodeManager to ensure it can manage the containers correctly. This may involve adjusting resource allocations or ensuring proper registration with the ResourceManager."
        },
        "possible_fix_code": {
            "ContainerManagerImpl.authorizeGetAndStopContainerRequest": "  protected void authorizeGetAndStopContainerRequest(ContainerId containerId,\n      Container container, boolean stopRequest) throws YarnException {\n\n    UserGroupInformation remoteUgi = getRemoteUgi();\n    NMTokenIdentifier identifier = selectNMTokenIdentifier(remoteUgi);\n\n    /*\n     * For get/stop container status; we need to verify that 1) User (NMToken)\n     * application attempt only has started container. 2) Requested containerId\n     * belongs to the same application attempt (NMToken) which was used. (Note:-\n     * This will prevent user in knowing another application's containers).\n     */\n\n    if ((!identifier.getApplicationAttemptId().equals(\n      containerId.getApplicationAttemptId()))\n        || (container != null && !identifier.getApplicationAttemptId().equals(\n          container.getContainerId().getApplicationAttemptId()))) {\n      if (stopRequest) {\n        LOG.warn(identifier.getApplicationAttemptId()\n            + \" attempted to stop non-application container : \"\n            + container.getContainerId().toString());\n        NMAuditLogger.logFailure(\"UnknownUser\", AuditConstants.STOP_CONTAINER,\n          \"ContainerManagerImpl\", \"Trying to stop unknown container!\",\n          identifier.getApplicationAttemptId().getApplicationId(),\n          container.getContainerId());\n      } else {\n        LOG.warn(identifier.getApplicationAttemptId()\n            + \" attempted to get get status for non-application container : \"\n            + container.getContainerId().toString());\n      }\n      throw RPCUtil.getRemoteException(\"Container \" + containerId.toString()\n          + \" is not started by this application attempt.\");\n    }\n\n    if (container == null) {\n      LOG.error(\"Container \" + containerId.toString() + \" is not recognized by this NodeManager.\");\n      throw RPCUtil.getRemoteException(\"Container \" + containerId.toString()\n          + \" is not handled by this NodeManager\");\n    }\n  }"
        }
    },
    {
        "filename": "YARN-8236.json",
        "creation_time": "2018-04-29T16:28:11.000+0000",
        "bug_report": {
            "Title": "Invalid kerberos principal file name causes NPE in native service",
            "Description": "The application encounters a NullPointerException (NPE) when attempting to add a keytab resource in a secure environment. The issue arises in the `addKeytabResourceIfSecure` method of the `ServiceClient` class, which is called during the application submission process. The stack trace indicates that the NPE occurs when the method tries to access the Kerberos principal name or keytab from a potentially null `Service` object.",
            "StackTrace": [
                "2018-04-29 16:22:54,266 WARN webapp.GenericExceptionHandler (GenericExceptionHandler.java:toResponse(98)) - INTERNAL_SERVER_ERROR",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.service.client.ServiceClient.addKeytabResourceIfSecure(ServiceClient.java:994)",
                "at org.apache.hadoop.yarn.service.client.ServiceClient.submitApp(ServiceClient.java:685)",
                "at org.apache.hadoop.yarn.service.client.ServiceClient.actionCreate(ServiceClient.java:269)"
            ],
            "RootCause": "The root cause of the NullPointerException is likely due to the `service` object being null when passed to the `addKeytabResourceIfSecure` method. This can occur if the `Service` object is not properly initialized or loaded from a JSON file, leading to null values for the Kerberos principal and keytab.",
            "StepsToReproduce": [
                "1. Attempt to submit an application with a Service object that has not been properly initialized.",
                "2. Ensure that the application is configured to use Kerberos security.",
                "3. Observe the logs for a NullPointerException in the `addKeytabResourceIfSecure` method."
            ],
            "ExpectedBehavior": "The application should successfully add the keytab resource without throwing a NullPointerException, provided that the Service object is correctly initialized with a valid Kerberos principal and keytab.",
            "ObservedBehavior": "The application throws a NullPointerException when trying to access the Kerberos principal name or keytab in the `addKeytabResourceIfSecure` method, indicating that the `service` object is null or improperly initialized.",
            "Suggestions": "Ensure that the `Service` object is properly initialized before it is passed to the `submitApp` method. Validate the existence and format of the JSON file used to load the Service object. Implement null checks in the `validateAndResolveService` method to prevent null service objects from being processed.",
            "problem_location": {
                "files": [
                    "ServiceClient.java",
                    "ServiceApiUtil.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.service.client.ServiceClient",
                    "org.apache.hadoop.yarn.service.utils.ServiceApiUtil"
                ],
                "methods": [
                    "ServiceClient.addKeytabResourceIfSecure",
                    "ServiceClient.submitApp",
                    "ServiceClient.actionCreate",
                    "ServiceApiUtil.validateAndResolveService"
                ]
            },
            "possible_fix": "Add null checks for the `service` object in the `validateAndResolveService` method to ensure it is not null before proceeding with further validations. Additionally, ensure that the `loadService` method correctly handles cases where the JSON file is missing or improperly formatted."
        },
        "possible_fix_code": {
            "ServiceApiUtil.validateAndResolveService": "  public static void validateAndResolveService(Service service,\n      SliderFileSystem fs, org.apache.hadoop.conf.Configuration conf) throws\n      IOException {\n    boolean dnsEnabled = conf.getBoolean(RegistryConstants.KEY_DNS_ENABLED,\n        RegistryConstants.DEFAULT_DNS_ENABLED);\n    if (dnsEnabled) {\n      if (RegistryUtils.currentUser().length()\n          > RegistryConstants.MAX_FQDN_LABEL_LENGTH) {\n        throw new IllegalArgumentException(\n            RestApiErrorMessages.ERROR_USER_NAME_INVALID);\n      }\n      userNamePattern.validate(RegistryUtils.currentUser());\n    }\n\n    if (service == null) {\n      throw new IllegalArgumentException(\"Service object cannot be null\");\n    }\n\n    if (StringUtils.isEmpty(service.getName())) {\n      throw new IllegalArgumentException(\n          RestApiErrorMessages.ERROR_APPLICATION_NAME_INVALID);\n    }\n\n    if (StringUtils.isEmpty(service.getVersion())) {\n      throw new IllegalArgumentException(String.format(\n          RestApiErrorMessages.ERROR_APPLICATION_VERSION_INVALID,\n          service.getName()));\n    }\n\n    validateNameFormat(service.getName(), conf);\n\n    // If the service has no components, throw error\n    if (!hasComponent(service)) {\n      throw new IllegalArgumentException(\n          \"No component specified for \" + service.getName());\n    }\n\n    if (UserGroupInformation.isSecurityEnabled()) {\n      if (!StringUtils.isEmpty(service.getKerberosPrincipal().getKeytab())) {\n        try {\n          // validate URI format\n          new URI(service.getKerberosPrincipal().getKeytab());\n        } catch (URISyntaxException e) {\n          throw new IllegalArgumentException(e);\n        }\n      }\n    }\n\n    // Validate the Docker client config.\n    try {\n      validateDockerClientConfiguration(service, conf);\n    } catch (IOException e) {\n      throw new IllegalArgumentException(e);\n    }\n\n    // Validate there are no component name collisions (collisions are not\n    // currently supported) and add any components from external services\n    Configuration globalConf = service.getConfiguration();\n    Set<String> componentNames = new HashSet<>();\n    List<Component> componentsToRemove = new ArrayList<>();\n    List<Component> componentsToAdd = new ArrayList<>();\n    for (Component comp : service.getComponents()) {\n      int maxCompLength = RegistryConstants.MAX_FQDN_LABEL_LENGTH;\n      maxCompLength = maxCompLength - Long.toString(Long.MAX_VALUE).length();\n      if (dnsEnabled && comp.getName().length() > maxCompLength) {\n        throw new IllegalArgumentException(String.format(RestApiErrorMessages\n            .ERROR_COMPONENT_NAME_INVALID, maxCompLength, comp.getName()));\n      }\n      if (componentNames.contains(comp.getName())) {\n        throw new IllegalArgumentException(\"Component name collision: \" +\n            comp.getName());\n      }\n      // If artifact is of type SERVICE (which cannot be filled from\n      // global), read external service and add its components to this\n      // service\n      if (comp.getArtifact() != null && comp.getArtifact().getType() ==\n          Artifact.TypeEnum.SERVICE) {\n        if (StringUtils.isEmpty(comp.getArtifact().getId())) {\n          throw new IllegalArgumentException(\n              RestApiErrorMessages.ERROR_ARTIFACT_ID_INVALID);\n        }\n        LOG.info(\"Marking {} for removal\", comp.getName());\n        componentsToRemove.add(comp);\n        List<Component> externalComponents = getComponents(fs,\n            comp.getArtifact().getId());\n        for (Component c : externalComponents) {\n          Component override = service.getComponent(c.getName());\n          if (override != null && override.getArtifact() == null) {\n            // allow properties from external components to be overridden /\n            // augmented by properties in this component, except for artifact\n            // which must be read from external component\n            override.mergeFrom(c);\n            LOG.info(\"Merging external component {} from external {}\", c\n                .getName(), comp.getName());\n          } else {\n            if (componentNames.contains(c.getName())) {\n              throw new IllegalArgumentException(\"Component name collision: \" +\n                  c.getName());\n            }\n            componentNames.add(c.getName());\n            componentsToAdd.add(c);\n            LOG.info(\"Adding component {} from external {}\", c.getName(),\n                comp.getName());\n          }\n        }\n      } else {\n        // otherwise handle as a normal component\n        componentNames.add(comp.getName());\n        // configuration\n        comp.getConfiguration().mergeFrom(globalConf);\n      }\n    }\n    service.getComponents().removeAll(componentsToRemove);\n    service.getComponents().addAll(componentsToAdd);\n\n    // Validate components and let global values take effect if component level\n    // values are not provided\n    Artifact globalArtifact = service.getArtifact();\n    Resource globalResource = service.getResource();\n    for (Component comp : service.getComponents()) {\n      // fill in global artifact unless it is type SERVICE\n      if (comp.getArtifact() == null && service.getArtifact() != null\n          && service.getArtifact().getType() != Artifact.TypeEnum\n          .SERVICE) {\n        comp.setArtifact(globalArtifact);\n      }\n      // fill in global resource\n      if (comp.getResource() == null) {\n        comp.setResource(globalResource);\n      }\n      // validate dependency existence\n      if (comp.getDependencies() != null) {\n        for (String dependency : comp.getDependencies()) {\n          if (!componentNames.contains(dependency)) {\n            throw new IllegalArgumentException(String.format(\n                RestApiErrorMessages.ERROR_DEPENDENCY_INVALID, dependency,\n                comp.getName()));\n          }\n        }\n      }\n      validateComponent(comp, fs.getFileSystem(), conf);\n    }\n    validatePlacementPolicy(service.getComponents(), componentNames);\n\n    // validate dependency tree\n    sortByDependencies(service.getComponents());\n\n    // Service lifetime if not specified, is set to unlimited lifetime\n    if (service.getLifetime() == null) {\n      service.setLifetime(RestApiConstants.DEFAULT_UNLIMITED_LIFETIME);\n    }\n  }"
        }
    },
    {
        "filename": "YARN-2857.json",
        "creation_time": "2014-10-24T20:47:51.000+0000",
        "bug_report": {
            "Title": "ConcurrentModificationException in ContainerLogAppender",
            "Description": "The issue arises in a Hadoop environment (version 2.3.0) when a job is submitted via Oozie (version 4.0.1) to execute a Pig script (version 0.11.x). The logs indicate a `ConcurrentModificationException` occurring during the shutdown process of the logging system, specifically within the `ContainerLogAppender` class. This exception typically occurs when a collection is modified while it is being iterated over, which can happen in multi-threaded scenarios.",
            "StackTrace": [
                "java.util.ConcurrentModificationException",
                "at java.util.LinkedList$ListItr.checkForComodification(LinkedList.java:966)",
                "at java.util.LinkedList$ListItr.next(LinkedList.java:888)",
                "at org.apache.hadoop.yarn.ContainerLogAppender.close(ContainerLogAppender.java:94)",
                "at org.apache.log4j.helpers.AppenderAttachableImpl.removeAllAppenders(AppenderAttachableImpl.java:141)",
                "at org.apache.log4j.Category.removeAllAppenders(Category.java:891)",
                "at org.apache.log4j.Hierarchy.shutdown(Hierarchy.java:471)",
                "at org.apache.log4j.LogManager.shutdown(LogManager.java:267)",
                "at org.apache.hadoop.mapred.TaskLog.syncLogsShutdown(TaskLog.java:286)",
                "at org.apache.hadoop.mapred.TaskLog$2.run(TaskLog.java:339)",
                "at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)"
            ],
            "RootCause": "The `ConcurrentModificationException` is triggered when the `ContainerLogAppender` attempts to close and remove appenders from a `LinkedList` while it is being modified by another thread, likely during the shutdown process.",
            "StepsToReproduce": [
                "Submit a job using Oozie to run a Pig script.",
                "Ensure that the job runs in a multi-threaded environment.",
                "Monitor the logs during the shutdown phase of the job."
            ],
            "ExpectedBehavior": "The logging system should shut down gracefully without throwing exceptions, ensuring all log appenders are removed properly.",
            "ObservedBehavior": "A `ConcurrentModificationException` is thrown, indicating that the logging system encountered an issue while trying to modify a collection during iteration.",
            "Suggestions": "To avoid this exception, consider using concurrent collections such as `CopyOnWriteArrayList` or implement synchronized blocks around the code that modifies the collection. Additionally, ensure that the `remove()` method of the iterator is used for safe removal during iteration.",
            "problem_location": {
                "files": [
                    "ContainerLogAppender.java",
                    "TaskLog.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.ContainerLogAppender",
                    "org.apache.hadoop.mapred.TaskLog"
                ],
                "methods": [
                    "ContainerLogAppender.close",
                    "TaskLog.syncLogsShutdown"
                ]
            },
            "possible_fix": "Refactor the `ContainerLogAppender.close` method to use a concurrent collection or synchronize access to the `LinkedList` to prevent concurrent modifications during iteration."
        },
        "possible_fix_code": {
            "ContainerLogAppender.close": "  public synchronized void close() {\n    if (tail != null) {\n      // Create a copy of the tail to avoid ConcurrentModificationException\n      List<LoggingEvent> eventsCopy = new ArrayList<>(tail);\n      for (LoggingEvent event : eventsCopy) {\n        super.append(event);\n      }\n    }\n    super.close();\n  }"
        }
    },
    {
        "filename": "YARN-2416.json",
        "creation_time": "2014-08-13T22:36:31.000+0000",
        "bug_report": {
            "Title": "InvalidStateTransitonException in ResourceManager if AMLauncher does not receive response for startContainers() call in time",
            "Description": "The issue arises when the AMLauncher invokes the startContainers(allRequests) method to launch a container for the application master. Under normal circumstances, this call returns promptly, allowing the RMAppAttempt to transition from the ALLOCATED state to the LAUNCHED state. However, in certain scenarios, the RPC call experiences significant delays, leading to the AM container being started while the RMAppAttempt remains in the ALLOCATED state. Consequently, when the ResourceManager receives the REGISTERED event from the application master, it triggers an InvalidStateTransitonException due to the invalid state transition attempt. This issue is compounded by subsequent STATUS_UPDATE and CONTAINER_ALLOCATED events, which also result in InvalidStateTransitonException errors.",
            "StackTrace": [
                "2014-07-05 08:59:05,021 ERROR org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: Can't handle this event at current state",
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: REGISTERED at ALLOCATED",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:652)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:106)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:752)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$ApplicationAttemptEventDispatcher.handle(ResourceManager.java:733)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:173)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:106)",
                "at java.lang.Thread.run(Thread.java:744)"
            ],
            "RootCause": "The root cause of the issue is the invalid state transitions occurring in the YARN framework, specifically when the application attempts to transition from the ALLOCATED state to other states (REGISTERED, STATUS_UPDATE, CONTAINER_ALLOCATED) that are not valid according to the state machine defined in YARN.",
            "StepsToReproduce": [
                "1. Launch an application using AMLauncher.",
                "2. Ensure that the startContainers(allRequests) call is delayed.",
                "3. Observe the state of the RMAppAttempt as it remains in ALLOCATED state.",
                "4. Trigger the REGISTERED event from the application master."
            ],
            "ExpectedBehavior": "The RMAppAttempt should transition from ALLOCATED to LAUNCHED state upon receiving the response from startContainers(). Subsequent events like REGISTERED, STATUS_UPDATE, and CONTAINER_ALLOCATED should be handled without errors.",
            "ObservedBehavior": "The RMAppAttempt remains in the ALLOCATED state, leading to InvalidStateTransitonException errors when the ResourceManager receives REGISTERED, STATUS_UPDATE, and CONTAINER_ALLOCATED events.",
            "Suggestions": "Review the state transition logic in the YARN source code to ensure valid transitions from the ALLOCATED state. Check the application code to confirm that it sends the correct events at the appropriate times. Investigate any configuration settings that might affect state transitions.",
            "problem_location": {
                "files": [
                    "RMAppAttemptImpl.java",
                    "ResourceManager.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl",
                    "org.apache.hadoop.yarn.server.resourcemanager.ResourceManager"
                ],
                "methods": [
                    "RMAppAttemptImpl.handle"
                ]
            },
            "possible_fix": "In the handle method of RMAppAttemptImpl, ensure that the state transitions are validated against the state machine before processing events. Implement error handling to manage invalid transitions gracefully."
        },
        "possible_fix_code": {
            "RMAppAttemptImpl.handle": "public void handle(RMAppAttemptEvent event) {\n\n    this.writeLock.lock();\n\n    try {\n      ApplicationAttemptId appAttemptID = event.getApplicationAttemptId();\n      LOG.debug(\"Processing event for \" + appAttemptID + \" of type \" + event.getType());\n      final RMAppAttemptState oldState = getAppAttemptState();\n      try {\n        /* keep the master in sync with the state machine */\n        this.stateMachine.doTransition(event.getType(), event);\n      } catch (InvalidStateTransitonException e) {\n        LOG.error(\"Can't handle this event at current state\", e);\n        // Handle invalid state transition gracefully\n        if (oldState == RMAppAttemptState.ALLOCATED) {\n            // Log a warning or take appropriate action\n            LOG.warn(\"Invalid transition from ALLOCATED state for event: \" + event.getType());\n            // Optionally, you could set a flag or notify the application\n        }\n        /* TODO fail the application on the failed transition */\n      }\n\n      if (oldState != getAppAttemptState()) {\n        LOG.info(appAttemptID + \" State change from \" + oldState + \" to \" + getAppAttemptState());\n      }\n    } finally {\n      this.writeLock.unlock();\n    }\n  }"
        }
    },
    {
        "filename": "YARN-345.json",
        "creation_time": "2013-01-17T12:57:46.000+0000",
        "bug_report": {
            "Title": "Many InvalidStateTransitonException errors for ApplicationImpl in Node Manager",
            "Description": "The application is encountering multiple instances of InvalidStateTransitonException errors while processing events in the YARN Node Manager. These errors indicate that the application is attempting to handle events such as FINISH_APPLICATION, APPLICATION_CONTAINER_FINISHED, and INIT_CONTAINER while in states where these events are not valid. This suggests a misconfiguration in the state machine's event handling and lifecycle management within the YARN framework.",
            "StackTrace": [
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: FINISH_APPLICATION at FINISHED",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:301)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:443)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.handle(ApplicationImpl.java:398)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.handle(ApplicationImpl.java:58)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher.handle(ContainerManagerImpl.java:520)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher.handle(ContainerManagerImpl.java:512)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:126)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:75)",
                "at java.lang.Thread.run(Thread.java:662)"
            ],
            "RootCause": "The root cause of the issue is that the application is trying to process events that are not valid for its current state, leading to InvalidStateTransitonException errors. This indicates a potential misconfiguration in the state machine's transitions, particularly in the ApplicationImpl class and its interaction with the StateMachineFactory.",
            "StepsToReproduce": [
                "Deploy a YARN application that transitions through various states.",
                "Trigger events such as FINISH_APPLICATION, APPLICATION_CONTAINER_FINISHED, and INIT_CONTAINER.",
                "Observe the logs for InvalidStateTransitonException errors."
            ],
            "ExpectedBehavior": "The application should handle events appropriately based on its current state without throwing InvalidStateTransitonException errors.",
            "ObservedBehavior": "The application throws InvalidStateTransitonException errors when attempting to handle events that are not valid for its current state.",
            "Suggestions": "Review the state transitions defined in the StateMachineFactory and ensure that the ApplicationImpl class is correctly configured to handle the expected events for each state. Consider adding missing transitions or correcting the event handling logic.",
            "problem_location": {
                "files": [
                    "StateMachineFactory.java",
                    "ApplicationImpl.java",
                    "ContainerManagerImpl.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.state.StateMachineFactory",
                    "org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl",
                    "org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl"
                ],
                "methods": [
                    "StateMachineFactory.doTransition",
                    "StateMachineFactory.addTransition",
                    "StateMachineFactory.make",
                    "ApplicationImpl.handle"
                ]
            },
            "possible_fix": "Investigate the transitions defined in the StateMachineFactory and ensure that the ApplicationImpl class is properly handling the lifecycle events. This may involve modifying the addTransition method to include necessary transitions or adjusting the event handling logic in the ApplicationImpl class."
        },
        "possible_fix_code": {
            "org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.handle": "public void handle(Event event) {\n    if (event == null) {\n        throw new IllegalArgumentException(\"Event cannot be null\");\n    }\n    switch (event.getType()) {\n        case FINISH_APPLICATION:\n            if (currentState != ApplicationState.FINISHED) {\n                throw new InvalidStateTransitonException(\"Cannot finish application in state: \" + currentState);\n            }\n            // Handle finishing application logic\n            break;\n        case APPLICATION_CONTAINER_FINISHED:\n            if (currentState != ApplicationState.RUNNING) {\n                throw new InvalidStateTransitonException(\"Cannot finish container in state: \" + currentState);\n            }\n            // Handle container finished logic\n            break;\n        case INIT_CONTAINER:\n            if (currentState != ApplicationState.INITIALIZED) {\n                throw new InvalidStateTransitonException(\"Cannot initialize container in state: \" + currentState);\n            }\n            // Handle initializing container logic\n            break;\n        default:\n            throw new UnsupportedOperationException(\"Event type not supported: \" + event.getType());\n    }\n}"
        }
    },
    {
        "filename": "YARN-3894.json",
        "creation_time": "2015-07-08T07:00:51.000+0000",
        "bug_report": {
            "Title": "RM startup should fail for wrong CS xml NodeLabel capacity configuration",
            "Description": "The ResourceManager (RM) fails to start correctly when there is a mismatch in the NodeLabel capacity configuration within the Capacity Scheduler. Specifically, if the capacity configuration for a NodeLabel is incorrect, the RM should not initialize successfully. The issue arises during the queue initialization process, particularly in the `parseQueue` method, where the label capacities are validated. If the capacities do not conform to the expected rules, an `IllegalArgumentException` is thrown, indicating an illegal capacity setting.",
            "StackTrace": [
                "2015-07-07 19:18:25,655 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Initialized queue: default: capacity=0.5, absoluteCapacity=0.5, usedResources=<memory:0, vCores:0>, usedCapacity=0.0, absoluteUsedCapacity=0.0, numApps=0, numContainers=0",
                "2015-07-07 19:18:25,656 WARN org.apache.hadoop.yarn.server.resourcemanager.AdminService: Exception refresh queues.",
                "java.io.IOException: Failed to re-init queues",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.reinitialize(CapacityScheduler.java:383)",
                "at org.apache.hadoop.yarn.server.resourcemanager.AdminService.refreshQueues(AdminService.java:376)",
                "at org.apache.hadoop.yarn.server.resourcemanager.AdminService.refreshAll(AdminService.java:605)",
                "at org.apache.hadoop.yarn.server.resourcemanager.AdminService.transitionToActive(AdminService.java:314)",
                "Caused by: java.lang.IllegalArgumentException: Illegal capacity of 0.5 for children of queue root for label=node2",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.setChildQueues(ParentQueue.java:159)"
            ],
            "RootCause": "The root cause of the issue is a misconfiguration in the YARN queue settings, specifically where the total capacity of child queues exceeds the allowed limits or does not match the parent queue's capacity requirements. This is validated in the `setChildQueues` method of the `ParentQueue` class, which throws an `IllegalArgumentException` when the capacities are invalid.",
            "StepsToReproduce": [
                "Configure RM with capacity scheduler.",
                "Add one or two node labels using rmadmin.",
                "Configure the capacity XML with node labels but introduce an issue with the capacity configuration for the already added label.",
                "Restart both RM instances.",
                "Check the service initialization of the capacity scheduler to see if the node label list is populated."
            ],
            "ExpectedBehavior": "The ResourceManager should not start if there is an illegal capacity configuration for the NodeLabels.",
            "ObservedBehavior": "The ResourceManager attempts to start but fails with an IOException indicating a failure to reinitialize queues due to an illegal capacity setting.",
            "Suggestions": "Review the queue configurations in the YARN configuration files to ensure that the capacities are set correctly according to the rules defined in the `setChildQueues` method. Adjust the capacities of the child queues to comply with the expected values.",
            "problem_location": {
                "files": [
                    "CapacityScheduler.java",
                    "ParentQueue.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler",
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue"
                ],
                "methods": [
                    "CapacityScheduler.reinitializeQueues",
                    "CapacityScheduler.parseQueue",
                    "ParentQueue.setChildQueues"
                ]
            },
            "possible_fix": "Ensure that the capacities defined in the configuration files for the queues are valid. For example, if the parent queue's capacity is set to 0, all child queues must also have a capacity of 0. Adjust the configuration to prevent illegal capacity settings."
        },
        "possible_fix_code": {
            "ParentQueue.setChildQueues": "  synchronized void setChildQueues(Collection<CSQueue> childQueues) {\n    // Validate\n    float childCapacities = 0;\n    for (CSQueue queue : childQueues) {\n      childCapacities += queue.getCapacity();\n    }\n    float delta = Math.abs(1.0f - childCapacities);  // crude way to check\n    // allow capacities being set to 0, and enforce child 0 if parent is 0\n    if (((queueCapacities.getCapacity() > 0) && (delta > PRECISION)) || \n        ((queueCapacities.getCapacity() == 0) && (childCapacities > 0))) {\n      throw new IllegalArgumentException(\"Illegal capacity of \" + childCapacities + \n          \" for children of queue \" + queueName);\n    }\n    // check label capacities\n    for (String nodeLabel : labelManager.getClusterNodeLabelNames()) {\n      float capacityByLabel = queueCapacities.getCapacity(nodeLabel);\n      // check children's labels\n      float sum = 0;\n      for (CSQueue queue : childQueues) {\n        sum += queue.getQueueCapacities().getCapacity(nodeLabel);\n      }\n      if ((capacityByLabel > 0 && Math.abs(1.0f - sum) > PRECISION)\n          || (capacityByLabel == 0) && (sum > 0)) {\n        throw new IllegalArgumentException(\"Illegal capacity of \" + sum + \n            \" for children of queue \" + queueName + \" for label=\" + nodeLabel);\n      }\n    }\n    \n    this.childQueues.clear();\n    this.childQueues.addAll(childQueues);\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"setChildQueues: \" + getChildQueuesToPrint());\n    }\n  }"
        }
    },
    {
        "filename": "YARN-4347.json",
        "creation_time": "2015-11-11T22:32:59.000+0000",
        "bug_report": {
            "Title": "Resource manager fails with Null pointer exception",
            "Description": "The Resource Manager encounters a Null Pointer Exception (NPE) when attempting to load or recover a finished application. The issue arises specifically in the `addApplicationAttempt` method of the `CapacityScheduler` class, where a null reference is accessed due to the application not being found in the `applications` map. This can occur if the application was rejected during the addition process, which is handled in the `addApplication` method.",
            "StackTrace": [
                "2015-11-11 17:53:22,351 ERROR resourcemanager.ResourceManager (ResourceManager.java:serviceStart(597)) - Failed to load/recover state",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.addApplicationAttempt(CapacityScheduler.java:746)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:1155)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:116)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl$AttemptRecoveredTransition.transition(RMAppAttemptImpl.java:1037)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl$AttemptRecoveredTransition.transition(RMAppAttemptImpl.java:1001)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$MultipleInternalArc.doTransition(StateMachineFactory.java:385)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:755)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle(RMAppAttemptImpl.java:106)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.recoverAppAttempts(RMAppImpl.java:839)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.access$1900(RMAppImpl.java:102)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl$RMAppRecoveredTransition.transition(RMAppImpl.java:854)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl$RMAppRecoveredTransition.transition(RMAppImpl.java:844)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$MultipleInternalArc.doTransition(StateMachineFactory.java:385)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.handle(RMAppImpl.java:719)",
                "at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.recoverApplication(RMAppManager.java:313)",
                "at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.recover(RMAppManager.java:411)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.recover(ResourceManager.java:1219)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$RMActiveServices.serviceStart(ResourceManager.java:593)",
                "at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.startActiveServices(ResourceManager.java:1026)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1067)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$1.run(ResourceManager.java:1063)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)"
            ],
            "RootCause": "The NullPointerException occurs in the `addApplicationAttempt` method of the `CapacityScheduler` class when the application associated with the given ApplicationAttemptId is not found in the `applications` map. This situation arises if the application was rejected during the addition process in the `addApplication` method due to invalid queue conditions or access control issues.",
            "StepsToReproduce": [
                "Attempt to load or recover a finished application in the Resource Manager.",
                "Ensure that the application was previously submitted to an invalid queue or was rejected due to access control issues."
            ],
            "ExpectedBehavior": "The Resource Manager should successfully load or recover the application state without throwing a NullPointerException.",
            "ObservedBehavior": "The Resource Manager throws a NullPointerException when trying to load or recover a finished application, indicating that the application cannot be found in the scheduler.",
            "Suggestions": "Review the application submission process to ensure that applications are correctly added to the `applications` map. Implement additional checks to handle cases where applications are not found, and log appropriate warnings or errors.",
            "problem_location": {
                "files": [
                    "CapacityScheduler.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler"
                ],
                "methods": [
                    "addApplicationAttempt",
                    "addApplication"
                ]
            },
            "possible_fix": "In the `addApplicationAttempt` method, add a check to handle the case where the application is not found in the `applications` map, and ensure that the application is successfully added in the `addApplication` method before attempting to recover it."
        },
        "possible_fix_code": {
            "addApplicationAttempt": "  private synchronized void addApplicationAttempt(\n      ApplicationAttemptId applicationAttemptId,\n      boolean transferStateFromPreviousAttempt,\n      boolean isAttemptRecovering) {\n    SchedulerApplication<FiCaSchedulerApp> application =\n        applications.get(applicationAttemptId.getApplicationId());\n    if (application == null) {\n      LOG.warn(\"Application \" + applicationAttemptId.getApplicationId() +\n          \" cannot be found in scheduler.\");\n      // Handle the case where the application is not found\n      // You may want to throw an exception or log an error\n      return;\n    }\n    CSQueue queue = (CSQueue) application.getQueue();\n\n    FiCaSchedulerApp attempt = new FiCaSchedulerApp(applicationAttemptId,\n        application.getUser(), queue, queue.getActiveUsersManager(), rmContext,\n        application.getPriority());\n    if (transferStateFromPreviousAttempt) {\n      attempt.transferStateFromPreviousAttempt(\n          application.getCurrentAppAttempt());\n    }\n    application.setCurrentAppAttempt(attempt);\n\n    // Update attempt priority to the latest to avoid race condition i.e\n    // SchedulerApplicationAttempt is created with old priority but it is not\n    // set to SchedulerApplication#setCurrentAppAttempt.\n    // Scenario would occur is\n    // 1. SchdulerApplicationAttempt is created with old priority.\n    // 2. updateApplicationPriority() updates SchedulerApplication. Since\n    // currentAttempt is null, it just return.\n    // 3. ScheduelerApplcationAttempt is set in\n    // SchedulerApplication#setCurrentAppAttempt.\n    attempt.setPriority(application.getPriority());\n\n    queue.submitApplicationAttempt(attempt, application.getUser());\n    LOG.info(\"Added Application Attempt \" + applicationAttemptId\n        + \" to scheduler from user \" + application.getUser() + \" in queue \"\n        + queue.getQueueName());\n    if (isAttemptRecovering) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(applicationAttemptId\n            + \" is recovering. Skipping notifying ATTEMPT_ADDED\");\n      }\n    } else {\n      rmContext.getDispatcher().getEventHandler().handle(\n        new RMAppAttemptEvent(applicationAttemptId,\n            RMAppAttemptEventType.ATTEMPT_ADDED));\n    }\n  }"
        }
    },
    {
        "filename": "YARN-1692.json",
        "creation_time": "2014-02-07T02:01:17.000+0000",
        "bug_report": {
            "Title": "ConcurrentModificationException in fair scheduler AppSchedulable",
            "Description": "A ConcurrentModificationException is thrown in the fair scheduler's AppSchedulable class during the execution of the updateDemand method. This exception typically occurs when a collection is modified while being iterated over, which is common in multi-threaded environments. The stack trace indicates that the exception arises from the HashMap used in the resource requests, specifically when the method attempts to iterate over the values of the resource requests without proper synchronization.",
            "StackTrace": [
                "2014-02-07 01:40:01,978 ERROR org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler: Exception in fair scheduler UpdateThread",
                "java.util.ConcurrentModificationException",
                "at java.util.HashMap$HashIterator.nextEntry(HashMap.java:926)",
                "at java.util.HashMap$ValueIterator.next(HashMap.java:954)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AppSchedulable.updateDemand(AppSchedulable.java:85)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue.updateDemand(FSLeafQueue.java:125)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSParentQueue.updateDemand(FSParentQueue.java:82)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.update(FairScheduler.java:217)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler$UpdateThread.run(FairScheduler.java:195)",
                "at java.lang.Thread.run(Thread.java:724)"
            ],
            "RootCause": "The root cause of the ConcurrentModificationException is the lack of synchronization when iterating over the values of a HashMap in the updateDemand method of the AppSchedulable class. This method is called in a multi-threaded context, where other threads may modify the HashMap concurrently.",
            "StepsToReproduce": [
                "1. Start the fair scheduler in a multi-threaded environment.",
                "2. Submit multiple applications that request resources concurrently.",
                "3. Monitor the logs for ConcurrentModificationException errors."
            ],
            "ExpectedBehavior": "The fair scheduler should update the resource demands of applications without throwing exceptions, ensuring that resource requests are processed correctly even in a multi-threaded environment.",
            "ObservedBehavior": "A ConcurrentModificationException is thrown, indicating that the resource requests HashMap was modified while being iterated over, leading to a failure in updating the resource demands.",
            "Suggestions": "Implement synchronization mechanisms around the iteration of the HashMap in the updateDemand method, or consider using concurrent collections such as ConcurrentHashMap to avoid this issue.",
            "problem_location": {
                "files": [
                    "AppSchedulable.java",
                    "FSLeafQueue.java",
                    "FSParentQueue.java",
                    "FairScheduler.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AppSchedulable",
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue",
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSParentQueue",
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler"
                ],
                "methods": [
                    "AppSchedulable.updateDemand",
                    "FSLeafQueue.updateDemand",
                    "FSParentQueue.updateDemand",
                    "FairScheduler.update"
                ]
            },
            "possible_fix": "To fix the issue, modify the updateDemand method in AppSchedulable to use a synchronized block or use a ConcurrentHashMap for resource requests. For example:\n\n```java\npublic void updateDemand() {\n    synchronized (app.getResourceRequests()) {\n        // existing logic\n    }\n}\n```\nAlternatively, replace HashMap with ConcurrentHashMap in the resource requests to allow safe concurrent access."
        },
        "possible_fix_code": {
            "AppSchedulable.updateDemand": "  public void updateDemand() {\n    demand = Resources.createResource(0);\n    // Demand is current consumption plus outstanding requests\n    Resources.addTo(demand, app.getCurrentConsumption());\n\n    // Add up outstanding resource requests\n    synchronized (app.getResourceRequests()) {\n      for (Priority p : app.getPriorities()) {\n        for (ResourceRequest r : app.getResourceRequests(p).values()) {\n          Resource total = Resources.multiply(r.getCapability(), r.getNumContainers());\n          Resources.addTo(demand, total);\n        }\n      }\n    }\n  }"
        }
    },
    {
        "filename": "YARN-7697.json",
        "creation_time": "2018-01-03T19:28:50.000+0000",
        "bug_report": {
            "Title": "NM goes down with OOM due to leak in log-aggregation",
            "Description": "The Node Manager (NM) encounters an OutOfMemoryError (OOM) due to excessive memory consumption during the log aggregation process in Hadoop YARN. The error occurs when the JVM cannot allocate memory for objects, leading to a crash of the LogAggregationService thread. The stack trace indicates that the issue arises from the LogAggregationIndexedFileController methods, particularly during the loading and initialization of log metadata and file operations.",
            "StackTrace": [
                "java.lang.OutOfMemoryError: Java heap space",
                "at org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController.loadIndexedLogsMeta(LogAggregationIndexedFileController.java:823)",
                "at org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController.loadIndexedLogsMeta(LogAggregationIndexedFileController.java:840)",
                "at org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController.initializeWriterInRolling(LogAggregationIndexedFileController.java:293)",
                "at org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController.access$600(LogAggregationIndexedFileController.java:98)",
                "at org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController$1.run(LogAggregationIndexedFileController.java:216)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1866)",
                "at org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController.initializeWriter(LogAggregationIndexedFileController.java:197)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl.uploadLogsForContainers(AppLogAggregatorImpl.java:205)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl.doAppLogAggregation(AppLogAggregatorImpl.java:312)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl.run(AppLogAggregatorImpl.java:284)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService$1.run(LogAggregationService.java:262)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)",
                "at java.lang.Thread.run(Thread.java:748)"
            ],
            "RootCause": "The OutOfMemoryError is primarily caused by inefficient memory management during the log aggregation process, particularly in the methods responsible for loading and initializing log metadata. The repeated creation of large collections and objects without proper cleanup leads to excessive memory consumption.",
            "StepsToReproduce": [
                "Start a Hadoop YARN application with a large number of containers.",
                "Enable log aggregation for the application.",
                "Monitor the memory usage of the Node Manager during the log aggregation process.",
                "Observe the OutOfMemoryError occurring in the logs."
            ],
            "ExpectedBehavior": "The Node Manager should successfully aggregate logs from all containers without running out of memory, allowing for efficient log management and retrieval.",
            "ObservedBehavior": "The Node Manager crashes with an OutOfMemoryError, causing the log aggregation service to shut down unexpectedly.",
            "Suggestions": "1. Increase the JVM heap size to allocate more memory for the Node Manager.\n2. Optimize the log aggregation logic to reduce memory consumption, such as reusing objects and avoiding unnecessary object creation.\n3. Implement monitoring tools to track memory usage and identify potential memory leaks.\n4. Ensure proper cleanup of temporary objects and resources after use.",
            "problem_location": {
                "files": [
                    "LogAggregationIndexedFileController.java",
                    "AppLogAggregatorImpl.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController",
                    "org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl"
                ],
                "methods": [
                    "loadIndexedLogsMeta(Path remoteLogPath)",
                    "initializeWriterInRolling(Path remoteLogFile, ApplicationId appId, String nodeId)",
                    "uploadLogsForContainers(boolean appFinished)",
                    "doAppLogAggregation()"
                ]
            },
            "possible_fix": "Consider increasing the JVM heap size in the Node Manager configuration. Additionally, review the log aggregation methods to optimize memory usage, such as reusing instances of IndexedLogsMeta and ContainerLogAggregator, and ensuring that collections are cleared after use."
        },
        "possible_fix_code": {
            "initializeWriterInRolling": "  private Path initializeWriterInRolling(final Path remoteLogFile,\n      final ApplicationId appId, final String nodeId) throws Exception {\n    Path aggregatedLogFile = null;\n    // check uuid\n    if (uuid == null) {\n      uuid = loadUUIDFromLogFile(fc, remoteLogFile.getParent(),\n            appId, nodeId);\n    }\n    Path currentRemoteLogFile = getCurrentRemoteLogFile(\n        fc, remoteLogFile.getParent(), nodeId);\n    // check checksum file\n    boolean overwriteCheckSum = true;\n    remoteLogCheckSumFile = new Path(remoteLogFile.getParent(),\n        (remoteLogFile.getName() + CHECK_SUM_FILE_SUFFIX));\n    if(fc.util().exists(remoteLogCheckSumFile)) {\n      // if the checksum file exists, we should reset cached\n      // indexedLogsMeta.\n      indexedLogsMeta.getLogMetas().clear();\n      if (currentRemoteLogFile != null) {\n        FSDataInputStream checksumFileInputStream = null;\n        try {\n          checksumFileInputStream = fc.open(remoteLogCheckSumFile);\n          int nameLength = checksumFileInputStream.readInt();\n          byte[] b = new byte[nameLength];\n          int actualLength = checksumFileInputStream.read(b);\n          if (actualLength == nameLength) {\n            String recoveredLogFile = new String(\n                b, Charset.forName(\"UTF-8\"));\n            if (recoveredLogFile.equals(\n                currentRemoteLogFile.getName())) {\n              overwriteCheckSum = false;\n              long endIndex = checksumFileInputStream.readLong();\n              IndexedLogsMeta recoveredLogsMeta = null;\n              try {\n                truncateFileWithRetries(fc, currentRemoteLogFile,\n                    endIndex);\n                recoveredLogsMeta = loadIndexedLogsMeta(\n                    currentRemoteLogFile);\n              } catch (Exception ex) {\n                recoveredLogsMeta = loadIndexedLogsMeta(\n                    currentRemoteLogFile, endIndex);\n              }\n              if (recoveredLogsMeta != null) {\n                indexedLogsMeta = recoveredLogsMeta;\n              }\n            }\n          }\n        } finally {\n          IOUtils.cleanupWithLogger(LOG, checksumFileInputStream);\n        }\n      }\n    }\n    // check whether we need roll over old logs\n    if (currentRemoteLogFile == null || isRollover(\n        fc, currentRemoteLogFile)) {\n      indexedLogsMeta.getLogMetas().clear();\n      overwriteCheckSum = true;\n      aggregatedLogFile = new Path(remoteLogFile.getParent(),\n          remoteLogFile.getName() + \"_\" + sysClock.getTime());\n      fsDataOStream = fc.create(aggregatedLogFile,\n          EnumSet.of(CreateFlag.CREATE, CreateFlag.OVERWRITE),\n          new Options.CreateOpts[] {});\n      // writes the uuid\n      fsDataOStream.write(uuid);\n      fsDataOStream.flush();\n    } else {\n      aggregatedLogFile = currentRemoteLogFile;\n      fsDataOStream = fc.create(currentRemoteLogFile,\n          EnumSet.of(CreateFlag.CREATE, CreateFlag.APPEND),\n          new Options.CreateOpts[] {});\n    }\n    // recreate checksum file if needed before aggregate the logs\n    if (overwriteCheckSum) {\n      final long currentAggregatedLogFileLength = fc\n          .getFileStatus(aggregatedLogFile).getLen();\n      FSDataOutputStream checksumFileOutputStream = null;\n      try {\n        checksumFileOutputStream = fc.create(remoteLogCheckSumFile,\n            EnumSet.of(CreateFlag.CREATE, CreateFlag.OVERWRITE),\n            new Options.CreateOpts[] {});\n        String fileName = aggregatedLogFile.getName();\n        checksumFileOutputStream.writeInt(fileName.length());\n        checksumFileOutputStream.write(fileName.getBytes(\n            Charset.forName(\"UTF-8\"));\n        checksumFileOutputStream.writeLong(\n            currentAggregatedLogFileLength);\n        checksumFileOutputStream.flush();\n      } finally {\n        IOUtils.cleanupWithLogger(LOG, checksumFileOutputStream);\n      }\n    }\n    return aggregatedLogFile;\n  }"
        }
    },
    {
        "filename": "YARN-7382.json",
        "creation_time": "2017-10-23T23:36:59.000+0000",
        "bug_report": {
            "Title": "NoSuchElementException in FairScheduler after failover causes RM crash",
            "Description": "During the execution of a MapReduce job, specifically when a ResourceManager (RM) failover occurs, the active RM crashes after the map tasks reach 100% completion. The crash is triggered by a `NoSuchElementException` in the FairScheduler, which is evident from the stack trace. The exception arises when the `getNextPendingAsk` method in the `AppSchedulingInfo` class attempts to access the first key of an empty `schedulerKeys` collection, indicating that there are no pending resource requests available for scheduling. This situation leads to the failure of the Event Dispatcher, ultimately resulting in the RM crash.",
            "StackTrace": [
                "2017-10-18 15:02:05,347 INFO org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl: container_1508361403235_0001_01_000002 Container Transitioned from RUNNING to COMPLETED",
                "2017-10-18 15:02:05,347 INFO org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger: USER=systest  OPERATION=AM Released Container TARGET=SchedulerApp RESULT=SUCCESS  APPID=application_1508361403235_0001 CONTAINERID=container_1508361403235_0001_01_000002 RESOURCE=<memory:1024, vCores:1>",
                "2017-10-18 15:02:05,349 FATAL org.apache.hadoop.yarn.event.EventDispatcher: Error in handling event type NODE_UPDATE to the Event Dispatcher",
                "java.util.NoSuchElementException",
                "at java.util.concurrent.ConcurrentSkipListMap.firstKey(ConcurrentSkipListMap.java:2036)",
                "at java.util.concurrent.ConcurrentSkipListSet.first(ConcurrentSkipListSet.java:396)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.getNextPendingAsk(AppSchedulingInfo.java:371)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt.isOverAMShareLimit(FSAppAttempt.java:901)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt.assignContainer(FSAppAttempt.java:1326)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue.assignContainer(FSLeafQueue.java:371)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSParentQueue.assignContainer(FSParentQueue.java:221)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSParentQueue.assignContainer(FSParentQueue.java:221)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.attemptScheduling(FairScheduler.java:1019)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.nodeUpdate(FairScheduler.java:887)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:1104)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle(FairScheduler.java:128)",
                "at org.apache.hadoop.yarn.event.EventDispatcher$EventProcessor.run(EventDispatcher.java:66)",
                "at java.lang.Thread.run(Thread.java:748)",
                "2017-10-18 15:02:05,360 INFO org.apache.hadoop.yarn.event.EventDispatcher: Exiting, bbye.."
            ],
            "RootCause": "The root cause of the `NoSuchElementException` is the `getNextPendingAsk` method in the `AppSchedulingInfo` class attempting to access the first key of an empty `schedulerKeys` collection, which occurs when there are no pending resource requests available for scheduling.",
            "StepsToReproduce": [
                "1. Start a MapReduce job (e.g., a sleep job).",
                "2. Trigger a ResourceManager failover during the job execution.",
                "3. Wait for the map tasks to reach 100% completion.",
                "4. Observe the ResourceManager crash due to the NoSuchElementException."
            ],
            "ExpectedBehavior": "The ResourceManager should handle the failover gracefully without crashing, allowing the job to continue processing or to be rescheduled appropriately.",
            "ObservedBehavior": "The ResourceManager crashes with a `NoSuchElementException`, leaving the cluster without any active ResourceManagers.",
            "Suggestions": "To mitigate this issue, ensure that the `schedulerKeys` collection is checked for emptiness before attempting to access its first key. Implementing a check in the `getNextPendingAsk` method could prevent the exception from being thrown.",
            "problem_location": {
                "files": [
                    "AppSchedulingInfo.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo"
                ],
                "methods": [
                    "getNextPendingAsk",
                    "getPendingAsk"
                ]
            },
            "possible_fix": "Modify the `getNextPendingAsk` method to include a check for an empty `schedulerKeys` collection before accessing its first key. For example:\n\npublic PendingAsk getNextPendingAsk() {\n    try {\n        readLock.lock();\n        if (schedulerKeys.isEmpty()) {\n            return PendingAsk.ZERO; // or handle as appropriate\n        }\n        SchedulerRequestKey firstRequestKey = schedulerKeys.first();\n        return getPendingAsk(firstRequestKey, ResourceRequest.ANY);\n    } finally {\n        readLock.unlock();\n    }\n}"
        },
        "possible_fix_code": {
            "getNextPendingAsk": "  public PendingAsk getNextPendingAsk() {\n    try {\n      readLock.lock();\n      if (schedulerKeys.isEmpty()) {\n        return PendingAsk.ZERO; // or handle as appropriate\n      }\n      SchedulerRequestKey firstRequestKey = schedulerKeys.first();\n      return getPendingAsk(firstRequestKey, ResourceRequest.ANY);\n    } finally {\n      readLock.unlock();\n    }\n  }"
        }
    },
    {
        "filename": "YARN-1094.json",
        "creation_time": "2013-08-23T19:06:17.000+0000",
        "bug_report": {
            "Title": "RM restart throws Null Pointer Exception in Secure Env",
            "Description": "When the Resource Manager (RM) restart feature is enabled, attempting to restart the Resource Manager while a job is running leads to a failure with a NullPointerException. The stack trace indicates that the exception occurs in the DelegationTokenRenewer class, specifically during the token renewal process. This issue may arise from uninitialized object references within the application recovery process.",
            "StackTrace": [
                "2013-08-23 17:57:40,705 INFO  resourcemanager.RMAppManager (RMAppManager.java:recover(370)) - Recovering application application_1377280618693_0001",
                "2013-08-23 17:57:40,763 ERROR resourcemanager.ResourceManager (ResourceManager.java:serviceStart(617)) - Failed to load/recover state",
                "java.lang.NullPointerException",
                "        at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.setTimerForTokenRenewal(DelegationTokenRenewer.java:371)",
                "        at org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.addApplication(DelegationTokenRenewer.java:307)",
                "        at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.submitApplication(RMAppManager.java:291)",
                "        at org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.recover(RMAppManager.java:371)",
                "        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.recover(ResourceManager.java:819)",
                "        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart(ResourceManager.java:613)",
                "        at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)",
                "        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:832)",
                "2013-08-23 17:57:40,766 INFO  util.ExitUtil (ExitUtil.java:terminate(124)) - Exiting with status 1"
            ],
            "RootCause": "The NullPointerException is likely caused by an uninitialized object reference in the DelegationTokenRenewer class during the token renewal process, specifically in the methods setTimerForTokenRenewal and addApplication.",
            "StepsToReproduce": [
                "Enable the rmrestart feature.",
                "Start a job in the Resource Manager.",
                "Attempt to restart the Resource Manager."
            ],
            "ExpectedBehavior": "The Resource Manager should restart successfully without throwing any exceptions, allowing the job to continue running.",
            "ObservedBehavior": "The Resource Manager fails to start and throws a NullPointerException, preventing the recovery of the application state.",
            "Suggestions": "Investigate the initialization of objects in the DelegationTokenRenewer class, particularly in the methods related to token renewal. Ensure that all necessary objects are properly instantiated before use.",
            "problem_location": {
                "files": [
                    "RMAppManager.java",
                    "ResourceManager.java",
                    "DelegationTokenRenewer.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.RMAppManager",
                    "org.apache.hadoop.yarn.server.resourcemanager.ResourceManager",
                    "org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer"
                ],
                "methods": [
                    "RMAppManager.recover",
                    "ResourceManager.serviceStart",
                    "DelegationTokenRenewer.setTimerForTokenRenewal",
                    "DelegationTokenRenewer.addApplication"
                ]
            },
            "possible_fix": "Review the DelegationTokenRenewer class to ensure that all necessary fields are initialized before they are accessed in the setTimerForTokenRenewal and addApplication methods. Consider adding null checks or initializing objects to prevent the NullPointerException."
        },
        "possible_fix_code": {
            "DelegationTokenRenewer.addApplication": "  public void addApplication(ApplicationId applicationId, Credentials credentials, boolean cancelTokensWhenComplete) throws IOException {\n    if (applicationId == null) {\n        throw new IllegalArgumentException(\"ApplicationId cannot be null\");\n    }\n    if (credentials == null) {\n        throw new IllegalArgumentException(\"Credentials cannot be null\");\n    }\n\n    // Initialize the renewal timer if it hasn't been initialized\n    if (this.tokenRenewalTimer == null) {\n        this.tokenRenewalTimer = new Timer();\n    }\n\n    // Add the application to the renewer\n    this.applications.put(applicationId, new ApplicationData(credentials, cancelTokensWhenComplete));\n    setTimerForTokenRenewal(applicationId);\n  }"
        }
    },
    {
        "filename": "YARN-7269.json",
        "creation_time": "2017-09-28T23:56:42.000+0000",
        "bug_report": {
            "Title": "Tracking URL in the app state does not get redirected to ApplicationMaster for Running applications",
            "Description": "The application fails to redirect the tracking URL to the ApplicationMaster for running applications, resulting in a ServletException. The error indicates that the system could not determine the proxy server for redirection, originating from the `AmIpFilter` class in the Hadoop YARN server's web proxy. This issue may stem from misconfigured proxy settings or missing proxy-related classes in the codebase.",
            "StackTrace": [
                "org.mortbay.log: /ws/v1/mapreduce/info",
                "javax.servlet.ServletException: Could not determine the proxy server for redirection",
                "at org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.findRedirectUrl(AmIpFilter.java:199)",
                "at org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.doFilter(AmIpFilter.java:141)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "at org.apache.hadoop.http.HttpServer2$QuotingInputFilter.doFilter(HttpServer2.java:1426)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "at org.apache.hadoop.http.NoCacheFilter.doFilter(ServletHandler.java:45)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)",
                "at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)",
                "at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)",
                "at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)",
                "at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)",
                "at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)",
                "at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)",
                "at org.mortbay.jetty.Server.handle(Server.java:326)",
                "at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)",
                "at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)",
                "at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)",
                "at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)",
                "at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)",
                "at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410)",
                "at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)"
            ],
            "RootCause": "The root cause of the issue is the inability of the `AmIpFilter` class to determine the proxy server for redirection, likely due to misconfigured proxy settings or missing configuration classes in the Hadoop YARN codebase.",
            "StepsToReproduce": [
                "1. Start a Hadoop YARN application.",
                "2. Attempt to access the tracking URL for the running application.",
                "3. Observe the resulting ServletException indicating the proxy server could not be determined."
            ],
            "ExpectedBehavior": "The tracking URL should successfully redirect to the ApplicationMaster without any exceptions, allowing users to view the application status.",
            "ObservedBehavior": "The application throws a ServletException stating that it could not determine the proxy server for redirection, preventing access to the ApplicationMaster.",
            "Suggestions": "Check the proxy configuration settings in Hadoop YARN. Ensure that the necessary proxy-related classes are present and correctly configured. Review the documentation for best practices on setting up proxy servers in Hadoop YARN.",
            "problem_location": {
                "files": [
                    "AmIpFilter.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter"
                ],
                "methods": [
                    "findRedirectUrl",
                    "doFilter"
                ]
            },
            "possible_fix": "Investigate the proxy configuration in the Hadoop YARN settings. If the `AmIpFilter` class is missing, consider implementing it or ensuring that the necessary dependencies are included in the project."
        },
        "possible_fix_code": {
            "findRedirectUrl": "  public String findRedirectUrl() throws ServletException {\n    String addr = null;\n    YarnConfiguration conf = new YarnConfiguration();\n    if (proxyUriBases.size() == 1) {\n      // external proxy or not RM HA\n      addr = proxyUriBases.values().iterator().next();\n    } else {\n      // RM HA\n      for (String rmId : getRmIds(conf)) {\n        String url = getUrlByRmId(conf, rmId);\n        if (isValidUrl(url)) {\n          addr = url;\n          break;\n        }\n      }\n    }\n\n    if (addr == null) {\n      throw new ServletException(\n          \"Could not determine the proxy server for redirection. Please check your proxy configuration settings.\");\n    }\n    return addr;\n  }"
        }
    },
    {
        "filename": "YARN-7249.json",
        "creation_time": "2017-09-25T16:49:46.000+0000",
        "bug_report": {
            "Title": "Fix CapacityScheduler NPE issue when a container is preempted while the node is being removed",
            "Description": "This bug occurs under specific conditions where a node is being removed from the scheduler while a container running on that node is being preempted. A race condition can lead to the scheduler passing a null node to the leaf queue, resulting in a NullPointerException (NPE). The stack trace indicates that the NPE is thrown in the `completedContainer` method of the `LeafQueue` class, which is part of the Apache Hadoop YARN resource manager's capacity scheduler. The proposed fix involves adding a null node check within the `CapacityScheduler` to prevent this exception.",
            "StackTrace": [
                "2017-08-31 02:51:24,748 FATAL resourcemanager.ResourceManager (ResourceManager.java:run(714)) - Error in handling event type KILL_RESERVED_CONTAINER to the scheduler",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.completedContainer(LeafQueue.java:1308)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.completedContainerInternal(CapacityScheduler.java:1469)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler.completedContainer(AbstractYarnScheduler.java:497)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.killReservedContainer(CapacityScheduler.java:1505)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:1341)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:127)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:705)"
            ],
            "RootCause": "The root cause of the issue is a race condition that allows a null node to be passed to the `completedContainer` method in the `LeafQueue` class when a container is preempted while the node is being removed.",
            "StepsToReproduce": [
                "1. Start the YARN ResourceManager with a node that has running containers.",
                "2. Initiate the removal of the node from the scheduler.",
                "3. Trigger the preemption of a container running on that node.",
                "4. Observe the logs for a NullPointerException in the stack trace."
            ],
            "ExpectedBehavior": "The system should handle the preemption of containers gracefully without throwing a NullPointerException, even when a node is being removed.",
            "ObservedBehavior": "A NullPointerException is thrown in the `completedContainer` method of the `LeafQueue` class when a container is preempted while the node is being removed.",
            "Suggestions": "Add a null check for the node in the `CapacityScheduler` before passing it to the `completedContainer` method in the `LeafQueue` class.",
            "problem_location": {
                "files": [
                    "CapacityScheduler.java",
                    "LeafQueue.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler",
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue"
                ],
                "methods": [
                    "CapacityScheduler.completedContainerInternal",
                    "LeafQueue.completedContainer"
                ]
            },
            "possible_fix": "In the `CapacityScheduler` class, modify the `completedContainerInternal` method to include a null check for the node before proceeding with the completion logic."
        },
        "possible_fix_code": {
            "CapacityScheduler.completedContainerInternal": "  protected void completedContainerInternal(\n      RMContainer rmContainer, ContainerStatus containerStatus,\n      RMContainerEventType event) {\n    Container container = rmContainer.getContainer();\n    ContainerId containerId = container.getId();\n\n    // Get the application for the finished container\n    FiCaSchedulerApp application = getCurrentAttemptForContainer(\n        container.getId());\n    ApplicationId appId =\n        containerId.getApplicationAttemptId().getApplicationId();\n    if (application == null) {\n      LOG.info(\n          \"Container \" + container + \" of\" + \" finished application \" + appId\n              + \" completed with event \" + event);\n      return;\n    }\n\n    // Get the node on which the container was allocated\n    FiCaSchedulerNode node = getNode(container.getNodeId());\n    if (node == null) {\n      LOG.info(\"Container \" + container + \" of\" + \" removed node \" + container\n          .getNodeId() + \" completed with event \" + event);\n      return;\n    }\n\n    // Check for null node before proceeding\n    if (node == null) {\n      LOG.warn(\"Node is null for container: \" + containerId);\n      return;\n    }\n\n    // Inform the queue\n    LeafQueue queue = (LeafQueue) application.getQueue();\n    queue.completedContainer(getClusterResource(), application, node,\n        rmContainer, containerStatus, event, null, true);\n  }"
        }
    },
    {
        "filename": "YARN-4598.json",
        "creation_time": "2016-01-15T06:48:48.000+0000",
        "bug_report": {
            "Title": "Invalid event: RESOURCE_FAILED at CONTAINER_CLEANEDUP_AFTER_KILL",
            "Description": "In our cluster, an issue has been identified where the container is unable to handle the event 'RESOURCE_FAILED' while in the state 'CONTAINER_CLEANEDUP_AFTER_KILL'. The logs indicate that the container transitioned from 'CONTAINER_CLEANEDUP_AFTER_KILL' to 'DONE', but it cannot process the 'RESOURCE_FAILED' event in this state, leading to an 'InvalidStateTransitionException'. This suggests a flaw in the state transition logic within the YARN framework.",
            "StackTrace": [
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: RESOURCE_FAILED at CONTAINER_CLEANEDUP_AFTER_KILL",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl.handle(ContainerImpl.java:1127)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl.handle(ContainerImpl.java:83)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ContainerEventDispatcher.handle(ContainerManagerImpl.java:1078)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:175)",
                "at java.lang.Thread.run(Thread.java:744)"
            ],
            "RootCause": "The root cause of the issue is that the 'RESOURCE_FAILED' event is not a valid event for the 'CONTAINER_CLEANEDUP_AFTER_KILL' state in the YARN container state machine. The state machine does not allow this transition, resulting in an 'InvalidStateTransitionException'.",
            "StepsToReproduce": [
                "1. Start a container in the YARN framework.",
                "2. Transition the container to the 'CONTAINER_CLEANEDUP_AFTER_KILL' state.",
                "3. Attempt to send a 'RESOURCE_FAILED' event to the container."
            ],
            "ExpectedBehavior": "The system should not process the 'RESOURCE_FAILED' event when the container is in the 'CONTAINER_CLEANEDUP_AFTER_KILL' state, and it should handle only valid events for that state.",
            "ObservedBehavior": "The system throws an 'InvalidStateTransitionException' when trying to process the 'RESOURCE_FAILED' event while the container is in the 'CONTAINER_CLEANEDUP_AFTER_KILL' state.",
            "Suggestions": "Review the state transition logic in the YARN framework to ensure that invalid events are not processed in inappropriate states. Consider adding checks to prevent the dispatching of invalid events.",
            "problem_location": {
                "files": [
                    "ContainerImpl.java",
                    "StateMachineFactory.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl",
                    "org.apache.hadoop.yarn.state.StateMachineFactory"
                ],
                "methods": [
                    "ContainerImpl.handle",
                    "StateMachineFactory.doTransition"
                ]
            },
            "possible_fix": "Modify the event handling logic in the 'ContainerImpl' class to prevent the processing of 'RESOURCE_FAILED' events when in the 'CONTAINER_CLEANEDUP_AFTER_KILL' state. This may involve adding a condition to check the current state before handling the event."
        },
        "possible_fix_code": {
            "ContainerImpl.handle": "  @Override\n  public void handle(ContainerEvent event) {\n    try {\n      this.writeLock.lock();\n\n      ContainerId containerID = event.getContainerID();\n      LOG.debug(\"Processing \" + containerID + \" of type \" + event.getType());\n\n      ContainerState oldState = stateMachine.getCurrentState();\n      ContainerState newState = null;\n\n      // Check if the event is RESOURCE_FAILED and the current state is CONTAINER_CLEANEDUP_AFTER_KILL\n      if (event.getType() == ContainerEventType.RESOURCE_FAILED && oldState == ContainerState.CONTAINER_CLEANEDUP_AFTER_KILL) {\n        LOG.warn(\"Ignoring RESOURCE_FAILED event for container \" + containerID + \" in state \" + oldState);\n        return; // Ignore the event\n      }\n\n      try {\n        newState =\n            stateMachine.doTransition(event.getType(), event);\n      } catch (InvalidStateTransitionException e) {\n        LOG.warn(\"Can't handle this event at current state: Current: [\"\n            + oldState + \"], eventType: [\" + event.getType() + \"]\", e);\n      }\n      if (oldState != newState) {\n        LOG.info(\"Container \" + containerID + \" transitioned from \"\n            + oldState\n            + \" to \" + newState);\n      }\n    } finally {\n      this.writeLock.unlock();\n    }\n  }"
        }
    },
    {
        "filename": "YARN-1149.json",
        "creation_time": "2013-09-04T21:46:58.000+0000",
        "bug_report": {
            "Title": "NM throws InvalidStateTransitonException: Invalid event: APPLICATION_LOG_HANDLING_FINISHED at RUNNING",
            "Description": "The NodeManager (NM) encounters an InvalidStateTransitionException when it receives a kill signal for an application that has completed execution, but the log aggregation process has not yet started. The exception indicates that the event 'APPLICATION_LOG_HANDLING_FINISHED' is not valid while the application is still in the 'RUNNING' state. This suggests a flaw in the state management logic or event handling within the YARN framework.",
            "StackTrace": [
                "2013-08-25 20:45:00,875 INFO  logaggregation.AppLogAggregatorImpl (AppLogAggregatorImpl.java:finishLogAggregation(254)) - Application just finished : application_1377459190746_0118",
                "2013-08-25 20:45:00,876 INFO  logaggregation.AppLogAggregatorImpl (AppLogAggregatorImpl.java:uploadLogsForContainer(105)) - Starting aggregate log-file for app application_1377459190746_0118 at /app-logs/foo/logs/application_1377459190746_0118/<host>_45454.tmp",
                "2013-08-25 20:45:00,876 INFO  logaggregation.LogAggregationService (LogAggregationService.java:stopAggregators(151)) - Waiting for aggregation to complete for application_1377459190746_0118",
                "2013-08-25 20:45:00,891 INFO  logaggregation.AppLogAggregatorImpl (AppLogAggregatorImpl.java:uploadLogsForContainer(122)) - Uploading logs for container container_1377459190746_0118_01_000004. Current good log dirs are /tmp/yarn/local",
                "2013-08-25 20:45:00,915 INFO  logaggregation.AppLogAggregatorImpl (AppLogAggregatorImpl.java:doAppLogAggregation(182)) - Finished aggregate log-file for app application_1377459190746_0118",
                "2013-08-25 20:45:00,925 WARN  application.Application (ApplicationImpl.java:handle(427)) - Can't handle this event at current state",
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: APPLICATION_LOG_HANDLING_FINISHED at RUNNING",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.handle(ApplicationImpl.java:425)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.handle(ApplicationImpl.java:59)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher.handle(ContainerManagerImpl.java:697)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher.handle(ContainerManagerImpl.java:689)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:134)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:81)",
                "at java.lang.Thread.run(Thread.java:662)",
                "2013-08-25 20:45:00,926 INFO  application.Application (ApplicationImpl.java:handle(430)) - Application application_1377459190746_0118 transitioned from RUNNING to null",
                "2013-08-25 20:45:00,927 WARN  monitor.ContainersMonitorImpl (ContainersMonitorImpl.java:run(463)) - org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl is interrupted. Exiting.",
                "2013-08-25 20:45:00,938 INFO  ipc.Server (Server.java:stop(2437)) - Stopping server on 8040"
            ],
            "RootCause": "The root cause of the issue is that the application is attempting to process the 'APPLICATION_LOG_HANDLING_FINISHED' event while still in the 'RUNNING' state, which is not a valid transition according to the state management logic of the YARN framework.",
            "StepsToReproduce": [
                "1. Start an application in YARN.",
                "2. Send a kill signal to the application before log aggregation has started.",
                "3. Observe the logs for the InvalidStateTransitionException."
            ],
            "ExpectedBehavior": "The application should handle the log aggregation process correctly and transition to a valid state without throwing an InvalidStateTransitionException.",
            "ObservedBehavior": "An InvalidStateTransitionException is thrown when the application receives the 'APPLICATION_LOG_HANDLING_FINISHED' event while still in the 'RUNNING' state.",
            "Suggestions": "Review the state management logic in the YARN framework to ensure that log handling events are only processed when the application is in an appropriate state, such as 'FINISHED' or 'FAILED'.",
            "problem_location": {
                "files": [
                    "AppLogAggregatorImpl.java",
                    "ApplicationImpl.java",
                    "ContainerManagerImpl.java",
                    "StateMachineFactory.java"
                ],
                "classes": [
                    "logaggregation.AppLogAggregatorImpl",
                    "application.ApplicationImpl",
                    "org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl",
                    "org.apache.hadoop.yarn.state.StateMachineFactory"
                ],
                "methods": [
                    "AppLogAggregatorImpl.finishLogAggregation",
                    "ApplicationImpl.handle",
                    "ContainerManagerImpl$ApplicationEventDispatcher.handle",
                    "StateMachineFactory.doTransition"
                ]
            },
            "possible_fix": "Investigate the state transition definitions in the YARN framework and modify the event handling logic to prevent the 'APPLICATION_LOG_HANDLING_FINISHED' event from being processed while the application is in the 'RUNNING' state."
        },
        "possible_fix_code": {
            "ApplicationImpl.handle": "  public void handle(ApplicationEvent event) {\n\n    this.writeLock.lock();\n\n    try {\n      ApplicationId applicationID = event.getApplicationID();\n      LOG.debug(\"Processing \" + applicationID + \" of type \" + event.getType());\n\n      ApplicationState oldState = stateMachine.getCurrentState();\n      ApplicationState newState = null;\n      try {\n        // Prevent processing APPLICATION_LOG_HANDLING_FINISHED event if in RUNNING state\n        if (event.getType() == ApplicationEventType.APPLICATION_LOG_HANDLING_FINISHED && oldState == ApplicationState.RUNNING) {\n          LOG.warn(\"Ignoring APPLICATION_LOG_HANDLING_FINISHED event in RUNNING state\");\n          return;\n        }\n        // queue event requesting init of the same app\n        newState = stateMachine.doTransition(event.getType(), event);\n      } catch (InvalidStateTransitonException e) {\n        LOG.warn(\"Can't handle this event at current state\", e);\n      }\n      if (oldState != newState) {\n        LOG.info(\"Application \" + applicationID + \" transitioned from \" + oldState + \" to \" + newState);\n      }\n    } finally {\n      this.writeLock.unlock();\n    }\n  }"
        }
    },
    {
        "filename": "YARN-7818.json",
        "creation_time": "2018-01-25T18:42:55.000+0000",
        "bug_report": {
            "Title": "Remove privileged operation warnings during container launch for the ContainerRuntimes",
            "Description": "The issue arises when launching containers in the Hadoop YARN NodeManager, specifically during the execution of privileged operations. The error occurs when the application attempts to restart the NodeManager (NM) while containers are still running, leading to a failure in launching new containers. The exit code 143 indicates that the process was terminated by a SIGTERM signal, which can be attributed to resource constraints or improper handling of container states during the restart process.",
            "StackTrace": [
                "2018-01-24 09:48:30,547 INFO  container.ContainerImpl (ContainerImpl.java:handle(2108)) - Container container_e04_1516787230461_0001_01_000003 transitioned from RUNNING to KILLING",
                "2018-01-24 09:48:30,547 INFO  launcher.ContainerLaunch (ContainerLaunch.java:cleanupContainer(668)) - Cleaning up container container_e04_1516787230461_0001_01_000003",
                "2018-01-24 09:48:30,552 WARN  privileged.PrivilegedOperationExecutor (PrivilegedOperationExecutor.java:executePrivilegedOperation(174)) - Shell execution returned exit code: 143.",
                "2018-01-24 09:48:30,553 WARN  runtime.DefaultLinuxContainerRuntime (DefaultLinuxContainerRuntime.java:launchContainer(127)) - Launch container failed. Exception: org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationException: ExitCodeException exitCode=143:",
                "2018-01-24 09:48:30,553 WARN  nodemanager.LinuxContainerExecutor (LinuxContainerExecutor.java:launchContainer(557)) - Exit code from container container_e04_1516787230461_0001_01_000003 is : 143"
            ],
            "RootCause": "The root cause of the issue is the failure of the container launch process due to a SIGTERM signal, which is likely triggered by resource constraints on the node or improper handling of container states during the restart of the NodeManager.",
            "StepsToReproduce": [
                "Run the Dshell Application using the provided command.",
                "Identify the host where the Application Master (AM) is running.",
                "Locate the containers launched by the application.",
                "Restart the NodeManager where the AM is running.",
                "Validate that the new attempt is not started and that the previously launched containers are in RUNNING state."
            ],
            "ExpectedBehavior": "The containers should remain in the RUNNING state after the NodeManager is restarted, and new attempts should be initiated without errors.",
            "ObservedBehavior": "The containers fail to launch with exit code 143, indicating that the process was terminated unexpectedly.",
            "Suggestions": "Investigate the resource utilization on the node to ensure there are sufficient resources available for launching containers. Additionally, review the handling of container states during NodeManager restarts to prevent premature termination of running containers.",
            "problem_location": {
                "files": [
                    "ContainerImpl.java",
                    "ContainerLaunch.java",
                    "PrivilegedOperationExecutor.java",
                    "DefaultLinuxContainerRuntime.java",
                    "LinuxContainerExecutor.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerImpl",
                    "org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DefaultLinuxContainerRuntime",
                    "org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationExecutor",
                    "org.apache.hadoop.yarn.server.nodemanager.containermanager.LinuxContainerExecutor"
                ],
                "methods": [
                    "ContainerImpl.handle",
                    "ContainerLaunch.cleanupContainer",
                    "PrivilegedOperationExecutor.executePrivilegedOperation",
                    "DefaultLinuxContainerRuntime.launchContainer",
                    "LinuxContainerExecutor.launchContainer"
                ]
            },
            "possible_fix": "Ensure that the NodeManager has adequate resources allocated and modify the container state management logic to handle restarts more gracefully, preventing the termination of running containers."
        },
        "possible_fix_code": {
            "DefaultLinuxContainerRuntime.launchContainer": "  public void launchContainer(ContainerRuntimeContext ctx)\n      throws ContainerExecutionException {\n    // Check resource utilization before launching the container\n    NodeResourceMonitor resourceMonitor = ctx.getNodeResourceMonitor();\n    ResourceUtilization utilization = resourceMonitor.getUtilization();\n\n    // Ensure there are sufficient resources available\n    if (utilization.getPhysicalMemory() < REQUIRED_MEMORY || utilization.getVirtualMemory() < REQUIRED_VIRTUAL_MEMORY || utilization.getCPU() < REQUIRED_CPU) {\n        throw new ContainerExecutionException(\"Insufficient resources to launch container\");\n    }\n\n    PrivilegedOperation launchOp = new PrivilegedOperation(\n        PrivilegedOperation.OperationType.LAUNCH_CONTAINER);\n\n    // All of these arguments are expected to be available in the runtime context\n    launchOp.appendArgs(ctx.getExecutionAttribute(RUN_AS_USER),\n        ctx.getExecutionAttribute(USER),\n        Integer.toString(PrivilegedOperation.\n            RunAsUserCommand.LAUNCH_CONTAINER.getValue()),\n        ctx.getExecutionAttribute(APPID),\n        ctx.getExecutionAttribute(CONTAINER_ID_STR),\n        ctx.getExecutionAttribute(CONTAINER_WORK_DIR).toString(),\n        ctx.getExecutionAttribute(NM_PRIVATE_CONTAINER_SCRIPT_PATH).toUri()\n            .getPath(),\n        ctx.getExecutionAttribute(NM_PRIVATE_TOKENS_PATH).toUri().getPath(),\n        ctx.getExecutionAttribute(PID_FILE_PATH).toString(),\n        StringUtils.join(PrivilegedOperation.LINUX_FILE_PATH_SEPARATOR,\n            ctx.getExecutionAttribute(LOCAL_DIRS)),\n        StringUtils.join(PrivilegedOperation.LINUX_FILE_PATH_SEPARATOR,\n            ctx.getExecutionAttribute(LOG_DIRS)),\n        ctx.getExecutionAttribute(RESOURCES_OPTIONS));\n\n    String tcCommandFile = ctx.getExecutionAttribute(TC_COMMAND_FILE);\n\n    if (tcCommandFile != null) {\n      launchOp.appendArgs(tcCommandFile);\n    }\n\n    // List<String> -> stored as List -> fetched/converted to List<String>\n    // we can't do better here thanks to type-erasure\n    @SuppressWarnings(\"unchecked\")\n    List<String> prefixCommands = (List<String>) ctx.getExecutionAttribute(\n        CONTAINER_LAUNCH_PREFIX_COMMANDS);\n\n    try {\n      privilegedOperationExecutor.executePrivilegedOperation(prefixCommands,\n            launchOp, null, null, false, false);\n    } catch (PrivilegedOperationException e) {\n      LOG.warn(\"Launch container failed. Exception: \", e);\n\n      throw new ContainerExecutionException(\"Launch container failed\", e\n          .getExitCode(), e.getOutput(), e.getErrorOutput());\n    }\n  }"
        }
    }
]