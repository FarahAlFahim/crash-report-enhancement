[
    {
        "filename": "STORM-2443.json",
        "creation_time": "2017-03-31T08:09:04.000+0000",
        "bug_report": {
            "Title": "Nimbus throws error when changing log level on UI topology page",
            "Description": "When attempting to change the log level on the UI topology page, Nimbus encounters a NullPointerException. This occurs specifically in the setLogConfig method of the Nimbus class, indicating that a required object is not initialized or is null when the method is called. The stack trace shows that the error originates from Nimbus.java at line 2688, which suggests that the method is trying to access a property or method of a null object.",
            "StackTrace": [
                "2017-03-30 16:53:26.954 o.a.s.d.n.Nimbus pool-14-thread-56 [WARN] set log config topology exception. (topology id='rolling-1-1490860365')",
                "java.lang.NullPointerException: null",
                "at org.apache.storm.daemon.nimbus.Nimbus.setLogConfig(Nimbus.java:2688) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.generated.Nimbus$Processor$setLogConfig.getResult(Nimbus.java:3295) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.generated.Nimbus$Processor$setLogConfig.getResult(Nimbus.java:3280) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.thrift.ProcessFunction.process(ProcessFunction.java:39) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.thrift.TBaseProcessor.process(TBaseProcessor.java:39) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.security.auth.SimpleTransportPlugin$SimpleWrapProcessor.process(SimpleTransportPlugin.java:160) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.thrift.server.AbstractNonblockingServer$FrameBuffer.invoke(AbstractNonblockingServer.java:518) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.thrift.server.Invocation.run(Invocation.java:18) ~[storm-core-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) [?:1.8.0_66]",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [?:1.8.0_66]",
                "at java.lang.Thread.run(Thread.java:745) [?:1.8.0_66]"
            ],
            "RootCause": "The NullPointerException in the setLogConfig method indicates that a required object (likely related to the topology configuration or logging settings) is not initialized before being accessed.",
            "StepsToReproduce": [
                "1. Navigate to the UI topology page.",
                "2. Attempt to change the log level for a specific topology.",
                "3. Observe the error in the Nimbus logs."
            ],
            "ExpectedBehavior": "The log level should change successfully without any errors, and the new log level should be reflected in the system.",
            "ObservedBehavior": "A NullPointerException is thrown, preventing the log level from being changed.",
            "Suggestions": "Ensure that all necessary objects are properly initialized before calling the setLogConfig method. Add null checks to handle cases where the expected objects may not be available.",
            "problem_location": {
                "files": [
                    "Nimbus.java"
                ],
                "classes": [
                    "org.apache.storm.daemon.nimbus.Nimbus"
                ],
                "methods": [
                    "setLogConfig"
                ]
            },
            "possible_fix": "In the setLogConfig method, add checks to ensure that all required objects are not null before proceeding with the configuration changes. For example:\n\nif (topologyConfig == null) {\n    throw new IllegalArgumentException(\"Topology configuration cannot be null\");\n}\n\nThis will help prevent the NullPointerException and provide clearer error handling."
        }
    },
    {
        "filename": "STORM-3213.json",
        "creation_time": "2018-09-05T16:16:45.000+0000",
        "bug_report": {
            "Title": "500 Server Error on __acker component page on Storm UI",
            "Description": "A 500 Internal Server Error occurs when attempting to access the __acker component page in the Storm UI. The error is caused by a NullPointerException in the Nimbus.getComponentPageInfo method, which is triggered when the UIHelpers.getComponentPage method calls the Nimbus client to retrieve component page information. The root cause appears to be related to the ResourceUtils.getBoltResources method, which fails to handle cases where the topology does not contain the specified component.",
            "StackTrace": [
                "org.apache.storm.thrift.TApplicationException: Internal error processing getComponentPageInfo",
                "at org.apache.storm.thrift.TServiceClient.receiveBase(TServiceClient.java:79)",
                "at org.apache.storm.generated.Nimbus$Client.recv_getComponentPageInfo(Nimbus.java:1359)",
                "at org.apache.storm.generated.Nimbus$Client.getComponentPageInfo(Nimbus.java:1343)",
                "at org.apache.storm.daemon.ui.UIHelpers.getComponentPage(UIHelpers.java:1559)",
                "at org.apache.storm.daemon.ui.resources.StormApiResource.getTopologyComponent(StormApiResource.java:438)",
                "java.lang.RuntimeException: java.lang.NullPointerException",
                "at org.apache.storm.daemon.nimbus.Nimbus.getComponentPageInfo(Nimbus.java:4238)",
                "at org.apache.storm.generated.Nimbus$Processor$getComponentPageInfo.getResult(Nimbus.java:4577)",
                "at org.apache.storm.thrift.ProcessFunction.process(ProcessFunction.java:38)",
                "at org.apache.storm.thrift.TBaseProcessor.process(TBaseProcessor.java:39)",
                "at org.apache.storm.security.auth.SimpleTransportPlugin$SimpleWrapProcessor.process(SimpleTransportPlugin.java:169)",
                "at org.apache.storm.thrift.server.AbstractNonblockingServer$FrameBuffer.invoke(AbstractNonblockingServer.java:518)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "at java.lang.Thread.run(Thread.java:748)",
                "Caused by: java.lang.NullPointerException",
                "at org.apache.storm.scheduler.resource.ResourceUtils.getBoltResources(ResourceUtils.java:37)",
                "at org.apache.storm.daemon.nimbus.Nimbus.getComponentPageInfo(Nimbus.java:4192)"
            ],
            "RootCause": "The root cause of the issue is a NullPointerException in the ResourceUtils.getBoltResources method, which occurs when the topology does not contain the specified component, leading to a failure in retrieving the component's resources.",
            "StepsToReproduce": [
                "1. Access the Storm UI.",
                "2. Navigate to the __acker component page.",
                "3. Observe the 500 Internal Server Error."
            ],
            "ExpectedBehavior": "The expected behavior is to successfully retrieve and display the __acker component page without any errors.",
            "ObservedBehavior": "The observed behavior is a 500 Internal Server Error, indicating an internal processing issue when attempting to access the component page.",
            "Suggestions": "Implement null checks in the ResourceUtils.getBoltResources method to handle cases where the component does not exist in the topology. Additionally, ensure that the Nimbus.getComponentPageInfo method gracefully handles such scenarios.",
            "problem_location": {
                "files": [
                    "storm-webapp/src/main/java/org/apache/storm/daemon/ui/UIHelpers.java",
                    "storm-server/src/main/java/org/apache/storm/scheduler/resource/ResourceUtils.java",
                    "storm-webapp/src/main/java/org/apache/storm/daemon/ui/resources/StormApiResource.java"
                ],
                "classes": [
                    "org.apache.storm.daemon.ui.UIHelpers",
                    "org.apache.storm.scheduler.resource.ResourceUtils",
                    "org.apache.storm.daemon.nimbus.Nimbus",
                    "org.apache.storm.daemon.ui.resources.StormApiResource"
                ],
                "methods": [
                    "UIHelpers.getComponentPage",
                    "ResourceUtils.getBoltResources",
                    "Nimbus.getComponentPageInfo",
                    "StormApiResource.getTopologyComponent"
                ]
            },
            "possible_fix": "In the ResourceUtils.getBoltResources method, add a check to ensure that the topology contains the specified component before attempting to access its resources. For example:\n\n```java\nif (topology.get_bolts() != null && topology.get_bolts().containsKey(componentId)) {\n    Bolt bolt = topology.get_bolts().get(componentId);\n    return new NormalizedResourceRequest(bolt.get_common(), topologyConf, componentId);\n}\nreturn null;\n```"
        }
    },
    {
        "filename": "STORM-2496.json",
        "creation_time": "2017-04-28T08:17:47.000+0000",
        "bug_report": {
            "Title": "Dependency artifacts should be uploaded to blobstore with READ permission for all",
            "Description": "When submitting a topology with dependency artifacts, the artifacts are uploaded to the blobstore with permissions that may not allow other users to access them. This results in an AuthorizationException when a supervisor attempts to download the artifact, leading to a crash. The system should ensure that all uploaded artifacts have READ permissions for all users, especially in non-secured clusters.",
            "StackTrace": [
                "org.apache.storm.generated.AuthorizationException: null",
                "at org.apache.storm.localizer.Localizer.downloadBlob(Localizer.java:535)",
                "at org.apache.storm.localizer.Localizer.access$000(Localizer.java:65)",
                "at org.apache.storm.localizer.Localizer$DownloadBlob.call(Localizer.java:505)",
                "at org.apache.storm.localizer.Localizer$DownloadBlob.call(Localizer.java:481)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:266)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "at java.lang.Thread.run(Thread.java:745)",
                "java.util.concurrent.ExecutionException: AuthorizationException(msg:<user> does not have READ access to dep-org.apache.curator-curator-framework-jar-2.10.0.jar)",
                "at java.util.concurrent.FutureTask.report(FutureTask.java:122)",
                "at java.util.concurrent.FutureTask.get(FutureTask.java:206)",
                "at org.apache.storm.localizer.LocalDownloadedResource$NoCancelFuture.get(LocalDownloadedResource.java:63)",
                "at org.apache.storm.daemon.supervisor.Slot.handleWaitingForBlobLocalization(Slot.java:380)",
                "at org.apache.storm.daemon.supervisor.Slot.stateMachineStep(Slot.java:275)",
                "at org.apache.storm.daemon.supervisor.Slot.run(Slot.java:740)",
                "Caused by: org.apache.storm.generated.AuthorizationException",
                "at org.apache.storm.localizer.Localizer.downloadBlob(Localizer.java:535)",
                "at org.apache.storm.localizer.Localizer.access$000(Localizer.java:65)",
                "at org.apache.storm.localizer.Localizer$DownloadBlob.call(Localizer.java:505)",
                "at org.apache.storm.localizer.Localizer$DownloadBlob.call(Localizer.java:481)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:266)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "at java.lang.Thread.run(Thread.java:745)",
                "java.lang.RuntimeException: Halting process: Error when processing an event",
                "at org.apache.storm.utils.Utils.exitProcess(Utils.java:177)",
                "at org.apache.storm.daemon.supervisor.Slot.run(Slot.java:774)"
            ],
            "RootCause": "The root cause of the issue is that the uploaded artifacts do not have READ permissions for all users, which leads to an AuthorizationException when a supervisor tries to access them.",
            "StepsToReproduce": [
                "1. Submit a topology with dependency artifacts using a specific user.",
                "2. Attempt to access the uploaded artifacts from another user account.",
                "3. Observe the AuthorizationException and subsequent crash."
            ],
            "ExpectedBehavior": "All users should have READ access to the uploaded artifacts in the blobstore, allowing them to download and use the artifacts without encountering authorization errors.",
            "ObservedBehavior": "The supervisor fails to download the artifact due to lack of READ access, resulting in an AuthorizationException and crashing the process.",
            "Suggestions": "Ensure that when artifacts are uploaded to the blobstore, they are assigned READ permissions for all users. This can be done by modifying the upload logic to set appropriate permissions.",
            "problem_location": {
                "files": [
                    "storm-server/src/main/java/org/apache/storm/localizer/Localizer.java",
                    "storm-server/src/main/java/org/apache/storm/daemon/supervisor/Slot.java",
                    "storm-server/src/main/java/org/apache/storm/utils/Utils.java"
                ],
                "classes": [
                    "org.apache.storm.localizer.Localizer",
                    "org.apache.storm.daemon.supervisor.Slot",
                    "org.apache.storm.utils.Utils"
                ],
                "methods": [
                    "Localizer.downloadBlob",
                    "Slot.handleWaitingForBlobLocalization",
                    "Utils.exitProcess"
                ]
            },
            "possible_fix": "Modify the 'downloadBlob' method in the Localizer class to ensure that the uploaded artifacts are assigned READ permissions for all users. This can be achieved by updating the 'setBlobPermissions' method to include a parameter that specifies global READ access."
        }
    },
    {
        "filename": "STORM-2879.json",
        "creation_time": "2018-01-03T07:07:49.000+0000",
        "bug_report": {
            "Title": "Supervisor collapses continuously due to expired assignment for overdue storm",
            "Description": "The supervisor fails to recover from an exception caused by the deletion of files related to an overdue storm assignment. When a topology is reassigned or killed, the supervisor attempts to delete four files: storm-code, storm-ser, storm-jar, and LocalAssignment. If an exception occurs during this deletion process, the local assignment remains on disk. Upon supervisor restart, it attempts to recover the container from LocalAssignments, leading to a KeyNotFoundException when it tries to fetch the files from the blob store, causing a continuous collapse of the supervisor.",
            "StackTrace": [
                "org.apache.storm.generated.KeyNotFoundException: null",
                "at org.apache.storm.generated.Nimbus$beginBlobDownload_result$beginBlobDownload_resultStandardScheme.read(Nimbus.java:26656) ~[storm-core-1.1.2-mt001.jar:?]",
                "at org.apache.storm.generated.Nimbus$beginBlobDownload_result$beginBlobDownload_resultStandardScheme.read(Nimbus.java:26624) ~[storm-core-1.1.2-mt001.jar:?]",
                "at org.apache.storm.generated.Nimbus$beginBlobDownload_result.read(Nimbus.java:26555) ~[storm-core-1.1.2-mt001.jar:?]",
                "at org.apache.storm.thrift.TServiceClient.receiveBase(TServiceClient.java:86) ~[storm-core-1.1.2-mt001.jar:?]",
                "at org.apache.storm.generated.Nimbus$Client.recv_beginBlobDownload(Nimbus.java:864) ~[storm-core-1.1.2-mt001.jar:?]",
                "at org.apache.storm.generated.Nimbus$Client.beginBlobDownload(Nimbus.java:851) ~[storm-core-1.1.2-mt001.jar:?]",
                "at org.apache.storm.blobstore.NimbusBlobStore.getBlob(NimbusBlobStore.java:357) ~[storm-core-1.1.2-mt001.jar:?]",
                "at org.apache.storm.utils.Utils.downloadResourcesAsSupervisorAttempt(Utils.java:598) ~[storm-core-1.1.2-mt001.jar:?]",
                "at org.apache.storm.utils.Utils.downloadResourcesAsSupervisorImpl(Utils.java:582) ~[storm-core-1.1.2-mt001.jar:?]",
                "at org.apache.storm.utils.Utils.downloadResourcesAsSupervisor(Utils.java:574) ~[storm-core-1.1.2-mt001.jar:?]"
            ],
            "RootCause": "The root cause of the issue is the lack of a transactional mechanism during the deletion of storm-related files. If an exception occurs during the deletion process, the local assignment remains, leading to a KeyNotFoundException when the supervisor attempts to recover.",
            "StepsToReproduce": [
                "1. Create an overdue storm assignment.",
                "2. Reassign or kill the topology for the cluster.",
                "3. Observe the deletion of storm-related files.",
                "4. Restart the supervisor after an exception occurs during file deletion."
            ],
            "ExpectedBehavior": "The supervisor should successfully delete all related files and recover without encountering a KeyNotFoundException.",
            "ObservedBehavior": "The supervisor continuously collapses due to a KeyNotFoundException when trying to recover from an expired local assignment.",
            "Suggestions": "Implement a transactional mechanism for file deletion to ensure that if an exception occurs, the local assignment is also cleaned up. Additionally, add error handling to manage the recovery process more gracefully.",
            "problem_location": {
                "files": [
                    "Slot.java",
                    "ConfigUtils.java",
                    "NimbusBlobStore.java"
                ],
                "classes": [
                    "org.apache.storm.d.s.Slot",
                    "org.apache.storm.utils.ConfigUtils",
                    "org.apache.storm.blobstore.NimbusBlobStore"
                ],
                "methods": [
                    "Slot.cleanupCurrentContainer",
                    "ConfigUtils.readSupervisorStormConfGivenPath",
                    "NimbusBlobStore.getBlob"
                ]
            },
            "possible_fix": "Add a transactional approach to the cleanup process in Slot.java's cleanupCurrentContainer method. Ensure that if an exception occurs during file deletion, the local assignment is also removed to prevent recovery issues."
        }
    },
    {
        "filename": "STORM-3012.json",
        "creation_time": "2018-03-27T15:30:32.000+0000",
        "bug_report": {
            "Title": "Nimbus crashes with NPE when pacemaker is restarted",
            "Description": "The Nimbus service crashes with a NullPointerException (NPE) when the pacemaker is restarted. This occurs due to a failure in handling responses from the PacemakerClient, specifically when a null response is returned after a connection loss. The NPE is triggered when the Nimbus attempts to process this null response, leading to a crash.",
            "StackTrace": [
                "org.apache.storm.pacemaker.PacemakerConnectionException: Timed out waiting for channel ready.",
                "at org.apache.storm.pacemaker.PacemakerClient.waitUntilReady(PacemakerClient.java:213)",
                "at org.apache.storm.pacemaker.PacemakerClient.send(PacemakerClient.java:182)",
                "at org.apache.storm.pacemaker.PacemakerClientPool.sendAll(PacemakerClientPool.java:65)",
                "at org.apache.storm.cluster.PaceMakerStateStorage.get_worker_hb_children(PaceMakerStateStorage.java:193)",
                "at org.apache.storm.cluster.StormClusterStateImpl.heartbeatStorms(StormClusterStateImpl.java:408)",
                "at org.apache.storm.daemon.nimbus.Nimbus.topoIdsToClean(Nimbus.java:765)",
                "at org.apache.storm.daemon.nimbus.Nimbus.doCleanup(Nimbus.java:2148)",
                "at org.apache.storm.daemon.nimbus.Nimbus.lambda$launchServer$36(Nimbus.java:2506)",
                "at org.apache.storm.StormTimer$1.run(StormTimer.java:207)",
                "at org.apache.storm.StormTimer$StormTimerTask.run(StormTimer.java:81)"
            ],
            "RootCause": "The root cause of the issue is that the PacemakerClient's send method can return a null response when the connection is lost and subsequently reconnected. This null response is not handled properly in the get_worker_hb_children method, leading to a NullPointerException when Nimbus tries to access properties of the null response.",
            "StepsToReproduce": [
                "1. Start the Nimbus service.",
                "2. Restart the pacemaker service.",
                "3. Observe the Nimbus logs for a crash due to NPE."
            ],
            "ExpectedBehavior": "Nimbus should handle the reconnection gracefully and not crash when the pacemaker is restarted.",
            "ObservedBehavior": "Nimbus crashes with a NullPointerException when the pacemaker is restarted.",
            "Suggestions": "Implement null checks for the responses received from the PacemakerClient to prevent NPE. Additionally, ensure that the system can handle reconnections without crashing.",
            "problem_location": {
                "files": [
                    "storm-client/src/jvm/org/apache/storm/pacemaker/PacemakerClient.java",
                    "storm-client/src/jvm/org/apache/storm/pacemaker/PacemakerClientPool.java",
                    "storm-client/src/jvm/org/apache/storm/cluster/PaceMakerStateStorage.java",
                    "storm-client/src/jvm/org/apache/storm/cluster/StormClusterStateImpl.java",
                    "storm-server/src/jvm/org/apache/storm/daemon/nimbus/Nimbus.java"
                ],
                "classes": [
                    "org.apache.storm.pacemaker.PacemakerClient",
                    "org.apache.storm.pacemaker.PacemakerClientPool",
                    "org.apache.storm.cluster.PaceMakerStateStorage",
                    "org.apache.storm.cluster.StormClusterStateImpl",
                    "org.apache.storm.daemon.nimbus.Nimbus"
                ],
                "methods": [
                    "PacemakerClient.send",
                    "PacemakerClientPool.sendAll",
                    "PaceMakerStateStorage.get_worker_hb_children",
                    "StormClusterStateImpl.heartbeatStorms",
                    "Nimbus.topoIdsToClean",
                    "Nimbus.doCleanup"
                ]
            },
            "possible_fix": "In the send method of PacemakerClient, check if the response is null before proceeding to use it. If it is null, handle it appropriately (e.g., log an error and return an empty response or throw a specific exception). Additionally, in the get_worker_hb_children method, add checks to ensure that the response is valid before accessing its properties."
        }
    },
    {
        "filename": "STORM-3073.json",
        "creation_time": "2018-05-15T11:12:21.000+0000",
        "bug_report": {
            "Title": "In some cases workers may crash because pendingEmits is full",
            "Description": "The issue arises when the executor's pendingEmits queue becomes full, leading to a crash when attempting to add another tuple. This situation can occur if the topology emits too many tuples in a single call or if there are too many metrics ticks between flushes of the pendingEmits queue. The LoadSpout's fail method can also trigger re-emission of failed tuples, compounding the issue if the queue is already near capacity.",
            "StackTrace": [
                "2018-05-15 11:35:28.365 o.a.s.u.Utils Thread-16-spout-executor[8, 8] [ERROR] Async loop died!",
                "java.lang.RuntimeException: java.lang.IllegalStateException: Queue full",
                "at org.apache.storm.executor.Executor.accept(Executor.java:282) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.utils.JCQueue.consumeImpl(JCQueue.java:133) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.utils.JCQueue.consume(JCQueue.java:110) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.utils.JCQueue.consume(JCQueue.java:101) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.executor.spout.SpoutExecutor$2.call(SpoutExecutor.java:168) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.executor.spout.SpoutExecutor$2.call(SpoutExecutor.java:157) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.utils.Utils$2.run(Utils.java:349) [storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at java.lang.Thread.run(Thread.java:748) [?:1.8.0_144]",
                "Caused by: java.lang.IllegalStateException: Queue full",
                "at java.util.AbstractQueue.add(AbstractQueue.java:98) ~[?:1.8.0_144]",
                "at org.apache.storm.daemon.worker.WorkerTransfer.tryTransferRemote(WorkerTransfer.java:113) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.daemon.worker.WorkerState.tryTransferRemote(WorkerState.java:516) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.executor.ExecutorTransfer.tryTransfer(ExecutorTransfer.java:66) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.executor.spout.SpoutOutputCollectorImpl.sendSpoutMsg(SpoutOutputCollectorImpl.java:140) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.executor.spout.SpoutOutputCollectorImpl.emit(SpoutOutputCollectorImpl.java:70) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.spout.SpoutOutputCollector.emit(SpoutOutputCollector.java:42) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.loadgen.LoadSpout.fail(LoadSpout.java:135) ~[stormjar.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.executor.spout.SpoutExecutor.failSpoutMsg(SpoutExecutor.java:360) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.executor.spout.SpoutExecutor$1.expire(SpoutExecutor.java:120) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.executor.spout.SpoutExecutor$1.expire(SpoutExecutor.java:113) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.utils.RotatingMap.rotate(RotatingMap.java:63) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.executor.spout.SpoutExecutor.tupleActionFn(SpoutExecutor.java:295) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.executor.Executor.accept(Executor.java:278) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]"
            ],
            "RootCause": "The root cause of the issue is that the pendingEmits queue becomes full, which prevents further tuples from being added. This can happen due to excessive tuple emissions or frequent metrics ticks that trigger additional emissions without sufficient processing of the queue.",
            "StepsToReproduce": [
                "Run the topology from the provided GitHub link.",
                "Ensure that the topology is configured to emit a high volume of tuples.",
                "Monitor the pendingEmits queue size during execution.",
                "Trigger tick tuples to observe the behavior when the queue is near capacity."
            ],
            "ExpectedBehavior": "The system should handle tuple emissions without crashing, even under high load, by properly managing the pendingEmits queue.",
            "ObservedBehavior": "The system crashes with a 'Queue full' error when attempting to add a tuple to the pendingEmits queue that is already full.",
            "Suggestions": "Consider implementing backpressure mechanisms to limit tuple emissions when the pendingEmits queue is nearing capacity. Additionally, review the logic in the fail method of LoadSpout to ensure it does not re-emit tuples excessively.",
            "problem_location": {
                "files": [
                    "storm-client/src/jvm/org/apache/storm/executor/Executor.java",
                    "storm-client/src/jvm/org/apache/storm/utils/JCQueue.java",
                    "storm-client/src/jvm/org/apache/storm/executor/spout/SpoutExecutor.java",
                    "storm-client/src/jvm/org/apache/storm/daemon/worker/WorkerTransfer.java",
                    "storm-client/src/jvm/org/apache/storm/daemon/worker/WorkerState.java",
                    "examples/storm-loadgen/src/main/java/org/apache/storm/loadgen/LoadSpout.java"
                ],
                "classes": [
                    "org.apache.storm.executor.Executor",
                    "org.apache.storm.utils.JCQueue",
                    "org.apache.storm.executor.spout.SpoutExecutor",
                    "org.apache.storm.daemon.worker.WorkerTransfer",
                    "org.apache.storm.daemon.worker.WorkerState",
                    "org.apache.storm.loadgen.LoadSpout"
                ],
                "methods": [
                    "Executor.accept",
                    "JCQueue.consumeImpl",
                    "SpoutExecutor.failSpoutMsg",
                    "WorkerTransfer.tryTransferRemote",
                    "WorkerState.tryTransferRemote",
                    "LoadSpout.fail"
                ]
            },
            "possible_fix": "To mitigate this issue, consider implementing a check in the emit methods to prevent emissions when the pendingEmits queue is close to its capacity. Additionally, review the logic in the fail method of LoadSpout to ensure it does not trigger excessive emissions during high load scenarios."
        }
    },
    {
        "filename": "STORM-1672.json",
        "creation_time": "2016-03-31T19:24:18.000+0000",
        "bug_report": {
            "Title": "ClassCastException in StatsUtil when processing component stats",
            "Description": "A ClassCastException occurs in the StatsUtil class when attempting to filter system streams in the method filterSysStreams. The exception is thrown because a Long object is being incorrectly cast to a Map, indicating a type mismatch in the data being processed. This issue arises during the aggregation of component execution statistics, specifically when handling the input statistics for a component.",
            "StackTrace": [
                "2016-03-31 14:21:44.576 o.a.s.t.s.AbstractNonblockingServer$FrameBuffer [ERROR] Unexpected throwable while invoking!",
                "java.lang.ClassCastException: java.lang.Long cannot be cast to java.util.Map",
                "at org.apache.storm.stats.StatsUtil.filterSysStreams(StatsUtil.java:1696)",
                "at org.apache.storm.stats.StatsUtil.aggPreMergeCompPageBolt(StatsUtil.java:240)",
                "at org.apache.storm.stats.StatsUtil.aggCompExecStats(StatsUtil.java:1130)",
                "at org.apache.storm.stats.StatsUtil.aggregateCompStats(StatsUtil.java:1108)",
                "at org.apache.storm.stats.StatsUtil.aggCompExecsStats(StatsUtil.java:1236)",
                "at org.apache.storm.daemon.nimbus$fn__3490$exec_fn__789__auto__$reify__3519.getComponentPageInfo(nimbus.clj:2130)",
                "at org.apache.storm.generated.Nimbus$Processor$getComponentPageInfo.getResult(Nimbus.java:3826)",
                "at org.apache.storm.generated.Nimbus$Processor$getComponentPageInfo.getResult(Nimbus.java:3810)",
                "at org.apache.storm.thrift.ProcessFunction.process(ProcessFunction.java:39)",
                "at org.apache.storm.thrift.TBaseProcessor.process(TBaseProcessor.java:39)",
                "at org.apache.storm.security.auth.SimpleTransportPlugin$SimpleWrapProcessor.process(SimpleTransportPlugin.java:158)",
                "at org.apache.storm.thrift.server.AbstractNonblockingServer$FrameBuffer.invoke(AbstractNonblockingServer.java:518)",
                "at org.apache.storm.thrift.server.Invocation.run(Invocation.java:18)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "at java.lang.Thread.run(Thread.java:744)"
            ],
            "RootCause": "The root cause of the ClassCastException is that the method filterSysStreams is attempting to cast a Long object to a Map. This indicates that the data structure being passed to this method is not as expected, likely due to incorrect handling of the input statistics in the previous aggregation methods.",
            "StepsToReproduce": [
                "Trigger the component statistics aggregation process in the UI.",
                "Ensure that the input data contains a Long value where a Map is expected.",
                "Observe the logs for the ClassCastException."
            ],
            "ExpectedBehavior": "The system should correctly aggregate and filter component statistics without throwing a ClassCastException.",
            "ObservedBehavior": "A ClassCastException is thrown, indicating a type mismatch when processing component statistics.",
            "Suggestions": "Review the data structures being passed to the filterSysStreams method. Ensure that the input statistics are correctly formatted as Maps and that any Long values are handled appropriately before reaching this method.",
            "problem_location": {
                "files": [
                    "storm-core/src/jvm/org/apache/storm/stats/StatsUtil.java"
                ],
                "classes": [
                    "org.apache.storm.stats.StatsUtil"
                ],
                "methods": [
                    "StatsUtil.filterSysStreams",
                    "StatsUtil.aggPreMergeCompPageBolt",
                    "StatsUtil.aggCompExecStats",
                    "StatsUtil.aggregateCompStats",
                    "StatsUtil.aggCompExecsStats"
                ]
            },
            "possible_fix": "In the aggPreMergeCompPageBolt method, ensure that the statistics being passed to filterSysStreams are validated and correctly formatted as Maps. If a Long value is encountered, handle it appropriately to avoid casting errors."
        }
    },
    {
        "filename": "STORM-1520.json",
        "creation_time": "2016-02-03T02:48:58.000+0000",
        "bug_report": {
            "Title": "Nimbus Clojure/Zookeeper issue ('stateChanged' method not found)",
            "Description": "The Nimbus service encounters an IllegalArgumentException due to the absence of the 'stateChanged' method in the expected class during the deployment and undeployment of topologies. This issue is logged in nimbus.log, indicating that the ListenerContainer is unable to invoke the method on a reified instance of the class, leading to Nimbus becoming unresponsive and requiring a manual restart.",
            "StackTrace": [
                "2016-02-02 21:34:04.308 o.a.s.s.o.a.c.f.l.ListenerContainer [ERROR] Listener (org.apache.storm.cluster_state.zookeeper_state_factory$_mkState$reify$reify__12660@22587507) threw an exception",
                "java.lang.IllegalArgumentException: No matching method found: stateChanged for class org.apache.storm.cluster$mk_storm_cluster_state$reify$reify__6413",
                "at clojure.lang.Reflector.invokeMatchingMethod(Reflector.java:53)",
                "at clojure.lang.Reflector.invokeInstanceMethod(Reflector.java:28)",
                "at org.apache.storm.cluster_state.zookeeper_state_factory$_mkState$reify$reify__12660.stateChanged(zookeeper_state_factory.clj:145)",
                "at org.apache.storm.shade.org.apache.curator.framework.state.ConnectionStateManager$2.apply(ConnectionStateManager.java:259)",
                "at org.apache.storm.shade.org.apache.curator.framework.state.ConnectionStateManager$2.apply(ConnectionStateManager.java:255)",
                "at org.apache.storm.shade.org.apache.curator.framework.listen.ListenerContainer$1.run(ListenerContainer.java:92)",
                "at org.apache.storm.shade.com.google.common.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:297)",
                "at org.apache.storm.shade.org.apache.curator.framework.listen.ListenerContainer.forEach(ListenerContainer.java:84)",
                "at org.apache.storm.shade.org.apache.curator.framework.state.ConnectionStateManager.processEvents(ConnectionStateManager.java:253)",
                "at org.apache.storm.shade.org.apache.curator.framework.state.ConnectionStateManager.access$000(ConnectionStateManager.java:43)",
                "at org.apache.storm.shade.org.apache.curator.framework.state.ConnectionStateManager$1.call(ConnectionStateManager.java:111)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:266)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "RootCause": "The root cause of the issue is that the 'stateChanged' method is not defined in the reified instance of the class expected by the ListenerContainer, leading to an IllegalArgumentException when the method is invoked.",
            "StepsToReproduce": [
                "Deploy a topology using Nimbus.",
                "Undeploy the same topology.",
                "Monitor the nimbus.log for errors related to the 'stateChanged' method."
            ],
            "ExpectedBehavior": "Nimbus should handle the deployment and undeployment of topologies without throwing exceptions and should remain responsive.",
            "ObservedBehavior": "Nimbus becomes unresponsive and requires a manual restart after encountering an IllegalArgumentException related to the missing 'stateChanged' method.",
            "Suggestions": "Ensure that the 'stateChanged' method is properly defined in the class that is expected to handle state changes. Review the implementation of the zookeeper_state_factory to confirm that all necessary methods are present.",
            "problem_location": {
                "files": [
                    "zookeeper_state_factory.clj"
                ],
                "classes": [
                    "org.apache.storm.cluster_state.zookeeper_state_factory"
                ],
                "methods": [
                    "stateChanged"
                ]
            },
            "possible_fix": "Define the 'stateChanged' method in the appropriate class or ensure that the correct class is being referenced in the ListenerContainer. For example, add the following method to the class: \n\n(defn stateChanged [this new-state] \n  ;; handle state change logic here\n)"
        }
    },
    {
        "filename": "STORM-1977.json",
        "creation_time": "2016-07-17T09:07:06.000+0000",
        "bug_report": {
            "Title": "Leader Nimbus crashes with getClusterInfo when it doesn't have one or more replicated topology codes",
            "Description": "The Nimbus server crashes when it attempts to retrieve cluster information using the getClusterInfo method after gaining leadership without having all required topology codes. This issue arises due to the removal of the previous logic that prevented Nimbus from assuming leadership without all topology codes, leading to a KeyNotFoundException when the getClusterInfo method is called.",
            "StackTrace": [
                "KeyNotFoundException(msg:production-topology-2-1468745167-stormcode.ser)",
                "at org.apache.storm.blobstore.LocalFsBlobStore.getStoredBlobMeta(LocalFsBlobStore.java:149)",
                "at org.apache.storm.blobstore.LocalFsBlobStore.getBlobReplication(LocalFsBlobStore.java:268)",
                "at org.apache.storm.daemon.nimbus$get_blob_replication_count.invoke(nimbus.clj:498)",
                "at org.apache.storm.daemon.nimbus$get_cluster_info$iter__9520__9524$fn__9525.invoke(nimbus.clj:1427)",
                "at org.apache.storm.daemon.nimbus$get_cluster_info.invoke(nimbus.clj:1401)",
                "at org.apache.storm.daemon.nimbus$mk_reified_nimbus$reify__9612.getClusterInfo(nimbus.clj:1838)",
                "at org.apache.storm.generated.Nimbus$Processor$getClusterInfo.getResult(Nimbus.java:3724)",
                "at org.apache.storm.thrift.ProcessFunction.process(ProcessFunction.java:39)",
                "at org.apache.storm.util$exit_process_BANG_.doInvoke(util.clj:341)"
            ],
            "RootCause": "The root cause of the crash is that Nimbus attempts to access a blob that does not exist in the BlobStore, resulting in a KeyNotFoundException. This occurs because Nimbus has assumed leadership without having all the necessary topology codes, which are required for the getClusterInfo method to function correctly.",
            "StepsToReproduce": [
                "Comment out cleanup-corrupt-topologies! from nimbus.clj.",
                "Patch the Storm cluster.",
                "Launch Nimbus 1 (leader).",
                "Run a topology.",
                "Kill Nimbus 1.",
                "Launch Nimbus 2 from a different node.",
                "Nimbus 2 gains leadership.",
                "Request getClusterInfo from Nimbus 2."
            ],
            "ExpectedBehavior": "Nimbus should be able to provide cluster information without crashing, even if it does not have all replicated topology codes.",
            "ObservedBehavior": "Nimbus crashes with a KeyNotFoundException when getClusterInfo is requested after gaining leadership without all required topology codes.",
            "Suggestions": "Reintroduce the logic that prevents Nimbus from gaining leadership without all topology codes, or handle the KeyNotFoundException gracefully in the getClusterInfo method to avoid crashing.",
            "problem_location": {
                "files": [
                    "nimbus.clj",
                    "LocalFsBlobStore.java"
                ],
                "classes": [
                    "org.apache.storm.daemon.nimbus",
                    "org.apache.storm.blobstore.LocalFsBlobStore"
                ],
                "methods": [
                    "get_blob_replication_count",
                    "get_cluster_info",
                    "getStoredBlobMeta"
                ]
            },
            "possible_fix": "Reintroduce the leadership check in Nimbus to ensure it only gains leadership if all topology codes are present. Alternatively, modify the getClusterInfo method to catch KeyNotFoundException and return a default response instead of crashing."
        }
    },
    {
        "filename": "STORM-2988.json",
        "creation_time": "2018-03-07T14:55:22.000+0000",
        "bug_report": {
            "Title": "Error on initialization of server mk-worker when using JmxStormReporter",
            "Description": "The issue arises when initializing a worker in Apache Storm with the JmxStormReporter configured in storm.yaml. The worker fails to start and logs an error indicating that it cannot convert the reporter configuration map to a string. This is due to the JmxStormReporter attempting to pass a map to a method that expects a string, leading to an IllegalArgumentException.",
            "StackTrace": [
                "2018-03-07 15:39:19.182 o.a.s.d.worker main [ERROR] Error on initialization of server mk-worker",
                "java.lang.IllegalArgumentException: Don't know how to convert {\"class\" \"org.apache.storm.metrics2.reporters.JmxStormReporter\", \"daemons\" [\"supervisor\" \"nimbus\" \"worker\"], \"report.period\" 10, \"report.period.units\" \"SECONDS\"} + to String",
                "at org.apache.storm.utils.Utils.getString(Utils.java:848) ~[storm-core-1.2.1.jar:1.2.1]",
                "at org.apache.storm.metrics2.reporters.JmxStormReporter.getMetricsJMXDomain(JmxStormReporter.java:70) ~[storm-core-1.2.1.jar:1.2.1]",
                "at org.apache.storm.metrics2.reporters.JmxStormReporter.prepare(JmxStormReporter.java:51) ~[storm-core-1.2.1.jar:1.2.1]",
                "at org.apache.storm.metrics2.StormMetricRegistry.startReporter(StormMetricRegistry.java:119) ~[storm-core-1.2.1.jar:1.2.1]",
                "at org.apache.storm.metrics2.StormMetricRegistry.start(StormMetricRegistry.java:102) ~[storm-core-1.2.1.jar:1.2.1]",
                "at org.apache.storm.daemon.worker$fn__5545$exec_fn__1369__auto____5546.invoke(worker.clj:611) ~[storm-core-1.2.1.jar:1.2.1]",
                "at clojure.lang.AFn.applyToHelper(AFn.java:178) ~[clojure-1.7.0.jar:?]",
                "at clojure.lang.AFn.applyTo(AFn.java:144) ~[clojure-1.7.0.jar:?]",
                "at org.apache.storm.daemon.worker$fn__5545$mk_worker__5636.doInvoke(worker.clj:598) [storm-core-1.2.1.jar:1.2.1]",
                "at clojure.lang.RestFn.invoke(RestFn.java:512) [storm-core-1.2.1.jar:1.2.1]",
                "at org.apache.storm.daemon.worker$_main.invoke(worker.clj:787) [storm-core-1.2.1.jar:1.2.1]",
                "at clojure.lang.AFn.applyToHelper(AFn.java:165) [storm-core-1.2.1.jar:?]",
                "at clojure.lang.AFn.applyTo(AFn.java:144) [storm-core-1.2.1.jar:?]",
                "at org.apache.storm.daemon.worker.main(Unknown Source) [storm-core-1.2.1.jar:1.2.1]"
            ],
            "RootCause": "The root cause of the issue is that the JmxStormReporter is passing a configuration map to the Utils.getString() method, which expects a string. This results in an IllegalArgumentException when the worker attempts to initialize.",
            "StepsToReproduce": [
                "Configure storm.yaml with the JmxStormReporter as specified.",
                "Start the nimbus and supervisor processes.",
                "Submit a topology to the cluster.",
                "Observe the worker initialization logs for errors."
            ],
            "ExpectedBehavior": "The worker should initialize successfully and report metrics to JMX without errors.",
            "ObservedBehavior": "The worker fails to initialize and logs an error related to converting the reporter configuration map to a string.",
            "Suggestions": "Modify the JmxStormReporter to ensure that it passes a string to the Utils.getString() method instead of a map. This may involve changing how the metrics JMX domain is retrieved from the configuration.",
            "problem_location": {
                "files": [
                    "JmxStormReporter.java",
                    "Utils.java"
                ],
                "classes": [
                    "org.apache.storm.metrics2.reporters.JmxStormReporter",
                    "org.apache.storm.utils.Utils"
                ],
                "methods": [
                    "JmxStormReporter.getMetricsJMXDomain",
                    "Utils.getString"
                ]
            },
            "possible_fix": "In the JmxStormReporter class, modify the getMetricsJMXDomain method to extract the necessary string value from the reporterConf map before passing it to Utils.getString(). For example:\n\npublic static String getMetricsJMXDomain(Map reporterConf) {\n    String domain = (String) reporterConf.get(\"domain_key\"); // Replace 'domain_key' with the actual key for the domain\n    return Utils.getString(domain, JMX_DOMAIN);\n}"
        }
    },
    {
        "filename": "STORM-2321.json",
        "creation_time": "2017-01-24T04:18:07.000+0000",
        "bug_report": {
            "Title": "Nimbus did not come up after restart",
            "Description": "After performing a restart of the Nimbus during high availability (HA) testing, the Nimbus service failed to come back online. The logs indicate that there was an attempt to update a blob in the blob store, but it failed due to a missing node in ZooKeeper, specifically for the key 'KillLeaderThenSubmitNewTopology1-1-1484715309-stormjar.jar'. This missing node led to a series of exceptions, ultimately causing the Nimbus to halt.",
            "StackTrace": [
                "org.apache.storm.shade.org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /blobstore/KillLeaderThenSubmitNewTopology1-1-1484715309-stormjar.jar",
                "at org.apache.storm.blobstore.KeySequenceNumber.getKeySequenceNumber(KeySequenceNumber.java:149)",
                "at org.apache.storm.daemon.nimbus$get_version_for_key.invoke(nimbus.clj:456)",
                "at org.apache.storm.daemon.nimbus$mk_reified_nimbus$reify__9548.createStateInZookeeper(nimbus.clj:2056)",
                "at org.apache.storm.blobstore.BlobSynchronizer.syncBlobs(BlobSynchronizer.java:92)",
                "at org.apache.storm.blobstore.BlobSynchronizer.updateKeySetForBlobStore(BlobSynchronizer.java:114)"
            ],
            "RootCause": "The root cause of the issue is the absence of the expected blob node in ZooKeeper, which is required for the Nimbus to function correctly after a restart. The Nimbus attempts to access a blob that does not exist, leading to a NoNodeException.",
            "StepsToReproduce": [
                "1. Start the Nimbus service.",
                "2. Perform a restart of the Nimbus service during HA testing.",
                "3. Observe the logs for errors related to blob updates."
            ],
            "ExpectedBehavior": "The Nimbus service should successfully restart and recover its state, including accessing the necessary blobs from ZooKeeper.",
            "ObservedBehavior": "The Nimbus service fails to restart due to a NoNodeException when trying to access a blob that does not exist in ZooKeeper.",
            "Suggestions": "Ensure that the necessary blobs are present in ZooKeeper before restarting the Nimbus. Implement checks to verify the existence of required blobs and handle cases where they are missing.",
            "problem_location": {
                "files": [
                    "storm-core/src/jvm/org/apache/storm/blobstore/BlobStoreUtils.java",
                    "storm-core/src/jvm/org/apache/storm/blobstore/KeySequenceNumber.java",
                    "storm-core/src/jvm/org/apache/storm/blobstore/NimbusBlobStore.java"
                ],
                "classes": [
                    "org.apache.storm.blobstore.BlobStoreUtils",
                    "org.apache.storm.blobstore.KeySequenceNumber",
                    "org.apache.storm.blobstore.NimbusBlobStore"
                ],
                "methods": [
                    "BlobStoreUtils.createStateInZookeeper",
                    "KeySequenceNumber.getKeySequenceNumber",
                    "NimbusBlobStore.createStateInZookeeper"
                ]
            },
            "possible_fix": "Before restarting the Nimbus, ensure that the blob with the key 'KillLeaderThenSubmitNewTopology1-1-1484715309-stormjar.jar' is created in ZooKeeper. This can be done by adding a pre-check in the Nimbus startup process to verify the existence of required blobs and log an error or halt the startup if they are missing."
        }
    },
    {
        "filename": "STORM-3013.json",
        "creation_time": "2018-03-28T04:47:28.000+0000",
        "bug_report": {
            "Title": "Deactivated topology restarts if data flows into Kafka",
            "Description": "When a Storm topology is deactivated, any subsequent data produced into Kafka leads to a RuntimeException due to an attempt to access a closed Kafka consumer. This occurs because the metrics collection process tries to fetch offsets from a Kafka consumer that has already been closed, resulting in an IllegalStateException.",
            "StackTrace": [
                "2018-03-28 09:50:23.804 o.a.s.d.executor Thread-83-kafkaLogs-executor[130 130] [INFO] Deactivating spout kafkaLogs:(130)",
                "2018-03-28 09:51:01.289 o.a.s.util Thread-17-kafkaLogs-executor[139 139] [ERROR] Async loop died!",
                "java.lang.RuntimeException: java.lang.IllegalStateException: This consumer has already been closed.",
                "at org.apache.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:522) ~[storm-core-1.2.1.jar:1.2.1]",
                "at org.apache.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:487) ~[storm-core-1.2.1.jar:1.2.1]",
                "at org.apache.storm.utils.DisruptorQueue.consumeBatch(DisruptorQueue.java:477) ~[storm-core-1.2.1.jar:1.2.1]",
                "at org.apache.storm.kafka.spout.metrics.KafkaOffsetMetric.getValueAndReset(KafkaOffsetMetric.java:79) ~[storm-core-1.2.1.jar:1.2.1]",
                "Caused by: java.lang.IllegalStateException: This consumer has already been closed.",
                "at org.apache.kafka.clients.consumer.KafkaConsumer.acquireAndEnsureOpen(KafkaConsumer.java:1787) ~[stormjar.jar:?]"
            ],
            "RootCause": "The root cause of the issue is that the Kafka consumer is being accessed after it has been closed when the topology is deactivated. The metrics collection process does not check if the consumer is still open before attempting to fetch offsets.",
            "StepsToReproduce": [
                "1. Deactivate the Storm topology.",
                "2. Produce records into the Kafka topic that the topology was consuming from.",
                "3. Observe the exception thrown in the logs."
            ],
            "ExpectedBehavior": "The system should handle the deactivation of the topology gracefully without throwing exceptions when data is produced into Kafka.",
            "ObservedBehavior": "An exception is thrown indicating that the Kafka consumer has already been closed, leading to a failure in the metrics collection process.",
            "Suggestions": "Implement a check to ensure that the Kafka consumer is open before attempting to access it for metrics collection. This can prevent the IllegalStateException from occurring.",
            "problem_location": {
                "files": [
                    "KafkaOffsetMetric.java"
                ],
                "classes": [
                    "org.apache.storm.kafka.spout.metrics.KafkaOffsetMetric"
                ],
                "methods": [
                    "KafkaOffsetMetric.getValueAndReset"
                ]
            },
            "possible_fix": "In the `getValueAndReset` method of `KafkaOffsetMetric`, add a check to verify if the `kafkaConsumer` is open before calling `beginningOffsets` and `endOffsets`. If the consumer is closed, return early or handle the situation appropriately."
        }
    },
    {
        "filename": "STORM-3117.json",
        "creation_time": "2018-06-20T21:37:56.000+0000",
        "bug_report": {
            "Title": "Deleting blobs for running topologies hoses Nimbus",
            "Description": "The issue arises when attempting to delete blobs for a running topology, which causes Nimbus to get stuck and restart. The sequence of operations in the provided pseudo-code leads to Nimbus trying to access deleted blobs, resulting in a KeyNotFoundException. This occurs because the blobs are deleted while Nimbus is still processing the topology, leading to a failure in retrieving necessary metadata and credentials.",
            "StackTrace": [
                "org.apache.storm.utils.WrappedKeyNotFoundException: wc-topology-test-1-1529509694-stormjar.jar",
                "at org.apache.storm.blobstore.LocalFsBlobStore.getStoredBlobMeta(LocalFsBlobStore.java:256)",
                "at org.apache.storm.blobstore.LocalFsBlobStore.getBlobMeta(LocalFsBlobStore.java:286)",
                "at org.apache.storm.daemon.nimbus.Nimbus.getBlobMeta(Nimbus.java:3483)",
                "at org.apache.storm.generated.Nimbus$Processor$getBlobMeta.getResult(Nimbus.java:4011)",
                "at org.apache.storm.thrift.ProcessFunction.process(ProcessFunction.java:38)",
                "at org.apache.storm.thrift.TBaseProcessor.process(TBaseProcessor.java:39)",
                "at org.apache.storm.security.auth.sasl.SaslTransportPlugin$TUGIWrapProcessor.process(SaslTransportPlugin.java:147)",
                "at org.apache.storm.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:291)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "at java.lang.Thread.run(Thread.java:748)"
            ],
            "RootCause": "The root cause of the issue is that Nimbus attempts to access blob metadata for a topology that has already had its blobs deleted. This results in a KeyNotFoundException, which leads to Nimbus being unable to process the topology correctly and subsequently restarting.",
            "StepsToReproduce": [
                "1. Submit a topology using cluster.submitTopology() with a valid configuration.",
                "2. Wait for the topology to be fully up using cluster.waitTopologyUp().",
                "3. Call cluster.deleteAllBlobs() while the topology is still running."
            ],
            "ExpectedBehavior": "Nimbus should be able to handle blob deletions gracefully without causing the system to restart or throw exceptions.",
            "ObservedBehavior": "Nimbus gets stuck and restarts due to KeyNotFoundException when trying to access deleted blobs.",
            "Suggestions": "Implement checks to ensure that blobs are not deleted while they are still in use by running topologies. Consider adding a mechanism to delay blob deletion until the topology is fully terminated.",
            "problem_location": {
                "files": [
                    "storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java",
                    "storm-server/src/main/java/org/apache/storm/blobstore/LocalFsBlobStore.java"
                ],
                "classes": [
                    "org.apache.storm.daemon.nimbus.Nimbus",
                    "org.apache.storm.blobstore.LocalFsBlobStore"
                ],
                "methods": [
                    "Nimbus.getBlobMeta",
                    "LocalFsBlobStore.getStoredBlobMeta"
                ]
            },
            "possible_fix": "Add a check in the deleteAllBlobs() method to ensure that no blobs are deleted if they are currently being accessed by any running topologies. This could involve maintaining a reference count or a status flag for each blob."
        }
    },
    {
        "filename": "STORM-2993.json",
        "creation_time": "2018-03-12T19:04:16.000+0000",
        "bug_report": {
            "Title": "Storm HDFS bolt throws ClosedChannelException when Time rotation policy is used",
            "Description": "The Storm HDFS bolt encounters a ClosedChannelException when attempting to write to HDFS during file rotation. This issue arises due to improper synchronization in the timed rotation policy, which can lead to attempts to write to a closed writer. The logs indicate that the bolt fails to write tuples, triggering a flush of existing data, but ultimately results in a ClosedChannelException.",
            "StackTrace": [
                "java.nio.channels.ClosedChannelException: null",
                "at org.apache.hadoop.hdfs.ExceptionLastSeen.throwException4Close(ExceptionLastSeen.java:73)",
                "at org.apache.hadoop.hdfs.DFSOutputStream.checkClosed(DFSOutputStream.java:153)",
                "at org.apache.hadoop.fs.FSOutputSummer.write(FSOutputSummer.java:105)",
                "at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:57)",
                "at java.io.DataOutputStream.write(DataOutputStream.java:107)",
                "at java.io.FilterOutputStream.write(FilterOutputStream.java:97)",
                "at org.apache.storm.hdfs.common.HDFSWriter.doWrite(HDFSWriter.java:48)",
                "at org.apache.storm.hdfs.common.AbstractHDFSWriter.write(AbstractHDFSWriter.java:40)",
                "at org.apache.storm.hdfs.bolt.AbstractHdfsBolt.execute(AbstractHdfsBolt.java:158)"
            ],
            "RootCause": "The root cause of the issue is that the timed rotation policy does not synchronize properly, allowing the HDFS bolt to attempt writing to a closed writer, resulting in a ClosedChannelException.",
            "StepsToReproduce": [
                "Configure the Storm HDFS bolt with a timed rotation policy.",
                "Run the Storm topology that uses the HDFS bolt.",
                "Monitor the worker logs for any ClosedChannelException errors during file writes."
            ],
            "ExpectedBehavior": "The HDFS bolt should successfully write tuples to HDFS without encountering a ClosedChannelException, even when the timed rotation policy is in effect.",
            "ObservedBehavior": "The HDFS bolt throws a ClosedChannelException when attempting to write to HDFS after a file rotation, causing failures in tuple processing.",
            "Suggestions": "Implement proper synchronization mechanisms in the timed rotation policy to ensure that the writer is not closed while still in use. Consider adding checks to prevent writes to a closed writer.",
            "problem_location": {
                "files": [
                    "external/storm-hdfs/src/main/java/org/apache/storm/hdfs/bolt/AbstractHdfsBolt.java",
                    "external/storm-hdfs/src/main/java/org/apache/storm/hdfs/common/HDFSWriter.java",
                    "external/storm-hdfs/src/main/java/org/apache/storm/hdfs/common/AbstractHDFSWriter.java"
                ],
                "classes": [
                    "org.apache.storm.hdfs.bolt.AbstractHdfsBolt",
                    "org.apache.storm.hdfs.common.HDFSWriter",
                    "org.apache.storm.hdfs.common.AbstractHDFSWriter"
                ],
                "methods": [
                    "AbstractHdfsBolt.execute",
                    "HDFSWriter.doWrite",
                    "AbstractHDFSWriter.write"
                ]
            },
            "possible_fix": "To fix the issue, ensure that the timed rotation policy properly synchronizes access to the writer. This can be done by adding checks before writing to ensure the writer is open and not closed. Additionally, consider implementing a retry mechanism for writes that fail due to a closed channel."
        }
    },
    {
        "filename": "STORM-1540.json",
        "creation_time": "2016-02-11T22:55:05.000+0000",
        "bug_report": {
            "Title": "Topology Debug/Sampling Breaks Trident Topologies",
            "Description": "When a Trident topology is deployed with debug/sampling enabled, workers crash due to a serialization issue. The error indicates that the class 'org.apache.storm.trident.tuple.ConsList' is not serializable, which leads to a RuntimeException during the serialization process. This occurs in the context of the Kryo serialization framework used by Apache Storm.",
            "StackTrace": [
                "2016-02-11 14:13:23.617 o.a.s.util [ERROR] Async loop died!",
                "java.lang.RuntimeException: java.lang.RuntimeException: java.io.NotSerializableException: org.apache.storm.trident.tuple.ConsList",
                "at org.apache.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:448)",
                "at org.apache.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:414)",
                "at org.apache.storm.disruptor$consume_batch_when_available.invoke(disruptor.clj:73)",
                "at org.apache.storm.disruptor$consume_loop_STAR_$fn__7651.invoke(disruptor.clj:83)",
                "at org.apache.storm.util$async_loop$fn__554.invoke(util.clj:484)",
                "at clojure.lang.AFn.run(AFn.java:22)",
                "at java.lang.Thread.run(Thread.java:745)",
                "Caused by: java.lang.RuntimeException: java.io.NotSerializableException: org.apache.storm.trident.tuple.ConsList",
                "at org.apache.storm.serialization.SerializableSerializer.write(SerializableSerializer.java:41)",
                "at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:568)",
                "at com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:75)",
                "at com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:18)",
                "at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:486)",
                "at org.apache.storm.serialization.KryoValuesSerializer.serializeInto(KryoValuesSerializer.java:44)",
                "at org.apache.storm.serialization.KryoTupleSerializer.serialize(KryoTupleSerializer.java:44)",
                "at org.apache.storm.daemon.worker$mk_transfer_fn$transfer_fn__8346.invoke(worker.clj:186)",
                "at org.apache.storm.daemon.executor$start_batch_transfer__GT_worker_handler_BANG_$fn__8037.invoke(executor.clj:309)",
                "at org.apache.storm.disruptor$clojure_handler$reify__7634.onEvent(disruptor.clj:40)",
                "at org.apache.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:435)"
            ],
            "RootCause": "The root cause of the issue is that the 'ConsList' class from the Trident framework is not serializable, which leads to a NotSerializableException during the serialization process when debug/sampling is enabled.",
            "StepsToReproduce": [
                "1. Deploy a Trident topology.",
                "2. Turn on debug/sampling."
            ],
            "ExpectedBehavior": "The Trident topology should run without crashing, and debug/sampling should function correctly without serialization errors.",
            "ObservedBehavior": "Workers crash with a NotSerializableException when debug/sampling is enabled.",
            "Suggestions": "Ensure that all classes used in the topology, especially those involved in serialization, implement the Serializable interface. Alternatively, consider using a different data structure that is serializable.",
            "problem_location": {
                "files": [
                    "storm-core/src/jvm/org/apache/storm/serialization/KryoValuesSerializer.java",
                    "storm-core/src/jvm/org/apache/storm/utils/DisruptorQueue.java",
                    "storm-core/src/jvm/org/apache/storm/serialization/KryoTupleSerializer.java",
                    "storm-core/src/jvm/org/apache/storm/serialization/SerializableSerializer.java"
                ],
                "classes": [
                    "org.apache.storm.serialization.KryoValuesSerializer",
                    "org.apache.storm.utils.DisruptorQueue",
                    "org.apache.storm.serialization.KryoTupleSerializer",
                    "org.apache.storm.serialization.SerializableSerializer"
                ],
                "methods": [
                    "KryoValuesSerializer.serializeInto",
                    "DisruptorQueue.consumeBatchToCursor",
                    "KryoTupleSerializer.serialize",
                    "SerializableSerializer.write"
                ]
            },
            "possible_fix": "To resolve this issue, modify the 'ConsList' class to implement the Serializable interface. If modifying the class is not feasible, consider replacing 'ConsList' with a standard Java collection that is serializable, such as ArrayList."
        }
    },
    {
        "filename": "STORM-2275.json",
        "creation_time": "2017-01-04T23:21:06.000+0000",
        "bug_report": {
            "Title": "Nimbus crashed during state transition of topology",
            "Description": "The Nimbus service crashes with a NullPointerException during the state transition of a topology. This occurs when the system attempts to process an event related to the topology, specifically when transitioning states. The root cause is an assumption in the code that a certain variable (likely related to the topology state) is non-null, which is not the case, leading to the crash.",
            "StackTrace": [
                "java.lang.RuntimeException: java.lang.NullPointerException",
                "at org.apache.storm.daemon.nimbus.Nimbus.lambda$delayEvent$16(Nimbus.java:1174)",
                "at org.apache.storm.StormTimer$StormTimerTask.run(StormTimer.java:83)",
                "Caused by: java.lang.NullPointerException",
                "at org.apache.storm.daemon.nimbus.Nimbus.transition(Nimbus.java:1215)",
                "at org.apache.storm.daemon.nimbus.Nimbus.lambda$delayEvent$16(Nimbus.java:1172)",
                "... 1 more",
                "java.lang.RuntimeException: Halting process: Error while processing event",
                "at org.apache.storm.utils.Utils.exitProcess(Utils.java:1792)",
                "at org.apache.storm.daemon.nimbus.Nimbus.lambda$new$15(Nimbus.java:1107)",
                "at org.apache.storm.StormTimer$StormTimerTask.run(StormTimer.java:104)"
            ],
            "RootCause": "The code incorrectly assumes that a variable related to the topology state is non-null, which leads to a NullPointerException when it is null.",
            "StepsToReproduce": [
                "1. Start the Nimbus service.",
                "2. Deploy a topology that triggers state transitions.",
                "3. Monitor the Nimbus logs for errors during the transition."
            ],
            "ExpectedBehavior": "The Nimbus service should handle state transitions without crashing, even if certain variables are null.",
            "ObservedBehavior": "The Nimbus service crashes with a NullPointerException during the state transition of a topology.",
            "Suggestions": "Implement null checks for the variables involved in the state transition logic to prevent NullPointerExceptions. Additionally, improve error handling to gracefully manage unexpected null values.",
            "problem_location": {
                "files": [
                    "Nimbus.java",
                    "Utils.java"
                ],
                "classes": [
                    "org.apache.storm.daemon.nimbus.Nimbus",
                    "org.apache.storm.utils.Utils"
                ],
                "methods": [
                    "Nimbus.transition",
                    "Utils.exitProcess"
                ]
            },
            "possible_fix": "In the Nimbus class, add null checks before accessing the variable that is assumed to be non-null in the transition method. For example:\n\nif (variable != null) {\n    // proceed with transition logic\n} else {\n    LOG.error(\"Variable is null, cannot proceed with transition.\");\n    return;\n}"
        }
    },
    {
        "filename": "STORM-2873.json",
        "creation_time": "2017-12-29T18:44:56.000+0000",
        "bug_report": {
            "Title": "Backpressure Implementation Deletes Ephemeral Nodes Too Frequently",
            "Description": "The backpressure implementation in the system is causing ephemeral znodes to be deleted too frequently, leading to a 'NoAuthException' from Zookeeper. This occurs when the system attempts to delete a znode that is either not present or when the user does not have the necessary permissions to perform the deletion. The excessive deletion and recreation of the same ephemeral znode can lead to instability and performance issues in the Zookeeper service.",
            "StackTrace": [
                "2017-09-18 15:00:34.980 b.s.util WorkerBackpressureThread [WARN] Expecting exception of class: class org.apache.storm.shade.org.apache.zookeeper.KeeperException$NoNodeException, but exception chain only contains: (#<NoAuthException org.apache.storm.shade.org.apache.zookeeper.KeeperException$NoAuthException: KeeperErrorCode = NoAuth for /backpressure/TestTopology--170916-005358-117-1505523256/d11af335-6e03-48b6-801e-7369acfe9ffd-127.0.0.1-6721>)",
                "2017-09-18 15:00:34.980 b.s.d.worker WorkerBackpressureThread [ERROR] workerBackpressure update failed when connecting to ZK ... will retry",
                "java.lang.RuntimeException: org.apache.storm.shade.org.apache.zookeeper.KeeperException$NoAuthException: KeeperErrorCode = NoAuth for /backpressure/TestTopology--170916-005358-117-1505523256/d11af335-6e03-48b6-801e-7369acfe9ffd-127.0.0.1-6721",
                "at backtype.storm.util$wrap_in_runtime.invoke(util.clj:52) ~[storm-core-0.10.2.y.jar:0.10.2.y]",
                "at backtype.storm.zookeeper$delete_node.doInvoke(zookeeper.clj:110) ~[storm-core-0.10.2.y.jar:0.10.2.y]",
                "at clojure.lang.RestFn.invoke(RestFn.java:464) ~[clojure-1.6.0.jar:?]",
                "at backtype.storm.zookeeper$delete_recursive.invoke(zookeeper.clj:189) ~[storm-core-0.10.2.y.jar:0.10.2.y]",
                "at backtype.storm.cluster_state.zookeeper_state_factory$_mkState$reify__4207.delete_node(zookeeper_state_factory.clj:117) ~[storm-core-0.10.2.y.jar:0.10.2.y]",
                "at backtype.storm.daemon.worker$mk_backpressure_handler$fn__7117.invoke(worker.clj:161) [storm-core-0.10.2.y.jar:0.10.2.y]",
                "at backtype.storm.disruptor$worker_backpressure_handler$reify__6432.onEvent(disruptor.clj:57) [storm-core-0.10.2.y.jar:0.10.2.y]",
                "at backtype.storm.utils.WorkerBackpressureThread.run(WorkerBackpressureThread.java:64) [storm-core-0.10.2.y.jar:0.10.2.y]"
            ],
            "RootCause": "The root cause of the issue is the backpressure implementation's logic that leads to the deletion of ephemeral znodes too frequently, which results in Zookeeper throwing a NoAuthException when the deletion is attempted without the necessary permissions.",
            "StepsToReproduce": [
                "1. Set up a Zookeeper instance and configure the backpressure implementation.",
                "2. Trigger conditions that lead to backpressure in the system.",
                "3. Observe the logs for frequent deletion attempts of ephemeral znodes.",
                "4. Note the occurrence of NoAuthException in the logs."
            ],
            "ExpectedBehavior": "The system should manage ephemeral znodes without excessive deletion and recreation, and should handle permission errors gracefully without causing instability.",
            "ObservedBehavior": "The system attempts to delete ephemeral znodes too frequently, resulting in NoAuthException errors from Zookeeper, which leads to retries and potential instability.",
            "Suggestions": "Review the logic in the backpressure implementation to ensure that znodes are only deleted when necessary. Implement checks to verify permissions before attempting to delete znodes. Consider adding a backoff strategy to reduce the frequency of deletion attempts.",
            "problem_location": {
                "files": [
                    "zookeeper.clj",
                    "worker.clj"
                ],
                "classes": [
                    "backtype.storm.zookeeper",
                    "backtype.storm.daemon.worker"
                ],
                "methods": [
                    "delete_node",
                    "mk_backpressure_handler"
                ]
            },
            "possible_fix": "Modify the delete_node method in zookeeper.clj to include permission checks before attempting to delete a znode. Additionally, implement a backoff mechanism to limit the frequency of deletion attempts."
        }
    },
    {
        "filename": "STORM-2279.json",
        "creation_time": "2017-01-05T20:59:11.000+0000",
        "bug_report": {
            "Title": "Unable to open bolt page of storm ui",
            "Description": "The user is unable to access the bolt information page in the Storm UI, resulting in an Internal Server Error. The error is caused by an ArrayIndexOutOfBoundsException when attempting to retrieve component page information, specifically due to a negative index derived from a hashcode operation. This issue arises in the Nimbus class when processing requests for component information.",
            "StackTrace": [
                "org.apache.storm.thrift.transport.TTransportException",
                "at org.apache.storm.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)",
                "at org.apache.storm.thrift.transport.TTransport.readAll(TTransport.java:86)",
                "at org.apache.storm.thrift.transport.TFramedTransport.readFrame(TFramedTransport.java:129)",
                "at org.apache.storm.thrift.transport.TFramedTransport.read(TFramedTransport.java:101)",
                "at org.apache.storm.thrift.transport.TTransport.readAll(TTransport.java:86)",
                "at org.apache.storm.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:429)",
                "at org.apache.storm.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:318)",
                "at org.apache.storm.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:219)",
                "at org.apache.storm.thrift.TServiceClient.receiveBase(TServiceClient.java:77)",
                "at org.apache.storm.generated.Nimbus$Client.recv_getComponentPageInfo(Nimbus.java:1369)",
                "at org.apache.storm.generated.Nimbus$Client.getComponentPageInfo(Nimbus.java:1353)",
                "at org.apache.storm.ui.core$component_page.invoke(core.clj:1026)",
                "at org.apache.storm.ui.core$fn__4308.invoke(core.clj:1214)",
                "at org.apache.storm.shade.compojure.core$make_route$fn__789.invoke(core.clj:100)",
                "at org.apache.storm.shade.compojure.core$if_route$fn__777.invoke(core.clj:46)",
                "at org.apache.storm.shade.compojure.core$if_method$fn__770.invoke(core.clj:31)",
                "at org.apache.storm.shade.compojure.core$routing$fn__795.invoke(core.clj:113)",
                "at clojure.core$some.invoke(core.clj:2570)",
                "at org.apache.storm.shade.compojure.core$routing.doInvoke(core.clj:113)",
                "at clojure.lang.RestFn.applyTo(RestFn.java:139)",
                "at clojure.core$apply.invoke(core.clj:632)",
                "at org.apache.storm.shade.compojure.core$routes$fn__799.invoke(core.clj:118)",
                "at org.apache.storm.shade.ring.middleware.json$wrap_json_params$fn__3573.invoke(json.clj:56)",
                "at org.apache.storm.shade.ring.middleware.multipart_params$wrap_multipart_params$fn__1924.invoke(multipart_params.clj:118)",
                "at org.apache.storm.shade.ring.middleware.reload$wrap_reload$fn__3102.invoke(reload.clj:22)",
                "at org.apache.storm.ui.helpers$requests_middleware$fn__2152.invoke(helpers.clj:54)",
                "at org.apache.storm.ui.core$catch_errors$fn__4474.invoke(core.clj:1460)",
                "at org.apache.storm.shade.ring.middleware.keyword_params$wrap_keyword_params$fn__1844.invoke(keyword_params.clj:35)",
                "at org.apache.storm.shade.ring.middleware.nested_params$wrap_nested_params$fn__1887.invoke(nested_params.clj:84)",
                "at org.apache.storm.shade.ring.middleware.params$wrap_params$fn__1816.invoke(params.clj:64)",
                "at org.apache.storm.shade.ring.middleware.multipart_params$wrap_multipart_params$fn__1924.invoke(multipart_params.clj:118)",
                "at org.apache.storm.shade.ring.middleware.flash$wrap_flash$fn__2139.invoke(flash.clj:35)",
                "at org.apache.storm.shade.ring.middleware.session$wrap_session$fn__2125.invoke(session.clj:98)",
                "at org.apache.storm.shade.ring.util.servlet$make_service_method$fn__1674.invoke(servlet.clj:127)",
                "at org.apache.storm.shade.ring.util.servlet$servlet$fn__1678.invoke(servlet.clj:136)",
                "at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:654)",
                "at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1320)",
                "at org.apache.storm.logging.filters.AccessLoggingFilter.handle(AccessLoggingFilter.java:47)",
                "at org.apache.storm.logging.filters.AccessLoggingFilter.doFilter(AccessLoggingFilter.java:39)",
                "at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1291)",
                "at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:443)",
                "at org.apache.storm.shade.org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1044)",
                "at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:372)",
                "at org.apache.storm.shade.org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:978)",
                "at org.apache.storm.shade.org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)",
                "at org.apache.storm.shade.org.eclipse.jetty.server.Server.handle(Server.java:369)",
                "at org.apache.storm.shade.org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:486)",
                "at org.apache.storm.shade.org.eclipse.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:933)",
                "at org.apache.storm.shade.org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:995)",
                "at org.apache.storm.shade.org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:644)",
                "at org.apache.storm.shade.org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235)",
                "at org.apache.storm.shade.org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82)",
                "at org.apache.storm.shade.org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:668)",
                "at org.apache.storm.shade.org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:52)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "RootCause": "The root cause of the issue is an ArrayIndexOutOfBoundsException occurring in the Nimbus class when trying to access an element in a list using a negative index. This negative index is derived from a hashcode operation that can yield negative values.",
            "StepsToReproduce": [
                "Set up the Vagrant environment with the latest Storm code.",
                "Navigate to the Storm UI.",
                "Attempt to access the bolt information page using the provided URL."
            ],
            "ExpectedBehavior": "The Storm UI should display the bolt information page without errors.",
            "ObservedBehavior": "An Internal Server Error is displayed, and the UI fails to load the bolt information.",
            "Suggestions": "Implement a check to ensure that the calculated index is non-negative before accessing the tasks list.",
            "problem_location": {
                "files": [
                    "Nimbus.java"
                ],
                "classes": [
                    "org.apache.storm.daemon.nimbus.Nimbus"
                ],
                "methods": [
                    "Nimbus.getComponentPageInfo"
                ]
            },
            "possible_fix": "Modify the index calculation in Nimbus.java to ensure it is non-negative:\n\nint taskIndex = Math.abs(TupleUtils.listHashCode(Arrays.asList(componentId))) % tasks.size();"
        }
    },
    {
        "filename": "STORM-3079.json",
        "creation_time": "2018-05-17T19:29:10.000+0000",
        "bug_report": {
            "Title": "Improve getMessage support for ThriftExceptions",
            "Description": "The current implementation of ThriftExceptions does not provide meaningful messages, leading to confusion when exceptions are logged. Specifically, the KeyNotFoundException is thrown with a null message, which makes it difficult to understand the context of the error. This issue arises from the lack of proper error handling and messaging in the methods that deal with blob storage and retrieval.",
            "StackTrace": [
                "org.apache.storm.generated.KeyNotFoundException: null",
                "at org.apache.storm.blobstore.LocalFsBlobStore.getStoredBlobMeta(LocalFsBlobStore.java:258)",
                "at org.apache.storm.blobstore.LocalFsBlobStore.getBlob(LocalFsBlobStore.java:393)",
                "at org.apache.storm.blobstore.BlobStore.readBlobTo(BlobStore.java:310)",
                "at org.apache.storm.blobstore.BlobStore.readBlob(BlobStore.java:339)",
                "at org.apache.storm.daemon.nimbus.TopoCache.readTopology(TopoCache.java:67)",
                "at org.apache.storm.daemon.nimbus.Nimbus.readStormTopologyAsNimbus(Nimbus.java:670)",
                "at org.apache.storm.daemon.nimbus.Nimbus.rmDependencyJarsInTopology(Nimbus.java:2333)",
                "at org.apache.storm.daemon.nimbus.Nimbus.doCleanup(Nimbus.java:2387)",
                "at org.apache.storm.daemon.nimbus.Nimbus.lambda$launchServer$37(Nimbus.java:2674)",
                "at org.apache.storm.StormTimer$1.run(StormTimer.java:111)",
                "at org.apache.storm.StormTimer$StormTimerTask.run(StormTimer.java:227)"
            ],
            "RootCause": "The KeyNotFoundException is thrown in the getStoredBlobMeta method when the requested blob metadata cannot be found. The exception is not providing a meaningful message, which is a result of the current implementation of the exception handling in the blob storage methods.",
            "StepsToReproduce": [
                "Attempt to retrieve a blob that does not exist in the blob store.",
                "Observe the logged exception message."
            ],
            "ExpectedBehavior": "When a KeyNotFoundException is thrown, it should provide a descriptive message indicating which key was not found.",
            "ObservedBehavior": "The exception is logged with a null message, making it unclear what caused the error.",
            "Suggestions": "Enhance the KeyNotFoundException to include the key that was not found in its message. This can be done by modifying the exception handling in the getStoredBlobMeta method.",
            "problem_location": {
                "files": [
                    "storm-server/src/main/java/org/apache/storm/blobstore/LocalFsBlobStore.java",
                    "storm-client/src/jvm/org/apache/storm/blobstore/BlobStore.java",
                    "storm-server/src/main/java/org/apache/storm/daemon/nimbus/TopoCache.java"
                ],
                "classes": [
                    "org.apache.storm.blobstore.LocalFsBlobStore",
                    "org.apache.storm.blobstore.BlobStore",
                    "org.apache.storm.daemon.nimbus.TopoCache"
                ],
                "methods": [
                    "LocalFsBlobStore.getStoredBlobMeta",
                    "LocalFsBlobStore.getBlob",
                    "BlobStore.readBlobTo",
                    "TopoCache.readTopology"
                ]
            },
            "possible_fix": "Modify the getStoredBlobMeta method to include the key in the KeyNotFoundException message. For example, change the line that throws the exception to: throw new KeyNotFoundException(\"Blob not found for key: \" + key);"
        }
    },
    {
        "filename": "STORM-3096.json",
        "creation_time": "2018-06-05T18:39:44.000+0000",
        "bug_report": {
            "Title": "Blobstores deleted before topologies can be submitted",
            "Description": "The issue arises from a race condition during topology submission where the Nimbus timer triggers the cleanup process (doCleanup) before all topologies are fully discovered and registered. This leads to the premature deletion of blobstores associated with the topology, resulting in WrappedKeyNotFoundException when attempting to access these blobs. The previous fix (STORM-3053) did not adequately address the timing of the cleanup process, as it still allows for the deletion of blobs before the topology submission is complete.",
            "StackTrace": [
                "org.apache.storm.utils.WrappedKeyNotFoundException: topology-testHardCoreFaultTolerance-4-18-1528026822-stormcode.ser",
                "at org.apache.storm.blobstore.LocalFsBlobStore.getStoredBlobMeta(LocalFsBlobStore.java:259)",
                "at org.apache.storm.blobstore.LocalFsBlobStore.getBlob(LocalFsBlobStore.java:394)",
                "at org.apache.storm.blobstore.BlobStore.readBlobTo(BlobStore.java:310)",
                "at org.apache.storm.blobstore.BlobStore.readBlob(BlobStore.java:339)",
                "at org.apache.storm.daemon.nimbus.TopoCache.readTopology(TopoCache.java:67)",
                "at org.apache.storm.daemon.nimbus.Nimbus.readStormTopologyAsNimbus(Nimbus.java:680)",
                "at org.apache.storm.daemon.nimbus.Nimbus.rmDependencyJarsInTopology(Nimbus.java:2389)",
                "at org.apache.storm.daemon.nimbus.Nimbus.doCleanup(Nimbus.java:2443)",
                "at org.apache.storm.daemon.nimbus.Nimbus.lambda$launchServer$37(Nimbus.java:2730)",
                "at org.apache.storm.StormTimer$1.run(StormTimer.java:111)",
                "at org.apache.storm.StormTimer$StormTimerTask.run(StormTimer.java:227)"
            ],
            "RootCause": "The cleanup process (doCleanup) is being executed before the topology submission is fully processed, leading to the deletion of necessary blobstores.",
            "StepsToReproduce": [
                "Submit a topology with a unique identifier.",
                "Monitor the logs for cleanup actions initiated by the Nimbus timer.",
                "Observe the WrappedKeyNotFoundException when the topology attempts to access its associated blobstores."
            ],
            "ExpectedBehavior": "The topology should be submitted successfully without any exceptions related to missing blobstores, and the associated blobs should remain intact until the topology is fully processed.",
            "ObservedBehavior": "The topology submission fails with WrappedKeyNotFoundException due to the premature deletion of blobstores.",
            "Suggestions": "Modify the timing of the cleanup process to ensure that it occurs only after all topologies have been fully discovered and registered. This may involve adjusting the order of operations in the Nimbus class or implementing a more robust synchronization mechanism.",
            "problem_location": {
                "files": [
                    "storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java",
                    "storm-server/src/main/java/org/apache/storm/blobstore/LocalFsBlobStore.java"
                ],
                "classes": [
                    "org.apache.storm.daemon.nimbus.Nimbus",
                    "org.apache.storm.blobstore.LocalFsBlobStore"
                ],
                "methods": [
                    "Nimbus.doCleanup",
                    "LocalFsBlobStore.getStoredBlobMeta"
                ]
            },
            "possible_fix": "In the Nimbus class, adjust the doCleanup method to ensure it is called only after the topology submission process is complete. This may involve adding a check to confirm that all topologies have been registered before proceeding with cleanup."
        }
    },
    {
        "filename": "STORM-1642.json",
        "creation_time": "2016-03-21T07:34:06.000+0000",
        "bug_report": {
            "Title": "NullPointerException when deserializing Thrift object",
            "Description": "A NullPointerException (NPE) occurs during the deserialization process of a Thrift object in a Storm application. The error is triggered when the KryoTupleDeserializer attempts to set a buffer in the Input class, which indicates that the buffer being passed is null. This issue arises despite the developer's assertion that OutputCollector is not used concurrently, suggesting that the problem may lie in the serialization logic or the state of the data being deserialized.",
            "StackTrace": [
                "2016-03-04 17:17:43.583 b.s.util [ERROR] Async loop died!",
                "java.lang.RuntimeException: java.lang.NullPointerException",
                "at backtype.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:135) ~[storm-core-0.10.0.jar:0.10.0]",
                "at backtype.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:106) ~[storm-core-0.10.0.jar:0.10.0]",
                "at backtype.storm.disruptor$consume_batch_when_available.invoke(disruptor.clj:80) ~[storm-core-0.10.0.jar:0.10.0]",
                "at backtype.storm.daemon.executor$fn__5694$fn__5707$fn__5758.invoke(executor.clj:819) ~[storm-core-0.10.0.jar:0.10.0]",
                "at backtype.storm.util$async_loop$fn__545.invoke(util.clj:479) [storm-core-0.10.0.jar:0.10.0]",
                "at clojure.lang.AFn.run(AFn.java:22) [clojure-1.6.0.jar:?]",
                "at java.lang.Thread.run(Thread.java:745) [?:1.8.0_60]",
                "Caused by: java.lang.NullPointerException",
                "at com.esotericsoftware.kryo.io.Input.setBuffer(Input.java:57) ~[kryo-2.21.jar:?]",
                "at backtype.storm.serialization.KryoTupleDeserializer.deserialize(KryoTupleDeserializer.java:47) ~[storm-core-0.10.0.jar:0.10.0]",
                "at backtype.storm.daemon.executor$mk_task_receiver$fn__5615.invoke(executor.clj:433) ~[storm-core-0.10.0.jar:0.10.0]",
                "at backtype.storm.disruptor$clojure_handler$reify__5189.onEvent(disruptor.clj:58) ~[storm-core-0.10.0.jar:0.10.0]",
                "at backtype.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:132) ~[storm-core-0.10.0.jar:0.10.0]",
                "... 6 more"
            ],
            "RootCause": "The root cause of the NullPointerException is that the buffer being passed to the Input.setBuffer method is null, which is likely due to an issue in the serialization process of the Thrift object.",
            "StepsToReproduce": [
                "1. Set up a Storm topology that uses a Thrift object for data transfer between bolts.",
                "2. Implement a custom serializer for the Thrift object.",
                "3. Trigger the deserialization process by executing the topology.",
                "4. Observe the logs for the NullPointerException."
            ],
            "ExpectedBehavior": "The Thrift object should be successfully deserialized without throwing a NullPointerException.",
            "ObservedBehavior": "A NullPointerException is thrown during the deserialization process, causing the Storm worker to die.",
            "Suggestions": "Review the custom serializer for the Thrift object to ensure that it correctly handles null values and that the buffer is properly initialized before being passed to the Input class.",
            "problem_location": {
                "files": [
                    "KryoTupleDeserializer.java",
                    "Input.java"
                ],
                "classes": [
                    "backtype.storm.serialization.KryoTupleDeserializer",
                    "com.esotericsoftware.kryo.io.Input"
                ],
                "methods": [
                    "KryoTupleDeserializer.deserialize",
                    "Input.setBuffer"
                ]
            },
            "possible_fix": "Ensure that the buffer is initialized correctly in the KryoTupleDeserializer before calling Input.setBuffer. For example, add a check to ensure the buffer is not null and initialize it if necessary."
        }
    },
    {
        "filename": "STORM-2700.json",
        "creation_time": "2017-08-21T14:09:50.000+0000",
        "bug_report": {
            "Title": "Blobstore shouldn't check ACL when Blobstore Acl validation disabled",
            "Description": "The Blobstore is incorrectly enforcing ACL checks even when the configuration parameter 'storm.blobstore.acl.validation.enabled' is set to false. This leads to an AuthorizationException being thrown when a user without the necessary permissions attempts to access a blob, causing the process to halt unexpectedly.",
            "StackTrace": [
                "java.util.concurrent.ExecutionException: AuthorizationException(msg:ethan does not have READ access to key1)",
                "at java.util.concurrent.FutureTask.report(FutureTask.java:122)",
                "at java.util.concurrent.FutureTask.get(FutureTask.java:206)",
                "at org.apache.storm.localizer.LocalDownloadedResource$NoCancelFuture.get(LocalDownloadedResource.java:63)",
                "at org.apache.storm.daemon.supervisor.Slot.handleWaitingForBlobLocalization(Slot.java:410)",
                "at org.apache.storm.daemon.supervisor.Slot.stateMachineStep(Slot.java:305)",
                "at org.apache.storm.daemon.supervisor.Slot.run(Slot.java:789)",
                "Caused by: org.apache.storm.generated.AuthorizationException",
                "at org.apache.storm.localizer.Localizer.downloadBlob(Localizer.java:527)",
                "at org.apache.storm.localizer.Localizer.access$000(Localizer.java:68)",
                "at org.apache.storm.localizer.Localizer$DownloadBlob.call(Localizer.java:497)",
                "at org.apache.storm.localizer.Localizer$DownloadBlob.call(Localizer.java:473)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:266)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "at java.lang.Thread.run(Thread.java:748)",
                "java.lang.RuntimeException: Halting process: Error when processing an event",
                "at org.apache.storm.utils.Utils.exitProcess(Utils.java:437)",
                "at org.apache.storm.daemon.supervisor.Slot.run(Slot.java:823)"
            ],
            "RootCause": "The root cause of the issue is that the ACL validation logic in the 'downloadBlob' method of the Localizer class does not respect the configuration setting 'storm.blobstore.acl.validation.enabled'. This results in unauthorized access attempts being checked against the ACLs even when validation is disabled.",
            "StepsToReproduce": [
                "1. Create a blobstore with permission set to one user (e.g., mapredqa).",
                "   sudo -u mapredqa storm blobstore create --file test-blobstore.txt --acl u:mapredqa:rwa key1",
                "2. Submit a topology with topology.blobstore.map config as someone else (e.g., ethan).",
                "   sudo -u ethan storm jar /tmp/storm-starter-2.0.0-SNAPSHOT.jar org.apache.storm.starter.WordCountTopology wc -c topology.blobstore.map='{\"key1\":{\"localname\":\"test-blobstore.txt\", \"uncompress\":false}}'"
            ],
            "ExpectedBehavior": "When 'storm.blobstore.acl.validation.enabled' is set to false, the blobstore should allow access to blobs without checking ACLs, regardless of the user's permissions.",
            "ObservedBehavior": "The blobstore checks ACLs and throws an AuthorizationException when a user without the necessary permissions attempts to access a blob, leading to a process halt.",
            "Suggestions": "Modify the 'downloadBlob' method in the Localizer class to check the 'storm.blobstore.acl.validation.enabled' configuration before performing any ACL checks.",
            "problem_location": {
                "files": [
                    "storm-server/src/main/java/org/apache/storm/localizer/Localizer.java",
                    "storm-server/src/main/java/org/apache/storm/daemon/supervisor/Slot.java",
                    "storm-client/src/jvm/org/apache/storm/utils/Utils.java"
                ],
                "classes": [
                    "org.apache.storm.localizer.Localizer",
                    "org.apache.storm.daemon.supervisor.Slot",
                    "org.apache.storm.utils.Utils"
                ],
                "methods": [
                    "Localizer.downloadBlob",
                    "Slot.run",
                    "Utils.exitProcess"
                ]
            },
            "possible_fix": "In the 'downloadBlob' method, add a check for the 'storm.blobstore.acl.validation.enabled' configuration before performing the ACL check:\n\nif (!Boolean.parseBoolean(conf.get(\"storm.blobstore.acl.validation.enabled\"))) {\n    // Skip ACL check\n} else {\n    if (!ServerUtils.canUserReadBlob(blobstore.getBlobMeta(key), user)) {\n        throw new AuthorizationException(user + \" does not have READ access to \" + key);\n    }\n}"
        }
    },
    {
        "filename": "STORM-1663.json",
        "creation_time": "2016-03-29T06:07:27.000+0000",
        "bug_report": {
            "Title": "Exception on Refreshing Active Topology Page in Storm UI",
            "Description": "When a user clicks on an active topology from the Storm UI home page and subsequently refreshes the page, an exception is thrown. The exception is of type `org.apache.storm.thrift.transport.TTransportException`, indicating a failure in the transport layer while trying to read data from the server. This issue arises during the invocation of the `getTopologyPageInfo` method in the Nimbus client, which is responsible for fetching the topology information.",
            "StackTrace": [
                "org.apache.storm.thrift.transport.TTransportException",
                "at org.apache.storm.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)",
                "at org.apache.storm.thrift.transport.TTransport.readAll(TTransport.java:86)",
                "at org.apache.storm.thrift.transport.TFramedTransport.readFrame(TFramedTransport.java:129)",
                "at org.apache.storm.thrift.transport.TFramedTransport.read(TFramedTransport.java:101)",
                "at org.apache.storm.thrift.transport.TTransport.readAll(TTransport.java:86)",
                "at org.apache.storm.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:429)",
                "at org.apache.storm.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:318)",
                "at org.apache.storm.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:219)",
                "at org.apache.storm.thrift.TServiceClient.receiveBase(TServiceClient.java:77)",
                "at org.apache.storm.generated.Nimbus$Client.recv_getTopologyPageInfo(Nimbus.java:1243)",
                "at org.apache.storm.generated.Nimbus$Client.getTopologyPageInfo(Nimbus.java:1228)",
                "at org.apache.storm.ui.core$topology_page.invoke(core.clj:638)",
                "at org.apache.storm.ui.core$fn__3662.invoke(core.clj:987)",
                "at org.apache.storm.shade.compojure.core$make_route$fn__302.invoke(core.clj:93)",
                "at org.apache.storm.shade.compojure.core$if_route$fn__290.invoke(core.clj:39)",
                "at org.apache.storm.shade.compojure.core$if_method$fn__283.invoke(core.clj:24)",
                "at org.apache.storm.shade.compojure.core$routing$fn__308.invoke(core.clj:106)",
                "at clojure.core$some.invoke(core.clj:2570)",
                "at org.apache.storm.shade.compojure.core$routing.doInvoke(core.clj:106)",
                "at clojure.lang.RestFn.applyTo(RestFn.java:139)",
                "at clojure.core$apply.invoke(core.clj:632)",
                "at org.apache.storm.shade.compojure.core$routes$fn__312.invoke(core.clj:111)",
                "at org.apache.storm.shade.ring.middleware.json$wrap_json_params$fn__1204.invoke(json.clj:56)",
                "at org.apache.storm.shade.ring.middleware.multipart_params$wrap_multipart_params$fn__765.invoke(multipart_params.clj:103)",
                "at org.apache.storm.shade.ring.middleware.reload$wrap_reload$fn__724.invoke(reload.clj:22)",
                "at org.apache.storm.ui.helpers$requests_middleware$fn__3091.invoke(helpers.clj:50)",
                "at org.apache.storm.ui.core$catch_errors$fn__3837.invoke(core.clj:1250)",
                "at org.apache.storm.shade.ring.middleware.keyword_params$wrap_keyword_params$fn__2852.invoke(keyword_params.clj:27)",
                "at org.apache.storm.shade.ring.middleware.nested_params$wrap_nested_params$fn__2892.invoke(nested_params.clj:65)",
                "at org.apache.storm.shade.ring.middleware.params$wrap_params$fn__2823.invoke(params.clj:55)",
                "at org.apache.storm.shade.ring.middleware.multipart_params$wrap_multipart_params$fn__765.invoke(multipart_params.clj:103)",
                "at org.apache.storm.shade.ring.middleware.flash$wrap_flash$fn__3075.invoke(flash.clj:14)",
                "at org.apache.storm.shade.ring.middleware.session$wrap_session$fn__3063.invoke(session.clj:43)",
                "at org.apache.storm.shade.ring.middleware.cookies$wrap_cookies$fn__2991.invoke(cookies.clj:160)",
                "at org.apache.storm.shade.ring.util.servlet$make_service_method$fn__2729.invoke(servlet.clj:127)",
                "at org.apache.storm.shade.ring.util.servlet$servlet$fn__2733.invoke(servlet.clj:136)",
                "at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:654)",
                "at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1320)",
                "at org.apache.storm.logging.filters.AccessLoggingFilter.handle(AccessLoggingFilter.java:47)",
                "at org.apache.storm.logging.filters.AccessLoggingFilter.doFilter(AccessLoggingFilter.java:39)",
                "at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1291)",
                "at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:443)",
                "at org.apache.storm.shade.org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1044)",
                "at org.apache.storm.shade.org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:372)",
                "at org.apache.storm.shade.org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:978)",
                "at org.apache.storm.shade.org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)",
                "at org.apache.storm.shade.org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)",
                "at org.apache.storm.shade.org.eclipse.jetty.server.Server.handle(Server.java:369)",
                "at org.apache.storm.shade.org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:486)",
                "at org.apache.storm.shade.org.eclipse.jetty.server.AbstractHttpConnection.headerComplete(AbstractHttpConnection.java:933)",
                "at org.apache.storm.shade.org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.headerComplete(AbstractHttpConnection.java:995)",
                "at org.apache.storm.shade.org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:644)",
                "at org.apache.storm.shade.org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:235)",
                "at org.apache.storm.shade.org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82)",
                "at org.apache.storm.shade.org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:668)",
                "at org.apache.storm.shade.org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:52)",
                "at org.apache.storm.shade.org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)",
                "at org.apache.storm.shade.org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "RootCause": "The root cause of the issue appears to be a failure in the transport layer when attempting to read data from the Nimbus server after a page refresh. This could be due to a timeout or a disconnection that occurs when the page is refreshed, leading to the `TTransportException` being thrown.",
            "StepsToReproduce": [
                "1. Navigate to the Storm UI home page.",
                "2. Click on an active topology.",
                "3. Refresh the page."
            ],
            "ExpectedBehavior": "The active topology page should refresh without throwing any exceptions, displaying the current state of the topology.",
            "ObservedBehavior": "Refreshing the active topology page throws a `TTransportException`, indicating a failure in reading data from the server.",
            "Suggestions": "Investigate the network connection stability and the server's ability to handle multiple requests. Consider implementing error handling to gracefully manage transport exceptions and provide user feedback.",
            "problem_location": {
                "files": [
                    "storm-core/src/jvm/org/apache/storm/logging/filters/AccessLoggingFilter.java"
                ],
                "classes": [
                    "org.apache.storm.logging.filters.AccessLoggingFilter"
                ],
                "methods": [
                    "AccessLoggingFilter.doFilter",
                    "AccessLoggingFilter.handle"
                ]
            },
            "possible_fix": "Implement a retry mechanism in the Nimbus client when a `TTransportException` is encountered. Additionally, ensure that the server is configured to handle concurrent requests effectively."
        }
    },
    {
        "filename": "STORM-2518.json",
        "creation_time": "2017-05-17T06:26:37.000+0000",
        "bug_report": {
            "Title": "NPE during uploading dependency artifacts with secured cluster",
            "Description": "A NullPointerException (NPE) occurs when attempting to upload dependency artifacts in a secured cluster. The issue arises because the 'name' field in the AccessControl object is optional according to the thrift specification. However, the method 'fixACLsForUser' in the BlobStoreAclHandler class attempts to access this field without checking for null values, leading to the NPE when the field is not provided.",
            "StackTrace": [
                "java.lang.NullPointerException: null",
                "at org.apache.storm.blobstore.BlobStoreAclHandler.fixACLsForUser(BlobStoreAclHandler.java:382)",
                "at org.apache.storm.blobstore.BlobStoreAclHandler.normalizeSettableACLs(BlobStoreAclHandler.java:357)",
                "at org.apache.storm.blobstore.BlobStoreAclHandler.normalizeSettableBlobMeta(BlobStoreAclHandler.java:306)",
                "at org.apache.storm.blobstore.LocalFsBlobStore.createBlob(LocalFsBlobStore.java:103)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:498)",
                "at clojure.lang.Reflector.invokeMatchingMethod(Reflector.java:93)",
                "at clojure.lang.Reflector.invokeInstanceMethod(Reflector.java:28)",
                "at org.apache.storm.daemon.nimbus$mk_reified_nimbus$reify__9064.beginCreateBlob(nimbus.clj:2047)",
                "at org.apache.storm.generated.Nimbus$Processor$beginCreateBlob.getResult(Nimbus.java:3430)",
                "at org.apache.storm.generated.Nimbus$Processor$beginCreateBlob.getResult(Nimbus.java:3414)",
                "at org.apache.storm.thrift.ProcessFunction.process(ProcessFunction.java:39)",
                "at org.apache.storm.thrift.TBaseProcessor.process(TBaseProcessor.java:39)",
                "at org.apache.storm.security.auth.SaslTransportPlugin$TUGIWrapProcessor.process(SaslTransportPlugin.java:144)",
                "at org.apache.storm.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:286)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "RootCause": "The root cause of the NPE is the lack of a null check for the 'name' field in the AccessControl object within the 'fixACLsForUser' method. When the 'name' is null, the call to 'control.get_name().equals(user)' results in a NullPointerException.",
            "StepsToReproduce": [
                "1. Set up a secured cluster with ACLs.",
                "2. Attempt to upload dependency artifacts without providing a 'name' field in the AccessControl object.",
                "3. Observe the NullPointerException in the logs."
            ],
            "ExpectedBehavior": "The system should handle the absence of the 'name' field gracefully, either by skipping the ACL check or by providing a default value, allowing the upload to proceed without errors.",
            "ObservedBehavior": "The upload of artifacts fails, and a NullPointerException is thrown, preventing topology submission.",
            "Suggestions": "Implement a null check for the 'name' field in the 'fixACLsForUser' method to avoid the NPE. If 'name' is null, the method should skip the comparison or handle it appropriately.",
            "problem_location": {
                "files": [
                    "storm-client/src/jvm/org/apache/storm/blobstore/BlobStoreAclHandler.java",
                    "storm-server/src/main/java/org/apache/storm/blobstore/LocalFsBlobStore.java"
                ],
                "classes": [
                    "org.apache.storm.blobstore.BlobStoreAclHandler",
                    "org.apache.storm.blobstore.LocalFsBlobStore"
                ],
                "methods": [
                    "BlobStoreAclHandler.fixACLsForUser",
                    "BlobStoreAclHandler.normalizeSettableACLs",
                    "LocalFsBlobStore.createBlob"
                ]
            },
            "possible_fix": "In the 'fixACLsForUser' method, add a null check for 'control.get_name()' before calling 'equals(user)'. For example:\n\nif (control.get_name() != null && control.get_name().equals(user)) {\n    // existing logic\n}"
        }
    },
    {
        "filename": "STORM-3124.json",
        "creation_time": "2018-06-27T13:28:01.000+0000",
        "bug_report": {
            "Title": "Failures talking to Pacemaker",
            "Description": "The system is experiencing intermittent failures when attempting to communicate with the Pacemaker service. The logs indicate repeated timeouts and null responses when trying to write to a channel, leading to a failure in launching topologies. The root cause appears to be a failure to connect to the Pacemaker server, which is critical for managing worker heartbeats and topology submissions.",
            "StackTrace": [
                "2018-06-25 20:21:05.220 o.a.s.p.PacemakerClient timer [ERROR] Not getting response or getting null response. Making 7 more attempts.",
                "2018-06-25 20:21:06.220 o.a.s.p.PacemakerClient timer [ERROR] error attempting to write to a channel Timed out waiting for channel ready..",
                "2018-06-25 20:21:13.224 o.a.s.p.PacemakerClientPool timer [WARN] Failed to connect to the pacemaker server openqe74blue-n2.blue.ygrid.yahoo.com",
                "java.lang.RuntimeException: java.lang.RuntimeException: org.apache.storm.pacemaker.PacemakerConnectionException: Failed to connect to any Pacemaker.",
                "Caused by: org.apache.storm.pacemaker.PacemakerConnectionException: Failed to connect to any Pacemaker."
            ],
            "RootCause": "The root cause of the issue is a failure to connect to the Pacemaker server, as indicated by the repeated 'Failed to connect to any Pacemaker' exceptions. This is likely due to network issues, server unavailability, or misconfiguration.",
            "StepsToReproduce": [
                "Attempt to launch a topology that requires communication with the Pacemaker service.",
                "Monitor the logs for connection attempts to the Pacemaker server.",
                "Observe the repeated timeout and null response errors."
            ],
            "ExpectedBehavior": "The system should successfully connect to the Pacemaker server and manage worker heartbeats without errors, allowing for the successful launch of topologies.",
            "ObservedBehavior": "The system fails to connect to the Pacemaker server, resulting in repeated timeouts and null responses, which prevents the launch of topologies.",
            "Suggestions": "Check the network connectivity to the Pacemaker server. Ensure that the server is running and accessible. Review the configuration settings for the Pacemaker client to ensure they are correct. Consider implementing a retry mechanism with exponential backoff for connection attempts.",
            "problem_location": {
                "files": [
                    "storm-client/src/jvm/org/apache/storm/pacemaker/PacemakerClientPool.java",
                    "storm-client/src/jvm/org/apache/storm/cluster/PaceMakerStateStorage.java",
                    "storm-client/src/jvm/org/apache/storm/cluster/StormClusterStateImpl.java"
                ],
                "classes": [
                    "org.apache.storm.pacemaker.PacemakerClientPool",
                    "org.apache.storm.cluster.PaceMakerStateStorage",
                    "org.apache.storm.cluster.StormClusterStateImpl"
                ],
                "methods": [
                    "PacemakerClientPool.sendAll",
                    "PaceMakerStateStorage.get_worker_hb_children",
                    "StormClusterStateImpl.heartbeatStorms"
                ]
            },
            "possible_fix": "To address the connection issue, verify the Pacemaker server's status and network configuration. If the server is down, restart it. If the configuration is incorrect, update the settings in the Pacemaker client to point to the correct server address. Additionally, consider adding error handling in the 'sendAll' method to manage connection retries more effectively."
        }
    },
    {
        "filename": "STORM-2095.json",
        "creation_time": "2016-09-14T16:00:30.000+0000",
        "bug_report": {
            "Title": "Nimbus dies and never recovers due to java.nio.file.DirectoryNotEmptyException",
            "Description": "When a blobstore key is created for a large file and Nimbus is restarted during the creation process, Nimbus fails to initialize due to a DirectoryNotEmptyException. This occurs because Nimbus attempts to delete a directory that still contains files, leading to a failure in the blobstore cleanup process.",
            "StackTrace": [
                "java.lang.RuntimeException: java.nio.file.DirectoryNotEmptyException: /opt/storm/storm-local/blobs/955/some_big_file",
                "at org.apache.storm.blobstore.LocalFsBlobStore.deleteBlob(LocalFsBlobStore.java:229)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:497)",
                "at clojure.lang.Reflector.invokeMatchingMethod(Reflector.java:93)",
                "at clojure.lang.Reflector.invokeInstanceMethod(Reflector.java:28)",
                "at org.apache.storm.daemon.nimbus$setup_blobstore.invoke(nimbus.clj:1196)",
                "at org.apache.storm.daemon.nimbus$fn__7064$exec_fn__2461__auto____7065.invoke(nimbus.clj:1416)",
                "at clojure.lang.AFn.applyToHelper(AFn.java:156)",
                "at clojure.lang.AFn.applyTo(AFn.java:144)",
                "at clojure.core$apply.invoke(core.clj:630)",
                "at org.apache.storm.daemon.nimbus$fn__7064$service_handler__7308.doInvoke(nimbus.clj:1358)",
                "at clojure.lang.RestFn.invoke(RestFn.java:421)",
                "at org.apache.storm.daemon.nimbus$launch_server_BANG_.invoke(nimbus.clj:2206)",
                "at org.apache.storm.daemon.nimbus$_launch.invoke(nimbus.clj:2239)",
                "at org.apache.storm.daemon.nimbus$_main.invoke(nimbus.clj:2262)",
                "at clojure.lang.AFn.applyToHelper(AFn.java:152)",
                "at clojure.lang.AFn.applyTo(AFn.java:144)",
                "at org.apache.storm.daemon.nimbus.main(Unknown Source)",
                "Caused by: java.nio.file.DirectoryNotEmptyException: /opt/storm/storm-local/blobs/955/some_big_file",
                "at sun.nio.fs.UnixFileSystemProvider.implDelete(UnixFileSystemProvider.java:242)",
                "at sun.nio.fs.AbstractFileSystemProvider.deleteIfExists(AbstractFileSystemProvider.java:108)",
                "at java.nio.file.Files.deleteIfExists(Files.java:1165)",
                "at org.apache.storm.blobstore.FileBlobStoreImpl.delete(FileBlobStoreImpl.java:239)",
                "at org.apache.storm.blobstore.FileBlobStoreImpl.deleteKey(FileBlobStoreImpl.java:178)",
                "at org.apache.storm.blobstore.LocalFsBlobStore.deleteBlob(LocalFsBlobStore.java:226)"
            ],
            "RootCause": "The root cause of the issue is that Nimbus attempts to delete a directory that is not empty when it restarts, leading to a DirectoryNotEmptyException. This happens because the blobstore cleanup process does not account for the possibility of partial blob creation.",
            "StepsToReproduce": [
                "1) Create a blobstore key for a large file (1 or 2 GB).",
                "2) While the blob is being created, restart Nimbus.",
                "3) Observe that Nimbus fails to start due to DirectoryNotEmptyException."
            ],
            "ExpectedBehavior": "Partial blobstore key is deleted cleanly and does not affect Nimbus.",
            "ObservedBehavior": "Nimbus fails to initialize and keeps dying due to DirectoryNotEmptyException.",
            "Suggestions": "Implement a check to ensure that the directory is empty before attempting to delete it. Alternatively, handle the DirectoryNotEmptyException gracefully to allow Nimbus to recover.",
            "problem_location": {
                "files": [
                    "storm-core/src/jvm/org/apache/storm/blobstore/LocalFsBlobStore.java",
                    "storm-core/src/jvm/org/apache/storm/blobstore/FileBlobStoreImpl.java"
                ],
                "classes": [
                    "org.apache.storm.blobstore.LocalFsBlobStore",
                    "org.apache.storm.blobstore.FileBlobStoreImpl"
                ],
                "methods": [
                    "LocalFsBlobStore.deleteBlob",
                    "FileBlobStoreImpl.delete",
                    "FileBlobStoreImpl.deleteKey"
                ]
            },
            "possible_fix": "In the deleteBlob method of LocalFsBlobStore, add a check to ensure that the directory is empty before calling deleteKey. If the directory is not empty, log a warning and skip the deletion to allow Nimbus to start successfully."
        }
    },
    {
        "filename": "STORM-2847.json",
        "creation_time": "2017-12-07T16:51:01.000+0000",
        "bug_report": {
            "Title": "Exception thrown after rebalance IllegalArgumentException",
            "Description": "The issue arises when the storm-kafka-client spout attempts to check the position of Kafka partitions that are no longer assigned to the current spout instance after a rebalance operation. This leads to an IllegalArgumentException being thrown, indicating that the consumer is trying to access the position of partitions that it does not own. This typically occurs in a topology with multiple spout instances where partition assignments can change dynamically.",
            "StackTrace": [
                "java.lang.IllegalArgumentException: You can only check the position for partitions assigned to this consumer.",
                "at org.apache.kafka.clients.consumer.KafkaConsumer.position(KafkaConsumer.java:1262)",
                "at org.apache.storm.kafka.spout.KafkaSpout.commitOffsetsForAckedTuples(KafkaSpout.java:473)"
            ],
            "RootCause": "The root cause of the issue is that the Kafka consumer is attempting to call the 'position' method on partitions that are not currently assigned to it after a rebalance, leading to an IllegalArgumentException.",
            "StepsToReproduce": [
                "1. Set up a Storm topology with multiple spout instances consuming from a Kafka topic.",
                "2. Trigger a rebalance by adding or removing spout instances.",
                "3. Ensure that the spouts attempt to commit offsets for tuples that may belong to partitions that are no longer assigned to them."
            ],
            "ExpectedBehavior": "The spout should only attempt to check the position of partitions that are currently assigned to it, avoiding any IllegalArgumentException.",
            "ObservedBehavior": "An IllegalArgumentException is thrown when the spout tries to check the position of partitions that are not assigned to the consumer after a rebalance.",
            "Suggestions": "Implement a check to ensure that the consumer only calls the 'position' method for partitions that are currently assigned to it. This can be done by maintaining a list of assigned partitions and validating against it before making the call.",
            "problem_location": {
                "files": [
                    "KafkaSpout.java"
                ],
                "classes": [
                    "org.apache.storm.kafka.spout.KafkaSpout"
                ],
                "methods": [
                    "commitOffsetsForAckedTuples"
                ]
            },
            "possible_fix": "Modify the 'commitOffsetsForAckedTuples' method to include a check for assigned partitions before calling 'kafkaConsumer.position(tp)'. For example:\n\nif (kafkaConsumer.assignment().contains(tp)) {\n    long position = kafkaConsumer.position(tp);\n    // Proceed with the rest of the logic\n}"
        }
    },
    {
        "filename": "STORM-1114.json",
        "creation_time": "2015-10-15T15:41:36.000+0000",
        "bug_report": {
            "Title": "Race Condition in Trident Zookeeper zk-node Create/Delete",
            "Description": "In a production environment, certain Trident topologies are encountering a race condition where multiple workers attempt to create a Zookeeper node that already exists or delete a node that has already been removed. This leads to exceptions being thrown, causing the worker processes to terminate unexpectedly. The issue arises from concurrent access to the Zookeeper API for node creation and deletion without proper synchronization.",
            "StackTrace": [
                "Caused by: org.apache.storm.shade.org.apache.zookeeper.KeeperException$NodeExistsException: KeeperErrorCode = NodeExists for /ignoreStoredMetadata",
                "at org.apache.storm.shade.org.apache.zookeeper.KeeperException.create(KeeperException.java:119) ~[storm-core-0.10.1.y.jar:0.10.1.y]",
                "at org.apache.storm.shade.org.apache.zookeeper.KeeperException.create(KeeperException.java:51) ~[storm-core-0.10.1.y.jar:0.10.1.y]",
                "at org.apache.storm.shade.org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:783) ~[storm-core-0.10.1.y.jar:0.10.1.y]",
                "at org.apache.storm.shade.org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:676) ~[storm-core-0.10.1.y.jar:0.10.1.y]",
                "at org.apache.storm.shade.org.apache.curator.framework.imps.CreateBuilderImpl$11.call(CreateBuilderImpl.java:660) ~[storm-core-0.10.1.y.jar:0.10.1.y]",
                "at org.apache.storm.shade.org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107) ~[storm-core-0.10.1.y.jar:0.10.1.y]",
                "at org.apache.storm.shade.org.apache.curator.framework.imps.CreateBuilderImpl.pathInForeground(CreateBuilderImpl.java:656) ~[storm-core-0.10.1.y.jar:0.10.1.y]",
                "at org.apache.storm.shade.org.apache.curator.framework.imps.CreateBuilderImpl.protectedPathInForeground(CreateBuilderImpl.java:441) ~[storm-core-0.10.1.y.jar:0.10.1.y]",
                "at org.apache.storm.shade.org.apache.curator.framework.imps.CreateBuilderImpl.forPath(CreateBuilderImpl.java:431) ~[storm-core-0.10.1.y.jar:0.10.1.y]",
                "at org.apache.storm.shade.org.apache.curator.framework.imps.CreateBuilderImpl$3.forPath(CreateBuilderImpl.java:239) ~[storm-core-0.10.1.y.jar:0.10.1.y]",
                "at org.apache.storm.shade.org.apache.curator.framework.imps.CreateBuilderImpl$3.forPath(CreateBuilderImpl.java:193) ~[storm-core-0.10.1.y.jar:0.10.1.y]",
                "at storm.trident.topology.state.TransactionalState.forPath(TransactionalState.java:83) ~[storm-core-0.10.1.y.jar:0.10.1.y]",
                "at storm.trident.topology.state.TransactionalState.createNode(TransactionalState.java:100) ~[storm-core-0.10.1.y.jar:0.10.1.y]",
                "at storm.trident.topology.state.TransactionalState.setData(TransactionalState.java:115) ~[storm-core-0.10.1.y.jar:0.10.1.y]",
                "Caused by: org.apache.storm.shade.org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /rainbowHdfsPath",
                "at org.apache.storm.shade.org.apache.zookeeper.KeeperException.create(KeeperException.java:111) ~[storm-core-0.10.1.y.jar:0.10.1.y]",
                "at org.apache.storm.shade.org.apache.zookeeper.KeeperException.create(KeeperException.java:51) ~[storm-core-0.10.1.y.jar:0.10.1.y]",
                "at org.apache.storm.shade.org.apache.zookeeper.ZooKeeper.delete(ZooKeeper.java:873) ~[storm-core-0.10.1.y.jar:0.10.1.y]",
                "at org.apache.storm.shade.org.apache.curator.framework.imps.DeleteBuilderImpl$5.call(DeleteBuilderImpl.java:239) ~[storm-core-0.10.1.y.jar:0.10.1.y]",
                "at org.apache.storm.shade.org.apache.curator.framework.imps.DeleteBuilderImpl$5.call(DeleteBuilderImpl.java:234) ~[storm-core-0.10.1.y.jar:0.10.1.y]",
                "at org.apache.storm.shade.org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:107) ~[storm-core-0.10.1.y.jar:0.10.1.y]",
                "at org.apache.storm.shade.org.apache.curator.framework.imps.DeleteBuilderImpl.pathInForeground(DeleteBuilderImpl.java:230) ~[storm-core-0.10.1.y.jar:0.10.1.y]",
                "at org.apache.storm.shade.org.apache.curator.framework.imps.DeleteBuilderImpl.forPath(DeleteBuilderImpl.java:215) ~[storm-core-0.10.1.y.jar:0.10.1.y]",
                "at org.apache.storm.shade.org.apache.curator.framework.imps.DeleteBuilderImpl.forPath(DeleteBuilderImpl.java:42) ~[storm-core-0.10.1.y.jar:0.10.1.y]",
                "at storm.trident.topology.state.TransactionalState.delete(TransactionalState.java:126) ~[storm-core-0.10.1.y.jar:0.10.1.y]"
            ],
            "RootCause": "The root cause of the issue is a race condition in the handling of Zookeeper node creation and deletion within the Trident framework. Multiple workers are concurrently trying to create or delete the same Zookeeper nodes without proper synchronization, leading to exceptions being thrown.",
            "StepsToReproduce": [
                "Deploy a Trident topology that utilizes Zookeeper for state management.",
                "Simulate multiple workers attempting to create and delete the same Zookeeper nodes concurrently.",
                "Observe the worker logs for exceptions related to node existence and deletion."
            ],
            "ExpectedBehavior": "Workers should be able to create and delete Zookeeper nodes without encountering exceptions, ensuring that the worker processes remain stable.",
            "ObservedBehavior": "Workers are crashing due to exceptions thrown when trying to create a node that already exists or delete a node that has already been deleted.",
            "Suggestions": "Implement synchronization mechanisms (e.g., using locks or Zookeeper's built-in features) to ensure that node creation and deletion operations are atomic and do not interfere with each other.",
            "problem_location": {
                "files": [
                    "TransactionalState.java"
                ],
                "classes": [
                    "storm.trident.topology.state.TransactionalState"
                ],
                "methods": [
                    "createNode",
                    "delete"
                ]
            },
            "possible_fix": "To fix the race condition, consider wrapping the create and delete operations in synchronized blocks or using Zookeeper's ephemeral nodes feature to manage node lifecycle more effectively. Additionally, implement checks before creating or deleting nodes to avoid unnecessary exceptions."
        }
    },
    {
        "filename": "STORM-2811.json",
        "creation_time": "2017-11-12T08:37:10.000+0000",
        "bug_report": {
            "Title": "Nimbus may throw NPE if the same topology is killed multiple times",
            "Description": "The issue arises when the Nimbus service attempts to kill a topology that has already been killed multiple times. The method `getTopoId` in `IStormClusterState` returns an `Optional<String>` that may be empty if the topology name does not match any active storms. If this empty value is not handled properly, it leads to a NullPointerException (NPE) when Nimbus tries to read the topology configuration.",
            "StackTrace": [
                "2017-11-12 08:45:50.353 o.a.s.d.n.Nimbus pool-14-thread-47 [WARN] Kill topology exception. (topology name='SlidingWindowTest-window20-slide10')",
                "java.lang.NullPointerException: null",
                "at org.apache.storm.cluster.IStormClusterState.getTopoId(IStormClusterState.java:171) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.daemon.nimbus.Nimbus.tryReadTopoConfFromName(Nimbus.java:1970) ~[storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.daemon.nimbus.Nimbus.killTopologyWithOpts(Nimbus.java:2760) ~[storm-server-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.generated.Nimbus$Processor$killTopologyWithOpts.getResult(Nimbus.java:3226) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.generated.Nimbus$Processor$killTopologyWithOpts.getResult(Nimbus.java:3210) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39) ~[libthrift-0.10.0.jar:0.10.0]",
                "at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39) ~[libthrift-0.10.0.jar:0.10.0]",
                "at org.apache.storm.security.auth.SimpleTransportPlugin$SimpleWrapProcessor.process(SimpleTransportPlugin.java:167) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.thrift.server.AbstractNonblockingServer$FrameBuffer.invoke(AbstractNonblockingServer.java:518) ~[libthrift-0.10.0.jar:0.10.0]",
                "at org.apache.thrift.server.Invocation.run(Invocation.java:18) ~[libthrift-0.10.0.jar:0.10.0]",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_144]",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_144]",
                "at java.lang.Thread.run(Thread.java:748) [?:1.8.0_144]"
            ],
            "RootCause": "The root cause of the NullPointerException is that the method `getTopoId` in `IStormClusterState` returns an empty Optional when the topology name does not match any active storms. This empty value is not checked before being used, leading to an NPE in the `tryReadTopoConfFromName` method.",
            "StepsToReproduce": [
                "1. Deploy a topology with a specific name.",
                "2. Kill the topology using the Nimbus service.",
                "3. Attempt to kill the same topology again multiple times."
            ],
            "ExpectedBehavior": "The Nimbus service should handle the case where a topology is already killed gracefully, without throwing a NullPointerException.",
            "ObservedBehavior": "The Nimbus service throws a NullPointerException when attempting to kill a topology that has already been killed.",
            "Suggestions": "Implement a check for the presence of a topology ID before proceeding with operations that assume its existence. Ensure that the `getTopoId` method is properly handled to avoid NPEs.",
            "problem_location": {
                "files": [
                    "IStormClusterState.java",
                    "Nimbus.java"
                ],
                "classes": [
                    "org.apache.storm.cluster.IStormClusterState",
                    "org.apache.storm.daemon.nimbus.Nimbus"
                ],
                "methods": [
                    "IStormClusterState.getTopoId",
                    "Nimbus.tryReadTopoConfFromName",
                    "Nimbus.killTopologyWithOpts"
                ]
            },
            "possible_fix": "In the `tryReadTopoConfFromName` method, add a check for the presence of the topology ID returned by `getTopoId`. If the ID is not present, log a warning and return early instead of proceeding with the operation that leads to the NPE."
        }
    },
    {
        "filename": "STORM-2903.json",
        "creation_time": "2018-01-19T17:10:01.000+0000",
        "bug_report": {
            "Title": "Fix possible NullPointerException in AbstractAutoCreds",
            "Description": "A NullPointerException is thrown during the execution of the Hive token mechanism, specifically in the method 'addTokensToUGI' of the AbstractAutoCreds class. This occurs when the 'subject' parameter passed to the 'populateSubject' method is null, leading to an attempt to access methods on a null reference.",
            "StackTrace": [
                "Caused by: java.lang.NullPointerException",
                "at org.apache.storm.common.AbstractAutoCreds.addTokensToUGI(AbstractAutoCreds.java:219)",
                "at org.apache.storm.common.AbstractAutoCreds.populateSubject(AbstractAutoCreds.java:118)",
                "at org.apache.storm.security.auth.AuthUtils.populateSubject(AuthUtils.java:228)",
                "... 10 more"
            ],
            "RootCause": "The root cause of the NullPointerException is that the 'subject' parameter in the 'populateSubject' method can be null, and if it is not initialized before being used, it leads to a null reference error.",
            "StepsToReproduce": [
                "Initialize the Hive token mechanism without providing a valid Subject.",
                "Call the method that triggers the population of the Subject.",
                "Observe the NullPointerException in the logs."
            ],
            "ExpectedBehavior": "The system should handle null subjects gracefully, either by initializing a new Subject or by providing a meaningful error message.",
            "ObservedBehavior": "A NullPointerException is thrown, causing the process to halt unexpectedly.",
            "Suggestions": "Add a null check for the 'subject' parameter in the 'populateSubject' method to ensure it is initialized before use.",
            "problem_location": {
                "files": [
                    "AbstractAutoCreds.java",
                    "AuthUtils.java"
                ],
                "classes": [
                    "org.apache.storm.common.AbstractAutoCreds",
                    "org.apache.storm.security.auth.AuthUtils"
                ],
                "methods": [
                    "AbstractAutoCreds.addTokensToUGI",
                    "AbstractAutoCreds.populateSubject",
                    "AuthUtils.populateSubject"
                ]
            },
            "possible_fix": "In the 'populateSubject' method, ensure that the 'subject' is initialized if it is null. For example:\n\n```java\nif (subject == null) {\n    subject = new Subject();\n}\n```"
        }
    },
    {
        "filename": "STORM-3168.json",
        "creation_time": "2018-08-01T19:31:42.000+0000",
        "bug_report": {
            "Title": "AsyncLocalizer cleanup appears to crash",
            "Description": "The AsyncLocalizer is failing to log cleanup messages and appears to crash due to an unhandled exception when attempting to download blobs. The issue arises from a KeyNotFoundException when the system tries to retrieve metadata for a blob that does not exist, leading to a RuntimeException that is not properly managed, causing the cleanup process to halt.",
            "StackTrace": [
                "java.util.concurrent.ExecutionException: java.lang.RuntimeException: Could not download...",
                "at java.util.concurrent.CompletableFuture.reportGet(CompletableFuture.java:357)",
                "at java.util.concurrent.CompletableFuture.get(CompletableFuture.java:1895)",
                "at org.apache.storm.localizer.AsyncLocalizer.updateBlobs(AsyncLocalizer.java:303)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)",
                "at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:308)",
                "at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:180)",
                "at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:294)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "at java.lang.Thread.run(Thread.java:748)",
                "Caused by: java.lang.RuntimeException: Could not download...",
                "at org.apache.storm.localizer.AsyncLocalizer.lambda$downloadOrUpdate$69(AsyncLocalizer.java:268)",
                "Caused by: org.apache.storm.generated.KeyNotFoundException",
                "at org.apache.storm.generated.Nimbus$getBlobMeta_result$getBlobMeta_resultStandardScheme.read(Nimbus.java:25853)",
                "at org.apache.storm.generated.Nimbus$getBlobMeta_result.getBlobMeta_resultStandardScheme.read(Nimbus.java:25821)",
                "at org.apache.storm.generated.Nimbus$getBlobMeta_result.read(Nimbus.java:25752)",
                "at org.apache.storm.thrift.TServiceClient.receiveBase(TServiceClient.java:88)",
                "at org.apache.storm.generated.Nimbus$Client.recv_getBlobMeta(Nimbus.java:798)",
                "at org.apache.storm.generated.Nimbus$Client.getBlobMeta(Nimbus.java:785)",
                "at org.apache.storm.blobstore.NimbusBlobStore.getBlobMeta(NimbusBlobStore.java:85)",
                "at org.apache.storm.localizer.LocallyCachedTopologyBlob.getRemoteVersion(LocallyCachedTopologyBlob.java:122)",
                "at org.apache.storm.localizer.AsyncLocalizer.lambda$downloadOrUpdate$69(AsyncLocalizer.java:252)"
            ],
            "RootCause": "The root cause of the issue is a KeyNotFoundException thrown when the AsyncLocalizer attempts to retrieve metadata for a blob that does not exist, which leads to a RuntimeException that is not handled properly.",
            "StepsToReproduce": [
                "Start the supervisor with blobstore downloads enabled.",
                "Monitor the logs for cleanup messages.",
                "Wait for the cleanup process to run and observe the logs for errors."
            ],
            "ExpectedBehavior": "The AsyncLocalizer should log cleanup messages every 30 seconds and handle any exceptions gracefully without crashing.",
            "ObservedBehavior": "The AsyncLocalizer fails to log cleanup messages and crashes due to unhandled exceptions, leading to repeated error messages in the logs.",
            "Suggestions": "Implement proper exception handling in the AsyncLocalizer to manage KeyNotFoundException and ensure that the cleanup process continues running even when errors occur.",
            "problem_location": {
                "files": [
                    "storm-server/src/main/java/org/apache/storm/localizer/AsyncLocalizer.java",
                    "storm-client/src/jvm/org/apache/storm/blobstore/NimbusBlobStore.java"
                ],
                "classes": [
                    "org.apache.storm.localizer.AsyncLocalizer",
                    "org.apache.storm.blobstore.NimbusBlobStore"
                ],
                "methods": [
                    "AsyncLocalizer.updateBlobs",
                    "NimbusBlobStore.getBlobMeta"
                ]
            },
            "possible_fix": "In the AsyncLocalizer.updateBlobs method, add a specific catch block for KeyNotFoundException to log the error and continue the cleanup process without crashing. For example:\n\n```java\ntry {\n    f.get();\n} catch (KeyNotFoundException e) {\n    LOG.error(\"Blob not found, continuing cleanup process\", e);\n} catch (Exception e) {\n    // existing error handling\n}\n```"
        }
    },
    {
        "filename": "STORM-2986.json",
        "creation_time": "2018-03-05T21:41:24.000+0000",
        "bug_report": {
            "Title": "NPE from LogCleaner",
            "Description": "The LogCleaner encounters a NullPointerException (NPE) when attempting to clean up old logs due to the absence of the 'workers-artifacts' directory. This issue arises when the LogCleaner tries to list files in a directory that does not exist, leading to a failure in the 'selectDirsForCleanup' method.",
            "StackTrace": [
                "java.lang.NullPointerException: null",
                "at java.util.Arrays.stream(Arrays.java:5004) ~[?:1.8.0_131]",
                "at org.apache.storm.daemon.logviewer.utils.LogCleaner.selectDirsForCleanup(LogCleaner.java:217) ~[storm-webapp-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.daemon.logviewer.utils.LogCleaner.run(LogCleaner.java:135) ~[storm-webapp-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.StormTimer$1.run(StormTimer.java:207) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]",
                "at org.apache.storm.StormTimer$StormTimerTask.run(StormTimer.java:81) ~[storm-client-2.0.0-SNAPSHOT.jar:2.0.0-SNAPSHOT]"
            ],
            "RootCause": "The root cause of the NullPointerException is that the LogCleaner attempts to list files in a directory that does not exist ('workers-artifacts'), which results in a null reference being passed to the Arrays.stream method.",
            "StepsToReproduce": [
                "Set the configuration 'logviewer.cleanup.interval.secs' to 10.",
                "Start the LogCleaner thread without creating the 'workers-artifacts' directory.",
                "Observe the log output for the NullPointerException."
            ],
            "ExpectedBehavior": "The LogCleaner should handle the absence of the 'workers-artifacts' directory gracefully, either by skipping the cleanup process or by creating the directory if it does not exist.",
            "ObservedBehavior": "The LogCleaner throws a NullPointerException when it attempts to clean up logs due to the missing 'workers-artifacts' directory.",
            "Suggestions": "Implement a check in the 'selectDirsForCleanup' method to verify if 'logRootDir' and its subdirectories exist before attempting to list files. If the directory does not exist, log a warning and return an empty set instead of proceeding with the cleanup.",
            "problem_location": {
                "files": [
                    "LogCleaner.java"
                ],
                "classes": [
                    "org.apache.storm.daemon.logviewer.utils.LogCleaner"
                ],
                "methods": [
                    "LogCleaner.selectDirsForCleanup",
                    "LogCleaner.run"
                ]
            },
            "possible_fix": "In the 'selectDirsForCleanup' method, add a check to ensure 'logRootDir' is not null and exists before calling 'listFiles'. If it does not exist, log a warning and return an empty set:\n\n```java\nSet<File> selectDirsForCleanup(long nowMillis) {\n    if (logRootDir == null || !logRootDir.exists()) {\n        LOG.warn(\"Log root directory does not exist: {}\", logRootDir);\n        return Collections.emptySet();\n    }\n    FileFilter fileFilter = mkFileFilterForLogCleanup(nowMillis);\n    return Arrays.stream(logRootDir.listFiles())\n            .flatMap(topoDir -> Arrays.stream(topoDir.listFiles(fileFilter)))\n            .collect(toCollection(TreeSet::new));\n}\n```"
        }
    },
    {
        "filename": "STORM-2197.json",
        "creation_time": "2016-11-10T03:57:30.000+0000",
        "bug_report": {
            "Title": "NimbusClient connections leak due to leakage in ThriftClient.",
            "Description": "The Nimbus client is failing to close connections properly when encountering errors during the connection process to Nimbus. Specifically, the TSocket instance created in the ThriftClient is not being closed in the event of a connection failure, leading to resource leaks. This issue is highlighted by the stack trace indicating a failure in the SaslClientTransport during session initiation, which results in an unhandled exception and subsequent connection leaks.",
            "StackTrace": [
                "org.apache.thrift7.transport.TTransportException: Peer indicated failure: GSS initiate failed",
                "at org.apache.thrift7.transport.TSaslTransport.receiveSaslMessage(TSaslTransport.java:199)",
                "at org.apache.thrift7.transport.TSaslTransport.open(TSaslTransport.java:277)",
                "at org.apache.thrift7.transport.TSaslClientTransport.open(TSaslClientTransport.java:37)",
                "at backtype.storm.security.auth.kerberos.KerberosSaslTransportPlugin$1.run(KerberosSaslTransportPlugin.java:145)",
                "at backtype.storm.security.auth.kerberos.KerberosSaslTransportPlugin$1.run(KerberosSaslTransportPlugin.java:141)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at backtype.storm.security.auth.kerberos.KerberosSaslTransportPlugin.connect(KerberosSaslTransportPlugin.java:140)",
                "at backtype.storm.security.auth.TBackoffConnect.doConnectWithRetry(TBackoffConnect.java:48)",
                "at backtype.storm.security.auth.ThriftClient.reconnect(ThriftClient.java:103)",
                "at backtype.storm.security.auth.ThriftClient.<init>(ThriftClient.java:72)",
                "at backtype.storm.utils.NimbusClient.<init>(NimbusClient.java:106)",
                "at backtype.storm.utils.NimbusClient.getConfiguredClientAs(NimbusClient.java:82)",
                "at backtype.storm.ui.core$nimbus_summary.invoke(core.clj:584)",
                "at backtype.storm.ui.core$fn__10334.invoke(core.clj:1009)",
                "at compojure.core$make_route$fn__7476.invoke(core.clj:93)",
                "at compojure.core$if_route$fn__7464.invoke(core.clj:39)",
                "at compojure.core$if_method$fn__7457.invoke(core.clj:24)",
                "at compojure.core$routing$fn__7482.invoke(core.clj:106)",
                "at clojure.core$some.invoke(core.clj:2515)"
            ],
            "RootCause": "The TSocket instance in the ThriftClient is not being closed when a connection error occurs, leading to resource leaks.",
            "StepsToReproduce": [
                "Attempt to connect to Nimbus with incorrect credentials or configuration.",
                "Observe the error logs indicating a failure in the SaslClientTransport.",
                "Check for open connections that should have been closed."
            ],
            "ExpectedBehavior": "The Nimbus client should properly close any open connections in the event of a connection failure, preventing resource leaks.",
            "ObservedBehavior": "Connections remain open even after a failure to connect, leading to potential resource exhaustion.",
            "Suggestions": "Implement proper error handling in the ThriftClient to ensure that the TSocket is closed in case of connection errors.",
            "problem_location": {
                "files": [
                    "ThriftClient.java",
                    "NimbusClient.java"
                ],
                "classes": [
                    "backtype.storm.security.auth.ThriftClient",
                    "backtype.storm.utils.NimbusClient"
                ],
                "methods": [
                    "ThriftClient.reconnect",
                    "ThriftClient.<init>",
                    "NimbusClient.<init>"
                ]
            },
            "possible_fix": "In the ThriftClient class, modify the reconnect method to include a finally block that closes the TSocket instance if it was created, ensuring it is closed on any exception."
        }
    },
    {
        "filename": "STORM-1596.json",
        "creation_time": "2016-03-02T23:42:56.000+0000",
        "bug_report": {
            "Title": "Multiple Subject sharing Kerberos TGT - causes services to fail",
            "Description": "The issue arises when multiple threads access the same Kerberos Subject, leading to a race condition where one thread can inadvertently destroy the ServiceTicket being used by another thread. This is particularly evident when running the BasicDRPCTopology with high parallelism in a secure cluster, resulting in SASL negotiation failures and GSS exceptions.",
            "StackTrace": [
                "2016-01-20 15:52:26.904 o.a.t.t.TSaslTransport [ERROR] SASL negotiation failure",
                "javax.security.sasl.SaslException: GSS initiate failed",
                "at com.sun.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:211) ~[?:1.8.0_40]",
                "at org.apache.thrift7.transport.TSaslClientTransport.handleSaslStartMessage(TSaslClientTransport.java:94) ~[storm-core-0.10.1.y.jar:0.10.1.y]",
                "at org.apache.thrift7.transport.TSaslTransport.open(TSaslTransport.java:271) [storm-core-0.10.1.y.jar:0.10.1.y]",
                "at org.apache.thrift7.transport.TSaslClientTransport.open(TSaslClientTransport.java:37) [storm-core-0.10.1.y.jar:0.10.1.y]",
                "at backtype.storm.security.auth.kerberos.KerberosSaslTransportPlugin$1.run(KerberosSaslTransportPlugin.java:195) [storm-core-0.10.1.y.jar:0.10.1.y]",
                "at backtype.storm.security.auth.kerberos.KerberosSaslTransportPlugin$1.run(KerberosSaslTransportPlugin.java:191) [storm-core-0.10.1.y.jar:0.10.1.y]",
                "at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_40]",
                "at javax.security.auth.Subject.doAs(Subject.java:422) [?:1.8.0_40]",
                "at backtype.storm.security.auth.kerberos.KerberosSaslTransportPlugin.connect(KerberosSaslTransportPlugin.java:190) [storm-core-0.10.1.y.jar:0.10.1.y]",
                "at backtype.storm.security.auth.TBackoffConnect.doConnectWithRetry(TBackoffConnect.java:54) [storm-core-0.10.1.y.jar:0.10.1.y]",
                "at backtype.storm.security.auth.ThriftClient.reconnect(ThriftClient.java:109) [storm-core-0.10.1.y.jar:0.10.1.y]",
                "at backtype.storm.drpc.DRPCInvocationsClient.reconnectClient(DRPCInvocationsClient.java:57) [storm-core-0.10.1.y.jar:0.10.1.y]",
                "at backtype.storm.drpc.ReturnResults.reconnectClient(ReturnResults.java:113) [storm-core-0.10.1.y.jar:0.10.1.y]",
                "at backtype.storm.drpc.ReturnResults.execute(ReturnResults.java:103) [storm-core-0.10.1.y.jar:0.10.1.y]",
                "at backtype.storm.daemon.executor$fn__6377$tuple_action_fn__6379.invoke(executor.clj:689) [storm-core-0.10.1.y.jar:0.10.1.y]",
                "at backtype.storm.daemon.executor$mk_task_receiver$fn__6301.invoke(executor.clj:448) [storm-core-0.10.1.y.jar:0.10.1.y]",
                "at backtype.storm.disruptor$clojure_handler$reify__6018.onEvent(disruptor.clj:40) [storm-core-0.10.1.y.jar:0.10.1.y]",
                "at backtype.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:437) [storm-core-0.10.1.y.jar:0.10.1.y]",
                "at backtype.storm.utils.DisruptorQueue.consumeBatchWhenAvailable(DisruptorQueue.java:416) [storm-core-0.10.1.y.jar:0.10.1.y]",
                "at backtype.storm.disruptor$consume_batch_when_available.invoke(disruptor.clj:73) [storm-core-0.10.1.y.jar:0.10.1.y]",
                "at backtype.storm.daemon.executor$fn__6377$fn__6390$fn__6441.invoke(executor.clj:801) [storm-core-0.10.1.y.jar:0.10.1.y]",
                "at backtype.storm.util$async_loop$fn__742.invoke(util.clj:482) [storm-core-0.10.1.y.jar:0.10.1]",
                "at clojure.lang.AFn.run(AFn.java:22) [clojure-1.6.0.jar:?]",
                "at java.lang.Thread.run(Thread.java:745) [?:1.8.0_40]"
            ],
            "RootCause": "The root cause of the issue is a race condition due to multiple threads accessing and modifying the same Kerberos Subject, leading to the destruction of the ServiceTicket in use by one thread by another thread.",
            "StepsToReproduce": [
                "Run BasicDRPCTopology with high parallelism in a secure cluster.",
                "Monitor the logs for SASL negotiation failures and GSS exceptions."
            ],
            "ExpectedBehavior": "Each thread should be able to access its own Kerberos Subject without interfering with others, allowing for successful SASL negotiation and service ticket usage.",
            "ObservedBehavior": "SASL negotiation failures occur, leading to GSS exceptions and service failures due to the destruction of ServiceTickets.",
            "Suggestions": "Implement thread-local storage for Kerberos Subjects to ensure that each thread has its own instance, preventing interference between threads.",
            "problem_location": {
                "files": [
                    "KerberosSaslTransportPlugin.java",
                    "TSaslTransport.java"
                ],
                "classes": [
                    "backtype.storm.security.auth.kerberos.KerberosSaslTransportPlugin",
                    "org.apache.thrift7.transport.TSaslTransport"
                ],
                "methods": [
                    "KerberosSaslTransportPlugin.connect",
                    "TSaslTransport.open"
                ]
            },
            "possible_fix": "Refactor the code to use ThreadLocal for the Kerberos Subject, ensuring that each thread has its own instance. For example:\n\n```java\nprivate static final ThreadLocal<Subject> threadLocalSubject = ThreadLocal.withInitial(() -> new Subject());\n```\n\nThis change will help isolate the Kerberos Subject for each thread, preventing the race condition."
        }
    },
    {
        "filename": "STORM-2142.json",
        "creation_time": "2016-10-10T04:42:01.000+0000",
        "bug_report": {
            "Title": "ReportErrorAndDie runs suicide function only when InterruptedException or InterruptedIOException is thrown",
            "Description": "The current implementation of the ReportErrorAndDie function behaves inconsistently compared to version 1.x. It is designed to invoke a suicide function only when InterruptedException or InterruptedIOException is thrown, which is contrary to the expected behavior. For other exceptions, it should log the error and not terminate the executor. This leads to unexpected termination of the async loop for the executor while allowing other processes to continue, which can cause instability in the system.",
            "StackTrace": [
                "java.lang.RuntimeException: java.lang.RuntimeException: java.lang.reflect.InvocationTargetException",
                "at org.apache.storm.utils.DisruptorQueue.consumeBatchToCursor(DisruptorQueue.java:468)",
                "Caused by: java.lang.reflect.InvocationTargetException",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:497)",
                "at org.codehaus.janino.ScriptEvaluator.evaluate(ScriptEvaluator.java:982)",
                "Caused by: java.lang.RuntimeException: Cannot convert null to int",
                "at org.apache.calcite.runtime.SqlFunctions.cannotConvert(SqlFunctions.java:1023)",
                "at org.apache.calcite.runtime.SqlFunctions.toInt(SqlFunctions.java:1134)"
            ],
            "RootCause": "The root cause of the issue lies in the inconsistent handling of exceptions in the ReportErrorAndDie function. Specifically, it incorrectly triggers a suicide function for InterruptedException and InterruptedIOException while failing to handle other exceptions appropriately, leading to unexpected behavior.",
            "StepsToReproduce": [
                "Trigger an exception in EvaluationFilter or EvaluationFunction that is not InterruptedException or InterruptedIOException.",
                "Observe the behavior of the async loop for the executor."
            ],
            "ExpectedBehavior": "The system should log the error and allow the async loop to continue running without invoking the suicide function for exceptions other than InterruptedException or InterruptedIOException.",
            "ObservedBehavior": "The async loop dies unexpectedly when exceptions other than InterruptedException or InterruptedIOException are thrown, leading to instability in the system.",
            "Suggestions": "Refactor the ReportErrorAndDie function to ensure that it only invokes the suicide function for InterruptedException or InterruptedIOException. For all other exceptions, it should log the error and allow the system to continue functioning.",
            "problem_location": {
                "files": [
                    "DisruptorQueue.java"
                ],
                "classes": [
                    "org.apache.storm.utils.DisruptorQueue"
                ],
                "methods": [
                    "consumeBatchToCursor"
                ]
            },
            "possible_fix": "Modify the ReportErrorAndDie function to check the type of exception thrown. If it is not InterruptedException or InterruptedIOException, log the error and do not call the suicide function. This can be achieved by adding a conditional check before invoking the suicide function."
        }
    },
    {
        "filename": "STORM-2400.json",
        "creation_time": "2017-03-08T04:32:34.000+0000",
        "bug_report": {
            "Title": "Intermittent failure in nimbus due to errors from LeaderLatch#getLeader()",
            "Description": "The issue arises from the method LeaderLatch#getLeader() throwing a KeeperException with the code NONODE intermittently. This occurs when a participant's ephemeral Zookeeper node is removed due to a closed connection or session timeout. The race condition happens when a participant node is retrieved, but by the time LeaderSelector#getLeader() is called, the node may have already been removed, leading to the exception. The current implementation does not retry on KeeperException with NoNode code, which is a missed opportunity to handle transient failures effectively.",
            "StackTrace": [
                "org.apache.storm.shade.org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /storm/leader-lock/_c_97c09eed-5bba-4ac8-a05f-abdc4e8e95cf-latch-0000000002",
                "at org.apache.storm.shade.org.apache.zookeeper.KeeperException.create(KeeperException.java:111)",
                "at org.apache.storm.shade.org.apache.zookeeper.KeeperException.create(KeeperException.java:51)",
                "at org.apache.storm.shade.org.apache.zookeeper.ZooKeeper.getData(ZooKeeper.java:1155)",
                "at org.apache.storm.shade.org.apache.curator.framework.imps.GetDataBuilderImpl$4.call(GetDataBuilderImpl.java:304)",
                "at org.apache.storm.shade.org.apache.curator.framework.imps.GetDataBuilderImpl$4.call(GetDataBuilderImpl.java:293)",
                "at org.apache.storm.shade.org.apache.curator.RetryLoop.callWithRetry(RetryLoop.java:108)",
                "at org.apache.storm.shade.org.apache.curator.framework.imps.GetDataBuilderImpl.pathInForeground(GetDataBuilderImpl.java:290)",
                "at org.apache.storm.shade.org.apache.curator.framework.imps.GetDataBuilderImpl.forPath(GetDataBuilderImpl.java:281)",
                "at org.apache.storm.shade.org.apache.curator.framework.imps.GetDataBuilderImpl.forPath(GetDataBuilderImpl.java:42)",
                "at org.apache.storm.shade.org.apache.curator.framework.recipes.leader.LeaderSelector.participantForPath(LeaderSelector.java:375)",
                "at org.apache.storm.shade.org.apache.curator.framework.recipes.leader.LeaderSelector.getLeader(LeaderSelector.java:346)",
                "at org.apache.storm.shade.org.apache.curator.framework.recipes.leader.LeaderLatch.getLeader(LeaderLatch.java:454)"
            ],
            "RootCause": "The root cause of the issue is a race condition where the ephemeral Zookeeper node for a participant is removed before the LeaderSelector#getLeader() method is invoked, resulting in a KeeperException with NoNode code.",
            "StepsToReproduce": [
                "Set up a Zookeeper cluster with ephemeral nodes.",
                "Create a LeaderLatch instance and register participants.",
                "Simulate a session timeout for one of the participants.",
                "Call LeaderLatch#getLeader() and observe the intermittent KeeperException with NoNode."
            ],
            "ExpectedBehavior": "The system should be able to handle transient failures and retry the operation when a KeeperException with NoNode code is encountered.",
            "ObservedBehavior": "The system throws a KeeperException with NoNode code and does not retry, leading to intermittent failures in obtaining the leader.",
            "Suggestions": "Implement a retry mechanism for KeeperException with NoNode code in the LeaderLatch#getLeader() method. Additionally, consider adding configuration options in CuratorFrameworkFactory.Builder to specify which KeeperException codes should trigger retries.",
            "problem_location": {
                "files": [
                    "LeaderLatch.java",
                    "LeaderSelector.java"
                ],
                "classes": [
                    "org.apache.curator.framework.recipes.leader.LeaderLatch",
                    "org.apache.curator.framework.recipes.leader.LeaderSelector"
                ],
                "methods": [
                    "LeaderLatch.getLeader",
                    "LeaderSelector.getLeader"
                ]
            },
            "possible_fix": "Modify the LeaderLatch#getLeader() method to include a retry mechanism for KeeperException with NoNode code. For example, wrap the call to LeaderSelector#getLeader() in a retry loop that checks for NoNode exceptions and retries a specified number of times before failing."
        }
    },
    {
        "filename": "STORM-3084.json",
        "creation_time": "2018-05-24T20:45:32.000+0000",
        "bug_report": {
            "Title": "2.x NPE on Nimbus startup",
            "Description": "The Nimbus server fails to start due to a NullPointerException (NPE) occurring during the processing of events. The error is triggered when the Nimbus attempts to read supervisor details, which indicates that there may be an uninitialized or improperly configured component in the system. This leads to a cascading failure that halts the process.",
            "StackTrace": [
                "java.lang.RuntimeException: java.lang.NullPointerException at org.apache.storm.daemon.nimbus.Nimbus.lambda$launchServer$37(Nimbus.java:2685)",
                "at org.apache.storm.StormTimer$1.run(StormTimer.java:111)",
                "at org.apache.storm.StormTimer$StormTimerTask.run(StormTimer.java:227)",
                "Caused by: java.lang.NullPointerException at org.apache.storm.daemon.nimbus.Nimbus.readAllSupervisorDetails(Nimbus.java:1814)",
                "at org.apache.storm.daemon.nimbus.Nimbus.computeNewSchedulerAssignments(Nimbus.java:1906)",
                "at org.apache.storm.daemon.nimbus.Nimbus.mkAssignments(Nimbus.java:2057)",
                "at org.apache.storm.daemon.nimbus.Nimbus.mkAssignments(Nimbus.java:2003)",
                "at org.apache.storm.daemon.nimbus.Nimbus.lambda$launchServer$37(Nimbus.java:2681)",
                "at org.apache.storm.utils.Utils.exitProcess(Utils.java:469)",
                "at org.apache.storm.daemon.nimbus.Nimbus.lambda$new$17(Nimbus.java:484)",
                "at org.apache.storm.StormTimer$StormTimerTask.run(StormTimer.java:252)"
            ],
            "RootCause": "The root cause of the issue is a NullPointerException occurring in the readAllSupervisorDetails method of the Nimbus class, likely due to missing or uninitialized supervisor information.",
            "StepsToReproduce": [
                "Start the Nimbus server with the command: `storm nimbus`.",
                "Observe the logs for any errors during startup."
            ],
            "ExpectedBehavior": "The Nimbus server should start successfully without any errors, and all supervisor details should be read correctly.",
            "ObservedBehavior": "The Nimbus server fails to start and logs a NullPointerException, leading to a process halt.",
            "Suggestions": "Ensure that all necessary configurations for supervisors are correctly set up before starting the Nimbus server. Additionally, add null checks in the readAllSupervisorDetails method to handle cases where supervisor information may not be available.",
            "problem_location": {
                "files": [
                    "Nimbus.java",
                    "Utils.java"
                ],
                "classes": [
                    "org.apache.storm.daemon.nimbus.Nimbus",
                    "org.apache.storm.utils.Utils"
                ],
                "methods": [
                    "Nimbus.readAllSupervisorDetails",
                    "Nimbus.computeNewSchedulerAssignments",
                    "Nimbus.mkAssignments",
                    "Utils.exitProcess"
                ]
            },
            "possible_fix": "Add null checks in the readAllSupervisorDetails method to prevent NullPointerExceptions. For example:\n\npublic void readAllSupervisorDetails() {\n    if (supervisorDetails == null) {\n        LOG.error(\"Supervisor details are not initialized.\");\n        return;\n    }\n    // existing logic to read supervisor details\n}"
        }
    },
    {
        "filename": "STORM-3118.json",
        "creation_time": "2018-06-21T13:46:08.000+0000",
        "bug_report": {
            "Title": "Netty incompatibilities with Pacemaker",
            "Description": "The application encounters an EncoderException due to an IndexOutOfBoundsException when attempting to write data to a Netty ByteBuf. The error occurs because the writer index exceeds the maximum capacity of the buffer, which prevents the successful encoding of messages. This issue arises during the channel activation process in the KerberosSaslClientHandler, leading to a failure in topology submission.",
            "StackTrace": [
                "org.apache.storm.shade.io.netty.handler.codec.EncoderException: java.lang.IndexOutOfBoundsException: writerIndex(713) + minWritableBytes(2) exceeds maxCapacity(713): UnpooledHeapByteBuf(ridx: 0, widx: 713, cap: 713/713)",
                "at org.apache.storm.shade.io.netty.handler.codec.MessageToMessageEncoder.write(MessageToMessageEncoder.java:106)",
                "at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.invokeWrite0(AbstractChannelHandlerContext.java:738)",
                "at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.invokeWriteAndFlush(AbstractChannelHandlerContext.java:801)",
                "at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.write(AbstractChannelHandlerContext.java:814)",
                "at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.writeAndFlush(AbstractChannelHandlerContext.java:794)",
                "at org.apache.storm.shade.io.netty.channel.DefaultChannelPipeline.writeAndFlush(DefaultChannelPipeline.java:1066)",
                "at org.apache.storm.shade.io.netty.channel.AbstractChannel.writeAndFlush(AbstractChannel.java:305)",
                "at org.apache.storm.messaging.netty.KerberosSaslClientHandler.channelActive(KerberosSaslClientHandler.java:65)",
                "at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.invokeChannelActive(AbstractChannelHandlerContext.java:213)",
                "at org.apache.storm.shade.io.netty.channel.AbstractChannelHandlerContext.fireChannelActive(AbstractChannelHandlerContext.java:192)",
                "at org.apache.storm.shade.io.netty.channel.DefaultChannelPipeline.fireChannelActive(DefaultChannelPipeline.java:941)",
                "at org.apache.storm.shade.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.fulfillConnectPromise(AbstractNioChannel.java:311)",
                "at org.apache.storm.shade.io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:341)",
                "at org.apache.storm.shade.io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:635)",
                "at org.apache.storm.shade.io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:461)",
                "at java.lang.Thread.run(Thread.java:748)"
            ],
            "RootCause": "The root cause of the issue is that the SaslMessageToken is attempting to write data to a Netty ByteBuf that has reached its maximum capacity. The method 'write' in SaslMessageToken does not check if there is enough writable space in the ByteBuf before writing, leading to an IndexOutOfBoundsException.",
            "StepsToReproduce": [
                "1. Start the Nimbus server.",
                "2. Attempt to submit a topology that uses Pacemaker.",
                "3. Observe the error in the logs indicating an EncoderException."
            ],
            "ExpectedBehavior": "The topology should be submitted successfully without any exceptions, and the messages should be encoded and sent over the network without exceeding buffer limits.",
            "ObservedBehavior": "The application throws an EncoderException due to an IndexOutOfBoundsException when trying to write to a full ByteBuf, preventing the topology from being submitted.",
            "Suggestions": "Implement a check in the SaslMessageToken.write method to ensure that there is enough writable space in the ByteBuf before attempting to write data. Additionally, consider increasing the buffer size or managing the buffer lifecycle more effectively to prevent reaching maximum capacity.",
            "problem_location": {
                "files": [
                    "storm-client/src/jvm/org/apache/storm/messaging/netty/SaslMessageToken.java",
                    "storm-client/src/jvm/org/apache/storm/pacemaker/codec/ThriftEncoder.java"
                ],
                "classes": [
                    "org.apache.storm.messaging.netty.SaslMessageToken",
                    "org.apache.storm.pacemaker.codec.ThriftEncoder"
                ],
                "methods": [
                    "SaslMessageToken.write",
                    "ThriftEncoder.encodeNettySerializable"
                ]
            },
            "possible_fix": "In the SaslMessageToken.write method, add a check for the writable space in the ByteBuf before writing data:\n\n```java\nif (dest.writableBytes() < (2 + payload_len)) {\n    throw new IndexOutOfBoundsException(\"Not enough writable space in ByteBuf\");\n}\n```"
        }
    },
    {
        "filename": "STORM-2158.json",
        "creation_time": "2016-10-20T12:56:58.000+0000",
        "bug_report": {
            "Title": "OutOfMemoryError in Nimbus' SimpleTransportPlugin",
            "Description": "The Nimbus server encounters an OutOfMemoryError when a malformed Thrift request is sent. This issue arises due to the absence of a specified maximum read buffer size in the THsHaServer configuration, leading to excessive memory consumption when handling requests.",
            "StackTrace": [
                "java.lang.OutOfMemoryError: Java heap space",
                "at java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:57)",
                "at java.nio.ByteBuffer.allocate(ByteBuffer.java:335)",
                "at org.apache.thrift7.server.AbstractNonblockingServer$FrameBuffer.read(AbstractNonblockingServer.java:371)",
                "at org.apache.thrift7.server.AbstractNonblockingServer$AbstractSelectThread.handleRead(AbstractNonblockingServer.java:203)",
                "at org.apache.thrift7.server.TNonblockingServer$SelectAcceptThread.select(TNonblockingServer.java:207)",
                "at org.apache.thrift7.server.TNonblockingServer$SelectAcceptThread.run(TNonblockingServer.java:158)"
            ],
            "RootCause": "The root cause of the OutOfMemoryError is the lack of a defined maxReadBufferBytes parameter in the THsHaServer configuration, which allows for unbounded memory allocation when processing incoming requests.",
            "StepsToReproduce": [
                "Start the Nimbus server.",
                "Send a malformed Thrift request using the command: echo 'Hello' | nc localhost 6627.",
                "Observe the server logs for the OutOfMemoryError."
            ],
            "ExpectedBehavior": "The Nimbus server should handle malformed requests gracefully without exhausting memory resources.",
            "ObservedBehavior": "The server throws an OutOfMemoryError, causing it to crash.",
            "Suggestions": "Implement a maximum read buffer size in the THsHaServer configuration to prevent excessive memory usage when handling malformed requests.",
            "problem_location": {
                "files": [
                    "THsHaServer.java"
                ],
                "classes": [
                    "org.apache.thrift7.server.THsHaServer"
                ],
                "methods": [
                    "THsHaServer.<init>"
                ]
            },
            "possible_fix": "Add a configuration parameter for maxReadBufferBytes in the THsHaServer constructor to limit the size of the read buffer. For example:\n\npublic THsHaServer(int maxReadBufferBytes) {\n    this.maxReadBufferBytes = maxReadBufferBytes;\n    // existing initialization code\n}\n\n// Ensure to set this parameter in the server startup configuration."
        }
    },
    {
        "filename": "STORM-2682.json",
        "creation_time": "2017-08-07T15:20:27.000+0000",
        "bug_report": {
            "Title": "Supervisor crashes with NullPointerException",
            "Description": "The Supervisor process crashes with a NullPointerException approximately 30 seconds after startup. The error occurs during the execution of the updateBlobs method in the Localizer class, specifically when attempting to retrieve a resource set for a user that may not exist, leading to a failure in processing blob updates for topologies.",
            "StackTrace": [
                "java.lang.NullPointerException: null",
                "at java.util.concurrent.ConcurrentHashMap.get(ConcurrentHashMap.java:936) ~[?:1.8.0_121]",
                "at org.apache.storm.localizer.Localizer.updateBlobs(Localizer.java:332) ~[storm-core-1.0.4.jar:1.0.4]",
                "at org.apache.storm.daemon.supervisor.timer.UpdateBlobs.updateBlobsForTopology(UpdateBlobs.java:99) ~[storm-core-1.0.4.jar:1.0.4]",
                "at org.apache.storm.daemon.supervisor.timer.UpdateBlobs.run(UpdateBlobs.java:72) ~[storm-core-1.0.4.jar:1.0.4]",
                "at org.apache.storm.event.EventManagerImp$1.run(EventManagerImp.java:54) ~[storm-core-1.0.4.jar:1.0.4]",
                "java.lang.RuntimeException: Halting process: Error when processing an event",
                "at org.apache.storm.utils.Utils.exitProcess(Utils.java:1750) ~[storm-core-1.0.4.jar:1.0.4]",
                "at org.apache.storm.event.EventManagerImp$1.run(EventManagerImp.java:63) ~[storm-core-1.0.4.jar:1.0.4]"
            ],
            "RootCause": "The root cause of the NullPointerException is that the LocalizedResourceSet for the user is null, indicating that the resource set has been removed or was never initialized, leading to a failure when attempting to update blobs.",
            "StepsToReproduce": [
                "Start the Supervisor process.",
                "Wait for approximately 30 seconds.",
                "Observe the logs for a NullPointerException."
            ],
            "ExpectedBehavior": "The Supervisor should run continuously without crashing, successfully processing blob updates for all topologies.",
            "ObservedBehavior": "The Supervisor crashes with a NullPointerException after about 30 seconds, halting the process.",
            "Suggestions": "Ensure that the LocalizedResourceSet is properly initialized for each user before attempting to update blobs. Implement checks to handle cases where the resource set may be null.",
            "problem_location": {
                "files": [
                    "storm-server/src/main/java/org/apache/storm/localizer/Localizer.java",
                    "storm-server/src/main/java/org/apache/storm/daemon/supervisor/timer/UpdateBlobs.java"
                ],
                "classes": [
                    "org.apache.storm.localizer.Localizer",
                    "org.apache.storm.daemon.supervisor.timer.UpdateBlobs"
                ],
                "methods": [
                    "Localizer.updateBlobs",
                    "UpdateBlobs.updateBlobsForTopology"
                ]
            },
            "possible_fix": "In the updateBlobs method of the Localizer class, add a check to ensure that the LocalizedResourceSet is not null before proceeding with blob updates. If it is null, log a warning and return an empty list to prevent the NullPointerException."
        }
    },
    {
        "filename": "STORM-3103.json",
        "creation_time": "2018-06-13T18:23:11.000+0000",
        "bug_report": {
            "Title": "Nimbus Stuck Shutting Down Causing Leadership Issues on Startup",
            "Description": "The Nimbus server is encountering a NullPointerException during startup, which leads to a forced halt of the process. This issue arises when Nimbus attempts to read supervisor details and compute new scheduler assignments, resulting in leadership confusion and errors in topology submission. The server fails to shut down cleanly, causing further complications in leadership management.",
            "StackTrace": [
                "java.lang.RuntimeException: java.lang.NullPointerException",
                "at org.apache.storm.daemon.nimbus.Nimbus.lambda$launchServer$37(Nimbus.java:2685)",
                "at org.apache.storm.StormTimer$1.run(StormTimer.java:111)",
                "at org.apache.storm.StormTimer$StormTimerTask.run(StormTimer.java:227)",
                "Caused by: java.lang.NullPointerException",
                "at org.apache.storm.daemon.nimbus.Nimbus.readAllSupervisorDetails(Nimbus.java:1814)",
                "at org.apache.storm.daemon.nimbus.Nimbus.computeNewSchedulerAssignments(Nimbus.java:1906)",
                "at org.apache.storm.daemon.nimbus.Nimbus.mkAssignments(Nimbus.java:2057)",
                "at org.apache.storm.daemon.nimbus.Nimbus.mkAssignments(Nimbus.java:2003)",
                "at org.apache.storm.daemon.nimbus.Nimbus.lambda$launchServer$37(Nimbus.java:2681)",
                "at org.apache.storm.utils.Utils.exitProcess(Utils.java:469)"
            ],
            "RootCause": "The root cause of the issue is a NullPointerException occurring in the Nimbus class when attempting to read supervisor details. This leads to a failure in processing events and ultimately causes the Nimbus server to halt unexpectedly.",
            "StepsToReproduce": [
                "Start the Nimbus server.",
                "Observe the logs for any NullPointerExceptions during the startup process.",
                "Attempt to submit a topology while the server is in a non-leader state."
            ],
            "ExpectedBehavior": "The Nimbus server should start up cleanly, process events without errors, and maintain a stable leadership state throughout its operation.",
            "ObservedBehavior": "The Nimbus server encounters a NullPointerException during startup, leading to a forced shutdown and confusion regarding leadership, resulting in errors when submitting topologies.",
            "Suggestions": "Implement null checks in the Nimbus class methods to prevent NullPointerExceptions. Ensure that the server can handle unexpected states gracefully and shut down cleanly.",
            "problem_location": {
                "files": [
                    "Nimbus.java",
                    "Utils.java"
                ],
                "classes": [
                    "org.apache.storm.daemon.nimbus.Nimbus",
                    "org.apache.storm.utils.Utils"
                ],
                "methods": [
                    "Nimbus.readAllSupervisorDetails",
                    "Nimbus.computeNewSchedulerAssignments",
                    "Utils.exitProcess"
                ]
            },
            "possible_fix": "Add null checks in the readAllSupervisorDetails method to ensure that it does not attempt to access null references. Additionally, modify the exitProcess method to allow for a more graceful shutdown process."
        }
    }
]