[
    {
        "filename": "MAPREDUCE-6633.json",
        "creation_time": "2016-02-10T21:12:09.000+0000",
        "bug_report": {
            "Title": "AM should retry map attempts if the reduce task encounters compression related errors.",
            "Description": "The Application Master (AM) fails to retry the corresponding map task when a reduce task encounters compression-related errors, specifically an ArrayIndexOutOfBoundsException during decompression. This issue arises when the map task runs on a node with a faulty drive, leading to incomplete or corrupted data being processed. The AM should implement a retry mechanism for the map task to ensure job success in such scenarios.",
            "StackTrace": [
                "2016-01-27 13:44:28,915 WARN [main] org.apache.hadoop.mapred.YarnChild: Exception running child : org.apache.hadoop.mapreduce.task.reduce.Shuffle$ShuffleError: error in shuffle in fetcher#29",
                "at org.apache.hadoop.mapreduce.task.reduce.Shuffle.run(Shuffle.java:134)",
                "at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:376)",
                "at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1694)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)",
                "Caused by: java.lang.ArrayIndexOutOfBoundsException",
                "at com.hadoop.compression.lzo.LzoDecompressor.setInput(LzoDecompressor.java:196)",
                "at org.apache.hadoop.io.compress.BlockDecompressorStream.decompress(BlockDecompressorStream.java:104)",
                "at org.apache.hadoop.io.compress.DecompressorStream.read(DecompressorStream.java:85)",
                "at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:192)",
                "at org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput.shuffle(InMemoryMapOutput.java:97)",
                "at org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyMapOutput(Fetcher.java:537)",
                "at org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyFromHost(Fetcher.java:336)",
                "at org.apache.hadoop.mapreduce.task.reduce.Fetcher.run(Fetcher.java:193)"
            ],
            "RootCause": "The root cause of the issue is an ArrayIndexOutOfBoundsException occurring in the LzoDecompressor when attempting to decompress data from a corrupted or incomplete map output. This is likely due to the map task running on a node with a faulty drive, which results in the data being improperly written or read.",
            "StepsToReproduce": [
                "1. Set up a Hadoop cluster with a node that has a faulty drive.",
                "2. Submit a job that includes map and reduce tasks with compression enabled.",
                "3. Monitor the reduce task for compression-related errors."
            ],
            "ExpectedBehavior": "The Application Master should retry the failed map task on a different node if the reduce task encounters compression-related errors, ensuring that the job can complete successfully.",
            "ObservedBehavior": "The Application Master does not retry the map task, leading to job failure when the reduce task encounters compression-related errors.",
            "Suggestions": "Implement a retry mechanism in the Application Master to handle cases where reduce tasks fail due to compression errors. This could involve tracking failed map tasks and rescheduling them on healthy nodes.",
            "problem_location": {
                "files": [
                    "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/ReduceTask.java",
                    "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/Fetcher.java",
                    "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/BlockDecompressorStream.java",
                    "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/IOUtils.java"
                ],
                "classes": [
                    "org.apache.hadoop.mapreduce.task.reduce.ReduceTask",
                    "org.apache.hadoop.mapreduce.task.reduce.Fetcher",
                    "org.apache.hadoop.io.compress.BlockDecompressorStream",
                    "org.apache.hadoop.io.IOUtils"
                ],
                "methods": [
                    "ReduceTask.run",
                    "Fetcher.copyMapOutput",
                    "BlockDecompressorStream.decompress",
                    "IOUtils.readFully"
                ]
            },
            "possible_fix": "To fix this issue, modify the Application Master to include a retry mechanism for map tasks that fail due to compression errors. This could involve adding logic to check for specific exceptions during the reduce phase and rescheduling the corresponding map tasks on healthy nodes."
        }
    },
    {
        "filename": "MAPREDUCE-6577.json",
        "creation_time": "2015-12-17T06:46:30.000+0000",
        "bug_report": {
            "Title": "MR AM unable to load native library without MR_AM_ADMIN_USER_ENV set",
            "Description": "The MapReduce Application Master (MR AM) fails to load the native Hadoop library when the environment variable 'yarn.app.mapreduce.am.admin.user.env' or 'yarn.app.mapreduce.am.env' is not configured to include 'LD_LIBRARY_PATH'. This results in a warning indicating that the native library cannot be loaded, and subsequently, any operations that depend on the native library, such as LZ4 compression, will fail. The stack trace shows that the failure occurs when the Lz4Codec attempts to access the native library, leading to a RuntimeException.",
            "StackTrace": [
                "2015-12-15 21:30:17,575 WARN [uber-SubtaskRunner] org.apache.hadoop.mapred.LocalContainerLauncher: Exception running local (uberized) 'child' : java.lang.RuntimeException: native lz4 library not available",
                "at org.apache.hadoop.io.compress.Lz4Codec.getCompressorType(Lz4Codec.java:125)",
                "at org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:148)",
                "at org.apache.hadoop.io.compress.CodecPool.getCompressor(CodecPool.java:163)",
                "at org.apache.hadoop.mapred.IFile$Writer.<init>(IFile.java:114)",
                "at org.apache.hadoop.mapred.IFile$Writer.<init>(IFile.java:97)",
                "at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1602)",
                "at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.flush(MapTask.java:1482)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:457)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)",
                "at org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler.runSubtask(LocalContainerLauncher.java:391)",
                "at org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler.runTask(LocalContainerLauncher.java:309)",
                "at org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler.access$200(LocalContainerLauncher.java:195)",
                "at org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler$1.run(LocalContainerLauncher.java:238)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:262)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "RootCause": "The root cause of the issue is the absence of the 'LD_LIBRARY_PATH' in the environment variables configured for the MR AM, which prevents the loading of the native Hadoop library required for operations like LZ4 compression.",
            "StepsToReproduce": [
                "Ensure that 'yarn.app.mapreduce.am.admin.user.env' or 'yarn.app.mapreduce.am.env' is not set or does not include 'LD_LIBRARY_PATH'.",
                "Run a MapReduce job that requires the native Hadoop library, such as one using LZ4 compression.",
                "Observe the warning and exception in the logs indicating that the native library is not available."
            ],
            "ExpectedBehavior": "The MR AM should successfully load the native Hadoop library and execute the MapReduce job without any errors related to missing native libraries.",
            "ObservedBehavior": "The MR AM fails to load the native library, resulting in a RuntimeException when attempting to use LZ4 compression, causing the MapReduce job to fail.",
            "Suggestions": "Configure the 'yarn.app.mapreduce.am.admin.user.env' or 'yarn.app.mapreduce.am.env' to include 'LD_LIBRARY_PATH' pointing to the directory containing the native Hadoop libraries.",
            "problem_location": {
                "files": [
                    "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/Lz4Codec.java",
                    "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/MapTask.java"
                ],
                "classes": [
                    "org.apache.hadoop.io.compress.Lz4Codec",
                    "org.apache.hadoop.mapred.MapTask"
                ],
                "methods": [
                    "Lz4Codec.getCompressorType",
                    "MapTask.runOldMapper",
                    "MapTask.run"
                ]
            },
            "possible_fix": "To resolve this issue, ensure that the environment variable 'LD_LIBRARY_PATH' is set correctly in the configuration for the MR AM. This can be done by adding the following line to the configuration: 'yarn.app.mapreduce.am.admin.user.env=LD_LIBRARY_PATH=/path/to/native/libs'."
        }
    },
    {
        "filename": "MAPREDUCE-5137.json",
        "creation_time": "2013-04-09T15:14:34.000+0000",
        "bug_report": {
            "Title": "AM web UI: clicking on Map Task results in 500 error",
            "Description": "When navigating to the running MapReduce application master web UI and attempting to access a specific MAP task, a 500 Internal Server Error is encountered. This issue arises when the application attempts to retrieve details for a specific map task, resulting in a NotFoundException. This behavior was not present in version 0.23.6, indicating a regression or change in the handling of task requests.",
            "StackTrace": [
                "2013-04-09 13:53:01,587 DEBUG [1088374@qtp-13877033-2 - /mapreduce/task/task_1365457322543_0004_m_000000] org.apache.hadoop.yarn.webapp.GenericExceptionHandler: GOT EXCEPITION",
                "com.sun.jersey.api.NotFoundException: null for uri: http://host.com:38158/mapreduce/task/task_1365457322543_0004_m_000000",
                "at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1470)",
                "at com.sun.jersey.server.impl.application.WebApplicationImpl._handleRequest(WebApplicationImpl.java:1400)",
                "at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1349)",
                "at com.sun.jersey.server.impl.application.WebApplicationImpl.handleRequest(WebApplicationImpl.java:1339)",
                "at com.sun.jersey.spi.container.servlet.WebComponent.service(WebComponent.java:416)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.service(ServletContainer.java:537)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:886)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:834)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:795)",
                "at com.google.inject.servlet.FilterDefinition.doFilter(FilterDefinition.java:163)",
                "at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:58)",
                "at com.google.inject.servlet.ManagedFilterPipeline.dispatch(ManagedFilterPipeline.java:118)",
                "at com.google.inject.servlet.GuiceFilter.doFilter(GuiceFilter.java:113)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "at org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.doFilter(AmIpFilter.java:123)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)",
                "at org.apache.hadoop.http.HttpServer$QuotingInputFilter.doFilter(HttpServer.java:1069)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)"
            ],
            "RootCause": "The root cause of the issue is a NotFoundException triggered when the web application attempts to access a specific URI for a map task that does not exist or is not available. This could be due to the task being completed, removed, or an incorrect task ID being referenced.",
            "StepsToReproduce": [
                "Navigate to the running MapReduce application master web UI.",
                "Click on the job to view its details.",
                "Select the MAP task type to display the list of map tasks.",
                "Click on a specific map task to view its details."
            ],
            "ExpectedBehavior": "The user should be able to view the details of the selected map task without encountering an error.",
            "ObservedBehavior": "A 500 Internal Server Error is displayed, indicating that the requested map task could not be found.",
            "Suggestions": "Check the validity of the task ID being requested. Ensure that the task is still active and has not been completed or removed. Implement error handling to provide a user-friendly message when a task is not found instead of a 500 error.",
            "problem_location": {
                "files": [
                    "WebApplicationImpl.java",
                    "GenericExceptionHandler.java"
                ],
                "classes": [
                    "com.sun.jersey.server.impl.application.WebApplicationImpl",
                    "org.apache.hadoop.yarn.webapp.GenericExceptionHandler"
                ],
                "methods": [
                    "WebApplicationImpl._handleRequest",
                    "GenericExceptionHandler.handleException"
                ]
            },
            "possible_fix": "Add a check in the WebApplicationImpl class to verify if the task ID exists before attempting to handle the request. If the task does not exist, return a 404 Not Found response instead of a 500 error."
        }
    },
    {
        "filename": "MAPREDUCE-4008.json",
        "creation_time": "2012-03-14T10:08:39.000+0000",
        "bug_report": {
            "Title": "ResourceManager throws MetricsException on startup due to existing QueueMetrics MBean",
            "Description": "During the startup of the ResourceManager, an exception is thrown indicating that the MBean for QueueMetrics already exists. This occurs when the ResourceManager attempts to register a metrics source that has already been registered, leading to a MetricsException. The issue arises from the MBean registration process where the system checks for existing MBeans and fails to handle the case where an MBean with the same name is already present.",
            "StackTrace": [
                "2012-03-14 15:22:23,089 WARN org.apache.hadoop.metrics2.util.MBeans: Error creating MBean object name: Hadoop:service=ResourceManager,name=QueueMetrics,q0=default",
                "org.apache.hadoop.metrics2.MetricsException: org.apache.hadoop.metrics2.MetricsException: Hadoop:service=ResourceManager,name=QueueMetrics,q0=default already exists!",
                "at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.newObjectName(DefaultMetricsSystem.java:117)",
                "at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.newMBeanName(DefaultMetricsSystem.java:102)",
                "at org.apache.hadoop.metrics2.util.MBeans.getMBeanName(MBeans.java:91)",
                "at org.apache.hadoop.metrics2.util.MBeans.register(MBeans.java:55)",
                "at org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.startMBeans(MetricsSourceAdapter.java:218)",
                "at org.apache.hadoop.metrics2.impl.MetricsSourceAdapter.startMBeans(MetricsSourceAdapter.java:93)",
                "at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.registerSource(MetricsSystemImpl.java:243)",
                "at org.apache.hadoop.metrics2.impl.MetricsSystemImpl$1.postStart(MetricsSystemImpl.java:227)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.start(ResourceManager.java:454)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:588)"
            ],
            "RootCause": "The root cause of the issue is that the MBean for QueueMetrics is being registered multiple times without proper checks to prevent duplicate registrations. The MetricsSystem's newObjectName method throws a MetricsException when it detects that an MBean with the same name already exists.",
            "StepsToReproduce": [
                "Start the ResourceManager in a Hadoop cluster.",
                "Ensure that the QueueMetrics MBean is already registered from a previous instance or configuration.",
                "Observe the logs for the MetricsException indicating that the MBean already exists."
            ],
            "ExpectedBehavior": "The ResourceManager should start without throwing an exception, and the MBean for QueueMetrics should be registered successfully if it does not already exist.",
            "ObservedBehavior": "The ResourceManager fails to start and throws a MetricsException indicating that the QueueMetrics MBean already exists.",
            "Suggestions": "Implement a check to see if the MBean is already registered before attempting to register it again. This can be done by modifying the MBeans.register method to handle existing MBeans more gracefully.",
            "problem_location": {
                "files": [
                    "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/util/MBeans.java",
                    "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceManager.java"
                ],
                "classes": [
                    "org.apache.hadoop.metrics2.util.MBeans",
                    "org.apache.hadoop.yarn.server.resourcemanager.ResourceManager"
                ],
                "methods": [
                    "MBeans.register",
                    "ResourceManager.start"
                ]
            },
            "possible_fix": "Modify the MBeans.register method to check if the MBean name already exists before attempting to register it. If it exists, log a warning and skip the registration to prevent the MetricsException."
        }
    },
    {
        "filename": "MAPREDUCE-6259.json",
        "creation_time": "2015-02-13T03:20:41.000+0000",
        "bug_report": {
            "Title": "IllegalArgumentException due to missing job submit time",
            "Description": "An IllegalArgumentException is thrown when attempting to parse the job state from the job history file due to a missing job submit time, which is represented as -1. This occurs when a job fails to initialize properly, leading to the job submit time not being updated from its initial value. The job history file name indicates that the job has failed, but the submit time remains -1, causing the parsing error.",
            "StackTrace": [
                "java.lang.IllegalArgumentException: No enum constant org.apache.hadoop.mapreduce.v2.api.records.JobState.0",
                "at java.lang.Enum.valueOf(Enum.java:236)",
                "at org.apache.hadoop.mapreduce.v2.api.records.JobState.valueOf(JobState.java:21)",
                "at org.apache.hadoop.mapreduce.v2.hs.PartialJob.getState(PartialJob.java:82)",
                "at org.apache.hadoop.mapreduce.v2.hs.PartialJob.<init>(PartialJob.java:59)",
                "at org.apache.hadoop.mapreduce.v2.hs.CachedHistoryStorage.getAllPartialJobs(CachedHistoryStorage.java:159)",
                "at org.apache.hadoop.mapreduce.v2.hs.CachedHistoryStorage.getPartialJobs(CachedHistoryStorage.java:173)",
                "at org.apache.hadoop.mapreduce.v2.hs.JobHistory.getPartialJobs(JobHistory.java:284)",
                "at org.apache.hadoop.mapreduce.v2.hs.webapp.HsWebServices.getJobs(HsWebServices.java:212)"
            ],
            "RootCause": "The root cause of the issue is that the job submit time is not updated due to an IOException occurring during the setup phase of the job. This results in the job remaining in the NEW state and the submit time retaining its initial value of -1.",
            "StepsToReproduce": [
                "Create a job using MRAppMaster#serviceStart.",
                "Send a JOB_INIT event to JobImpl.",
                "Trigger an IOException in JobImpl#setup.",
                "Observe that the job submit time remains -1 and the job fails to initialize properly."
            ],
            "ExpectedBehavior": "The job submit time should be updated correctly upon job initialization, allowing the job state to be parsed without errors.",
            "ObservedBehavior": "The job submit time remains -1, leading to an IllegalArgumentException when attempting to parse the job state from the job history file.",
            "Suggestions": "Implement error handling in JobImpl#setup to ensure that the job submit time is updated even if an IOException occurs. Additionally, validate the job state before attempting to parse it.",
            "problem_location": {
                "files": [
                    "hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/hs/PartialJob.java",
                    "hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/hs/JobImpl.java"
                ],
                "classes": [
                    "org.apache.hadoop.mapreduce.v2.hs.PartialJob",
                    "org.apache.hadoop.mapreduce.v2.hs.JobImpl"
                ],
                "methods": [
                    "PartialJob.getState",
                    "JobImpl.setup"
                ]
            },
            "possible_fix": "In JobImpl#setup, ensure that the job submit time is set correctly even if an IOException occurs. For example, add a fallback mechanism to set the submit time to the current time if it remains -1 after the setup process."
        }
    },
    {
        "filename": "MAPREDUCE-3333.json",
        "creation_time": "2011-11-02T14:00:54.000+0000",
        "bug_report": {
            "Title": "MR AM for sort-job going out of memory",
            "Description": "The sort job on a 350 node cluster encountered an OutOfMemoryError, causing it to hang and eventually fail after an hour, significantly longer than the usual 20 minutes. The error is traced back to the inability to create new native threads, which is likely due to resource exhaustion on the system.",
            "StackTrace": [
                "2011-11-02 11:40:36,438 ERROR [ContainerLauncher #258] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Container launch failed for container_1320233407485_0002_01_001434 : java.lang.reflect.UndeclaredThrowableException",
                "at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagerPBClientImpl.startContainer(ContainerManagerPBClientImpl.java:88)",
                "at org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl$EventProcessor.run(ContainerLauncherImpl.java:290)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)",
                "at java.lang.Thread.run(Thread.java:619)",
                "Caused by: com.google.protobuf.ServiceException: java.io.IOException: Failed on local exception: java.io.IOException: Couldn't set up IO streams; Host Details : local host is: \"gsbl91281.blue.ygrid.yahoo.com/98.137.101.189\"; destination host is: \"gsbl91525.blue.ygrid.yahoo.com\":45450;",
                "at org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Invoker.invoke(ProtoOverHadoopRpcEngine.java:139)",
                "at $Proxy20.startContainer(Unknown Source)",
                "at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagerPBClientImpl.startContainer(ContainerManagerPBClientImpl.java:81)",
                "Caused by: java.io.IOException: Failed on local exception: java.io.IOException: Couldn't set up IO streams; Host Details : local host is: \"gsbl91281.blue.ygrid.yahoo.com/98.137.101.189\"; destination host is: \"gsbl91525.blue.ygrid.yahoo.com\":45450;",
                "at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:655)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1089)",
                "at org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Invoker.invoke(ProtoOverHadoopRpcEngine.java:136)",
                "Caused by: java.io.IOException: Couldn't set up IO streams",
                "at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:621)",
                "at org.apache.hadoop.ipc.Client$Connection.access$2000(Client.java:205)",
                "at org.apache.hadoop.ipc.Client.getConnection(Client.java:1195)",
                "Caused by: java.lang.OutOfMemoryError: unable to create new native thread",
                "at java.lang.Thread.start0(Native Method)",
                "at java.lang.Thread.start(Thread.java:597)",
                "at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:614)"
            ],
            "RootCause": "The OutOfMemoryError is caused by the inability to create new native threads, likely due to resource limits being reached on the system, which prevents the application from establishing necessary IO streams for communication.",
            "StepsToReproduce": [
                "Deploy a sort job on a 350 node cluster.",
                "Monitor the job execution and resource usage.",
                "Observe the job hanging and eventually failing after an hour."
            ],
            "ExpectedBehavior": "The sort job should complete successfully within the usual time frame of approximately 20 minutes without running out of memory.",
            "ObservedBehavior": "The sort job hangs and fails after an hour due to an OutOfMemoryError, indicating resource exhaustion.",
            "Suggestions": "Increase the memory allocation for the job or the cluster. Review the system's thread limits and adjust them if necessary. Optimize the job to use fewer resources.",
            "problem_location": {
                "files": [
                    "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Client.java",
                    "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/api/impl/pb/client/ContainerManagerPBClientImpl.java",
                    "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/net/NetUtils.java"
                ],
                "classes": [
                    "org.apache.hadoop.ipc.Client",
                    "org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagerPBClientImpl",
                    "org.apache.hadoop.net.NetUtils"
                ],
                "methods": [
                    "Client.getConnection",
                    "ContainerManagerPBClientImpl.startContainer",
                    "NetUtils.wrapException"
                ]
            },
            "possible_fix": "Consider increasing the maximum number of threads allowed for the JVM by adjusting the '-Xss' and '-Xmx' parameters. Additionally, review the job configuration to ensure it is optimized for resource usage."
        }
    },
    {
        "filename": "MAPREDUCE-7059.json",
        "creation_time": "2018-02-26T09:38:39.000+0000",
        "bug_report": {
            "Title": "Downward Compatibility issue: MR job fails due to unknown setErasureCodingPolicy method from 3.x client to HDFS 2.x cluster",
            "Description": "The MapReduce job fails when attempting to run teragen with Hadoop 3.1 against an HDFS 2.8 cluster. The failure occurs because the HDFS 2.8 server does not support the setErasureCodingPolicy method, which is present in Hadoop 3.x. This results in a RemoteException being thrown, indicating that the method is unknown. The issue arises during the resource upload phase of the job submission process, specifically in the JobResourceUploader class.",
            "StackTrace": [
                "org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.ipc.RpcNoSuchMethodException): Unknown method setErasureCodingPolicy called on org.apache.hadoop.hdfs.protocol.ClientProtocol protocol.",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:436)",
                "at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)",
                "at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:846)",
                "at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:789)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:422)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1804)",
                "at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2457)",
                "at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1491)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1437)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1347)",
                "at org.apache.hadoop.hdfs.DFSClient.setErasureCodingPolicy(DFSClient.java:2678)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.setErasureCodingPolicy(DistributedFileSystem.java:2680)",
                "at org.apache.hadoop.mapreduce.JobResourceUploader.disableErasureCodingForPath(JobResourceUploader.java:882)",
                "at org.apache.hadoop.mapreduce.JobResourceUploader.uploadResourcesInternal(JobResourceUploader.java:174)",
                "at org.apache.hadoop.mapreduce.JobResourceUploader.uploadResources(JobResourceUploader.java:131)",
                "at org.apache.hadoop.mapreduce.JobSubmitter.copyAndConfigureFiles(JobSubmitter.java:102)",
                "at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:197)",
                "at org.apache.hadoop.mapreduce.Job.submit(Job.java:1567)",
                "at org.apache.hadoop.examples.terasort.TeraGen.run(TeraGen.java:304)",
                "at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:76)",
                "at org.apache.hadoop.examples.terasort.TeraGen.main(TeraGen.java:308)"
            ],
            "RootCause": "The root cause of the issue is the invocation of the setErasureCodingPolicy method, which is not available in the HDFS 2.8 server. This method was introduced in Hadoop 3.x, leading to a compatibility issue when a 3.x client attempts to communicate with a 2.x server.",
            "StepsToReproduce": [
                "Set up an HDFS 2.8 cluster.",
                "Use a Hadoop 3.1 client.",
                "Run the teragen job using the command: bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.0-SNAPSHOT.jar teragen 1000000 /teragen"
            ],
            "ExpectedBehavior": "The teragen job should complete successfully without any exceptions, and the data should be generated in the specified HDFS path.",
            "ObservedBehavior": "The teragen job fails with a RemoteException indicating that the setErasureCodingPolicy method is unknown on the HDFS 2.8 server.",
            "Suggestions": "One potential solution is to modify the JobResourceUploader class to handle the RemoteException when the setErasureCodingPolicy method is not found. This can be done by catching the RpcNoSuchMethodException and skipping the call to setErasureCodingPolicy.",
            "problem_location": {
                "files": [
                    "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/JobResourceUploader.java",
                    "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/JobSubmitter.java"
                ],
                "classes": [
                    "org.apache.hadoop.mapreduce.JobResourceUploader",
                    "org.apache.hadoop.mapreduce.JobSubmitter"
                ],
                "methods": [
                    "JobResourceUploader.disableErasureCodingForPath",
                    "JobSubmitter.submitJobInternal"
                ]
            },
            "possible_fix": "Modify the disableErasureCodingForPath method in JobResourceUploader to catch the RpcNoSuchMethodException and log a warning instead of throwing an exception. This will allow the job to proceed without attempting to set the erasure coding policy on unsupported HDFS versions."
        }
    },
    {
        "filename": "MAPREDUCE-4843.json",
        "creation_time": "2012-12-04T13:39:40.000+0000",
        "bug_report": {
            "Title": "When using DefaultTaskController, JobLocalizer not thread safe",
            "Description": "In a clustered environment, jobs occasionally fail due to a DiskErrorException when attempting to initialize job attempts. The issue arises from the non-thread-safe nature of the JobLocalizer, which leads to conflicts when multiple TaskLauncher threads attempt to initialize jobs simultaneously. This results in the configuration being overwritten, causing the job.xml file to be misplaced in the wrong user's directory.",
            "StackTrace": [
                "2012-12-03 23:11:54,811 WARN org.apache.hadoop.mapred.TaskTracker: Error initializing attempt_201212031626_1115_r_000023_0:",
                "org.apache.hadoop.util.DiskChecker$DiskErrorException: Could not find taskTracker/$username/jobcache/job_201212031626_1115/job.xml in any of the configured local directories",
                "at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathToRead(LocalDirAllocator.java:424)",
                "at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathToRead(LocalDirAllocator.java:160)",
                "at org.apache.hadoop.mapred.TaskTracker.initializeJob(TaskTracker.java:1175)",
                "at org.apache.hadoop.mapred.TaskTracker.localizeJob(TaskTracker.java:1058)",
                "at org.apache.hadoop.mapred.TaskTracker.startNewTask(TaskTracker.java:2213)"
            ],
            "RootCause": "The JobLocalizer is not thread-safe, leading to race conditions when multiple threads attempt to access and modify the shared configuration instance.",
            "StepsToReproduce": [
                "Set up a cluster with multiple TaskLauncher threads.",
                "Submit multiple jobs that require localization at the same time.",
                "Observe the logs for DiskErrorException related to job.xml file not found."
            ],
            "ExpectedBehavior": "Each job should be able to localize its resources independently without interference from other jobs, ensuring that job.xml files are correctly placed in their respective directories.",
            "ObservedBehavior": "Jobs occasionally fail with a DiskErrorException indicating that the job.xml file could not be found in the configured local directories due to configuration overwrites.",
            "Suggestions": "Implement synchronization mechanisms within the JobLocalizer to ensure thread safety. Alternatively, consider using separate configuration instances for each JobLocalizer to prevent shared state issues.",
            "problem_location": {
                "files": [
                    "TaskTracker.java",
                    "LocalDirAllocator.java"
                ],
                "classes": [
                    "org.apache.hadoop.mapred.TaskTracker",
                    "org.apache.hadoop.fs.LocalDirAllocator"
                ],
                "methods": [
                    "TaskTracker.initializeJob",
                    "LocalDirAllocator.getLocalPathToRead"
                ]
            },
            "possible_fix": "To fix the thread safety issue, modify the JobLocalizer to create a new instance of Configuration for each job initialization. This can be done by cloning the existing configuration or creating a new one based on the current context. Additionally, ensure that any shared resources are properly synchronized."
        }
    },
    {
        "filename": "MAPREDUCE-5028.json",
        "creation_time": "2013-02-26T03:54:25.000+0000",
        "bug_report": {
            "Title": "Maps fail when io.sort.mb is set to high value",
            "Description": "The issue occurs when running a MapReduce job with a high value for the 'io.sort.mb' configuration parameter. Specifically, when set to 1280 MB, the maps fail during the sorting and spilling phase, leading to an IOException. The error is caused by an EOFException when attempting to read data from the output buffer, indicating that the expected data is not available, likely due to buffer overflow or mismanagement of memory allocation.",
            "StackTrace": [
                "java.io.IOException: Spill failed",
                "at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:1031)",
                "at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:692)",
                "at org.apache.hadoop.mapreduce.TaskInputOutputContext.write(TaskInputOutputContext.java:80)",
                "at org.apache.hadoop.examples.WordCount$TokenizerMapper.map(WordCount.java:45)",
                "at org.apache.hadoop.examples.WordCount$TokenizerMapper.map(WordCount.java:34)",
                "at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)",
                "at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:766)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:370)",
                "at org.apache.hadoop.mapred.Child$4.run(Child.java:255)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:396)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1149)",
                "at org.apache.hadoop.mapred.Child.main(Child.java:249)",
                "Caused by: java.io.EOFException",
                "at java.io.DataInputStream.readInt(DataInputStream.java:375)",
                "at org.apache.hadoop.io.IntWritable.readFields(IntWritable.java:38)",
                "at org.apache.hadoop.io.serializer.WritableSerialization$WritableDeserializer.deserialize(WritableSerialization.java:67)",
                "at org.apache.hadoop.io.serializer.WritableSerialization$WritableDeserializer.deserialize(WritableSerialization.java:40)",
                "at org.apache.hadoop.mapreduce.ReduceContext.nextKeyValue(ReduceContext.java:116)",
                "at org.apache.hadoop.mapreduce.ReduceContext.nextKey(ReduceContext.java:92)",
                "at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:175)",
                "at org.apache.hadoop.mapred.Task$NewCombinerRunner.combine(Task.java:1505)",
                "at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1438)",
                "at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.access$1800(MapTask.java:855)",
                "at org.apache.hadoop.mapred.MapTask$MapOutputBuffer$SpillThread.run(MapTask.java:1346)"
            ],
            "RootCause": "The root cause of the issue is an EOFException occurring during the reading of data from the output buffer, which is likely due to the high memory allocation for sorting and spilling exceeding the available buffer size, leading to data corruption or loss.",
            "StepsToReproduce": [
                "Set up a Hadoop cluster in pseudo-distributed mode.",
                "Configure the following parameters: mapred.child.java.opts=-Xmx2048m, io.sort.mb=1280, dfs.block.size=2147483648.",
                "Run the teragen command to generate 4 GB of data.",
                "Execute the wordcount job on the generated data."
            ],
            "ExpectedBehavior": "The MapReduce job should complete successfully without any IOException or EOFException, and the output should reflect the correct word count from the input data.",
            "ObservedBehavior": "The MapReduce job fails with an IOException during the map phase, specifically indicating that the spill operation failed due to an EOFException.",
            "Suggestions": "Consider reducing the value of 'io.sort.mb' to a lower threshold that is manageable within the available memory limits. Additionally, review the memory allocation settings for the map tasks to ensure they are appropriate for the data size being processed.",
            "problem_location": {
                "files": [
                    "hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/MapTask.java",
                    "hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/MapOutputBuffer.java",
                    "hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/Child.java",
                    "hadoop-common/src/main/java/org/apache/hadoop/io/IntWritable.java"
                ],
                "classes": [
                    "org.apache.hadoop.mapred.MapTask",
                    "org.apache.hadoop.mapred.MapOutputBuffer",
                    "org.apache.hadoop.mapred.Child",
                    "org.apache.hadoop.io.IntWritable"
                ],
                "methods": [
                    "MapTask.runNewMapper",
                    "MapOutputBuffer.collect",
                    "Child.main",
                    "IntWritable.readFields"
                ]
            },
            "possible_fix": "Reduce the 'io.sort.mb' configuration value to a lower limit, such as 256 MB or 512 MB, to prevent buffer overflow. Additionally, ensure that the memory settings for the map tasks are appropriately configured to handle the data size."
        }
    },
    {
        "filename": "MAPREDUCE-4300.json",
        "creation_time": "2012-05-31T20:44:00.000+0000",
        "bug_report": {
            "Title": "OOM in AM can turn it into a zombie.",
            "Description": "The application master (AM) encounters an OutOfMemoryError (OOM) due to excessive memory consumption by multiple threads, leading to a failure in processing tasks. This issue arises when the AM attempts to handle speculative task scheduling and data transfer acknowledgments, which results in the AM becoming unresponsive (zombie state). The logs indicate that several threads, including the response processor and the default speculator, are affected by the OOM error.",
            "StackTrace": [
                "Exception in thread \"ResponseProcessor for block BP-1114822160-<IP>-1322528669066:blk_-6528896407411719649_34227308\" java.lang.OutOfMemoryError: Java heap space",
                "at com.google.protobuf.CodedInputStream.<init>(CodedInputStream.java:538)",
                "at com.google.protobuf.CodedInputStream.newInstance(CodedInputStream.java:55)",
                "at com.google.protobuf.AbstractMessageLite$Builder.mergeFrom(AbstractMessageLite.java:201)",
                "at com.google.protobuf.AbstractMessage$Builder.mergeFrom(AbstractMessage.java:738)",
                "at org.apache.hadoop.hdfs.protocol.proto.DataTransferProtos$PipelineAckProto.parseFrom(DataTransferProtos.java:7287)",
                "at org.apache.hadoop.hdfs.protocol.datatransfer.PipelineAck.readFields(PipelineAck.java:95)",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer$ResponseProcessor.run(DFSOutputStream.java:656)",
                "Exception in thread \"DefaultSpeculator background processing\" java.lang.OutOfMemoryError: Java heap space",
                "at java.util.HashMap.resize(HashMap.java:462)",
                "at java.util.HashMap.addEntry(HashMap.java:755)",
                "at java.util.HashMap.put(HashMap.java:385)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.getTasks(JobImpl.java:632)",
                "at org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator.maybeScheduleASpeculation(DefaultSpeculator.java:465)",
                "at org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator.maybeScheduleAMapSpeculation(DefaultSpeculator.java:433)",
                "at org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator.computeSpeculations(DefaultSpeculator.java:509)",
                "at org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator.access$100(DefaultSpeculator.java:56)",
                "at org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator$1.run(DefaultSpeculator.java:176)",
                "at java.lang.Thread.run(Thread.java:619)",
                "Exception in thread \"Timer for 'MRAppMaster' metrics system\" java.lang.OutOfMemoryError: Java heap space",
                "Exception in thread \"Socket Reader #4 for port 50500\" java.lang.OutOfMemoryError: Java heap space"
            ],
            "RootCause": "The root cause of the OutOfMemoryError is the excessive memory usage during speculative task scheduling and data processing, particularly in the DefaultSpeculator and response processing threads. The system is unable to handle the load, leading to multiple threads crashing due to insufficient heap space.",
            "StepsToReproduce": [
                "Run a MapReduce job with a high number of tasks and speculative execution enabled.",
                "Monitor the memory usage of the application master.",
                "Observe the logs for OutOfMemoryError occurrences."
            ],
            "ExpectedBehavior": "The application master should handle task scheduling and data processing without running out of memory, allowing for successful job execution and response handling.",
            "ObservedBehavior": "The application master encounters OutOfMemoryError, causing multiple threads to crash and the AM to become unresponsive.",
            "Suggestions": "Increase the heap size allocated to the application master. Review the speculative execution logic to optimize memory usage. Consider limiting the number of speculative tasks or implementing a more efficient data processing strategy.",
            "problem_location": {
                "files": [
                    "hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/speculate/DefaultSpeculator.java",
                    "hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/JobImpl.java"
                ],
                "classes": [
                    "org.apache.hadoop.mapreduce.v2.app.speculate.DefaultSpeculator",
                    "org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl"
                ],
                "methods": [
                    "DefaultSpeculator.maybeScheduleAMapSpeculation",
                    "DefaultSpeculator.maybeScheduleASpeculation",
                    "JobImpl.getTasks"
                ]
            },
            "possible_fix": "Increase the heap size for the application master by adjusting the configuration settings (e.g., setting 'yarn.app.mapreduce.am.resource.mb' to a higher value). Additionally, review the speculative task scheduling logic to ensure it does not overload the memory with unnecessary tasks."
        }
    },
    {
        "filename": "MAPREDUCE-3241.json",
        "creation_time": "2011-10-21T16:25:33.000+0000",
        "bug_report": {
            "Title": "(Rumen)TraceBuilder throws IllegalArgumentException",
            "Description": "The TraceBuilder encounters an IllegalArgumentException when processing job history files. This occurs specifically when the JobBuilder attempts to process an unknown event type from the job history, leading to a failure in outputting the expected map and reduce task information. The issue arises during the parsing of job history events, where the event type is not recognized, causing the JobBuilder to throw an exception.",
            "StackTrace": [
                "java.lang.IllegalArgumentException: JobBuilder.process(HistoryEvent): unknown event type",
                "at org.apache.hadoop.tools.rumen.JobBuilder.process(JobBuilder.java:165)",
                "at org.apache.hadoop.tools.rumen.TraceBuilder.processJobHistory(TraceBuilder.java:304)",
                "at org.apache.hadoop.tools.rumen.TraceBuilder.run(TraceBuilder.java:258)",
                "at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:69)",
                "at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:83)",
                "at org.apache.hadoop.tools.rumen.TraceBuilder.main(TraceBuilder.java:185)"
            ],
            "RootCause": "The root cause of the issue is that the JobBuilder is attempting to process a HistoryEvent with an unknown event type, which is not handled in the current implementation of the process method in JobBuilder.",
            "StepsToReproduce": [
                "Run the TraceBuilder with a job history file that contains an unknown event type.",
                "Observe the output and the exception thrown."
            ],
            "ExpectedBehavior": "The TraceBuilder should successfully process the job history file and output the map and reduce task information without throwing an exception.",
            "ObservedBehavior": "The TraceBuilder throws an IllegalArgumentException and fails to output the expected information.",
            "Suggestions": "Implement error handling in the JobBuilder.process method to manage unknown event types gracefully. Consider logging the unknown event type for further analysis.",
            "problem_location": {
                "files": [
                    "hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/JobBuilder.java",
                    "hadoop-mapreduce-project/src/tools/org/apache/hadoop/tools/rumen/TraceBuilder.java"
                ],
                "classes": [
                    "org.apache.hadoop.tools.rumen.JobBuilder",
                    "org.apache.hadoop.tools.rumen.TraceBuilder"
                ],
                "methods": [
                    "JobBuilder.process",
                    "TraceBuilder.processJobHistory"
                ]
            },
            "possible_fix": "In the JobBuilder.process method, add a check for the event type before processing. If the event type is unknown, log a warning and skip processing that event instead of throwing an exception."
        }
    },
    {
        "filename": "MAPREDUCE-6002.json",
        "creation_time": "2014-07-24T02:49:25.000+0000",
        "bug_report": {
            "Title": "MR task should prevent report error to AM when process is shutting down",
            "Description": "The issue arises when a MapReduce (MR) task is preempted and subsequently fails to report correctly to the Application Master (AM) during the shutdown process. Specifically, if the FileSystem is in use and a shutdown hook is triggered, it can lead to an IOException indicating that the filesystem is closed. This exception is incorrectly reported as a task failure instead of a preemption, which can mislead the AM about the task's status. The root of the problem lies in the timing of the shutdown and the task's interaction with the FileSystem.",
            "StackTrace": [
                "2014-07-22 01:46:19,613 FATAL [IPC Server handler 10 on 56903] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Task: attempt_1405985051088_0018_m_000025_0 - exited : java.io.IOException: Filesystem closed",
                "at org.apache.hadoop.hdfs.DFSClient.checkOpen(DFSClient.java:707)",
                "at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:776)",
                "at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:837)",
                "at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:645)",
                "at java.io.DataInputStream.readByte(DataInputStream.java:265)",
                "at org.apache.hadoop.io.WritableUtils.readVLong(WritableUtils.java:308)",
                "at org.apache.hadoop.io.WritableUtils.readVIntInRange(WritableUtils.java:348)",
                "at org.apache.hadoop.io.Text.readString(Text.java:464)",
                "at org.apache.hadoop.io.Text.readString(Text.java:457)",
                "at org.apache.hadoop.mapred.MapTask.getSplitDetails(MapTask.java:357)",
                "at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:731)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:340)",
                "at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:167)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1594)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:162)"
            ],
            "RootCause": "The root cause of the issue is the premature closure of the FileSystem during the shutdown process, which leads to an IOException being thrown when the MR task attempts to read split details from HDFS. This exception is incorrectly reported as a task failure instead of being handled as a preemption.",
            "StepsToReproduce": [
                "1. Start a MapReduce job that utilizes HDFS.",
                "2. Trigger a shutdown of the FileSystem while the job is running.",
                "3. Observe the logs for any IOException related to 'Filesystem closed'.",
                "4. Check the status reported to the Application Master."
            ],
            "ExpectedBehavior": "The MR task should handle the preemption gracefully and not report any exceptions related to the FileSystem closure to the Application Master.",
            "ObservedBehavior": "The MR task fails and reports a fatal error to the Application Master when the FileSystem is closed during the shutdown process.",
            "Suggestions": "Implement a check to determine if the FileSystem is in the process of shutting down before attempting to read from it. If it is, the task should not report a failure to the Application Master.",
            "problem_location": {
                "files": [
                    "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/MapTask.java",
                    "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/Text.java"
                ],
                "classes": [
                    "org.apache.hadoop.mapred.MapTask",
                    "org.apache.hadoop.io.Text"
                ],
                "methods": [
                    "MapTask.getSplitDetails",
                    "Text.readString"
                ]
            },
            "possible_fix": "Modify the MapTask.getSplitDetails method to check if the FileSystem is closed before attempting to read split details. If the FileSystem is shutting down, log a warning and return without reporting a failure to the Application Master."
        }
    },
    {
        "filename": "MAPREDUCE-6452.json",
        "creation_time": "2015-08-14T12:22:27.000+0000",
        "bug_report": {
            "Title": "NPE when intermediate encrypt enabled for LocalRunner",
            "Description": "When running a MapReduce job with the local framework and encrypted intermediate data enabled, a NullPointerException (NPE) occurs. This issue arises during the initialization of the CryptoOutputStream, which is expected to handle encrypted data. The stack trace indicates that the NPE is thrown when the CryptoOutputStream constructor is called, suggesting that a required parameter is null.",
            "StackTrace": [
                "2015-08-14 16:27:25,248 WARN  [Thread-21] mapred.LocalJobRunner (LocalJobRunner.java:run(561)) - job_local473843898_0001",
                "java.lang.Exception: java.lang.NullPointerException",
                "at org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:463)",
                "at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:523)",
                "Caused by: java.lang.NullPointerException",
                "at org.apache.hadoop.crypto.CryptoOutputStream.<init>(CryptoOutputStream.java:92)",
                "at org.apache.hadoop.fs.crypto.CryptoFSDataOutputStream.<init>(CryptoFSDataOutputStream.java:31)",
                "at org.apache.hadoop.mapreduce.CryptoUtils.wrapIfNecessary(CryptoUtils.java:112)",
                "at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1611)",
                "at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.flush(MapTask.java:1492)",
                "at org.apache.hadoop.mapred.MapTask$NewOutputCollector.close(MapTask.java:723)",
                "at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:793)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)",
                "at org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:244)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:266)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "RootCause": "The NullPointerException is caused by a null parameter being passed to the CryptoOutputStream constructor, likely due to the configuration not being set up correctly for encrypted intermediate data.",
            "StepsToReproduce": [
                "Set the property mapreduce.framework.name to local.",
                "Set the property mapreduce.job.encrypted-intermediate-data to true.",
                "Run a MapReduce job using the local framework."
            ],
            "ExpectedBehavior": "The MapReduce job should run successfully without throwing a NullPointerException, and the intermediate data should be encrypted as expected.",
            "ObservedBehavior": "The MapReduce job fails with a NullPointerException when trying to initialize the CryptoOutputStream for encrypted intermediate data.",
            "Suggestions": "Ensure that all necessary configurations for encryption are set correctly. Check if the encryption key and codec are properly initialized before creating the CryptoOutputStream.",
            "problem_location": {
                "files": [
                    "LocalJobRunner.java",
                    "CryptoOutputStream.java",
                    "CryptoFSDataOutputStream.java",
                    "CryptoUtils.java",
                    "MapTask.java"
                ],
                "classes": [
                    "org.apache.hadoop.mapred.LocalJobRunner",
                    "org.apache.hadoop.crypto.CryptoOutputStream",
                    "org.apache.hadoop.fs.crypto.CryptoFSDataOutputStream",
                    "org.apache.hadoop.mapreduce.CryptoUtils",
                    "org.apache.hadoop.mapred.MapTask"
                ],
                "methods": [
                    "LocalJobRunner$Job.runTasks",
                    "CryptoOutputStream.<init>",
                    "CryptoFSDataOutputStream.<init>",
                    "CryptoUtils.wrapIfNecessary",
                    "MapTask$MapOutputBuffer.sortAndSpill"
                ]
            },
            "possible_fix": "Check the configuration for encryption settings in the job configuration. Ensure that the CryptoCodec and encryption key are properly initialized before the CryptoOutputStream is created. For example, add checks to ensure that the configuration is not null and contains valid values before proceeding with the encryption setup."
        }
    },
    {
        "filename": "MAPREDUCE-6649.json",
        "creation_time": "2016-03-07T16:37:13.000+0000",
        "bug_report": {
            "Title": "getFailureInfo not returning any failure info",
            "Description": "The command executed does not provide any failure information when a job fails, which is inconsistent with expected behavior. The issue arises when the job fails without a clear exit code or error message, leading to a lack of diagnostic information. This is particularly evident when comparing the output of two different job executions, where one provides detailed failure diagnostics while the other does not.",
            "StackTrace": [
                "2016-03-07 10:34:58,112 INFO  [main] mapreduce.Job (Job.java:monitorAndPrintJob(1431)) - Job job_1457364518683_0004 failed with state FAILED due to:",
                "ExitCodeException exitCode=1: ",
                "at org.apache.hadoop.util.Shell.runCommand(Shell.java:927)",
                "at org.apache.hadoop.util.Shell.run(Shell.java:838)",
                "at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1117)",
                "at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:227)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:319)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:88)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:266)",
                "at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "RootCause": "The root cause of the issue appears to be related to the handling of process exit codes and error messages in the Shell class. Specifically, the runCommand method does not properly capture or relay the error output when a command fails, leading to a lack of failure information.",
            "StepsToReproduce": [
                "Run the command: $HADOOP_PREFIX/bin/hadoop jar $HADOOP_PREFIX/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-${HADOOP_VERSION}-tests.jar sleep -Dmapreduce.jobtracker.split.metainfo.maxsize=10 -Dmapreduce.job.queuename=default -m 1 -r 1 -mt 1 -rt 1",
                "Observe the output for failure information."
            ],
            "ExpectedBehavior": "The system should provide detailed failure information, including exit codes and error messages, when a job fails.",
            "ObservedBehavior": "The command fails without providing any failure information, making it difficult to diagnose the issue.",
            "Suggestions": "Review the Shell class's runCommand method to ensure that error messages are captured and returned correctly when a command fails. Consider adding logging for exit codes and error messages to improve diagnostics.",
            "problem_location": {
                "files": [
                    "Shell.java"
                ],
                "classes": [
                    "org.apache.hadoop.util.Shell"
                ],
                "methods": [
                    "Shell.runCommand"
                ]
            },
            "possible_fix": "In the runCommand method, ensure that the error output from the process is captured and logged correctly. Modify the error handling to include more detailed logging of the exit code and any error messages returned by the process."
        }
    },
    {
        "filename": "MAPREDUCE-4048.json",
        "creation_time": "2012-03-21T10:01:09.000+0000",
        "bug_report": {
            "Title": "NullPointerException exception while accessing the Application Master UI",
            "Description": "A NullPointerException occurs when attempting to access the Application Master UI, specifically when handling a request for job attempts. The error is triggered due to missing or null parameters that are expected to be present in the request, leading to a failure in constructing the response.",
            "StackTrace": [
                "java.lang.reflect.InvocationTargetException",
                "at sun.reflect.GeneratedMethodAccessor50.invoke(Unknown Source)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "at java.lang.reflect.Method.invoke(Method.java:597)",
                "at org.apache.hadoop.yarn.webapp.Dispatcher.service(Dispatcher.java:150)",
                "at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)",
                "at com.google.inject.servlet.ServletDefinition.doService(ServletDefinition.java:263)",
                "at com.google.inject.servlet.ServletDefinition.service(ServletDefinition.java:178)",
                "at com.google.inject.servlet.ManagedServletPipeline.service(ManagedServletPipeline.java:91)",
                "at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:62)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:900)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:834)",
                "at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)",
                "at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410)",
                "at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)",
                "Caused by: java.lang.NullPointerException",
                "at com.google.common.base.Joiner.toString(Joiner.java:317)",
                "at com.google.common.base.Joiner.appendTo(Joiner.java:97)",
                "at com.google.common.base.Joiner.appendTo(Joiner.java:127)",
                "at com.google.common.base.Joiner.join(Joiner.java:158)",
                "at com.google.common.base.Joiner.join(Joiner.java:166)",
                "at org.apache.hadoop.yarn.util.StringHelper.join(StringHelper.java:102)",
                "at org.apache.hadoop.mapreduce.v2.app.webapp.AppController.badRequest(AppController.java:319)",
                "at org.apache.hadoop.mapreduce.v2.app.webapp.AppController.attempts(AppController.java:286)"
            ],
            "RootCause": "The root cause of the NullPointerException is the failure to provide necessary parameters (taskType and attemptState) in the request, which are required for constructing the response in the AppController.attempts method.",
            "StepsToReproduce": [
                "1. Start the application and submit a job.",
                "2. Access the Application Master UI.",
                "3. Navigate to the attempts section of the job.",
                "4. Observe the NullPointerException in the logs."
            ],
            "ExpectedBehavior": "The Application Master UI should display the job attempts without any errors, even if some parameters are missing.",
            "ObservedBehavior": "A NullPointerException is thrown, resulting in an error page being displayed instead of the expected job attempts.",
            "Suggestions": "Implement checks for null or empty parameters before attempting to use them in the join operation. Provide default values or error messages to guide the user.",
            "problem_location": {
                "files": [
                    "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/webapp/AppController.java",
                    "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/webapp/Dispatcher.java",
                    "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/StringHelper.java"
                ],
                "classes": [
                    "org.apache.hadoop.mapreduce.v2.app.webapp.AppController",
                    "org.apache.hadoop.yarn.webapp.Dispatcher",
                    "org.apache.hadoop.yarn.util.StringHelper"
                ],
                "methods": [
                    "AppController.attempts",
                    "AppController.badRequest",
                    "StringHelper.join"
                ]
            },
            "possible_fix": "In the AppController.attempts method, add checks for taskType and attemptState before using them in the join operation. For example:\n\nif (taskType == null || taskType.isEmpty()) {\n    throw new RuntimeException(\"missing task-type.\");\n}\nif (attemptState == null || attemptState.isEmpty()) {\n    throw new RuntimeException(\"missing attempt-state.\");\n}\n\nThis will prevent the NullPointerException from occurring."
        }
    },
    {
        "filename": "MAPREDUCE-5198.json",
        "creation_time": "2013-04-30T22:22:33.000+0000",
        "bug_report": {
            "Title": "Race condition in cleanup during task tracker reinit with LinuxTaskController",
            "Description": "The issue arises when the Job Tracker is restarted while jobs are still running, leading to a race condition during the reinitialization of the Task Tracker. This results in a ClosedChannelException when the Task Tracker attempts to write to a channel that has already been closed, causing the Task Tracker to fail during its cleanup process.",
            "StackTrace": [
                "2013-04-27 20:19:09,628 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 42075 caught: java.nio.channels.ClosedChannelException",
                "at sun.nio.ch.SocketChannelImpl.ensureWriteOpen(SocketChannelImpl.java:133)",
                "at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:324)",
                "at org.apache.hadoop.ipc.Server.channelWrite(Server.java:1717)",
                "at org.apache.hadoop.ipc.Server.access$2000(Server.java:98)",
                "at org.apache.hadoop.ipc.Server$Responder.processResponse(Server.java:744)",
                "at org.apache.hadoop.ipc.Server$Responder.doRespond(Server.java:808)",
                "at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1433)",
                "2013-04-27 20:19:10,414 ERROR org.apache.hadoop.mapred.TaskTracker: Got fatal exception while reinitializing TaskTracker: org.apache.hadoop.util.Shell$ExitCodeException",
                "at org.apache.hadoop.util.Shell.runCommand(Shell.java:255)",
                "at org.apache.hadoop.util.Shell.run(Shell.java:182)",
                "at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:375)",
                "at org.apache.hadoop.mapred.LinuxTaskController.deleteAsUser(LinuxTaskController.java:281)",
                "at org.apache.hadoop.mapred.TaskTracker.deleteUserDirectories(TaskTracker.java:779)",
                "at org.apache.hadoop.mapred.TaskTracker.initialize(TaskTracker.java:816)",
                "at org.apache.hadoop.mapred.TaskTracker.run(TaskTracker.java:2704)",
                "at org.apache.hadoop.mapred.TaskTracker.main(TaskTracker.java:3934)"
            ],
            "RootCause": "The root cause of the issue is a race condition that occurs when the Task Tracker attempts to write to a channel that has been closed due to the Job Tracker being restarted. This leads to a ClosedChannelException during the cleanup process.",
            "StepsToReproduce": [
                "1. Start a job using the Job Tracker.",
                "2. While the job is running, restart the Job Tracker.",
                "3. Observe the Task Tracker logs for errors."
            ],
            "ExpectedBehavior": "The Task Tracker should be able to reinitialize without encountering a ClosedChannelException, allowing for a smooth cleanup and restart process.",
            "ObservedBehavior": "The Task Tracker fails to reinitialize and throws a ClosedChannelException, leading to a fatal error and preventing the cleanup process from completing successfully.",
            "Suggestions": "Implement synchronization mechanisms to ensure that the Task Tracker does not attempt to write to a closed channel during reinitialization. Consider adding checks to verify the state of the channel before performing write operations.",
            "problem_location": {
                "files": [
                    "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Server.java",
                    "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/Shell.java",
                    "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/mapred/LinuxTaskController.java",
                    "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/mapred/TaskTracker.java"
                ],
                "classes": [
                    "org.apache.hadoop.ipc.Server",
                    "org.apache.hadoop.util.Shell",
                    "org.apache.hadoop.mapred.LinuxTaskController",
                    "org.apache.hadoop.mapred.TaskTracker"
                ],
                "methods": [
                    "Server.channelWrite",
                    "Shell.runCommand",
                    "LinuxTaskController.deleteAsUser",
                    "TaskTracker.deleteUserDirectories",
                    "TaskTracker.initialize"
                ]
            },
            "possible_fix": "To mitigate the race condition, implement a locking mechanism around the channel write operations in the Server class. Additionally, ensure that the Task Tracker checks the state of the channel before attempting to write to it. This can be done by modifying the channelWrite method to include a check for the channel's open state."
        }
    },
    {
        "filename": "MAPREDUCE-7028.json",
        "creation_time": "2017-12-20T15:33:20.000+0000",
        "bug_report": {
            "Title": "Concurrent task progress updates causing NPE in Application Master",
            "Description": "The Application Master encounters a NullPointerException (NPE) when handling concurrent task progress updates. This issue arises from the `TaskAttemptImpl.handle` method, where the state transition logic does not adequately handle concurrent updates, leading to a race condition that results in a null reference being accessed. The problem is exacerbated during high-load scenarios, such as large word count jobs, where task updates are frequent.",
            "StackTrace": [
                "2017-12-20 06:49:42,383 FATAL [AsyncDispatcher event handler] org.apache.hadoop.yarn.event.AsyncDispatcher: Error in dispatcher thread",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$StatusUpdater.transition(TaskAttemptImpl.java:2450)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$StatusUpdater.transition(TaskAttemptImpl.java:2433)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:362)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$500(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:487)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:1362)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:154)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:1543)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:1535)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:197)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:126)",
                "at java.lang.Thread.run(Thread.java:748)"
            ],
            "RootCause": "The root cause of the NullPointerException is a race condition in the state transition logic of the TaskAttemptImpl class, specifically in the handle method, where concurrent updates to task progress are not properly synchronized.",
            "StepsToReproduce": [
                "Run a large word count job with multiple concurrent task updates.",
                "Increase the frequency of task progress updates artificially.",
                "Observe the logs for NullPointerException in the Application Master."
            ],
            "ExpectedBehavior": "The Application Master should handle concurrent task progress updates without throwing a NullPointerException, ensuring that all state transitions are processed correctly.",
            "ObservedBehavior": "The Application Master throws a NullPointerException when processing concurrent task progress updates, leading to application instability.",
            "Suggestions": "Implement proper synchronization mechanisms to handle concurrent updates in the TaskAttemptImpl class. Consider using a more robust state management approach to prevent race conditions.",
            "problem_location": {
                "files": [
                    "TaskAttemptImpl.java"
                ],
                "classes": [
                    "org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl"
                ],
                "methods": [
                    "TaskAttemptImpl.handle"
                ]
            },
            "possible_fix": "To fix the issue, modify the handle method in TaskAttemptImpl to ensure that state transitions are thread-safe. This can be achieved by using a more granular locking mechanism or by employing concurrent data structures that can handle multiple updates without leading to null references."
        }
    },
    {
        "filename": "MAPREDUCE-3790.json",
        "creation_time": "2012-02-02T18:04:23.000+0000",
        "bug_report": {
            "Title": "Broken pipe on streaming job can lead to truncated output for a successful job",
            "Description": "The issue arises when a streaming job does not consume all of its input data, leading to a situation where the job is marked as successful despite producing truncated output. This occurs because the PipeMapRed class's method mapRedFinished() does not properly handle IOExceptions, allowing the job to complete without ensuring that all output has been processed. Consequently, if the output threads are still busy when the DFS streams are closed, the job may lose output data.",
            "StackTrace": [
                "2012-02-02 11:27:25,054 WARN [main] org.apache.hadoop.streaming.PipeMapRed: java.io.IOException: Broken pipe",
                "2012-02-02 11:27:25,056 WARN [Thread-12] org.apache.hadoop.streaming.PipeMapRed: java.io.IOException: Bad file descriptor",
                "2012-02-02 11:27:25,127 WARN [Thread-11] org.apache.hadoop.streaming.PipeMapRed: java.io.IOException: DFSOutputStream is closed"
            ],
            "RootCause": "The root cause of the issue is that the PipeMapRed.mapRedFinished() method swallows IOExceptions and returns without waiting for the output threads to complete, which can lead to premature closure of DFS streams and loss of output data.",
            "StepsToReproduce": [
                "Create a simple streaming job with a non-zero-length input file.",
                "Run the job using the Hadoop streaming jar with a mapper that does not consume all input.",
                "Check the output directory for truncated output."
            ],
            "ExpectedBehavior": "The job should either consume all input data and produce complete output or fail if it does not consume all input.",
            "ObservedBehavior": "The job is marked as successful even though the output is truncated due to unconsumed input.",
            "Suggestions": "Modify the PipeMapRed.mapRedFinished() method to properly handle IOExceptions and ensure that the job only completes after all output threads have finished processing. Additionally, clarify the expected behavior when a job does not consume all input, and ensure consistency in job success criteria.",
            "problem_location": {
                "files": [
                    "hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/streaming/PipeMapRed.java",
                    "hadoop-streaming/src/main/java/org/apache/hadoop/streaming/PipeMapper.java"
                ],
                "classes": [
                    "org.apache.hadoop.streaming.PipeMapRed",
                    "org.apache.hadoop.streaming.PipeMapper"
                ],
                "methods": [
                    "PipeMapRed.mapRedFinished",
                    "PipeMapper.map"
                ]
            },
            "possible_fix": "In the PipeMapRed class, modify the mapRedFinished() method to wait for output threads to complete before closing the DFS streams. This can be done by adding a synchronization mechanism to ensure that all output has been processed before marking the job as finished."
        }
    },
    {
        "filename": "MAPREDUCE-6357.json",
        "creation_time": "2015-05-05T18:11:38.000+0000",
        "bug_report": {
            "Title": "MultipleOutputs.write() API should document that output committing is not utilized when input path is absolute",
            "Description": "The issue arises when using the MultipleOutputs.write() method with an absolute path for the baseOutputPath. This leads to a situation where output committing is not utilized, causing reduce tasks to fail on retry due to file existence conflicts. The documentation should clarify that using absolute paths may lead to improper execution of tasks, especially when speculative execution is enabled.",
            "StackTrace": [
                "2015-04-28 23:13:10,452 WARN [main] org.apache.hadoop.mapred.YarnChild: Exception running child : java.io.IOException: File already exists: wasb://full20150320@bgtstoragefull.blob.core.windows.net/user/hadoop/some/path/block-r-00299.bz2",
                "at org.apache.hadoop.fs.azure.NativeAzureFileSystem.create(NativeAzureFileSystem.java:1354)",
                "at org.apache.hadoop.fs.azure.NativeAzureFileSystem.create(NativeAzureFileSystem.java:1195)",
                "at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:908)",
                "at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:889)",
                "at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:786)",
                "at org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:135)",
                "at org.apache.hadoop.mapreduce.lib.output.MultipleOutputs.getRecordWriter(MultipleOutputs.java:475)",
                "at org.apache.hadoop.mapreduce.lib.output.MultipleOutputs.write(MultipleOutputs.java:433)",
                "at com.ancestry.bigtree.hadoop.LevelReducer.processValue(LevelReducer.java:91)",
                "at com.ancestry.bigtree.hadoop.LevelReducer.reduce(LevelReducer.java:69)",
                "at com.ancestry.bigtree.hadoop.LevelReducer.reduce(LevelReducer.java:14)",
                "at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:171)",
                "at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:627)",
                "at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:389)",
                "at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)"
            ],
            "RootCause": "The root cause of the issue is that when an absolute path is provided to the MultipleOutputs.write() method, the output committing mechanism is bypassed, leading to file existence errors during retries.",
            "StepsToReproduce": [
                "1. Set up a Hadoop job that uses MultipleOutputs.write() with an absolute output path.",
                "2. Run the job and observe the behavior.",
                "3. Trigger a retry of the reduce task and check for exceptions related to file existence."
            ],
            "ExpectedBehavior": "The system should handle output committing correctly, allowing for retries without file existence errors.",
            "ObservedBehavior": "The system throws an IOException indicating that the file already exists when a reduce task is retried.",
            "Suggestions": "Update the documentation for MultipleOutputs.write() to include a warning about the use of absolute paths and their implications on output committing.",
            "problem_location": {
                "files": [
                    "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapred/MultipleOutputs.java",
                    "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/ReduceTask.java"
                ],
                "classes": [
                    "org.apache.hadoop.mapreduce.lib.output.MultipleOutputs",
                    "org.apache.hadoop.mapred.ReduceTask"
                ],
                "methods": [
                    "MultipleOutputs.write",
                    "ReduceTask.run"
                ]
            },
            "possible_fix": "Add a note in the MultipleOutputs.write() documentation stating that using absolute paths may lead to improper execution of tasks on retry or when MR speculative execution is enabled."
        }
    },
    {
        "filename": "MAPREDUCE-6091.json",
        "creation_time": "2014-09-16T01:19:58.000+0000",
        "bug_report": {
            "Title": "YARNRunner.getJobStatus() fails with ApplicationNotFoundException if the job rolled off the RM view",
            "Description": "The method YARNRunner.getJobStatus() is unable to retrieve the status of a job that has rolled off the ResourceManager (RM) view, resulting in an ApplicationNotFoundException. This issue arises because the ClientRMService now throws an ApplicationNotFoundException for unknown application IDs, a change introduced in YARN-873. The previous behavior allowed fallback to the job history server, which is no longer the case.",
            "StackTrace": [
                "2014-09-15 07:09:51,084 ERROR org.apache.pig.tools.grunt.Grunt: ERROR 6017: JobID: job_1410289045532_90542 Reason: java.io.IOException: org.apache.hadoop.yarn.exceptions.ApplicationNotFoundException: Application with id 'application_1410289045532_90542' doesn't exist in RM.",
                "at org.apache.hadoop.yarn.server.resourcemanager.ClientRMService.getApplicationReport(ClientRMService.java:288)",
                "at org.apache.hadoop.yarn.api.impl.pb.service.ApplicationClientProtocolPBServiceImpl.getApplicationReport(ApplicationClientProtocolPBServiceImpl.java:150)",
                "at org.apache.hadoop.yarn.proto.ApplicationClientProtocol$ApplicationClientProtocolService$2.callBlockingMethod(ApplicationClientProtocol.java:337)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:587)",
                "at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2058)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2054)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1547)",
                "at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2052)",
                "at org.apache.hadoop.mapred.ClientServiceDelegate.invoke(ClientServiceDelegate.java:348)",
                "at org.apache.hadoop.mapred.ClientServiceDelegate.getJobStatus(ClientServiceDelegate.java:419)",
                "at org.apache.hadoop.mapred.YARNRunner.getJobStatus(YARNRunner.java:559)",
                "at org.apache.hadoop.mapreduce.Job$1.run(Job.java:314)",
                "at org.apache.hadoop.mapreduce.Job$1.run(Job.java:311)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:396)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1547)",
                "at org.apache.hadoop.mapreduce.Job.updateStatus(Job.java:311)",
                "at org.apache.hadoop.mapreduce.Job.isComplete(Job.java:599)",
                "at org.apache.hadoop.mapreduce.lib.jobcontrol.ControlledJob.checkRunningState(ControlledJob.java:257)",
                "at org.apache.hadoop.mapreduce.lib.jobcontrol.ControlledJob.checkState(ControlledJob.java:282)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "at java.lang.reflect.Method.invoke(Method.java:597)",
                "at org.apache.pig.backend.hadoop23.PigJobControl.checkState(PigJobControl.java:120)",
                "at org.apache.pig.backend.hadoop23.PigJobControl.run(PigJobControl.java:180)",
                "at java.lang.Thread.run(Thread.java:662)",
                "at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher$1.run(MapReduceLauncher.java:279)"
            ],
            "RootCause": "The root cause of the issue is the change in behavior of the ClientRMService, which now throws an ApplicationNotFoundException for application IDs that are no longer present in the ResourceManager view, instead of returning null. This change was introduced in YARN-873.",
            "StepsToReproduce": [
                "Submit a job to YARN.",
                "Wait for the job to complete and roll off the ResourceManager view.",
                "Attempt to query the job status using YARNRunner.getJobStatus() with the job ID."
            ],
            "ExpectedBehavior": "The system should be able to retrieve the job status from the job history server even if the job has rolled off the ResourceManager view.",
            "ObservedBehavior": "The system throws an ApplicationNotFoundException when attempting to retrieve the job status for a job that has rolled off the RM view.",
            "Suggestions": "Consider modifying the ClientServiceDelegate to check the job history server for job status if the application ID is not found in the ResourceManager view.",
            "problem_location": {
                "files": [
                    "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/main/java/org/apache/hadoop/mapred/YARNRunner.java",
                    "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/main/java/org/apache/hadoop/mapred/ClientServiceDelegate.java",
                    "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/UserGroupInformation.java"
                ],
                "classes": [
                    "org.apache.hadoop.mapred.YARNRunner",
                    "org.apache.hadoop.mapred.ClientServiceDelegate",
                    "org.apache.hadoop.security.UserGroupInformation"
                ],
                "methods": [
                    "YARNRunner.getJobStatus",
                    "ClientServiceDelegate.getJobStatus",
                    "UserGroupInformation.doAs"
                ]
            },
            "possible_fix": "Modify the getJobStatus method in ClientServiceDelegate to include a fallback mechanism to query the job history server if an ApplicationNotFoundException is caught. This could involve checking if the job ID is valid and then attempting to retrieve the job status from the history server."
        }
    },
    {
        "filename": "MAPREDUCE-4825.json",
        "creation_time": "2012-11-27T19:01:45.000+0000",
        "bug_report": {
            "Title": "JobImpl.finished doesn't expect ERROR as a final job state",
            "Description": "The method JobImpl.finished is not designed to handle the ERROR state as a valid final job state. When an event with the ERROR state is processed, it leads to an IllegalArgumentException being thrown, causing the AsyncDispatcher to exit unexpectedly. This issue arises during the execution of the test case TestMRApp.testJobError, where an invalid event is dispatched, resulting in a fatal error in the dispatcher thread.",
            "StackTrace": [
                "2012-11-27 18:46:15,240 ERROR [AsyncDispatcher event handler] impl.TaskImpl (TaskImpl.java:internalError(665)) - Invalid event T_SCHEDULE on Task task_0_0000_m_000000",
                "2012-11-27 18:46:15,242 FATAL [AsyncDispatcher event handler] event.AsyncDispatcher (AsyncDispatcher.java:dispatch(132)) - Error in dispatcher thread",
                "java.lang.IllegalArgumentException: Illegal job state: ERROR",
                "\tat org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.finished(JobImpl.java:838)",
                "\tat org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$InternalErrorTransition.transition(JobImpl.java:1622)",
                "\tat org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$InternalErrorTransition.transition(JobImpl.java:1)",
                "\tat org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:359)",
                "\tat org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:299)",
                "\tat org.apache.hadoop.yarn.state.StateMachineFactory.access$3(StateMachineFactory.java:287)",
                "\tat org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:445)",
                "\tat org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:723)",
                "\tat org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:1)",
                "\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:974)",
                "\tat org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:1)",
                "\tat org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:128)",
                "\tat org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:77)",
                "\tat java.lang.Thread.run(Thread.java:662)",
                "2012-11-27 18:46:15,242 INFO  [AsyncDispatcher event handler] event.AsyncDispatcher (AsyncDispatcher.java:dispatch(135)) - Exiting, bbye.."
            ],
            "RootCause": "The root cause of the issue is that the JobImpl.finished method does not account for the ERROR state as a valid final state, leading to an IllegalArgumentException when such a state is encountered.",
            "StepsToReproduce": [
                "Run the test case TestMRApp.testJobError.",
                "Observe the console output for errors related to invalid job states.",
                "Note the exception thrown in the AsyncDispatcher."
            ],
            "ExpectedBehavior": "The system should handle the ERROR state gracefully without throwing an exception, allowing the dispatcher to continue operating.",
            "ObservedBehavior": "The system throws an IllegalArgumentException when encountering the ERROR state, causing the AsyncDispatcher to exit unexpectedly.",
            "Suggestions": "Modify the JobImpl.finished method to handle the ERROR state appropriately, either by logging it or by defining a specific behavior for it.",
            "problem_location": {
                "files": [
                    "hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/JobImpl.java"
                ],
                "classes": [
                    "org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl"
                ],
                "methods": [
                    "JobImpl.finished",
                    "JobImpl.handle"
                ]
            },
            "possible_fix": "In the JobImpl.finished method, add a case for the ERROR state in the switch statement to handle it appropriately, such as logging the error or transitioning to a safe state."
        }
    },
    {
        "filename": "MAPREDUCE-3319.json",
        "creation_time": "2011-10-31T21:41:36.000+0000",
        "bug_report": {
            "Title": "ClassCastException in multifilewc example in Hadoop 0.20.205.0",
            "Description": "The multifile word count (multifilewc) example in Hadoop version 0.20.205.0 fails with a ClassCastException when attempting to run a job with multiple input files. The error occurs because the LongSumReducer is expecting LongWritable values but is receiving IntWritable values instead, leading to a casting error.",
            "StackTrace": [
                "java.lang.ClassCastException: org.apache.hadoop.io.IntWritable cannot be cast to org.apache.hadoop.io.LongWritable",
                "at org.apache.hadoop.mapred.lib.LongSumReducer.reduce(LongSumReducer.java:44)",
                "at org.apache.hadoop.mapred.Task$OldCombinerRunner.combine(Task.java:1431)",
                "at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1436)",
                "at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.flush(MapTask.java:1298)",
                "at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:437)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:372)",
                "at org.apache.hadoop.mapred.Child$4.run(Child.java:255)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:396)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1059)",
                "at org.apache.hadoop.mapred.Child.main(Child.java:249)"
            ],
            "RootCause": "The root cause of the issue is that the LongSumReducer is designed to work with LongWritable values, but the input data is providing IntWritable values, resulting in a ClassCastException when the reducer attempts to cast the input values.",
            "StepsToReproduce": [
                "Run the command: /usr/lib/hadoop/bin/hadoop jar /usr/lib/hadoop/hadoop-examples-0.20.205.0.22.jar multifilewc examples/text examples-output/multifilewc",
                "Observe the logs for the ClassCastException."
            ],
            "ExpectedBehavior": "The multifilewc job should successfully process the input files and produce the correct word count output without any exceptions.",
            "ObservedBehavior": "The job fails with a ClassCastException, preventing the completion of the word count task.",
            "Suggestions": "Modify the input data to ensure that the values being processed by the LongSumReducer are of type LongWritable. Alternatively, consider using a different reducer that can handle IntWritable values.",
            "problem_location": {
                "files": [
                    "hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/lib/LongSumReducer.java",
                    "hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/Task.java",
                    "hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/MapTask.java"
                ],
                "classes": [
                    "org.apache.hadoop.mapred.lib.LongSumReducer",
                    "org.apache.hadoop.mapred.Task",
                    "org.apache.hadoop.mapred.MapTask"
                ],
                "methods": [
                    "LongSumReducer.reduce",
                    "Task$OldCombinerRunner.combine",
                    "MapTask$MapOutputBuffer.sortAndSpill",
                    "MapTask.runOldMapper",
                    "MapTask.run"
                ]
            },
            "possible_fix": "To fix the issue, ensure that the input data for the LongSumReducer is of type LongWritable. This may involve modifying the mapper to output LongWritable values instead of IntWritable. For example, if the mapper currently outputs IntWritable, change it to output LongWritable by using 'new LongWritable(value)' instead of 'new IntWritable(value)'."
        }
    },
    {
        "filename": "MAPREDUCE-6361.json",
        "creation_time": "2015-05-11T15:36:10.000+0000",
        "bug_report": {
            "Title": "NPE issue in shuffle caused by concurrent issue between copySucceeded() in one thread and copyFailed() in another thread on the same host",
            "Description": "The bug report indicates a NullPointerException (NPE) occurring during the shuffle process in a Hadoop MapReduce job. This issue arises from a race condition where one thread is executing the copySucceeded() method while another thread is executing copyFailed() on the same host. The NPE is triggered when the copyFailed() method attempts to access a resource that has already been cleaned up or is in an inconsistent state due to concurrent modifications.",
            "StackTrace": [
                "2015-05-08 21:00:00,513 WARN [main] org.apache.hadoop.mapred.YarnChild: Exception running child : org.apache.hadoop.mapreduce.task.reduce.Shuffle$ShuffleError: error in shuffle in fetcher#25",
                "at org.apache.hadoop.mapreduce.task.reduce.Shuffle.run(Shuffle.java:134)",
                "at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:376)",
                "at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:163)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)",
                "Caused by: java.lang.NullPointerException",
                "at org.apache.hadoop.mapreduce.task.reduce.ShuffleSchedulerImpl.copyFailed(ShuffleSchedulerImpl.java:267)",
                "at org.apache.hadoop.mapreduce.task.reduce.Fetcher.copyFromHost(Fetcher.java:308)",
                "at org.apache.hadoop.mapreduce.task.reduce.Fetcher.run(Fetcher.java:193)"
            ],
            "RootCause": "The root cause of the NPE is a race condition between the copySucceeded() and copyFailed() methods, where shared resources are accessed concurrently without proper synchronization, leading to inconsistent states.",
            "StepsToReproduce": [
                "Run a Hadoop MapReduce job with multiple reducers.",
                "Simulate a failure in one of the map tasks while another task is successfully completing.",
                "Observe the logs for NPE in the shuffle process."
            ],
            "ExpectedBehavior": "The shuffle process should handle concurrent success and failure scenarios gracefully without throwing exceptions.",
            "ObservedBehavior": "A NullPointerException is thrown during the shuffle process, causing the job to fail.",
            "Suggestions": "Implement proper synchronization mechanisms (e.g., using locks) around the shared resources accessed by copySucceeded() and copyFailed() methods to prevent race conditions.",
            "problem_location": {
                "files": [
                    "ShuffleSchedulerImpl.java",
                    "Fetcher.java",
                    "Shuffle.java",
                    "ReduceTask.java"
                ],
                "classes": [
                    "org.apache.hadoop.mapreduce.task.reduce.ShuffleSchedulerImpl",
                    "org.apache.hadoop.mapreduce.task.reduce.Fetcher",
                    "org.apache.hadoop.mapreduce.task.reduce.Shuffle",
                    "org.apache.hadoop.mapred.ReduceTask"
                ],
                "methods": [
                    "ShuffleSchedulerImpl.copyFailed",
                    "Fetcher.copyFromHost",
                    "Fetcher.run",
                    "Shuffle.run"
                ]
            },
            "possible_fix": "Add synchronized blocks or use concurrent data structures in the copyFailed() and copySucceeded() methods to ensure thread safety. For example, modify the copyFailed() method to synchronize access to shared resources that may be modified by copySucceeded()."
        }
    },
    {
        "filename": "MAPREDUCE-4848.json",
        "creation_time": "2012-12-05T04:01:47.000+0000",
        "bug_report": {
            "Title": "TaskAttemptContext cast error during AM recovery",
            "Description": "During the recovery of an Application Master (AM) after a failure, a ClassCastException occurs when the system attempts to cast an instance of TaskAttemptContextImpl to TaskAttemptContext. This issue arises in the OutputCommitter's recoverTask method, which expects a TaskAttemptContext but receives a TaskAttemptContextImpl instead. This leads to a failure in the recovery process, causing the AM to exit prematurely.",
            "StackTrace": [
                "java.lang.ClassCastException: org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl cannot be cast to org.apache.hadoop.mapred.TaskAttemptContext",
                "at org.apache.hadoop.mapred.OutputCommitter.recoverTask(OutputCommitter.java:284)",
                "at org.apache.hadoop.mapreduce.v2.app.recover.RecoveryService$InterceptingEventHandler.handle(RecoveryService.java:361)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$ContainerAssignedTransition.transition(TaskAttemptImpl.java:1211)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$ContainerAssignedTransition.transition(TaskAttemptImpl.java:1177)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:357)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:298)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:443)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:958)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:135)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:926)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:918)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:126)",
                "at org.apache.hadoop.mapreduce.v2.app.recover.RecoveryService$RecoveryDispatcher.realDispatch(RecoveryService.java:285)",
                "at org.apache.hadoop.mapreduce.v2.app.recover.RecoveryService$RecoveryDispatcher.dispatch(RecoveryService.java:281)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:75)",
                "at java.lang.Thread.run(Thread.java:619)"
            ],
            "RootCause": "The root cause of the issue is a ClassCastException occurring when the OutputCommitter's recoverTask method attempts to cast a TaskAttemptContextImpl object to a TaskAttemptContext. This indicates a mismatch between the expected and actual types being used during the recovery process.",
            "StepsToReproduce": [
                "1. Start an Application Master (AM) with a job.",
                "2. Force the AM to fail (e.g., by simulating a crash).",
                "3. Observe the recovery process initiated by the Resource Manager (RM).",
                "4. Monitor the logs for ClassCastException during the recovery phase."
            ],
            "ExpectedBehavior": "The AM should recover successfully without throwing a ClassCastException, allowing the job to continue processing.",
            "ObservedBehavior": "The AM fails during recovery due to a ClassCastException, causing it to exit prematurely.",
            "Suggestions": "Refactor the recoverTask method to ensure that it accepts the correct type of TaskAttemptContext. Alternatively, modify the casting logic to handle the TaskAttemptContextImpl appropriately.",
            "problem_location": {
                "files": [
                    "OutputCommitter.java",
                    "TaskAttemptImpl.java"
                ],
                "classes": [
                    "org.apache.hadoop.mapred.OutputCommitter",
                    "org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl"
                ],
                "methods": [
                    "OutputCommitter.recoverTask",
                    "TaskAttemptImpl.handle"
                ]
            },
            "possible_fix": "Modify the recoverTask method in OutputCommitter to accept TaskAttemptContextImpl directly or implement a proper conversion mechanism to avoid ClassCastException. Example change:\n\nvoid recoverTask(TaskAttemptContextImpl taskContext) throws IOException {\n    // Recovery logic here\n}"
        }
    },
    {
        "filename": "MAPREDUCE-3226.json",
        "creation_time": "2011-10-20T06:38:04.000+0000",
        "bug_report": {
            "Title": "Few reduce tasks hanging in a gridmix-run",
            "Description": "During a gridmix run with approximately 1000 jobs, several reducer tasks are hanging after successfully downloading all map outputs. The hanging reducers are observed to be in a TIMED_WAITING state, indicating they are waiting for a condition that is not being met. The issue appears to stem from the EventFetcher thread, which is responsible for fetching map completion events. The thread dump shows that the EventFetcher is in a sleep state, while the main thread is waiting for it to complete, leading to a deadlock situation.",
            "StackTrace": [
                "\"EventFetcher for fetching Map Completion Events\" daemon prio=10 tid=0xa325fc00 nid=0x1ca4 waiting on condition [0xa315c000]",
                "java.lang.Thread.State: TIMED_WAITING (sleeping)",
                "at java.lang.Thread.sleep(Native Method)",
                "at org.apache.hadoop.mapreduce.task.reduce.EventFetcher.run(EventFetcher.java:71)",
                "\"main\" prio=10 tid=0x080ed400 nid=0x1c71 in Object.wait() [0xf73a2000]",
                "java.lang.Thread.State: WAITING (on object monitor)",
                "at java.lang.Object.wait(Native Method)",
                "- waiting on <0xa94b23d8> (a org.apache.hadoop.mapreduce.task.reduce.EventFetcher)",
                "at java.lang.Thread.join(Thread.java:1143)",
                "at java.lang.Thread.join(Thread.java:1196)",
                "at org.apache.hadoop.mapreduce.task.reduce.Shuffle.run(Shuffle.java:135)",
                "at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:367)",
                "at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:147)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:396)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1135)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:142)"
            ],
            "RootCause": "The root cause of the hanging reducers is due to the EventFetcher thread being in a TIMED_WAITING state while the main thread is waiting for it to complete. This indicates a potential deadlock or failure in fetching map completion events, which prevents the reducers from progressing.",
            "StepsToReproduce": [
                "Run a gridmix job with approximately 1000 jobs.",
                "Monitor the reducer tasks for any hanging states.",
                "Check the thread dumps for the EventFetcher and main threads."
            ],
            "ExpectedBehavior": "All reducer tasks should complete successfully without hanging, and the EventFetcher should fetch map completion events without entering a sleep state indefinitely.",
            "ObservedBehavior": "Several reducer tasks are hanging, with the EventFetcher thread in a TIMED_WAITING state and the main thread waiting for it to complete, leading to a deadlock.",
            "Suggestions": "Investigate the logic in the EventFetcher to ensure it handles failures and retries correctly. Consider adding timeout mechanisms or error handling to prevent the thread from entering a sleep state indefinitely.",
            "problem_location": {
                "files": [
                    "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/task/reduce/EventFetcher.java",
                    "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/ReduceTask.java",
                    "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapred/YarnChild.java"
                ],
                "classes": [
                    "org.apache.hadoop.mapreduce.task.reduce.EventFetcher",
                    "org.apache.hadoop.mapred.ReduceTask",
                    "org.apache.hadoop.mapred.YarnChild"
                ],
                "methods": [
                    "EventFetcher.run",
                    "ReduceTask.run",
                    "YarnChild.main"
                ]
            },
            "possible_fix": "Review the EventFetcher.run method to ensure that it properly handles exceptions and does not enter a sleep state indefinitely. Implement a timeout for the sleep period and add logging to capture any failures in fetching map completion events."
        }
    },
    {
        "filename": "MAPREDUCE-3070.json",
        "creation_time": "2011-09-22T14:33:58.000+0000",
        "bug_report": {
            "Title": "NM not able to register with RM after NM restart",
            "Description": "After gracefully stopping the NodeManager (NM) and subsequently restarting it, the NM fails to register with the ResourceManager (RM) due to a 'Duplicate registration from the node!' error. This issue arises during the registration process where the NM attempts to register itself with the RM, but the RM already has a record of the NM's previous registration, leading to a conflict.",
            "StackTrace": [
                "2011-09-23 01:50:46,705 FATAL nodemanager.NodeManager (NodeManager.java:main(204)) - Error starting NodeManager",
                "org.apache.hadoop.yarn.YarnException: Failed to Start org.apache.hadoop.yarn.server.nodemanager.NodeManager",
                "at org.apache.hadoop.yarn.service.CompositeService.start(CompositeService.java:78)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.start(NodeManager.java:153)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.main(NodeManager.java:202)",
                "Caused by: org.apache.avro.AvroRuntimeException: org.apache.hadoop.yarn.exceptions.impl.pb.YarnRemoteExceptionPBImpl: Duplicate registration from the node!",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.start(NodeStatusUpdaterImpl.java:141)",
                "at org.apache.hadoop.yarn.service.CompositeService.start(CompositeService.java:68)",
                "... 2 more",
                "Caused by: org.apache.hadoop.yarn.exceptions.impl.pb.YarnRemoteExceptionPBImpl: Duplicate registration from the node!",
                "at org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Invoker.invoke(ProtoOverHadoopRpcEngine.java:142)",
                "at $Proxy13.registerNodeManager(Unknown Source)",
                "at org.apache.hadoop.yarn.server.api.impl.pb.client.ResourceTrackerPBClientImpl.registerNodeManager(ResourceTrackerPBClientImpl.java:59)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.registerWithRM(NodeStatusUpdaterImpl.java:175)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.start(NodeStatusUpdaterImpl.java:137)",
                "... 3 more"
            ],
            "RootCause": "The root cause of the issue is that the NodeManager is attempting to register with the ResourceManager while a previous registration still exists. This can occur if the NodeManager does not properly deregister itself before shutting down or if the ResourceManager has not cleared the previous registration in a timely manner.",
            "StepsToReproduce": [
                "1. Start the NodeManager.",
                "2. Gracefully stop the NodeManager.",
                "3. Start the NodeManager again.",
                "4. Observe the logs for registration errors."
            ],
            "ExpectedBehavior": "The NodeManager should successfully register with the ResourceManager after a restart without any errors.",
            "ObservedBehavior": "The NodeManager fails to register with the ResourceManager, resulting in a 'Duplicate registration from the node!' error.",
            "Suggestions": "Ensure that the NodeManager properly deregisters itself from the ResourceManager before shutting down. Additionally, consider implementing a delay or a check to confirm that the previous registration has been cleared before allowing a new registration attempt.",
            "problem_location": {
                "files": [
                    "NodeManager.java",
                    "NodeStatusUpdaterImpl.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.nodemanager.NodeManager",
                    "org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl"
                ],
                "methods": [
                    "NodeManager.start",
                    "NodeStatusUpdaterImpl.registerWithRM",
                    "NodeStatusUpdaterImpl.start"
                ]
            },
            "possible_fix": "In the NodeStatusUpdaterImpl class, modify the registerWithRM method to check for existing registrations and handle them appropriately. For example, implement a deregistration process before attempting to register again."
        }
    },
    {
        "filename": "MAPREDUCE-3932.json",
        "creation_time": "2012-02-27T22:39:43.000+0000",
        "bug_report": {
            "Title": "MR tasks failing and crashing the AM when available-resources/headRoom becomes zero",
            "Description": "The issue arises when the available resources (headRoom) for memory drop to zero, leading to the preemption of reduce tasks. This preemption causes the Application Master (AM) to crash due to invalid state transitions when handling task attempts that cannot be launched. The logs indicate that the system attempts to assign containers for tasks, but when the headRoom is zero, it leads to a failure in launching the containers, resulting in an InvalidStateTransitonException.",
            "StackTrace": [
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: TA_CONTAINER_LAUNCH_FAILED at KILL_TASK_CLEANUP",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:301)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:926)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:870)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:848)"
            ],
            "RootCause": "The root cause of the issue is the lack of proper handling for task attempts when the available resources (headRoom) become zero, leading to invalid state transitions in the task attempt and job state machines.",
            "StepsToReproduce": [
                "1. Start a MapReduce job with a configuration that allows for resource allocation.",
                "2. Monitor the available resources and ensure that the headRoom drops to zero during execution.",
                "3. Observe the behavior of the Application Master and the task attempts."
            ],
            "ExpectedBehavior": "The Application Master should handle the situation gracefully without crashing, allowing for proper state transitions and error handling when resources are insufficient.",
            "ObservedBehavior": "The Application Master crashes with an InvalidStateTransitonException when it attempts to handle task attempts that cannot be launched due to zero available resources.",
            "Suggestions": "Implement checks to prevent task attempts from being assigned when headRoom is zero. Additionally, improve error handling in the state machine to manage invalid transitions more gracefully.",
            "problem_location": {
                "files": [
                    "hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/JobImpl.java",
                    "hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/state/StateMachineFactory.java",
                    "hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java"
                ],
                "classes": [
                    "org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl",
                    "org.apache.hadoop.yarn.state.StateMachineFactory",
                    "org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl"
                ],
                "methods": [
                    "JobImpl.handle",
                    "StateMachineFactory.doTransition",
                    "TaskAttemptImpl.handle"
                ]
            },
            "possible_fix": "In the TaskAttemptImpl.handle method, add a check to ensure that task attempts are only processed if there are sufficient resources available. If headRoom is zero, log an appropriate message and prevent further processing of the task attempt."
        }
    },
    {
        "filename": "MAPREDUCE-4062.json",
        "creation_time": "2012-03-23T21:42:23.000+0000",
        "bug_report": {
            "Title": "AM Launcher thread can hang forever",
            "Description": "The ResourceManager (RM) can become unresponsive when the Application Master (AM) launcher thread hangs indefinitely due to issues with the NodeManager (NM) node. This situation arises because the current implementation only allows a single launcher thread, which can lead to a bottleneck if multiple nodes fail. The observed hang lasted approximately 9 hours, indicating a critical flaw in the handling of container launches.",
            "StackTrace": [
                "\"pool-1-thread-1\" prio=10 tid=0x000000004343e800 nid=0x3a4c in Object.wait()",
                "[0x000000004fad2000]",
                "java.lang.Thread.State: WAITING (on object monitor)",
                "at java.lang.Object.wait(Native Method)",
                "at java.lang.Object.wait(Object.java:485)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1076)",
                "- locked <0x00002aab05a4f3f0> (a org.apache.hadoop.ipc.Client$Call)",
                "at org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Invoker.invoke(ProtoOverHadoopRpcEngine.java:135)",
                "at $Proxy76.startContainer(Unknown Source)",
                "at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagerPBClientImpl.startContainer(ContainerManagerPBClientImpl.java:87)",
                "at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.launch(AMLauncher.java:118)",
                "at org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.run(AMLauncher.java:265)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)",
                "at java.lang.Thread.run(Thread.java:619)"
            ],
            "RootCause": "The root cause of the issue is that the AM launcher thread is waiting indefinitely for a response from the NodeManager when attempting to start a container. This occurs due to a lack of proper error handling and timeout mechanisms in the communication between the ResourceManager and NodeManager.",
            "StepsToReproduce": [
                "1. Deploy a cluster with a single NodeManager.",
                "2. Simulate a failure in the NodeManager.",
                "3. Attempt to launch an Application Master that requires the failed NodeManager.",
                "4. Observe the ResourceManager's AM launcher thread hanging indefinitely."
            ],
            "ExpectedBehavior": "The ResourceManager should handle failures gracefully, either by retrying the launch on another NodeManager or by failing the application attempt with an appropriate error message.",
            "ObservedBehavior": "The ResourceManager hangs indefinitely, preventing any further application launches and leading to a complete stall in resource management operations.",
            "Suggestions": "Implement a timeout mechanism for the call to start a container and enhance error handling to manage NodeManager failures more effectively. Additionally, consider increasing the number of launcher threads to allow for concurrent handling of multiple application launches.",
            "problem_location": {
                "files": [
                    "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/amlauncher/AMLauncher.java",
                    "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/api/impl/pb/client/ContainerManagerPBClientImpl.java",
                    "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Client.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher",
                    "org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagerPBClientImpl",
                    "org.apache.hadoop.ipc.Client"
                ],
                "methods": [
                    "AMLauncher.launch",
                    "ContainerManagerPBClientImpl.startContainer",
                    "Client.call"
                ]
            },
            "possible_fix": "Add a timeout to the Client.call method to prevent indefinite waiting. For example, modify the call method to include a timeout parameter and handle the timeout exception appropriately. Additionally, consider implementing a retry mechanism in the AMLauncher.launch method to attempt launching the container on a different NodeManager if the first attempt fails."
        }
    },
    {
        "filename": "MAPREDUCE-3066.json",
        "creation_time": "2011-09-21T22:42:00.000+0000",
        "bug_report": {
            "Title": "YARN NM fails to start",
            "Description": "The NodeManager fails to start due to an improperly configured resource tracker address. The error message indicates that the configuration for 'yarn.resourcemanager.resource-tracker.address' is not a valid host:port pair, which leads to a RuntimeException during the NodeStatusUpdaterImpl's attempt to create a socket address for the ResourceManager client.",
            "StackTrace": [
                "2011-09-21 15:36:33,534 INFO  service.AbstractService (AbstractService.java:stop(71)) - Service:org.apache.hadoop.yarn.server.nodemanager.DeletionService is stopped.",
                "2011-09-21 15:36:33,534 FATAL nodemanager.NodeManager (NodeManager.java:main(204)) - Error starting NodeManager",
                "org.apache.hadoop.yarn.YarnException: Failed to Start org.apache.hadoop.yarn.server.nodemanager.NodeManager",
                "at org.apache.hadoop.yarn.service.CompositeService.start(CompositeService.java:78)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.start(NodeManager.java:153)",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeManager.main(NodeManager.java:202)",
                "Caused by: org.apache.avro.AvroRuntimeException: java.lang.RuntimeException: Not a host:port pair: yarn.resourcemanager.resource-tracker.address",
                "at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.start(NodeStatusUpdaterImpl.java:141)",
                "at org.apache.hadoop.yarn.service.CompositeService.start(CompositeService.java:68)",
                "... 2 more"
            ],
            "RootCause": "The root cause of the issue is that the configuration for 'yarn.resourcemanager.resource-tracker.address' is not set correctly, leading to a failure in creating a socket address in the NetUtils.createSocketAddr method.",
            "StepsToReproduce": [
                "1. Check the configuration file for YARN.",
                "2. Ensure that 'yarn.resourcemanager.resource-tracker.address' is set to a valid host:port pair.",
                "3. Run the NodeManager."
            ],
            "ExpectedBehavior": "The NodeManager should start successfully without any errors related to the resource tracker address.",
            "ObservedBehavior": "The NodeManager fails to start and logs an error indicating that the resource tracker address is not a valid host:port pair.",
            "Suggestions": "Verify and correct the configuration for 'yarn.resourcemanager.resource-tracker.address' in the YARN configuration file.",
            "problem_location": {
                "files": [
                    "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeStatusUpdaterImpl.java",
                    "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/service/CompositeService.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl",
                    "org.apache.hadoop.yarn.service.CompositeService"
                ],
                "methods": [
                    "NodeStatusUpdaterImpl.start",
                    "CompositeService.start"
                ]
            },
            "possible_fix": "Ensure that the configuration for 'yarn.resourcemanager.resource-tracker.address' is set correctly in the YARN configuration file. For example, it should be in the format 'hostname:port'. If the configuration is missing, add it with a valid value."
        }
    },
    {
        "filename": "MAPREDUCE-3123.json",
        "creation_time": "2011-09-29T19:03:45.000+0000",
        "bug_report": {
            "Title": "Symbolic links with special chars causing container/task.sh to fail",
            "Description": "The job fails to execute when symbolic links contain special characters. The error occurs during the execution of a shell command in the task.sh script, specifically when trying to create a symbolic link with special characters in its name. The shell interprets these characters incorrectly, leading to a syntax error.",
            "StackTrace": [
                "org.apache.hadoop.util.Shell$ExitCodeException: /tmp/mapred-local/usercache/hadoopuser/appcache/application_1317077272567_0239/container_1317077272567_0239_01_000001/task.sh: line 26: syntax error near unexpected token `-_+='",
                "/tmp/mapred-local/usercache/hadoopuser/appcache/application_1317077272567_0239/container_1317077272567_0239_01_000001/task.sh: line 26: `ln -sf /tmp/mapred-local/usercache/hadoopqa/filecache/-1888139433818483070/InputDir testlink!@$&*()-_+='kson-jaxrs-1.7.1.jar:/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.7.3/jackson-mapper-asl-1.7.3.jar:.m2/repository/org/codehaus/jackson/jackson-xc/1.7.1/jackson-xc-1.7.1.jar:"
            ],
            "RootCause": "The root cause of the issue is the presence of special characters in the symbolic link name, which causes a syntax error in the shell command executed in task.sh. The shell does not handle these characters correctly, leading to an unexpected token error.",
            "StepsToReproduce": [
                "Create a symbolic link with special characters in its name.",
                "Run the Hadoop streaming job with the symbolic link included in the input path.",
                "Observe the failure in the task execution."
            ],
            "ExpectedBehavior": "The job should execute successfully, creating the symbolic link without errors, and process the input files as intended.",
            "ObservedBehavior": "The job fails with a syntax error in the task.sh script due to special characters in the symbolic link name.",
            "Suggestions": "Consider sanitizing the symbolic link names to remove or escape special characters before they are used in shell commands. Alternatively, modify the shell command to handle special characters correctly.",
            "problem_location": {
                "files": [
                    "task.sh",
                    "Shell.java",
                    "LinuxContainerExecutor.java",
                    "ContainerLaunch.java"
                ],
                "classes": [
                    "org.apache.hadoop.util.Shell",
                    "org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor",
                    "org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch"
                ],
                "methods": [
                    "Shell.runCommand",
                    "LinuxContainerExecutor.launchContainer",
                    "ContainerLaunch.call"
                ]
            },
            "possible_fix": "Modify the task.sh script to escape special characters in symbolic link names. For example, use a function to sanitize the link name before executing the ln command. Additionally, consider updating the Shell.runCommand method to handle special characters more gracefully."
        }
    },
    {
        "filename": "MAPREDUCE-6439.json",
        "creation_time": "2015-07-30T20:31:00.000+0000",
        "bug_report": {
            "Title": "AM may fail instead of retrying if RM shuts down during the allocate call",
            "Description": "The Application Master (AM) encounters a YarnRuntimeException when the Resource Manager (RM) shuts down during an allocate call. This results in the AM incorrectly believing it has exhausted its retry attempts, leading to a failure in resource allocation. The root of the issue lies in the handling of InterruptedExceptions, which are not being properly managed during the communication between the AM and RM.",
            "StackTrace": [
                "2015-07-25 20:07:27,346 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: Error communicating with RM: java.lang.InterruptedException",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$GenericEventHandler.handle(AsyncDispatcher.java:245)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService.allocate(ApplicationMasterService.java:469)",
                "at org.apache.hadoop.yarn.api.impl.pb.service.ApplicationMasterProtocolPBServiceImpl.allocate(ApplicationMasterProtocolPBServiceImpl.java:60)",
                "at org.apache.hadoop.yarn.proto.ApplicationMasterProtocol$ApplicationMasterProtocolService$2.callBlockingMethod(ApplicationMasterProtocol.java:99)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:587)",
                "at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1642)",
                "at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)",
                "Caused by: java.lang.InterruptedException",
                "at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireInterruptibly(AbstractQueuedSynchronizer.java:1219)",
                "at java.util.concurrent.locks.ReentrantLock.lockInterruptibly(ReentrantLock.java:340)",
                "at java.util.concurrent.LinkedBlockingQueue.put(LinkedBlockingQueue.java:338)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$GenericEventHandler.handle(AsyncDispatcher.java:240)"
            ],
            "RootCause": "The root cause of the issue is the improper handling of InterruptedExceptions during the communication between the Application Master and Resource Manager. When the RM shuts down, the AM receives an InterruptedException, which leads to a failure in retrying the allocation request.",
            "StepsToReproduce": [
                "1. Start the Resource Manager (RM).",
                "2. Start the Application Master (AM) and initiate an allocate call.",
                "3. Simulate a shutdown of the RM during the allocate call.",
                "4. Observe the behavior of the AM and check for YarnRuntimeException."
            ],
            "ExpectedBehavior": "The Application Master should handle the InterruptedException gracefully and retry the allocation request instead of failing.",
            "ObservedBehavior": "The Application Master fails with a YarnRuntimeException and does not retry the allocation request, leading to resource allocation failure.",
            "Suggestions": "Implement better exception handling in the AM to manage InterruptedExceptions and ensure that retries are attempted when the RM is temporarily unavailable.",
            "problem_location": {
                "files": [
                    "org/apache/hadoop/yarn/event/AsyncDispatcher.java",
                    "org/apache/hadoop/yarn/server/resourcemanager/ApplicationMasterService.java",
                    "org/apache/hadoop/yarn/api/impl/pb/service/ApplicationMasterProtocolPBServiceImpl.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.event.AsyncDispatcher",
                    "org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService",
                    "org.apache.hadoop.yarn.api.impl.pb.service.ApplicationMasterProtocolPBServiceImpl"
                ],
                "methods": [
                    "AsyncDispatcher.GenericEventHandler.handle",
                    "ApplicationMasterService.allocate",
                    "ApplicationMasterProtocolPBServiceImpl.allocate"
                ]
            },
            "possible_fix": "In the AsyncDispatcher's GenericEventHandler, modify the handle method to catch InterruptedExceptions and implement a retry mechanism. For example:\n\n```java\ntry {\n    // existing handling code\n} catch (InterruptedException e) {\n    // log the exception and implement retry logic\n    retryAllocation();\n}\n```"
        }
    },
    {
        "filename": "MAPREDUCE-4490.json",
        "creation_time": "2012-07-27T01:22:21.000+0000",
        "bug_report": {
            "Title": "JVM reuse is incompatible with LinuxTaskController (and therefore incompatible with Security)",
            "Description": "The issue arises when using the LinuxTaskController with JVM reuse enabled. Specifically, when there are more map tasks than available map slots, the second task in each JVM fails immediately due to the absence of the required userlog directory. This directory is only created for the first task in the JVM, leading to a failure when the second task attempts to write its log index file, resulting in an ENOENT error. This behavior is not observed with the DefaultTaskController, which creates a separate log directory for each task.",
            "StackTrace": [
                "2012-07-24 14:29:11,914 INFO org.apache.hadoop.mapred.TaskLog: Starting logging for a new task attempt_201207241401_0013_m_000027_0 in the same JVM as that of the first task /var/log/hadoop/mapred/userlogs/job_201207241401_0013/attempt_201207241401_0013_m_000006_0",
                "2012-07-24 14:29:11,915 WARN org.apache.hadoop.mapred.Child: Error running child",
                "ENOENT: No such file or directory",
                "at org.apache.hadoop.io.nativeio.NativeIO.open(Native Method)",
                "at org.apache.hadoop.io.SecureIOUtils.createForWrite(SecureIOUtils.java:161)",
                "at org.apache.hadoop.mapred.TaskLog.writeToIndexFile(TaskLog.java:296)",
                "at org.apache.hadoop.mapred.TaskLog.syncLogs(TaskLog.java:369)",
                "at org.apache.hadoop.mapred.Child.main(Child.java:229)"
            ],
            "RootCause": "The root cause of the issue is that the LinuxTaskController only creates the userlog directory for the first task in a JVM. Subsequent tasks fail to find their respective log directories, leading to ENOENT errors when attempting to write log index files.",
            "StepsToReproduce": [
                "1. Configure the Hadoop cluster to use LinuxTaskController.",
                "2. Set the JVM reuse parameter (mapred.job.reuse.jvm.num.tasks) to a value greater than 1.",
                "3. Submit a job with more map tasks than available map slots.",
                "4. Observe the task failures for the second task in each JVM."
            ],
            "ExpectedBehavior": "Each task should successfully create its own userlog directory and write its log index file without errors.",
            "ObservedBehavior": "The second task in each JVM fails with an ENOENT error due to the absence of the required userlog directory.",
            "Suggestions": "Implement a new command in the task-controller to initialize task directories. Modify the LinuxTaskController#createLogDir method to call this command, ensuring that userlog directories are created for each task attempt.",
            "problem_location": {
                "files": [
                    "hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/TaskLog.java",
                    "hadoop-common/src/main/java/org/apache/hadoop/io/SecureIOUtils.java"
                ],
                "classes": [
                    "org.apache.hadoop.mapred.TaskLog",
                    "org.apache.hadoop.io.SecureIOUtils"
                ],
                "methods": [
                    "TaskLog.writeToIndexFile",
                    "SecureIOUtils.createForWrite",
                    "TaskLog.syncLogs"
                ]
            },
            "possible_fix": "Add a new command to the task-controller to create attempt directories. Call this command in the LinuxTaskController#createLogDir method to ensure that userlog directories are created for each task attempt."
        }
    },
    {
        "filename": "MAPREDUCE-3744.json",
        "creation_time": "2012-01-27T20:31:19.000+0000",
        "bug_report": {
            "Title": "Unable to retrieve application logs via 'yarn logs' or 'mapred job -logs'",
            "Description": "The application logs cannot be retrieved using the 'yarn logs' command, resulting in a FileNotFoundException indicating that the log file does not exist. Additionally, using the 'mapred job -logs' command leads to a warning about the Job History Server not being configured, despite the process running. This suggests a potential misconfiguration or issue with log aggregation.",
            "StackTrace": [
                "Exception in thread \"main\" java.io.FileNotFoundException: File /tmp/logs/application_1327694122989_0001 does not exist.",
                "at org.apache.hadoop.fs.Hdfs$DirListingIterator.<init>(Hdfs.java:226)",
                "at org.apache.hadoop.fs.Hdfs$DirListingIterator.<init>(Hdfs.java:217)",
                "at org.apache.hadoop.fs.Hdfs$2.<init>(Hdfs.java:192)",
                "at org.apache.hadoop.fs.Hdfs.listStatusIterator(Hdfs.java:192)",
                "at org.apache.hadoop.fs.FileContext$20.next(FileContext.java:1371)",
                "at org.apache.hadoop.fs.FileContext$20.next(FileContext.java:1)",
                "at org.apache.hadoop.fs.FileContext$FSLinkResolver.resolve(FileContext.java:2319)",
                "at org.apache.hadoop.fs.FileContext.listStatus(FileContext.java:1373)",
                "at org.apache.hadoop.yarn.logaggregation.LogDumper.dumpAllContainersLogs(LogDumper.java:191)",
                "at org.apache.hadoop.yarn.logaggregation.LogDumper.run(LogDumper.java:107)",
                "at org.apache.hadoop.yarn.logaggregation.LogDumper.main(LogDumper.java:226)",
                "2012-01-27 14:05:52,040 INFO  mapred.ClientServiceDelegate (ClientServiceDelegate.java:getProxy(246)) - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server",
                "2012-01-27 14:05:52,041 WARN  mapred.ClientServiceDelegate (ClientServiceDelegate.java:checkAndGetHSProxy(257)) - Job History Server is not configured.",
                "Unable to get log information for job: job_1327694122989_0001"
            ],
            "RootCause": "The root cause appears to be that the log files for the application are not being generated or are being deleted before the retrieval attempt. Additionally, the Job History Server is not configured, which prevents access to job logs.",
            "StepsToReproduce": [
                "Submit a job to the YARN cluster.",
                "Attempt to retrieve logs using the command 'yarn logs -applicationId <application_id>'.",
                "Attempt to retrieve logs using the command 'mapred job -logs <job_id>'."
            ],
            "ExpectedBehavior": "The application logs should be retrievable without errors, and the Job History Server should be properly configured to allow access to job logs.",
            "ObservedBehavior": "The logs cannot be retrieved, resulting in a FileNotFoundException and warnings about the Job History Server not being configured.",
            "Suggestions": "Check the configuration of the log aggregation settings in YARN. Ensure that the log files are being written to the expected directory and that the Job History Server is properly configured and running.",
            "problem_location": {
                "files": [
                    "LogDumper.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.logaggregation.LogDumper"
                ],
                "methods": [
                    "LogDumper.dumpAllContainersLogs",
                    "LogDumper.run"
                ]
            },
            "possible_fix": "Verify the configuration settings for log aggregation in the YARN configuration files. Ensure that the paths for remote application logs are correctly set and that the Job History Server is configured in the YARN settings."
        }
    },
    {
        "filename": "MAPREDUCE-6815.json",
        "creation_time": "2016-12-02T00:30:37.000+0000",
        "bug_report": {
            "Title": "Fix flaky TestKill.testKillTask()",
            "Description": "The test 'TestKill.testKillTask()' is failing intermittently due to a mismatch between the expected job state and the actual job state. The test expects the job to be in the 'SUCCEEDED' state, but it is instead in the 'ERROR' state. This indicates that the job is not completing successfully, possibly due to a timeout or an unhandled exception during execution.",
            "StackTrace": [
                "java.lang.AssertionError: Job state is not correct (timedout) expected:<SUCCEEDED> but was:<ERROR>",
                "at org.junit.Assert.fail(Assert.java:88)",
                "at org.junit.Assert.failNotEquals(Assert.java:743)",
                "at org.junit.Assert.assertEquals(Assert.java:118)",
                "at org.apache.hadoop.mapreduce.v2.app.MRApp.waitForState(MRApp.java:416)",
                "at org.apache.hadoop.mapreduce.v2.app.TestKill.testKillTask(TestKill.java:124)"
            ],
            "RootCause": "The job is transitioning to an ERROR state instead of SUCCEEDED, likely due to a timeout or an exception that is not being handled properly in the job execution logic.",
            "StepsToReproduce": [
                "Run the test suite that includes TestKill.testKillTask() multiple times.",
                "Observe the intermittent failure of the test."
            ],
            "ExpectedBehavior": "The job should complete successfully and transition to the SUCCEEDED state.",
            "ObservedBehavior": "The job fails and transitions to the ERROR state, causing the test to fail.",
            "Suggestions": "Investigate the job execution logic to identify potential causes for the job failing. Ensure that all exceptions are handled properly and consider increasing the timeout duration if necessary.",
            "problem_location": {
                "files": [
                    "MRApp.java",
                    "TestKill.java"
                ],
                "classes": [
                    "org.apache.hadoop.mapreduce.v2.app.MRApp",
                    "org.apache.hadoop.mapreduce.v2.app.TestKill"
                ],
                "methods": [
                    "MRApp.waitForState",
                    "TestKill.testKillTask"
                ]
            },
            "possible_fix": "Review the implementation of the MRApp class, particularly the waitForState method, to ensure that it correctly handles job state transitions and exceptions. Additionally, consider adding logging to capture more details about the job's execution when it fails."
        }
    },
    {
        "filename": "MAPREDUCE-3053.json",
        "creation_time": "2011-09-20T22:32:42.000+0000",
        "bug_report": {
            "Title": "YARN Protobuf RPC Failures in RM",
            "Description": "The ApplicationMaster fails to register with YARN's ResourceManager due to a NullPointerException occurring during the registration process. The issue arises when the ApplicationMaster attempts to create a registration request, specifically when the request parameters such as host, RPC port, and tracking URL are not properly set, leading to a failure in the RPC call.",
            "StackTrace": [
                "Exception in thread \"main\" java.lang.reflect.UndeclaredThrowableException",
                "at org.apache.hadoop.yarn.api.impl.pb.client.AMRMProtocolPBClientImpl.registerApplicationMaster(AMRMProtocolPBClientImpl.java:108)",
                "at kafka.yarn.util.ApplicationMasterHelper.registerWithResourceManager(YarnHelper.scala:48)",
                "at kafka.yarn.ApplicationMaster$.main(ApplicationMaster.scala:32)",
                "at kafka.yarn.ApplicationMaster.main(ApplicationMaster.scala)",
                "Caused by: com.google.protobuf.ServiceException: java.lang.NullPointerException: java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.proto.ClientRMProtocol$ClientRMProtocolService$2.getRequestPrototype(ClientRMProtocol.java:186)",
                "at org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Server.call(ProtoOverHadoopRpcEngine.java:323)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1489)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1485)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:396)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1135)",
                "at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1483)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1084)",
                "at org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Invoker.invoke(ProtoOverHadoopRpcEngine.java:130)"
            ],
            "RootCause": "The root cause of the issue is a NullPointerException triggered by the registration request parameters being improperly initialized. Specifically, the host, RPC port, and tracking URL are set to empty or default values, which leads to a failure in the RPC call to the ResourceManager.",
            "StepsToReproduce": [
                "1. Start the ApplicationMaster with valid application ID, attempt ID, and timestamp.",
                "2. Ensure that the parameters for host, RPC port, and tracking URL are not set correctly.",
                "3. Observe the logs for the NullPointerException during the registration process."
            ],
            "ExpectedBehavior": "The ApplicationMaster should successfully register with the ResourceManager without any exceptions.",
            "ObservedBehavior": "The ApplicationMaster fails to register, resulting in a NullPointerException and an UndeclaredThrowableException in the logs.",
            "Suggestions": "Ensure that the host, RPC port, and tracking URL are properly set before making the registration call. Consider adding validation checks for these parameters.",
            "problem_location": {
                "files": [
                    "ApplicationMaster.scala",
                    "ApplicationMasterHelper.scala"
                ],
                "classes": [
                    "kafka.yarn.ApplicationMaster",
                    "kafka.yarn.util.ApplicationMasterHelper"
                ],
                "methods": [
                    "ApplicationMaster.main",
                    "ApplicationMasterHelper.registerWithResourceManager"
                ]
            },
            "possible_fix": "In the registerWithResourceManager method, set the host, RPC port, and tracking URL to valid values instead of empty strings or defaults. For example:\n\nreq.setHost(\"localhost\");\nreq.setRpcPort(8080);\nreq.setTrackingUrl(\"http://localhost:8080/tracking\");"
        }
    },
    {
        "filename": "MAPREDUCE-5884.json",
        "creation_time": "2014-04-16T01:07:22.000+0000",
        "bug_report": {
            "Title": "History server uses short user name when canceling tokens",
            "Description": "When a token owner attempts to cancel their token, an AccessControlException is thrown due to a mismatch between the full principal name of the token owner and the short user name of the canceller. The cancelToken method in AbstractDelegationTokenSecretManager checks for authorization using the short name of the canceller, which can lead to authorization failures if the canceller's name is not in the expected format.",
            "StackTrace": [
                "org.apache.hadoop.security.AccessControlException: <someuser> is not authorized to cancel the token",
                "at org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.cancelToken(AbstractDelegationTokenSecretManager.java:429)",
                "at org.apache.hadoop.mapreduce.v2.hs.HistoryClientService$HSClientProtocolHandler.cancelDelegationToken(HistoryClientService.java:400)",
                "at org.apache.hadoop.mapreduce.v2.api.impl.pb.service.MRClientProtocolPBServiceImpl.cancelDelegationToken(MRClientProtocolPBServiceImpl.java:286)",
                "at org.apache.hadoop.yarn.proto.MRClientProtocol$MRClientProtocolService$2.callBlockingMethod(MRClientProtocol.java:301)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:585)",
                "at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:928)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1962)",
                "at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1958)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)",
                "at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1956)"
            ],
            "RootCause": "The root cause of the issue is the inconsistency in the user name format used for authorization checks in the cancelToken method. The method compares the full principal name of the token owner with the short name of the canceller, leading to authorization failures when they do not match.",
            "StepsToReproduce": [
                "1. Obtain a delegation token as a user.",
                "2. Attempt to cancel the token using a method that retrieves the short user name.",
                "3. Observe the AccessControlException thrown due to the mismatch in user name formats."
            ],
            "ExpectedBehavior": "The token cancellation should succeed if the canceller is the owner of the token, regardless of whether the canceller's name is in short or full principal format.",
            "ObservedBehavior": "An AccessControlException is thrown indicating that the canceller is not authorized to cancel the token due to a mismatch in user name formats.",
            "Suggestions": "Consider implementing a solution where the cancelToken method checks both the short name and the full principal name of the canceller against the owner's name, or ensure that all callers provide a consistent user name format.",
            "problem_location": {
                "files": [
                    "AbstractDelegationTokenSecretManager.java",
                    "HistoryClientService.java",
                    "MRClientProtocolPBServiceImpl.java"
                ],
                "classes": [
                    "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager",
                    "org.apache.hadoop.mapreduce.v2.hs.HistoryClientService",
                    "org.apache.hadoop.mapreduce.v2.api.impl.pb.service.MRClientProtocolPBServiceImpl"
                ],
                "methods": [
                    "AbstractDelegationTokenSecretManager.cancelToken",
                    "HistoryClientService$HSClientProtocolHandler.cancelDelegationToken",
                    "MRClientProtocolPBServiceImpl.cancelDelegationToken"
                ]
            },
            "possible_fix": "In the cancelToken method, modify the authorization check to compare both the short name and the full principal name of the canceller against the owner's name. This can be done by adding an additional condition to the if statement that checks for equality with the full principal name."
        }
    },
    {
        "filename": "MAPREDUCE-3706.json",
        "creation_time": "2012-01-21T04:01:30.000+0000",
        "bug_report": {
            "Title": "HTTP Circular redirect error on the job attempts page",
            "Description": "When attempting to access the job attempts page for a submitted job, a circular redirect error occurs, resulting in an HTTP 500 error. The issue arises when the application attempts to redirect to a URL that leads back to itself, causing an infinite loop. This behavior is observed when directly accessing the job attempts URL without first visiting the proxy page.",
            "StackTrace": [
                "org.apache.commons.httpclient.CircularRedirectException: Circular redirect to 'http://amhost.domain.com:44869/mapreduce/attempts/job_1326992308313_4_4/m/NEW'",
                "at org.apache.commons.httpclient.HttpMethodDirector.processRedirectResponse(HttpMethodDirector.java:638)",
                "at org.apache.commons.httpclient.HttpMethodDirector.executeMethod(HttpMethodDirector.java:179)",
                "at org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:397)",
                "at org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:323)",
                "at org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet.proxyLink(WebAppProxyServlet.java:148)",
                "at org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet.doGet(WebAppProxyServlet.java:269)",
                "at javax.servlet.http.HttpServlet.service(HttpServlet.java:707)",
                "at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)",
                "at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)",
                "at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1221)",
                "at com.google.inject.servlet.FilterChainInvocation.doFilter(FilterChainInvocation.java:66)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:900)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:834)",
                "at com.sun.jersey.spi.container.servlet.ServletContainer.doFilter(ServletContainer.java:795)"
            ],
            "RootCause": "The root cause of the issue is a circular redirect occurring in the proxyLink method of the WebAppProxyServlet class. The application attempts to redirect to a URL that leads back to itself, causing an infinite loop.",
            "StepsToReproduce": [
                "Submit a job in the application.",
                "Directly access the job attempts URL: http://rmhost.domain.com:8088/proxy/application_1326992308313_0004/mapreduce/attempts/job_1326992308313_4_4/m/NEW."
            ],
            "ExpectedBehavior": "The user should be able to access the job attempts page without encountering a circular redirect error.",
            "ObservedBehavior": "An HTTP 500 error occurs due to a circular redirect when accessing the job attempts page directly.",
            "Suggestions": "Implement a check in the proxyLink method to prevent circular redirects. Ensure that the application does not redirect to a URL that matches the original request URL.",
            "problem_location": {
                "files": [
                    "WebAppProxyServlet.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.webproxy.WebAppProxyServlet"
                ],
                "methods": [
                    "WebAppProxyServlet.proxyLink"
                ]
            },
            "possible_fix": "In the proxyLink method, add a check to compare the original request URI with the target redirect URI. If they match, return an error response instead of performing the redirect."
        }
    },
    {
        "filename": "MAPREDUCE-5451.json",
        "creation_time": "2013-05-30T00:08:18.000+0000",
        "bug_report": {
            "Title": "MR uses LD_LIBRARY_PATH which doesn't mean anything in Windows",
            "Description": "The MapReduce (MR) framework is incorrectly configured to use the LD_LIBRARY_PATH environment variable for loading native libraries on Windows systems. This is problematic because LD_LIBRARY_PATH is not recognized in Windows. Instead, the framework should utilize the PATH variable for Windows environments. Additionally, the current implementation incorrectly references the Hadoop common home directory using $HADOOP_COMMON_HOME instead of the Windows-compatible %HADOOP_COMMON_HOME%. As a result, MR jobs fail to execute properly, leading to UnsatisfiedLinkError due to the inability to access native methods.",
            "StackTrace": [
                "2013-05-29 13:51:41,049 FATAL [main] org.apache.hadoop.mapred.YarnChild: Error running child : java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z",
                "at org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)",
                "at org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:393)",
                "at org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:928)",
                "at org.apache.hadoop.util.DiskChecker.checkAccessByFileMethods(DiskChecker.java:177)",
                "at org.apache.hadoop.util.DiskChecker.checkDirAccess(DiskChecker.java:164)",
                "at org.apache.hadoop.util.DiskChecker.checkDir(DiskChecker.java:98)",
                "at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.confChanged(LocalDirAllocator.java:288)",
                "at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathToRead(LocalDirAllocator.java:431)",
                "at org.apache.hadoop.fs.LocalDirAllocator.getLocalPathToRead(LocalDirAllocator.java:164)",
                "at org.apache.hadoop.mapred.YarnChild.configureLocalDirs(YarnChild.java:235)",
                "at org.apache.hadoop.mapred.YarnChild.configureTask(YarnChild.java:294)",
                "at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:143)"
            ],
            "RootCause": "The root cause of the issue is the reliance on the LD_LIBRARY_PATH environment variable for native library loading in a Windows environment, where it is not applicable. The configuration should be adjusted to use the PATH variable instead.",
            "StepsToReproduce": [
                "Set up a Hadoop MapReduce job on a Windows environment.",
                "Ensure that the LD_LIBRARY_PATH is set in the configuration.",
                "Run the MapReduce job and observe the failure due to UnsatisfiedLinkError."
            ],
            "ExpectedBehavior": "The MapReduce job should execute successfully without any errors related to native library loading.",
            "ObservedBehavior": "The MapReduce job fails with an UnsatisfiedLinkError indicating that the native method cannot be accessed due to incorrect library path settings.",
            "Suggestions": "Modify the MapReduce configuration to use the PATH variable for Windows environments instead of LD_LIBRARY_PATH. Additionally, ensure that the Hadoop common home directory is referenced correctly using %HADOOP_COMMON_HOME%.",
            "problem_location": {
                "files": [
                    "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/DiskChecker.java",
                    "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapred/YarnChild.java"
                ],
                "classes": [
                    "org.apache.hadoop.util.DiskChecker",
                    "org.apache.hadoop.mapred.YarnChild"
                ],
                "methods": [
                    "DiskChecker.checkAccessByFileMethods",
                    "DiskChecker.checkDirAccess",
                    "YarnChild.configureLocalDirs",
                    "YarnChild.configureTask"
                ]
            },
            "possible_fix": "Update the configuration logic in YarnChild to check the operating system and set the appropriate environment variable for native library paths. For example:\n\nif (System.getProperty(\"os.name\").toLowerCase().contains(\"win\")) {\n    // Use PATH instead of LD_LIBRARY_PATH\n    // Set the necessary configurations accordingly\n} else {\n    // Existing logic for non-Windows systems\n}"
        }
    },
    {
        "filename": "MAPREDUCE-2463.json",
        "creation_time": "2011-05-02T06:35:07.000+0000",
        "bug_report": {
            "Title": "Job History files are not moving to done folder when job history location is HDFS",
            "Description": "The system fails to move job history files to the 'done' folder when the job history location is set to an HDFS path. This issue arises during the initialization of the Job Tracker or after job completion, leading to an IllegalArgumentException indicating a mismatch in the expected file system type. The error message suggests that the system is trying to treat an HDFS path as a local file system path, which is causing the failure.",
            "StackTrace": [
                "2011-04-29 15:27:27,813 ERROR org.apache.hadoop.mapreduce.jobhistory.JobHistory: Unable to move history file to DONE folder.",
                "java.lang.IllegalArgumentException: Wrong FS: hdfs://10.18.52.146:9000/history/job_201104291518_0001_root, expected: file:///",
                "at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:402)",
                "at org.apache.hadoop.fs.RawLocalFileSystem.pathToFile(RawLocalFileSystem.java:58)",
                "at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:419)",
                "at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:294)",
                "at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:215)",
                "at org.apache.hadoop.fs.FileSystem.copyFromLocalFile(FileSystem.java:1516)",
                "at org.apache.hadoop.fs.FileSystem.copyFromLocalFile(FileSystem.java:1492)",
                "at org.apache.hadoop.fs.FileSystem.moveFromLocalFile(FileSystem.java:1482)",
                "at org.apache.hadoop.mapreduce.jobhistory.JobHistory.moveToDoneNow(JobHistory.java:348)",
                "at org.apache.hadoop.mapreduce.jobhistory.JobHistory.access$200(JobHistory.java:61)",
                "at org.apache.hadoop.mapreduce.jobhistory.JobHistory$1.run(JobHistory.java:439)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)",
                "at java.lang.Thread.run(Thread.java:619)"
            ],
            "RootCause": "The root cause of the issue is that the JobHistory class is attempting to move files from an HDFS location to a local file system path, which is incompatible. The expected file system type is 'file:///' while the actual file system is 'hdfs://'.",
            "StepsToReproduce": [
                "Set the configuration 'mapreduce.jobtracker.jobhistory.location' to an HDFS path.",
                "Run a job that generates job history files.",
                "Check the 'done' folder for the moved job history files."
            ],
            "ExpectedBehavior": "Job history files should be successfully moved to the 'done' folder in the specified HDFS location after job completion.",
            "ObservedBehavior": "Job history files are not moved to the 'done' folder, and an IllegalArgumentException is thrown indicating a wrong file system type.",
            "Suggestions": "Modify the JobHistory class to correctly handle HDFS paths when moving job history files. Ensure that the file system type is checked and handled appropriately based on the configured job history location.",
            "problem_location": {
                "files": [
                    "JobHistory.java"
                ],
                "classes": [
                    "org.apache.hadoop.mapreduce.jobhistory.JobHistory"
                ],
                "methods": [
                    "moveToDoneNow"
                ]
            },
            "possible_fix": "In the 'moveToDoneNow' method of the JobHistory class, add a check to determine if the destination path is an HDFS path and handle the file movement accordingly. For example, use the appropriate FileSystem instance for HDFS instead of assuming a local file system."
        }
    },
    {
        "filename": "MAPREDUCE-3531.json",
        "creation_time": "2011-12-12T05:42:24.000+0000",
        "bug_report": {
            "Title": "java.lang.IllegalArgumentException: Invalid key to HMAC computation in NODE_UPDATE causing RM to stop scheduling",
            "Description": "The ResourceManager (RM) fails to allocate resources for jobs due to an IllegalArgumentException triggered during the HMAC computation in the node update process. This issue arises when the system attempts to create a password for a container token using an invalid secret key, leading to the failure of the scheduling process.",
            "StackTrace": [
                "2011-12-01 11:56:25,202 ERROR org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Error in handling event type NODE_UPDATE to the scheduler",
                "java.lang.IllegalArgumentException: Invalid key to HMAC computation",
                "at org.apache.hadoop.security.token.SecretManager.createPassword(SecretManager.java:141)",
                "at org.apache.hadoop.yarn.server.security.ContainerTokenSecretManager.createPassword(ContainerTokenSecretManager.java:61)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.createContainer(LeafQueue.java:1108)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.getContainer(LeafQueue.java:1091)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainer(LeafQueue.java:1137)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignNodeLocalContainers(LeafQueue.java:1001)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainersOnNode(LeafQueue.java:973)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainers(LeafQueue.java:760)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.assignContainersToChildQueues(ParentQueue.java:583)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.assignContainers(ParentQueue.java:513)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.nodeUpdate(CapacityScheduler.java:569)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:611)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:77)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:294)",
                "at java.lang.Thread.run(Thread.java:619)",
                "Caused by: java.security.InvalidKeyException: Secret key expected",
                "at com.sun.crypto.provider.HmacCore.a(DashoA13*..)",
                "at com.sun.crypto.provider.HmacSHA1.engineInit(DashoA13*..)",
                "at javax.crypto.Mac.init(DashoA13*..)",
                "at org.apache.hadoop.security.token.SecretManager.createPassword(SecretManager.java:139)"
            ],
            "RootCause": "The root cause of the issue is an InvalidKeyException thrown when the SecretManager attempts to initialize the HMAC computation with an invalid secret key. This occurs during the creation of a password for the container token, which is essential for resource allocation.",
            "StepsToReproduce": [
                "Start a cluster with 350 nodes.",
                "Submit a large sleep job.",
                "Observe the ResourceManager logs for errors related to NODE_UPDATE."
            ],
            "ExpectedBehavior": "The ResourceManager should successfully allocate resources to the submitted job without throwing any exceptions.",
            "ObservedBehavior": "The ResourceManager fails to allocate resources, and an IllegalArgumentException is thrown, causing the scheduling process to stop.",
            "Suggestions": "Ensure that the secret key used for HMAC computation is valid and properly initialized before it is used in the SecretManager. Review the configuration settings related to security tokens and secret keys.",
            "problem_location": {
                "files": [
                    "SecretManager.java",
                    "ContainerTokenSecretManager.java",
                    "LeafQueue.java",
                    "CapacityScheduler.java"
                ],
                "classes": [
                    "org.apache.hadoop.security.token.SecretManager",
                    "org.apache.hadoop.yarn.server.security.ContainerTokenSecretManager",
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue",
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler"
                ],
                "methods": [
                    "SecretManager.createPassword",
                    "ContainerTokenSecretManager.createPassword",
                    "LeafQueue.createContainer",
                    "CapacityScheduler.nodeUpdate"
                ]
            },
            "possible_fix": "Verify and update the initialization of the secret key in the configuration. Ensure that the key is correctly set and not null before it is passed to the HMAC computation. For example, check the configuration files for any missing or incorrect entries related to security tokens."
        }
    },
    {
        "filename": "MAPREDUCE-3931.json",
        "creation_time": "2012-02-27T22:30:59.000+0000",
        "bug_report": {
            "Title": "MR tasks failing due to changing timestamps on Resources to download",
            "Description": "During gridmix runs, tasks are failing intermittently due to a mismatch in expected and actual modification timestamps of resources being downloaded. The error indicates that the resource's timestamp has changed on the source filesystem, which leads to an IOException being thrown. This issue can occur if the resource is modified after the job has started but before it is downloaded, causing the system to expect an outdated timestamp.",
            "StackTrace": [
                "java.io.IOException: Resource hdfs://hostname.com:8020/user/hadoop15/.staging/job_1330116323296_0140/job.jar changed on src filesystem (expected 2971811411, was 1330116705875",
                "at org.apache.hadoop.yarn.util.FSDownload.copy(FSDownload.java:90)",
                "at org.apache.hadoop.yarn.util.FSDownload.access$000(FSDownload.java:49)",
                "at org.apache.hadoop.yarn.util.FSDownload$1.run(FSDownload.java:157)",
                "at org.apache.hadoop.yarn.util.FSDownload$1.run(FSDownload.java:155)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:396)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1177)",
                "at org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:153)",
                "at org.apache.hadoop.yarn.util.FSDownload.call(FSDownload.java:49)",
                "at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:138)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)",
                "at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:138)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)",
                "at java.lang.Thread.run(Thread.java:619)"
            ],
            "RootCause": "The root cause of the issue is that the resource file's modification timestamp is being updated on the source filesystem after the job has started, leading to a mismatch between the expected and actual timestamps during the download process.",
            "StepsToReproduce": [
                "1. Start a gridmix job that requires downloading resources from HDFS.",
                "2. Modify the resource file on the source filesystem while the job is running.",
                "3. Observe the task failures due to timestamp mismatch."
            ],
            "ExpectedBehavior": "The job should successfully download the required resources without any timestamp mismatch errors.",
            "ObservedBehavior": "Tasks fail with an IOException indicating that the resource has changed on the source filesystem, causing the expected and actual timestamps to differ.",
            "Suggestions": "To mitigate this issue, ensure that resources are not modified while jobs are running. Alternatively, implement a mechanism to check for changes in resource timestamps before downloading.",
            "problem_location": {
                "files": [
                    "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/util/FSDownload.java",
                    "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/api/protocolrecords/impl/pb/LocalResourceStatusPBImpl.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.util.FSDownload",
                    "org.apache.hadoop.yarn.server.nodemanager.api.protocolrecords.impl.pb.LocalResourceStatusPBImpl"
                ],
                "methods": [
                    "FSDownload.copy",
                    "LocalResourceStatusPBImpl.getException"
                ]
            },
            "possible_fix": "To fix this issue, consider adding a check before the download process in the FSDownload.copy method to ensure that the resource has not been modified since the job started. If it has, either abort the download or retry after a certain interval."
        }
    },
    {
        "filename": "MAPREDUCE-4467.json",
        "creation_time": "2012-07-20T18:20:20.000+0000",
        "bug_report": {
            "Title": "IndexCache failures due to missing synchronization",
            "Description": "The test 'TestMRJobs.testSleepJob' fails intermittently due to a synchronization issue in the 'IndexCache' class. The root of the problem lies in the removal of the 'synchronized' keyword from the method that handles index information retrieval. This leads to an 'IllegalMonitorStateException' when the code attempts to call 'info.wait()' without the necessary synchronization context, causing the application to fail under certain conditions.",
            "StackTrace": [
                "java.lang.IllegalMonitorStateException",
                "at java.lang.Object.wait(Native Method)",
                "at org.apache.hadoop.mapred.IndexCache.getIndexInformation(IndexCache.java:74)",
                "at org.apache.hadoop.mapred.ShuffleHandler$Shuffle.sendMapOutput(ShuffleHandler.java:471)",
                "at org.apache.hadoop.mapred.ShuffleHandler$Shuffle.messageReceived(ShuffleHandler.java:397)",
                "at org.jboss.netty.handler.stream.ChunkedWriteHandler.handleUpstream(ChunkedWriteHandler.java:148)",
                "at org.jboss.netty.handler.codec.http.HttpChunkAggregator.messageReceived(HttpChunkAggregator.java:116)",
                "at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:302)",
                "at org.jboss.netty.handler.codec.replay.ReplayingDecoder.unfoldAndfireMessageReceived(ReplayingDecoder.java:522)",
                "at org.jboss.netty.handler.codec.replay.ReplayingDecoder.callDecode(ReplayingDecoder.java:506)",
                "at org.jboss.netty.handler.codec.replay.ReplayingDecoder.messageReceived(ReplayingDecoder.java:443)",
                "at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:274)",
                "at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:261)",
                "at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:349)",
                "at org.jboss.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:280)",
                "at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:200)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)",
                "at java.lang.Thread.run(Thread.java:662)"
            ],
            "RootCause": "The root cause of the issue is the absence of synchronization around the 'info.wait()' call in the 'getIndexInformation' method of the 'IndexCache' class. The removal of the 'synchronized' keyword has led to an 'IllegalMonitorStateException' when the method attempts to wait on the 'info' object.",
            "StepsToReproduce": [
                "Run the test suite that includes 'TestMRJobs.testSleepJob'.",
                "Observe the intermittent failures related to synchronization issues."
            ],
            "ExpectedBehavior": "The 'getIndexInformation' method should successfully wait for the 'info' object to be ready without throwing an 'IllegalMonitorStateException'.",
            "ObservedBehavior": "The method throws an 'IllegalMonitorStateException' when it attempts to call 'info.wait()' due to missing synchronization.",
            "Suggestions": "Reintroduce the 'synchronized' keyword to the 'getIndexInformation' method to ensure that the 'info.wait()' call is executed within a synchronized context. This will prevent the 'IllegalMonitorStateException' from occurring.",
            "problem_location": {
                "files": [
                    "IndexCache.java"
                ],
                "classes": [
                    "org.apache.hadoop.mapred.IndexCache"
                ],
                "methods": [
                    "IndexCache.getIndexInformation"
                ]
            },
            "possible_fix": "To fix the issue, modify the 'getIndexInformation' method as follows:\n\n```java\npublic synchronized IndexRecord getIndexInformation(String mapId, int reduce,\n                                         Path fileName, String expectedIndexOwner)\n    throws IOException {\n    // existing code...\n}\n```"
        }
    },
    {
        "filename": "MAPREDUCE-3463.json",
        "creation_time": "2011-11-23T13:45:46.000+0000",
        "bug_report": {
            "Title": "Second AM fails to recover properly when first AM is killed with java.lang.IllegalArgumentException causing lost job",
            "Description": "When the first Application Master (AM) is forcefully terminated, the second AM fails to recover due to an IllegalArgumentException caused by an invalid NodeId format. This results in the job being marked as lost and not completing successfully. The issue arises when the RecoveryService attempts to send an assigned event to a task, but the NodeId is not in the expected 'host:port' format, leading to a failure in the recovery process.",
            "StackTrace": [
                "org.apache.hadoop.yarn.exceptions.impl.pb.YarnRemoteExceptionPBImpl: Unknown job job_1322040898409_0005",
                "at org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Invoker.invoke(ProtoOverHadoopRpcEngine.java:151)",
                "at $Proxy10.getTaskAttemptCompletionEvents(Unknown Source)",
                "at org.apache.hadoop.mapreduce.v2.api.impl.pb.client.MRClientProtocolPBClientImpl.getTaskAttemptCompletionEvents(MRClientProtocolPBClientImpl.java:172)",
                "at org.apache.hadoop.mapred.ClientServiceDelegate.getTaskCompletionEvents(ClientServiceDelegate.java:320)",
                "at org.apache.hadoop.mapred.YARNRunner.getTaskCompletionEvents(YARNRunner.java:438)",
                "at org.apache.hadoop.mapreduce.Job.getTaskCompletionEvents(Job.java:621)",
                "at org.apache.hadoop.mapreduce.Job.monitorAndPrintJob(Job.java:1231)",
                "at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1179)",
                "at org.apache.hadoop.examples.Sort.run(Sort.java:181)",
                "at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:69)",
                "at org.apache.hadoop.examples.Sort.main(Sort.java:192)",
                "at org.apache.hadoop.mapreduce.v2.app.recover.RecoveryService.sendAssignedEvent(RecoveryService.java:410)",
                "at org.apache.hadoop.mapreduce.v2.app.recover.RecoveryService.handle(RecoveryService.java:314)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:851)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:853)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:116)"
            ],
            "RootCause": "The root cause of the issue is an IllegalArgumentException thrown by the ConverterUtils.toNodeId method when it receives an invalid NodeId string that does not conform to the expected 'host:port' format. This occurs during the recovery process of the second AM after the first AM is killed.",
            "StepsToReproduce": [
                "1. Set yarn.resourcemanager.am.max-retries=5 in yarn-site.xml.",
                "2. Start a YARN cluster with 4 nodes.",
                "3. Run a job (e.g., Randowriter/Sort/Sort-validate) successfully.",
                "4. While the job is running (e.g., at 50% completion), kill the first Application Master (AM) using 'kill -9'.",
                "5. Observe the logs for the second AM and the client side for errors."
            ],
            "ExpectedBehavior": "The second Application Master should successfully recover and continue the job without losing any progress.",
            "ObservedBehavior": "The second Application Master fails to recover, resulting in the job being marked as lost and an IllegalArgumentException being thrown due to an invalid NodeId.",
            "Suggestions": "Ensure that the NodeId passed to the RecoveryService is always in the correct 'host:port' format. Implement additional validation checks before attempting to convert the NodeId string.",
            "problem_location": {
                "files": [
                    "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/main/java/org/apache/hadoop/yarn/util/ConverterUtils.java",
                    "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/recover/RecoveryService.java",
                    "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.util.ConverterUtils",
                    "org.apache.hadoop.mapreduce.v2.app.recover.RecoveryService",
                    "org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl"
                ],
                "methods": [
                    "ConverterUtils.toNodeId",
                    "RecoveryService.sendAssignedEvent",
                    "TaskAttemptImpl.handle"
                ]
            },
            "possible_fix": "Add validation in the RecoveryService to check the format of the NodeId before calling ConverterUtils.toNodeId. For example, modify the sendAssignedEvent method to include a check for the NodeId format and log an error if it is invalid."
        }
    },
    {
        "filename": "MAPREDUCE-4164.json",
        "creation_time": "2012-04-17T21:30:13.000+0000",
        "bug_report": {
            "Title": "Hadoop 22 Exception thrown after task completion causes its reexecution",
            "Description": "The issue arises when a Hadoop task completes and attempts to commit its results. During this process, a communication exception occurs due to a ClosedByInterruptException, which leads to the task being re-executed unnecessarily. This is likely caused by the interruption of the thread responsible for sending the task's completion status, resulting in a failure to communicate with the server.",
            "StackTrace": [
                "2012-02-28 19:18:08,774 INFO org.apache.hadoop.mapred.Task: Communication exception: java.io.IOException: Call to /127.0.0.1:35400 failed on local exception: java.nio.channels.ClosedByInterruptException",
                "at org.apache.hadoop.ipc.Client.wrapException(Client.java:1094)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1062)",
                "at org.apache.hadoop.ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:198)",
                "at $Proxy0.statusUpdate(Unknown Source)",
                "at org.apache.hadoop.mapred.Task$TaskReporter.run(Task.java:650)",
                "at java.lang.Thread.run(Thread.java:662)",
                "Caused by: java.nio.channels.ClosedByInterruptException",
                "at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:184)",
                "at sun.nio.ch.SocketChannelImpl.write(SocketChannelImpl.java:341)",
                "at org.apache.hadoop.net.SocketOutputStream$Writer.performIO(SocketOutputStream$Writer.java:60)",
                "at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:142)",
                "at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:151)",
                "at org.apache.hadoop.net.SocketOutputStream.write(SocketOutputStream.java:112)",
                "at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65)",
                "at java.io.BufferedOutputStream.flush(BufferedOutputStream.java:123)",
                "at java.io.DataOutputStream.flush(DataOutputStream.java:106)",
                "at org.apache.hadoop.ipc.Client$Connection.sendParam(Client.java:769)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1040)"
            ],
            "RootCause": "The root cause of the issue is the interruption of the thread that is responsible for sending the task's completion status to the server, which results in a ClosedByInterruptException being thrown.",
            "StepsToReproduce": [
                "Run a Hadoop task that completes successfully.",
                "Ensure that the task's completion triggers a communication to the server.",
                "Interrupt the thread during the communication process."
            ],
            "ExpectedBehavior": "The task should complete and commit its results without any exceptions, and no unnecessary re-execution should occur.",
            "ObservedBehavior": "The task completes but throws a communication exception, leading to its re-execution.",
            "Suggestions": "Investigate the thread management during task completion and ensure that interruptions do not occur during critical communication phases. Consider implementing a retry mechanism for transient errors.",
            "problem_location": {
                "files": [
                    "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Client.java",
                    "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/net/SocketOutputStream.java",
                    "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/net/SocketIOWithTimeout.java"
                ],
                "classes": [
                    "org.apache.hadoop.ipc.Client",
                    "org.apache.hadoop.net.SocketOutputStream",
                    "org.apache.hadoop.net.SocketIOWithTimeout"
                ],
                "methods": [
                    "Client.call",
                    "SocketOutputStream.write",
                    "SocketIOWithTimeout.doIO"
                ]
            },
            "possible_fix": "To prevent the ClosedByInterruptException, ensure that the thread handling the communication is not interrupted during the critical section of sending the task's completion status. This may involve restructuring the thread management or adding checks to handle interruptions gracefully."
        }
    },
    {
        "filename": "MAPREDUCE-5260.json",
        "creation_time": "2013-05-20T05:43:32.000+0000",
        "bug_report": {
            "Title": "Job failed due to JvmManager running into inconsistent state",
            "Description": "Jobs in the cluster are failing intermittently due to the JvmManager entering an inconsistent state. This issue is primarily caused by a NullPointerException occurring in the JvmManager's getDetails method, which is invoked during the JVM management process. The failure in task initialization is linked to the inability of the TaskTracker to exit cleanly, leading to cascading failures in job execution.",
            "StackTrace": [
                "java.lang.Throwable: Child Error",
                "\tat org.apache.hadoop.mapred.TaskRunner.run(TaskRunner.java:271)",
                "Caused by: java.lang.NullPointerException",
                "\tat org.apache.hadoop.mapred.JvmManager$JvmManagerForType.getDetails(JvmManager.java:402)",
                "\tat org.apache.hadoop.mapred.JvmManager$JvmManagerForType.reapJvm(JvmManager.java:387)",
                "\tat org.apache.hadoop.mapred.JvmManager$JvmManagerForType.access$000(JvmManager.java:192)",
                "\tat org.apache.hadoop.mapred.JvmManager.launchJvm(JvmManager.java:125)",
                "\tat org.apache.hadoop.mapred.TaskRunner.launchJvmAndWait(TaskRunner.java:292)",
                "\tat org.apache.hadoop.mapred.TaskRunner.run(TaskRunner.java:251)"
            ],
            "RootCause": "The root cause of the issue is a NullPointerException in the JvmManager's getDetails method, which indicates that the method is attempting to access an object that has not been properly initialized or is null.",
            "StepsToReproduce": [
                "Deploy the application in a cluster environment.",
                "Submit a job that requires JVM initialization.",
                "Monitor the logs for any NullPointerExceptions related to JvmManager."
            ],
            "ExpectedBehavior": "The JvmManager should initialize and manage JVM instances without throwing exceptions, allowing jobs to run successfully.",
            "ObservedBehavior": "Jobs fail to initialize due to a NullPointerException in the JvmManager, leading to inconsistent states and task failures.",
            "Suggestions": "Implement null checks in the getDetails method of JvmManager to prevent NullPointerExceptions. Additionally, ensure that all necessary objects are properly initialized before being accessed.",
            "problem_location": {
                "files": [
                    "JvmManager.java",
                    "TaskRunner.java"
                ],
                "classes": [
                    "org.apache.hadoop.mapred.JvmManager",
                    "org.apache.hadoop.mapred.TaskRunner"
                ],
                "methods": [
                    "JvmManager$JvmManagerForType.getDetails",
                    "JvmManager$JvmManagerForType.reapJvm",
                    "JvmManager.launchJvm",
                    "TaskRunner.launchJvmAndWait"
                ]
            },
            "possible_fix": "In the JvmManager class, specifically in the getDetails method, add null checks for any objects that could potentially be null. For example:\n\n```java\nif (someObject == null) {\n    // Handle the null case appropriately\n}\n```"
        }
    },
    {
        "filename": "MAPREDUCE-4774.json",
        "creation_time": "2012-11-06T11:28:43.000+0000",
        "bug_report": {
            "Title": "JobImpl does not handle asynchronous task events in FAILED state",
            "Description": "The test org.apache.hadoop.mapred.TestClusterMRNotification.testMR frequently fails in the mapred build due to an incorrect handling of job state transitions when a task fails. Specifically, when a job's task transitions from RUNNING to FAILED, the system attempts to process a JOB_TASK_ATTEMPT_COMPLETED event, which is not allowed in the FAILED state according to the event transition map. This results in an InvalidStateTransitonException being thrown, leading to the job entering an INTERNAL_ERROR state instead of properly reporting a FAILED state.",
            "StackTrace": [
                "2012-11-06 12:22:02,335 ERROR [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: Can't handle this event at current state",
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: JOB_TASK_ATTEMPT_COMPLETED at FAILED",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:309)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$3(StateMachineFactory.java:290)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:454)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:716)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:1)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:917)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:1)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:130)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:79)",
                "at java.lang.Thread.run(Thread.java:662)"
            ],
            "RootCause": "The root cause of the issue is the system's attempt to process a JOB_TASK_ATTEMPT_COMPLETED event when the job is in the FAILED state, which is not allowed according to the event transition map.",
            "StepsToReproduce": [
                "Run the test org.apache.hadoop.mapred.TestClusterMRNotification.testMR.",
                "Ensure that the test is configured to simulate a job failure by throwing a RuntimeException.",
                "Observe the job state transitions and the notifications received."
            ],
            "ExpectedBehavior": "The job should transition to a FAILED state and send the appropriate notifications reflecting this state.",
            "ObservedBehavior": "The job transitions to an INTERNAL_ERROR state and sends notifications indicating an ERROR status instead of FAILED.",
            "Suggestions": "Implement a check in the JobImpl class to prevent the processing of JOB_TASK_ATTEMPT_COMPLETED events when the job is in the FAILED state. This could involve modifying the state transition logic to handle such cases appropriately.",
            "problem_location": {
                "files": [
                    "JobImpl.java"
                ],
                "classes": [
                    "org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl"
                ],
                "methods": [
                    "JobImpl.handle"
                ]
            },
            "possible_fix": "Modify the handle method in JobImpl to include a check for the job's current state before processing the JOB_TASK_ATTEMPT_COMPLETED event. If the job is in the FAILED state, log a warning and skip processing this event."
        }
    },
    {
        "filename": "MAPREDUCE-3005.json",
        "creation_time": "2011-09-14T04:29:08.000+0000",
        "bug_report": {
            "Title": "MR app hangs because of a NPE in ResourceManager",
            "Description": "The application experiences intermittent hangs due to a NullPointerException (NPE) occurring in the ResourceManager during the handling of NODE_UPDATE events. This issue has been observed multiple times during sort runs on a large cluster, indicating a potential race condition or improper state management within the scheduling logic.",
            "StackTrace": [
                "2011-09-12 15:02:33,715 ERROR org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Error in handling event type NODE_UPDATE to the scheduler",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.allocateNodeLocal(AppSchedulingInfo.java:244)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.allocate(AppSchedulingInfo.java:206)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApp.allocate(SchedulerApp.java:230)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainer(LeafQueue.java:1120)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignNodeLocalContainers(LeafQueue.java:961)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainersOnNode(LeafQueue.java:933)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainers(LeafQueue.java:725)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.assignContainersToChildQueues(ParentQueue.java:577)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.assignContainers(ParentQueue.java:509)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.nodeUpdate(CapacityScheduler.java:579)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:620)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:75)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:266)",
                "at java.lang.Thread.run(Thread.java:619)"
            ],
            "RootCause": "The NPE occurs in the method AppSchedulingInfo.allocateNodeLocal, specifically when attempting to allocate resources for a node. This suggests that the application state or resource requests may not be properly initialized or managed, leading to a null reference when the scheduler attempts to allocate resources.",
            "StepsToReproduce": [
                "Run the MR application on a large cluster.",
                "Perform sort operations multiple times.",
                "Observe the application behavior for hangs or crashes."
            ],
            "ExpectedBehavior": "The ResourceManager should handle NODE_UPDATE events without throwing exceptions, allowing the application to allocate resources smoothly and without interruption.",
            "ObservedBehavior": "The application hangs intermittently due to a NullPointerException in the ResourceManager, causing resource allocation to fail.",
            "Suggestions": "Review the initialization and state management of resource requests in the AppSchedulingInfo class. Ensure that all necessary objects are properly instantiated before they are accessed. Consider adding null checks or default values to prevent NPEs.",
            "problem_location": {
                "files": [
                    "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/AppSchedulingInfo.java",
                    "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/SchedulerApp.java",
                    "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
                    "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/ParentQueue.java",
                    "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo",
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApp",
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue",
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue",
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler"
                ],
                "methods": [
                    "AppSchedulingInfo.allocateNodeLocal",
                    "AppSchedulingInfo.allocate",
                    "SchedulerApp.allocate",
                    "LeafQueue.assignContainer",
                    "LeafQueue.assignNodeLocalContainers",
                    "LeafQueue.assignContainersOnNode",
                    "LeafQueue.assignContainers",
                    "ParentQueue.assignContainersToChildQueues",
                    "ParentQueue.assignContainers",
                    "CapacityScheduler.nodeUpdate",
                    "CapacityScheduler.handle"
                ]
            },
            "possible_fix": "Add null checks in the allocateNodeLocal method of AppSchedulingInfo to ensure that all required objects are initialized before use. For example, check if nodeLocalRequest is null before attempting to access its methods."
        }
    },
    {
        "filename": "MAPREDUCE-6895.json",
        "creation_time": "2017-05-25T21:07:48.000+0000",
        "bug_report": {
            "Title": "Job end notification not sent due to YarnRuntimeException",
            "Description": "The job end notification fails to send because the MRAppMaster encounters a YarnRuntimeException during the shutdown process. This exception is triggered by a ClosedChannelException, which occurs when the system attempts to write to a closed output stream. The issue arises during the graceful stop of the JobHistoryEventHandler, which is responsible for writing job history events, including the job end notification.",
            "StackTrace": [
                "2017-05-24 12:14:02,165 WARN [Thread-693] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: Graceful stop failed",
                "org.apache.hadoop.yarn.exceptions.YarnRuntimeException: java.nio.channels.ClosedChannelException",
                "at org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.handleEvent(JobHistoryEventHandler.java:531)",
                "at org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.serviceStop(JobHistoryEventHandler.java:360)",
                "at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:221)",
                "at org.apache.hadoop.service.ServiceOperations.stop(ServiceOperations.java:52)",
                "at org.apache.hadoop.service.ServiceOperations.stopQuietly(ServiceOperations.java:80)",
                "at org.apache.hadoop.service.CompositeService.stop(CompositeService.java:157)",
                "at org.apache.hadoop.service.CompositeService.serviceStop(CompositeService.java:131)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.serviceStop(MRAppMaster.java:1476)",
                "at org.apache.hadoop.service.AbstractService.stop(AbstractService.java:221)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.stop(MRAppMaster.java:1090)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.shutDownJob(MRAppMaster.java:554)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobFinishEventHandler$1.run(MRAppMaster.java:605)",
                "Caused by: java.nio.channels.ClosedChannelException",
                "at org.apache.hadoop.hdfs.DFSOutputStream.checkClosed(DFSOutputStream.java:1528)",
                "at org.apache.hadoop.fs.FSOutputSummer.write(FSOutputSummer.java:98)",
                "at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:58)",
                "at java.io.DataOutputStream.write(DataOutputStream.java:107)",
                "at org.codehaus.jackson.impl.Utf8Generator._flushBuffer(Utf8Generator.java:1754)",
                "at org.codehaus.jackson.impl.Utf8Generator.flush(Utf8Generator.java:1088)",
                "at org.apache.avro.io.JsonEncoder.flush(JsonEncoder.java:67)",
                "at org.apache.hadoop.mapreduce.jobhistory.EventWriter.write(EventWriter.java:67)",
                "at org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler$MetaInfo.writeEvent(JobHistoryEventHandler.java:886)",
                "at org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.handleEvent(JobHistoryEventHandler.java:520)"
            ],
            "RootCause": "The root cause of the issue is a ClosedChannelException that occurs when the JobHistoryEventHandler attempts to write to a closed output stream during the shutdown process of the MRAppMaster.",
            "StepsToReproduce": [
                "Submit a job to the MRAppMaster.",
                "Wait for the job to complete.",
                "Observe the shutdown process of the MRAppMaster."
            ],
            "ExpectedBehavior": "The job end notification should be sent successfully after the job completes.",
            "ObservedBehavior": "The job end notification is not sent due to a YarnRuntimeException caused by a ClosedChannelException.",
            "Suggestions": "Ensure that the output stream is open before attempting to write to it during the shutdown process. Implement error handling to manage cases where the output stream may be closed unexpectedly.",
            "problem_location": {
                "files": [
                    "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/jobhistory/JobHistoryEventHandler.java"
                ],
                "classes": [
                    "org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler"
                ],
                "methods": [
                    "JobHistoryEventHandler.handleEvent"
                ]
            },
            "possible_fix": "In the JobHistoryEventHandler, ensure that the output stream is checked for closure before writing events. Modify the serviceStop method to handle potential exceptions more gracefully, allowing for retries or alternative handling of the job end notification."
        }
    },
    {
        "filename": "MAPREDUCE-4451.json",
        "creation_time": "2012-07-17T10:34:13.000+0000",
        "bug_report": {
            "Title": "FairScheduler fails to initialize job with Kerberos authentication configured",
            "Description": "The FairScheduler in Hadoop 1.0.3 fails to initialize jobs when Kerberos authentication is configured. The error occurs due to the absence of valid Kerberos credentials (TGT) when attempting to write security tokens to HDFS during job initialization. This is caused by the use of a UserGroupInformation (UGI) instance that does not have a valid TGT, leading to a SaslException during the RPC call.",
            "StackTrace": [
                "2012-07-17 15:15:09,220 ERROR org.apache.hadoop.mapred.JobTracker: Job initialization failed:",
                "java.io.IOException: Call to /192.168.7.80:8020 failed on local exception: java.io.IOException: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]",
                "at org.apache.hadoop.ipc.Client.wrapException(Client.java:1129)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1097)",
                "at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:229)",
                "at $Proxy7.getProtocolVersion(Unknown Source)",
                "at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:411)",
                "at org.apache.hadoop.hdfs.DFSClient.createRPCNamenode(DFSClient.java:125)",
                "at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:329)",
                "at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:294)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:100)",
                "at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:1411)",
                "at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:66)",
                "at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:1429)",
                "at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:254)",
                "at org.apache.hadoop.security.Credentials.writeTokenStorageFile(Credentials.java:169)",
                "at org.apache.hadoop.mapred.JobInProgress.generateAndStoreTokens(JobInProgress.java:3558)",
                "at org.apache.hadoop.mapred.JobInProgress.initTasks(JobInProgress.java:696)",
                "at org.apache.hadoop.mapred.JobTracker.initJob(JobTracker.java:3911)",
                "at org.apache.hadoop.mapred.FairScheduler$JobInitializer$InitJob.run(FairScheduler.java:301)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)",
                "at java.lang.Thread.run(Thread.java:662)"
            ],
            "RootCause": "The root cause of the issue is that the UserGroupInformation (UGI) instance used for the RPC call does not have a valid Kerberos ticket-granting ticket (TGT), which is necessary for authentication when using Kerberos.",
            "StepsToReproduce": [
                "1. Configure Kerberos authentication in Hadoop 1.0.3.",
                "2. Submit a job using FairScheduler.",
                "3. Observe the job initialization failure with the specified error."
            ],
            "ExpectedBehavior": "The job should initialize successfully with Kerberos authentication, allowing the FairScheduler to write security tokens to HDFS.",
            "ObservedBehavior": "The job fails to initialize, resulting in an IOException due to missing Kerberos credentials.",
            "Suggestions": "Ensure that the UserGroupInformation instance used for the RPC call has a valid TGT. This may involve modifying the code to use the UGI associated with the JobTracker instead of a remote user.",
            "problem_location": {
                "files": [
                    "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/mapred/JobInProgress.java",
                    "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/mapred/JobTracker.java",
                    "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/UserGroupInformation.java"
                ],
                "classes": [
                    "org.apache.hadoop.mapred.JobInProgress",
                    "org.apache.hadoop.mapred.JobTracker",
                    "org.apache.hadoop.security.UserGroupInformation"
                ],
                "methods": [
                    "JobInProgress.generateAndStoreTokens",
                    "JobTracker.initJob",
                    "UserGroupInformation.createRemoteUser"
                ]
            },
            "possible_fix": "Modify the JobInProgress.generateAndStoreTokens method to use the UGI associated with the JobTracker instead of creating a new remote user UGI. This ensures that the correct credentials are used for the RPC call."
        }
    },
    {
        "filename": "MAPREDUCE-4144.json",
        "creation_time": "2012-04-12T16:28:30.000+0000",
        "bug_report": {
            "Title": "ResourceManager NPE while handling NODE_UPDATE",
            "Description": "The ResourceManager is encountering a NullPointerException (NPE) when processing NODE_UPDATE events. This issue arises specifically in the AppSchedulingInfo.allocateNodeLocal method, where it attempts to allocate resources for a node but fails due to a null reference. The NPE occurs when the application scheduling information does not have valid data for the requested node, leading to a crash of the ResourceManager.",
            "StackTrace": [
                "2012-04-12 02:09:01,672 FATAL org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Error in handling event type NODE_UPDATE to the scheduler",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.allocateNodeLocal(AppSchedulingInfo.java:261)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.allocate(AppSchedulingInfo.java:223)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApp.allocate(SchedulerApp.java:246)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainer(LeafQueue.java:1229)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignNodeLocalContainers(LeafQueue.java:1078)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainersOnNode(LeafQueue.java:1048)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignReservedContainer(LeafQueue.java:859)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainers(LeafQueue.java:756)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.nodeUpdate(CapacityScheduler.java:573)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:622)",
                "at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:78)",
                "at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:302)",
                "at java.lang.Thread.run(Thread.java:619)"
            ],
            "RootCause": "The root cause of the NPE is likely due to the AppSchedulingInfo not being properly initialized or updated with the necessary data for the node being processed during the NODE_UPDATE event. Specifically, the allocateNodeLocal method is trying to access a resource request that does not exist, leading to a null reference.",
            "StepsToReproduce": [
                "1. Start the ResourceManager in a cluster environment.",
                "2. Trigger a NODE_UPDATE event by adding or updating a node in the cluster.",
                "3. Observe the logs for a NullPointerException in the ResourceManager."
            ],
            "ExpectedBehavior": "The ResourceManager should handle NODE_UPDATE events gracefully without throwing exceptions, ensuring that resources are allocated correctly based on the current state of the nodes and applications.",
            "ObservedBehavior": "The ResourceManager crashes with a NullPointerException when processing NODE_UPDATE events, leading to service interruptions.",
            "Suggestions": "Ensure that the AppSchedulingInfo is correctly populated with valid resource requests for all nodes before processing NODE_UPDATE events. Implement additional null checks in the allocateNodeLocal method to prevent NPEs.",
            "problem_location": {
                "files": [
                    "AppSchedulingInfo.java",
                    "LeafQueue.java",
                    "CapacityScheduler.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo",
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue",
                    "org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler"
                ],
                "methods": [
                    "AppSchedulingInfo.allocateNodeLocal",
                    "LeafQueue.assignContainersOnNode",
                    "CapacityScheduler.nodeUpdate"
                ]
            },
            "possible_fix": "In the AppSchedulingInfo.allocateNodeLocal method, add checks to ensure that the node and its associated resource requests are not null before proceeding with allocation. For example:\n\n```java\nif (node == null || nodeLocalRequest == null) {\n    LOG.error(\"Node or resource request is null, cannot allocate.\");\n    return;\n}\n```"
        }
    },
    {
        "filename": "MAPREDUCE-5924.json",
        "creation_time": "2014-06-13T05:02:09.000+0000",
        "bug_report": {
            "Title": "Windows: Sort Job failed due to 'Invalid event: TA_COMMIT_PENDING at COMMIT_PENDING'",
            "Description": "The Sort job processing over 1GB of data encountered an error during the commit phase. The system attempted to transition to a 'COMMIT_PENDING' state but failed due to an invalid event being triggered at that state. This indicates a possible issue with the state management of task attempts in the Hadoop MapReduce framework.",
            "StackTrace": [
                "2014-06-09 09:15:38,746 INFO [Socket Reader #1 for port 63415] SecurityLogger.org.apache.hadoop.ipc.Server: Auth successful for job_1402304714683_0002 (auth:SIMPLE)",
                "2014-06-09 09:15:38,750 INFO [IPC Server handler 13 on 63415] org.apache.hadoop.mapred.TaskAttemptListenerImpl: Commit-pending state update from attempt_1402304714683_0002_r_000015_1000",
                "2014-06-09 09:15:38,751 ERROR [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl: Can't handle this event at current state for attempt_1402304714683_0002_r_000015_1000",
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: TA_COMMIT_PENDING at COMMIT_PENDING",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:305)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:1058)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:145)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:1271)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:1263)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:173)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:106)",
                "at java.lang.Thread.run(Thread.java:722)",
                "2014-06-09 09:15:38,753 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: job_1402304714683_0002 Job Transitioned from RUNNING to ERROR"
            ],
            "RootCause": "The root cause of the issue is an invalid state transition triggered by the 'TA_COMMIT_PENDING' event while the task attempt is already in the 'COMMIT_PENDING' state. This indicates a flaw in the state management logic within the TaskAttemptImpl class.",
            "StepsToReproduce": [
                "1. Submit a Sort job with over 1GB of data.",
                "2. Monitor the job execution and state transitions.",
                "3. Observe the error when the job attempts to transition to 'COMMIT_PENDING'."
            ],
            "ExpectedBehavior": "The job should successfully transition to the 'COMMIT_PENDING' state and complete without errors.",
            "ObservedBehavior": "The job fails with an 'Invalid event: TA_COMMIT_PENDING at COMMIT_PENDING' error, causing the job to transition to an ERROR state.",
            "Suggestions": "Review the state transition logic in the TaskAttemptImpl class to ensure that the 'TA_COMMIT_PENDING' event is only processed when the task is not already in the 'COMMIT_PENDING' state. Implement checks to prevent invalid state transitions.",
            "problem_location": {
                "files": [
                    "TaskAttemptImpl.java"
                ],
                "classes": [
                    "org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl"
                ],
                "methods": [
                    "TaskAttemptImpl.handle"
                ]
            },
            "possible_fix": "In the handle method of TaskAttemptImpl, add a check before processing the TA_COMMIT_PENDING event to ensure that the current state is not already COMMIT_PENDING. If it is, log a warning and skip processing the event."
        }
    },
    {
        "filename": "MAPREDUCE-3649.json",
        "creation_time": "2012-01-09T23:53:06.000+0000",
        "bug_report": {
            "Title": "Job End Notification Fails with UnknownServiceException",
            "Description": "The job end notification process fails when attempting to notify the specified URL, resulting in a java.net.UnknownServiceException due to the absence of a content-type in the response. This issue occurs in the JobEndNotifier class when the notifyURLOnce method is called, which attempts to open a connection to the notification URL and read the response.",
            "StackTrace": [
                "2012-01-09 23:45:41,732 WARN [AsyncDispatcher event handler] org.mortbay.log: Job end notification to http://HOST:11000/oozie/v0/callback?id=0000000-120109234442311-oozie-oozi-W@mr-node&status=SUCCEEDED& failed",
                "java.net.UnknownServiceException: no content-type",
                "at java.net.URLConnection.getContentHandler(URLConnection.java:1192)",
                "at java.net.URLConnection.getContent(URLConnection.java:689)",
                "at org.apache.hadoop.mapreduce.v2.app.JobEndNotifier.notifyURLOnce(JobEndNotifier.java:95)",
                "at org.apache.hadoop.mapreduce.v2.app.JobEndNotifier.notify(JobEndNotifier.java:139)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobFinishEventHandler.handle(MRAppMaster.java:388)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobFinishEventHandler.handle(MRAppMaster.java:375)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:125)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:82)"
            ],
            "RootCause": "The root cause of the issue is that the URL being notified does not return a valid content-type in its response, which leads to the UnknownServiceException when the JobEndNotifier attempts to process the response.",
            "StepsToReproduce": [
                "1. Submit a job to the Oozie workflow.",
                "2. Ensure that the job completes successfully.",
                "3. Observe the logs for the job end notification process."
            ],
            "ExpectedBehavior": "The job end notification should successfully notify the specified URL without throwing an exception.",
            "ObservedBehavior": "The job end notification fails with a java.net.UnknownServiceException due to the absence of a content-type in the response from the notification URL.",
            "Suggestions": "Ensure that the notification URL returns a valid content-type in its response. Additionally, consider adding error handling in the notifyURLOnce method to manage cases where the content-type is missing.",
            "problem_location": {
                "files": [
                    "hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/JobEndNotifier.java",
                    "hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/event/AsyncDispatcher.java"
                ],
                "classes": [
                    "org.apache.hadoop.mapreduce.v2.app.JobEndNotifier",
                    "org.apache.hadoop.yarn.event.AsyncDispatcher"
                ],
                "methods": [
                    "JobEndNotifier.notifyURLOnce",
                    "JobEndNotifier.notify",
                    "AsyncDispatcher.dispatch"
                ]
            },
            "possible_fix": "Modify the notifyURLOnce method to check for the content-type of the response before proceeding. If the content-type is missing, log a warning and handle the exception gracefully instead of allowing it to propagate."
        }
    },
    {
        "filename": "MAPREDUCE-7077.json",
        "creation_time": "2018-04-11T18:35:34.000+0000",
        "bug_report": {
            "Title": "Pipe mapreduce job fails with Permission denied for jobTokenPassword",
            "Description": "The MapReduce job fails when attempting to write the job token password to a local file due to insufficient permissions. The error occurs during the execution of the PipesReducer, specifically when the application tries to create a file for storing the job token password. The stack trace indicates a FileNotFoundException caused by a 'Permission denied' error when accessing the specified path.",
            "StackTrace": [
                "java.io.FileNotFoundException: /grid/0/hadoop/yarn/local/usercache/hrt_qa/appcache/application_1517534613368_0041/jobTokenPassword (Permission denied)",
                "at java.io.FileOutputStream.open0(Native Method)",
                "at java.io.FileOutputStream.open(FileOutputStream.java:270)",
                "at java.io.FileOutputStream.<init>(FileOutputStream.java:213)",
                "at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:236)",
                "at org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:219)",
                "at org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:318)",
                "at org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:307)",
                "at org.apache.hadoop.mapred.pipes.Application.writePasswordToLocalFile(Application.java:173)",
                "at org.apache.hadoop.mapred.pipes.Application.<init>(Application.java:109)",
                "at org.apache.hadoop.mapred.pipes.PipesReducer.startApplication(PipesReducer.java:87)"
            ],
            "RootCause": "The root cause of the issue is that the application does not have the necessary permissions to create or write to the specified file path for the job token password. This is likely due to incorrect file system permissions on the directory '/grid/0/hadoop/yarn/local/usercache/hrt_qa/appcache/application_1517534613368_0041/' or the user running the job not having the required access rights.",
            "StepsToReproduce": [
                "Launch the wordcount example with pipe using the provided command.",
                "Monitor the application logs for any permission-related errors."
            ],
            "ExpectedBehavior": "The MapReduce job should successfully create the job token password file and proceed with execution without any permission errors.",
            "ObservedBehavior": "The job fails with a FileNotFoundException indicating 'Permission denied' when attempting to create the job token password file.",
            "Suggestions": "Check and update the permissions of the directory '/grid/0/hadoop/yarn/local/usercache/hrt_qa/appcache/application_1517534613368_0041/' to ensure that the user running the MapReduce job has write access. Additionally, verify that the Hadoop configuration allows for the creation of local files in the specified path.",
            "problem_location": {
                "files": [
                    "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/pipes/Application.java",
                    "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/RawLocalFileSystem.java"
                ],
                "classes": [
                    "org.apache.hadoop.mapred.pipes.Application",
                    "org.apache.hadoop.fs.RawLocalFileSystem"
                ],
                "methods": [
                    "Application.writePasswordToLocalFile",
                    "RawLocalFileSystem.create"
                ]
            },
            "possible_fix": "Ensure that the directory '/grid/0/hadoop/yarn/local/usercache/hrt_qa/appcache/application_1517534613368_0041/' has the correct permissions set. You can use the command 'chmod 755 /grid/0/hadoop/yarn/local/usercache/hrt_qa/appcache/application_1517534613368_0041/' to grant the necessary permissions. Additionally, verify that the user running the job is part of the group that has access to this directory."
        }
    },
    {
        "filename": "MAPREDUCE-4102.json",
        "creation_time": "2012-04-03T19:51:52.000+0000",
        "bug_report": {
            "Title": "Job counters not available in Jobhistory web UI for killed jobs",
            "Description": "When a job is killed before completion, accessing the 'Counters' link in the Job History web UI results in a '500 error'. This issue arises due to a NullPointerException when attempting to retrieve counters for the killed job, as the job context does not contain valid counter data at that point.",
            "StackTrace": [
                "Caused by: com.google.inject.ProvisionException: Guice provision errors:",
                "ERROR org.apache.hadoop.yarn.webapp.Dispatcher: error handling URI: /jobhistory/jobcounters/job_1333482028750_0001",
                "java.lang.reflect.InvocationTargetException",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "at java.lang.reflect.Method.invoke(Method.java:597)",
                "Error injecting constructor, java.lang.NullPointerException",
                "at org.apache.hadoop.mapreduce.v2.app.webapp.CountersBlock.<init>(CountersBlock.java:56)",
                "Caused by: java.lang.NullPointerException at org.apache.hadoop.mapreduce.counters.AbstractCounters.incrAllCounters(AbstractCounters.java:328)",
                "at org.apache.hadoop.mapreduce.v2.app.webapp.CountersBlock.getCounters(CountersBlock.java:188)",
                "at org.apache.hadoop.mapreduce.v2.app.webapp.CountersBlock.<init>(CountersBlock.java:57)"
            ],
            "RootCause": "The root cause of the issue is a NullPointerException occurring in the method 'incrAllCounters' of the AbstractCounters class when trying to access counters for a job that has been killed, leading to an incomplete or null job context.",
            "StepsToReproduce": [
                "Run a simple wordcount or sleep job.",
                "Kill the job before it finishes.",
                "Navigate to the Job History web UI.",
                "Click on the 'Counters' link for the killed job."
            ],
            "ExpectedBehavior": "The Job History web UI should display the counters for the killed job or provide a meaningful message instead of a '500 error'.",
            "ObservedBehavior": "The Job History web UI displays a '500 error' when attempting to access the counters for a killed job.",
            "Suggestions": "Implement a check in the 'getCounters' method to handle cases where the job has been killed and ensure that the counters are not accessed if they are null.",
            "problem_location": {
                "files": [
                    "hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/counters/AbstractCounters.java",
                    "hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/webapp/CountersBlock.java"
                ],
                "classes": [
                    "org.apache.hadoop.mapreduce.counters.AbstractCounters",
                    "org.apache.hadoop.mapreduce.v2.app.webapp.CountersBlock"
                ],
                "methods": [
                    "AbstractCounters.incrAllCounters",
                    "CountersBlock.getCounters"
                ]
            },
            "possible_fix": "In the 'getCounters' method of the 'CountersBlock' class, add a check to ensure that the job context is valid and that counters are available before attempting to access them. For example:\n\nif (job == null || job.getAllCounters() == null) {\n    return;\n}"
        }
    },
    {
        "filename": "MAPREDUCE-6353.json",
        "creation_time": "2015-05-03T14:10:44.000+0000",
        "bug_report": {
            "Title": "Divide by zero error in MR AM when calculating available containers",
            "Description": "The application encounters an ArithmeticException due to a division by zero when calculating available containers in the ResourceCalculatorUtils.computeAvailableContainers method. This occurs specifically when the job is configured with zero CPU vcores, leading to an attempt to divide by zero in the calculation of available containers.",
            "StackTrace": [
                "2015-04-30 06:41:06,954 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM.",
                "java.lang.ArithmeticException: / by zero",
                "at org.apache.hadoop.mapreduce.v2.app.rm.ResourceCalculatorUtils.computeAvailableContainers(ResourceCalculatorUtils.java:38)",
                "at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests.assign(RMContainerAllocator.java:947)",
                "at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests.access$200(RMContainerAllocator.java:840)",
                "at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.heartbeat(RMContainerAllocator.java:247)",
                "at org.apache.hadoop.mapreduce.v2.app.rm.RMCommunicator$1.run(RMCommunicator.java:282)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "RootCause": "The root cause of the issue is the division by zero in the computeAvailableContainers method when both available.getVirtualCores() and required.getVirtualCores() are zero.",
            "StepsToReproduce": [
                "1. Configure a sleep job with zero CPU vcores.",
                "2. Run the job in the MR AM environment.",
                "3. Observe the logs for the ArithmeticException."
            ],
            "ExpectedBehavior": "The system should handle the case of zero CPU vcores gracefully without throwing an ArithmeticException.",
            "ObservedBehavior": "The system throws an ArithmeticException due to a division by zero when calculating available containers.",
            "Suggestions": "Implement a check in the computeAvailableContainers method to handle cases where required.getVirtualCores() is zero, preventing division by zero.",
            "problem_location": {
                "files": [
                    "ResourceCalculatorUtils.java",
                    "RMContainerAllocator.java"
                ],
                "classes": [
                    "org.apache.hadoop.mapreduce.v2.app.rm.ResourceCalculatorUtils",
                    "org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator"
                ],
                "methods": [
                    "ResourceCalculatorUtils.computeAvailableContainers",
                    "RMContainerAllocator.heartbeat"
                ]
            },
            "possible_fix": "Modify the computeAvailableContainers method to include a check for zero virtual cores:\n\npublic static int computeAvailableContainers(Resource available,\n      Resource required, EnumSet<SchedulerResourceTypes> resourceTypes) {\n    if (resourceTypes.contains(SchedulerResourceTypes.CPU)) {\n      if (required.getVirtualCores() == 0) {\n        return 0; // or handle as appropriate\n      }\n      return Math.min(available.getMemory() / required.getMemory(),\n        available.getVirtualCores() / required.getVirtualCores());\n    }\n    return available.getMemory() / required.getMemory();\n  }"
        }
    },
    {
        "filename": "MAPREDUCE-4913.json",
        "creation_time": "2013-01-05T02:41:54.000+0000",
        "bug_report": {
            "Title": "TestMRAppMaster#testMRAppMasterMissingStaging occasionally exits",
            "Description": "The test 'testMRAppMasterMissingStaging' intermittently causes the JVM to exit unexpectedly due to an unhandled event type in the AsyncDispatcher. Specifically, the error occurs when the dispatcher receives an 'AM_STARTED' event without a registered handler for the 'EventType'. This results in a fatal error that leads to the termination of the JVM, causing the build process to fail as it is treated as a critical error rather than a mere test failure.",
            "StackTrace": [
                "2013-01-05 02:14:54,682 FATAL [AsyncDispatcher event handler] event.AsyncDispatcher (AsyncDispatcher.java:dispatch(137)) - Error in dispatcher thread",
                "java.lang.Exception: No handler for registered for class org.apache.hadoop.mapreduce.jobhistory.EventType, cannot deliver EventType: AM_STARTED",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:132)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:77)",
                "at java.lang.Thread.run(Thread.java:662)",
                "2013-01-05 02:14:54,682 INFO  [AsyncDispatcher event handler] event.AsyncDispatcher (AsyncDispatcher.java:dispatch(140)) - Exiting, bbye.."
            ],
            "RootCause": "The root cause of the issue is the absence of a registered handler for the 'AM_STARTED' event type in the AsyncDispatcher, which leads to an unhandled exception and subsequent JVM exit.",
            "StepsToReproduce": [
                "Run the test 'testMRAppMasterMissingStaging' in the MRAppMaster test suite.",
                "Observe the behavior of the AsyncDispatcher when it receives the 'AM_STARTED' event."
            ],
            "ExpectedBehavior": "The AsyncDispatcher should handle all registered event types appropriately without causing the JVM to exit.",
            "ObservedBehavior": "The JVM exits unexpectedly with a fatal error when the AsyncDispatcher receives an unhandled 'AM_STARTED' event.",
            "Suggestions": "Implement a handler for the 'AM_STARTED' event type in the AsyncDispatcher to prevent the JVM from exiting. Additionally, ensure that all event types are properly registered and handled.",
            "problem_location": {
                "files": [
                    "AsyncDispatcher.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.event.AsyncDispatcher"
                ],
                "methods": [
                    "AsyncDispatcher.dispatch"
                ]
            },
            "possible_fix": "Add a handler for the 'AM_STARTED' event type in the AsyncDispatcher class. For example:\n\n```java\npublic void handleAMStartedEvent(AMStartedEvent event) {\n    // Handle the AM_STARTED event appropriately\n}\n\n// Register the handler in the constructor or initialization method\nregisterHandler(AMStartedEvent.class, this::handleAMStartedEvent);\n```"
        }
    },
    {
        "filename": "MAPREDUCE-6492.json",
        "creation_time": "2015-09-28T10:16:44.000+0000",
        "bug_report": {
            "Title": "AsyncDispatcher exit with NPE on TaskAttemptImpl#sendJHStartEventForAssignedFailTask",
            "Description": "The application encounters a NullPointerException (NPE) when attempting to send a Job History start event for a task that is in the UNASSIGNED state. This occurs specifically in the method `sendJHStartEventForAssignedFailTask`, where the code attempts to access the HTTP address of the container associated with the task attempt. If the container is not properly initialized or is null, this results in an NPE when calling `taskAttempt.container.getNodeHttpAddress()`.",
            "StackTrace": [
                "2015-09-28 18:01:48,656 FATAL [AsyncDispatcher event handler] org.apache.hadoop.yarn.event.AsyncDispatcher: Error in dispatcher thread",
                "java.lang.NullPointerException",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.sendJHStartEventForAssignedFailTask(TaskAttemptImpl.java:1494)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.access$2900(TaskAttemptImpl.java:147)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$DeallocateContainerTransition.transition(TaskAttemptImpl.java:1700)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$DeallocateContainerTransition.transition(TaskAttemptImpl.java:1686)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:362)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$3(StateMachineFactory.java:290)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:1190)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:146)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:1415)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:1407)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:183)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:109)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "RootCause": "The root cause of the NPE is that the `container` object associated with the `taskAttempt` is null when `sendJHStartEventForAssignedFailTask` is called, leading to a failure when trying to access `getNodeHttpAddress()`.",
            "StepsToReproduce": [
                "1. Create a task attempt in the UNASSIGNED state.",
                "2. Trigger the `DeallocateContainerTransition` event for the task attempt.",
                "3. Observe the logs for the NPE in the AsyncDispatcher."
            ],
            "ExpectedBehavior": "The system should handle the case where the task attempt's container is null gracefully, without throwing a NullPointerException.",
            "ObservedBehavior": "The system throws a NullPointerException when trying to access the HTTP address of a null container, causing the AsyncDispatcher to exit unexpectedly.",
            "Suggestions": "Implement a null check for the `container` object before attempting to access its properties in the `sendJHStartEventForAssignedFailTask` method.",
            "problem_location": {
                "files": [
                    "TaskAttemptImpl.java"
                ],
                "classes": [
                    "org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl"
                ],
                "methods": [
                    "sendJHStartEventForAssignedFailTask",
                    "handle"
                ]
            },
            "possible_fix": "In the `sendJHStartEventForAssignedFailTask` method, add a null check for `taskAttempt.container` before accessing `getNodeHttpAddress()`. For example:\n\n```java\nif (taskAttempt.container != null) {\n    InetSocketAddress nodeHttpInetAddr = NetUtils.createSocketAddr(taskAttempt.container.getNodeHttpAddress());\n    taskAttempt.trackerName = nodeHttpInetAddr.getHostName();\n    taskAttempt.httpPort = nodeHttpInetAddr.getPort();\n} else {\n    LOG.warn(\"Container is null for task attempt: \" + taskAttempt.attemptId);\n}\n```"
        }
    },
    {
        "filename": "MAPREDUCE-5744.json",
        "creation_time": "2014-02-06T19:05:01.000+0000",
        "bug_report": {
            "Title": "Job hangs because RMContainerAllocator$AssignedRequests.preemptReduce() violates the comparator contract",
            "Description": "The job hangs due to an IllegalArgumentException thrown by the preemptReduce() method in RMContainerAllocator$AssignedRequests. This occurs when the comparator used for sorting does not adhere to its contract, particularly when it returns 0 for equal elements, leading to an infinite loop in the sorting algorithm. The issue arises during the preemption of reduce tasks when there are insufficient resources for map tasks.",
            "StackTrace": [
                "2014-02-06 16:43:45,183 ERROR [RMCommunicator Allocator] org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator: ERROR IN CONTACTING RM.",
                "java.lang.IllegalArgumentException: Comparison method violates its general contract!",
                "at java.util.TimSort.mergeLo(TimSort.java:747)",
                "at java.util.TimSort.mergeAt(TimSort.java:483)",
                "at java.util.TimSort.mergeCollapse(TimSort.java:408)",
                "at java.util.TimSort.sort(TimSort.java:214)",
                "at java.util.TimSort.sort(TimSort.java:173)",
                "at java.util.Arrays.sort(Arrays.java:659)",
                "at java.util.Collections.sort(Collections.java:217)",
                "at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$AssignedRequests.preemptReduce(RMContainerAllocator.java:1106)",
                "at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.preemptReducesIfNeeded(RMContainerAllocator.java:416)",
                "at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.heartbeat(RMContainerAllocator.java:230)",
                "at org.apache.hadoop.mapreduce.v2.app.rm.RMCommunicator$1.run(RMCommunicator.java:252)",
                "at java.lang.Thread.run(Thread.java:744)"
            ],
            "RootCause": "The comparator used in the preemptReduce() method does not satisfy the contract defined by the Comparator interface, specifically when it returns 0 for equal elements, causing the sorting algorithm to fail.",
            "StepsToReproduce": [
                "Run a job with a configuration that leads to resource contention between map and reduce tasks.",
                "Ensure that the number of reduce tasks exceeds the available resources.",
                "Observe the logs for the IllegalArgumentException related to the comparator."
            ],
            "ExpectedBehavior": "The job should successfully preempt reduce tasks to allocate resources for map tasks without throwing an exception.",
            "ObservedBehavior": "The job hangs indefinitely due to an IllegalArgumentException caused by a comparator violation.",
            "Suggestions": "Review and correct the comparator implementation in the preemptReduce() method to ensure it adheres to the Comparator contract. Ensure that it consistently returns a negative, zero, or positive integer when comparing two elements.",
            "problem_location": {
                "files": [
                    "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/RMContainerAllocator.java"
                ],
                "classes": [
                    "org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator",
                    "org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$AssignedRequests"
                ],
                "methods": [
                    "RMContainerAllocator.preemptReducesIfNeeded",
                    "RMContainerAllocator$AssignedRequests.preemptReduce"
                ]
            },
            "possible_fix": "Ensure that the comparator used in the preemptReduce() method is correctly implemented. For example, if the comparator is comparing two requests, it should consistently return a negative value if the first request is less than the second, a positive value if greater, and zero only if they are equal in all relevant aspects."
        }
    },
    {
        "filename": "MAPREDUCE-3583.json",
        "creation_time": "2011-12-20T15:07:55.000+0000",
        "bug_report": {
            "Title": "ProcfsBasedProcessTree#constructProcessInfo() may throw NumberFormatException",
            "Description": "The method constructProcessInfo() in the ProcfsBasedProcessTree class is encountering a NumberFormatException due to an attempt to parse a process ID (ppid) that exceeds the maximum value for a signed long in Java. The input string '18446743988060683582' is a 64-bit unsigned integer, which cannot be represented as a signed long, leading to the exception during parsing.",
            "StackTrace": [
                "java.lang.NumberFormatException: For input string: \"18446743988060683582\"",
                "at java.lang.NumberFormatException.forInputString(NumberFormatException.java:48)",
                "at java.lang.Long.parseLong(Long.java:422)",
                "at java.lang.Long.parseLong(Long.java:468)",
                "at org.apache.hadoop.util.ProcfsBasedProcessTree.constructProcessInfo(ProcfsBasedProcessTree.java:413)",
                "at org.apache.hadoop.util.ProcfsBasedProcessTree.getProcessTree(ProcfsBasedProcessTree.java:148)",
                "at org.apache.hadoop.util.LinuxResourceCalculatorPlugin.getProcResourceValues(LinuxResourceCalculatorPlugin.java:401)",
                "at org.apache.hadoop.mapred.Task.initialize(Task.java:536)",
                "at org.apache.hadoop.mapred.MapTask.run(MapTask.java:353)",
                "at org.apache.hadoop.mapred.Child$4.run(Child.java:255)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:396)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1083)",
                "at org.apache.hadoop.mapred.Child.main(Child.java:249)"
            ],
            "RootCause": "The root cause of the NumberFormatException is the attempt to parse a 64-bit unsigned integer (ppid) as a signed long, which exceeds the maximum value for a signed long in Java.",
            "StepsToReproduce": [
                "Run HBase PreCommit builds on a system where the process ID exceeds the signed long limit.",
                "Observe the logs for NumberFormatException related to ppid parsing."
            ],
            "ExpectedBehavior": "The system should handle process IDs correctly without throwing a NumberFormatException.",
            "ObservedBehavior": "The system throws a NumberFormatException when attempting to parse a process ID that is too large for a signed long.",
            "Suggestions": "Change the data structure used to store process information from a long to a String to avoid parsing issues with large integers.",
            "problem_location": {
                "files": [
                    "ProcfsBasedProcessTree.java",
                    "LinuxResourceCalculatorPlugin.java",
                    "Task.java",
                    "MapTask.java",
                    "UserGroupInformation.java"
                ],
                "classes": [
                    "org.apache.hadoop.util.ProcfsBasedProcessTree",
                    "org.apache.hadoop.util.LinuxResourceCalculatorPlugin",
                    "org.apache.hadoop.mapred.Task",
                    "org.apache.hadoop.mapred.MapTask",
                    "org.apache.hadoop.security.UserGroupInformation"
                ],
                "methods": [
                    "ProcfsBasedProcessTree.constructProcessInfo",
                    "ProcfsBasedProcessTree.getProcessTree",
                    "LinuxResourceCalculatorPlugin.getProcResourceValues",
                    "Task.initialize",
                    "MapTask.run",
                    "UserGroupInformation.doAs"
                ]
            },
            "possible_fix": "Change the method constructProcessInfo() to store the ppid as a String instead of parsing it into a long. This can be done by modifying the updateProcessInfo method to accept a String for ppid, thus avoiding the NumberFormatException for large values."
        }
    },
    {
        "filename": "MAPREDUCE-2238.json",
        "creation_time": "2011-01-03T22:57:22.000+0000",
        "bug_report": {
            "Title": "Undeletable build directories",
            "Description": "The Hudson job for the Hadoop Mapreduce project is failing due to an inability to delete a build directory. This issue arises when a test modifies the permissions of the build directory, preventing subsequent cleanup operations. The failure occurs during the checkout process, where the system attempts to delete the existing build directory but encounters a permission error.",
            "StackTrace": [
                "hudson.util.IOException2: remote file operation failed: /grid/0/hudson/hudson-slave/workspace/Hadoop-Mapreduce-trunk at hudson.remoting.Channel@2545938c:hadoop7",
                "at hudson.FilePath.act(FilePath.java:749)",
                "at hudson.FilePath.act(FilePath.java:735)",
                "at hudson.scm.SubversionSCM.checkout(SubversionSCM.java:589)",
                "at hudson.scm.SubversionSCM.checkout(SubversionSCM.java:537)",
                "at hudson.model.AbstractProject.checkout(AbstractProject.java:1116)",
                "at hudson.model.AbstractBuild$AbstractRunner.checkout(AbstractBuild$AbstractRunner.java:479)",
                "at hudson.model.AbstractBuild$AbstractRunner.run(AbstractBuild$AbstractRunner.java:411)",
                "at hudson.model.Run.run(Run.java:1324)",
                "at hudson.model.FreeStyleBuild.run(FreeStyleBuild.java:46)",
                "at hudson.model.ResourceController.execute(ResourceController.java:88)",
                "at hudson.model.Executor.run(Executor.java:139)",
                "Caused by: java.io.IOException: Unable to delete /grid/0/hudson/hudson-slave/workspace/Hadoop-Mapreduce-trunk/trunk/build/test/logs/userlogs/job_20101230131139886_0001/attempt_20101230131139886_0001_m_000000_0"
            ],
            "RootCause": "The root cause of the issue is that a test case is changing the permissions of the build directory, which prevents the Hudson job from deleting it during the cleanup phase.",
            "StepsToReproduce": [
                "Run the Hudson job for the Hadoop Mapreduce project.",
                "Ensure that a test case modifies the permissions of the build directory.",
                "Observe the failure during the checkout process."
            ],
            "ExpectedBehavior": "The Hudson job should successfully delete the existing build directory and proceed with the checkout without errors.",
            "ObservedBehavior": "The Hudson job fails with an IOException indicating that it cannot delete the specified build directory due to permission issues.",
            "Suggestions": "Review the test cases that modify directory permissions and ensure they restore the original permissions after execution. Additionally, implement checks to handle permission errors gracefully during the cleanup process.",
            "problem_location": {
                "files": [
                    "FilePath.java",
                    "SubversionSCM.java",
                    "AbstractProject.java"
                ],
                "classes": [
                    "hudson.FilePath",
                    "hudson.scm.SubversionSCM",
                    "hudson.model.AbstractProject"
                ],
                "methods": [
                    "FilePath.act",
                    "SubversionSCM.checkout",
                    "AbstractProject.checkout"
                ]
            },
            "possible_fix": "Modify the test cases to ensure they do not leave the build directory in a state that prevents deletion. For example, add cleanup code to reset permissions after tests are run."
        }
    },
    {
        "filename": "MAPREDUCE-6410.json",
        "creation_time": "2015-06-04T06:34:33.000+0000",
        "bug_report": {
            "Title": "Aggregated Logs Deletion fails after refreshing Log Retention Settings in secure cluster",
            "Description": "After executing the command 'bin/mapred hsadmin -refreshLogRetentionSettings' in a secure cluster, subsequent attempts to delete aggregated logs result in a GSSException. This issue arises due to the failure to obtain valid Kerberos credentials, which is necessary for secure communication between the history server and the HDFS. The error indicates that no valid credentials were provided, leading to the inability to perform log deletion operations.",
            "StackTrace": [
                "2015-06-04 14:14:40,070 | ERROR | Timer-3 | Error reading root log dir this deletion attempt is being aborted | AggregatedLogDeletionService.java:127",
                "java.io.IOException: Failed on local exception: java.io.IOException: javax.security.sasl.SaslException: GSS initiate failed [Caused by GSSException: No valid credentials provided (Mechanism level: Failed to find any Kerberos tgt)]",
                "at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:764)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1414)",
                "at org.apache.hadoop.ipc.Client.call(Client.java:1363)",
                "at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)",
                "at com.sun.proxy.$Proxy9.getListing(Unknown Source)",
                "at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getListing(ClientNamenodeProtocolTranslatorPB.java:519)",
                "at sun.reflect.GeneratedMethodAccessor16.invoke(Unknown Source)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)",
                "at java.lang.reflect.Method.invoke(Method.java:606)",
                "at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)",
                "at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)",
                "at com.sun.proxy.$Proxy10.getListing(Unknown Source)",
                "at org.apache.hadoop.hdfs.DFSClient.listPaths(DFSClient.java:1767)",
                "at org.apache.hadoop.hdfs.DFSClient.listPaths(DFSClient.java:1750)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.listStatusInternal(DistributedFileSystem.java:691)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.access$600(DistributedFileSystem.java:102)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$15.doCall(DistributedFileSystem.java:753)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$15.doCall(DistributedFileSystem.java:749)",
                "at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.listStatus(DistributedFileSystem.java:749)",
                "at org.apache.hadoop.yarn.logaggregation.AggregatedLogDeletionService$LogDeletionTask.run(AggregatedLogDeletionService.java:68)",
                "at java.util.TimerThread.mainLoop(Timer.java:555)",
                "at java.util.TimerThread.run(Timer.java:505)"
            ],
            "RootCause": "The root cause of the issue is the failure to obtain valid Kerberos credentials after refreshing the log retention settings. This results in a GSSException when attempting to delete aggregated logs, as the secure connection cannot be established without valid credentials.",
            "StepsToReproduce": [
                "1. Start the history server in a secure cluster.",
                "2. Perform log deletion as expected.",
                "3. Execute the command 'mapred hsadmin -refreshLogRetentionSettings' to refresh the configuration.",
                "4. Attempt to delete logs again and observe the GSSException."
            ],
            "ExpectedBehavior": "Log deletion should succeed without any exceptions after refreshing the log retention settings.",
            "ObservedBehavior": "Log deletion fails with a GSSException indicating that no valid credentials were provided.",
            "Suggestions": "Ensure that valid Kerberos credentials are available before attempting log deletion after refreshing settings. This may involve re-authenticating or renewing the Kerberos ticket-granting ticket (TGT).",
            "problem_location": {
                "files": [
                    "AggregatedLogDeletionService.java",
                    "Client.java",
                    "NetUtils.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.logaggregation.AggregatedLogDeletionService",
                    "org.apache.hadoop.ipc.Client",
                    "org.apache.hadoop.net.NetUtils"
                ],
                "methods": [
                    "AggregatedLogDeletionService$LogDeletionTask.run",
                    "Client.call",
                    "NetUtils.wrapException"
                ]
            },
            "possible_fix": "To resolve the issue, ensure that the Kerberos credentials are valid and available before executing log deletion commands. This may involve implementing a check for valid credentials or automatically renewing the Kerberos ticket before log deletion attempts."
        }
    },
    {
        "filename": "MAPREDUCE-6693.json",
        "creation_time": "2016-05-10T11:30:38.000+0000",
        "bug_report": {
            "Title": "ArrayIndexOutOfBoundsException occurs when the length of the job name is equal to mapreduce.jobhistory.jobname.limit",
            "Description": "An ArrayIndexOutOfBoundsException is thrown when the job name length matches the configured limit (mapreduce.jobhistory.jobname.limit). This occurs during the processing of job history files, specifically when the job name is being trimmed to fit within the specified limit. The method trimURLEncodedString in FileNameIndexUtils does not handle the case where the encoded string length is exactly equal to the limit, leading to an attempt to access an index that is out of bounds.",
            "StackTrace": [
                "java.lang.ArrayIndexOutOfBoundsException: 50",
                "at org.apache.hadoop.mapreduce.v2.jobhistory.FileNameIndexUtils.trimURLEncodedString(FileNameIndexUtils.java:326)",
                "at org.apache.hadoop.mapreduce.v2.jobhistory.FileNameIndexUtils.getDoneFileName(FileNameIndexUtils.java:86)",
                "at org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.processDoneFiles(JobHistoryEventHandler.java:1147)",
                "at org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler.handleEvent(JobHistoryEventHandler.java:635)",
                "at org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler$1.run(JobHistoryEventHandler.java:341)",
                "at java.lang.Thread.run(Thread.java:745)"
            ],
            "RootCause": "The root cause of the issue is that the method trimURLEncodedString does not properly handle the case where the length of the encoded string is equal to the specified limit, leading to an ArrayIndexOutOfBoundsException when trying to access an index that is out of bounds.",
            "StepsToReproduce": [
                "Set the job name to a string that is exactly equal to the value of mapreduce.jobhistory.jobname.limit.",
                "Submit a job with this name.",
                "Observe the job history processing logs for the ArrayIndexOutOfBoundsException."
            ],
            "ExpectedBehavior": "The job history should be processed without throwing an exception, and the job name should be correctly trimmed to fit within the specified limit.",
            "ObservedBehavior": "An ArrayIndexOutOfBoundsException is thrown, causing the job history entry to be missing.",
            "Suggestions": "Modify the trimURLEncodedString method to handle the case where the encoded string length is equal to the limit, ensuring that it does not attempt to access an index that is out of bounds.",
            "problem_location": {
                "files": [
                    "FileNameIndexUtils.java",
                    "JobHistoryEventHandler.java"
                ],
                "classes": [
                    "org.apache.hadoop.mapreduce.v2.jobhistory.FileNameIndexUtils",
                    "org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler"
                ],
                "methods": [
                    "FileNameIndexUtils.trimURLEncodedString",
                    "FileNameIndexUtils.getDoneFileName",
                    "JobHistoryEventHandler.processDoneFiles"
                ]
            },
            "possible_fix": "In the trimURLEncodedString method, add a check to ensure that if the encodedString length is equal to limitLength, it does not attempt to access an index beyond the length of the string. For example:\n\nif (index + increase >= limitLength) {\n    break;\n}"
        }
    },
    {
        "filename": "MAPREDUCE-5912.json",
        "creation_time": "2014-06-04T15:37:03.000+0000",
        "bug_report": {
            "Title": "Task.calculateOutputSize does not handle Windows files after MAPREDUCE-5196",
            "Description": "The method Task.calculateOutputSize is failing to correctly handle file paths on Windows systems after the changes made in MAPREDUCE-5196. The issue arises when local output files are incorrectly routed through HDFS, leading to an IllegalArgumentException due to invalid DFS filenames. The root of the problem lies in the way the output file's filesystem is determined, which should use the local filesystem for local paths instead of attempting to resolve them through HDFS.",
            "StackTrace": [
                "java.lang.IllegalArgumentException: Pathname /c:/Hadoop/Data/Hadoop/local/usercache/HadoopUser/appcache/application_1401693085139_0001/output/attempt_1401693085139_0001_m_000000_0/file.out from c:/Hadoop/Data/Hadoop/local/usercache/HadoopUser/appcache/application_1401693085139_0001/output/attempt_1401693085139_0001_m_000000_0/file.out is not a valid DFS filename.",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:187)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.access$000(DistributedFileSystem.java:101)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$17.doCall(DistributedFileSystem.java:1024)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem$17.doCall(DistributedFileSystem.java:1020)",
                "at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)",
                "at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1020)",
                "at org.apache.hadoop.mapred.Task.calculateOutputSize(Task.java:1124)",
                "at org.apache.hadoop.mapred.Task.sendLastUpdate(Task.java:1102)",
                "at org.apache.hadoop.mapred.Task.done(Task.java:1048)"
            ],
            "RootCause": "The root cause of the issue is that the method calculateOutputSize is using the wrong filesystem to retrieve the file status. It should be using the local filesystem for local paths instead of attempting to resolve them through HDFS.",
            "StepsToReproduce": [
                "Set up a Hadoop environment on a Windows machine.",
                "Create a MapReduce job that outputs files to a local directory.",
                "Run the job and observe the logs for the IllegalArgumentException related to invalid DFS filenames."
            ],
            "ExpectedBehavior": "The method should correctly calculate the output size of local files without attempting to route them through HDFS, thus avoiding any IllegalArgumentException.",
            "ObservedBehavior": "The method throws an IllegalArgumentException indicating that the pathname is not a valid DFS filename, causing the job to fail.",
            "Suggestions": "Modify the calculateOutputSize method to ensure that it uses the local filesystem when dealing with local output files. This can be achieved by checking the filesystem type before attempting to retrieve the file status.",
            "problem_location": {
                "files": [
                    "Task.java"
                ],
                "classes": [
                    "org.apache.hadoop.mapred.Task"
                ],
                "methods": [
                    "Task.calculateOutputSize"
                ]
            },
            "possible_fix": "Change the implementation of calculateOutputSize to use the local filesystem for local paths. For example, replace the line 'FileSystem fs = mapOutput.getFileSystem(conf);' with 'FileSystem fs = FileSystem.getLocal(conf);' when the output path is local."
        }
    },
    {
        "filename": "MAPREDUCE-5952.json",
        "creation_time": "2014-06-30T23:08:52.000+0000",
        "bug_report": {
            "Title": "LocalContainerLauncher#renameMapOutputForReduce incorrectly assumes a single dir for mapOutIndex",
            "Description": "The method renameMapOutputForReduce in LocalContainerLauncher is failing due to an incorrect assumption that there is a single output directory for map outputs. The method attempts to rename a file that does not exist, leading to a FileNotFoundException. This issue arises because the mapOutIndex is not being set correctly based on the subMapOutputFile's output index file, which is dependent on LOCAL_DIRS.",
            "StackTrace": [
                "2014-06-30 14:48:35,574 WARN [uber-SubtaskRunner] org.apache.hadoop.mapred.LocalContainerLauncher: Exception running local (uberized) 'child' : java.io.FileNotFoundException: File /Users/gshegalov/workspace/hadoop-common/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/target/org.apache.hadoop.mapreduce.v2.TestMRJobs/org.apache.hadoop.mapreduce.v2.TestMRJobs-localDir-nm-2_3/usercache/gshegalov/appcache/application_1404164272885_0001/output/file.out.index does not exist",
                "at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:517)",
                "at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:726)",
                "at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:507)",
                "at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:337)",
                "at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:289)",
                "at org.apache.hadoop.fs.RawLocalFileSystem.rename(RawLocalFileSystem.java:334)",
                "at org.apache.hadoop.fs.ChecksumFileSystem.rename(ChecksumFileSystem.java:504)",
                "at org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler.renameMapOutputForReduce(LocalContainerLauncher.java:471)",
                "at org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler.runSubtask(LocalContainerLauncher.java:370)",
                "at org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler.runTask(LocalContainerLauncher.java:292)",
                "at org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler.access$200(LocalContainerLauncher.java:178)",
                "at org.apache.hadoop.mapred.LocalContainerLauncher$EventHandler$1.run(LocalContainerLauncher.java:221)",
                "at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:439)",
                "at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)",
                "at java.util.concurrent.FutureTask.run(FutureTask.java:138)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)",
                "at java.lang.Thread.run(Thread.java:695)"
            ],
            "RootCause": "The root cause of the issue is that the method renameMapOutputForReduce is trying to rename a file that does not exist because mapOutIndex is not set correctly based on the output index file of subMapOutputFile. This is due to the assumption of a single output directory, which is incorrect.",
            "StepsToReproduce": [
                "Set up a Hadoop job that uses LocalContainerLauncher.",
                "Configure the job to use multiple LOCAL_DIRS.",
                "Run the job and observe the logs for the FileNotFoundException."
            ],
            "ExpectedBehavior": "The renameMapOutputForReduce method should correctly handle multiple output directories and set the mapOutIndex appropriately, allowing the renaming of output files without encountering a FileNotFoundException.",
            "ObservedBehavior": "The method fails with a FileNotFoundException because it attempts to access a non-existent file due to incorrect assumptions about the output directory structure.",
            "Suggestions": "Update the renameMapOutputForReduce method to correctly determine the output index file based on the subMapOutputFile and handle multiple output directories appropriately.",
            "problem_location": {
                "files": [
                    "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/mapred/LocalContainerLauncher.java",
                    "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/RawLocalFileSystem.java"
                ],
                "classes": [
                    "org.apache.hadoop.mapred.LocalContainerLauncher",
                    "org.apache.hadoop.fs.RawLocalFileSystem"
                ],
                "methods": [
                    "LocalContainerLauncher.renameMapOutputForReduce",
                    "RawLocalFileSystem.rename"
                ]
            },
            "possible_fix": "In the renameMapOutputForReduce method, ensure that mapOutIndex is set to subMapOutputFile.getOutputIndexFile() based on the correct output directory structure. This may involve checking the LOCAL_DIRS configuration and adjusting the logic to handle multiple directories."
        }
    },
    {
        "filename": "MAPREDUCE-3306.json",
        "creation_time": "2011-10-28T16:59:34.000+0000",
        "bug_report": {
            "Title": "Cannot run apps after MAPREDUCE-2989",
            "Description": "The application fails to run due to a NoSuchElementException occurring in the AsyncDispatcher when processing application events. This issue arises during the transition of application states, specifically when the application is initialized. The error indicates that the dispatcher is attempting to access an element that does not exist in a HashMap, leading to a fatal error and termination of the dispatcher thread.",
            "StackTrace": [
                "2011-10-28 21:40:21,263 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.application.Application: Processing application_1319818154209_0001 of type APPLICATION_INITED",
                "2011-10-28 21:40:21,264 FATAL org.apache.hadoop.yarn.event.AsyncDispatcher: Error in dispatcher thread. Exiting..",
                "java.util.NoSuchElementException",
                "at java.util.HashMap$HashIterator.nextEntry(HashMap.java:796)",
                "at java.util.HashMap$ValueIterator.next(HashMap.java:822)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl$AppInitDoneTransition.transition(ApplicationImpl.java:251)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl$AppInitDoneTransition.transition(ApplicationImpl.java:245)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:357)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:298)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:443)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.handle(ApplicationImpl.java:385)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.handle(ApplicationImpl.java:58)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher.handle(ContainerManagerImpl.java:407)",
                "at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher.handle(ContainerManagerImpl.java:399)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:116)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:75)",
                "at java.lang.Thread.run(Thread.java:662)"
            ],
            "RootCause": "The root cause of the issue is a NoSuchElementException thrown when the AsyncDispatcher attempts to access an element in a HashMap that does not exist. This occurs during the state transition of an application in the ApplicationImpl class.",
            "StepsToReproduce": [
                "1. Attempt to run a job on the NodeManager after the changes introduced in MAPREDUCE-2989.",
                "2. Check the NodeManager logs for errors related to application processing."
            ],
            "ExpectedBehavior": "The application should initialize and run without errors, allowing jobs to be processed successfully.",
            "ObservedBehavior": "The application fails to run, and the NodeManager logs show a fatal error due to a NoSuchElementException in the dispatcher thread.",
            "Suggestions": "Review the state transition logic in the ApplicationImpl class to ensure that all expected elements are present in the HashMap before accessing them. Implement checks to handle cases where elements may not exist.",
            "problem_location": {
                "files": [
                    "hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/application/ApplicationImpl.java",
                    "hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/state/StateMachineFactory.java",
                    "hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/event/AsyncDispatcher.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl",
                    "org.apache.hadoop.yarn.state.StateMachineFactory",
                    "org.apache.hadoop.yarn.event.AsyncDispatcher"
                ],
                "methods": [
                    "ApplicationImpl.handle",
                    "StateMachineFactory.doTransition",
                    "AsyncDispatcher.dispatch"
                ]
            },
            "possible_fix": "In the ApplicationImpl.handle method, add a check to ensure that the event type is valid and that the corresponding state exists in the HashMap before attempting to transition. For example:\n\nif (eventDispatchers.containsKey(type)) {\n    eventDispatchers.get(type).handle(event);\n} else {\n    LOG.warn(\"No dispatcher found for event type: \" + type);\n}"
        }
    },
    {
        "filename": "MAPREDUCE-6554.json",
        "creation_time": "2015-11-21T07:34:31.000+0000",
        "bug_report": {
            "Title": "MRAppMaster service start failing with NPE in MRAppMaster#parsePreviousJobHistory",
            "Description": "The MRAppMaster fails to start due to a NullPointerException (NPE) when attempting to recover previous job history. This occurs during the execution of the parsePreviousJobHistory method, which is called as part of the serviceStart process. The NPE is likely caused by an attempt to read from a null input stream when parsing the job history file.",
            "StackTrace": [
                "2015-11-21 13:52:27,722 INFO [main] org.apache.hadoop.service.AbstractService: Service org.apache.hadoop.mapreduce.v2.app.MRAppMaster failed in state STARTED; cause: java.lang.NullPointerException",
                "java.lang.NullPointerException",
                "at java.io.StringReader.<init>(StringReader.java:50)",
                "at org.apache.avro.Schema$Parser.parse(Schema.java:917)",
                "at org.apache.avro.Schema.parse(Schema.java:966)",
                "at org.apache.hadoop.mapreduce.jobhistory.EventReader.<init>(EventReader.java:75)",
                "at org.apache.hadoop.mapreduce.jobhistory.JobHistoryParser.parse(JobHistoryParser.java:139)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.parsePreviousJobHistory(MRAppMaster.java:1256)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.processRecovery(MRAppMaster.java:1225)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.serviceStart(MRAppMaster.java:1087)"
            ],
            "RootCause": "The root cause of the NPE is likely due to the getPreviousJobHistoryStream method returning a null FSDataInputStream, which is then passed to the JobHistoryParser, leading to a failure when attempting to read from it.",
            "StepsToReproduce": [
                "Create a scenario where the MRAppMaster is preempted.",
                "Launch the MRAppMaster again to recover the previous job history."
            ],
            "ExpectedBehavior": "The MRAppMaster should successfully recover from the previous job history without throwing a NullPointerException.",
            "ObservedBehavior": "The MRAppMaster fails to start and throws a NullPointerException during the recovery process.",
            "Suggestions": "Ensure that the getPreviousJobHistoryStream method correctly retrieves the job history stream and does not return null. Implement null checks before attempting to read from the stream.",
            "problem_location": {
                "files": [
                    "MRAppMaster.java",
                    "JobHistoryParser.java"
                ],
                "classes": [
                    "org.apache.hadoop.mapreduce.v2.app.MRAppMaster",
                    "org.apache.hadoop.mapreduce.jobhistory.JobHistoryParser"
                ],
                "methods": [
                    "MRAppMaster.parsePreviousJobHistory",
                    "JobHistoryParser.parse"
                ]
            },
            "possible_fix": "In the parsePreviousJobHistory method, add a null check for the FSDataInputStream returned by getPreviousJobHistoryStream. If it is null, log an error and handle the situation gracefully instead of proceeding to parse."
        }
    },
    {
        "filename": "MAPREDUCE-4457.json",
        "creation_time": "2012-07-18T21:38:17.000+0000",
        "bug_report": {
            "Title": "Invalid State Transition in Task Attempt Handling",
            "Description": "The system encountered an invalid state transition when processing task attempts, specifically transitioning from a FAILED state due to multiple TA_TOO_MANY_FETCH_FAILURE events. The first event caused the task to fail, while the second event could not be processed because the task was already in a FAILED state, leading to an InvalidStateTransitonException.",
            "StackTrace": [
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: TA_TOO_MANY_FETCH_FAILURE at FAILED",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:301)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:443)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:954)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:133)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:913)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:905)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:126)",
                "at org.apache.hadoop.mapreduce.v2.app.recover.RecoveryService$RecoveryDispatcher.realDispatch(RecoveryService.java:285)",
                "at org.apache.hadoop.mapreduce.v2.app.recover.RecoveryService$RecoveryDispatcher.dispatch(RecoveryService.java:281)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:75)",
                "at java.lang.Thread.run(Thread.java:619)"
            ],
            "RootCause": "The root cause of the issue is the handling of multiple TA_TOO_MANY_FETCH_FAILURE events when the task attempt is already in a FAILED state, which leads to an InvalidStateTransitonException.",
            "StepsToReproduce": [
                "1. Submit a job that is likely to encounter multiple fetch failures.",
                "2. Monitor the task attempts for state transitions.",
                "3. Observe the logs for the InvalidStateTransitonException when the second TA_TOO_MANY_FETCH_FAILURE event is processed."
            ],
            "ExpectedBehavior": "The system should handle multiple TA_TOO_MANY_FETCH_FAILURE events gracefully, without throwing an InvalidStateTransitonException.",
            "ObservedBehavior": "The system throws an InvalidStateTransitonException when a TA_TOO_MANY_FETCH_FAILURE event is received while the task attempt is already in a FAILED state.",
            "Suggestions": "Implement a check in the TaskAttemptImpl.handle method to ignore subsequent TA_TOO_MANY_FETCH_FAILURE events if the task is already in a FAILED state.",
            "problem_location": {
                "files": [
                    "hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/TaskAttemptImpl.java",
                    "hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/state/StateMachineFactory.java"
                ],
                "classes": [
                    "org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl",
                    "org.apache.hadoop.yarn.state.StateMachineFactory"
                ],
                "methods": [
                    "TaskAttemptImpl.handle",
                    "StateMachineFactory.doTransition"
                ]
            },
            "possible_fix": "In the handle method of TaskAttemptImpl, add a condition to check if the current state is FAILED before processing a TA_TOO_MANY_FETCH_FAILURE event. If it is, log a warning and return early without attempting to transition states."
        }
    },
    {
        "filename": "MAPREDUCE-2716.json",
        "creation_time": "2011-07-20T21:13:36.000+0000",
        "bug_report": {
            "Title": "MR279: MRReliabilityTest job fails because of missing job-file.",
            "Description": "The MRReliabilityTest job fails due to a missing jobFile, which is expected to be a valid HDFS path. The jobFile is hardcoded to an empty string in TypeConverter.java, leading to an IllegalArgumentException when attempting to create a Path object from this empty string. This issue prevents the job from being executed successfully.",
            "StackTrace": [
                "java.lang.IllegalArgumentException: Can not create a Path from an empty string",
                "at org.apache.hadoop.fs.Path.checkPathArg(Path.java:88)",
                "at org.apache.hadoop.fs.Path.<init>(Path.java:96)",
                "at org.apache.hadoop.mapred.JobConf.<init>(JobConf.java:445)",
                "at org.apache.hadoop.mapreduce.Cluster.getJobs(Cluster.java:104)",
                "at org.apache.hadoop.mapreduce.Cluster.getAllJobs(Cluster.java:218)",
                "at org.apache.hadoop.mapred.JobClient.getAllJobs(JobClient.java:757)",
                "at org.apache.hadoop.mapred.JobClient.jobsToComplete(JobClient.java:741)",
                "at org.apache.hadoop.mapred.ReliabilityTest.runTest(ReliabilityTest.java:219)",
                "at org.apache.hadoop.mapred.ReliabilityTest.runSleepJobTest(ReliabilityTest.java:133)",
                "at org.apache.hadoop.mapred.ReliabilityTest.run(ReliabilityTest.java:116)",
                "at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:69)",
                "at org.apache.hadoop.mapred.ReliabilityTest.main(ReliabilityTest.java:504)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "at java.lang.reflect.Method.invoke(Method.java:597)",
                "at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:72)",
                "at org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:144)",
                "at org.apache.hadoop.test.MapredTestDriver.run(MapredTestDriver.java:111)",
                "at org.apache.hadoop.test.MapredTestDriver.main(MapredTestDriver.java:118)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)",
                "at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)",
                "at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)",
                "at java.lang.reflect.Method.invoke(Method.java:597)",
                "at org.apache.hadoop.util.RunJar.main(RunJar.java:192)"
            ],
            "RootCause": "The jobFile is hardcoded to an empty string in TypeConverter.java, which leads to an IllegalArgumentException when creating a Path object.",
            "StepsToReproduce": [
                "Run the MRReliabilityTest job without a valid jobFile.",
                "Observe the error message indicating that a Path cannot be created from an empty string."
            ],
            "ExpectedBehavior": "The MRReliabilityTest job should execute successfully with a valid jobFile path.",
            "ObservedBehavior": "The job fails with an IllegalArgumentException due to the jobFile being an empty string.",
            "Suggestions": "Modify the TypeConverter.java to ensure that the jobFile is set to a valid HDFS path instead of an empty string.",
            "problem_location": {
                "files": [
                    "TypeConverter.java",
                    "JobClient.java",
                    "Cluster.java"
                ],
                "classes": [
                    "org.apache.hadoop.mapred.TypeConverter",
                    "org.apache.hadoop.mapred.JobClient",
                    "org.apache.hadoop.mapreduce.Cluster"
                ],
                "methods": [
                    "TypeConverter.getJobFile",
                    "JobClient.getAllJobs",
                    "Cluster.getJobs"
                ]
            },
            "possible_fix": "In TypeConverter.java, update the method that sets the jobFile to retrieve a valid path from the job configuration instead of hardcoding it to an empty string."
        }
    },
    {
        "filename": "MAPREDUCE-6702.json",
        "creation_time": "2016-05-17T22:11:38.000+0000",
        "bug_report": {
            "Title": "TestMiniMRChildTask.testTaskEnv and TestMiniMRChildTask.testTaskOldEnv are failing",
            "Description": "The tests 'testTaskEnv' and 'testTaskOldEnv' in the 'TestMiniMRChildTask' class are failing due to an assertion error indicating that the environment checker job has failed. This suggests that the expected environment setup for the tasks is not being met, leading to the assertion failure when the tests check for the successful execution of the environment checker job.",
            "StackTrace": [
                "java.lang.AssertionError: The environment checker job failed.",
                "at org.junit.Assert.fail(Assert.java:88)",
                "at org.junit.Assert.assertTrue(Assert.java:41)",
                "at org.apache.hadoop.mapred.TestMiniMRChildTask.runTestTaskEnv(TestMiniMRChildTask.java:550)",
                "at org.apache.hadoop.mapred.TestMiniMRChildTask.testTaskEnv(TestMiniMRChildTask.java:472)",
                "java.lang.AssertionError: The environment checker job failed.",
                "at org.junit.Assert.fail(Assert.java:88)",
                "at org.junit.Assert.assertTrue(Assert.java:41)",
                "at org.apache.hadoop.mapred.TestMiniMRChildTask.runTestTaskEnv(TestMiniMRChildTask.java:550)",
                "at org.apache.hadoop.mapred.TestMiniMRChildTask.testTaskOldEnv(TestMiniMRChildTask.java:496)"
            ],
            "RootCause": "The environment checker job is failing, which indicates that the necessary environment variables or configurations required for the tasks are not correctly set up or are missing.",
            "StepsToReproduce": [
                "Run the test suite for 'TestMiniMRChildTask'.",
                "Observe the failures in 'testTaskEnv' and 'testTaskOldEnv'."
            ],
            "ExpectedBehavior": "The environment checker job should complete successfully, allowing the tests to pass without assertion errors.",
            "ObservedBehavior": "The tests fail with an assertion error indicating that the environment checker job has failed.",
            "Suggestions": "Investigate the setup of the environment for the tests. Ensure that all necessary environment variables and configurations are correctly initialized before the tests are run. Consider adding logging to capture the state of the environment before the assertion check.",
            "problem_location": {
                "files": [
                    "TestMiniMRChildTask.java"
                ],
                "classes": [
                    "org.apache.hadoop.mapred.TestMiniMRChildTask"
                ],
                "methods": [
                    "runTestTaskEnv",
                    "testTaskEnv",
                    "testTaskOldEnv"
                ]
            },
            "possible_fix": "Check the initialization of environment variables in the 'TestMiniMRChildTask' class. Ensure that the environment is correctly set up before invoking the environment checker job. If necessary, add default values or mock configurations to simulate the expected environment."
        }
    },
    {
        "filename": "MAPREDUCE-4748.json",
        "creation_time": "2012-10-24T21:53:19.000+0000",
        "bug_report": {
            "Title": "Invalid event: T_ATTEMPT_SUCCEEDED at SUCCEEDED",
            "Description": "The error occurs when a task attempts to transition to the 'SUCCEEDED' state while it is already in that state. This situation arises during the handling of task events, particularly when speculative execution is enabled, leading to multiple attempts of the same task. The state machine does not allow this transition, resulting in an InvalidStateTransitionException.",
            "StackTrace": [
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: T_ATTEMPT_SUCCEEDED at SUCCEEDED",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:301)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:443)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl.handle(TaskImpl.java:604)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl.handle(TaskImpl.java:89)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskEventDispatcher.handle(MRAppMaster.java:914)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskEventDispatcher.handle(MRAppMaster.java:908)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:126)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:75)",
                "at java.lang.Thread.run(Thread.java:619)"
            ],
            "RootCause": "The root cause of the issue is the attempt to transition a task to the 'SUCCEEDED' state when it is already in that state, which is not allowed by the state machine.",
            "StepsToReproduce": [
                "Enable speculative execution in the Hadoop configuration.",
                "Run a large Pig script that triggers multiple attempts of the same task.",
                "Monitor the task events and observe the state transitions."
            ],
            "ExpectedBehavior": "The task should successfully transition to the 'SUCCEEDED' state without throwing an exception, even if speculative execution is enabled.",
            "ObservedBehavior": "An InvalidStateTransitionException is thrown when the task attempts to transition to the 'SUCCEEDED' state while already in that state.",
            "Suggestions": "Review the state machine logic to handle cases where a task may receive a 'T_ATTEMPT_SUCCEEDED' event while already in the 'SUCCEEDED' state. Consider adding checks to prevent this transition or to handle it gracefully.",
            "problem_location": {
                "files": [
                    "TaskImpl.java"
                ],
                "classes": [
                    "org.apache.hadoop.mapreduce.v2.app.job.impl.TaskImpl"
                ],
                "methods": [
                    "TaskImpl.handle"
                ]
            },
            "possible_fix": "Modify the handle method in TaskImpl to check if the current state is 'SUCCEEDED' before attempting to transition to that state. If it is already 'SUCCEEDED', log a warning and skip the transition."
        }
    },
    {
        "filename": "MAPREDUCE-3062.json",
        "creation_time": "2011-09-21T17:31:02.000+0000",
        "bug_report": {
            "Title": "YARN NM/RM fail to start",
            "Description": "The ResourceManager fails to start due to an improperly configured address for the admin service. The error message indicates that the configuration for 'yarn.resourcemanager.admin.address' does not contain a valid host:port pair, which is required for establishing a socket connection. This issue arises during the initialization of the AdminService, where the address is parsed to create a socket address.",
            "StackTrace": [
                "2011-09-21 10:21:41,932 FATAL resourcemanager.ResourceManager (ResourceManager.java:main(502)) - Error starting ResourceManager",
                "java.lang.RuntimeException: Not a host:port pair: yarn.resourcemanager.admin.address",
                "\tat org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:148)",
                "\tat org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:132)",
                "\tat org.apache.hadoop.yarn.server.resourcemanager.AdminService.init(AdminService.java:88)",
                "\tat org.apache.hadoop.yarn.service.CompositeService.init(CompositeService.java:58)",
                "\tat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.init(ResourceManager.java:191)",
                "\tat org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:497)"
            ],
            "RootCause": "The root cause of the issue is that the configuration for 'yarn.resourcemanager.admin.address' is either missing or incorrectly formatted, leading to a failure in creating a valid socket address.",
            "StepsToReproduce": [
                "1. Start the YARN ResourceManager with an invalid or missing 'yarn.resourcemanager.admin.address' configuration.",
                "2. Observe the logs for the fatal error indicating the failure to start the ResourceManager."
            ],
            "ExpectedBehavior": "The ResourceManager should start successfully without any fatal errors, and the admin service should be initialized with a valid host:port configuration.",
            "ObservedBehavior": "The ResourceManager fails to start, logging a fatal error due to an invalid configuration for the admin address.",
            "Suggestions": "Ensure that the 'yarn.resourcemanager.admin.address' configuration is set correctly in the configuration files, following the format 'hostname:port'. If the configuration is missing, add it with a valid value.",
            "problem_location": {
                "files": [
                    "ResourceManager.java",
                    "AdminService.java",
                    "NetUtils.java"
                ],
                "classes": [
                    "org.apache.hadoop.yarn.server.resourcemanager.ResourceManager",
                    "org.apache.hadoop.yarn.server.resourcemanager.AdminService",
                    "org.apache.hadoop.net.NetUtils"
                ],
                "methods": [
                    "ResourceManager.main",
                    "AdminService.init",
                    "NetUtils.createSocketAddr"
                ]
            },
            "possible_fix": "Check the configuration file (e.g., yarn-site.xml) for the property 'yarn.resourcemanager.admin.address' and ensure it is set to a valid host:port pair, such as 'localhost:8080'. If it is not set, add the following line: <property><name>yarn.resourcemanager.admin.address</name><value>localhost:8080</value></property>."
        }
    },
    {
        "filename": "MAPREDUCE-5724.json",
        "creation_time": "2014-01-15T00:58:12.000+0000",
        "bug_report": {
            "Title": "JobHistoryServer does not start if HDFS is not running",
            "Description": "The Job History Server (JHS) fails to start when the Hadoop Distributed File System (HDFS) is not running. This results in a YarnRuntimeException due to an inability to create the required 'done' directory in HDFS. The error message indicates a connection refusal when attempting to access the HDFS service at 'hdfs://localhost:8020'.",
            "StackTrace": [
                "org.apache.hadoop.yarn.exceptions.YarnRuntimeException: Error creating done directory: [hdfs://localhost:8020/tmp/hadoop-yarn/staging/history/done]",
                "at org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager.serviceInit(HistoryFileManager.java:505)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)",
                "at org.apache.hadoop.mapreduce.v2.hs.JobHistory.serviceInit(JobHistory.java:94)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)",
                "at org.apache.hadoop.service.CompositeService.serviceInit(CompositeService.java:108)",
                "at org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer.serviceInit(JobHistoryServer.java:143)",
                "at org.apache.hadoop.service.AbstractService.init(AbstractService.java:163)",
                "at org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer.launchJobHistoryServer(JobHistoryServer.java:207)",
                "at org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer.main(JobHistoryServer.java:217)",
                "Caused by: java.net.ConnectException: Call From dontknow.local/172.20.10.4 to localhost:8020 failed on connection exception: java.net.ConnectException: Connection refused"
            ],
            "RootCause": "The root cause of the issue is that the Job History Server attempts to connect to HDFS at 'localhost:8020' to create necessary directories, but fails because HDFS is not running, resulting in a connection refused error.",
            "StepsToReproduce": [
                "Ensure that HDFS is not running.",
                "Attempt to start the Job History Server."
            ],
            "ExpectedBehavior": "The Job History Server should start successfully, even if HDFS is not running, or provide a clear error message indicating that HDFS must be running.",
            "ObservedBehavior": "The Job History Server fails to start and throws a YarnRuntimeException due to the inability to create the 'done' directory in HDFS.",
            "Suggestions": "Implement a check in the Job History Server startup process to verify if HDFS is running before attempting to create directories. If HDFS is not running, provide a user-friendly error message.",
            "problem_location": {
                "files": [
                    "hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/JobHistoryServer.java",
                    "hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/HistoryFileManager.java"
                ],
                "classes": [
                    "org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer",
                    "org.apache.hadoop.mapreduce.v2.hs.HistoryFileManager"
                ],
                "methods": [
                    "JobHistoryServer.serviceInit",
                    "HistoryFileManager.serviceInit"
                ]
            },
            "possible_fix": "Add a check in the JobHistoryServer.serviceInit method to verify if HDFS is accessible before proceeding with the initialization. If not, log a clear error message and prevent the server from starting."
        }
    },
    {
        "filename": "MAPREDUCE-5358.json",
        "creation_time": "2013-06-28T05:40:45.000+0000",
        "bug_report": {
            "Title": "MRAppMaster throws invalid transitions for JobImpl",
            "Description": "The MRAppMaster is encountering an InvalidStateTransitionException when processing job events that are not valid for the current job state. Specifically, events like JOB_TASK_ATTEMPT_COMPLETED and JOB_MAP_TASK_RESCHEDULED are being received while the job is in the SUCCEEDED state, which is not allowed according to the state machine's transition rules.",
            "StackTrace": [
                "2013-06-26 11:39:50,128 ERROR [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: Can't handle this event at current state",
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: JOB_TASK_ATTEMPT_COMPLETED at SUCCEEDED",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:301)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:443)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:720)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:119)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:962)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:958)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:128)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:77)",
                "at java.lang.Thread.run(Thread.java:662)",
                "2013-06-26 11:39:50,129 ERROR [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl: Can't handle this event at current state",
                "org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: JOB_MAP_TASK_RESCHEDULED at SUCCEEDED",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:301)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:443)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:720)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:119)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:962)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:958)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:128)",
                "at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:77)",
                "at java.lang.Thread.run(Thread.java:662)"
            ],
            "RootCause": "The root cause of the issue is that the JobImpl class is receiving job events that are not valid for the current state of the job (SUCCEEDED). The state machine does not allow transitions from the SUCCEEDED state for the events JOB_TASK_ATTEMPT_COMPLETED and JOB_MAP_TASK_RESCHEDULED.",
            "StepsToReproduce": [
                "Submit a job that completes successfully.",
                "Simulate the sending of JOB_TASK_ATTEMPT_COMPLETED and JOB_MAP_TASK_RESCHEDULED events after the job has reached the SUCCEEDED state."
            ],
            "ExpectedBehavior": "The system should ignore or handle events that are not valid for the current job state without throwing an exception.",
            "ObservedBehavior": "The system throws an InvalidStateTransitionException when it receives invalid events for a job that has already succeeded.",
            "Suggestions": "Implement checks in the JobImpl.handle method to validate the state before processing events. If an event is received that is not valid for the current state, log a warning and ignore the event instead of throwing an exception.",
            "problem_location": {
                "files": [
                    "JobImpl.java"
                ],
                "classes": [
                    "org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl"
                ],
                "methods": [
                    "JobImpl.handle"
                ]
            },
            "possible_fix": "In the handle method of JobImpl, add a check to see if the event type is valid for the current state before calling doTransition. If the event is invalid, log a warning and return early without processing the event."
        }
    },
    {
        "filename": "MAPREDUCE-5837.json",
        "creation_time": "2014-04-15T20:24:58.000+0000",
        "bug_report": {
            "Title": "MRAppMaster fails when checking on uber mode",
            "Description": "The MRAppMaster encounters a failure when determining if a job should run in uber mode due to the unhandled NoClassDefError that arises when a required class is not found. Specifically, the method isChainJob attempts to load the class specified in the configuration, and if that class has dependencies that are not available, it results in a NoClassDefFoundError. This issue is particularly evident when the job requires Scala classes, such as scala.Function1, which are not present in the classpath.",
            "StackTrace": [
                "2014-04-15 11:52:55,877 FATAL [main] org.apache.hadoop.mapreduce.v2.app.MRAppMaster: Error starting MRAppMaster",
                "java.lang.NoClassDefFoundError: scala/Function1",
                "at java.lang.Class.forName0(Native Method)",
                "at java.lang.Class.forName(Class.java:190)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.isChainJob(JobImpl.java:1282)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.makeUberDecision(JobImpl.java:1224)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.access$3700(JobImpl.java:136)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$InitTransition.transition(JobImpl.java:1425)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl$InitTransition.transition(JobImpl.java:1363)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$MultipleInternalArc.doTransition(StateMachineFactory.java:385)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:302)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:46)",
                "at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:448)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:976)",
                "at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.handle(JobImpl.java:135)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$JobEventDispatcher.handle(MRAppMaster.java:1263)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.serviceStart(MRAppMaster.java:1063)",
                "at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$1.run(MRAppMaster.java:1480)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:415)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1606)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.initAndStartAppMaster(MRAppMaster.java:1476)",
                "at org.apache.hadoop.mapreduce.v2.app.MRAppMaster.main(MRAppMaster.java:1409)",
                "Caused by: java.lang.ClassNotFoundException: scala.Function1",
                "at java.net.URLClassLoader$1.run(URLClassLoader.java:366)",
                "at java.net.URLClassLoader$1.run(URLClassLoader.java:355)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at java.net.URLClassLoader.findClass(URLClassLoader.java:354)",
                "at java.lang.ClassLoader.loadClass(ClassLoader.java:425)",
                "at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)",
                "at java.lang.ClassLoader.loadClass(ClassLoader.java:358)"
            ],
            "RootCause": "The root cause of the issue is the unhandled NoClassDefError that occurs when the MRAppMaster attempts to load a class that has dependencies not available in the classpath, specifically when the class is derived from ChainMapper.",
            "StepsToReproduce": [
                "Configure a MapReduce job that requires Scala classes.",
                "Ensure that the required Scala libraries are not included in the classpath.",
                "Submit the job to the MRAppMaster.",
                "Observe the failure in the MRAppMaster logs."
            ],
            "ExpectedBehavior": "The MRAppMaster should gracefully handle the absence of required classes and not fail with a fatal error.",
            "ObservedBehavior": "The MRAppMaster fails to start and logs a NoClassDefFoundError, preventing the job from executing.",
            "Suggestions": "Catch the NoClassDefError in the isChainJob method and handle it appropriately to prevent the MRAppMaster from failing.",
            "problem_location": {
                "files": [
                    "hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/job/impl/JobImpl.java",
                    "hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/MRAppMaster.java"
                ],
                "classes": [
                    "org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl",
                    "org.apache.hadoop.mapreduce.v2.app.MRAppMaster"
                ],
                "methods": [
                    "JobImpl.isChainJob",
                    "JobImpl.makeUberDecision",
                    "MRAppMaster.serviceStart"
                ]
            },
            "possible_fix": "Modify the isChainJob method to catch NoClassDefError in addition to ClassNotFoundException. This will allow the MRAppMaster to continue running even if certain classes are not found. Example code change:\n\ntry {\n    String mapClassName = conf.get(MRJobConfig.MAP_CLASS_ATTR);\n    if (mapClassName != null) {\n        Class<?> mapClass = Class.forName(mapClassName);\n        if (ChainMapper.class.isAssignableFrom(mapClass)\n            isChainJob = true;\n    }\n} catch (ClassNotFoundException | NoClassDefFoundError e) {\n    // don't care; assume it's not derived from ChainMapper\n}"
        }
    },
    {
        "filename": "MAPREDUCE-3058.json",
        "creation_time": "2011-09-21T12:53:39.000+0000",
        "bug_report": {
            "Title": "Reducer task hangs despite syslog indicating shutdown",
            "Description": "While executing GridMixV3, a reducer task became unresponsive and remained in a running state for over 15 hours, despite the syslog indicating that it had shut down. The logs show an IOException related to a bad connection with a datanode, which likely caused the task to hang. The task was still sending updates to the Application Master, indicating that it was still alive, but it was not making progress due to the underlying data transfer issues.",
            "StackTrace": [
                "java.io.EOFException: Premature EOF: no length prefix available",
                "at org.apache.hadoop.hdfs.protocol.HdfsProtoUtil.vintPrefixed(HdfsProtoUtil.java:158)",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.transfer(DFSOutputStream.java:860)",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:838)",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:929)",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:740)",
                "at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:415)",
                "at org.apache.hadoop.mapred.YarnChild.run(YarnChild.java:170)"
            ],
            "RootCause": "The reducer task hangs due to an IOException caused by a bad connection with a datanode, leading to a failure in data streaming and recovery processes.",
            "StepsToReproduce": [
                "Run GridMixV3 with a job that includes a reducer.",
                "Simulate a network issue with one of the datanodes.",
                "Monitor the syslog and task logs for the reducer."
            ],
            "ExpectedBehavior": "The reducer task should terminate within a short period (e.g., 20 seconds) if it encounters a critical error, such as a bad connection.",
            "ObservedBehavior": "The reducer task remains in a running state for over 15 hours, despite syslog indicating it has shut down.",
            "Suggestions": "Investigate the network stability between the reducer and datanodes. Implement a timeout mechanism for tasks that encounter critical errors to ensure they do not hang indefinitely.",
            "problem_location": {
                "files": [
                    "DFSOutputStream.java",
                    "YarnChild.java"
                ],
                "classes": [
                    "org.apache.hadoop.hdfs.DFSOutputStream",
                    "org.apache.hadoop.mapred.YarnChild"
                ],
                "methods": [
                    "DFSOutputStream$DataStreamer.transfer",
                    "YarnChild.run"
                ]
            },
            "possible_fix": "Consider adding a timeout mechanism in the data streaming process to handle cases where a datanode becomes unresponsive. For example, modify the 'transfer' method in DFSOutputStream to check for prolonged inactivity and terminate the task if necessary."
        }
    },
    {
        "filename": "MAPREDUCE-5088.json",
        "creation_time": "2013-03-15T17:55:08.000+0000",
        "bug_report": {
            "Title": "MR Client gets a renewer token exception while Oozie is submitting a job",
            "Description": "After the fix for HADOOP-9299, an exception occurs in Oozie when submitting a job, indicating that a required field 'renewer' is missing in the delegation token request. This issue appears to be related to Kerberos authentication, as the renewer field is essential for obtaining a valid delegation token.",
            "StackTrace": [
                "2013-03-15 13:34:16,555  WARN ActionStartXCommand:542 - USER[hue] GROUP[-] TOKEN[] APP[MapReduce] JOB[0000001-130315123130987-oozie-oozi-W] ACTION[0000001-130315123130987-oozie-oozi-W@Sleep] Error starting action [Sleep]. ErrorType [ERROR], ErrorCode [UninitializedMessageException], Message [UninitializedMessageException: Message missing required fields: renewer]",
                "org.apache.oozie.action.ActionExecutorException: UninitializedMessageException: Message missing required fields: renewer",
                "at org.apache.oozie.action.ActionExecutor.convertException(ActionExecutor.java:401)",
                "at org.apache.oozie.action.hadoop.JavaActionExecutor.submitLauncher(JavaActionExecutor.java:738)",
                "at org.apache.oozie.action.hadoop.JavaActionExecutor.start(JavaActionExecutor.java:889)",
                "at org.apache.oozie.command.wf.ActionStartXCommand.execute(ActionStartXCommand.java:211)",
                "at org.apache.oozie.command.wf.ActionStartXCommand.execute(ActionStartXCommand.java:59)",
                "at org.apache.oozie.command.XCommand.call(XCommand.java:277)",
                "at org.apache.oozie.service.CallableQueueService$CompositeCallable.call(CallableQueueService.java:326)",
                "at org.apache.oozie.service.CallableQueueService$CompositeCallable.call(CallableQueueService.java:255)",
                "at org.apache.oozie.service.CallableQueueService$CallableWrapper.run(CallableQueueService.java:175)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)",
                "at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)",
                "at java.lang.Thread.run(Thread.java:662)",
                "Caused by: com.google.protobuf.UninitializedMessageException: Message missing required fields: renewer",
                "at com.google.protobuf.AbstractMessage$Builder.newUninitializedMessageException(AbstractMessage.java:605)",
                "at org.apache.hadoop.security.proto.SecurityProtos$GetDelegationTokenRequestProto$Builder.build(SecurityProtos.java:973)",
                "at org.apache.hadoop.mapreduce.v2.api.protocolrecords.impl.pb.GetDelegationTokenRequestPBImpl.mergeLocalToProto(GetDelegationTokenRequestPBImpl.java:84)",
                "at org.apache.hadoop.mapreduce.v2.api.protocolrecords.impl.pb.GetDelegationTokenRequestPBImpl.getProto(GetDelegationTokenRequestPBImpl.java:67)",
                "at org.apache.hadoop.mapreduce.v2.api.impl.pb.client.MRClientProtocolPBClientImpl.getDelegationToken(MRClientProtocolPBClientImpl.java:200)",
                "at org.apache.hadoop.mapred.YARNRunner.getDelegationTokenFromHS(YARNRunner.java:194)",
                "at org.apache.hadoop.mapred.YARNRunner.submitJob(YARNRunner.java:273)",
                "at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:392)",
                "at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1218)",
                "at org.apache.hadoop.mapreduce.Job$11.run(Job.java:1215)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:396)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1439)",
                "at org.apache.hadoop.mapreduce.Job.submit(Job.java:1215)",
                "at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:581)",
                "at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:576)",
                "at java.security.AccessController.doPrivileged(Native Method)",
                "at javax.security.auth.Subject.doAs(Subject.java:396)",
                "at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1439)",
                "at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:576)",
                "at org.apache.oozie.action.hadoop.JavaActionExecutor.submitLauncher(JavaActionExecutor.java:723)"
            ],
            "RootCause": "The root cause of the issue is that the 'renewer' field is not being set in the GetDelegationTokenRequest when the job is submitted. This is likely due to the Master principal not being correctly retrieved or set in the configuration.",
            "StepsToReproduce": [
                "1. Ensure that the environment is set up with Kerberos authentication.",
                "2. Attempt to submit a job using Oozie.",
                "3. Observe the logs for the UninitializedMessageException related to the 'renewer' field."
            ],
            "ExpectedBehavior": "The job should be submitted successfully without any exceptions, and a valid delegation token should be obtained.",
            "ObservedBehavior": "An UninitializedMessageException is thrown indicating that the 'renewer' field is missing, preventing the job from being submitted.",
            "Suggestions": "Ensure that the Master principal is correctly configured and that the 'renewer' field is set in the GetDelegationTokenRequest before submission.",
            "problem_location": {
                "files": [
                    "hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/JobClient.java",
                    "hadoop-mapreduce-client-jobclient/src/main/java/org/apache/hadoop/mapred/YARNRunner.java",
                    "hadoop-mapreduce-client-common/src/main/java/org/apache/hadoop/mapreduce/v2/api/protocolrecords/impl/pb/GetDelegationTokenRequestPBImpl.java"
                ],
                "classes": [
                    "org.apache.hadoop.mapreduce.JobSubmitter",
                    "org.apache.hadoop.mapred.YARNRunner",
                    "org.apache.hadoop.mapreduce.v2.api.protocolrecords.impl.pb.GetDelegationTokenRequestPBImpl"
                ],
                "methods": [
                    "JobSubmitter.submitJobInternal",
                    "YARNRunner.getDelegationTokenFromHS",
                    "GetDelegationTokenRequestPBImpl.getProto"
                ]
            },
            "possible_fix": "In the method 'getDelegationTokenFromHS', ensure that the 'renewer' field is set correctly by verifying the Master principal configuration. For example, modify the line: 'request.setRenewer(Master.getMasterPrincipal(conf));' to ensure that 'Master.getMasterPrincipal(conf)' returns a valid principal."
        }
    }
]