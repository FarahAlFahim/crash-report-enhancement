[
    {
        "filename": "YARN-5918.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.OpportunisticContainerAllocatorAMService.getLeastLoadedNodes": {
                "code_before_change": "  private synchronized List<RemoteNode> getLeastLoadedNodes() {\n    long currTime = System.currentTimeMillis();\n    if ((currTime - lastCacheUpdateTime > cacheRefreshInterval)\n        || cachedNodes == null) {\n      cachedNodes = convertToRemoteNodes(\n          this.nodeMonitor.selectLeastLoadedNodes(this.k));\n      lastCacheUpdateTime = currTime;\n    }\n    return cachedNodes;\n  }",
                "code_after_change": "  private synchronized List<RemoteNode> getLeastLoadedNodes() {\n    long currTime = System.currentTimeMillis();\n    if ((currTime - lastCacheUpdateTime > cacheRefreshInterval)\n        || cachedNodes == null) {\n      cachedNodes = convertToRemoteNodes(\n          this.nodeMonitor.selectLeastLoadedNodes(this.k));\n      lastCacheUpdateTime = currTime;\n    }\n    return cachedNodes;\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.distributed.NodeQueueLoadMonitor.initThresholdCalculator": {
                "code_before_change": "  public void initThresholdCalculator(float sigma, int limitMin, int limitMax) {\n    this.thresholdCalculator =\n        new QueueLimitCalculator(this, sigma, limitMin, limitMax);\n  }",
                "code_after_change": "  public void initThresholdCalculator(float sigma, int limitMin, int limitMax) {\n    this.thresholdCalculator =\n        new QueueLimitCalculator(this, sigma, limitMin, limitMax);\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.OpportunisticContainerAllocatorAMService.convertToRemoteNode": {
                "code_before_change": "  private RemoteNode convertToRemoteNode(NodeId nodeId) {\n    return RemoteNode.newInstance(nodeId,\n        ((AbstractYarnScheduler)rmContext.getScheduler()).getNode(nodeId)\n            .getHttpAddress());\n  }",
                "code_after_change": "  private RemoteNode convertToRemoteNode(NodeId nodeId) {\n    SchedulerNode node =\n        ((AbstractYarnScheduler) rmContext.getScheduler()).getNode(nodeId);\n    return node != null ? RemoteNode.newInstance(nodeId, node.getHttpAddress())\n        : null;\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.OpportunisticContainerAllocatorAMService.convertToRemoteNodes": {
                "code_before_change": "  private List<RemoteNode> convertToRemoteNodes(List<NodeId> nodeIds) {\n    ArrayList<RemoteNode> retNodes = new ArrayList<>();\n    for (NodeId nId : nodeIds) {\n      retNodes.add(convertToRemoteNode(nId));\n    }\n    return retNodes;\n  }",
                "code_after_change": "  private List<RemoteNode> convertToRemoteNodes(List<NodeId> nodeIds) {\n    ArrayList<RemoteNode> retNodes = new ArrayList<>();\n    for (NodeId nId : nodeIds) {\n      RemoteNode remoteNode = convertToRemoteNode(nId);\n      if (null != remoteNode) {\n        retNodes.add(remoteNode);\n      }\n    }\n    return retNodes;\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.distributed.NodeQueueLoadMonitor.addNode": {
                "code_before_change": "  public void addNode(List<NMContainerStatus> containerStatuses, RMNode\n      rmNode) {\n    LOG.debug(\"Node added event from: \" + rmNode.getNode().getName());\n    // Ignoring this currently : at least one NODE_UPDATE heartbeat is\n    // required to ensure node eligibility.\n  }",
                "code_after_change": "  public void addNode(List<NMContainerStatus> containerStatuses,\n      RMNode rmNode) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Node added event from: \" + rmNode.getNode().getName());\n    }\n    // Ignoring this currently : at least one NODE_UPDATE heartbeat is\n    // required to ensure node eligibility.\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Correct",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report precisely identifies the root cause of the bug as a `NullPointerException` in the `convertToRemoteNode` method, which is part of the ground truth methods. The fix suggestion is correct as it matches the developer's fix, which involves adding null checks in the `convertToRemoteNode` and `convertToRemoteNodes` methods. The problem location identification is precise because the `problem_location` field mentions the exact methods from the ground truth list. There is no wrong information in the bug report as all the details are consistent with the context of the bug."
        }
    },
    {
        "filename": "YARN-8629.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsHandlerImpl.logLineFromTasksFile": {
                "code_before_change": "  private void logLineFromTasksFile(File cgf) {\n    String str;\n    if (LOG.isDebugEnabled()) {\n      try (BufferedReader inl =\n          new BufferedReader(new InputStreamReader(new FileInputStream(cgf\n              + \"/tasks\"), \"UTF-8\"))) {\n        str = inl.readLine();\n        if (str != null) {\n          LOG.debug(\"First line in cgroup tasks file: \" + cgf + \" \" + str);\n        }\n      } catch (IOException e) {\n        LOG.warn(\"Failed to read cgroup tasks file. \", e);\n      }\n    }\n  }",
                "code_after_change": "  private void logLineFromTasksFile(File cgf) {\n    String str;\n    if (LOG.isDebugEnabled()) {\n      try (BufferedReader inl =\n          new BufferedReader(new InputStreamReader(new FileInputStream(cgf\n              + \"/tasks\"), \"UTF-8\"))) {\n        str = inl.readLine();\n        if (str != null) {\n          LOG.debug(\"First line in cgroup tasks file: \" + cgf + \" \" + str);\n        }\n      } catch (IOException e) {\n        LOG.warn(\"Failed to read cgroup tasks file. \", e);\n      }\n    }\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Preventive",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the root cause as a `FileNotFoundException` due to the cgroup directory not being created, which is related to the ground truth method `logLineFromTasksFile` as it deals with reading the cgroup tasks file. However, it does not precisely identify the method where the fix was made, hence it is classified as 'Partial' with 'Shared Stack Trace Context' since `checkAndDeleteCgroup` is in the same stack trace. The fix suggestion is 'Preventive' as it suggests ensuring the cgroup directory is created, which would prevent the error. The problem location identification is 'Partial' with 'Shared Stack Trace Context' because it mentions methods like `checkAndDeleteCgroup` and `createCGroup` that are in the same stack trace as the ground truth method. There is no wrong information in the bug report as all details are relevant to the context of the bug."
        }
    },
    {
        "filename": "YARN-4431.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.run": {
                "code_before_change": "      public void run() {\n        int lastHeartbeatID = 0;\n        while (!isStopped) {\n          // Send heartbeat\n          try {\n            NodeHeartbeatResponse response = null;\n            Set<NodeLabel> nodeLabelsForHeartbeat =\n                nodeLabelsHandler.getNodeLabelsForHeartbeat();\n            NodeStatus nodeStatus = getNodeStatus(lastHeartbeatID);\n\n            NodeHeartbeatRequest request =\n                NodeHeartbeatRequest.newInstance(nodeStatus,\n                    NodeStatusUpdaterImpl.this.context\n                        .getContainerTokenSecretManager().getCurrentKey(),\n                    NodeStatusUpdaterImpl.this.context\n                        .getNMTokenSecretManager().getCurrentKey(),\n                    nodeLabelsForHeartbeat);\n\n            if (logAggregationEnabled) {\n              // pull log aggregation status for application running in this NM\n              List<LogAggregationReport> logAggregationReports =\n                  getLogAggregationReportsForApps(context\n                    .getLogAggregationStatusForApps());\n              if (logAggregationReports != null\n                  && !logAggregationReports.isEmpty()) {\n                request.setLogAggregationReportsForApps(logAggregationReports);\n              }\n            }\n\n            response = resourceTracker.nodeHeartbeat(request);\n            //get next heartbeat interval from response\n            nextHeartBeatInterval = response.getNextHeartBeatInterval();\n            updateMasterKeys(response);\n\n            if (response.getNodeAction() == NodeAction.SHUTDOWN) {\n              LOG.warn(\"Recieved SHUTDOWN signal from Resourcemanager as part of\"\n                  + \" heartbeat, hence shutting down.\");\n              LOG.warn(\"Message from ResourceManager: \"\n                  + response.getDiagnosticsMessage());\n              context.setDecommissioned(true);\n              dispatcher.getEventHandler().handle(\n                  new NodeManagerEvent(NodeManagerEventType.SHUTDOWN));\n              break;\n            }\n            if (response.getNodeAction() == NodeAction.RESYNC) {\n              LOG.warn(\"Node is out of sync with ResourceManager,\"\n                  + \" hence resyncing.\");\n              LOG.warn(\"Message from ResourceManager: \"\n                  + response.getDiagnosticsMessage());\n              // Invalidate the RMIdentifier while resync\n              NodeStatusUpdaterImpl.this.rmIdentifier =\n                  ResourceManagerConstants.RM_INVALID_IDENTIFIER;\n              dispatcher.getEventHandler().handle(\n                  new NodeManagerEvent(NodeManagerEventType.RESYNC));\n              pendingCompletedContainers.clear();\n              break;\n            }\n\n            nodeLabelsHandler.verifyRMHeartbeatResponseForNodeLabels(response);\n\n            // Explicitly put this method after checking the resync response. We\n            // don't want to remove the completed containers before resync\n            // because these completed containers will be reported back to RM\n            // when NM re-registers with RM.\n            // Only remove the cleanedup containers that are acked\n            removeOrTrackCompletedContainersFromContext(response\n                  .getContainersToBeRemovedFromNM());\n\n            logAggregationReportForAppsTempList.clear();\n            lastHeartbeatID = response.getResponseId();\n            List<ContainerId> containersToCleanup = response\n                .getContainersToCleanup();\n            if (!containersToCleanup.isEmpty()) {\n              dispatcher.getEventHandler().handle(\n                  new CMgrCompletedContainersEvent(containersToCleanup,\n                    CMgrCompletedContainersEvent.Reason.BY_RESOURCEMANAGER));\n            }\n            List<ApplicationId> appsToCleanup =\n                response.getApplicationsToCleanup();\n            //Only start tracking for keepAlive on FINISH_APP\n            trackAppsForKeepAlive(appsToCleanup);\n            if (!appsToCleanup.isEmpty()) {\n              dispatcher.getEventHandler().handle(\n                  new CMgrCompletedAppsEvent(appsToCleanup,\n                      CMgrCompletedAppsEvent.Reason.BY_RESOURCEMANAGER));\n            }\n\n            Map<ApplicationId, ByteBuffer> systemCredentials =\n                response.getSystemCredentialsForApps();\n            if (systemCredentials != null && !systemCredentials.isEmpty()) {\n              ((NMContext) context)\n                .setSystemCrendentialsForApps(parseCredentials(systemCredentials));\n            }\n\n            List<org.apache.hadoop.yarn.api.records.Container>\n                containersToDecrease = response.getContainersToDecrease();\n            if (!containersToDecrease.isEmpty()) {\n              dispatcher.getEventHandler().handle(\n                  new CMgrDecreaseContainersResourceEvent(containersToDecrease)\n              );\n            }\n\n            // SignalContainer request originally comes from end users via\n            // ClientRMProtocol's SignalContainer. Forward the request to\n            // ContainerManager which will dispatch the event to ContainerLauncher.\n            List<SignalContainerRequest> containersToSignal = response\n                .getContainersToSignalList();\n            if (containersToSignal.size() != 0) {\n              dispatcher.getEventHandler().handle(\n                  new CMgrSignalContainersEvent(containersToSignal));\n            }\n          } catch (ConnectException e) {\n            //catch and throw the exception if tried MAX wait time to connect RM\n            dispatcher.getEventHandler().handle(\n                new NodeManagerEvent(NodeManagerEventType.SHUTDOWN));\n            throw new YarnRuntimeException(e);\n          } catch (Throwable e) {\n\n            // TODO Better error handling. Thread can die with the rest of the\n            // NM still running.\n            LOG.error(\"Caught exception in status-updater\", e);\n          } finally {\n            synchronized (heartbeatMonitor) {\n              nextHeartBeatInterval = nextHeartBeatInterval <= 0 ?\n                  YarnConfiguration.DEFAULT_RM_NM_HEARTBEAT_INTERVAL_MS :\n                    nextHeartBeatInterval;\n              try {\n                heartbeatMonitor.wait(nextHeartBeatInterval);\n              } catch (InterruptedException e) {\n                // Do Nothing\n              }\n            }\n          }\n        }\n      }",
                "code_after_change": "      public void run() {\n        int lastHeartbeatID = 0;\n        while (!isStopped) {\n          // Send heartbeat\n          try {\n            NodeHeartbeatResponse response = null;\n            Set<NodeLabel> nodeLabelsForHeartbeat =\n                nodeLabelsHandler.getNodeLabelsForHeartbeat();\n            NodeStatus nodeStatus = getNodeStatus(lastHeartbeatID);\n\n            NodeHeartbeatRequest request =\n                NodeHeartbeatRequest.newInstance(nodeStatus,\n                    NodeStatusUpdaterImpl.this.context\n                        .getContainerTokenSecretManager().getCurrentKey(),\n                    NodeStatusUpdaterImpl.this.context\n                        .getNMTokenSecretManager().getCurrentKey(),\n                    nodeLabelsForHeartbeat);\n\n            if (logAggregationEnabled) {\n              // pull log aggregation status for application running in this NM\n              List<LogAggregationReport> logAggregationReports =\n                  getLogAggregationReportsForApps(context\n                    .getLogAggregationStatusForApps());\n              if (logAggregationReports != null\n                  && !logAggregationReports.isEmpty()) {\n                request.setLogAggregationReportsForApps(logAggregationReports);\n              }\n            }\n\n            response = resourceTracker.nodeHeartbeat(request);\n            //get next heartbeat interval from response\n            nextHeartBeatInterval = response.getNextHeartBeatInterval();\n            updateMasterKeys(response);\n\n            if (response.getNodeAction() == NodeAction.SHUTDOWN) {\n              LOG.warn(\"Recieved SHUTDOWN signal from Resourcemanager as part of\"\n                  + \" heartbeat, hence shutting down.\");\n              LOG.warn(\"Message from ResourceManager: \"\n                  + response.getDiagnosticsMessage());\n              context.setDecommissioned(true);\n              dispatcher.getEventHandler().handle(\n                  new NodeManagerEvent(NodeManagerEventType.SHUTDOWN));\n              break;\n            }\n            if (response.getNodeAction() == NodeAction.RESYNC) {\n              LOG.warn(\"Node is out of sync with ResourceManager,\"\n                  + \" hence resyncing.\");\n              LOG.warn(\"Message from ResourceManager: \"\n                  + response.getDiagnosticsMessage());\n              // Invalidate the RMIdentifier while resync\n              NodeStatusUpdaterImpl.this.rmIdentifier =\n                  ResourceManagerConstants.RM_INVALID_IDENTIFIER;\n              dispatcher.getEventHandler().handle(\n                  new NodeManagerEvent(NodeManagerEventType.RESYNC));\n              pendingCompletedContainers.clear();\n              break;\n            }\n\n            nodeLabelsHandler.verifyRMHeartbeatResponseForNodeLabels(response);\n\n            // Explicitly put this method after checking the resync response. We\n            // don't want to remove the completed containers before resync\n            // because these completed containers will be reported back to RM\n            // when NM re-registers with RM.\n            // Only remove the cleanedup containers that are acked\n            removeOrTrackCompletedContainersFromContext(response\n                  .getContainersToBeRemovedFromNM());\n\n            logAggregationReportForAppsTempList.clear();\n            lastHeartbeatID = response.getResponseId();\n            List<ContainerId> containersToCleanup = response\n                .getContainersToCleanup();\n            if (!containersToCleanup.isEmpty()) {\n              dispatcher.getEventHandler().handle(\n                  new CMgrCompletedContainersEvent(containersToCleanup,\n                    CMgrCompletedContainersEvent.Reason.BY_RESOURCEMANAGER));\n            }\n            List<ApplicationId> appsToCleanup =\n                response.getApplicationsToCleanup();\n            //Only start tracking for keepAlive on FINISH_APP\n            trackAppsForKeepAlive(appsToCleanup);\n            if (!appsToCleanup.isEmpty()) {\n              dispatcher.getEventHandler().handle(\n                  new CMgrCompletedAppsEvent(appsToCleanup,\n                      CMgrCompletedAppsEvent.Reason.BY_RESOURCEMANAGER));\n            }\n\n            Map<ApplicationId, ByteBuffer> systemCredentials =\n                response.getSystemCredentialsForApps();\n            if (systemCredentials != null && !systemCredentials.isEmpty()) {\n              ((NMContext) context)\n                .setSystemCrendentialsForApps(parseCredentials(systemCredentials));\n            }\n\n            List<org.apache.hadoop.yarn.api.records.Container>\n                containersToDecrease = response.getContainersToDecrease();\n            if (!containersToDecrease.isEmpty()) {\n              dispatcher.getEventHandler().handle(\n                  new CMgrDecreaseContainersResourceEvent(containersToDecrease)\n              );\n            }\n\n            // SignalContainer request originally comes from end users via\n            // ClientRMProtocol's SignalContainer. Forward the request to\n            // ContainerManager which will dispatch the event to ContainerLauncher.\n            List<SignalContainerRequest> containersToSignal = response\n                .getContainersToSignalList();\n            if (containersToSignal.size() != 0) {\n              dispatcher.getEventHandler().handle(\n                  new CMgrSignalContainersEvent(containersToSignal));\n            }\n          } catch (ConnectException e) {\n            //catch and throw the exception if tried MAX wait time to connect RM\n            dispatcher.getEventHandler().handle(\n                new NodeManagerEvent(NodeManagerEventType.SHUTDOWN));\n            // failed to connect to RM.\n            failedToConnect = true;\n            throw new YarnRuntimeException(e);\n          } catch (Throwable e) {\n\n            // TODO Better error handling. Thread can die with the rest of the\n            // NM still running.\n            LOG.error(\"Caught exception in status-updater\", e);\n          } finally {\n            synchronized (heartbeatMonitor) {\n              nextHeartBeatInterval = nextHeartBeatInterval <= 0 ?\n                  YarnConfiguration.DEFAULT_RM_NM_HEARTBEAT_INTERVAL_MS :\n                    nextHeartBeatInterval;\n              try {\n                heartbeatMonitor.wait(nextHeartBeatInterval);\n              } catch (InterruptedException e) {\n                // Do Nothing\n              }\n            }\n          }\n        }\n      }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.serviceStop": {
                "code_before_change": "  protected void serviceStop() throws Exception {\n    // the isStopped check is for avoiding multiple unregistrations.\n    if (this.registeredWithRM && !this.isStopped\n        && !isNMUnderSupervisionWithRecoveryEnabled()\n        && !context.getDecommissioned()) {\n      unRegisterNM();\n    }\n    // Interrupt the updater.\n    this.isStopped = true;\n    stopRMProxy();\n    super.serviceStop();\n  }",
                "code_after_change": "  protected void serviceStop() throws Exception {\n    // the isStopped check is for avoiding multiple unregistrations.\n    if (this.registeredWithRM && !this.isStopped\n        && !isNMUnderSupervisionWithRecoveryEnabled()\n        && !context.getDecommissioned() && !failedToConnect) {\n      unRegisterNM();\n    }\n    // Interrupt the updater.\n    this.isStopped = true;\n    stopRMProxy();\n    super.serviceStop();\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Correct",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report precisely identifies the root cause as the unnecessary unregistration of the NodeManager during shutdown due to connection issues, which aligns with the ground truth method 'NodeStatusUpdaterImpl.serviceStop'. The fix suggestion in the report matches the developer's fix, which involves modifying the 'serviceStop' method to prevent unregistration when the shutdown is due to connection issues. The problem location is precisely identified as it mentions 'serviceStop', which is one of the ground truth methods. There is no wrong information in the bug report as all details are relevant to the context of the bug."
        }
    },
    {
        "filename": "YARN-2273.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.nodeUpdate": {
                "code_before_change": "  private synchronized void nodeUpdate(RMNode nm) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"nodeUpdate: \" + nm + \" cluster capacity: \" + clusterResource);\n    }\n    eventLog.log(\"HEARTBEAT\", nm.getHostName());\n    FSSchedulerNode node = getFSSchedulerNode(nm.getNodeID());\n\n    // Update resource if any change\n    SchedulerUtils.updateResourceIfChanged(node, nm, clusterResource, LOG);\n    \n    List<UpdatedContainerInfo> containerInfoList = nm.pullContainerUpdates();\n    List<ContainerStatus> newlyLaunchedContainers = new ArrayList<ContainerStatus>();\n    List<ContainerStatus> completedContainers = new ArrayList<ContainerStatus>();\n    for(UpdatedContainerInfo containerInfo : containerInfoList) {\n      newlyLaunchedContainers.addAll(containerInfo.getNewlyLaunchedContainers());\n      completedContainers.addAll(containerInfo.getCompletedContainers());\n    } \n    // Processing the newly launched containers\n    for (ContainerStatus launchedContainer : newlyLaunchedContainers) {\n      containerLaunchedOnNode(launchedContainer.getContainerId(), node);\n    }\n\n    // Process completed containers\n    for (ContainerStatus completedContainer : completedContainers) {\n      ContainerId containerId = completedContainer.getContainerId();\n      LOG.debug(\"Container FINISHED: \" + containerId);\n      completedContainer(getRMContainer(containerId),\n          completedContainer, RMContainerEventType.FINISHED);\n    }\n\n    if (continuousSchedulingEnabled) {\n      if (!completedContainers.isEmpty()) {\n        attemptScheduling(node);\n      }\n    } else {\n      attemptScheduling(node);\n    }\n  }",
                "code_after_change": "  private synchronized void nodeUpdate(RMNode nm) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"nodeUpdate: \" + nm + \" cluster capacity: \" + clusterResource);\n    }\n    eventLog.log(\"HEARTBEAT\", nm.getHostName());\n    FSSchedulerNode node = getFSSchedulerNode(nm.getNodeID());\n\n    // Update resource if any change\n    SchedulerUtils.updateResourceIfChanged(node, nm, clusterResource, LOG);\n    \n    List<UpdatedContainerInfo> containerInfoList = nm.pullContainerUpdates();\n    List<ContainerStatus> newlyLaunchedContainers = new ArrayList<ContainerStatus>();\n    List<ContainerStatus> completedContainers = new ArrayList<ContainerStatus>();\n    for(UpdatedContainerInfo containerInfo : containerInfoList) {\n      newlyLaunchedContainers.addAll(containerInfo.getNewlyLaunchedContainers());\n      completedContainers.addAll(containerInfo.getCompletedContainers());\n    } \n    // Processing the newly launched containers\n    for (ContainerStatus launchedContainer : newlyLaunchedContainers) {\n      containerLaunchedOnNode(launchedContainer.getContainerId(), node);\n    }\n\n    // Process completed containers\n    for (ContainerStatus completedContainer : completedContainers) {\n      ContainerId containerId = completedContainer.getContainerId();\n      LOG.debug(\"Container FINISHED: \" + containerId);\n      completedContainer(getRMContainer(containerId),\n          completedContainer, RMContainerEventType.FINISHED);\n    }\n\n    if (continuousSchedulingEnabled) {\n      if (!completedContainers.isEmpty()) {\n        attemptScheduling(node);\n      }\n    } else {\n      attemptScheduling(node);\n    }\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.initScheduler": {
                "code_before_change": "  private synchronized void initScheduler(Configuration conf)\n      throws IOException {\n    this.conf = new FairSchedulerConfiguration(conf);\n    validateConf(this.conf);\n    minimumAllocation = this.conf.getMinimumAllocation();\n    maximumAllocation = this.conf.getMaximumAllocation();\n    incrAllocation = this.conf.getIncrementAllocation();\n    continuousSchedulingEnabled = this.conf.isContinuousSchedulingEnabled();\n    continuousSchedulingSleepMs =\n        this.conf.getContinuousSchedulingSleepMs();\n    nodeLocalityThreshold = this.conf.getLocalityThresholdNode();\n    rackLocalityThreshold = this.conf.getLocalityThresholdRack();\n    nodeLocalityDelayMs = this.conf.getLocalityDelayNodeMs();\n    rackLocalityDelayMs = this.conf.getLocalityDelayRackMs();\n    preemptionEnabled = this.conf.getPreemptionEnabled();\n    preemptionUtilizationThreshold =\n        this.conf.getPreemptionUtilizationThreshold();\n    assignMultiple = this.conf.getAssignMultiple();\n    maxAssign = this.conf.getMaxAssign();\n    sizeBasedWeight = this.conf.getSizeBasedWeight();\n    preemptionInterval = this.conf.getPreemptionInterval();\n    waitTimeBeforeKill = this.conf.getWaitTimeBeforeKill();\n    usePortForNodeName = this.conf.getUsePortForNodeName();\n\n    rootMetrics = FSQueueMetrics.forQueue(\"root\", null, true, conf);\n    // This stores per-application scheduling information\n    this.applications =\n        new ConcurrentHashMap<ApplicationId,SchedulerApplication<FSSchedulerApp>>();\n    this.eventLog = new FairSchedulerEventLog();\n    eventLog.init(this.conf);\n\n    allocConf = new AllocationConfiguration(conf);\n    try {\n      queueMgr.initialize(conf);\n    } catch (Exception e) {\n      throw new IOException(\"Failed to start FairScheduler\", e);\n    }\n\n    updateThread = new Thread(new UpdateThread());\n    updateThread.setName(\"FairSchedulerUpdateThread\");\n    updateThread.setDaemon(true);\n\n    if (continuousSchedulingEnabled) {\n      // start continuous scheduling thread\n      schedulingThread = new Thread(\n          new Runnable() {\n            @Override\n            public void run() {\n              continuousScheduling();\n            }\n          }\n      );\n      schedulingThread.setName(\"ContinuousScheduling\");\n      schedulingThread.setDaemon(true);\n    }\n\n    allocsLoader.init(conf);\n    allocsLoader.setReloadListener(new AllocationReloadListener());\n    // If we fail to load allocations file on initialize, we want to fail\n    // immediately.  After a successful load, exceptions on future reloads\n    // will just result in leaving things as they are.\n    try {\n      allocsLoader.reloadAllocations();\n    } catch (Exception e) {\n      throw new IOException(\"Failed to initialize FairScheduler\", e);\n    }\n  }",
                "code_after_change": "  private synchronized void initScheduler(Configuration conf)\n      throws IOException {\n    this.conf = new FairSchedulerConfiguration(conf);\n    validateConf(this.conf);\n    minimumAllocation = this.conf.getMinimumAllocation();\n    maximumAllocation = this.conf.getMaximumAllocation();\n    incrAllocation = this.conf.getIncrementAllocation();\n    continuousSchedulingEnabled = this.conf.isContinuousSchedulingEnabled();\n    continuousSchedulingSleepMs =\n        this.conf.getContinuousSchedulingSleepMs();\n    nodeLocalityThreshold = this.conf.getLocalityThresholdNode();\n    rackLocalityThreshold = this.conf.getLocalityThresholdRack();\n    nodeLocalityDelayMs = this.conf.getLocalityDelayNodeMs();\n    rackLocalityDelayMs = this.conf.getLocalityDelayRackMs();\n    preemptionEnabled = this.conf.getPreemptionEnabled();\n    preemptionUtilizationThreshold =\n        this.conf.getPreemptionUtilizationThreshold();\n    assignMultiple = this.conf.getAssignMultiple();\n    maxAssign = this.conf.getMaxAssign();\n    sizeBasedWeight = this.conf.getSizeBasedWeight();\n    preemptionInterval = this.conf.getPreemptionInterval();\n    waitTimeBeforeKill = this.conf.getWaitTimeBeforeKill();\n    usePortForNodeName = this.conf.getUsePortForNodeName();\n\n    rootMetrics = FSQueueMetrics.forQueue(\"root\", null, true, conf);\n    // This stores per-application scheduling information\n    this.applications =\n        new ConcurrentHashMap<ApplicationId,SchedulerApplication<FSSchedulerApp>>();\n    this.eventLog = new FairSchedulerEventLog();\n    eventLog.init(this.conf);\n\n    allocConf = new AllocationConfiguration(conf);\n    try {\n      queueMgr.initialize(conf);\n    } catch (Exception e) {\n      throw new IOException(\"Failed to start FairScheduler\", e);\n    }\n\n    updateThread = new Thread(new UpdateThread());\n    updateThread.setName(\"FairSchedulerUpdateThread\");\n    updateThread.setDaemon(true);\n\n    if (continuousSchedulingEnabled) {\n      // start continuous scheduling thread\n      schedulingThread = new Thread(\n          new Runnable() {\n            @Override\n            public void run() {\n              while (!Thread.currentThread().isInterrupted()) {\n                try {\n                  continuousSchedulingAttempt();\n                  Thread.sleep(getContinuousSchedulingSleepMs());\n                } catch (InterruptedException e) {\n                  LOG.error(\"Continuous scheduling thread interrupted. Exiting. \",\n                      e);\n                  return;\n                }\n              }\n            }\n          }\n      );\n      schedulingThread.setName(\"ContinuousScheduling\");\n      schedulingThread.setDaemon(true);\n    }\n\n    allocsLoader.init(conf);\n    allocsLoader.setReloadListener(new AllocationReloadListener());\n    // If we fail to load allocations file on initialize, we want to fail\n    // immediately.  After a successful load, exceptions on future reloads\n    // will just result in leaving things as they are.\n    try {\n      allocsLoader.reloadAllocations();\n    } catch (Exception e) {\n      throw new IOException(\"Failed to initialize FairScheduler\", e);\n    }\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Preventive",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the root cause as a NullPointerException in the NodeAvailableResourceComparator's compare method, which is mentioned in the stack trace. However, the actual fix was made in the nodeUpdate and initScheduler methods, which are not directly mentioned. Therefore, the root cause identification is Partial with a Shared Stack Trace Context sub-category. The fix suggestion is Preventive as it suggests checking for null values, which would mitigate the issue but does not match the actual fix. The problem location identification is Partial with a Shared Stack Trace Context sub-category because the problem location is identified in the stack trace but not in the ground truth methods. There is no wrong information in the bug report as all details are relevant to the context of the bug."
        }
    },
    {
        "filename": "YARN-2834.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.handle": {
                "code_before_change": "  public void handle(RMAppEvent event) {\n\n    this.writeLock.lock();\n\n    try {\n      ApplicationId appID = event.getApplicationId();\n      LOG.debug(\"Processing event for \" + appID + \" of type \"\n          + event.getType());\n      final RMAppState oldState = getState();\n      try {\n        /* keep the master in sync with the state machine */\n        this.stateMachine.doTransition(event.getType(), event);\n      } catch (InvalidStateTransitonException e) {\n        LOG.error(\"Can't handle this event at current state\", e);\n        /* TODO fail the application on the failed transition */\n      }\n\n      if (oldState != getState()) {\n        LOG.info(appID + \" State change from \" + oldState + \" to \"\n            + getState());\n      }\n    } finally {\n      this.writeLock.unlock();\n    }\n  }",
                "code_after_change": "  public void handle(RMAppEvent event) {\n\n    this.writeLock.lock();\n\n    try {\n      ApplicationId appID = event.getApplicationId();\n      LOG.debug(\"Processing event for \" + appID + \" of type \"\n          + event.getType());\n      final RMAppState oldState = getState();\n      try {\n        /* keep the master in sync with the state machine */\n        this.stateMachine.doTransition(event.getType(), event);\n      } catch (InvalidStateTransitonException e) {\n        LOG.error(\"Can't handle this event at current state\", e);\n        /* TODO fail the application on the failed transition */\n      }\n\n      if (oldState != getState()) {\n        LOG.info(appID + \" State change from \" + oldState + \" to \"\n            + getState());\n      }\n    } finally {\n      this.writeLock.unlock();\n    }\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.transition": {
                "code_before_change": "    public void transition(RMAppImpl app, RMAppEvent event) {\n    };",
                "code_after_change": "    public void transition(RMAppImpl app, RMAppEvent event) {\n    };"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.getApplicationResourceUsageReport": {
                "code_before_change": "  public ApplicationResourceUsageReport getApplicationResourceUsageReport() {\n    this.readLock.lock();\n    try {\n      ApplicationResourceUsageReport report =\n          scheduler.getAppResourceUsageReport(this.getAppAttemptId());\n      if (report == null) {\n        report = RMServerUtils.DUMMY_APPLICATION_RESOURCE_USAGE_REPORT;\n      }\n      AggregateAppResourceUsage resUsage =\n          this.attemptMetrics.getAggregateAppResourceUsage();\n      report.setMemorySeconds(resUsage.getMemorySeconds());\n      report.setVcoreSeconds(resUsage.getVcoreSeconds());\n      return report;\n    } finally {\n      this.readLock.unlock();\n    }\n  }",
                "code_after_change": "  public ApplicationResourceUsageReport getApplicationResourceUsageReport() {\n    this.readLock.lock();\n    try {\n      ApplicationResourceUsageReport report =\n          scheduler.getAppResourceUsageReport(this.getAppAttemptId());\n      if (report == null) {\n        report = RMServerUtils.DUMMY_APPLICATION_RESOURCE_USAGE_REPORT;\n      }\n      AggregateAppResourceUsage resUsage =\n          this.attemptMetrics.getAggregateAppResourceUsage();\n      report.setMemorySeconds(resUsage.getMemorySeconds());\n      report.setVcoreSeconds(resUsage.getVcoreSeconds());\n      return report;\n    } finally {\n      this.readLock.unlock();\n    }\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.transferStateFromPreviousAttempt": {
                "code_before_change": "  public void transferStateFromPreviousAttempt(RMAppAttempt attempt) {\n    this.justFinishedContainers = attempt.getJustFinishedContainersReference();\n    this.finishedContainersSentToAM =\n        attempt.getFinishedContainersSentToAMReference();\n  }",
                "code_after_change": "  public void transferStateFromPreviousAttempt(RMAppAttempt attempt) {\n    this.justFinishedContainers = attempt.getJustFinishedContainersReference();\n    this.finishedContainersSentToAM =\n        attempt.getFinishedContainersSentToAMReference();\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Preventive",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the root cause as a Null Pointer Exception in the 'addApplicationAttempt' method, which is mentioned in the stack trace but is not the ground truth method. Therefore, it is classified as 'Partial' with 'Shared Stack Trace Context'. The fix suggestion is 'Preventive' as it suggests checking initialization and configuration, which could prevent the issue but does not match the developer's fix. The problem location is also 'Partial' with 'Shared Stack Trace Context' because it mentions methods in the stack trace but not the ground truth methods. There is no wrong information in the bug report as all details are relevant to the context of the bug."
        }
    },
    {
        "filename": "YARN-3675.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.compare": {
                "code_before_change": "    public int compare(NodeId n1, NodeId n2) {\n      if (!nodes.containsKey(n1)) {\n        return 1;\n      }\n      if (!nodes.containsKey(n2)) {\n        return -1;\n      }\n      return RESOURCE_CALCULATOR.compare(clusterResource,\n              nodes.get(n2).getAvailableResource(),\n              nodes.get(n1).getAvailableResource());\n    }",
                "code_after_change": "    public int compare(NodeId n1, NodeId n2) {\n      if (!nodes.containsKey(n1)) {\n        return 1;\n      }\n      if (!nodes.containsKey(n2)) {\n        return -1;\n      }\n      return RESOURCE_CALCULATOR.compare(clusterResource,\n              nodes.get(n2).getAvailableResource(),\n              nodes.get(n1).getAvailableResource());\n    }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Preventive",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the `NullPointerException` in the `unreserve` method of the `FSAppAttempt` class, which is part of the stack trace but not the ground truth method. This places it in the 'Shared Stack Trace Context' sub-category for root cause identification. The fix suggestion involves adding null checks, which is preventive as it would mitigate the issue but does not match the actual fix in the `compare` method. The problem location identification is also partial, as it points to methods in the stack trace but not the ground truth method. There is no wrong information in the bug report as all details are relevant to the context of the bug."
        }
    },
    {
        "filename": "YARN-4763.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.webapp.RMAppsBlock.renderData": {
                "code_before_change": "  protected void renderData(Block html) {\n    TBODY<TABLE<Hamlet>> tbody =\n        html.table(\"#apps\").thead().tr().th(\".id\", \"ID\").th(\".user\", \"User\")\n          .th(\".name\", \"Name\").th(\".type\", \"Application Type\")\n          .th(\".queue\", \"Queue\").th(\".priority\", \"Application Priority\")\n          .th(\".starttime\", \"StartTime\")\n          .th(\".finishtime\", \"FinishTime\").th(\".state\", \"State\")\n          .th(\".finalstatus\", \"FinalStatus\")\n          .th(\".runningcontainer\", \"Running Containers\")\n          .th(\".allocatedCpu\", \"Allocated CPU VCores\")\n          .th(\".allocatedMemory\", \"Allocated Memory MB\")\n          .th(\".queuePercentage\", \"% of Queue\")\n          .th(\".clusterPercentage\", \"% of Cluster\")\n          .th(\".progress\", \"Progress\")\n          .th(\".ui\", \"Tracking UI\")\n          .th(\".blacklisted\", \"Blacklisted Nodes\")._()\n          ._().tbody();\n\n    StringBuilder appsTableData = new StringBuilder(\"[\\n\");\n    for (ApplicationReport appReport : appReports) {\n      // TODO: remove the following condition. It is still here because\n      // the history side implementation of ApplicationBaseProtocol\n      // hasn't filtering capability (YARN-1819).\n      if (!reqAppStates.isEmpty()\n          && !reqAppStates.contains(appReport.getYarnApplicationState())) {\n        continue;\n      }\n\n      AppInfo app = new AppInfo(appReport);\n      ApplicationAttemptId appAttemptId =\n          ConverterUtils.toApplicationAttemptId(app.getCurrentAppAttemptId());\n      String queuePercent = \"N/A\";\n      String clusterPercent = \"N/A\";\n      if(appReport.getApplicationResourceUsageReport() != null) {\n        queuePercent = String.format(\"%.1f\",\n            appReport.getApplicationResourceUsageReport()\n                .getQueueUsagePercentage());\n        clusterPercent = String.format(\"%.1f\",\n            appReport.getApplicationResourceUsageReport().getClusterUsagePercentage());\n      }\n\n      String blacklistedNodesCount = \"N/A\";\n      Set<String> nodes = rm.getRMContext().getRMApps()\n          .get(appAttemptId.getApplicationId()).getAppAttempts()\n          .get(appAttemptId).getBlacklistedNodes();\n      if (nodes != null) {\n        blacklistedNodesCount = String.valueOf(nodes.size());\n      }\n      String percent = StringUtils.format(\"%.1f\", app.getProgress());\n      appsTableData\n        .append(\"[\\\"<a href='\")\n        .append(url(\"app\", app.getAppId()))\n        .append(\"'>\")\n        .append(app.getAppId())\n        .append(\"</a>\\\",\\\"\")\n        .append(\n          StringEscapeUtils.escapeJavaScript(\n              StringEscapeUtils.escapeHtml(app.getUser())))\n        .append(\"\\\",\\\"\")\n        .append(\n          StringEscapeUtils.escapeJavaScript(\n              StringEscapeUtils.escapeHtml(app.getName())))\n        .append(\"\\\",\\\"\")\n        .append(\n          StringEscapeUtils.escapeJavaScript(StringEscapeUtils.escapeHtml(app\n            .getType())))\n        .append(\"\\\",\\\"\")\n        .append(\n          StringEscapeUtils.escapeJavaScript(StringEscapeUtils.escapeHtml(app\n             .getQueue()))).append(\"\\\",\\\"\").append(String\n             .valueOf(app.getPriority()))\n        .append(\"\\\",\\\"\").append(app.getStartedTime())\n        .append(\"\\\",\\\"\").append(app.getFinishedTime())\n        .append(\"\\\",\\\"\")\n        .append(app.getAppState() == null ? UNAVAILABLE : app.getAppState())\n        .append(\"\\\",\\\"\")\n        .append(app.getFinalAppStatus())\n        .append(\"\\\",\\\"\")\n        .append(app.getRunningContainers() == -1 ? \"N/A\" : String\n            .valueOf(app.getRunningContainers()))\n        .append(\"\\\",\\\"\")\n        .append(app.getAllocatedCpuVcores() == -1 ? \"N/A\" : String\n            .valueOf(app.getAllocatedCpuVcores()))\n        .append(\"\\\",\\\"\")\n        .append(app.getAllocatedMemoryMB() == -1 ? \"N/A\" :\n            String.valueOf(app.getAllocatedMemoryMB()))\n        .append(\"\\\",\\\"\")\n        .append(queuePercent)\n        .append(\"\\\",\\\"\")\n        .append(clusterPercent)\n        .append(\"\\\",\\\"\")\n        // Progress bar\n          .append(\"<br title='\").append(percent).append(\"'> <div class='\")\n        .append(C_PROGRESSBAR).append(\"' title='\").append(join(percent, '%'))\n        .append(\"'> \").append(\"<div class='\").append(C_PROGRESSBAR_VALUE)\n        .append(\"' style='\").append(join(\"width:\", percent, '%'))\n        .append(\"'> </div> </div>\").append(\"\\\",\\\"<a \");\n\n      String trackingURL =\n          app.getTrackingUrl() == null\n              || app.getTrackingUrl().equals(UNAVAILABLE)\n              || app.getAppState() == YarnApplicationState.NEW ? null : app\n              .getTrackingUrl();\n\n      String trackingUI =\n          app.getTrackingUrl() == null\n              || app.getTrackingUrl().equals(UNAVAILABLE)\n              || app.getAppState() == YarnApplicationState.NEW ? \"Unassigned\"\n              : app.getAppState() == YarnApplicationState.FINISHED\n              || app.getAppState() == YarnApplicationState.FAILED\n              || app.getAppState() == YarnApplicationState.KILLED ? \"History\"\n              : \"ApplicationMaster\";\n      appsTableData.append(trackingURL == null ? \"#\" : \"href='\" + trackingURL)\n        .append(\"'>\").append(trackingUI).append(\"</a>\\\",\").append(\"\\\"\")\n        .append(blacklistedNodesCount).append(\"\\\"],\\n\");\n\n    }\n    if (appsTableData.charAt(appsTableData.length() - 2) == ',') {\n      appsTableData.delete(appsTableData.length() - 2,\n        appsTableData.length() - 1);\n    }\n    appsTableData.append(\"]\");\n    html.script().$type(\"text/javascript\")\n      ._(\"var appsTableData=\" + appsTableData)._();\n\n    tbody._()._();\n  }",
                "code_after_change": "  protected void renderData(Block html) {\n    TBODY<TABLE<Hamlet>> tbody =\n        html.table(\"#apps\").thead().tr().th(\".id\", \"ID\").th(\".user\", \"User\")\n          .th(\".name\", \"Name\").th(\".type\", \"Application Type\")\n          .th(\".queue\", \"Queue\").th(\".priority\", \"Application Priority\")\n          .th(\".starttime\", \"StartTime\")\n          .th(\".finishtime\", \"FinishTime\").th(\".state\", \"State\")\n          .th(\".finalstatus\", \"FinalStatus\")\n          .th(\".runningcontainer\", \"Running Containers\")\n          .th(\".allocatedCpu\", \"Allocated CPU VCores\")\n          .th(\".allocatedMemory\", \"Allocated Memory MB\")\n          .th(\".queuePercentage\", \"% of Queue\")\n          .th(\".clusterPercentage\", \"% of Cluster\")\n          .th(\".progress\", \"Progress\")\n          .th(\".ui\", \"Tracking UI\")\n          .th(\".blacklisted\", \"Blacklisted Nodes\")._()\n          ._().tbody();\n\n    StringBuilder appsTableData = new StringBuilder(\"[\\n\");\n    for (ApplicationReport appReport : appReports) {\n      // TODO: remove the following condition. It is still here because\n      // the history side implementation of ApplicationBaseProtocol\n      // hasn't filtering capability (YARN-1819).\n      if (!reqAppStates.isEmpty()\n          && !reqAppStates.contains(appReport.getYarnApplicationState())) {\n        continue;\n      }\n\n      AppInfo app = new AppInfo(appReport);\n      ApplicationAttemptId appAttemptId =\n          ConverterUtils.toApplicationAttemptId(app.getCurrentAppAttemptId());\n      String queuePercent = \"N/A\";\n      String clusterPercent = \"N/A\";\n      if(appReport.getApplicationResourceUsageReport() != null) {\n        queuePercent = String.format(\"%.1f\",\n            appReport.getApplicationResourceUsageReport()\n                .getQueueUsagePercentage());\n        clusterPercent = String.format(\"%.1f\",\n            appReport.getApplicationResourceUsageReport().getClusterUsagePercentage());\n      }\n\n      String blacklistedNodesCount = \"N/A\";\n      RMAppAttempt appAttempt =\n          rm.getRMContext().getRMApps().get(appAttemptId.getApplicationId())\n              .getAppAttempts().get(appAttemptId);\n      Set<String> nodes =\n          null == appAttempt ? null : appAttempt.getBlacklistedNodes();\n      if (nodes != null) {\n        blacklistedNodesCount = String.valueOf(nodes.size());\n      }\n      String percent = StringUtils.format(\"%.1f\", app.getProgress());\n      appsTableData\n        .append(\"[\\\"<a href='\")\n        .append(url(\"app\", app.getAppId()))\n        .append(\"'>\")\n        .append(app.getAppId())\n        .append(\"</a>\\\",\\\"\")\n        .append(\n          StringEscapeUtils.escapeJavaScript(\n              StringEscapeUtils.escapeHtml(app.getUser())))\n        .append(\"\\\",\\\"\")\n        .append(\n          StringEscapeUtils.escapeJavaScript(\n              StringEscapeUtils.escapeHtml(app.getName())))\n        .append(\"\\\",\\\"\")\n        .append(\n          StringEscapeUtils.escapeJavaScript(StringEscapeUtils.escapeHtml(app\n            .getType())))\n        .append(\"\\\",\\\"\")\n        .append(\n          StringEscapeUtils.escapeJavaScript(StringEscapeUtils.escapeHtml(app\n             .getQueue()))).append(\"\\\",\\\"\").append(String\n             .valueOf(app.getPriority()))\n        .append(\"\\\",\\\"\").append(app.getStartedTime())\n        .append(\"\\\",\\\"\").append(app.getFinishedTime())\n        .append(\"\\\",\\\"\")\n        .append(app.getAppState() == null ? UNAVAILABLE : app.getAppState())\n        .append(\"\\\",\\\"\")\n        .append(app.getFinalAppStatus())\n        .append(\"\\\",\\\"\")\n        .append(app.getRunningContainers() == -1 ? \"N/A\" : String\n            .valueOf(app.getRunningContainers()))\n        .append(\"\\\",\\\"\")\n        .append(app.getAllocatedCpuVcores() == -1 ? \"N/A\" : String\n            .valueOf(app.getAllocatedCpuVcores()))\n        .append(\"\\\",\\\"\")\n        .append(app.getAllocatedMemoryMB() == -1 ? \"N/A\" :\n            String.valueOf(app.getAllocatedMemoryMB()))\n        .append(\"\\\",\\\"\")\n        .append(queuePercent)\n        .append(\"\\\",\\\"\")\n        .append(clusterPercent)\n        .append(\"\\\",\\\"\")\n        // Progress bar\n          .append(\"<br title='\").append(percent).append(\"'> <div class='\")\n        .append(C_PROGRESSBAR).append(\"' title='\").append(join(percent, '%'))\n        .append(\"'> \").append(\"<div class='\").append(C_PROGRESSBAR_VALUE)\n        .append(\"' style='\").append(join(\"width:\", percent, '%'))\n        .append(\"'> </div> </div>\").append(\"\\\",\\\"<a \");\n\n      String trackingURL =\n          app.getTrackingUrl() == null\n              || app.getTrackingUrl().equals(UNAVAILABLE)\n              || app.getAppState() == YarnApplicationState.NEW ? null : app\n              .getTrackingUrl();\n\n      String trackingUI =\n          app.getTrackingUrl() == null\n              || app.getTrackingUrl().equals(UNAVAILABLE)\n              || app.getAppState() == YarnApplicationState.NEW ? \"Unassigned\"\n              : app.getAppState() == YarnApplicationState.FINISHED\n              || app.getAppState() == YarnApplicationState.FAILED\n              || app.getAppState() == YarnApplicationState.KILLED ? \"History\"\n              : \"ApplicationMaster\";\n      appsTableData.append(trackingURL == null ? \"#\" : \"href='\" + trackingURL)\n        .append(\"'>\").append(trackingUI).append(\"</a>\\\",\").append(\"\\\"\")\n        .append(blacklistedNodesCount).append(\"\\\"],\\n\");\n\n    }\n    if (appsTableData.charAt(appsTableData.length() - 2) == ',') {\n      appsTableData.delete(appsTableData.length() - 2,\n        appsTableData.length() - 1);\n    }\n    appsTableData.append(\"]\");\n    html.script().$type(\"text/javascript\")\n      ._(\"var appsTableData=\" + appsTableData)._();\n\n    tbody._()._();\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Correct",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report precisely identifies the root cause of the NullPointerException in the `renderData` method of the `RMAppsBlock` class, which matches the ground truth method. The fix suggestion in the report is correct as it aligns with the developer's fix, which involves ensuring that the `appReports` variable is properly initialized before use. The problem location is also precisely identified as the `renderData` method in the `RMAppsBlock` class, which is the ground truth method. There is no wrong information in the bug report as all details are consistent with the context of the bug."
        }
    },
    {
        "filename": "YARN-8202.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils.normalizeAndvalidateRequest": {
                "code_before_change": "  public static void normalizeAndvalidateRequest(ResourceRequest resReq,\n      Resource maximumResource, String queueName, YarnScheduler scheduler,\n      RMContext rmContext) throws InvalidResourceRequestException {\n    normalizeAndvalidateRequest(resReq, maximumResource, queueName, scheduler,\n        rmContext, null);\n  }",
                "code_after_change": "  public static void normalizeAndvalidateRequest(ResourceRequest resReq,\n      Resource maximumResource, String queueName, YarnScheduler scheduler,\n      RMContext rmContext) throws InvalidResourceRequestException {\n    normalizeAndvalidateRequest(resReq, maximumResource, queueName, scheduler,\n        rmContext, null);\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils.validateResourceRequest": {
                "code_before_change": "  private static void validateResourceRequest(ResourceRequest resReq,\n      Resource maximumResource, QueueInfo queueInfo, RMContext rmContext)\n      throws InvalidResourceRequestException {\n    Resource requestedResource = resReq.getCapability();\n    for (int i = 0; i < ResourceUtils.getNumberOfKnownResourceTypes(); i++) {\n      ResourceInformation reqRI = requestedResource.getResourceInformation(i);\n      ResourceInformation maxRI = maximumResource.getResourceInformation(i);\n      if (reqRI.getValue() < 0 || reqRI.getValue() > maxRI.getValue()) {\n        throw new InvalidResourceRequestException(\n            \"Invalid resource request, requested resource type=[\" + reqRI\n                .getName()\n                + \"] < 0 or greater than maximum allowed allocation. Requested \"\n                + \"resource=\" + requestedResource\n                + \", maximum allowed allocation=\" + maximumResource\n                + \", please note that maximum allowed allocation is calculated \"\n                + \"by scheduler based on maximum resource of registered \"\n                + \"NodeManagers, which might be less than configured \"\n                + \"maximum allocation=\" + ResourceUtils\n                .getResourceTypesMaximumAllocation());\n      }\n    }\n    String labelExp = resReq.getNodeLabelExpression();\n    // we don't allow specify label expression other than resourceName=ANY now\n    if (!ResourceRequest.ANY.equals(resReq.getResourceName())\n        && labelExp != null && !labelExp.trim().isEmpty()) {\n      throw new InvalidLabelResourceRequestException(\n          \"Invalid resource request, queue=\" + queueInfo.getQueueName()\n              + \" specified node label expression in a \"\n              + \"resource request has resource name = \"\n              + resReq.getResourceName());\n    }\n\n    // we don't allow specify label expression with more than one node labels now\n    if (labelExp != null && labelExp.contains(\"&&\")) {\n      throw new InvalidLabelResourceRequestException(\n          \"Invalid resource request, queue=\" + queueInfo.getQueueName()\n              + \" specified more than one node label \"\n              + \"in a node label expression, node label expression = \"\n              + labelExp);\n    }\n\n    if (labelExp != null && !labelExp.trim().isEmpty() && queueInfo != null) {\n      if (!checkQueueLabelExpression(queueInfo.getAccessibleNodeLabels(),\n          labelExp, rmContext)) {\n        throw new InvalidLabelResourceRequestException(\n            \"Invalid resource request\" + \", queue=\" + queueInfo.getQueueName()\n                + \" doesn't have permission to access all labels \"\n                + \"in resource request. labelExpression of resource request=\"\n                + labelExp + \". Queue labels=\"\n                + (queueInfo.getAccessibleNodeLabels() == null ? \"\"\n                    : StringUtils.join(\n                        queueInfo.getAccessibleNodeLabels().iterator(), ',')));\n      } else {\n        checkQueueLabelInLabelManager(labelExp, rmContext);\n      }\n    }\n  }",
                "code_after_change": "  private static void validateResourceRequest(ResourceRequest resReq,\n      Resource maximumResource, QueueInfo queueInfo, RMContext rmContext)\n      throws InvalidResourceRequestException {\n    final Resource requestedResource = resReq.getCapability();\n    checkResourceRequestAgainstAvailableResource(requestedResource,\n        maximumResource);\n\n    String labelExp = resReq.getNodeLabelExpression();\n    // we don't allow specify label expression other than resourceName=ANY now\n    if (!ResourceRequest.ANY.equals(resReq.getResourceName())\n        && labelExp != null && !labelExp.trim().isEmpty()) {\n      throw new InvalidLabelResourceRequestException(\n          \"Invalid resource request, queue=\" + queueInfo.getQueueName()\n              + \" specified node label expression in a \"\n              + \"resource request has resource name = \"\n              + resReq.getResourceName());\n    }\n\n    // we don't allow specify label expression with more than one node labels now\n    if (labelExp != null && labelExp.contains(\"&&\")) {\n      throw new InvalidLabelResourceRequestException(\n          \"Invalid resource request, queue=\" + queueInfo.getQueueName()\n              + \" specified more than one node label \"\n              + \"in a node label expression, node label expression = \"\n              + labelExp);\n    }\n\n    if (labelExp != null && !labelExp.trim().isEmpty() && queueInfo != null) {\n      if (!checkQueueLabelExpression(queueInfo.getAccessibleNodeLabels(),\n          labelExp, rmContext)) {\n        throw new InvalidLabelResourceRequestException(\n            \"Invalid resource request\" + \", queue=\" + queueInfo.getQueueName()\n                + \" doesn't have permission to access all labels \"\n                + \"in resource request. labelExpression of resource request=\"\n                + labelExp + \". Queue labels=\"\n                + (queueInfo.getAccessibleNodeLabels() == null ? \"\"\n                    : StringUtils.join(\n                        queueInfo.getAccessibleNodeLabels().iterator(), ',')));\n      } else {\n        checkQueueLabelInLabelManager(labelExp, rmContext);\n      }\n    }\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-api.src.main.java.org.apache.hadoop.yarn.util.UnitsConversionUtil.convert": {
                "code_before_change": "  public static long convert(String fromUnit, String toUnit, long fromValue) {\n    if (toUnit == null || fromUnit == null) {\n      throw new IllegalArgumentException(\"One or more arguments are null\");\n    }\n\n    if (fromUnit.equals(toUnit)) {\n      return fromValue;\n    }\n    Converter fc = getConverter(fromUnit);\n    Converter tc = getConverter(toUnit);\n    long numerator = fc.numerator * tc.denominator;\n    long denominator = fc.denominator * tc.numerator;\n    long numeratorMultiplierLimit = Long.MAX_VALUE / numerator;\n    if (numerator < denominator) {\n      if (numeratorMultiplierLimit < fromValue) {\n        String overflowMsg =\n            \"Converting \" + fromValue + \" from '\" + fromUnit + \"' to '\" + toUnit\n                + \"' will result in an overflow of Long\";\n        throw new IllegalArgumentException(overflowMsg);\n      }\n      return (fromValue * numerator) / denominator;\n    }\n    if (numeratorMultiplierLimit > fromValue) {\n      return (numerator * fromValue) / denominator;\n    }\n    long tmp = numerator / denominator;\n    if ((Long.MAX_VALUE / tmp) < fromValue) {\n      String overflowMsg =\n          \"Converting \" + fromValue + \" from '\" + fromUnit + \"' to '\" + toUnit\n              + \"' will result in an overflow of Long\";\n      throw new IllegalArgumentException(overflowMsg);\n    }\n    return fromValue * tmp;\n  }",
                "code_after_change": "  public static long convert(String fromUnit, String toUnit, long fromValue) {\n    if (toUnit == null || fromUnit == null) {\n      throw new IllegalArgumentException(\"One or more arguments are null\");\n    }\n\n    if (fromUnit.equals(toUnit)) {\n      return fromValue;\n    }\n    Converter fc = getConverter(fromUnit);\n    Converter tc = getConverter(toUnit);\n    long numerator = fc.numerator * tc.denominator;\n    long denominator = fc.denominator * tc.numerator;\n    long numeratorMultiplierLimit = Long.MAX_VALUE / numerator;\n    if (numerator < denominator) {\n      if (numeratorMultiplierLimit < fromValue) {\n        String overflowMsg =\n            \"Converting \" + fromValue + \" from '\" + fromUnit + \"' to '\" + toUnit\n                + \"' will result in an overflow of Long\";\n        throw new IllegalArgumentException(overflowMsg);\n      }\n      return (fromValue * numerator) / denominator;\n    }\n    if (numeratorMultiplierLimit > fromValue) {\n      return (numerator * fromValue) / denominator;\n    }\n    long tmp = numerator / denominator;\n    if ((Long.MAX_VALUE / tmp) < fromValue) {\n      String overflowMsg =\n          \"Converting \" + fromValue + \" from '\" + fromUnit + \"' to '\" + toUnit\n              + \"' will result in an overflow of Long\";\n      throw new IllegalArgumentException(overflowMsg);\n    }\n    return fromValue * tmp;\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-api.src.main.java.org.apache.hadoop.yarn.util.UnitsConversionUtil.compare": {
                "code_before_change": "  public static int compare(String unitA, long valueA, String unitB,\n      long valueB) {\n    if (unitA == null || unitB == null || !KNOWN_UNITS.contains(unitA)\n        || !KNOWN_UNITS.contains(unitB)) {\n      throw new IllegalArgumentException(\"Units cannot be null\");\n    }\n    if (!KNOWN_UNITS.contains(unitA)) {\n      throw new IllegalArgumentException(\"Unknown unit '\" + unitA + \"'\");\n    }\n    if (!KNOWN_UNITS.contains(unitB)) {\n      throw new IllegalArgumentException(\"Unknown unit '\" + unitB + \"'\");\n    }\n    if (unitA.equals(unitB)) {\n      return Long.compare(valueA, valueB);\n    }\n    Converter unitAC = getConverter(unitA);\n    Converter unitBC = getConverter(unitB);\n    int unitAPos = SORTED_UNITS.indexOf(unitA);\n    int unitBPos = SORTED_UNITS.indexOf(unitB);\n    try {\n      long tmpA = valueA;\n      long tmpB = valueB;\n      if (unitAPos < unitBPos) {\n        tmpB = convert(unitB, unitA, valueB);\n      } else {\n        tmpA = convert(unitA, unitB, valueA);\n      }\n      return Long.compare(tmpA, tmpB);\n    } catch (IllegalArgumentException ie) {\n      BigInteger tmpA = BigInteger.valueOf(valueA);\n      BigInteger tmpB = BigInteger.valueOf(valueB);\n      if (unitAPos < unitBPos) {\n        tmpB = tmpB.multiply(BigInteger.valueOf(unitBC.numerator));\n        tmpB = tmpB.multiply(BigInteger.valueOf(unitAC.denominator));\n        tmpB = tmpB.divide(BigInteger.valueOf(unitBC.denominator));\n        tmpB = tmpB.divide(BigInteger.valueOf(unitAC.numerator));\n      } else {\n        tmpA = tmpA.multiply(BigInteger.valueOf(unitAC.numerator));\n        tmpA = tmpA.multiply(BigInteger.valueOf(unitBC.denominator));\n        tmpA = tmpA.divide(BigInteger.valueOf(unitAC.denominator));\n        tmpA = tmpA.divide(BigInteger.valueOf(unitBC.numerator));\n      }\n      return tmpA.compareTo(tmpB);\n    }\n  }",
                "code_after_change": "  public static int compare(String unitA, long valueA, String unitB,\n      long valueB) {\n    checkUnitArgument(unitA);\n    checkUnitArgument(unitB);\n    if (unitA.equals(unitB)) {\n      return Long.compare(valueA, valueB);\n    }\n    Converter unitAC = getConverter(unitA);\n    Converter unitBC = getConverter(unitB);\n    int unitAPos = SORTED_UNITS.indexOf(unitA);\n    int unitBPos = SORTED_UNITS.indexOf(unitB);\n    try {\n      long tmpA = valueA;\n      long tmpB = valueB;\n      if (unitAPos < unitBPos) {\n        tmpB = convert(unitB, unitA, valueB);\n      } else {\n        tmpA = convert(unitA, unitB, valueA);\n      }\n      return Long.compare(tmpA, tmpB);\n    } catch (IllegalArgumentException ie) {\n      BigInteger tmpA = BigInteger.valueOf(valueA);\n      BigInteger tmpB = BigInteger.valueOf(valueB);\n      if (unitAPos < unitBPos) {\n        tmpB = tmpB.multiply(BigInteger.valueOf(unitBC.numerator));\n        tmpB = tmpB.multiply(BigInteger.valueOf(unitAC.denominator));\n        tmpB = tmpB.divide(BigInteger.valueOf(unitBC.denominator));\n        tmpB = tmpB.divide(BigInteger.valueOf(unitAC.numerator));\n      } else {\n        tmpA = tmpA.multiply(BigInteger.valueOf(unitAC.numerator));\n        tmpA = tmpA.multiply(BigInteger.valueOf(unitBC.denominator));\n        tmpA = tmpA.divide(BigInteger.valueOf(unitAC.denominator));\n        tmpA = tmpA.divide(BigInteger.valueOf(unitBC.numerator));\n      }\n      return tmpA.compareTo(tmpB);\n    }\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-common.src.main.java.org.apache.hadoop.yarn.server.utils.BuilderUtils.newContainerId": {
                "code_before_change": "  public static ContainerId newContainerId(ApplicationAttemptId appAttemptId,\n      long containerId) {\n    return ContainerId.newContainerId(appAttemptId, containerId);\n  }",
                "code_after_change": "  public static ContainerId newContainerId(ApplicationAttemptId appAttemptId,\n      long containerId) {\n    return ContainerId.newContainerId(appAttemptId, containerId);\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-common.src.main.java.org.apache.hadoop.yarn.server.utils.BuilderUtils.newApplicationReport": {
                "code_before_change": "  public static ApplicationReport newApplicationReport(\n      ApplicationId applicationId, ApplicationAttemptId applicationAttemptId,\n      String user, String queue, String name, String host, int rpcPort,\n      Token clientToAMToken, YarnApplicationState state, String diagnostics,\n      String url, long startTime, long launchTime, long finishTime,\n      FinalApplicationStatus finalStatus,\n      ApplicationResourceUsageReport appResources, String origTrackingUrl,\n      float progress, String appType, Token amRmToken, Set<String> tags,\n      Priority priority) {\n    ApplicationReport report = recordFactory\n        .newRecordInstance(ApplicationReport.class);\n    report.setApplicationId(applicationId);\n    report.setCurrentApplicationAttemptId(applicationAttemptId);\n    report.setUser(user);\n    report.setQueue(queue);\n    report.setName(name);\n    report.setHost(host);\n    report.setRpcPort(rpcPort);\n    report.setClientToAMToken(clientToAMToken);\n    report.setYarnApplicationState(state);\n    report.setDiagnostics(diagnostics);\n    report.setTrackingUrl(url);\n    report.setStartTime(startTime);\n    report.setLaunchTime(launchTime);\n    report.setFinishTime(finishTime);\n    report.setFinalApplicationStatus(finalStatus);\n    report.setApplicationResourceUsageReport(appResources);\n    report.setOriginalTrackingUrl(origTrackingUrl);\n    report.setProgress(progress);\n    report.setApplicationType(appType);\n    report.setAMRMToken(amRmToken);\n    report.setApplicationTags(tags);\n    report.setPriority(priority);\n    return report;\n  }",
                "code_after_change": "  public static ApplicationReport newApplicationReport(\n      ApplicationId applicationId, ApplicationAttemptId applicationAttemptId,\n      String user, String queue, String name, String host, int rpcPort,\n      Token clientToAMToken, YarnApplicationState state, String diagnostics,\n      String url, long startTime, long finishTime,\n      FinalApplicationStatus finalStatus,\n      ApplicationResourceUsageReport appResources, String origTrackingUrl,\n      float progress, String appType, Token amRmToken, Set<String> tags,\n      Priority priority) {\n    ApplicationReport report = recordFactory\n        .newRecordInstance(ApplicationReport.class);\n    report.setApplicationId(applicationId);\n    report.setCurrentApplicationAttemptId(applicationAttemptId);\n    report.setUser(user);\n    report.setQueue(queue);\n    report.setName(name);\n    report.setHost(host);\n    report.setRpcPort(rpcPort);\n    report.setClientToAMToken(clientToAMToken);\n    report.setYarnApplicationState(state);\n    report.setDiagnostics(diagnostics);\n    report.setTrackingUrl(url);\n    report.setStartTime(startTime);\n    report.setFinishTime(finishTime);\n    report.setFinalApplicationStatus(finalStatus);\n    report.setApplicationResourceUsageReport(appResources);\n    report.setOriginalTrackingUrl(origTrackingUrl);\n    report.setProgress(progress);\n    report.setApplicationType(appType);\n    report.setAMRMToken(amRmToken);\n    report.setApplicationTags(tags);\n    report.setPriority(priority);\n    return report;\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-common.src.main.java.org.apache.hadoop.yarn.server.utils.BuilderUtils.newResource": {
                "code_before_change": "  public static Resource newResource(long memory, int vCores) {\n    Resource resource = recordFactory.newRecordInstance(Resource.class);\n    resource.setMemorySize(memory);\n    resource.setVirtualCores(vCores);\n    return resource;\n  }",
                "code_after_change": "  public static Resource newResource(long memory, int vCores) {\n    Resource resource = recordFactory.newRecordInstance(Resource.class);\n    resource.setMemorySize(memory);\n    resource.setVirtualCores(vCores);\n    return resource;\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Correct",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report precisely identifies the root cause as the `validateResourceRequest` method in the `SchedulerUtils` class, which is one of the ground truth methods. The fix suggestion in the report matches the developer's fix, which involves modifying the `validateResourceRequest` method to include checks for resource units. The problem location is also precisely identified as the `validateResourceRequest` method, which is part of the ground truth. There is no wrong information in the bug report as all the details align with the context of the bug and the actual fix."
        }
    },
    {
        "filename": "YARN-7118.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-common.src.main.java.org.apache.hadoop.yarn.server.webapp.WebServices.run": {
                "code_before_change": "          public Collection<ApplicationReport> run() throws Exception {\n            return appBaseProt.getApplications(request).getApplicationList();\n          }",
                "code_after_change": "              public Collection<ApplicationReport> run() throws Exception {\n                return getApplicationsReport(request);\n              }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Direct Caller/Callee"
            },
            "fix_suggestion": "Preventive",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Direct Caller/Callee"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the root cause as a NullPointerException due to uninitialized variables in the `getApps` method, which is a direct caller of the ground truth method `run`. The fix suggestion involves adding null checks, which is preventive but does not match the actual fix, which involved changing the method call in `run`. The problem location is identified as `getApps`, which is a direct caller of the ground truth method. There is no wrong information in the bug report as it accurately describes the issue and its context."
        }
    },
    {
        "filename": "YARN-4743.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.policies.FairSharePolicy.compare": {
                "code_before_change": "    public int compare(Schedulable s1, Schedulable s2) {\n      double minShareRatio1, minShareRatio2;\n      double useToWeightRatio1, useToWeightRatio2;\n      Resource minShare1 = Resources.min(RESOURCE_CALCULATOR, null,\n          s1.getMinShare(), s1.getDemand());\n      Resource minShare2 = Resources.min(RESOURCE_CALCULATOR, null,\n          s2.getMinShare(), s2.getDemand());\n      boolean s1Needy = Resources.lessThan(RESOURCE_CALCULATOR, null,\n          s1.getResourceUsage(), minShare1);\n      boolean s2Needy = Resources.lessThan(RESOURCE_CALCULATOR, null,\n          s2.getResourceUsage(), minShare2);\n      minShareRatio1 = (double) s1.getResourceUsage().getMemory()\n          / Resources.max(RESOURCE_CALCULATOR, null, minShare1, ONE).getMemory();\n      minShareRatio2 = (double) s2.getResourceUsage().getMemory()\n          / Resources.max(RESOURCE_CALCULATOR, null, minShare2, ONE).getMemory();\n      useToWeightRatio1 = s1.getResourceUsage().getMemory() /\n          s1.getWeights().getWeight(ResourceType.MEMORY);\n      useToWeightRatio2 = s2.getResourceUsage().getMemory() /\n          s2.getWeights().getWeight(ResourceType.MEMORY);\n      int res = 0;\n      if (s1Needy && !s2Needy)\n        res = -1;\n      else if (s2Needy && !s1Needy)\n        res = 1;\n      else if (s1Needy && s2Needy)\n        res = (int) Math.signum(minShareRatio1 - minShareRatio2);\n      else\n        // Neither schedulable is needy\n        res = (int) Math.signum(useToWeightRatio1 - useToWeightRatio2);\n      if (res == 0) {\n        // Apps are tied in fairness ratio. Break the tie by submit time and job\n        // name to get a deterministic ordering, which is useful for unit tests.\n        res = (int) Math.signum(s1.getStartTime() - s2.getStartTime());\n        if (res == 0)\n          res = s1.getName().compareTo(s2.getName());\n      }\n      return res;\n    }",
                "code_after_change": "    public int compare(Schedulable s1, Schedulable s2) {\n      double minShareRatio1, minShareRatio2;\n      double useToWeightRatio1, useToWeightRatio2;\n      double weight1, weight2;\n      Resource minShare1 = Resources.min(RESOURCE_CALCULATOR, null,\n          s1.getMinShare(), s1.getDemand());\n      Resource minShare2 = Resources.min(RESOURCE_CALCULATOR, null,\n          s2.getMinShare(), s2.getDemand());\n      boolean s1Needy = Resources.lessThan(RESOURCE_CALCULATOR, null,\n          s1.getResourceUsage(), minShare1);\n      boolean s2Needy = Resources.lessThan(RESOURCE_CALCULATOR, null,\n          s2.getResourceUsage(), minShare2);\n      minShareRatio1 = (double) s1.getResourceUsage().getMemorySize()\n          / Resources.max(RESOURCE_CALCULATOR, null, minShare1, ONE).getMemorySize();\n      minShareRatio2 = (double) s2.getResourceUsage().getMemorySize()\n          / Resources.max(RESOURCE_CALCULATOR, null, minShare2, ONE).getMemorySize();\n\n      weight1 = s1.getWeights().getWeight(ResourceType.MEMORY);\n      weight2 = s2.getWeights().getWeight(ResourceType.MEMORY);\n      if (weight1 > 0.0 && weight2 > 0.0) {\n        useToWeightRatio1 = s1.getResourceUsage().getMemorySize() / weight1;\n        useToWeightRatio2 = s2.getResourceUsage().getMemorySize() / weight2;\n      } else { // Either weight1 or weight2 equals to 0\n        if (weight1 == weight2) {\n          // If they have same weight, just compare usage\n          useToWeightRatio1 = s1.getResourceUsage().getMemorySize();\n          useToWeightRatio2 = s2.getResourceUsage().getMemorySize();\n        } else {\n          // By setting useToWeightRatios to negative weights, we give the\n          // zero-weight one less priority, so the non-zero weight one will\n          // be given slots.\n          useToWeightRatio1 = -weight1;\n          useToWeightRatio2 = -weight2;\n        }\n      }\n\n      int res = 0;\n      if (s1Needy && !s2Needy)\n        res = -1;\n      else if (s2Needy && !s1Needy)\n        res = 1;\n      else if (s1Needy && s2Needy)\n        res = (int) Math.signum(minShareRatio1 - minShareRatio2);\n      else\n        // Neither schedulable is needy\n        res = (int) Math.signum(useToWeightRatio1 - useToWeightRatio2);\n      if (res == 0) {\n        // Apps are tied in fairness ratio. Break the tie by submit time and job\n        // name to get a deterministic ordering, which is useful for unit tests.\n        res = (int) Math.signum(s1.getStartTime() - s2.getStartTime());\n        if (res == 0)\n          res = s1.getName().compareTo(s2.getName());\n      }\n      return res;\n    }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.policies.FairSharePolicy.getName": {
                "code_before_change": "  public String getName() {\n    return NAME;\n  }",
                "code_after_change": "  public String getName() {\n    return NAME;\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Correct",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Same Class or Module"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report precisely identifies the root cause as a comparator in the FairSharePolicy that does not adhere to the required contract, which is directly related to the ground truth method FairSharePolicy.compare. The fix suggestion is correct as it matches the developer's fix, which involves modifying the comparator to handle cases where memorySize and weight are both zero. The problem location identification is partial because the report mentions methods like FSLeafQueue.assignContainer and FairScheduler.nodeUpdate, which are in the same module but not the exact ground truth methods. There is no wrong information in the bug report as all details are relevant and accurate."
        }
    },
    {
        "filename": "YARN-2414.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.webapp.AppBlock.render": {
                "code_before_change": "  protected void render(Block html) {\n    String aid = $(APPLICATION_ID);\n    if (aid.isEmpty()) {\n      puts(\"Bad request: requires application ID\");\n      return;\n    }\n\n    ApplicationId appID = null;\n    try {\n      appID = Apps.toAppID(aid);\n    } catch (Exception e) {\n      puts(\"Invalid Application ID: \" + aid);\n      return;\n    }\n\n    RMContext context = getInstance(RMContext.class);\n    RMApp rmApp = context.getRMApps().get(appID);\n    if (rmApp == null) {\n      puts(\"Application not found: \"+ aid);\n      return;\n    }\n    AppInfo app = new AppInfo(rmApp, true, WebAppUtils.getHttpSchemePrefix(conf));\n\n    // Check for the authorization.\n    String remoteUser = request().getRemoteUser();\n    UserGroupInformation callerUGI = null;\n    if (remoteUser != null) {\n      callerUGI = UserGroupInformation.createRemoteUser(remoteUser);\n    }\n    if (callerUGI != null\n        && !(this.aclsManager.checkAccess(callerUGI,\n                ApplicationAccessType.VIEW_APP, app.getUser(), appID) ||\n             this.queueACLsManager.checkAccess(callerUGI,\n                QueueACL.ADMINISTER_QUEUE, app.getQueue()))) {\n      puts(\"You (User \" + remoteUser\n          + \") are not authorized to view application \" + appID);\n      return;\n    }\n\n    setTitle(join(\"Application \", aid));\n\n    RMAppMetrics appMerics = rmApp.getRMAppMetrics();\n    RMAppAttemptMetrics attemptMetrics =\n        rmApp.getCurrentAppAttempt().getRMAppAttemptMetrics();\n    info(\"Application Overview\")\n        ._(\"User:\", app.getUser())\n        ._(\"Name:\", app.getName())\n        ._(\"Application Type:\", app.getApplicationType())\n        ._(\"Application Tags:\", app.getApplicationTags())\n        ._(\"State:\", app.getState())\n        ._(\"FinalStatus:\", app.getFinalStatus())\n        ._(\"Started:\", Times.format(app.getStartTime()))\n        ._(\"Elapsed:\",\n            StringUtils.formatTime(Times.elapsed(app.getStartTime(),\n                app.getFinishTime())))\n        ._(\"Tracking URL:\",\n            !app.isTrackingUrlReady() ? \"#\" : app.getTrackingUrlPretty(),\n            app.getTrackingUI())\n        ._(\"Diagnostics:\", app.getNote());\n\n    DIV<Hamlet> pdiv = html.\n        _(InfoBlock.class).\n        div(_INFO_WRAP);\n    info(\"Application Overview\").clear();\n    info(\"Application Metrics\")\n        ._(\"Total Resource Preempted:\",\n          appMerics.getResourcePreempted())\n        ._(\"Total Number of Non-AM Containers Preempted:\",\n          String.valueOf(appMerics.getNumNonAMContainersPreempted()))\n        ._(\"Total Number of AM Containers Preempted:\",\n          String.valueOf(appMerics.getNumAMContainersPreempted()))\n        ._(\"Resource Preempted from Current Attempt:\",\n          attemptMetrics.getResourcePreempted())\n        ._(\"Number of Non-AM Containers Preempted from Current Attempt:\",\n          String.valueOf(attemptMetrics\n            .getNumNonAMContainersPreempted()));\n    pdiv._();\n\n    Collection<RMAppAttempt> attempts = rmApp.getAppAttempts().values();\n    String amString =\n        attempts.size() == 1 ? \"ApplicationMaster\" : \"ApplicationMasters\";\n\n    DIV<Hamlet> div = html.\n        _(InfoBlock.class).\n        div(_INFO_WRAP);\n    // MRAppMasters Table\n    TABLE<DIV<Hamlet>> table = div.table(\"#app\");\n    table.\n      tr().\n        th(amString).\n      _().\n      tr().\n        th(_TH, \"Attempt Number\").\n        th(_TH, \"Start Time\").\n        th(_TH, \"Node\").\n        th(_TH, \"Logs\").\n      _();\n\n    boolean odd = false;\n    for (RMAppAttempt attempt : attempts) {\n      AppAttemptInfo attemptInfo = new AppAttemptInfo(attempt, app.getUser());\n      table.tr((odd = !odd) ? _ODD : _EVEN).\n        td(String.valueOf(attemptInfo.getAttemptId())).\n        td(Times.format(attemptInfo.getStartTime())).\n        td().a(\".nodelink\", url(\"//\",\n            attemptInfo.getNodeHttpAddress()),\n            attemptInfo.getNodeHttpAddress())._().\n        td().a(\".logslink\", url(attemptInfo.getLogsLink()), \"logs\")._().\n      _();\n    }\n\n    table._();\n    div._();\n  }",
                "code_after_change": "  protected void render(Block html) {\n    String aid = $(APPLICATION_ID);\n    if (aid.isEmpty()) {\n      puts(\"Bad request: requires application ID\");\n      return;\n    }\n\n    ApplicationId appID = null;\n    try {\n      appID = Apps.toAppID(aid);\n    } catch (Exception e) {\n      puts(\"Invalid Application ID: \" + aid);\n      return;\n    }\n\n    RMContext context = getInstance(RMContext.class);\n    RMApp rmApp = context.getRMApps().get(appID);\n    if (rmApp == null) {\n      puts(\"Application not found: \"+ aid);\n      return;\n    }\n    AppInfo app = new AppInfo(rmApp, true, WebAppUtils.getHttpSchemePrefix(conf));\n\n    // Check for the authorization.\n    String remoteUser = request().getRemoteUser();\n    UserGroupInformation callerUGI = null;\n    if (remoteUser != null) {\n      callerUGI = UserGroupInformation.createRemoteUser(remoteUser);\n    }\n    if (callerUGI != null\n        && !(this.aclsManager.checkAccess(callerUGI,\n                ApplicationAccessType.VIEW_APP, app.getUser(), appID) ||\n             this.queueACLsManager.checkAccess(callerUGI,\n                QueueACL.ADMINISTER_QUEUE, app.getQueue()))) {\n      puts(\"You (User \" + remoteUser\n          + \") are not authorized to view application \" + appID);\n      return;\n    }\n\n    setTitle(join(\"Application \", aid));\n\n    RMAppMetrics appMerics = rmApp.getRMAppMetrics();\n    \n    // Get attempt metrics and fields, it is possible currentAttempt of RMApp is\n    // null. In that case, we will assume resource preempted and number of Non\n    // AM container preempted on that attempt is 0\n    RMAppAttemptMetrics attemptMetrics;\n    if (null == rmApp.getCurrentAppAttempt()) {\n      attemptMetrics = null;\n    } else {\n      attemptMetrics = rmApp.getCurrentAppAttempt().getRMAppAttemptMetrics();\n    }\n    Resource attemptResourcePreempted =\n        attemptMetrics == null ? Resources.none() : attemptMetrics\n            .getResourcePreempted();\n    int attemptNumNonAMContainerPreempted =\n        attemptMetrics == null ? 0 : attemptMetrics\n            .getNumNonAMContainersPreempted();\n    \n    info(\"Application Overview\")\n        ._(\"User:\", app.getUser())\n        ._(\"Name:\", app.getName())\n        ._(\"Application Type:\", app.getApplicationType())\n        ._(\"Application Tags:\", app.getApplicationTags())\n        ._(\"State:\", app.getState())\n        ._(\"FinalStatus:\", app.getFinalStatus())\n        ._(\"Started:\", Times.format(app.getStartTime()))\n        ._(\"Elapsed:\",\n            StringUtils.formatTime(Times.elapsed(app.getStartTime(),\n                app.getFinishTime())))\n        ._(\"Tracking URL:\",\n            !app.isTrackingUrlReady() ? \"#\" : app.getTrackingUrlPretty(),\n            app.getTrackingUI())\n        ._(\"Diagnostics:\", app.getNote());\n\n    DIV<Hamlet> pdiv = html.\n        _(InfoBlock.class).\n        div(_INFO_WRAP);\n    info(\"Application Overview\").clear();\n    info(\"Application Metrics\")\n        ._(\"Total Resource Preempted:\",\n          appMerics.getResourcePreempted())\n        ._(\"Total Number of Non-AM Containers Preempted:\",\n          String.valueOf(appMerics.getNumNonAMContainersPreempted()))\n        ._(\"Total Number of AM Containers Preempted:\",\n          String.valueOf(appMerics.getNumAMContainersPreempted()))\n        ._(\"Resource Preempted from Current Attempt:\",\n          attemptResourcePreempted)\n        ._(\"Number of Non-AM Containers Preempted from Current Attempt:\",\n          attemptNumNonAMContainerPreempted)\n        ._(\"Aggregate Resource Allocation:\",\n          String.format(\"%d MB-seconds, %d vcore-seconds\", \n              appMerics.getMemorySeconds(), appMerics.getVcoreSeconds()));\n    pdiv._();\n\n    Collection<RMAppAttempt> attempts = rmApp.getAppAttempts().values();\n    String amString =\n        attempts.size() == 1 ? \"ApplicationMaster\" : \"ApplicationMasters\";\n\n    DIV<Hamlet> div = html.\n        _(InfoBlock.class).\n        div(_INFO_WRAP);\n    // MRAppMasters Table\n    TABLE<DIV<Hamlet>> table = div.table(\"#app\");\n    table.\n      tr().\n        th(amString).\n      _().\n      tr().\n        th(_TH, \"Attempt Number\").\n        th(_TH, \"Start Time\").\n        th(_TH, \"Node\").\n        th(_TH, \"Logs\").\n      _();\n\n    boolean odd = false;\n    for (RMAppAttempt attempt : attempts) {\n      AppAttemptInfo attemptInfo = new AppAttemptInfo(attempt, app.getUser());\n      table.tr((odd = !odd) ? _ODD : _EVEN).\n        td(String.valueOf(attemptInfo.getAttemptId())).\n        td(Times.format(attemptInfo.getStartTime())).\n        td().a(\".nodelink\", url(\"//\",\n            attemptInfo.getNodeHttpAddress()),\n            attemptInfo.getNodeHttpAddress())._().\n        td().a(\".logslink\", url(attemptInfo.getLogsLink()), \"logs\")._().\n      _();\n    }\n\n    table._();\n    div._();\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.security.ApplicationACLsManager.ApplicationACLsManager": {
                "code_before_change": [],
                "code_after_change": []
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.security.QueueACLsManager.QueueACLsManager": {
                "code_before_change": [],
                "code_after_change": []
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Correct",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report precisely identifies the root cause as a NullPointerException in the AppBlock.render method, which matches the ground truth method. The fix suggestion in the report, which involves adding null checks in the AppBlock.render method, aligns with the developer's actual fix, making it a correct suggestion. The problem location is also precisely identified as the AppBlock.render method, which is part of the ground truth methods. There is no wrong information in the bug report as all details are consistent with the context of the bug."
        }
    },
    {
        "filename": "YARN-3878.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.event.AsyncDispatcher.handle": {
                "code_before_change": "    public void handle(Event event) {\n      if (blockNewEvents) {\n        return;\n      }\n      drained = false;\n\n      /* all this method does is enqueue all the events onto the queue */\n      int qSize = eventQueue.size();\n      if (qSize !=0 && qSize %1000 == 0) {\n        LOG.info(\"Size of event-queue is \" + qSize);\n      }\n      int remCapacity = eventQueue.remainingCapacity();\n      if (remCapacity < 1000) {\n        LOG.warn(\"Very low remaining capacity in the event-queue: \"\n            + remCapacity);\n      }\n      try {\n        eventQueue.put(event);\n      } catch (InterruptedException e) {\n        if (!stopped) {\n          LOG.warn(\"AsyncDispatcher thread interrupted\", e);\n        }\n        throw new YarnRuntimeException(e);\n      }\n    };",
                "code_after_change": "    public void handle(Event event) {\n      if (blockNewEvents) {\n        return;\n      }\n      drained = false;\n\n      /* all this method does is enqueue all the events onto the queue */\n      int qSize = eventQueue.size();\n      if (qSize !=0 && qSize %1000 == 0) {\n        LOG.info(\"Size of event-queue is \" + qSize);\n      }\n      int remCapacity = eventQueue.remainingCapacity();\n      if (remCapacity < 1000) {\n        LOG.warn(\"Very low remaining capacity in the event-queue: \"\n            + remCapacity);\n      }\n      try {\n        eventQueue.put(event);\n      } catch (InterruptedException e) {\n        if (!stopped) {\n          LOG.warn(\"AsyncDispatcher thread interrupted\", e);\n        }\n        // Need to reset drained flag to true if event queue is empty,\n        // otherwise dispatcher will hang on stop.\n        drained = eventQueue.isEmpty();\n        throw new YarnRuntimeException(e);\n      }\n    };"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.event.AsyncDispatcher.run": {
                "code_before_change": "      public void run() {\n        while (!stopped && !Thread.currentThread().isInterrupted()) {\n          drained = eventQueue.isEmpty();\n          // blockNewEvents is only set when dispatcher is draining to stop,\n          // adding this check is to avoid the overhead of acquiring the lock\n          // and calling notify every time in the normal run of the loop.\n          if (blockNewEvents) {\n            synchronized (waitForDrained) {\n              if (drained) {\n                waitForDrained.notify();\n              }\n            }\n          }\n          Event event;\n          try {\n            event = eventQueue.take();\n          } catch(InterruptedException ie) {\n            if (!stopped) {\n              LOG.warn(\"AsyncDispatcher thread interrupted\", ie);\n            }\n            return;\n          }\n          if (event != null) {\n            dispatch(event);\n          }\n        }\n      }",
                "code_after_change": "      public void run() {\n        while (!stopped && !Thread.currentThread().isInterrupted()) {\n          drained = eventQueue.isEmpty();\n          // blockNewEvents is only set when dispatcher is draining to stop,\n          // adding this check is to avoid the overhead of acquiring the lock\n          // and calling notify every time in the normal run of the loop.\n          if (blockNewEvents) {\n            synchronized (waitForDrained) {\n              if (drained) {\n                waitForDrained.notify();\n              }\n            }\n          }\n          Event event;\n          try {\n            event = eventQueue.take();\n          } catch(InterruptedException ie) {\n            if (!stopped) {\n              LOG.warn(\"AsyncDispatcher thread interrupted\", ie);\n            }\n            return;\n          }\n          if (event != null) {\n            dispatch(event);\n          }\n        }\n      }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Correct",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report accurately identifies the root cause as the interruption of the event handling thread during the shutdown process of the AsyncDispatcher, which aligns with the changes made in the 'handle' method of the AsyncDispatcher. The fix suggestion in the report matches the developer's fix, which involves handling the InterruptedException more effectively and ensuring the drained condition can be met. The problem location is precisely identified as it mentions the 'AsyncDispatcher.serviceStop' and 'AsyncDispatcher.handle' methods, which are part of the ground truth methods. There is no wrong information in the bug report as all details are relevant and correctly describe the issue."
        }
    },
    {
        "filename": "YARN-6683.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.AddApplicationToSchedulerTransition": {
                "code_before_change": [],
                "code_after_change": []
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.RMAppRecoveredTransition": {
                "code_before_change": [],
                "code_after_change": []
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.transition": {
                "code_before_change": "    public void transition(RMAppImpl app, RMAppEvent event) {\n    };",
                "code_after_change": "    public void transition(RMAppImpl app, RMAppEvent event) {\n    };"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.StartAppAttemptTransition": {
                "code_before_change": [],
                "code_after_change": []
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.ResourceTrackerService.updateAppCollectorsMap": {
                "code_before_change": "  private void updateAppCollectorsMap(NodeHeartbeatRequest request) {\n    Map<ApplicationId, String> registeredCollectorsMap =\n        request.getRegisteredCollectors();\n    if (registeredCollectorsMap != null\n        && !registeredCollectorsMap.isEmpty()) {\n      Map<ApplicationId, RMApp> rmApps = rmContext.getRMApps();\n      for (Map.Entry<ApplicationId, String> entry:\n          registeredCollectorsMap.entrySet()) {\n        ApplicationId appId = entry.getKey();\n        String collectorAddr = entry.getValue();\n        if (collectorAddr != null && !collectorAddr.isEmpty()) {\n          RMApp rmApp = rmApps.get(appId);\n          if (rmApp == null) {\n            LOG.warn(\"Cannot update collector info because application ID: \" +\n                appId + \" is not found in RMContext!\");\n          } else {\n            String previousCollectorAddr = rmApp.getCollectorAddr();\n            if (previousCollectorAddr == null\n                || !previousCollectorAddr.equals(collectorAddr)) {\n              // sending collector update event.\n              RMAppCollectorUpdateEvent event =\n                  new RMAppCollectorUpdateEvent(appId, collectorAddr);\n              rmContext.getDispatcher().getEventHandler().handle(event);\n            }\n          }\n        }\n      }\n    }\n  }",
                "code_after_change": "  private void updateAppCollectorsMap(NodeHeartbeatRequest request) {\n    Map<ApplicationId, String> registeredCollectorsMap =\n        request.getRegisteredCollectors();\n    if (registeredCollectorsMap != null\n        && !registeredCollectorsMap.isEmpty()) {\n      Map<ApplicationId, RMApp> rmApps = rmContext.getRMApps();\n      for (Map.Entry<ApplicationId, String> entry:\n          registeredCollectorsMap.entrySet()) {\n        ApplicationId appId = entry.getKey();\n        String collectorAddr = entry.getValue();\n        if (collectorAddr != null && !collectorAddr.isEmpty()) {\n          RMApp rmApp = rmApps.get(appId);\n          if (rmApp == null) {\n            LOG.warn(\"Cannot update collector info because application ID: \" +\n                appId + \" is not found in RMContext!\");\n          } else {\n            String previousCollectorAddr = rmApp.getCollectorAddr();\n            if (previousCollectorAddr == null\n                || !previousCollectorAddr.equals(collectorAddr)) {\n              rmApp.setCollectorAddr(collectorAddr);\n            }\n          }\n        }\n      }\n    }\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.KillAttemptTransition": {
                "code_before_change": [],
                "code_after_change": []
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Buggy Method"
            },
            "fix_suggestion": "Correct",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Buggy Method"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the 'RMAppImpl.handle' method as the location of the error, which is where the error occurs but not where the fix was made, thus it is classified as 'Partial' under 'Buggy Method' for both root cause and problem location identification. The fix suggestion in the report matches the developer's fix, which involves directly updating the collector address, so it is 'Correct'. There is no wrong information in the bug report as all details are relevant to the context of the bug."
        }
    },
    {
        "filename": "YARN-2910.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue.updateDemandForApp": {
                "code_before_change": [],
                "code_after_change": []
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue.updateDemand": {
                "code_before_change": "  public void updateDemand() {\n    // Compute demand by iterating through apps in the queue\n    // Limit demand to maxResources\n    Resource maxRes = queueMgr.getMaxResources(getName());\n    demand = Resources.createResource(0);\n    for (AppSchedulable sched : appScheds) {\n      sched.updateDemand();\n      Resource toAdd = sched.getDemand();\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Counting resource from \" + sched.getName() + \" \" + toAdd\n            + \"; Total resource consumption for \" + getName() + \" now \"\n            + demand);\n      }\n      demand = Resources.add(demand, toAdd);\n      demand = Resources.componentwiseMin(demand, maxRes);\n      if (Resources.equals(demand, maxRes)) {\n        break;\n      }\n    }\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"The updated demand for \" + getName() + \" is \" + demand\n          + \"; the max is \" + maxRes);\n    }\n  }",
                "code_after_change": "  public void updateDemand() {\n    // Compute demand by iterating through apps in the queue\n    // Limit demand to maxResources\n    Resource maxRes = queueMgr.getMaxResources(getName());\n    demand = Resources.createResource(0);\n    for (AppSchedulable sched : appScheds) {\n      sched.updateDemand();\n      Resource toAdd = sched.getDemand();\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Counting resource from \" + sched.getName() + \" \" + toAdd\n            + \"; Total resource consumption for \" + getName() + \" now \"\n            + demand);\n      }\n      demand = Resources.add(demand, toAdd);\n      demand = Resources.componentwiseMin(demand, maxRes);\n      if (Resources.equals(demand, maxRes)) {\n        break;\n      }\n    }\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"The updated demand for \" + getName() + \" is \" + demand\n          + \"; the max is \" + maxRes);\n    }\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt.unreserveInternal": {
                "code_before_change": [],
                "code_after_change": []
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue.preemptContainer": {
                "code_before_change": [],
                "code_after_change": []
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue.FSLeafQueue": {
                "code_before_change": [],
                "code_after_change": []
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue.assignContainer": {
                "code_before_change": "  public Resource assignContainer(FSSchedulerNode node) {\n    Resource assigned = Resources.none();\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Node offered to queue: \" + getName());\n    }\n\n    if (!assignContainerPreCheck(node)) {\n      return assigned;\n    }\n\n    Comparator<Schedulable> comparator = policy.getComparator();\n    Collections.sort(appScheds, comparator);\n    for (AppSchedulable sched : appScheds) {\n      if (sched.getRunnable()) {\n        assigned = sched.assignContainer(node);\n        if (!assigned.equals(Resources.none())) {\n          break;\n        }\n      }\n    }\n    return assigned;\n  }",
                "code_after_change": "  public Resource assignContainer(FSSchedulerNode node) {\n    Resource assigned = Resources.none();\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Node offered to queue: \" + getName());\n    }\n\n    if (!assignContainerPreCheck(node)) {\n      return assigned;\n    }\n\n    Comparator<Schedulable> comparator = policy.getComparator();\n    Collections.sort(appScheds, comparator);\n    for (AppSchedulable sched : appScheds) {\n      if (sched.getRunnable()) {\n        assigned = sched.assignContainer(node);\n        if (!assigned.equals(Resources.none())) {\n          break;\n        }\n      }\n    }\n    return assigned;\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue.getDemand": {
                "code_before_change": "  public Resource getDemand() {\n    return demand;\n  }",
                "code_after_change": "  public Resource getDemand() {\n    return demand;\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue.addAppSchedulable": {
                "code_before_change": "  void addAppSchedulable(AppSchedulable appSched) {\n    appScheds.add(appSched);\n  }",
                "code_after_change": "  void addAppSchedulable(AppSchedulable appSched) {\n    appScheds.add(appSched);\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSLeafQueue.getNonRunnableAppSchedulables": {
                "code_before_change": [],
                "code_after_change": []
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Same Class or Module"
            },
            "fix_suggestion": "Alternative Fix",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Same Class or Module"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the root cause as a ConcurrentModificationException in the getResourceUsage method of the FSLeafQueue class, which is not a ground truth method but is in the same class as several ground truth methods. The fix suggestion to use CopyOnWriteArrayList is an alternative fix that would likely resolve the issue by preventing concurrent modifications, although it does not match the developer's fix. The problem location is identified as the getResourceUsage and updateRunnability methods, which are not ground truth methods but are in the same class as the ground truth methods. There is no wrong information in the bug report as all details are relevant to the context of the bug."
        }
    },
    {
        "filename": "YARN-192.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AppSchedulable.assignContainer": {
                "code_before_change": "  private Resource assignContainer(FSSchedulerNode node,\n      FSSchedulerApp application, Priority priority,\n      ResourceRequest request, NodeType type, boolean reserved) {\n\n    // How much does this request need?\n    Resource capability = request.getCapability();\n\n    // How much does the node have?\n    Resource available = node.getAvailableResource();\n\n    Container container = null;\n    if (reserved) {\n      container = node.getReservedContainer().getContainer();\n    } else {\n      container = createContainer(application, node, capability, priority);\n    }\n\n    // Can we allocate a container on this node?\n    int availableContainers =\n        available.getMemory() / capability.getMemory();\n\n    if (availableContainers > 0) {\n      // Inform the application of the new container for this request\n      RMContainer allocatedContainer =\n          application.allocate(type, node, priority, request, container);\n      if (allocatedContainer == null) {\n        // Did the application need this resource?\n        return Resources.none();\n      }\n      else {\n        // TODO this should subtract resource just assigned\n        // TEMPROARY\n        getMetrics().setAvailableResourcesToQueue(\n            this.scheduler.getClusterCapacity());\n      }\n\n\n      // If we had previously made a reservation, delete it\n      if (reserved) {\n        this.unreserve(application, priority, node);\n      }\n\n      // Inform the node\n      node.allocateContainer(application.getApplicationId(),\n          allocatedContainer);\n\n      return container.getResource();\n    } else {\n      // The desired container won't fit here, so reserve\n      reserve(application, priority, node, container, reserved);\n\n      return Resources.none();\n    }\n  }",
                "code_after_change": "  private Resource assignContainer(FSSchedulerNode node,\n      FSSchedulerApp application, Priority priority,\n      ResourceRequest request, NodeType type, boolean reserved) {\n\n    // How much does this request need?\n    Resource capability = request.getCapability();\n\n    // How much does the node have?\n    Resource available = node.getAvailableResource();\n\n    Container container = null;\n    if (reserved) {\n      container = node.getReservedContainer().getContainer();\n    } else {\n      container = createContainer(application, node, capability, priority);\n    }\n\n    // Can we allocate a container on this node?\n    int availableContainers =\n        available.getMemory() / capability.getMemory();\n\n    if (availableContainers > 0) {\n      // Inform the application of the new container for this request\n      RMContainer allocatedContainer =\n          application.allocate(type, node, priority, request, container);\n      if (allocatedContainer == null) {\n        // Did the application need this resource?\n        return Resources.none();\n      }\n      else {\n        // TODO this should subtract resource just assigned\n        // TEMPROARY\n        getMetrics().setAvailableResourcesToQueue(\n            scheduler.getClusterCapacity());\n      }\n\n\n      // If we had previously made a reservation, delete it\n      if (reserved) {\n        unreserve(application, priority, node);\n      }\n\n      // Inform the node\n      node.allocateContainer(application.getApplicationId(),\n          allocatedContainer);\n\n      return container.getResource();\n    } else {\n      // The desired container won't fit here, so reserve\n      reserve(application, priority, node, container, reserved);\n\n      return Resources.none();\n    }\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Direct Caller/Callee"
            },
            "fix_suggestion": "Preventive",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Direct Caller/Callee"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "1. **Root Cause Identification**: The bug report identifies the `unreserve` method in `FSSchedulerApp` as the location of the NPE, which is a direct callee of the `assignContainer` method in `AppSchedulable`, where the actual fix was made. Thus, it is classified as 'Partial' with 'Direct Caller/Callee' as the sub-category.\n\n2. **Fix Suggestion**: The suggestion to add null checks in the `unreserve` method is preventive, as it would prevent the NPE from occurring, even though it does not match the exact developer's fix.\n\n3. **Problem Location Identification**: The problem location is identified as `FSSchedulerApp.unreserve`, which is a direct callee of the ground truth method `AppSchedulable.assignContainer`. Therefore, it is classified as 'Partial' with 'Direct Caller/Callee' as the sub-category.\n\n4. **Wrong Information**: The bug report does not contain any incorrect or unrelated information. All details are relevant to the context of the bug."
        }
    },
    {
        "filename": "YARN-4581.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-applicationhistoryservice.src.main.java.org.apache.hadoop.yarn.server.applicationhistoryservice.FileSystemApplicationHistoryStore.HistoryFileWriter": {
                "code_before_change": [],
                "code_after_change": []
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Buggy Method"
            },
            "fix_suggestion": "Preventive",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Same Class or Module"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the root cause as an IOException due to improper file initialization or failure to close previous write operations, and an OutOfMemoryError due to thread creation limits. This aligns with the issues in the 'HistoryFileWriter' method, but it does not precisely identify the ground truth method, hence 'Partial' with 'Buggy Method' as it points to the method where the error occurred. The fix suggestion is 'Preventive' as it suggests ensuring proper file initialization and closure, which would mitigate the bug. The problem location identification is 'Partial' with 'Same Class or Module' because it mentions 'FileSystemApplicationHistoryStore' class, which contains the ground truth method. There is no wrong information in the bug report as all details are relevant to the context of the bug."
        }
    },
    {
        "filename": "YARN-7786.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.run": {
                "code_before_change": "  public void run() {\n    switch (eventType) {\n    case LAUNCH:\n      try {\n        LOG.info(\"Launching master\" + application.getAppAttemptId());\n        launch();\n        handler.handle(new RMAppAttemptEvent(application.getAppAttemptId(),\n            RMAppAttemptEventType.LAUNCHED));\n      } catch(Exception ie) {\n        String message = \"Error launching \" + application.getAppAttemptId()\n            + \". Got exception: \" + StringUtils.stringifyException(ie);\n        LOG.info(message);\n        handler.handle(new RMAppAttemptEvent(application\n            .getAppAttemptId(), RMAppAttemptEventType.LAUNCH_FAILED, message));\n      }\n      break;\n    case CLEANUP:\n      try {\n        LOG.info(\"Cleaning master \" + application.getAppAttemptId());\n        cleanup();\n      } catch(IOException ie) {\n        LOG.info(\"Error cleaning master \", ie);\n      } catch (YarnException e) {\n        StringBuilder sb = new StringBuilder(\"Container \");\n        sb.append(masterContainer.getId().toString());\n        sb.append(\" is not handled by this NodeManager\");\n        if (!e.getMessage().contains(sb.toString())) {\n          // Ignoring if container is already killed by Node Manager.\n          LOG.info(\"Error cleaning master \", e);\n        }\n      }\n      break;\n    default:\n      LOG.warn(\"Received unknown event-type \" + eventType + \". Ignoring.\");\n      break;\n    }\n  }",
                "code_after_change": "  public void run() {\n    switch (eventType) {\n    case LAUNCH:\n      try {\n        LOG.info(\"Launching master\" + application.getAppAttemptId());\n        launch();\n        handler.handle(new RMAppAttemptEvent(application.getAppAttemptId(),\n            RMAppAttemptEventType.LAUNCHED));\n      } catch(Exception ie) {\n        onAMLaunchFailed(masterContainer.getId(), ie);\n      }\n      break;\n    case CLEANUP:\n      try {\n        LOG.info(\"Cleaning master \" + application.getAppAttemptId());\n        cleanup();\n      } catch(IOException ie) {\n        LOG.info(\"Error cleaning master \", ie);\n      } catch (YarnException e) {\n        StringBuilder sb = new StringBuilder(\"Container \");\n        sb.append(masterContainer.getId().toString());\n        sb.append(\" is not handled by this NodeManager\");\n        if (!e.getMessage().contains(sb.toString())) {\n          // Ignoring if container is already killed by Node Manager.\n          LOG.info(\"Error cleaning master \", e);          \n        }\n      }\n      break;\n    default:\n      LOG.warn(\"Received unknown event-type \" + eventType + \". Ignoring.\");\n      break;\n    }\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.createAMContainerLaunchContext": {
                "code_before_change": "  private ContainerLaunchContext createAMContainerLaunchContext(\n      ApplicationSubmissionContext applicationMasterContext,\n      ContainerId containerID) throws IOException {\n\n    // Construct the actual Container\n    ContainerLaunchContext container =\n        applicationMasterContext.getAMContainerSpec();\n\n    // Finalize the container\n    setupTokens(container, containerID);\n    // set the flow context optionally for timeline service v.2\n    setFlowContext(container);\n\n    return container;\n  }",
                "code_after_change": "  private ContainerLaunchContext createAMContainerLaunchContext(\n      ApplicationSubmissionContext applicationMasterContext,\n      ContainerId containerID) throws IOException {\n\n    // Construct the actual Container\n    ContainerLaunchContext container = \n        applicationMasterContext.getAMContainerSpec();\n\n    if (container == null){\n      throw new IOException(containerID +\n            \" has been cleaned before launched\");\n    }\n    // Finalize the container\n    setupTokens(container, containerID);\n    // set the flow context optionally for timeline service v.2\n    setFlowContext(container);\n\n    return container;\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.launch": {
                "code_before_change": "  private void launch() throws IOException, YarnException {\n    connect();\n    ContainerId masterContainerID = masterContainer.getId();\n    ApplicationSubmissionContext applicationContext =\n      application.getSubmissionContext();\n    LOG.info(\"Setting up container \" + masterContainer\n        + \" for AM \" + application.getAppAttemptId());\n    ContainerLaunchContext launchContext =\n        createAMContainerLaunchContext(applicationContext, masterContainerID);\n\n    StartContainerRequest scRequest =\n        StartContainerRequest.newInstance(launchContext,\n          masterContainer.getContainerToken());\n    List<StartContainerRequest> list = new ArrayList<StartContainerRequest>();\n    list.add(scRequest);\n    StartContainersRequest allRequests =\n        StartContainersRequest.newInstance(list);\n\n    StartContainersResponse response =\n        containerMgrProxy.startContainers(allRequests);\n    if (response.getFailedRequests() != null\n        && response.getFailedRequests().containsKey(masterContainerID)) {\n      Throwable t =\n          response.getFailedRequests().get(masterContainerID).deSerialize();\n      parseAndThrowException(t);\n    } else {\n      LOG.info(\"Done launching container \" + masterContainer + \" for AM \"\n          + application.getAppAttemptId());\n    }\n  }",
                "code_after_change": "  private void launch() throws IOException, YarnException {\n    connect();\n    ContainerId masterContainerID = masterContainer.getId();\n    ApplicationSubmissionContext applicationContext =\n        application.getSubmissionContext();\n    LOG.info(\"Setting up container \" + masterContainer\n        + \" for AM \" + application.getAppAttemptId());  \n    ContainerLaunchContext launchContext =\n        createAMContainerLaunchContext(applicationContext, masterContainerID);\n\n    StartContainerRequest scRequest =\n        StartContainerRequest.newInstance(launchContext,\n          masterContainer.getContainerToken());\n    List<StartContainerRequest> list = new ArrayList<StartContainerRequest>();\n    list.add(scRequest);\n    StartContainersRequest allRequests =\n        StartContainersRequest.newInstance(list);\n\n    StartContainersResponse response =\n        containerMgrProxy.startContainers(allRequests);\n    if (response.getFailedRequests() != null\n        && response.getFailedRequests().containsKey(masterContainerID)) {\n      Throwable t =\n          response.getFailedRequests().get(masterContainerID).deSerialize();\n      parseAndThrowException(t);\n    } else {\n      LOG.info(\"Done launching container \" + masterContainer + \" for AM \"\n          + application.getAppAttemptId());\n    }\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.parseAndThrowException": {
                "code_before_change": "  private void parseAndThrowException(Throwable t) throws YarnException,\n      IOException {\n    if (t instanceof YarnException) {\n      throw (YarnException) t;\n    } else if (t instanceof InvalidToken) {\n      throw (InvalidToken) t;\n    } else {\n      throw (IOException) t;\n    }\n  }",
                "code_after_change": "  private void parseAndThrowException(Throwable t) throws YarnException,\n      IOException {\n    if (t instanceof YarnException) {\n      throw (YarnException) t;\n    } else if (t instanceof InvalidToken) {\n      throw (InvalidToken) t;\n    } else {\n      throw (IOException) t;\n    }\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Buggy Method"
            },
            "fix_suggestion": "Preventive",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the root cause as a NullPointerException due to potentially null objects in the `setupTokens` method, which is a buggy method but not where the actual fix was made. The fix suggestion involves adding null checks, which is preventive as it would mitigate the bug by ensuring objects are not null before access. The problem location is precise as it mentions methods from the ground truth list, including `AMLauncher.run`, `AMLauncher.createAMContainerLaunchContext`, and `AMLauncher.launch`. There is no wrong information in the bug report as all details are relevant to the context of the bug."
        }
    },
    {
        "filename": "YARN-8035.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainerMetrics.recordCpuUsage": {
                "code_before_change": [],
                "code_after_change": []
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Buggy Method"
            },
            "fix_suggestion": "Preventive",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Buggy Method"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the root cause as the attempt to register a tag named {{ContainerPid}} in the {{MetricsRegistry}} that has already been registered, leading to a {{MetricsException}}. This is related to the method 'ContainerMetrics.recordProcessId', which is a buggy method but not the exact ground truth method 'ContainerMetrics.recordCpuUsage'. The fix suggestion is preventive as it suggests checking if the {{ContainerPid}} tag already exists before registering it again, which would prevent the error. The problem location identification is partial because it points to 'ContainerMetrics.recordProcessId', which is a buggy method but not the exact ground truth method. There is no wrong information in the bug report as all the details are relevant to the context of the bug."
        }
    },
    {
        "filename": "YARN-4152.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService.stopContainer": {
                "code_before_change": "  private void stopContainer(ContainerId containerId, int exitCode) {\n\n    // A container is complete. Put this containers' logs up for aggregation if\n    // this containers' logs are needed.\n\n    AppLogAggregator aggregator = this.appLogAggregators.get(\n        containerId.getApplicationAttemptId().getApplicationId());\n    if (aggregator == null) {\n      LOG.warn(\"Log aggregation is not initialized for \" + containerId\n          + \", did it fail to start?\");\n      return;\n    }\n    ContainerType containerType = context.getContainers().get(\n        containerId).getContainerTokenIdentifier().getContainerType();\n    aggregator.startContainerLogAggregation(\n        new ContainerLogContext(containerId, containerType, exitCode));\n  }",
                "code_after_change": "  private void stopContainer(ContainerId containerId, int exitCode) {\n\n    // A container is complete. Put this containers' logs up for aggregation if\n    // this containers' logs are needed.\n\n    AppLogAggregator aggregator = this.appLogAggregators.get(\n        containerId.getApplicationAttemptId().getApplicationId());\n    if (aggregator == null) {\n      LOG.warn(\"Log aggregation is not initialized for \" + containerId\n          + \", did it fail to start?\");\n      return;\n    }\n    Container container = context.getContainers().get(containerId);\n    if (null == container) {\n      LOG.warn(\"Log aggregation cannot be started for \" + containerId\n          + \", as its an absent container\");\n      return;\n    }\n    ContainerType containerType =\n        container.getContainerTokenIdentifier().getContainerType();\n    aggregator.startContainerLogAggregation(\n        new ContainerLogContext(containerId, containerType, exitCode));\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Correct",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report precisely identifies the root cause of the NullPointerException in the 'stopContainer' method, which is part of the ground truth methods. The fix suggestion in the bug report matches the developer's fix, as it suggests adding a null check for the container in the 'stopContainer' method. The problem location is also precise, as it directly mentions the 'stopContainer' method, which is the ground truth method. There is no wrong information in the bug report, as all details are consistent with the context of the bug."
        }
    },
    {
        "filename": "YARN-3697.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.continuousSchedulingAttempt": {
                "code_before_change": "  void continuousSchedulingAttempt() throws InterruptedException {\n    long start = getClock().getTime();\n    List<NodeId> nodeIdList = new ArrayList<NodeId>(nodes.keySet());\n    // Sort the nodes by space available on them, so that we offer\n    // containers on emptier nodes first, facilitating an even spread. This\n    // requires holding the scheduler lock, so that the space available on a\n    // node doesn't change during the sort.\n    synchronized (this) {\n      Collections.sort(nodeIdList, nodeAvailableResourceComparator);\n    }\n\n    // iterate all nodes\n    for (NodeId nodeId : nodeIdList) {\n      FSSchedulerNode node = getFSSchedulerNode(nodeId);\n      try {\n        if (node != null && Resources.fitsIn(minimumAllocation,\n            node.getAvailableResource())) {\n          attemptScheduling(node);\n        }\n      } catch (Throwable ex) {\n        LOG.error(\"Error while attempting scheduling for node \" + node +\n            \": \" + ex.toString(), ex);\n      }\n    }\n\n    long duration = getClock().getTime() - start;\n    fsOpDurations.addContinuousSchedulingRunDuration(duration);\n  }",
                "code_after_change": "  void continuousSchedulingAttempt() throws InterruptedException {\n    long start = getClock().getTime();\n    List<NodeId> nodeIdList = new ArrayList<NodeId>(nodes.keySet());\n    // Sort the nodes by space available on them, so that we offer\n    // containers on emptier nodes first, facilitating an even spread. This\n    // requires holding the scheduler lock, so that the space available on a\n    // node doesn't change during the sort.\n    synchronized (this) {\n      Collections.sort(nodeIdList, nodeAvailableResourceComparator);\n    }\n\n    // iterate all nodes\n    for (NodeId nodeId : nodeIdList) {\n      FSSchedulerNode node = getFSSchedulerNode(nodeId);\n      try {\n        if (node != null && Resources.fitsIn(minimumAllocation,\n            node.getAvailableResource())) {\n          attemptScheduling(node);\n        }\n      } catch (Throwable ex) {\n        LOG.error(\"Error while attempting scheduling for node \" + node +\n            \": \" + ex.toString(), ex);\n        if ((ex instanceof YarnRuntimeException) &&\n            (ex.getCause() instanceof InterruptedException)) {\n          // AsyncDispatcher translates InterruptedException to\n          // YarnRuntimeException with cause InterruptedException.\n          // Need to throw InterruptedException to stop schedulingThread.\n          throw (InterruptedException)ex.getCause();\n        }\n      }\n    }\n\n    long duration = getClock().getTime() - start;\n    fsOpDurations.addContinuousSchedulingRunDuration(duration);\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Correct",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report precisely identifies the root cause as the improper handling of InterruptedException in the FairScheduler's continuousSchedulingAttempt method, which matches the ground truth method. The fix suggestion is correct as it aligns with the developer's fix, which involves adding a catch block for InterruptedException in the continuousSchedulingAttempt method. The problem location is also precise, as it directly mentions the FairScheduler.continuousSchedulingAttempt method, which is the ground truth method. There is no wrong information in the bug report as all details are consistent with the context of the bug."
        }
    },
    {
        "filename": "YARN-2340.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.addApplication": {
                "code_before_change": "  private synchronized void addApplication(ApplicationId applicationId,\n      String queueName, String user, boolean isAppRecovering) {\n    // santiy checks.\n    CSQueue queue = getQueue(queueName);\n    if (queue == null) {\n      String message = \"Application \" + applicationId + \n      \" submitted by user \" + user + \" to unknown queue: \" + queueName;\n      this.rmContext.getDispatcher().getEventHandler()\n          .handle(new RMAppRejectedEvent(applicationId, message));\n      return;\n    }\n    if (!(queue instanceof LeafQueue)) {\n      String message = \"Application \" + applicationId + \n          \" submitted by user \" + user + \" to non-leaf queue: \" + queueName;\n      this.rmContext.getDispatcher().getEventHandler()\n          .handle(new RMAppRejectedEvent(applicationId, message));\n      return;\n    }\n    // Submit to the queue\n    try {\n      queue.submitApplication(applicationId, user, queueName);\n    } catch (AccessControlException ace) {\n      LOG.info(\"Failed to submit application \" + applicationId + \" to queue \"\n          + queueName + \" from user \" + user, ace);\n      this.rmContext.getDispatcher().getEventHandler()\n          .handle(new RMAppRejectedEvent(applicationId, ace.toString()));\n      return;\n    }\n    SchedulerApplication<FiCaSchedulerApp> application =\n        new SchedulerApplication<FiCaSchedulerApp>(queue, user);\n    applications.put(applicationId, application);\n    LOG.info(\"Accepted application \" + applicationId + \" from user: \" + user\n        + \", in queue: \" + queueName);\n    if (isAppRecovering) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(applicationId + \" is recovering. Skip notifying APP_ACCEPTED\");\n      }\n    } else {\n      rmContext.getDispatcher().getEventHandler()\n        .handle(new RMAppEvent(applicationId, RMAppEventType.APP_ACCEPTED));\n    }\n  }",
                "code_after_change": "  private synchronized void addApplication(ApplicationId applicationId,\n    String queueName, String user, boolean isAppRecovering) {\n\n    if (mappings != null && mappings.size() > 0) {\n      try {\n        String mappedQueue = getMappedQueue(user);\n        if (mappedQueue != null) {\n          // We have a mapping, should we use it?\n          if (queueName.equals(YarnConfiguration.DEFAULT_QUEUE_NAME)\n              || overrideWithQueueMappings) {\n            LOG.info(\"Application \" + applicationId + \" user \" + user\n                + \" mapping [\" + queueName + \"] to [\" + mappedQueue\n                + \"] override \" + overrideWithQueueMappings);\n            queueName = mappedQueue;\n            RMApp rmApp = rmContext.getRMApps().get(applicationId);\n            rmApp.setQueue(queueName);\n          }\n        }\n      } catch (IOException ioex) {\n        String message = \"Failed to submit application \" + applicationId +\n            \" submitted by user \" + user + \" reason: \" + ioex.getMessage();\n        this.rmContext.getDispatcher().getEventHandler()\n            .handle(new RMAppRejectedEvent(applicationId, message));\n        return;\n      }\n    }\n\n    // sanity checks.\n    CSQueue queue = getQueue(queueName);\n    if (queue == null) {\n      //During a restart, this indicates a queue was removed, which is\n      //not presently supported\n      if (isAppRecovering) {\n        String queueErrorMsg = \"Queue named \" + queueName\n           + \" missing during application recovery.\"\n           + \" Queue removal during recovery is not presently supported by the\"\n           + \" capacity scheduler, please restart with all queues configured\"\n           + \" which were present before shutdown/restart.\";\n        LOG.fatal(queueErrorMsg);\n        throw new QueueNotFoundException(queueErrorMsg);\n      }\n      String message = \"Application \" + applicationId + \n      \" submitted by user \" + user + \" to unknown queue: \" + queueName;\n      this.rmContext.getDispatcher().getEventHandler()\n          .handle(new RMAppRejectedEvent(applicationId, message));\n      return;\n    }\n    if (!(queue instanceof LeafQueue)) {\n      String message = \"Application \" + applicationId + \n          \" submitted by user \" + user + \" to non-leaf queue: \" + queueName;\n      this.rmContext.getDispatcher().getEventHandler()\n          .handle(new RMAppRejectedEvent(applicationId, message));\n      return;\n    }\n    // Submit to the queue\n    try {\n      queue.submitApplication(applicationId, user, queueName);\n    } catch (AccessControlException ace) {\n      // Ignore the exception for recovered app as the app was previously accepted\n      if (!isAppRecovering) {\n        LOG.info(\"Failed to submit application \" + applicationId + \" to queue \"\n            + queueName + \" from user \" + user, ace);\n        this.rmContext.getDispatcher().getEventHandler()\n            .handle(new RMAppRejectedEvent(applicationId, ace.toString()));\n        return;\n      }\n    }\n    // update the metrics\n    queue.getMetrics().submitApp(user);\n    SchedulerApplication<FiCaSchedulerApp> application =\n        new SchedulerApplication<FiCaSchedulerApp>(queue, user);\n    applications.put(applicationId, application);\n    LOG.info(\"Accepted application \" + applicationId + \" from user: \" + user\n        + \", in queue: \" + queueName);\n    if (isAppRecovering) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(applicationId + \" is recovering. Skip notifying APP_ACCEPTED\");\n      }\n    } else {\n      rmContext.getDispatcher().getEventHandler()\n        .handle(new RMAppEvent(applicationId, RMAppEventType.APP_ACCEPTED));\n    }\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Buggy Method"
            },
            "fix_suggestion": "Preventive",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the `addApplicationAttempt` method as the location of the NullPointerException, which is not the exact root cause but is closely related as it is the method where the error occurred. This places it in the 'Buggy Method' sub-category for the root cause identification. The fix suggestion involves adding null checks, which is a preventive measure to avoid the NullPointerException, although it does not match the exact developer's fix, which involved handling queue mappings and recovery scenarios. The problem location is precise as it correctly identifies the `addApplicationAttempt` and `handle` methods in the `CapacityScheduler` class, which are related to the ground truth method. There is no wrong information in the bug report as all details are relevant to the context of the bug."
        }
    },
    {
        "filename": "YARN-8022.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-common.src.main.java.org.apache.hadoop.yarn.server.webapp.AppBlock.render": {
                "code_before_change": "  protected void render(Block html) {\n    String webUiType = $(WEB_UI_TYPE);\n    String aid = $(APPLICATION_ID);\n    if (aid.isEmpty()) {\n      puts(\"Bad request: requires Application ID\");\n      return;\n    }\n\n    try {\n      appID = Apps.toAppID(aid);\n    } catch (Exception e) {\n      puts(\"Invalid Application ID: \" + aid);\n      return;\n    }\n\n    UserGroupInformation callerUGI = getCallerUGI();\n    ApplicationReport appReport;\n    try {\n      final GetApplicationReportRequest request =\n          GetApplicationReportRequest.newInstance(appID);\n      if (callerUGI == null) {\n        appReport =\n            appBaseProt.getApplicationReport(request).getApplicationReport();\n      } else {\n        appReport = callerUGI.doAs(\n            new PrivilegedExceptionAction<ApplicationReport> () {\n          @Override\n          public ApplicationReport run() throws Exception {\n            return getApplicationReport(request);\n          }\n        });\n      }\n    } catch (Exception e) {\n      String message = \"Failed to read the application \" + appID + \".\";\n      LOG.error(message, e);\n      html.p().__(message).__();\n      return;\n    }\n\n    if (appReport == null) {\n      puts(\"Application not found: \" + aid);\n      return;\n    }\n\n    AppInfo app = new AppInfo(appReport);\n\n    setTitle(join(\"Application \", aid));\n\n    //Validate if able to read application attempts\n    // which should also validate if kill is allowed for the user based on ACLs\n\n    Collection<ApplicationAttemptReport> attempts;\n    try {\n      final GetApplicationAttemptsRequest request =\n          GetApplicationAttemptsRequest.newInstance(appID);\n      attempts = callerUGI.doAs(\n          new PrivilegedExceptionAction<Collection<\n              ApplicationAttemptReport>>() {\n            @Override\n            public Collection<ApplicationAttemptReport> run() throws Exception {\n              return getApplicationAttemptsReport(request);\n            }\n          });\n    } catch (Exception e) {\n      String message =\n          \"Failed to read the attempts of the application \" + appID + \".\";\n      LOG.error(message, e);\n      html.p().__(message).__();\n      return;\n    }\n\n\n    // YARN-6890. for secured cluster allow anonymous UI access, application kill\n    // shouldn't be there.\n    boolean unsecuredUIForSecuredCluster = UserGroupInformation.isSecurityEnabled()\n        && this.unsecuredUI;\n\n    if (webUiType != null\n        && webUiType.equals(YarnWebParams.RM_WEB_UI)\n        && conf.getBoolean(YarnConfiguration.RM_WEBAPP_UI_ACTIONS_ENABLED,\n          YarnConfiguration.DEFAULT_RM_WEBAPP_UI_ACTIONS_ENABLED)\n            && !unsecuredUIForSecuredCluster\n            && !isAppInFinalState(app)) {\n      // Application Kill\n      html.div()\n        .button()\n          .$onclick(\"confirmAction()\").b(\"Kill Application\").__()\n          .__();\n\n      StringBuilder script = new StringBuilder();\n      script.append(\"function confirmAction() {\")\n          .append(\" b = confirm(\\\"Are you sure?\\\");\")\n          .append(\" if (b == true) {\")\n          .append(\" $.ajax({\")\n          .append(\" type: 'PUT',\")\n          .append(\" url: '/ws/v1/cluster/apps/\").append(aid).append(\"/state',\")\n          .append(\" contentType: 'application/json',\")\n          .append(getCSRFHeaderString(conf))\n          .append(\" data: '{\\\"state\\\":\\\"KILLED\\\"}',\")\n          .append(\" dataType: 'json'\")\n          .append(\" }).done(function(data){\")\n          .append(\" setTimeout(function(){\")\n          .append(\" location.href = '/cluster/app/\").append(aid).append(\"';\")\n          .append(\" }, 1000);\")\n          .append(\" }).fail(function(data){\")\n          .append(\" console.log(data);\")\n          .append(\" });\")\n          .append(\" }\")\n          .append(\"}\");\n\n      html.script().$type(\"text/javascript\").__(script.toString()).__();\n    }\n\n    String schedulerPath = WebAppUtils.getResolvedRMWebAppURLWithScheme(conf) +\n        \"/cluster/scheduler?openQueues=\" + app.getQueue();\n\n    ResponseInfo overviewTable = info(\"Application Overview\")\n      .__(\"User:\", schedulerPath, app.getUser())\n      .__(\"Name:\", app.getName())\n      .__(\"Application Type:\", app.getType())\n      .__(\"Application Tags:\",\n        app.getApplicationTags() == null ? \"\" : app.getApplicationTags())\n      .__(\"Application Priority:\", clarifyAppPriority(app.getPriority()))\n      .__(\n        \"YarnApplicationState:\",\n        app.getAppState() == null ? UNAVAILABLE : clarifyAppState(app\n          .getAppState()))\n      .__(\"Queue:\", schedulerPath, app.getQueue())\n      .__(\"FinalStatus Reported by AM:\",\n        clairfyAppFinalStatus(app.getFinalAppStatus()))\n      .__(\"Started:\", Times.format(app.getStartedTime()))\n      .__(\n        \"Elapsed:\",\n        StringUtils.formatTime(Times.elapsed(app.getStartedTime(),\n          app.getFinishedTime())))\n      .__(\n        \"Tracking URL:\",\n        app.getTrackingUrl() == null\n            || app.getTrackingUrl().equals(UNAVAILABLE) ? null : root_url(app\n          .getTrackingUrl()),\n        app.getTrackingUrl() == null\n            || app.getTrackingUrl().equals(UNAVAILABLE) ? \"Unassigned\" : app\n          .getAppState() == YarnApplicationState.FINISHED\n            || app.getAppState() == YarnApplicationState.FAILED\n            || app.getAppState() == YarnApplicationState.KILLED ? \"History\"\n            : \"ApplicationMaster\");\n    if (webUiType != null\n        && webUiType.equals(YarnWebParams.RM_WEB_UI)) {\n      LogAggregationStatus status = getLogAggregationStatus();\n      if (status == null) {\n        overviewTable.__(\"Log Aggregation Status:\", \"N/A\");\n      } else if (status == LogAggregationStatus.DISABLED\n          || status == LogAggregationStatus.NOT_START\n          || status == LogAggregationStatus.SUCCEEDED) {\n        overviewTable.__(\"Log Aggregation Status:\", status.name());\n      } else {\n        overviewTable.__(\"Log Aggregation Status:\",\n            root_url(\"logaggregationstatus\", app.getAppId()), status.name());\n      }\n      long timeout = appReport.getApplicationTimeouts()\n          .get(ApplicationTimeoutType.LIFETIME).getRemainingTime();\n      if (timeout < 0) {\n        overviewTable.__(\"Application Timeout (Remaining Time):\", \"Unlimited\");\n      } else {\n        overviewTable.__(\"Application Timeout (Remaining Time):\",\n            String.format(\"%d seconds\", timeout));\n      }\n    }\n    overviewTable.__(\"Diagnostics:\",\n        app.getDiagnosticsInfo() == null ? \"\" : app.getDiagnosticsInfo());\n    overviewTable.__(\"Unmanaged Application:\", app.isUnmanagedApp());\n    overviewTable.__(\"Application Node Label expression:\",\n        app.getAppNodeLabelExpression() == null ? \"<Not set>\"\n            : app.getAppNodeLabelExpression());\n    overviewTable.__(\"AM container Node Label expression:\",\n        app.getAmNodeLabelExpression() == null ? \"<Not set>\"\n            : app.getAmNodeLabelExpression());\n\n    try {\n      final GetApplicationAttemptsRequest request =\n          GetApplicationAttemptsRequest.newInstance(appID);\n      if (callerUGI == null) {\n        attempts = appBaseProt.getApplicationAttempts(request)\n            .getApplicationAttemptList();\n      } else {\n        attempts = callerUGI.doAs(\n            new PrivilegedExceptionAction<Collection<ApplicationAttemptReport>> () {\n          @Override\n          public Collection<ApplicationAttemptReport> run() throws Exception {\n            return appBaseProt.getApplicationAttempts(request)\n                .getApplicationAttemptList();\n          }\n        });\n      }\n    } catch (Exception e) {\n      String message =\n          \"Failed to read the attempts of the application \" + appID + \".\";\n      LOG.error(message, e);\n      html.p().__(message).__();\n      return;\n    }\n\n    createApplicationMetricsTable(html);\n\n    html.__(InfoBlock.class);\n\n    generateApplicationTable(html, callerUGI, attempts);\n\n  }",
                "code_after_change": "  protected void render(Block html) {\n    String webUiType = $(WEB_UI_TYPE);\n    String aid = $(APPLICATION_ID);\n    if (aid.isEmpty()) {\n      puts(\"Bad request: requires Application ID\");\n      return;\n    }\n\n    try {\n      appID = Apps.toAppID(aid);\n    } catch (Exception e) {\n      puts(\"Invalid Application ID: \" + aid);\n      return;\n    }\n\n    UserGroupInformation callerUGI = getCallerUGI();\n    ApplicationReport appReport;\n    try {\n      final GetApplicationReportRequest request =\n          GetApplicationReportRequest.newInstance(appID);\n      if (callerUGI == null) {\n        appReport = getApplicationReport(request);\n      } else {\n        appReport = callerUGI.doAs(\n            new PrivilegedExceptionAction<ApplicationReport> () {\n          @Override\n          public ApplicationReport run() throws Exception {\n            return getApplicationReport(request);\n          }\n        });\n      }\n    } catch (Exception e) {\n      String message = \"Failed to read the application \" + appID + \".\";\n      LOG.error(message, e);\n      html.p().__(message).__();\n      return;\n    }\n\n    if (appReport == null) {\n      puts(\"Application not found: \" + aid);\n      return;\n    }\n\n    AppInfo app = new AppInfo(appReport);\n\n    setTitle(join(\"Application \", aid));\n\n    //Validate if able to read application attempts\n    // which should also validate if kill is allowed for the user based on ACLs\n\n    Collection<ApplicationAttemptReport> attempts;\n    try {\n      final GetApplicationAttemptsRequest request =\n          GetApplicationAttemptsRequest.newInstance(appID);\n      if (callerUGI == null) {\n        attempts = getApplicationAttemptsReport(request);\n      } else {\n        attempts = callerUGI.doAs(\n          new PrivilegedExceptionAction<Collection<\n              ApplicationAttemptReport>>() {\n            @Override\n            public Collection<ApplicationAttemptReport> run()\n                throws Exception {\n              return getApplicationAttemptsReport(request);\n            }\n          });\n      }\n    } catch (Exception e) {\n      String message =\n          \"Failed to read the attempts of the application \" + appID + \".\";\n      LOG.error(message, e);\n      html.p().__(message).__();\n      return;\n    }\n\n\n    // YARN-6890. for secured cluster allow anonymous UI access, application kill\n    // shouldn't be there.\n    boolean unsecuredUIForSecuredCluster = UserGroupInformation.isSecurityEnabled()\n        && this.unsecuredUI;\n\n    if (webUiType != null\n        && webUiType.equals(YarnWebParams.RM_WEB_UI)\n        && conf.getBoolean(YarnConfiguration.RM_WEBAPP_UI_ACTIONS_ENABLED,\n          YarnConfiguration.DEFAULT_RM_WEBAPP_UI_ACTIONS_ENABLED)\n            && !unsecuredUIForSecuredCluster\n            && !isAppInFinalState(app)) {\n      // Application Kill\n      html.div()\n        .button()\n          .$onclick(\"confirmAction()\").b(\"Kill Application\").__()\n          .__();\n\n      StringBuilder script = new StringBuilder();\n      script.append(\"function confirmAction() {\")\n          .append(\" b = confirm(\\\"Are you sure?\\\");\")\n          .append(\" if (b == true) {\")\n          .append(\" $.ajax({\")\n          .append(\" type: 'PUT',\")\n          .append(\" url: '/ws/v1/cluster/apps/\").append(aid).append(\"/state',\")\n          .append(\" contentType: 'application/json',\")\n          .append(getCSRFHeaderString(conf))\n          .append(\" data: '{\\\"state\\\":\\\"KILLED\\\"}',\")\n          .append(\" dataType: 'json'\")\n          .append(\" }).done(function(data){\")\n          .append(\" setTimeout(function(){\")\n          .append(\" location.href = '/cluster/app/\").append(aid).append(\"';\")\n          .append(\" }, 1000);\")\n          .append(\" }).fail(function(data){\")\n          .append(\" console.log(data);\")\n          .append(\" });\")\n          .append(\" }\")\n          .append(\"}\");\n\n      html.script().$type(\"text/javascript\").__(script.toString()).__();\n    }\n\n    String schedulerPath = WebAppUtils.getResolvedRMWebAppURLWithScheme(conf) +\n        \"/cluster/scheduler?openQueues=\" + app.getQueue();\n\n    generateOverviewTable(app, schedulerPath, webUiType, appReport);\n\n    createApplicationMetricsTable(html);\n\n    html.__(InfoBlock.class);\n\n    generateApplicationTable(html, callerUGI, attempts);\n\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-common.src.main.java.org.apache.hadoop.yarn.server.webapp.AppBlock.run": {
                "code_before_change": "          public ApplicationReport run() throws Exception {\n            return getApplicationReport(request);\n          }",
                "code_after_change": "          public ApplicationReport run() throws Exception {\n            return getApplicationReport(request);\n          }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-common.src.main.java.org.apache.hadoop.yarn.server.webapp.AppBlock.generateOverviewTable": {
                "code_before_change": [],
                "code_after_change": "  private void generateOverviewTable(AppInfo app, String schedulerPath,\n      String webUiType, ApplicationReport appReport) {\n    ResponseInfo overviewTable = info(\"Application Overview\")\n        .__(\"User:\", schedulerPath, app.getUser())\n        .__(\"Name:\", app.getName())\n        .__(\"Application Type:\", app.getType())\n        .__(\"Application Tags:\",\n            app.getApplicationTags() == null ? \"\" : app.getApplicationTags())\n        .__(\"Application Priority:\", clarifyAppPriority(app.getPriority()))\n        .__(\n            \"YarnApplicationState:\",\n            app.getAppState() == null ? UNAVAILABLE : clarifyAppState(app\n                .getAppState()))\n        .__(\"Queue:\", schedulerPath, app.getQueue())\n        .__(\"FinalStatus Reported by AM:\",\n            clairfyAppFinalStatus(app.getFinalAppStatus()))\n        .__(\"Started:\", Times.format(app.getStartedTime()))\n        .__(\n            \"Elapsed:\",\n            StringUtils.formatTime(Times.elapsed(app.getStartedTime(),\n                app.getFinishedTime())))\n        .__(\n            \"Tracking URL:\",\n            app.getTrackingUrl() == null\n                || app.getTrackingUrl().equals(UNAVAILABLE) ? null : root_url(app\n                .getTrackingUrl()),\n            app.getTrackingUrl() == null\n                || app.getTrackingUrl().equals(UNAVAILABLE) ? \"Unassigned\" : app\n                .getAppState() == YarnApplicationState.FINISHED\n                || app.getAppState() == YarnApplicationState.FAILED\n                || app.getAppState() == YarnApplicationState.KILLED ? \"History\"\n                : \"ApplicationMaster\");\n    if (webUiType != null\n        && webUiType.equals(YarnWebParams.RM_WEB_UI)) {\n      LogAggregationStatus status = getLogAggregationStatus();\n      if (status == null) {\n        overviewTable.__(\"Log Aggregation Status:\", \"N/A\");\n      } else if (status == LogAggregationStatus.DISABLED\n          || status == LogAggregationStatus.NOT_START\n          || status == LogAggregationStatus.SUCCEEDED) {\n        overviewTable.__(\"Log Aggregation Status:\", status.name());\n      } else {\n        overviewTable.__(\"Log Aggregation Status:\",\n            root_url(\"logaggregationstatus\", app.getAppId()), status.name());\n      }\n      long timeout = appReport.getApplicationTimeouts()\n          .get(ApplicationTimeoutType.LIFETIME).getRemainingTime();\n      if (timeout < 0) {\n        overviewTable.__(\"Application Timeout (Remaining Time):\", \"Unlimited\");\n      } else {\n        overviewTable.__(\"Application Timeout (Remaining Time):\",\n            String.format(\"%d seconds\", timeout));\n      }\n    }\n    overviewTable.__(\"Diagnostics:\",\n        app.getDiagnosticsInfo() == null ? \"\" : app.getDiagnosticsInfo());\n    overviewTable.__(\"Unmanaged Application:\", app.isUnmanagedApp());\n    overviewTable.__(\"Application Node Label expression:\",\n        app.getAppNodeLabelExpression() == null ? \"<Not set>\"\n            : app.getAppNodeLabelExpression());\n    overviewTable.__(\"AM container Node Label expression:\",\n        app.getAmNodeLabelExpression() == null ? \"<Not set>\"\n            : app.getAmNodeLabelExpression());\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Correct",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report accurately identifies the root cause as a NullPointerException due to an invalid or missing application ID, which aligns with the issue in the 'AppBlock.render' method. The fix suggestion to implement validation checks for the application ID and ensure the application exists in the ResourceManager context matches the developer's fix, which involves handling null application objects. The problem location is precisely identified as it mentions 'AppBlock.render', which is part of the ground truth methods. There is no wrong information in the bug report; all details are relevant and correctly describe the issue and its resolution."
        }
    },
    {
        "filename": "YARN-3793.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl.uploadLogsForContainers": {
                "code_before_change": "  private void uploadLogsForContainers(boolean appFinished) {\n    if (this.logAggregationDisabled) {\n      return;\n    }\n\n    if (UserGroupInformation.isSecurityEnabled()) {\n      Credentials systemCredentials =\n          context.getSystemCredentialsForApps().get(appId);\n      if (systemCredentials != null) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Adding new framework-token for \" + appId\n              + \" for log-aggregation: \" + systemCredentials.getAllTokens()\n              + \"; userUgi=\" + userUgi);\n        }\n        // this will replace old token\n        userUgi.addCredentials(systemCredentials);\n      }\n    }\n\n    // Create a set of Containers whose logs will be uploaded in this cycle.\n    // It includes:\n    // a) all containers in pendingContainers: those containers are finished\n    //    and satisfy the retentionPolicy.\n    // b) some set of running containers: For all the Running containers,\n    // we have ContainerLogsRetentionPolicy.AM_AND_FAILED_CONTAINERS_ONLY,\n    // so simply set wasContainerSuccessful as true to\n    // bypass FAILED_CONTAINERS check and find the running containers \n    // which satisfy the retentionPolicy.\n    Set<ContainerId> pendingContainerInThisCycle = new HashSet<ContainerId>();\n    this.pendingContainers.drainTo(pendingContainerInThisCycle);\n    Set<ContainerId> finishedContainers =\n        new HashSet<ContainerId>(pendingContainerInThisCycle);\n    if (this.context.getApplications().get(this.appId) != null) {\n      for (ContainerId container : this.context.getApplications()\n        .get(this.appId).getContainers().keySet()) {\n        if (shouldUploadLogs(container, true)) {\n          pendingContainerInThisCycle.add(container);\n        }\n      }\n    }\n\n    LogWriter writer = null;\n    try {\n      try {\n        writer =\n            new LogWriter(this.conf, this.remoteNodeTmpLogFileForApp,\n              this.userUgi);\n        // Write ACLs once when the writer is created.\n        writer.writeApplicationACLs(appAcls);\n        writer.writeApplicationOwner(this.userUgi.getShortUserName());\n\n      } catch (IOException e1) {\n        LOG.error(\"Cannot create writer for app \" + this.applicationId\n            + \". Skip log upload this time. \", e1);\n        return;\n      }\n\n      boolean uploadedLogsInThisCycle = false;\n      for (ContainerId container : pendingContainerInThisCycle) {\n        ContainerLogAggregator aggregator = null;\n        if (containerLogAggregators.containsKey(container)) {\n          aggregator = containerLogAggregators.get(container);\n        } else {\n          aggregator = new ContainerLogAggregator(container);\n          containerLogAggregators.put(container, aggregator);\n        }\n        Set<Path> uploadedFilePathsInThisCycle =\n            aggregator.doContainerLogAggregation(writer, appFinished);\n        if (uploadedFilePathsInThisCycle.size() > 0) {\n          uploadedLogsInThisCycle = true;\n        }\n        this.delService.delete(this.userUgi.getShortUserName(), null,\n          uploadedFilePathsInThisCycle\n            .toArray(new Path[uploadedFilePathsInThisCycle.size()]));\n\n        // This container is finished, and all its logs have been uploaded,\n        // remove it from containerLogAggregators.\n        if (finishedContainers.contains(container)) {\n          containerLogAggregators.remove(container);\n        }\n      }\n\n      // Before upload logs, make sure the number of existing logs\n      // is smaller than the configured NM log aggregation retention size.\n      if (uploadedLogsInThisCycle) {\n        cleanOldLogs();\n      }\n\n      if (writer != null) {\n        writer.close();\n      }\n\n      long currentTime = System.currentTimeMillis();\n      final Path renamedPath = this.rollingMonitorInterval <= 0\n              ? remoteNodeLogFileForApp : new Path(\n                remoteNodeLogFileForApp.getParent(),\n                remoteNodeLogFileForApp.getName() + \"_\"\n                    + currentTime);\n\n      String diagnosticMessage = \"\";\n      boolean logAggregationSucceedInThisCycle = true;\n      final boolean rename = uploadedLogsInThisCycle;\n      try {\n        userUgi.doAs(new PrivilegedExceptionAction<Object>() {\n          @Override\n          public Object run() throws Exception {\n            FileSystem remoteFS = remoteNodeLogFileForApp.getFileSystem(conf);\n            if (remoteFS.exists(remoteNodeTmpLogFileForApp)) {\n              if (rename) {\n                remoteFS.rename(remoteNodeTmpLogFileForApp, renamedPath);\n              } else {\n                remoteFS.delete(remoteNodeTmpLogFileForApp, false);\n              }\n            }\n            return null;\n          }\n        });\n        diagnosticMessage =\n            \"Log uploaded successfully for Application: \" + appId\n                + \" in NodeManager: \"\n                + LogAggregationUtils.getNodeString(nodeId) + \" at \"\n                + Times.format(currentTime) + \"\\n\";\n      } catch (Exception e) {\n        LOG.error(\n          \"Failed to move temporary log file to final location: [\"\n              + remoteNodeTmpLogFileForApp + \"] to [\"\n              + renamedPath + \"]\", e);\n        diagnosticMessage =\n            \"Log uploaded failed for Application: \" + appId\n                + \" in NodeManager: \"\n                + LogAggregationUtils.getNodeString(nodeId) + \" at \"\n                + Times.format(currentTime) + \"\\n\";\n        renameTemporaryLogFileFailed = true;\n        logAggregationSucceedInThisCycle = false;\n      }\n\n      LogAggregationReport report =\n          Records.newRecord(LogAggregationReport.class);\n      report.setApplicationId(appId);\n      report.setDiagnosticMessage(diagnosticMessage);\n      report.setLogAggregationStatus(logAggregationSucceedInThisCycle\n          ? LogAggregationStatus.RUNNING\n          : LogAggregationStatus.RUNNING_WITH_FAILURE);\n      this.context.getLogAggregationStatusForApps().add(report);\n      if (appFinished) {\n        // If the app is finished, one extra final report with log aggregation\n        // status SUCCEEDED/FAILED will be sent to RM to inform the RM\n        // that the log aggregation in this NM is completed.\n        LogAggregationReport finalReport =\n            Records.newRecord(LogAggregationReport.class);\n        finalReport.setApplicationId(appId);\n        finalReport.setLogAggregationStatus(renameTemporaryLogFileFailed\n            ? LogAggregationStatus.FAILED : LogAggregationStatus.SUCCEEDED);\n        this.context.getLogAggregationStatusForApps().add(finalReport);\n      }\n    } finally {\n      if (writer != null) {\n        writer.close();\n      }\n    }\n  }",
                "code_after_change": "  private void uploadLogsForContainers(boolean appFinished) {\n    if (this.logAggregationDisabled) {\n      return;\n    }\n\n    if (UserGroupInformation.isSecurityEnabled()) {\n      Credentials systemCredentials =\n          context.getSystemCredentialsForApps().get(appId);\n      if (systemCredentials != null) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Adding new framework-token for \" + appId\n              + \" for log-aggregation: \" + systemCredentials.getAllTokens()\n              + \"; userUgi=\" + userUgi);\n        }\n        // this will replace old token\n        userUgi.addCredentials(systemCredentials);\n      }\n    }\n\n    // Create a set of Containers whose logs will be uploaded in this cycle.\n    // It includes:\n    // a) all containers in pendingContainers: those containers are finished\n    //    and satisfy the retentionPolicy.\n    // b) some set of running containers: For all the Running containers,\n    // we have ContainerLogsRetentionPolicy.AM_AND_FAILED_CONTAINERS_ONLY,\n    // so simply set wasContainerSuccessful as true to\n    // bypass FAILED_CONTAINERS check and find the running containers \n    // which satisfy the retentionPolicy.\n    Set<ContainerId> pendingContainerInThisCycle = new HashSet<ContainerId>();\n    this.pendingContainers.drainTo(pendingContainerInThisCycle);\n    Set<ContainerId> finishedContainers =\n        new HashSet<ContainerId>(pendingContainerInThisCycle);\n    if (this.context.getApplications().get(this.appId) != null) {\n      for (ContainerId container : this.context.getApplications()\n        .get(this.appId).getContainers().keySet()) {\n        if (shouldUploadLogs(container, true)) {\n          pendingContainerInThisCycle.add(container);\n        }\n      }\n    }\n\n    LogWriter writer = null;\n    try {\n      try {\n        writer =\n            new LogWriter(this.conf, this.remoteNodeTmpLogFileForApp,\n              this.userUgi);\n        // Write ACLs once when the writer is created.\n        writer.writeApplicationACLs(appAcls);\n        writer.writeApplicationOwner(this.userUgi.getShortUserName());\n\n      } catch (IOException e1) {\n        LOG.error(\"Cannot create writer for app \" + this.applicationId\n            + \". Skip log upload this time. \", e1);\n        return;\n      }\n\n      boolean uploadedLogsInThisCycle = false;\n      for (ContainerId container : pendingContainerInThisCycle) {\n        ContainerLogAggregator aggregator = null;\n        if (containerLogAggregators.containsKey(container)) {\n          aggregator = containerLogAggregators.get(container);\n        } else {\n          aggregator = new ContainerLogAggregator(container);\n          containerLogAggregators.put(container, aggregator);\n        }\n        Set<Path> uploadedFilePathsInThisCycle =\n            aggregator.doContainerLogAggregation(writer, appFinished);\n        if (uploadedFilePathsInThisCycle.size() > 0) {\n          uploadedLogsInThisCycle = true;\n          this.delService.delete(this.userUgi.getShortUserName(), null,\n              uploadedFilePathsInThisCycle\n                  .toArray(new Path[uploadedFilePathsInThisCycle.size()]));\n        }\n\n        // This container is finished, and all its logs have been uploaded,\n        // remove it from containerLogAggregators.\n        if (finishedContainers.contains(container)) {\n          containerLogAggregators.remove(container);\n        }\n      }\n\n      // Before upload logs, make sure the number of existing logs\n      // is smaller than the configured NM log aggregation retention size.\n      if (uploadedLogsInThisCycle) {\n        cleanOldLogs();\n      }\n\n      if (writer != null) {\n        writer.close();\n      }\n\n      final Path renamedPath = this.rollingMonitorInterval <= 0\n              ? remoteNodeLogFileForApp : new Path(\n                remoteNodeLogFileForApp.getParent(),\n                remoteNodeLogFileForApp.getName() + \"_\"\n                    + System.currentTimeMillis());\n\n      final boolean rename = uploadedLogsInThisCycle;\n      try {\n        userUgi.doAs(new PrivilegedExceptionAction<Object>() {\n          @Override\n          public Object run() throws Exception {\n            FileSystem remoteFS = FileSystem.get(conf);\n            if (remoteFS.exists(remoteNodeTmpLogFileForApp)) {\n              if (rename) {\n                remoteFS.rename(remoteNodeTmpLogFileForApp, renamedPath);\n              } else {\n                remoteFS.delete(remoteNodeTmpLogFileForApp, false);\n              }\n            }\n            return null;\n          }\n        });\n      } catch (Exception e) {\n        LOG.error(\n          \"Failed to move temporary log file to final location: [\"\n              + remoteNodeTmpLogFileForApp + \"] to [\"\n              + renamedPath + \"]\", e);\n      }\n    } finally {\n      if (writer != null) {\n        writer.close();\n      }\n    }\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Preventive",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the root cause as a NullPointerException due to a null 'subDir' in the 'DeletionAsUserContext', which is not the exact root cause in the ground truth method but is related to the methods in the stack trace. The fix suggestion is preventive as it suggests adding null checks and ensuring 'subDir' is set, which would prevent the NPE but does not match the actual fix in the ground truth method. The problem location identification is partial because it mentions methods in the stack trace but not the ground truth method. There is no wrong information in the bug report as all details are relevant to the context of the bug."
        }
    },
    {
        "filename": "YARN-6102.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.RMContextImpl.isWorkPreservingRecoveryEnabled": {
                "code_before_change": "  public boolean isWorkPreservingRecoveryEnabled() {\n    return activeServiceContext.isWorkPreservingRecoveryEnabled();\n  }",
                "code_after_change": "  public boolean isWorkPreservingRecoveryEnabled() {\n    return activeServiceContext.isWorkPreservingRecoveryEnabled();\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.reinitialize": {
                "code_before_change": "  void reinitialize(boolean initialize) {\n    ClusterMetrics.destroy();\n    QueueMetrics.clearQueueMetrics();\n    if (initialize) {\n      resetDispatcher();\n      createAndInitActiveServices(true);\n    }\n  }",
                "code_after_change": "  void reinitialize(boolean initialize) {\n    ClusterMetrics.destroy();\n    QueueMetrics.clearQueueMetrics();\n    if (initialize) {\n      resetRMContext();\n      createAndInitActiveServices(true);\n    }\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.setupDispatcher": {
                "code_before_change": "  private Dispatcher setupDispatcher() {\n    Dispatcher dispatcher = createDispatcher();\n    dispatcher.register(RMFatalEventType.class,\n        new ResourceManager.RMFatalEventDispatcher());\n    return dispatcher;\n  }",
                "code_after_change": "  private Dispatcher setupDispatcher() {\n    Dispatcher dispatcher = createDispatcher();\n    dispatcher.register(RMFatalEventType.class,\n        new ResourceManager.RMFatalEventDispatcher());\n    return dispatcher;\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.AdminService.addToClusterNodeLabels": {
                "code_before_change": "  public AddToClusterNodeLabelsResponse addToClusterNodeLabels(AddToClusterNodeLabelsRequest request)\n      throws YarnException, IOException {\n    final String operation = \"addToClusterNodeLabels\";\n    final String msg = \"add labels.\";\n    UserGroupInformation user = checkAcls(operation);\n\n    checkRMStatus(user.getShortUserName(), operation, msg);\n\n    AddToClusterNodeLabelsResponse response =\n        recordFactory.newRecordInstance(AddToClusterNodeLabelsResponse.class);\n    try {\n      rmContext.getNodeLabelManager().addToCluserNodeLabels(request.getNodeLabels());\n      RMAuditLogger\n          .logSuccess(user.getShortUserName(), operation, \"AdminService\");\n      return response;\n    } catch (IOException ioe) {\n      throw logAndWrapException(ioe, user.getShortUserName(), operation, msg);\n    }\n  }",
                "code_after_change": "  public AddToClusterNodeLabelsResponse addToClusterNodeLabels(AddToClusterNodeLabelsRequest request)\n      throws YarnException, IOException {\n    final String operation = \"addToClusterNodeLabels\";\n    final String msg = \"add labels.\";\n    UserGroupInformation user = checkAcls(operation);\n\n    checkRMStatus(user.getShortUserName(), operation, msg);\n\n    AddToClusterNodeLabelsResponse response =\n        recordFactory.newRecordInstance(AddToClusterNodeLabelsResponse.class);\n    try {\n      rm.getRMContext().getNodeLabelManager()\n          .addToCluserNodeLabels(request.getNodeLabels());\n      RMAuditLogger.logSuccess(user.getShortUserName(), operation,\n          \"AdminService\");\n      return response;\n    } catch (IOException ioe) {\n      throw logAndWrapException(ioe, user.getShortUserName(), operation, msg);\n    }\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.ActiveStandbyElectorBasedElectorService.becomeActive": {
                "code_before_change": "  public void becomeActive() throws ServiceFailedException {\n    cancelDisconnectTimer();\n\n    try {\n      rmContext.getRMAdminService().transitionToActive(req);\n    } catch (Exception e) {\n      throw new ServiceFailedException(\"RM could not transition to Active\", e);\n    }\n  }",
                "code_after_change": "  public void becomeActive() throws ServiceFailedException {\n    cancelDisconnectTimer();\n\n    try {\n      rm.getRMContext().getRMAdminService().transitionToActive(req);\n    } catch (Exception e) {\n      throw new ServiceFailedException(\"RM could not transition to Active\", e);\n    }\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.AdminService.refreshNodesResources": {
                "code_before_change": "  public RefreshNodesResourcesResponse refreshNodesResources(\n      RefreshNodesResourcesRequest request)\n      throws YarnException, StandbyException {\n    final String operation = \"refreshNodesResources\";\n    UserGroupInformation user = checkAcls(operation);\n    final String msg = \"refresh nodes.\";\n\n    checkRMStatus(user.getShortUserName(), operation, msg);\n\n    RefreshNodesResourcesResponse response =\n        recordFactory.newRecordInstance(RefreshNodesResourcesResponse.class);\n\n    try {\n      Configuration conf = getConfig();\n      Configuration configuration = new Configuration(conf);\n      DynamicResourceConfiguration newConf;\n\n      InputStream drInputStream =\n          this.rmContext.getConfigurationProvider().getConfigurationInputStream(\n              configuration, YarnConfiguration.DR_CONFIGURATION_FILE);\n\n      if (drInputStream != null) {\n        newConf = new DynamicResourceConfiguration(configuration,\n            drInputStream);\n      } else {\n        newConf = new DynamicResourceConfiguration(configuration);\n      }\n\n      if (newConf.getNodes() != null && newConf.getNodes().length != 0) {\n        Map<NodeId, ResourceOption> nodeResourceMap =\n            newConf.getNodeResourceMap();\n        UpdateNodeResourceRequest updateRequest =\n            UpdateNodeResourceRequest.newInstance(nodeResourceMap);\n        updateNodeResource(updateRequest);\n      }\n      // refresh dynamic resource in ResourceTrackerService\n      this.rmContext.getResourceTrackerService().\n          updateDynamicResourceConfiguration(newConf);\n      RMAuditLogger.logSuccess(user.getShortUserName(), operation,\n              \"AdminService\");\n      return response;\n    } catch (IOException ioe) {\n      throw logAndWrapException(ioe, user.getShortUserName(), operation, msg);\n    }\n  }",
                "code_after_change": "  public RefreshNodesResourcesResponse refreshNodesResources(\n      RefreshNodesResourcesRequest request)\n      throws YarnException, StandbyException {\n    final String operation = \"refreshNodesResources\";\n    UserGroupInformation user = checkAcls(operation);\n    final String msg = \"refresh nodes.\";\n\n    checkRMStatus(user.getShortUserName(), operation, msg);\n\n    RefreshNodesResourcesResponse response =\n        recordFactory.newRecordInstance(RefreshNodesResourcesResponse.class);\n\n    try {\n      Configuration conf = getConfig();\n      Configuration configuration = new Configuration(conf);\n      DynamicResourceConfiguration newConf;\n\n      InputStream drInputStream =\n          this.rm.getRMContext().getConfigurationProvider()\n              .getConfigurationInputStream(\n              configuration, YarnConfiguration.DR_CONFIGURATION_FILE);\n\n      if (drInputStream != null) {\n        newConf = new DynamicResourceConfiguration(configuration,\n            drInputStream);\n      } else {\n        newConf = new DynamicResourceConfiguration(configuration);\n      }\n\n      if (newConf.getNodes() != null && newConf.getNodes().length != 0) {\n        Map<NodeId, ResourceOption> nodeResourceMap =\n            newConf.getNodeResourceMap();\n        UpdateNodeResourceRequest updateRequest =\n            UpdateNodeResourceRequest.newInstance(nodeResourceMap);\n        updateNodeResource(updateRequest);\n      }\n      // refresh dynamic resource in ResourceTrackerService\n      this.rm.getRMContext().getResourceTrackerService().\n          updateDynamicResourceConfiguration(newConf);\n      RMAuditLogger.logSuccess(user.getShortUserName(), operation,\n              \"AdminService\");\n      return response;\n    } catch (IOException ioe) {\n      throw logAndWrapException(ioe, user.getShortUserName(), operation, msg);\n    }\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.RMContextImpl.getSystemCredentialsForApps": {
                "code_before_change": "  public ConcurrentMap<ApplicationId, ByteBuffer> getSystemCredentialsForApps() {\n    return activeServiceContext.getSystemCredentialsForApps();\n  }",
                "code_after_change": "  public ConcurrentMap<ApplicationId, ByteBuffer> getSystemCredentialsForApps() {\n    return activeServiceContext.getSystemCredentialsForApps();\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.AdminService.updateNodeResource": {
                "code_before_change": "  public UpdateNodeResourceResponse updateNodeResource(\n      UpdateNodeResourceRequest request) throws YarnException, IOException {\n    final String operation = \"updateNodeResource\";\n    UserGroupInformation user = checkAcls(operation);\n\n    checkRMStatus(user.getShortUserName(), operation, \"update node resource.\");\n\n    Map<NodeId, ResourceOption> nodeResourceMap = request.getNodeResourceMap();\n    Set<NodeId> nodeIds = nodeResourceMap.keySet();\n    // verify nodes are all valid first.\n    // if any invalid nodes, throw exception instead of partially updating\n    // valid nodes.\n    for (NodeId nodeId : nodeIds) {\n      RMNode node = this.rmContext.getRMNodes().get(nodeId);\n      if (node == null) {\n        LOG.error(\"Resource update get failed on all nodes due to change \"\n            + \"resource on an unrecognized node: \" + nodeId);\n        throw RPCUtil.getRemoteException(\n            \"Resource update get failed on all nodes due to change resource \"\n                + \"on an unrecognized node: \" + nodeId);\n      }\n    }\n\n    // do resource update on each node.\n    // Notice: it is still possible to have invalid NodeIDs as nodes decommission\n    // may happen just at the same time. This time, only log and skip absent\n    // nodes without throwing any exceptions.\n    boolean allSuccess = true;\n    for (Map.Entry<NodeId, ResourceOption> entry : nodeResourceMap.entrySet()) {\n      ResourceOption newResourceOption = entry.getValue();\n      NodeId nodeId = entry.getKey();\n      RMNode node = this.rmContext.getRMNodes().get(nodeId);\n\n      if (node == null) {\n        LOG.warn(\"Resource update get failed on an unrecognized node: \" + nodeId);\n        allSuccess = false;\n      } else {\n        // update resource to RMNode\n        this.rmContext.getDispatcher().getEventHandler()\n          .handle(new RMNodeResourceUpdateEvent(nodeId, newResourceOption));\n        LOG.info(\"Update resource on node(\" + node.getNodeID()\n            + \") with resource(\" + newResourceOption.toString() + \")\");\n\n      }\n    }\n    if (allSuccess) {\n      RMAuditLogger.logSuccess(user.getShortUserName(), operation,\n          \"AdminService\");\n    }\n    UpdateNodeResourceResponse response =\n        UpdateNodeResourceResponse.newInstance();\n    return response;\n  }",
                "code_after_change": "  public UpdateNodeResourceResponse updateNodeResource(\n      UpdateNodeResourceRequest request) throws YarnException, IOException {\n    final String operation = \"updateNodeResource\";\n    UserGroupInformation user = checkAcls(operation);\n\n    checkRMStatus(user.getShortUserName(), operation, \"update node resource.\");\n\n    Map<NodeId, ResourceOption> nodeResourceMap = request.getNodeResourceMap();\n    Set<NodeId> nodeIds = nodeResourceMap.keySet();\n    // verify nodes are all valid first.\n    // if any invalid nodes, throw exception instead of partially updating\n    // valid nodes.\n    for (NodeId nodeId : nodeIds) {\n      RMNode node = this.rm.getRMContext().getRMNodes().get(nodeId);\n      if (node == null) {\n        LOG.error(\"Resource update get failed on all nodes due to change \"\n            + \"resource on an unrecognized node: \" + nodeId);\n        throw RPCUtil.getRemoteException(\n            \"Resource update get failed on all nodes due to change resource \"\n                + \"on an unrecognized node: \" + nodeId);\n      }\n    }\n\n    // do resource update on each node.\n    // Notice: it is still possible to have invalid NodeIDs as nodes decommission\n    // may happen just at the same time. This time, only log and skip absent\n    // nodes without throwing any exceptions.\n    boolean allSuccess = true;\n    for (Map.Entry<NodeId, ResourceOption> entry : nodeResourceMap.entrySet()) {\n      ResourceOption newResourceOption = entry.getValue();\n      NodeId nodeId = entry.getKey();\n      RMNode node = this.rm.getRMContext().getRMNodes().get(nodeId);\n\n      if (node == null) {\n        LOG.warn(\"Resource update get failed on an unrecognized node: \" + nodeId);\n        allSuccess = false;\n      } else {\n        // update resource to RMNode\n        this.rm.getRMContext().getDispatcher().getEventHandler()\n          .handle(new RMNodeResourceUpdateEvent(nodeId, newResourceOption));\n        LOG.info(\"Update resource on node(\" + node.getNodeID()\n            + \") with resource(\" + newResourceOption.toString() + \")\");\n\n      }\n    }\n    if (allSuccess) {\n      RMAuditLogger.logSuccess(user.getShortUserName(), operation,\n          \"AdminService\");\n    }\n    UpdateNodeResourceResponse response =\n        UpdateNodeResourceResponse.newInstance();\n    return response;\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.createEmbeddedElector": {
                "code_before_change": "  protected EmbeddedElector createEmbeddedElector() throws IOException {\n    EmbeddedElector elector;\n    curatorEnabled =\n        conf.getBoolean(YarnConfiguration.CURATOR_LEADER_ELECTOR,\n            YarnConfiguration.DEFAULT_CURATOR_LEADER_ELECTOR_ENABLED);\n    if (curatorEnabled) {\n      this.curator = createAndStartCurator(conf);\n      elector = new CuratorBasedElectorService(rmContext, this);\n    } else {\n      elector = new ActiveStandbyElectorBasedElectorService(rmContext);\n    }\n    return elector;\n  }",
                "code_after_change": "  protected EmbeddedElector createEmbeddedElector() throws IOException {\n    EmbeddedElector elector;\n    curatorEnabled =\n        conf.getBoolean(YarnConfiguration.CURATOR_LEADER_ELECTOR,\n            YarnConfiguration.DEFAULT_CURATOR_LEADER_ELECTOR_ENABLED);\n    if (curatorEnabled) {\n      this.curator = createAndStartCurator(conf);\n      elector = new CuratorBasedElectorService(this);\n    } else {\n      elector = new ActiveStandbyElectorBasedElectorService(this);\n    }\n    return elector;\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.RMContextImpl.setResourceTrackerService": {
                "code_before_change": "  void setResourceTrackerService(ResourceTrackerService resourceTrackerService) {\n    activeServiceContext.setResourceTrackerService(resourceTrackerService);\n  }",
                "code_after_change": "  void setResourceTrackerService(ResourceTrackerService resourceTrackerService) {\n    activeServiceContext.setResourceTrackerService(resourceTrackerService);\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.AdminService.transitionToActive": {
                "code_before_change": "  public synchronized void transitionToActive(\n      HAServiceProtocol.StateChangeRequestInfo reqInfo) throws IOException {\n    // call refreshAdminAcls before HA state transition\n    // for the case that adminAcls have been updated in previous active RM\n    try {\n      refreshAdminAcls(false);\n    } catch (YarnException ex) {\n      throw new ServiceFailedException(\"Can not execute refreshAdminAcls\", ex);\n    }\n\n    UserGroupInformation user = checkAccess(\"transitionToActive\");\n    checkHaStateChange(reqInfo);\n\n    try {\n      // call all refresh*s for active RM to get the updated configurations.\n      refreshAll();\n    } catch (Exception e) {\n      LOG.error(\"RefreshAll failed so firing fatal event\", e);\n      rmContext\n          .getDispatcher()\n          .getEventHandler()\n          .handle(\n              new RMFatalEvent(RMFatalEventType.TRANSITION_TO_ACTIVE_FAILED,\n                  e));\n      throw new ServiceFailedException(\n          \"Error on refreshAll during transition to Active\", e);\n    }\n\n    try {\n      rm.transitionToActive();\n    } catch (Exception e) {\n      RMAuditLogger.logFailure(user.getShortUserName(), \"transitionToActive\",\n          \"\", \"RM\",\n          \"Exception transitioning to active\");\n      throw new ServiceFailedException(\n          \"Error when transitioning to Active mode\", e);\n    }\n\n    RMAuditLogger.logSuccess(user.getShortUserName(), \"transitionToActive\",\n        \"RM\");\n  }",
                "code_after_change": "  public synchronized void transitionToActive(\n      HAServiceProtocol.StateChangeRequestInfo reqInfo) throws IOException {\n    if (isRMActive()) {\n      return;\n    }\n    // call refreshAdminAcls before HA state transition\n    // for the case that adminAcls have been updated in previous active RM\n    try {\n      refreshAdminAcls(false);\n    } catch (YarnException ex) {\n      throw new ServiceFailedException(\"Can not execute refreshAdminAcls\", ex);\n    }\n\n    UserGroupInformation user = checkAccess(\"transitionToActive\");\n    checkHaStateChange(reqInfo);\n\n    try {\n      // call all refresh*s for active RM to get the updated configurations.\n      refreshAll();\n    } catch (Exception e) {\n      rm.getRMContext()\n          .getDispatcher()\n          .getEventHandler()\n          .handle(\n              new RMFatalEvent(RMFatalEventType.TRANSITION_TO_ACTIVE_FAILED,\n                  e, \"failure to refresh configuration settings\"));\n      throw new ServiceFailedException(\n          \"Error on refreshAll during transition to Active\", e);\n    }\n\n    try {\n      rm.transitionToActive();\n    } catch (Exception e) {\n      RMAuditLogger.logFailure(user.getShortUserName(), \"transitionToActive\",\n          \"\", \"RM\",\n          \"Exception transitioning to active\");\n      throw new ServiceFailedException(\n          \"Error when transitioning to Active mode\", e);\n    }\n\n    RMAuditLogger.logSuccess(user.getShortUserName(), \"transitionToActive\",\n        \"RM\");\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.RMContextImpl.getRMAppLifetimeMonitor": {
                "code_before_change": "  public RMAppLifetimeMonitor getRMAppLifetimeMonitor() {\n    return this.activeServiceContext.getRMAppLifetimeMonitor();\n  }",
                "code_after_change": "  public RMAppLifetimeMonitor getRMAppLifetimeMonitor() {\n    return this.activeServiceContext.getRMAppLifetimeMonitor();\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.createRMApplicationHistoryWriter": {
                "code_before_change": "  protected RMApplicationHistoryWriter createRMApplicationHistoryWriter() {\n    return new RMApplicationHistoryWriter();\n  }",
                "code_after_change": "  protected RMApplicationHistoryWriter createRMApplicationHistoryWriter() {\n    return new RMApplicationHistoryWriter();\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.AdminService.startServer": {
                "code_before_change": "  protected void startServer() throws Exception {\n    Configuration conf = getConfig();\n    YarnRPC rpc = YarnRPC.create(conf);\n    this.server = (Server) rpc.getServer(\n        ResourceManagerAdministrationProtocol.class, this, masterServiceBindAddress,\n        conf, null,\n        conf.getInt(YarnConfiguration.RM_ADMIN_CLIENT_THREAD_COUNT,\n            YarnConfiguration.DEFAULT_RM_ADMIN_CLIENT_THREAD_COUNT));\n\n    // Enable service authorization?\n    if (conf.getBoolean(\n        CommonConfigurationKeysPublic.HADOOP_SECURITY_AUTHORIZATION,\n        false)) {\n      refreshServiceAcls(\n          getConfiguration(conf,\n              YarnConfiguration.HADOOP_POLICY_CONFIGURATION_FILE),\n          RMPolicyProvider.getInstance());\n    }\n\n    if (rmContext.isHAEnabled()) {\n      RPC.setProtocolEngine(conf, HAServiceProtocolPB.class,\n          ProtobufRpcEngine.class);\n\n      HAServiceProtocolServerSideTranslatorPB haServiceProtocolXlator =\n          new HAServiceProtocolServerSideTranslatorPB(this);\n      BlockingService haPbService =\n          HAServiceProtocolProtos.HAServiceProtocolService\n              .newReflectiveBlockingService(haServiceProtocolXlator);\n      server.addProtocol(RPC.RpcKind.RPC_PROTOCOL_BUFFER,\n          HAServiceProtocol.class, haPbService);\n    }\n\n    this.server.start();\n    conf.updateConnectAddr(YarnConfiguration.RM_BIND_HOST,\n                           YarnConfiguration.RM_ADMIN_ADDRESS,\n                           YarnConfiguration.DEFAULT_RM_ADMIN_ADDRESS,\n                           server.getListenerAddress());\n  }",
                "code_after_change": "  protected void startServer() throws Exception {\n    Configuration conf = getConfig();\n    YarnRPC rpc = YarnRPC.create(conf);\n    this.server = (Server) rpc.getServer(\n        ResourceManagerAdministrationProtocol.class, this, masterServiceBindAddress,\n        conf, null,\n        conf.getInt(YarnConfiguration.RM_ADMIN_CLIENT_THREAD_COUNT,\n            YarnConfiguration.DEFAULT_RM_ADMIN_CLIENT_THREAD_COUNT));\n\n    // Enable service authorization?\n    if (conf.getBoolean(\n        CommonConfigurationKeysPublic.HADOOP_SECURITY_AUTHORIZATION,\n        false)) {\n      refreshServiceAcls(\n          getConfiguration(conf,\n              YarnConfiguration.HADOOP_POLICY_CONFIGURATION_FILE),\n          RMPolicyProvider.getInstance());\n    }\n\n    if (rm.getRMContext().isHAEnabled()) {\n      RPC.setProtocolEngine(conf, HAServiceProtocolPB.class,\n          ProtobufRpcEngine.class);\n\n      HAServiceProtocolServerSideTranslatorPB haServiceProtocolXlator =\n          new HAServiceProtocolServerSideTranslatorPB(this);\n      BlockingService haPbService =\n          HAServiceProtocolProtos.HAServiceProtocolService\n              .newReflectiveBlockingService(haServiceProtocolXlator);\n      server.addProtocol(RPC.RpcKind.RPC_PROTOCOL_BUFFER,\n          HAServiceProtocol.class, haPbService);\n    }\n\n    this.server.start();\n    conf.updateConnectAddr(YarnConfiguration.RM_BIND_HOST,\n                           YarnConfiguration.RM_ADMIN_ADDRESS,\n                           YarnConfiguration.DEFAULT_RM_ADMIN_ADDRESS,\n                           server.getListenerAddress());\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.AdminService.checkHaStateChange": {
                "code_before_change": "  private void checkHaStateChange(StateChangeRequestInfo req)\n      throws AccessControlException {\n    switch (req.getSource()) {\n      case REQUEST_BY_USER:\n        if (autoFailoverEnabled) {\n          throw new AccessControlException(\n              \"Manual failover for this ResourceManager is disallowed, \" +\n                  \"because automatic failover is enabled.\");\n        }\n        break;\n      case REQUEST_BY_USER_FORCED:\n        if (autoFailoverEnabled) {\n          LOG.warn(\"Allowing manual failover from \" +\n              org.apache.hadoop.ipc.Server.getRemoteAddress() +\n              \" even though automatic failover is enabled, because the user \" +\n              \"specified the force flag\");\n        }\n        break;\n      case REQUEST_BY_ZKFC:\n        if (!autoFailoverEnabled) {\n          throw new AccessControlException(\n              \"Request from ZK failover controller at \" +\n                  org.apache.hadoop.ipc.Server.getRemoteAddress() + \" denied \" +\n                  \"since automatic failover is not enabled\");\n        }\n        break;\n    }\n  }",
                "code_after_change": "  private void checkHaStateChange(StateChangeRequestInfo req)\n      throws AccessControlException {\n    switch (req.getSource()) {\n      case REQUEST_BY_USER:\n        if (autoFailoverEnabled) {\n          throw new AccessControlException(\n              \"Manual failover for this ResourceManager is disallowed, \" +\n                  \"because automatic failover is enabled.\");\n        }\n        break;\n      case REQUEST_BY_USER_FORCED:\n        if (autoFailoverEnabled) {\n          LOG.warn(\"Allowing manual failover from \" +\n              org.apache.hadoop.ipc.Server.getRemoteAddress() +\n              \" even though automatic failover is enabled, because the user \" +\n              \"specified the force flag\");\n        }\n        break;\n      case REQUEST_BY_ZKFC:\n        if (!autoFailoverEnabled) {\n          throw new AccessControlException(\n              \"Request from ZK failover controller at \" +\n                  org.apache.hadoop.ipc.Server.getRemoteAddress() + \" denied \" +\n                  \"since automatic failover is not enabled\");\n        }\n        break;\n    }\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.ActiveStandbyElectorBasedElectorService.becomeStandby": {
                "code_before_change": "  public void becomeStandby() {\n    cancelDisconnectTimer();\n\n    try {\n      rmContext.getRMAdminService().transitionToStandby(req);\n    } catch (Exception e) {\n      LOG.error(\"RM could not transition to Standby\", e);\n    }\n  }",
                "code_after_change": "  public void becomeStandby() {\n    cancelDisconnectTimer();\n\n    try {\n      rm.getRMContext().getRMAdminService().transitionToStandby(req);\n    } catch (Exception e) {\n      LOG.error(\"RM could not transition to Standby\", e);\n    }\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.AdminService.transitionToStandby": {
                "code_before_change": "  public synchronized void transitionToStandby(\n      HAServiceProtocol.StateChangeRequestInfo reqInfo) throws IOException {\n    // call refreshAdminAcls before HA state transition\n    // for the case that adminAcls have been updated in previous active RM\n    try {\n      refreshAdminAcls(false);\n    } catch (YarnException ex) {\n      throw new ServiceFailedException(\"Can not execute refreshAdminAcls\", ex);\n    }\n    UserGroupInformation user = checkAccess(\"transitionToStandby\");\n    checkHaStateChange(reqInfo);\n    try {\n      rm.transitionToStandby(true);\n      RMAuditLogger.logSuccess(user.getShortUserName(),\n          \"transitionToStandby\", \"RM\");\n    } catch (Exception e) {\n      RMAuditLogger.logFailure(user.getShortUserName(), \"transitionToStandby\",\n          \"\", \"RM\",\n          \"Exception transitioning to standby\");\n      throw new ServiceFailedException(\n          \"Error when transitioning to Standby mode\", e);\n    }\n  }",
                "code_after_change": "  public synchronized void transitionToStandby(\n      HAServiceProtocol.StateChangeRequestInfo reqInfo) throws IOException {\n    // call refreshAdminAcls before HA state transition\n    // for the case that adminAcls have been updated in previous active RM\n    try {\n      refreshAdminAcls(false);\n    } catch (YarnException ex) {\n      throw new ServiceFailedException(\"Can not execute refreshAdminAcls\", ex);\n    }\n    UserGroupInformation user = checkAccess(\"transitionToStandby\");\n    checkHaStateChange(reqInfo);\n    try {\n      rm.transitionToStandby(true);\n      RMAuditLogger.logSuccess(user.getShortUserName(),\n          \"transitionToStandby\", \"RM\");\n    } catch (Exception e) {\n      RMAuditLogger.logFailure(user.getShortUserName(), \"transitionToStandby\",\n          \"\", \"RM\",\n          \"Exception transitioning to standby\");\n      throw new ServiceFailedException(\n          \"Error when transitioning to Standby mode\", e);\n    }\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.AdminService.checkForDecommissioningNodes": {
                "code_before_change": "  public CheckForDecommissioningNodesResponse checkForDecommissioningNodes(\n      CheckForDecommissioningNodesRequest checkForDecommissioningNodesRequest)\n      throws IOException, YarnException {\n    final String operation = \"checkForDecommissioningNodes\";\n    final String msg = \"check for decommissioning nodes.\";\n    UserGroupInformation user = checkAcls(\"checkForDecommissioningNodes\");\n\n    checkRMStatus(user.getShortUserName(), operation, msg);\n\n    Set<NodeId> decommissioningNodes = rmContext.getNodesListManager()\n        .checkForDecommissioningNodes();\n    RMAuditLogger.logSuccess(user.getShortUserName(), operation,\n            \"AdminService\");\n    CheckForDecommissioningNodesResponse response = recordFactory\n        .newRecordInstance(CheckForDecommissioningNodesResponse.class);\n    response.setDecommissioningNodes(decommissioningNodes);\n    return response;\n  }",
                "code_after_change": "  public CheckForDecommissioningNodesResponse checkForDecommissioningNodes(\n      CheckForDecommissioningNodesRequest checkForDecommissioningNodesRequest)\n      throws IOException, YarnException {\n    final String operation = \"checkForDecommissioningNodes\";\n    final String msg = \"check for decommissioning nodes.\";\n    UserGroupInformation user = checkAcls(\"checkForDecommissioningNodes\");\n\n    checkRMStatus(user.getShortUserName(), operation, msg);\n\n    Set<NodeId> decommissioningNodes = rm.getRMContext().getNodesListManager()\n        .checkForDecommissioningNodes();\n    RMAuditLogger.logSuccess(user.getShortUserName(), operation,\n            \"AdminService\");\n    CheckForDecommissioningNodesResponse response = recordFactory\n        .newRecordInstance(CheckForDecommissioningNodesResponse.class);\n    response.setDecommissioningNodes(decommissioningNodes);\n    return response;\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.CuratorBasedElectorService.getZookeeperConnectionState": {
                "code_before_change": "  public String getZookeeperConnectionState() {\n    return \"Connected to zookeeper : \" +\n        curator.getZookeeperClient().isConnected();\n  }",
                "code_after_change": "  public String getZookeeperConnectionState() {\n    return \"Connected to zookeeper : \" +\n        curator.getZookeeperClient().isConnected();\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.RMActiveServiceContext.setRMAppLifetimeMonitor": {
                "code_before_change": "  public void setRMAppLifetimeMonitor(\n      RMAppLifetimeMonitor lifetimeMonitor) {\n    this.rmAppLifetimeMonitor = lifetimeMonitor;\n  }",
                "code_after_change": "  public void setRMAppLifetimeMonitor(\n      RMAppLifetimeMonitor lifetimeMonitor) {\n    this.rmAppLifetimeMonitor = lifetimeMonitor;\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.createSystemMetricsPublisher": {
                "code_before_change": "  protected SystemMetricsPublisher createSystemMetricsPublisher() {\n    SystemMetricsPublisher publisher;\n    if (YarnConfiguration.timelineServiceEnabled(conf) &&\n        YarnConfiguration.systemMetricsPublisherEnabled(conf)) {\n      if (YarnConfiguration.timelineServiceV2Enabled(conf)) {\n        // we're dealing with the v.2.x publisher\n        LOG.info(\"system metrics publisher with the timeline service V2 is \" +\n            \"configured\");\n        publisher = new TimelineServiceV2Publisher(rmContext);\n      } else {\n        // we're dealing with the v.1.x publisher\n        LOG.info(\"system metrics publisher with the timeline service V1 is \" +\n            \"configured\");\n        publisher = new TimelineServiceV1Publisher();\n      }\n    } else {\n      LOG.info(\"TimelineServicePublisher is not configured\");\n      publisher = new NoOpSystemMetricPublisher();\n    }\n    return publisher;\n  }",
                "code_after_change": "  protected SystemMetricsPublisher createSystemMetricsPublisher() {\n    SystemMetricsPublisher publisher;\n    if (YarnConfiguration.timelineServiceEnabled(conf) &&\n        YarnConfiguration.systemMetricsPublisherEnabled(conf)) {\n      if (YarnConfiguration.timelineServiceV2Enabled(conf)) {\n        // we're dealing with the v.2.x publisher\n        LOG.info(\"system metrics publisher with the timeline service V2 is \" +\n            \"configured\");\n        publisher = new TimelineServiceV2Publisher(\n            rmContext.getRMTimelineCollectorManager());\n      } else {\n        // we're dealing with the v.1.x publisher\n        LOG.info(\"system metrics publisher with the timeline service V1 is \" +\n            \"configured\");\n        publisher = new TimelineServiceV1Publisher();\n      }\n    } else {\n      LOG.info(\"TimelineServicePublisher is not configured\");\n      publisher = new NoOpSystemMetricPublisher();\n    }\n    return publisher;\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.CuratorBasedElectorService.closeLeaderLatch": {
                "code_before_change": "  private void closeLeaderLatch() throws IOException {\n    if (leaderLatch != null) {\n      leaderLatch.close();\n    }\n  }",
                "code_after_change": "  private void closeLeaderLatch() throws IOException {\n    if (leaderLatch != null) {\n      leaderLatch.close();\n    }\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.RMContextImpl.RMContextImpl": {
                "code_before_change": [],
                "code_after_change": []
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.AdminService.replaceLabelsOnNode": {
                "code_before_change": "  public ReplaceLabelsOnNodeResponse replaceLabelsOnNode(\n      ReplaceLabelsOnNodeRequest request) throws YarnException, IOException {\n    final String operation = \"replaceLabelsOnNode\";\n    final String msg = \"set node to labels.\";\n\n    try {\n      NodeLabelsUtils.verifyCentralizedNodeLabelConfEnabled(operation,\n          isCentralizedNodeLabelConfiguration);\n    } catch (IOException ioe) {\n      throw RPCUtil.getRemoteException(ioe);\n    }\n\n    UserGroupInformation user = checkAcls(operation);\n\n    checkRMStatus(user.getShortUserName(), operation, msg);\n\n    ReplaceLabelsOnNodeResponse response =\n        recordFactory.newRecordInstance(ReplaceLabelsOnNodeResponse.class);\n\n    if (request.getFailOnUnknownNodes()) {\n      // verify if nodes have registered to RM\n      List<NodeId> unknownNodes = new ArrayList<>();\n      for (NodeId requestedNode : request.getNodeToLabels().keySet()) {\n        boolean isKnown = false;\n        // both active and inactive nodes are recognized as known nodes\n        if (requestedNode.getPort() != 0) {\n          if (rmContext.getRMNodes().containsKey(requestedNode)\n              || rmContext.getInactiveRMNodes().containsKey(requestedNode)) {\n            isKnown = true;\n          }\n        } else {\n          for (NodeId knownNode : rmContext.getRMNodes().keySet()) {\n            if (knownNode.getHost().equals(requestedNode.getHost())) {\n              isKnown = true;\n              break;\n            }\n          }\n          if (!isKnown) {\n            for (NodeId knownNode : rmContext.getInactiveRMNodes().keySet()) {\n              if (knownNode.getHost().equals(requestedNode.getHost())) {\n                isKnown = true;\n                break;\n              }\n            }\n          }\n        }\n        if (!isKnown) {\n          unknownNodes.add(requestedNode);\n        }\n      }\n\n      if (!unknownNodes.isEmpty()) {\n        RMAuditLogger.logFailure(user.getShortUserName(), operation, \"\",\n            \"AdminService\",\n            \"Failed to replace labels as there are unknown nodes:\"\n                + Arrays.toString(unknownNodes.toArray()));\n        throw RPCUtil.getRemoteException(new IOException(\n            \"Failed to replace labels as there are unknown nodes:\"\n                + Arrays.toString(unknownNodes.toArray())));\n      }\n    }\n    try {\n      rmContext.getNodeLabelManager().replaceLabelsOnNode(\n          request.getNodeToLabels());\n      RMAuditLogger\n          .logSuccess(user.getShortUserName(), operation, \"AdminService\");\n      return response;\n    } catch (IOException ioe) {\n      throw logAndWrapException(ioe, user.getShortUserName(), operation, msg);\n    }\n  }",
                "code_after_change": "  public ReplaceLabelsOnNodeResponse replaceLabelsOnNode(\n      ReplaceLabelsOnNodeRequest request) throws YarnException, IOException {\n    final String operation = \"replaceLabelsOnNode\";\n    final String msg = \"set node to labels.\";\n\n    try {\n      NodeLabelsUtils.verifyCentralizedNodeLabelConfEnabled(operation,\n          isCentralizedNodeLabelConfiguration);\n    } catch (IOException ioe) {\n      throw RPCUtil.getRemoteException(ioe);\n    }\n\n    UserGroupInformation user = checkAcls(operation);\n\n    checkRMStatus(user.getShortUserName(), operation, msg);\n\n    ReplaceLabelsOnNodeResponse response =\n        recordFactory.newRecordInstance(ReplaceLabelsOnNodeResponse.class);\n\n    if (request.getFailOnUnknownNodes()) {\n      // verify if nodes have registered to RM\n      List<NodeId> unknownNodes = new ArrayList<>();\n      for (NodeId requestedNode : request.getNodeToLabels().keySet()) {\n        boolean isKnown = false;\n        // both active and inactive nodes are recognized as known nodes\n        if (requestedNode.getPort() != 0) {\n          if (rm.getRMContext().getRMNodes().containsKey(requestedNode) || rm\n              .getRMContext().getInactiveRMNodes().containsKey(requestedNode)) {\n            isKnown = true;\n          }\n        } else {\n          for (NodeId knownNode : rm.getRMContext().getRMNodes().keySet()) {\n            if (knownNode.getHost().equals(requestedNode.getHost())) {\n              isKnown = true;\n              break;\n            }\n          }\n          if (!isKnown) {\n            for (NodeId knownNode : rm.getRMContext().getInactiveRMNodes()\n                .keySet()) {\n              if (knownNode.getHost().equals(requestedNode.getHost())) {\n                isKnown = true;\n                break;\n              }\n            }\n          }\n        }\n        if (!isKnown) {\n          unknownNodes.add(requestedNode);\n        }\n      }\n\n      if (!unknownNodes.isEmpty()) {\n        RMAuditLogger.logFailure(user.getShortUserName(), operation, \"\",\n            \"AdminService\",\n            \"Failed to replace labels as there are unknown nodes:\"\n                + Arrays.toString(unknownNodes.toArray()));\n        throw RPCUtil.getRemoteException(new IOException(\n            \"Failed to replace labels as there are unknown nodes:\"\n                + Arrays.toString(unknownNodes.toArray())));\n      }\n    }\n    try {\n      rm.getRMContext().getNodeLabelManager().replaceLabelsOnNode(\n          request.getNodeToLabels());\n      RMAuditLogger\n          .logSuccess(user.getShortUserName(), operation, \"AdminService\");\n      return response;\n    } catch (IOException ioe) {\n      throw logAndWrapException(ioe, user.getShortUserName(), operation, msg);\n    }\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.AdminService.refreshClusterMaxPriority": {
                "code_before_change": "  public RefreshClusterMaxPriorityResponse refreshClusterMaxPriority(\n      RefreshClusterMaxPriorityRequest request) throws YarnException,\n      IOException {\n    final String operation = \"refreshClusterMaxPriority\";\n    final String msg = \"refresh cluster max priority\";\n    UserGroupInformation user = checkAcls(operation);\n\n    checkRMStatus(user.getShortUserName(), operation, msg);\n    try {\n      refreshClusterMaxPriority();\n\n      RMAuditLogger\n          .logSuccess(user.getShortUserName(), operation, \"AdminService\");\n      return recordFactory\n          .newRecordInstance(RefreshClusterMaxPriorityResponse.class);\n    } catch (YarnException e) {\n      throw logAndWrapException(e, user.getShortUserName(), operation, msg);\n    }\n  }",
                "code_after_change": "  public RefreshClusterMaxPriorityResponse refreshClusterMaxPriority(\n      RefreshClusterMaxPriorityRequest request) throws YarnException,\n      IOException {\n    final String operation = \"refreshClusterMaxPriority\";\n    final String msg = \"refresh cluster max priority\";\n    UserGroupInformation user = checkAcls(operation);\n\n    checkRMStatus(user.getShortUserName(), operation, msg);\n    try {\n      refreshClusterMaxPriority();\n\n      RMAuditLogger\n          .logSuccess(user.getShortUserName(), operation, \"AdminService\");\n      return recordFactory\n          .newRecordInstance(RefreshClusterMaxPriorityResponse.class);\n    } catch (YarnException e) {\n      throw logAndWrapException(e, user.getShortUserName(), operation, msg);\n    }\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.createApplicationMasterService": {
                "code_before_change": "  protected ApplicationMasterService createApplicationMasterService() {\n    Configuration config = this.rmContext.getYarnConfiguration();\n    if (YarnConfiguration.isOpportunisticContainerAllocationEnabled(config)\n        || YarnConfiguration.isDistSchedulingEnabled(config)) {\n      if (YarnConfiguration.isDistSchedulingEnabled(config) &&\n          !YarnConfiguration\n              .isOpportunisticContainerAllocationEnabled(config)) {\n        throw new YarnRuntimeException(\n            \"Invalid parameters: opportunistic container allocation has to \" +\n                \"be enabled when distributed scheduling is enabled.\");\n      }\n      OpportunisticContainerAllocatorAMService\n          oppContainerAllocatingAMService =\n          new OpportunisticContainerAllocatorAMService(this.rmContext,\n              scheduler);\n      EventDispatcher oppContainerAllocEventDispatcher =\n          new EventDispatcher(oppContainerAllocatingAMService,\n              OpportunisticContainerAllocatorAMService.class.getName());\n      // Add an event dispatcher for the\n      // OpportunisticContainerAllocatorAMService to handle node\n      // additions, updates and removals. Since the SchedulerEvent is currently\n      // a super set of theses, we register interest for it.\n      addService(oppContainerAllocEventDispatcher);\n      rmDispatcher.register(SchedulerEventType.class,\n          oppContainerAllocEventDispatcher);\n      this.rmContext.setContainerQueueLimitCalculator(\n          oppContainerAllocatingAMService.getNodeManagerQueueLimitCalculator());\n      return oppContainerAllocatingAMService;\n    }\n    return new ApplicationMasterService(this.rmContext, scheduler);\n  }",
                "code_after_change": "  protected ApplicationMasterService createApplicationMasterService() {\n    Configuration config = this.rmContext.getYarnConfiguration();\n    if (YarnConfiguration.isOpportunisticContainerAllocationEnabled(config)\n        || YarnConfiguration.isDistSchedulingEnabled(config)) {\n      if (YarnConfiguration.isDistSchedulingEnabled(config) &&\n          !YarnConfiguration\n              .isOpportunisticContainerAllocationEnabled(config)) {\n        throw new YarnRuntimeException(\n            \"Invalid parameters: opportunistic container allocation has to \" +\n                \"be enabled when distributed scheduling is enabled.\");\n      }\n      OpportunisticContainerAllocatorAMService\n          oppContainerAllocatingAMService =\n          new OpportunisticContainerAllocatorAMService(this.rmContext,\n              scheduler);\n      EventDispatcher oppContainerAllocEventDispatcher =\n          new EventDispatcher(oppContainerAllocatingAMService,\n              OpportunisticContainerAllocatorAMService.class.getName());\n      // Add an event dispatcher for the\n      // OpportunisticContainerAllocatorAMService to handle node\n      // additions, updates and removals. Since the SchedulerEvent is currently\n      // a super set of theses, we register interest for it.\n      addService(oppContainerAllocEventDispatcher);\n      rmDispatcher.register(SchedulerEventType.class,\n          oppContainerAllocEventDispatcher);\n      this.rmContext.setContainerQueueLimitCalculator(\n          oppContainerAllocatingAMService.getNodeManagerQueueLimitCalculator());\n      return oppContainerAllocatingAMService;\n    }\n    return new ApplicationMasterService(this.rmContext, scheduler);\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.RMContextImpl.setQueuePlacementManager": {
                "code_before_change": "  public void setQueuePlacementManager(PlacementManager placementMgr) {\n    this.activeServiceContext.setQueuePlacementManager(placementMgr);\n  }",
                "code_after_change": "  public void setQueuePlacementManager(PlacementManager placementMgr) {\n    this.activeServiceContext.setQueuePlacementManager(placementMgr);\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.AdminService.refreshNodes": {
                "code_before_change": "  public RefreshNodesResponse refreshNodes(RefreshNodesRequest request)\n      throws YarnException, StandbyException {\n    final String operation = \"refreshNodes\";\n    final String msg = \"refresh nodes.\";\n    UserGroupInformation user = checkAcls(\"refreshNodes\");\n\n    checkRMStatus(user.getShortUserName(), operation, msg);\n\n    try {\n      Configuration conf =\n          getConfiguration(new Configuration(false),\n              YarnConfiguration.YARN_SITE_CONFIGURATION_FILE);\n      switch (request.getDecommissionType()) {\n      case NORMAL:\n        rmContext.getNodesListManager().refreshNodes(conf);\n        break;\n      case GRACEFUL:\n        rmContext.getNodesListManager().refreshNodesGracefully(\n            conf, request.getDecommissionTimeout());\n        break;\n      case FORCEFUL:\n        rmContext.getNodesListManager().refreshNodesForcefully();\n        break;\n      }\n      RMAuditLogger.logSuccess(user.getShortUserName(), operation,\n          \"AdminService\");\n      return recordFactory.newRecordInstance(RefreshNodesResponse.class);\n    } catch (IOException ioe) {\n      throw logAndWrapException(ioe, user.getShortUserName(), operation, msg);\n    }\n  }",
                "code_after_change": "  public RefreshNodesResponse refreshNodes(RefreshNodesRequest request)\n      throws YarnException, StandbyException {\n    final String operation = \"refreshNodes\";\n    final String msg = \"refresh nodes.\";\n    UserGroupInformation user = checkAcls(\"refreshNodes\");\n\n    checkRMStatus(user.getShortUserName(), operation, msg);\n\n    try {\n      Configuration conf =\n          getConfiguration(new Configuration(false),\n              YarnConfiguration.YARN_SITE_CONFIGURATION_FILE);\n      switch (request.getDecommissionType()) {\n      case NORMAL:\n        rm.getRMContext().getNodesListManager().refreshNodes(conf);\n        break;\n      case GRACEFUL:\n        rm.getRMContext().getNodesListManager().refreshNodesGracefully(\n            conf, request.getDecommissionTimeout());\n        break;\n      case FORCEFUL:\n        rm.getRMContext().getNodesListManager().refreshNodesForcefully();\n        break;\n      }\n      RMAuditLogger.logSuccess(user.getShortUserName(), operation,\n          \"AdminService\");\n      return recordFactory.newRecordInstance(RefreshNodesResponse.class);\n    } catch (IOException ioe) {\n      throw logAndWrapException(ioe, user.getShortUserName(), operation, msg);\n    }\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.RMActiveServiceContext.isWorkPreservingRecoveryEnabled": {
                "code_before_change": "  public boolean isWorkPreservingRecoveryEnabled() {\n    return this.isWorkPreservingRecoveryEnabled;\n  }",
                "code_after_change": "  public boolean isWorkPreservingRecoveryEnabled() {\n    return this.isWorkPreservingRecoveryEnabled;\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.RMContextImpl.getClientToAMTokenSecretManager": {
                "code_before_change": "  public ClientToAMTokenSecretManagerInRM getClientToAMTokenSecretManager() {\n    return activeServiceContext.getClientToAMTokenSecretManager();\n  }",
                "code_after_change": "  public ClientToAMTokenSecretManagerInRM getClientToAMTokenSecretManager() {\n    return activeServiceContext.getClientToAMTokenSecretManager();\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.AdminService.refreshActiveServicesAcls": {
                "code_before_change": "  private void refreshActiveServicesAcls() throws IOException, YarnException  {\n    PolicyProvider policyProvider = RMPolicyProvider.getInstance();\n    Configuration conf =\n        getConfiguration(new Configuration(false),\n            YarnConfiguration.HADOOP_POLICY_CONFIGURATION_FILE);\n    rmContext.getClientRMService().refreshServiceAcls(conf, policyProvider);\n    rmContext.getApplicationMasterService().refreshServiceAcls(\n        conf, policyProvider);\n    rmContext.getResourceTrackerService().refreshServiceAcls(\n        conf, policyProvider);\n  }",
                "code_after_change": "  private void refreshActiveServicesAcls() throws IOException, YarnException  {\n    PolicyProvider policyProvider = RMPolicyProvider.getInstance();\n    Configuration conf =\n        getConfiguration(new Configuration(false),\n            YarnConfiguration.HADOOP_POLICY_CONFIGURATION_FILE);\n    rm.getRMContext().getClientRMService().refreshServiceAcls(conf,\n        policyProvider);\n    rm.getRMContext().getApplicationMasterService().refreshServiceAcls(\n        conf, policyProvider);\n    rm.getRMContext().getResourceTrackerService().refreshServiceAcls(\n        conf, policyProvider);\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.AdminService.removeFromClusterNodeLabels": {
                "code_before_change": "  public RemoveFromClusterNodeLabelsResponse removeFromClusterNodeLabels(\n      RemoveFromClusterNodeLabelsRequest request) throws YarnException, IOException {\n    final String operation = \"removeFromClusterNodeLabels\";\n    final String msg = \"remove labels.\";\n\n    UserGroupInformation user = checkAcls(operation);\n\n    checkRMStatus(user.getShortUserName(), operation, msg);\n\n    RemoveFromClusterNodeLabelsResponse response =\n        recordFactory.newRecordInstance(RemoveFromClusterNodeLabelsResponse.class);\n    try {\n      rmContext.getNodeLabelManager().removeFromClusterNodeLabels(request.getNodeLabels());\n      RMAuditLogger\n          .logSuccess(user.getShortUserName(), operation, \"AdminService\");\n      return response;\n    } catch (IOException ioe) {\n      throw logAndWrapException(ioe, user.getShortUserName(), operation, msg);\n    }\n  }",
                "code_after_change": "  public RemoveFromClusterNodeLabelsResponse removeFromClusterNodeLabels(\n      RemoveFromClusterNodeLabelsRequest request) throws YarnException, IOException {\n    final String operation = \"removeFromClusterNodeLabels\";\n    final String msg = \"remove labels.\";\n\n    UserGroupInformation user = checkAcls(operation);\n\n    checkRMStatus(user.getShortUserName(), operation, msg);\n\n    RemoveFromClusterNodeLabelsResponse response =\n        recordFactory.newRecordInstance(RemoveFromClusterNodeLabelsResponse.class);\n    try {\n      rm.getRMContext().getNodeLabelManager()\n          .removeFromClusterNodeLabels(request.getNodeLabels());\n      RMAuditLogger\n          .logSuccess(user.getShortUserName(), operation, \"AdminService\");\n      return response;\n    } catch (IOException ioe) {\n      throw logAndWrapException(ioe, user.getShortUserName(), operation, msg);\n    }\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.RMContextImpl.getResourceTrackerService": {
                "code_before_change": "  public ResourceTrackerService getResourceTrackerService() {\n    return activeServiceContext.getResourceTrackerService();\n  }",
                "code_after_change": "  public ResourceTrackerService getResourceTrackerService() {\n    return activeServiceContext.getResourceTrackerService();\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.ActiveStandbyElectorBasedElectorService.run": {
                "code_before_change": "          public void run() {\n            synchronized (zkDisconnectLock) {\n              // Only run if the timer hasn't been cancelled\n              if (zkDisconnectTimer != null) {\n                becomeStandby();\n              }\n            }\n          }",
                "code_after_change": "          public void run() {\n            synchronized (zkDisconnectLock) {\n              // Only run if the timer hasn't been cancelled\n              if (zkDisconnectTimer != null) {\n                becomeStandby();\n              }\n            }\n          }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.AdminService.refreshQueues": {
                "code_before_change": "  public RefreshQueuesResponse refreshQueues(RefreshQueuesRequest request)\n      throws YarnException, StandbyException {\n    final String operation = \"refreshQueues\";\n    final String msg = \"refresh queues.\";\n    UserGroupInformation user = checkAcls(operation);\n\n    checkRMStatus(user.getShortUserName(), operation, msg);\n\n    RefreshQueuesResponse response =\n        recordFactory.newRecordInstance(RefreshQueuesResponse.class);\n    try {\n      refreshQueues();\n      RMAuditLogger.logSuccess(user.getShortUserName(), operation,\n          \"AdminService\");\n      return response;\n    } catch (IOException ioe) {\n      throw logAndWrapException(ioe, user.getShortUserName(), operation, msg);\n    }\n  }",
                "code_after_change": "  public RefreshQueuesResponse refreshQueues(RefreshQueuesRequest request)\n      throws YarnException, StandbyException {\n    final String operation = \"refreshQueues\";\n    final String msg = \"refresh queues.\";\n    UserGroupInformation user = checkAcls(operation);\n\n    checkRMStatus(user.getShortUserName(), operation, msg);\n\n    RefreshQueuesResponse response =\n        recordFactory.newRecordInstance(RefreshQueuesResponse.class);\n    try {\n      refreshQueues();\n      RMAuditLogger.logSuccess(user.getShortUserName(), operation,\n          \"AdminService\");\n      return response;\n    } catch (IOException ioe) {\n      throw logAndWrapException(ioe, user.getShortUserName(), operation, msg);\n    }\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Buggy Method"
            },
            "fix_suggestion": "Correct",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the issue with the ordering of operations during failover, specifically mentioning the 'ResourceManager.reinitialize' method, which is a ground truth method. However, it does not precisely identify the root cause in the exact method where the fix was made, hence classified as 'Partial' with 'Buggy Method' sub-category. The fix suggestion in the bug report matches the developer's fix, which involves modifying the order of operations in the 'ResourceManager.reinitialize' method. The problem location identification is precise as it mentions 'ResourceManager.reinitialize', which is a ground truth method. There is no wrong information in the bug report as all details are relevant to the context of the bug."
        }
    },
    {
        "filename": "YARN-8409.json",
        "code_diff": {
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ha.ZKFailoverController.printUsage": {
                "code_before_change": "  private void printUsage() {\n    System.err.println(USAGE + \"\\n\");\n  }",
                "code_after_change": "  private void printUsage() {\n    System.err.println(USAGE + \"\\n\");\n  }"
            },
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ha.ActiveStandbyElector.parentZNodeExists": {
                "code_before_change": "  public synchronized boolean parentZNodeExists()\n      throws IOException, InterruptedException {\n    Preconditions.checkState(zkClient != null);\n    try {\n      return zkClient.exists(znodeWorkingDir, false) != null;\n    } catch (KeeperException e) {\n      throw new IOException(\"Couldn't determine existence of znode '\" +\n          znodeWorkingDir + \"'\", e);\n    }\n  }",
                "code_after_change": "  public synchronized boolean parentZNodeExists()\n      throws IOException, InterruptedException {\n    Preconditions.checkState(zkClient != null);\n    try {\n      return zkClient.exists(znodeWorkingDir, false) != null;\n    } catch (KeeperException e) {\n      throw new IOException(\"Couldn't determine existence of znode '\" +\n          znodeWorkingDir + \"'\", e);\n    }\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Same Class or Module"
            },
            "fix_suggestion": "Preventive",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Same Class or Module"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the root cause as a NullPointerException in the ActiveStandbyElector class, which is in the same module as the ground truth method ActiveStandbyElector.parentZNodeExists. However, it does not precisely identify the ground truth method. The fix suggestion is preventive as it suggests checking for uninitialized variables, which could prevent the NPE, but it does not match the developer's fix. The problem location is partially identified as it mentions methods in the same class/module as the ground truth methods. There is no wrong information in the bug report as all details are relevant to the context of the bug."
        }
    },
    {
        "filename": "YARN-8223.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.AuxServices.serviceInit": {
                "code_before_change": "  public void serviceInit(Configuration conf) throws Exception {\n    final FsPermission storeDirPerms = new FsPermission((short)0700);\n    Path stateStoreRoot = null;\n    FileSystem stateStoreFs = null;\n    boolean recoveryEnabled = conf.getBoolean(\n        YarnConfiguration.NM_RECOVERY_ENABLED,\n        YarnConfiguration.DEFAULT_NM_RECOVERY_ENABLED);\n    if (recoveryEnabled) {\n      stateStoreRoot = new Path(conf.get(YarnConfiguration.NM_RECOVERY_DIR),\n          STATE_STORE_ROOT_NAME);\n      stateStoreFs = FileSystem.getLocal(conf);\n    }\n    Collection<String> auxNames = conf.getStringCollection(\n        YarnConfiguration.NM_AUX_SERVICES);\n    for (final String sName : auxNames) {\n      try {\n        Preconditions\n            .checkArgument(\n                validateAuxServiceName(sName),\n                \"The ServiceName: \" + sName + \" set in \" +\n                YarnConfiguration.NM_AUX_SERVICES +\" is invalid.\" +\n                \"The valid service name should only contain a-zA-Z0-9_ \" +\n                \"and can not start with numbers\");\n        String classKey = String.format(\n            YarnConfiguration.NM_AUX_SERVICE_FMT, sName);\n        String className = conf.get(classKey);\n        final String appLocalClassPath = conf.get(String.format(\n            YarnConfiguration.NM_AUX_SERVICES_CLASSPATH, sName));\n        final String appRemoteClassPath = conf.get(String.format(\n            YarnConfiguration.NM_AUX_SERVICE_REMOTE_CLASSPATH, sName));\n        AuxiliaryService s = null;\n        boolean useCustomerClassLoader = ((appLocalClassPath != null\n            && !appLocalClassPath.isEmpty()) ||\n            (appRemoteClassPath != null && !appRemoteClassPath.isEmpty()))\n            && className != null && !className.isEmpty();\n        if (useCustomerClassLoader) {\n          // load AuxiliaryService from local class path\n          if (appRemoteClassPath == null || appRemoteClassPath.isEmpty()) {\n            s = AuxiliaryServiceWithCustomClassLoader.getInstance(\n                conf, className, appLocalClassPath);\n          } else {\n            // load AuxiliaryService from remote class path\n            if (appLocalClassPath != null && !appLocalClassPath.isEmpty()) {\n              throw new YarnRuntimeException(\"The aux serivce:\" + sName\n                  + \" has configured local classpath:\" + appLocalClassPath\n                  + \" and remote classpath:\" + appRemoteClassPath\n                  + \". Only one of them should be configured.\");\n            }\n            FileContext localLFS = getLocalFileContext(conf);\n            // create NM aux-service dir in NM localdir if it does not exist.\n            Path nmAuxDir = dirsHandler.getLocalPathForWrite(\".\"\n                + Path.SEPARATOR + NM_AUX_SERVICE_DIR);\n            if (!localLFS.util().exists(nmAuxDir)) {\n              try {\n                localLFS.mkdir(nmAuxDir, NM_AUX_SERVICE_DIR_PERM, true);\n              } catch (IOException ex) {\n                throw new YarnRuntimeException(\"Fail to create dir:\"\n                    + nmAuxDir.toString(), ex);\n              }\n            }\n            Path src = new Path(appRemoteClassPath);\n            FileContext remoteLFS = getRemoteFileContext(src.toUri(), conf);\n            FileStatus scFileStatus = remoteLFS.getFileStatus(src);\n            if (!scFileStatus.getOwner().equals(\n                this.userUGI.getShortUserName())) {\n              throw new YarnRuntimeException(\"The remote jarfile owner:\"\n                  + scFileStatus.getOwner() + \" is not the same as the NM user:\"\n                  + this.userUGI.getShortUserName() + \".\");\n            }\n            if ((scFileStatus.getPermission().toShort() & 0022) != 0) {\n              throw new YarnRuntimeException(\"The remote jarfile should not \"\n                  + \"be writable by group or others. \"\n                  + \"The current Permission is \"\n                  + scFileStatus.getPermission().toShort());\n            }\n            Path dest = null;\n            Path downloadDest = new Path(nmAuxDir,\n                className + \"_\" + scFileStatus.getModificationTime());\n            // check whether we need to re-download the jar\n            // from remote directory\n            Path targetDirPath = new Path(downloadDest,\n                scFileStatus.getPath().getName());\n            FileStatus[] allSubDirs = localLFS.util().listStatus(nmAuxDir);\n            boolean reDownload = true;\n            for (FileStatus sub : allSubDirs) {\n              if (sub.getPath().getName().equals(downloadDest.getName())) {\n                reDownload = false;\n                dest = new Path(targetDirPath + Path.SEPARATOR + \"*\");\n                break;\n              } else {\n                if (sub.getPath().getName().contains(className) &&\n                    !sub.getPath().getName().endsWith(DEL_SUFFIX)) {\n                  Path delPath = new Path(sub.getPath().getParent(),\n                      sub.getPath().getName() + DEL_SUFFIX);\n                  localLFS.rename(sub.getPath(), delPath);\n                  LOG.info(\"delete old aux service jar dir:\"\n                      + delPath.toString());\n                  FileDeletionTask deletionTask = new FileDeletionTask(\n                      this.delService, null, delPath, null);\n                  this.delService.delete(deletionTask);\n                }\n              }\n            }\n            if (reDownload) {\n              LocalResource scRsrc = LocalResource.newInstance(\n                  URL.fromURI(src.toUri()),\n                  LocalResourceType.ARCHIVE, LocalResourceVisibility.PRIVATE,\n                  scFileStatus.getLen(), scFileStatus.getModificationTime());\n              FSDownload download = new FSDownload(localLFS, null, conf,\n                  downloadDest, scRsrc, null);\n              try {\n                Path downloaded = download.call();\n                dest = new Path(downloaded + Path.SEPARATOR + \"*\");\n              } catch (Exception ex) {\n                throw new YarnRuntimeException(\n                    \"Exception happend while downloading files \"\n                    + \"for aux-service:\" + sName + \" and remote-file-path:\"\n                    + src + \".\\n\" + ex.getMessage());\n              }\n            }\n            s = AuxiliaryServiceWithCustomClassLoader.getInstance(\n                conf, className, dest.toString());\n          }\n          LOG.info(\"The aux service:\" + sName\n              + \" are using the custom classloader\");\n        } else {\n          Class<? extends AuxiliaryService> sClass = conf.getClass(\n              classKey, null, AuxiliaryService.class);\n\n          if (sClass == null) {\n            throw new RuntimeException(\"No class defined for \" + sName);\n          }\n          s = ReflectionUtils.newInstance(sClass, conf);\n        }\n        if (s == null) {\n          throw new RuntimeException(\"No object created for \" + sName);\n        }\n        // TODO better use s.getName()?\n        if(!sName.equals(s.getName())) {\n          LOG.warn(\"The Auxiliary Service named '\"+sName+\"' in the \"\n              +\"configuration is for \"+s.getClass()+\" which has \"\n              +\"a name of '\"+s.getName()+\"'. Because these are \"\n              +\"not the same tools trying to send ServiceData and read \"\n              +\"Service Meta Data may have issues unless the refer to \"\n              +\"the name in the config.\");\n        }\n        s.setAuxiliaryLocalPathHandler(auxiliaryLocalPathHandler);\n        addService(sName, s);\n        if (recoveryEnabled) {\n          Path storePath = new Path(stateStoreRoot, sName);\n          stateStoreFs.mkdirs(storePath, storeDirPerms);\n          s.setRecoveryPath(storePath);\n        }\n        s.init(conf);\n      } catch (RuntimeException e) {\n        LOG.error(\"Failed to initialize \" + sName, e);\n        throw e;\n      }\n    }\n    super.serviceInit(conf);\n  }",
                "code_after_change": "  public void serviceInit(Configuration conf) throws Exception {\n    final FsPermission storeDirPerms = new FsPermission((short)0700);\n    Path stateStoreRoot = null;\n    FileSystem stateStoreFs = null;\n    boolean recoveryEnabled = conf.getBoolean(\n        YarnConfiguration.NM_RECOVERY_ENABLED,\n        YarnConfiguration.DEFAULT_NM_RECOVERY_ENABLED);\n    if (recoveryEnabled) {\n      stateStoreRoot = new Path(conf.get(YarnConfiguration.NM_RECOVERY_DIR),\n          STATE_STORE_ROOT_NAME);\n      stateStoreFs = FileSystem.getLocal(conf);\n    }\n    Collection<String> auxNames = conf.getStringCollection(\n        YarnConfiguration.NM_AUX_SERVICES);\n    for (final String sName : auxNames) {\n      try {\n        Preconditions\n            .checkArgument(\n                validateAuxServiceName(sName),\n                \"The ServiceName: \" + sName + \" set in \" +\n                YarnConfiguration.NM_AUX_SERVICES +\" is invalid.\" +\n                \"The valid service name should only contain a-zA-Z0-9_ \" +\n                \"and can not start with numbers\");\n        String classKey = String.format(\n            YarnConfiguration.NM_AUX_SERVICE_FMT, sName);\n        String className = conf.get(classKey);\n        final String appLocalClassPath = conf.get(String.format(\n            YarnConfiguration.NM_AUX_SERVICES_CLASSPATH, sName));\n        final String appRemoteClassPath = conf.get(String.format(\n            YarnConfiguration.NM_AUX_SERVICE_REMOTE_CLASSPATH, sName));\n        AuxiliaryService s = null;\n        boolean useCustomerClassLoader = ((appLocalClassPath != null\n            && !appLocalClassPath.isEmpty()) ||\n            (appRemoteClassPath != null && !appRemoteClassPath.isEmpty()))\n            && className != null && !className.isEmpty();\n        if (useCustomerClassLoader) {\n          // load AuxiliaryService from local class path\n          if (appRemoteClassPath == null || appRemoteClassPath.isEmpty()) {\n            s = AuxiliaryServiceWithCustomClassLoader.getInstance(\n                conf, className, appLocalClassPath);\n          } else {\n            // load AuxiliaryService from remote class path\n            if (appLocalClassPath != null && !appLocalClassPath.isEmpty()) {\n              throw new YarnRuntimeException(\"The aux serivce:\" + sName\n                  + \" has configured local classpath:\" + appLocalClassPath\n                  + \" and remote classpath:\" + appRemoteClassPath\n                  + \". Only one of them should be configured.\");\n            }\n            FileContext localLFS = getLocalFileContext(conf);\n            // create NM aux-service dir in NM localdir if it does not exist.\n            Path nmAuxDir = dirsHandler.getLocalPathForWrite(\".\"\n                + Path.SEPARATOR + NM_AUX_SERVICE_DIR);\n            if (!localLFS.util().exists(nmAuxDir)) {\n              try {\n                localLFS.mkdir(nmAuxDir, NM_AUX_SERVICE_DIR_PERM, true);\n              } catch (IOException ex) {\n                throw new YarnRuntimeException(\"Fail to create dir:\"\n                    + nmAuxDir.toString(), ex);\n              }\n            }\n            Path src = new Path(appRemoteClassPath);\n            FileContext remoteLFS = getRemoteFileContext(src.toUri(), conf);\n            FileStatus scFileStatus = remoteLFS.getFileStatus(src);\n            if (!scFileStatus.getOwner().equals(\n                this.userUGI.getShortUserName())) {\n              throw new YarnRuntimeException(\"The remote jarfile owner:\"\n                  + scFileStatus.getOwner() + \" is not the same as the NM user:\"\n                  + this.userUGI.getShortUserName() + \".\");\n            }\n            if ((scFileStatus.getPermission().toShort() & 0022) != 0) {\n              throw new YarnRuntimeException(\"The remote jarfile should not \"\n                  + \"be writable by group or others. \"\n                  + \"The current Permission is \"\n                  + scFileStatus.getPermission().toShort());\n            }\n            Path dest = null;\n            Path downloadDest = new Path(nmAuxDir,\n                className + \"_\" + scFileStatus.getModificationTime());\n            // check whether we need to re-download the jar\n            // from remote directory\n            Path targetDirPath = new Path(downloadDest,\n                scFileStatus.getPath().getName());\n            FileStatus[] allSubDirs = localLFS.util().listStatus(nmAuxDir);\n            boolean reDownload = true;\n            for (FileStatus sub : allSubDirs) {\n              if (sub.getPath().getName().equals(downloadDest.getName())) {\n                reDownload = false;\n                dest = new Path(targetDirPath + Path.SEPARATOR + \"*\");\n                break;\n              } else {\n                if (sub.getPath().getName().contains(className) &&\n                    !sub.getPath().getName().endsWith(DEL_SUFFIX)) {\n                  Path delPath = new Path(sub.getPath().getParent(),\n                      sub.getPath().getName() + DEL_SUFFIX);\n                  localLFS.rename(sub.getPath(), delPath);\n                  LOG.info(\"delete old aux service jar dir:\"\n                      + delPath.toString());\n                  FileDeletionTask deletionTask = new FileDeletionTask(\n                      this.delService, null, delPath, null);\n                  this.delService.delete(deletionTask);\n                }\n              }\n            }\n            if (reDownload) {\n              LocalResourceType srcType = null;\n              String lowerDst = StringUtils.toLowerCase(src.toString());\n              if (lowerDst.endsWith(\".jar\")) {\n                srcType = LocalResourceType.FILE;\n              } else if (lowerDst.endsWith(\".zip\") ||\n                  lowerDst.endsWith(\".tar.gz\") || lowerDst.endsWith(\".tgz\")\n                  || lowerDst.endsWith(\".tar\")) {\n                srcType = LocalResourceType.ARCHIVE;\n              } else {\n                throw new YarnRuntimeException(\n                    \"Can not unpack file from remote-file-path:\" + src\n                        + \"for aux-service:\" + \".\\n\");\n              }\n              LocalResource scRsrc = LocalResource.newInstance(\n                  URL.fromURI(src.toUri()),\n                  srcType, LocalResourceVisibility.PRIVATE,\n                  scFileStatus.getLen(), scFileStatus.getModificationTime());\n              FSDownload download = new FSDownload(localLFS, null, conf,\n                  downloadDest, scRsrc, null);\n              try {\n                Path downloaded = download.call();\n                // don't need to convert downloaded path into a dir\n                // since its already a jar path.\n                dest = downloaded;\n              } catch (Exception ex) {\n                throw new YarnRuntimeException(\n                    \"Exception happend while downloading files \"\n                    + \"for aux-service:\" + sName + \" and remote-file-path:\"\n                    + src + \".\\n\" + ex.getMessage());\n              }\n            }\n            s = AuxiliaryServiceWithCustomClassLoader.getInstance(\n                conf, className, dest.toString());\n          }\n          LOG.info(\"The aux service:\" + sName\n              + \" are using the custom classloader\");\n        } else {\n          Class<? extends AuxiliaryService> sClass = conf.getClass(\n              classKey, null, AuxiliaryService.class);\n\n          if (sClass == null) {\n            throw new RuntimeException(\"No class defined for \" + sName);\n          }\n          s = ReflectionUtils.newInstance(sClass, conf);\n        }\n        if (s == null) {\n          throw new RuntimeException(\"No object created for \" + sName);\n        }\n        // TODO better use s.getName()?\n        if(!sName.equals(s.getName())) {\n          LOG.warn(\"The Auxiliary Service named '\"+sName+\"' in the \"\n              +\"configuration is for \"+s.getClass()+\" which has \"\n              +\"a name of '\"+s.getName()+\"'. Because these are \"\n              +\"not the same tools trying to send ServiceData and read \"\n              +\"Service Meta Data may have issues unless the refer to \"\n              +\"the name in the config.\");\n        }\n        s.setAuxiliaryLocalPathHandler(auxiliaryLocalPathHandler);\n        addService(sName, s);\n        if (recoveryEnabled) {\n          Path storePath = new Path(stateStoreRoot, sName);\n          stateStoreFs.mkdirs(storePath, storeDirPerms);\n          s.setRecoveryPath(storePath);\n        }\n        s.init(conf);\n      } catch (RuntimeException e) {\n        LOG.error(\"Failed to initialize \" + sName, e);\n        throw e;\n      }\n    }\n    super.serviceInit(conf);\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Correct",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report precisely identifies the root cause as a ClassNotFoundException due to an empty classpath when loading from HDFS, which aligns with the ground truth method 'AuxServices.serviceInit'. The fix suggestion to ensure the correct classpath for HDFS is included in the configuration matches the developer's fix, which involved handling the classpath correctly. The problem location is precisely identified as 'AuxServices.serviceInit', which is the ground truth method. There is no wrong information in the bug report as all details are relevant and accurate."
        }
    },
    {
        "filename": "YARN-8331.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainersLauncher.handle": {
                "code_before_change": "  public void handle(ContainersLauncherEvent event) {\n    // TODO: ContainersLauncher launches containers one by one!!\n    Container container = event.getContainer();\n    ContainerId containerId = container.getContainerId();\n    switch (event.getType()) {\n      case LAUNCH_CONTAINER:\n        Application app =\n          context.getApplications().get(\n              containerId.getApplicationAttemptId().getApplicationId());\n\n        ContainerLaunch launch =\n            new ContainerLaunch(context, getConfig(), dispatcher, exec, app,\n              event.getContainer(), dirsHandler, containerManager);\n        containerLauncher.submit(launch);\n        running.put(containerId, launch);\n        break;\n      case RELAUNCH_CONTAINER:\n        app = context.getApplications().get(\n                containerId.getApplicationAttemptId().getApplicationId());\n\n        ContainerRelaunch relaunch =\n            new ContainerRelaunch(context, getConfig(), dispatcher, exec, app,\n                event.getContainer(), dirsHandler, containerManager);\n        containerLauncher.submit(relaunch);\n        running.put(containerId, relaunch);\n        break;\n      case RECOVER_CONTAINER:\n        app = context.getApplications().get(\n            containerId.getApplicationAttemptId().getApplicationId());\n        launch = new RecoveredContainerLaunch(context, getConfig(), dispatcher,\n            exec, app, event.getContainer(), dirsHandler, containerManager);\n        containerLauncher.submit(launch);\n        running.put(containerId, launch);\n        break;\n      case RECOVER_PAUSED_CONTAINER:\n        app = context.getApplications().get(\n            containerId.getApplicationAttemptId().getApplicationId());\n        launch = new RecoverPausedContainerLaunch(context, getConfig(),\n            dispatcher, exec, app, event.getContainer(), dirsHandler,\n            containerManager);\n        containerLauncher.submit(launch);\n        break;\n      case CLEANUP_CONTAINER:\n      case CLEANUP_CONTAINER_FOR_REINIT:\n        ContainerLaunch launcher = running.remove(containerId);\n        if (launcher == null) {\n          // Container not launched. So nothing needs to be done.\n          return;\n        }\n\n        // Cleanup a container whether it is running/killed/completed, so that\n        // no sub-processes are alive.\n        try {\n          launcher.cleanupContainer();\n        } catch (IOException e) {\n          LOG.warn(\"Got exception while cleaning container \" + containerId\n              + \". Ignoring.\");\n        }\n        break;\n      case SIGNAL_CONTAINER:\n        SignalContainersLauncherEvent signalEvent =\n            (SignalContainersLauncherEvent) event;\n        ContainerLaunch runningContainer = running.get(containerId);\n        if (runningContainer == null) {\n          // Container not launched. So nothing needs to be done.\n          LOG.info(\"Container \" + containerId + \" not running, nothing to signal.\");\n          return;\n        }\n\n        try {\n          runningContainer.signalContainer(signalEvent.getCommand());\n        } catch (IOException e) {\n          LOG.warn(\"Got exception while signaling container \" + containerId\n              + \" with command \" + signalEvent.getCommand());\n        }\n        break;\n      case PAUSE_CONTAINER:\n        ContainerLaunch launchedContainer = running.get(containerId);\n        if (launchedContainer == null) {\n          // Container not launched. So nothing needs to be done.\n          return;\n        }\n\n        // Pause the container\n        try {\n          launchedContainer.pauseContainer();\n        } catch (Exception e) {\n          LOG.info(\"Got exception while pausing container: \" +\n            StringUtils.stringifyException(e));\n        }\n        break;\n      case RESUME_CONTAINER:\n        ContainerLaunch launchCont = running.get(containerId);\n        if (launchCont == null) {\n          // Container not launched. So nothing needs to be done.\n          return;\n        }\n\n        // Resume the container.\n        try {\n          launchCont.resumeContainer();\n        } catch (Exception e) {\n          LOG.info(\"Got exception while resuming container: \" +\n            StringUtils.stringifyException(e));\n        }\n        break;\n    }\n  }",
                "code_after_change": "  public void handle(ContainersLauncherEvent event) {\n    // TODO: ContainersLauncher launches containers one by one!!\n    Container container = event.getContainer();\n    ContainerId containerId = container.getContainerId();\n    switch (event.getType()) {\n      case LAUNCH_CONTAINER:\n        Application app =\n          context.getApplications().get(\n              containerId.getApplicationAttemptId().getApplicationId());\n\n        ContainerLaunch launch =\n            new ContainerLaunch(context, getConfig(), dispatcher, exec, app,\n              event.getContainer(), dirsHandler, containerManager);\n        containerLauncher.submit(launch);\n        running.put(containerId, launch);\n        break;\n      case RELAUNCH_CONTAINER:\n        app = context.getApplications().get(\n                containerId.getApplicationAttemptId().getApplicationId());\n\n        ContainerRelaunch relaunch =\n            new ContainerRelaunch(context, getConfig(), dispatcher, exec, app,\n                event.getContainer(), dirsHandler, containerManager);\n        containerLauncher.submit(relaunch);\n        running.put(containerId, relaunch);\n        break;\n      case RECOVER_CONTAINER:\n        app = context.getApplications().get(\n            containerId.getApplicationAttemptId().getApplicationId());\n        launch = new RecoveredContainerLaunch(context, getConfig(), dispatcher,\n            exec, app, event.getContainer(), dirsHandler, containerManager);\n        containerLauncher.submit(launch);\n        running.put(containerId, launch);\n        break;\n      case RECOVER_PAUSED_CONTAINER:\n        app = context.getApplications().get(\n            containerId.getApplicationAttemptId().getApplicationId());\n        launch = new RecoverPausedContainerLaunch(context, getConfig(),\n            dispatcher, exec, app, event.getContainer(), dirsHandler,\n            containerManager);\n        containerLauncher.submit(launch);\n        break;\n      case CLEANUP_CONTAINER:\n      case CLEANUP_CONTAINER_FOR_REINIT:\n        ContainerLaunch launcher = running.remove(containerId);\n        if (launcher == null) {\n          // Container not launched.\n          // triggering KILLING to CONTAINER_CLEANEDUP_AFTER_KILL transition.\n          dispatcher.getEventHandler().handle(\n              new ContainerExitEvent(containerId,\n                  ContainerEventType.CONTAINER_KILLED_ON_REQUEST,\n                  Shell.WINDOWS ? ContainerExecutor.ExitCode.FORCE_KILLED.getExitCode() :\n                  ContainerExecutor.ExitCode.TERMINATED.getExitCode(),\n                  \"Container terminated before launch.\"));\n          return;\n        }\n\n        // Cleanup a container whether it is running/killed/completed, so that\n        // no sub-processes are alive.\n        try {\n          launcher.cleanupContainer();\n        } catch (IOException e) {\n          LOG.warn(\"Got exception while cleaning container \" + containerId\n              + \". Ignoring.\");\n        }\n        break;\n      case SIGNAL_CONTAINER:\n        SignalContainersLauncherEvent signalEvent =\n            (SignalContainersLauncherEvent) event;\n        ContainerLaunch runningContainer = running.get(containerId);\n        if (runningContainer == null) {\n          // Container not launched. So nothing needs to be done.\n          LOG.info(\"Container \" + containerId + \" not running, nothing to signal.\");\n          return;\n        }\n\n        try {\n          runningContainer.signalContainer(signalEvent.getCommand());\n        } catch (IOException e) {\n          LOG.warn(\"Got exception while signaling container \" + containerId\n              + \" with command \" + signalEvent.getCommand());\n        }\n        break;\n      case PAUSE_CONTAINER:\n        ContainerLaunch launchedContainer = running.get(containerId);\n        if (launchedContainer == null) {\n          // Container not launched. So nothing needs to be done.\n          return;\n        }\n\n        // Pause the container\n        try {\n          launchedContainer.pauseContainer();\n        } catch (Exception e) {\n          LOG.info(\"Got exception while pausing container: \" +\n            StringUtils.stringifyException(e));\n        }\n        break;\n      case RESUME_CONTAINER:\n        ContainerLaunch launchCont = running.get(containerId);\n        if (launchCont == null) {\n          // Container not launched. So nothing needs to be done.\n          return;\n        }\n\n        // Resume the container.\n        try {\n          launchCont.resumeContainer();\n        } catch (Exception e) {\n          LOG.info(\"Got exception while resuming container: \" +\n            StringUtils.stringifyException(e));\n        }\n        break;\n    }\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl.KillOnPauseTransition": {
                "code_before_change": [],
                "code_after_change": []
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.handleContainerExitCode": {
                "code_before_change": "  protected void handleContainerExitCode(int exitCode, Path containerLogDir) {\n    ContainerId containerId = container.getContainerId();\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Container \" + containerId + \" completed with exit code \"\n          + exitCode);\n    }\n\n    StringBuilder diagnosticInfo =\n        new StringBuilder(\"Container exited with a non-zero exit code \");\n    diagnosticInfo.append(exitCode);\n    diagnosticInfo.append(\". \");\n    if (exitCode == ExitCode.FORCE_KILLED.getExitCode()\n        || exitCode == ExitCode.TERMINATED.getExitCode()) {\n      // If the process was killed, Send container_cleanedup_after_kill and\n      // just break out of this method.\n\n      // If Container was killed before starting... NO need to do this.\n      if (!killedBeforeStart) {\n        dispatcher.getEventHandler().handle(\n            new ContainerExitEvent(containerId,\n                ContainerEventType.CONTAINER_KILLED_ON_REQUEST, exitCode,\n                diagnosticInfo.toString()));\n      }\n    } else if (exitCode != 0) {\n      handleContainerExitWithFailure(containerId, exitCode, containerLogDir,\n          diagnosticInfo);\n    } else {\n      LOG.info(\"Container \" + containerId + \" succeeded \");\n      dispatcher.getEventHandler().handle(\n          new ContainerEvent(containerId,\n              ContainerEventType.CONTAINER_EXITED_WITH_SUCCESS));\n    }\n  }",
                "code_after_change": "  protected void handleContainerExitCode(int exitCode, Path containerLogDir) {\n    ContainerId containerId = container.getContainerId();\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Container \" + containerId + \" completed with exit code \"\n          + exitCode);\n    }\n\n    StringBuilder diagnosticInfo =\n        new StringBuilder(\"Container exited with a non-zero exit code \");\n    diagnosticInfo.append(exitCode);\n    diagnosticInfo.append(\". \");\n    if (exitCode == ExitCode.FORCE_KILLED.getExitCode()\n        || exitCode == ExitCode.TERMINATED.getExitCode()) {\n      // If the process was killed, Send container_cleanedup_after_kill and\n      // just break out of this method.\n      dispatcher.getEventHandler().handle(\n          new ContainerExitEvent(containerId,\n              ContainerEventType.CONTAINER_KILLED_ON_REQUEST, exitCode,\n              diagnosticInfo.toString()));\n    } else if (exitCode != 0) {\n      handleContainerExitWithFailure(containerId, exitCode, containerLogDir,\n          diagnosticInfo);\n    } else {\n      LOG.info(\"Container \" + containerId + \" succeeded \");\n      dispatcher.getEventHandler().handle(\n          new ContainerEvent(containerId,\n              ContainerEventType.CONTAINER_EXITED_WITH_SUCCESS));\n    }\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl.LaunchTransition": {
                "code_before_change": [],
                "code_after_change": []
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Same Class or Module"
            },
            "fix_suggestion": "Preventive",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Same Class or Module"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the root cause as the CONTAINER_LAUNCHED event being sent to a container in the DONE state, which is not a valid state transition. This is related to the ContainerImpl class, which is in the same module as the ground truth methods, but it does not precisely identify the ground truth methods. The fix suggestion to add a condition in the handle method of ContainerImpl to check if the container is in the DONE state is preventive, as it would prevent the invalid state transition. The problem location mentions methods in the same class or module as the ground truth methods, but not the exact methods. There is no wrong information in the bug report as all the details are relevant to the context of the bug."
        }
    },
    {
        "filename": "YARN-2931.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService.addResource": {
                "code_before_change": "    public void addResource(LocalizerResourceRequestEvent request) {\n      // TODO handle failures, cancellation, requests by other containers\n      LocalizedResource rsrc = request.getResource();\n      LocalResourceRequest key = rsrc.getRequest();\n      LOG.info(\"Downloading public rsrc:\" + key);\n      /*\n       * Here multiple containers may request the same resource. So we need\n       * to start downloading only when\n       * 1) ResourceState == DOWNLOADING\n       * 2) We are able to acquire non blocking semaphore lock.\n       * If not we will skip this resource as either it is getting downloaded\n       * or it FAILED / LOCALIZED.\n       */\n\n      if (rsrc.tryAcquire()) {\n        if (rsrc.getState().equals(ResourceState.DOWNLOADING)) {\n          LocalResource resource = request.getResource().getRequest();\n          try {\n            Path publicRootPath =\n                dirsHandler.getLocalPathForWrite(\".\" + Path.SEPARATOR\n                    + ContainerLocalizer.FILECACHE,\n                  ContainerLocalizer.getEstimatedSize(resource), true);\n            Path publicDirDestPath =\n                publicRsrc.getPathForLocalization(key, publicRootPath);\n            if (!publicDirDestPath.getParent().equals(publicRootPath)) {\n              DiskChecker.checkDir(new File(publicDirDestPath.toUri().getPath()));\n            }\n            // explicitly synchronize pending here to avoid future task\n            // completing and being dequeued before pending updated\n            synchronized (pending) {\n              pending.put(queue.submit(new FSDownload(lfs, null, conf,\n                  publicDirDestPath, resource, request.getContext().getStatCache())),\n                  request);\n            }\n          } catch (IOException e) {\n            rsrc.unlock();\n            publicRsrc.handle(new ResourceFailedLocalizationEvent(request\n              .getResource().getRequest(), e.getMessage()));\n            LOG.error(\"Local path for public localization is not found. \"\n                + \" May be disks failed.\", e);\n          } catch (RejectedExecutionException re) {\n            rsrc.unlock();\n            publicRsrc.handle(new ResourceFailedLocalizationEvent(request\n              .getResource().getRequest(), re.getMessage()));\n            LOG.error(\"Failed to submit rsrc \" + rsrc + \" for download.\"\n                + \" Either queue is full or threadpool is shutdown.\", re);\n          }\n        } else {\n          rsrc.unlock();\n        }\n      }\n    }",
                "code_after_change": "    public void addResource(LocalizerResourceRequestEvent request) {\n      // TODO handle failures, cancellation, requests by other containers\n      LocalizedResource rsrc = request.getResource();\n      LocalResourceRequest key = rsrc.getRequest();\n      LOG.info(\"Downloading public rsrc:\" + key);\n      /*\n       * Here multiple containers may request the same resource. So we need\n       * to start downloading only when\n       * 1) ResourceState == DOWNLOADING\n       * 2) We are able to acquire non blocking semaphore lock.\n       * If not we will skip this resource as either it is getting downloaded\n       * or it FAILED / LOCALIZED.\n       */\n\n      if (rsrc.tryAcquire()) {\n        if (rsrc.getState().equals(ResourceState.DOWNLOADING)) {\n          LocalResource resource = request.getResource().getRequest();\n          try {\n            Path publicRootPath =\n                dirsHandler.getLocalPathForWrite(\".\" + Path.SEPARATOR\n                    + ContainerLocalizer.FILECACHE,\n                  ContainerLocalizer.getEstimatedSize(resource), true);\n            Path publicDirDestPath =\n                publicRsrc.getPathForLocalization(key, publicRootPath);\n            if (!publicDirDestPath.getParent().equals(publicRootPath)) {\n              DiskChecker.checkDir(new File(publicDirDestPath.toUri().getPath()));\n            }\n\n            // In case this is not a newly initialized nm state, ensure\n            // initialized local/log dirs similar to LocalizerRunner\n            getInitializedLocalDirs();\n            getInitializedLogDirs();\n\n            // explicitly synchronize pending here to avoid future task\n            // completing and being dequeued before pending updated\n            synchronized (pending) {\n              pending.put(queue.submit(new FSDownload(lfs, null, conf,\n                  publicDirDestPath, resource, request.getContext().getStatCache())),\n                  request);\n            }\n          } catch (IOException e) {\n            rsrc.unlock();\n            publicRsrc.handle(new ResourceFailedLocalizationEvent(request\n              .getResource().getRequest(), e.getMessage()));\n            LOG.error(\"Local path for public localization is not found. \"\n                + \" May be disks failed.\", e);\n          } catch (RejectedExecutionException re) {\n            rsrc.unlock();\n            publicRsrc.handle(new ResourceFailedLocalizationEvent(request\n              .getResource().getRequest(), re.getMessage()));\n            LOG.error(\"Failed to submit rsrc \" + rsrc + \" for download.\"\n                + \" Either queue is full or threadpool is shutdown.\", re);\n          }\n        } else {\n          rsrc.unlock();\n        }\n      }\n    }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Preventive",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The root cause identification is partial because the bug report mentions the FileNotFoundException and the failure of the mkdirs method, which are related to the stack trace context but not the exact root cause in the ground truth method. The fix suggestion is preventive as it suggests ensuring directory creation and checking permissions, which would mitigate the issue but does not match the developer's fix. The problem location identification is partial because the methods mentioned in the problem_location field are related to the stack trace context but do not include the ground truth method. There is no wrong information in the bug report as all statements are relevant to the context of the bug."
        }
    },
    {
        "filename": "YARN-6837.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.api.records.impl.pb.ContainerLaunchContextPBImpl.checkLocalResources": {
                "code_before_change": "  private void checkLocalResources(Map<String, LocalResource> localResources) {\n    for (Map.Entry<String, LocalResource> rsrcEntry : localResources\n        .entrySet()) {\n      if (rsrcEntry.getValue() == null\n          || rsrcEntry.getValue().getResource() == null) {\n        throw new NullPointerException(\n            \"Null resource URL for local resource \" + rsrcEntry.getKey() + \" : \"\n                + rsrcEntry.getValue());\n      }\n    }\n  }",
                "code_after_change": "  private void checkLocalResources(Map<String, LocalResource> localResources) {\n    for (Map.Entry<String, LocalResource> rsrcEntry : localResources\n        .entrySet()) {\n      if (rsrcEntry.getValue() == null\n          || rsrcEntry.getValue().getResource() == null) {\n        throw new NullPointerException(\n            \"Null resource URL for local resource \" + rsrcEntry.getKey() + \" : \"\n                + rsrcEntry.getValue());\n      } else if (rsrcEntry.getValue().getType() == null) {\n        throw new NullPointerException(\n            \"Null resource type for local resource \" + rsrcEntry.getKey() + \" : \"\n                + rsrcEntry.getValue());\n      } else if (rsrcEntry.getValue().getVisibility() == null) {\n          throw new NullPointerException(\n            \"Null resource visibility for local resource \" + rsrcEntry.getKey() + \" : \"\n                + rsrcEntry.getValue());\n      }\n    }\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.startContainerInternal": {
                "code_before_change": "  protected void startContainerInternal(\n      ContainerTokenIdentifier containerTokenIdentifier,\n      StartContainerRequest request) throws YarnException, IOException {\n\n    ContainerId containerId = containerTokenIdentifier.getContainerID();\n    String containerIdStr = containerId.toString();\n    String user = containerTokenIdentifier.getApplicationSubmitter();\n\n    LOG.info(\"Start request for \" + containerIdStr + \" by user \" + user);\n\n    ContainerLaunchContext launchContext = request.getContainerLaunchContext();\n\n    // Sanity check for local resources\n    for (Map.Entry<String, LocalResource> rsrc : launchContext\n        .getLocalResources().entrySet()) {\n      if (rsrc.getValue() == null || rsrc.getValue().getResource() == null) {\n        throw new YarnException(\n            \"Null resource URL for local resource \" + rsrc.getKey() + \" : \" + rsrc.getValue());\n      }\n    }\n\n    Credentials credentials =\n        YarnServerSecurityUtils.parseCredentials(launchContext);\n\n    Container container =\n        new ContainerImpl(getConfig(), this.dispatcher,\n            launchContext, credentials, metrics, containerTokenIdentifier,\n            context);\n    ApplicationId applicationID =\n        containerId.getApplicationAttemptId().getApplicationId();\n    if (context.getContainers().putIfAbsent(containerId, container) != null) {\n      NMAuditLogger.logFailure(user, AuditConstants.START_CONTAINER,\n        \"ContainerManagerImpl\", \"Container already running on this node!\",\n        applicationID, containerId);\n      throw RPCUtil.getRemoteException(\"Container \" + containerIdStr\n          + \" already is running on this node!!\");\n    }\n\n    this.readLock.lock();\n    try {\n      if (!isServiceStopped()) {\n        if (!context.getApplications().containsKey(applicationID)) {\n          // Create the application\n          // populate the flow context from the launch context if the timeline\n          // service v.2 is enabled\n          FlowContext flowContext = null;\n          if (YarnConfiguration.timelineServiceV2Enabled(getConfig())) {\n            String flowName = launchContext.getEnvironment()\n                .get(TimelineUtils.FLOW_NAME_TAG_PREFIX);\n            String flowVersion = launchContext.getEnvironment()\n                .get(TimelineUtils.FLOW_VERSION_TAG_PREFIX);\n            String flowRunIdStr = launchContext.getEnvironment()\n                .get(TimelineUtils.FLOW_RUN_ID_TAG_PREFIX);\n            long flowRunId = 0L;\n            if (flowRunIdStr != null && !flowRunIdStr.isEmpty()) {\n              flowRunId = Long.parseLong(flowRunIdStr);\n            }\n            flowContext = new FlowContext(flowName, flowVersion, flowRunId);\n            if (LOG.isDebugEnabled()) {\n              LOG.debug(\"Flow context: \" + flowContext\n                  + \" created for an application \" + applicationID);\n            }\n          }\n\n          Application application =\n              new ApplicationImpl(dispatcher, user, flowContext,\n                  applicationID, credentials, context);\n          if (context.getApplications().putIfAbsent(applicationID,\n              application) == null) {\n            LOG.info(\"Creating a new application reference for app \"\n                + applicationID);\n            LogAggregationContext logAggregationContext =\n                containerTokenIdentifier.getLogAggregationContext();\n            Map<ApplicationAccessType, String> appAcls =\n                container.getLaunchContext().getApplicationACLs();\n            context.getNMStateStore().storeApplication(applicationID,\n                buildAppProto(applicationID, user, credentials, appAcls,\n                    logAggregationContext, flowContext));\n            dispatcher.getEventHandler().handle(new ApplicationInitEvent(\n                applicationID, appAcls, logAggregationContext));\n          }\n        }\n\n        this.context.getNMStateStore().storeContainer(containerId,\n            containerTokenIdentifier.getVersion(), request);\n        dispatcher.getEventHandler().handle(\n          new ApplicationContainerInitEvent(container));\n\n        this.context.getContainerTokenSecretManager().startContainerSuccessful(\n          containerTokenIdentifier);\n        NMAuditLogger.logSuccess(user, AuditConstants.START_CONTAINER,\n          \"ContainerManageImpl\", applicationID, containerId);\n        // TODO launchedContainer misplaced -> doesn't necessarily mean a container\n        // launch. A finished Application will not launch containers.\n        metrics.launchedContainer();\n        metrics.allocateContainer(containerTokenIdentifier.getResource());\n      } else {\n        throw new YarnException(\n            \"Container start failed as the NodeManager is \" +\n            \"in the process of shutting down\");\n      }\n    } finally {\n      this.readLock.unlock();\n    }\n  }",
                "code_after_change": "  private void startContainerInternal(NMTokenIdentifier nmTokenIdentifier,\n      ContainerTokenIdentifier containerTokenIdentifier,\n      StartContainerRequest request) throws YarnException, IOException {\n\n    /*\n     * 1) It should save the NMToken into NMTokenSecretManager. This is done\n     * here instead of RPC layer because at the time of opening/authenticating\n     * the connection it doesn't know what all RPC calls user will make on it.\n     * Also new NMToken is issued only at startContainer (once it gets renewed).\n     * \n     * 2) It should validate containerToken. Need to check below things. a) It\n     * is signed by correct master key (part of retrieve password). b) It\n     * belongs to correct Node Manager (part of retrieve password). c) It has\n     * correct RMIdentifier. d) It is not expired.\n     */\n    authorizeStartAndResourceIncreaseRequest(\n        nmTokenIdentifier, containerTokenIdentifier, true);\n    // update NMToken\n    updateNMTokenIdentifier(nmTokenIdentifier);\n\n    ContainerId containerId = containerTokenIdentifier.getContainerID();\n    String containerIdStr = containerId.toString();\n    String user = containerTokenIdentifier.getApplicationSubmitter();\n\n    LOG.info(\"Start request for \" + containerIdStr + \" by user \" + user);\n\n    ContainerLaunchContext launchContext = request.getContainerLaunchContext();\n\n    Map<String, ByteBuffer> serviceData = getAuxServiceMetaData();\n    if (launchContext.getServiceData()!=null && \n        !launchContext.getServiceData().isEmpty()) {\n      for (Map.Entry<String, ByteBuffer> meta : launchContext.getServiceData()\n          .entrySet()) {\n        if (null == serviceData.get(meta.getKey())) {\n          throw new InvalidAuxServiceException(\"The auxService:\" + meta.getKey()\n              + \" does not exist\");\n        }\n      }\n    }\n\n    // Sanity check for local resources\n    for (Map.Entry<String, LocalResource> rsrc : launchContext\n        .getLocalResources().entrySet()) {\n      if (rsrc.getValue() == null || rsrc.getValue().getResource() == null) {\n        throw new YarnException(\n            \"Null resource URL for local resource \" + rsrc.getKey() + \" : \" + rsrc.getValue());\n      } else if (rsrc.getValue().getType() == null) {\n        throw new YarnException(\n            \"Null resource type for local resource \" + rsrc.getKey() + \" : \" + rsrc.getValue());\n      } else if (rsrc.getValue().getVisibility() == null) {\n        throw new YarnException(\n            \"Null resource visibility for local resource \" + rsrc.getKey() + \" : \" + rsrc.getValue());\n      }\n    }\n\n    Credentials credentials =\n        YarnServerSecurityUtils.parseCredentials(launchContext);\n\n    Container container =\n        new ContainerImpl(getConfig(), this.dispatcher,\n            launchContext, credentials, metrics, containerTokenIdentifier,\n            context);\n    ApplicationId applicationID =\n        containerId.getApplicationAttemptId().getApplicationId();\n    if (context.getContainers().putIfAbsent(containerId, container) != null) {\n      NMAuditLogger.logFailure(user, AuditConstants.START_CONTAINER,\n        \"ContainerManagerImpl\", \"Container already running on this node!\",\n        applicationID, containerId);\n      throw RPCUtil.getRemoteException(\"Container \" + containerIdStr\n          + \" already is running on this node!!\");\n    }\n\n    this.readLock.lock();\n    try {\n      if (!serviceStopped) {\n        // Create the application\n        Application application =\n            new ApplicationImpl(dispatcher, user, applicationID, credentials, context);\n        if (null == context.getApplications().putIfAbsent(applicationID,\n          application)) {\n          LOG.info(\"Creating a new application reference for app \" + applicationID);\n          LogAggregationContext logAggregationContext =\n              containerTokenIdentifier.getLogAggregationContext();\n          Map<ApplicationAccessType, String> appAcls =\n              container.getLaunchContext().getApplicationACLs();\n          context.getNMStateStore().storeApplication(applicationID,\n              buildAppProto(applicationID, user, credentials, appAcls,\n                logAggregationContext));\n          dispatcher.getEventHandler().handle(\n            new ApplicationInitEvent(applicationID, appAcls,\n              logAggregationContext));\n        }\n\n        this.context.getNMStateStore().storeContainer(containerId,\n            containerTokenIdentifier.getVersion(), request);\n        dispatcher.getEventHandler().handle(\n          new ApplicationContainerInitEvent(container));\n\n        this.context.getContainerTokenSecretManager().startContainerSuccessful(\n          containerTokenIdentifier);\n        NMAuditLogger.logSuccess(user, AuditConstants.START_CONTAINER,\n          \"ContainerManageImpl\", applicationID, containerId);\n        // TODO launchedContainer misplaced -> doesn't necessarily mean a container\n        // launch. A finished Application will not launch containers.\n        metrics.launchedContainer();\n        metrics.allocateContainer(containerTokenIdentifier.getResource());\n      } else {\n        throw new YarnException(\n            \"Container start failed as the NodeManager is \" +\n            \"in the process of shutting down\");\n      }\n    } finally {\n      this.readLock.unlock();\n    }\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Preventive",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the root cause as a NullPointerException in the `addResources` method of the `ResourceSet` class, which is mentioned in the stack trace. However, the actual fix was made in the `checkLocalResources` method of the `ContainerLaunchContextPBImpl` class and `startContainerInternal` method of the `ContainerManagerImpl` class, which are not directly mentioned in the report. Therefore, the root cause identification is partial with a shared stack trace context. The fix suggestion is preventive as it suggests adding a check for null visibility, which aligns with the developer's fix to handle null visibility. The problem location identification is partial because the report mentions the `addResources` method, which is in the stack trace but not the actual ground truth methods. There is no wrong information in the report as all details are relevant to the context of the bug."
        }
    },
    {
        "filename": "YARN-4762.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DelegatingLinuxContainerRuntime.initialize": {
                "code_before_change": "  public void initialize(Configuration conf)\n      throws ContainerExecutionException {\n    PrivilegedOperationExecutor privilegedOperationExecutor =\n        PrivilegedOperationExecutor.getInstance(conf);\n    CGroupsHandler cGroupsHandler;\n    try {\n      cGroupsHandler = ResourceHandlerModule.getCGroupsHandler(conf);\n    } catch (ResourceHandlerException e) {\n      LOG.error(\"Unable to get cgroups handle.\");\n      throw new ContainerExecutionException(e);\n    }\n\n    defaultLinuxContainerRuntime = new DefaultLinuxContainerRuntime(\n        privilegedOperationExecutor);\n    defaultLinuxContainerRuntime.initialize(conf);\n    dockerLinuxContainerRuntime = new DockerLinuxContainerRuntime(\n        privilegedOperationExecutor, cGroupsHandler);\n    dockerLinuxContainerRuntime.initialize(conf);\n  }",
                "code_after_change": "  public void initialize(Configuration conf)\n      throws ContainerExecutionException {\n    PrivilegedOperationExecutor privilegedOperationExecutor =\n        PrivilegedOperationExecutor.getInstance(conf);\n    defaultLinuxContainerRuntime = new DefaultLinuxContainerRuntime(\n        privilegedOperationExecutor);\n    defaultLinuxContainerRuntime.initialize(conf);\n    dockerLinuxContainerRuntime = new DockerLinuxContainerRuntime(\n        privilegedOperationExecutor);\n    dockerLinuxContainerRuntime.initialize(conf);\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DockerLinuxContainerRuntime.isDockerContainerRequested": {
                "code_before_change": "  public static boolean isDockerContainerRequested(\n      Map<String, String> env) {\n    if (env == null) {\n      return false;\n    }\n\n    String type = env.get(ContainerRuntimeConstants.ENV_CONTAINER_TYPE);\n\n    return type != null && type.equals(\"docker\");\n  }",
                "code_after_change": "  public static boolean isDockerContainerRequested(\n      Map<String, String> env) {\n    if (env == null) {\n      return false;\n    }\n\n    String type = env.get(ContainerRuntimeConstants.ENV_CONTAINER_TYPE);\n\n    return type != null && type.equals(\"docker\");\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.ResourceHandlerModule.getCgroupsBlkioResourceHandler": {
                "code_before_change": "  private static CGroupsBlkioResourceHandlerImpl getCgroupsBlkioResourceHandler(\n      Configuration conf) throws ResourceHandlerException {\n    if (cGroupsBlkioResourceHandler == null) {\n      synchronized (DiskResourceHandler.class) {\n        if (cGroupsBlkioResourceHandler == null) {\n          LOG.debug(\"Creating new cgroups blkio handler\");\n          cGroupsBlkioResourceHandler =\n              new CGroupsBlkioResourceHandlerImpl(getCGroupsHandler(conf));\n        }\n      }\n    }\n    return cGroupsBlkioResourceHandler;\n  }",
                "code_after_change": "  private static CGroupsBlkioResourceHandlerImpl getCgroupsBlkioResourceHandler(\n      Configuration conf) throws ResourceHandlerException {\n    if (cGroupsBlkioResourceHandler == null) {\n      synchronized (DiskResourceHandler.class) {\n        if (cGroupsBlkioResourceHandler == null) {\n          LOG.debug(\"Creating new cgroups blkio handler\");\n          cGroupsBlkioResourceHandler =\n              new CGroupsBlkioResourceHandlerImpl(\n                  getInitializedCGroupsHandler(conf));\n        }\n      }\n    }\n    return cGroupsBlkioResourceHandler;\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.ResourceHandlerModule.getInitializedCGroupsHandler": {
                "code_before_change": [],
                "code_after_change": "  private static CGroupsHandler getInitializedCGroupsHandler(Configuration conf)\n      throws ResourceHandlerException {\n    if (cGroupsHandler == null) {\n      synchronized (CGroupsHandler.class) {\n        if (cGroupsHandler == null) {\n          cGroupsHandler = new CGroupsHandlerImpl(conf,\n              PrivilegedOperationExecutor.getInstance(conf));\n        }\n      }\n    }\n\n    return cGroupsHandler;\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DockerLinuxContainerRuntime.prepareContainer": {
                "code_before_change": "  public void prepareContainer(ContainerRuntimeContext ctx)\n      throws ContainerExecutionException {\n\n  }",
                "code_after_change": "  public void prepareContainer(ContainerRuntimeContext ctx)\n      throws ContainerExecutionException {\n\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.ResourceHandlerModule.getCGroupsCpuResourceHandler": {
                "code_before_change": [],
                "code_after_change": "  private static CGroupsCpuResourceHandlerImpl getCGroupsCpuResourceHandler(\n      Configuration conf) throws ResourceHandlerException {\n    boolean cgroupsCpuEnabled =\n        conf.getBoolean(YarnConfiguration.NM_CPU_RESOURCE_ENABLED,\n            YarnConfiguration.DEFAULT_NM_CPU_RESOURCE_ENABLED);\n    boolean cgroupsLCEResourcesHandlerEnabled =\n        conf.getClass(YarnConfiguration.NM_LINUX_CONTAINER_RESOURCES_HANDLER,\n            DefaultLCEResourcesHandler.class)\n            .equals(CgroupsLCEResourcesHandler.class);\n    if (cgroupsCpuEnabled || cgroupsLCEResourcesHandlerEnabled) {\n      if (cGroupsCpuResourceHandler == null) {\n        synchronized (CpuResourceHandler.class) {\n          if (cGroupsCpuResourceHandler == null) {\n            LOG.debug(\"Creating new cgroups cpu handler\");\n            cGroupsCpuResourceHandler =\n                new CGroupsCpuResourceHandlerImpl(\n                    getInitializedCGroupsHandler(conf));\n            return cGroupsCpuResourceHandler;\n          }\n        }\n      }\n    }\n    return null;\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.ResourceHandlerModule.getMemoryResourceHandler": {
                "code_before_change": "  public static MemoryResourceHandler getMemoryResourceHandler(\n      Configuration conf) throws ResourceHandlerException {\n    if (conf.getBoolean(YarnConfiguration.NM_MEMORY_RESOURCE_ENABLED,\n        YarnConfiguration.DEFAULT_NM_MEMORY_RESOURCE_ENABLED)) {\n      return getCgroupsMemoryResourceHandler(conf);\n    }\n    return null;\n  }",
                "code_after_change": "  public static MemoryResourceHandler getMemoryResourceHandler(\n      Configuration conf) throws ResourceHandlerException {\n    if (conf.getBoolean(YarnConfiguration.NM_MEMORY_RESOURCE_ENABLED,\n        YarnConfiguration.DEFAULT_NM_MEMORY_RESOURCE_ENABLED)) {\n      return getCgroupsMemoryResourceHandler(conf);\n    }\n    return null;\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.ResourceHandlerModule.initializeConfiguredResourceHandlerChain": {
                "code_before_change": "  private static void initializeConfiguredResourceHandlerChain(\n      Configuration conf) throws ResourceHandlerException {\n    ArrayList<ResourceHandler> handlerList = new ArrayList<>();\n\n    addHandlerIfNotNull(handlerList, getOutboundBandwidthResourceHandler(conf));\n    addHandlerIfNotNull(handlerList, getDiskResourceHandler(conf));\n    addHandlerIfNotNull(handlerList, getMemoryResourceHandler(conf));\n    addHandlerIfNotNull(handlerList, getcGroupsCpuResourceHandler(conf));\n    resourceHandlerChain = new ResourceHandlerChain(handlerList);\n  }",
                "code_after_change": "  private static void initializeConfiguredResourceHandlerChain(\n      Configuration conf) throws ResourceHandlerException {\n    ArrayList<ResourceHandler> handlerList = new ArrayList<>();\n\n    addHandlerIfNotNull(handlerList, getOutboundBandwidthResourceHandler(conf));\n    addHandlerIfNotNull(handlerList, getDiskResourceHandler(conf));\n    addHandlerIfNotNull(handlerList, getMemoryResourceHandler(conf));\n    addHandlerIfNotNull(handlerList, getCGroupsCpuResourceHandler(conf));\n    resourceHandlerChain = new ResourceHandlerChain(handlerList);\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Direct Caller/Callee"
            },
            "fix_suggestion": "Preventive",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Direct Caller/Callee"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the root cause as a failure in the `init` method of the `LinuxContainerExecutor`, which is a direct callee of the ground truth method `DelegatingLinuxContainerRuntime.initialize`. This makes it a 'Partial' identification with a 'Direct Caller/Callee' sub-category. The fix suggestion involves checking configuration settings and permissions, which is preventive as it aims to mitigate the issue by ensuring proper setup, rather than directly matching the code changes made by the developer. The problem location is identified as `LinuxContainerExecutor.init`, which is a direct callee of the ground truth method, thus 'Partial' with a 'Direct Caller/Callee' sub-category. There is no wrong information in the bug report as all details are relevant to the context of the bug."
        }
    },
    {
        "filename": "YARN-2823.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.transition": {
                "code_before_change": "    public void transition(RMAppAttemptImpl appAttempt,\n        RMAppAttemptEvent event) {\n    }",
                "code_after_change": "    public void transition(RMAppAttemptImpl appAttempt,\n        RMAppAttemptEvent event) {\n    }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Preventive",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "1. **Root Cause Identification**: The bug report identifies the `transferStateFromPreviousAttempt` method as the location of the NullPointerException, which is mentioned in the stack trace. However, the actual root cause is in the `RMAppAttemptImpl.transition` method, which is not mentioned. Thus, it is classified as 'Partial' with 'Shared Stack Trace Context' as the sub-category.\n\n2. **Fix Suggestion**: The suggestion to add null checks in the `transferStateFromPreviousAttempt` method is preventive, as it would prevent the NullPointerException from occurring, even though it does not match the developer's fix.\n\n3. **Problem Location Identification**: The problem location mentions methods like `transferStateFromPreviousAttempt` and `addApplicationAttempt`, which are in the stack trace but not the ground truth method. Therefore, it is 'Partial' with 'Shared Stack Trace Context' as the sub-category.\n\n4. **Wrong Information**: The bug report does not contain any incorrect or unrelated information. All details are relevant to the context of the bug."
        }
    },
    {
        "filename": "YARN-5098.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.requestNewHdfsDelegationTokenIfNeeded": {
                "code_before_change": "  private void requestNewHdfsDelegationTokenIfNeeded(\n      final DelegationTokenToRenew dttr) throws IOException,\n      InterruptedException {\n\n    if (hasProxyUserPrivileges\n        && dttr.maxDate - dttr.expirationDate < credentialsValidTimeRemaining\n        && dttr.token.getKind().equals(HDFS_DELEGATION_KIND)) {\n\n      final Collection<ApplicationId> applicationIds;\n      synchronized (dttr.referringAppIds) {\n        applicationIds = new HashSet<>(dttr.referringAppIds);\n        dttr.referringAppIds.clear();\n      }\n      // remove all old expiring hdfs tokens for this application.\n      for (ApplicationId appId : applicationIds) {\n        Set<DelegationTokenToRenew> tokenSet = appTokens.get(appId);\n        if (tokenSet == null || tokenSet.isEmpty()) {\n          continue;\n        }\n        Iterator<DelegationTokenToRenew> iter = tokenSet.iterator();\n        synchronized (tokenSet) {\n          while (iter.hasNext()) {\n            DelegationTokenToRenew t = iter.next();\n            if (t.token.getKind().equals(HDFS_DELEGATION_KIND)) {\n              iter.remove();\n              allTokens.remove(t.token);\n              t.cancelTimer();\n              LOG.info(\"Removed expiring token \" + t);\n            }\n          }\n        }\n      }\n      LOG.info(\"Token= (\" + dttr + \") is expiring, request new token.\");\n      requestNewHdfsDelegationToken(applicationIds, dttr.user,\n          dttr.shouldCancelAtEnd);\n    }\n  }",
                "code_after_change": "  private void requestNewHdfsDelegationTokenIfNeeded(\n      final DelegationTokenToRenew dttr) throws IOException,\n      InterruptedException {\n\n    if (hasProxyUserPrivileges\n        && dttr.maxDate - dttr.expirationDate < credentialsValidTimeRemaining\n        && dttr.token.getKind().equals(HDFS_DELEGATION_KIND)) {\n\n      final Collection<ApplicationId> applicationIds;\n      synchronized (dttr.referringAppIds) {\n        applicationIds = new HashSet<>(dttr.referringAppIds);\n        dttr.referringAppIds.clear();\n      }\n      // remove all old expiring hdfs tokens for this application.\n      for (ApplicationId appId : applicationIds) {\n        Set<DelegationTokenToRenew> tokenSet = appTokens.get(appId);\n        if (tokenSet == null || tokenSet.isEmpty()) {\n          continue;\n        }\n        Iterator<DelegationTokenToRenew> iter = tokenSet.iterator();\n        synchronized (tokenSet) {\n          while (iter.hasNext()) {\n            DelegationTokenToRenew t = iter.next();\n            if (t.token.getKind().equals(HDFS_DELEGATION_KIND)) {\n              iter.remove();\n              allTokens.remove(t.token);\n              t.cancelTimer();\n              LOG.info(\"Removed expiring token \" + t);\n            }\n          }\n        }\n      }\n      LOG.info(\"Token= (\" + dttr + \") is expiring, request new token.\");\n      requestNewHdfsDelegationTokenAsProxyUser(applicationIds, dttr.user,\n          dttr.shouldCancelAtEnd);\n    }\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.run": {
                "code_before_change": "    public void run() {\n      TokenWithConf tokenWithConf = null;\n      while (true) {\n        try {\n          tokenWithConf = queue.take();\n          final TokenWithConf current = tokenWithConf;\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Cancelling token \" + tokenWithConf.token.getService());\n          }\n          // need to use doAs so that http can find the kerberos tgt\n          UserGroupInformation.getLoginUser()\n            .doAs(new PrivilegedExceptionAction<Void>(){\n\n              @Override\n              public Void run() throws Exception {\n                current.token.cancel(current.conf);\n                return null;\n              }\n            });\n        } catch (IOException e) {\n          LOG.warn(\"Failed to cancel token \" + tokenWithConf.token + \" \" +  \n              StringUtils.stringifyException(e));\n        } catch (RuntimeException e) {\n          LOG.warn(\"Failed to cancel token \" + tokenWithConf.token + \" \" +  \n              StringUtils.stringifyException(e));\n        } catch (InterruptedException ie) {\n          return;\n        } catch (Throwable t) {\n          LOG.warn(\"Got exception \" + StringUtils.stringifyException(t) + \n                   \". Exiting..\");\n          System.exit(-1);\n        }\n      }\n    }",
                "code_after_change": "    public void run() {\n      TokenWithConf tokenWithConf = null;\n      while (true) {\n        try {\n          tokenWithConf = queue.take();\n          final TokenWithConf current = tokenWithConf;\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Cancelling token \" + tokenWithConf.token.getService());\n          }\n          // need to use doAs so that http can find the kerberos tgt\n          UserGroupInformation.getLoginUser()\n            .doAs(new PrivilegedExceptionAction<Void>(){\n\n              @Override\n              public Void run() throws Exception {\n                current.token.cancel(current.conf);\n                return null;\n              }\n            });\n        } catch (IOException e) {\n          LOG.warn(\"Failed to cancel token \" + tokenWithConf.token + \" \" +  \n              StringUtils.stringifyException(e));\n        } catch (RuntimeException e) {\n          LOG.warn(\"Failed to cancel token \" + tokenWithConf.token + \" \" +  \n              StringUtils.stringifyException(e));\n        } catch (InterruptedException ie) {\n          return;\n        } catch (Throwable t) {\n          LOG.warn(\"Got exception \" + StringUtils.stringifyException(t) + \n                   \". Exiting..\");\n          System.exit(-1);\n        }\n      }\n    }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.handleAppSubmitEvent": {
                "code_before_change": "  private void handleAppSubmitEvent(AbstractDelegationTokenRenewerAppEvent evt)\n      throws IOException, InterruptedException {\n    ApplicationId applicationId = evt.getApplicationId();\n    Credentials ts = evt.getCredentials();\n    boolean shouldCancelAtEnd = evt.shouldCancelAtEnd();\n    if (ts == null) {\n      return; // nothing to add\n    }\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Registering tokens for renewal for:\" +\n          \" appId = \" + applicationId);\n    }\n\n    Collection<Token<?>> tokens = ts.getAllTokens();\n    long now = System.currentTimeMillis();\n\n    // find tokens for renewal, but don't add timers until we know\n    // all renewable tokens are valid\n    // At RM restart it is safe to assume that all the previously added tokens\n    // are valid\n    appTokens.put(applicationId,\n      Collections.synchronizedSet(new HashSet<DelegationTokenToRenew>()));\n    Set<DelegationTokenToRenew> tokenList = new HashSet<DelegationTokenToRenew>();\n    boolean hasHdfsToken = false;\n    for (Token<?> token : tokens) {\n      if (token.isManaged()) {\n        if (token.getKind().equals(HDFS_DELEGATION_KIND)) {\n          LOG.info(applicationId + \" found existing hdfs token \" + token);\n          hasHdfsToken = true;\n        }\n        if (skipTokenRenewal(token)) {\n          continue;\n        }\n\n        DelegationTokenToRenew dttr = allTokens.get(token);\n        if (dttr == null) {\n          dttr = new DelegationTokenToRenew(Arrays.asList(applicationId), token,\n              getConfig(), now, shouldCancelAtEnd, evt.getUser());\n          try {\n            renewToken(dttr);\n          } catch (IOException ioe) {\n            throw new IOException(\"Failed to renew token: \" + dttr.token, ioe);\n          }\n        }\n        tokenList.add(dttr);\n      }\n    }\n\n    if (!tokenList.isEmpty()) {\n      // Renewing token and adding it to timer calls are separated purposefully\n      // If user provides incorrect token then it should not be added for\n      // renewal.\n      for (DelegationTokenToRenew dtr : tokenList) {\n        DelegationTokenToRenew currentDtr =\n            allTokens.putIfAbsent(dtr.token, dtr);\n        if (currentDtr != null) {\n          // another job beat us\n          currentDtr.referringAppIds.add(applicationId);\n          appTokens.get(applicationId).add(currentDtr);\n        } else {\n          appTokens.get(applicationId).add(dtr);\n          setTimerForTokenRenewal(dtr);\n        }\n      }\n    }\n\n    if (!hasHdfsToken) {\n      requestNewHdfsDelegationToken(Arrays.asList(applicationId), evt.getUser(),\n        shouldCancelAtEnd);\n    }\n  }",
                "code_after_change": "  private void handleAppSubmitEvent(AbstractDelegationTokenRenewerAppEvent evt)\n      throws IOException, InterruptedException {\n    ApplicationId applicationId = evt.getApplicationId();\n    Credentials ts = evt.getCredentials();\n    boolean shouldCancelAtEnd = evt.shouldCancelAtEnd();\n    if (ts == null) {\n      return; // nothing to add\n    }\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Registering tokens for renewal for:\" +\n          \" appId = \" + applicationId);\n    }\n\n    Collection<Token<?>> tokens = ts.getAllTokens();\n    long now = System.currentTimeMillis();\n\n    // find tokens for renewal, but don't add timers until we know\n    // all renewable tokens are valid\n    // At RM restart it is safe to assume that all the previously added tokens\n    // are valid\n    appTokens.put(applicationId,\n      Collections.synchronizedSet(new HashSet<DelegationTokenToRenew>()));\n    Set<DelegationTokenToRenew> tokenList = new HashSet<DelegationTokenToRenew>();\n    boolean hasHdfsToken = false;\n    for (Token<?> token : tokens) {\n      if (token.isManaged()) {\n        if (token.getKind().equals(HDFS_DELEGATION_KIND)) {\n          LOG.info(applicationId + \" found existing hdfs token \" + token);\n          hasHdfsToken = true;\n        }\n        if (skipTokenRenewal(token)) {\n          continue;\n        }\n\n        DelegationTokenToRenew dttr = allTokens.get(token);\n        if (dttr == null) {\n          dttr = new DelegationTokenToRenew(Arrays.asList(applicationId), token,\n              getConfig(), now, shouldCancelAtEnd, evt.getUser());\n          try {\n            renewToken(dttr);\n          } catch (IOException ioe) {\n            if (ioe instanceof SecretManager.InvalidToken\n                && dttr.maxDate < Time.now()\n                && evt instanceof DelegationTokenRenewerAppRecoverEvent\n                && token.getKind().equals(HDFS_DELEGATION_KIND)) {\n              LOG.info(\"Failed to renew hdfs token \" + dttr\n                  + \" on recovery as it expired, requesting new hdfs token for \"\n                  + applicationId + \", user=\" + evt.getUser(), ioe);\n              requestNewHdfsDelegationTokenAsProxyUser(\n                  Arrays.asList(applicationId), evt.getUser(),\n                  evt.shouldCancelAtEnd());\n              continue;\n            }\n            throw new IOException(\"Failed to renew token: \" + dttr.token, ioe);\n          }\n        }\n        tokenList.add(dttr);\n      }\n    }\n\n    if (!tokenList.isEmpty()) {\n      // Renewing token and adding it to timer calls are separated purposefully\n      // If user provides incorrect token then it should not be added for\n      // renewal.\n      for (DelegationTokenToRenew dtr : tokenList) {\n        DelegationTokenToRenew currentDtr =\n            allTokens.putIfAbsent(dtr.token, dtr);\n        if (currentDtr != null) {\n          // another job beat us\n          currentDtr.referringAppIds.add(applicationId);\n          appTokens.get(applicationId).add(currentDtr);\n        } else {\n          appTokens.get(applicationId).add(dtr);\n          setTimerForTokenRenewal(dtr);\n        }\n      }\n    }\n\n    if (!hasHdfsToken) {\n      requestNewHdfsDelegationTokenAsProxyUser(Arrays.asList(applicationId),\n          evt.getUser(),\n        shouldCancelAtEnd);\n    }\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.handleDTRenewerAppRecoverEvent": {
                "code_before_change": "  private void handleDTRenewerAppRecoverEvent(\n      DelegationTokenRenewerAppRecoverEvent event) {\n    try {\n      // Setup tokens for renewal during recovery\n      DelegationTokenRenewer.this.handleAppSubmitEvent(event);\n    } catch (Throwable t) {\n      LOG.warn(\n          \"Unable to add the application to the delegation token renewer.\", t);\n    }\n  }",
                "code_after_change": "  private void handleDTRenewerAppRecoverEvent(\n      DelegationTokenRenewerAppRecoverEvent event) {\n    try {\n      // Setup tokens for renewal during recovery\n      DelegationTokenRenewer.this.handleAppSubmitEvent(event);\n    } catch (Throwable t) {\n      LOG.warn(\"Unable to add the application to the delegation token\"\n          + \" renewer on recovery.\", t);\n    }\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Preventive",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the issue with HDFS delegation token expiration or misconfiguration, which is related to the ground truth methods dealing with token renewal. However, it does not precisely identify the root cause in the specific methods where the fix was applied. The methods mentioned in the bug report are part of the stack trace, indicating a shared context but not the exact location of the fix. The fix suggestion is preventive as it suggests checking configurations and ensuring token renewal, which aligns with the nature of the actual fix but does not match it exactly. There is no wrong information in the report as it accurately describes the problem context and potential causes."
        }
    },
    {
        "filename": "YARN-3971.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.nodelabels.CommonNodeLabelsManager.normalizeNodeIdToLabels": {
                "code_before_change": "  protected Map<NodeId, Set<String>> normalizeNodeIdToLabels(\n      Map<NodeId, Set<String>> nodeIdToLabels) {\n    Map<NodeId, Set<String>> newMap = new HashMap<NodeId, Set<String>>();\n    for (Entry<NodeId, Set<String>> entry : nodeIdToLabels.entrySet()) {\n      NodeId id = entry.getKey();\n      Set<String> labels = entry.getValue();\n      newMap.put(id, normalizeLabels(labels)); \n    }\n    return newMap;\n  }",
                "code_after_change": "  protected Map<NodeId, Set<String>> normalizeNodeIdToLabels(\n      Map<NodeId, Set<String>> nodeIdToLabels) {\n    Map<NodeId, Set<String>> newMap = new TreeMap<NodeId, Set<String>>();\n    for (Entry<NodeId, Set<String>> entry : nodeIdToLabels.entrySet()) {\n      NodeId id = entry.getKey();\n      Set<String> labels = entry.getValue();\n      newMap.put(id, normalizeLabels(labels)); \n    }\n    return newMap;\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.nodelabels.CommonNodeLabelsManager.startDispatcher": {
                "code_before_change": "  protected void startDispatcher() {\n    // start dispatcher\n    AsyncDispatcher asyncDispatcher = (AsyncDispatcher) dispatcher;\n    asyncDispatcher.start();\n  }",
                "code_after_change": "  protected void startDispatcher() {\n    // start dispatcher\n    AsyncDispatcher asyncDispatcher = (AsyncDispatcher) dispatcher;\n    asyncDispatcher.start();\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.nodelabels.RMNodeLabelsManager.removeFromClusterNodeLabels": {
                "code_before_change": "  public void removeFromClusterNodeLabels(Collection<String> labelsToRemove)\n      throws IOException {\n    try {\n      writeLock.lock();\n\n      checkRemoveFromClusterNodeLabelsOfQueue(labelsToRemove);\n\n      // copy before NMs\n      Map<String, Host> before = cloneNodeMap();\n\n      super.removeFromClusterNodeLabels(labelsToRemove);\n\n      updateResourceMappings(before, nodeCollections);\n    } finally {\n      writeLock.unlock();\n    }\n  }",
                "code_after_change": "  public void removeFromClusterNodeLabels(Collection<String> labelsToRemove)\n      throws IOException {\n    try {\n      writeLock.lock();\n      if (!isInitNodeLabelStoreInProgress()) {\n        // We cannot remove node labels from collection when some queue(s) are\n        // using any of them.\n        // We will not do remove when recovery is in prpgress. During\n        // service starting, we will replay edit logs and recover state. It is\n        // possible that a history operation removed some labels which were not\n        // used by some queues in the past but are used by current queues.\n        checkRemoveFromClusterNodeLabelsOfQueue(labelsToRemove);\n      }\n      // copy before NMs\n      Map<String, Host> before = cloneNodeMap();\n\n      super.removeFromClusterNodeLabels(labelsToRemove);\n\n      updateResourceMappings(before, nodeCollections);\n    } finally {\n      writeLock.unlock();\n    }\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.nodelabels.CommonNodeLabelsManager.serviceInit": {
                "code_before_change": "  protected void serviceInit(Configuration conf) throws Exception {\n    // set if node labels enabled\n    nodeLabelsEnabled =\n        conf.getBoolean(YarnConfiguration.NODE_LABELS_ENABLED,\n            YarnConfiguration.DEFAULT_NODE_LABELS_ENABLED);\n\n    isDistributedNodeLabelConfiguration  =\n        YarnConfiguration.isDistributedNodeLabelConfiguration(conf);\n    \n    labelCollections.put(NO_LABEL, new RMNodeLabel(NO_LABEL));\n  }",
                "code_after_change": "  protected void serviceInit(Configuration conf) throws Exception {\n    // set if node labels enabled\n    nodeLabelsEnabled = YarnConfiguration.areNodeLabelsEnabled(conf);\n\n    isCentralizedNodeLabelConfiguration  =\n        YarnConfiguration.isCentralizedNodeLabelConfiguration(conf);\n\n    labelCollections.put(NO_LABEL, new RMNodeLabel(NO_LABEL));\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Direct Caller/Callee"
            },
            "fix_suggestion": "Preventive",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Direct Caller/Callee"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the root cause as the inability to remove a label due to its association with a queue, which is enforced by the `checkRemoveFromClusterNodeLabelsOfQueue` method. This method is a direct callee of the ground truth method `RMNodeLabelsManager.removeFromClusterNodeLabels`, hence the root cause identification is 'Partial' with 'Direct Caller/Callee' as the sub-category. The fix suggestion is 'Preventive' because it suggests removing the label from queues before attempting to remove it from the cluster, which would prevent the IOException. The problem location identification is 'Partial' with 'Direct Caller/Callee' as the sub-category because the report mentions `RMNodeLabelsManager.checkRemoveFromClusterNodeLabelsOfQueue`, which is directly called by the ground truth method. There is no wrong information in the bug report as all details are relevant to the context of the bug."
        }
    },
    {
        "filename": "YARN-6948.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.StatusUpdateTransition": {
                "code_before_change": [],
                "code_after_change": []
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.handle": {
                "code_before_change": "  public void handle(RMAppAttemptEvent event) {\n\n    this.writeLock.lock();\n\n    try {\n      ApplicationAttemptId appAttemptID = event.getApplicationAttemptId();\n      LOG.debug(\"Processing event for \" + appAttemptID + \" of type \"\n          + event.getType());\n      final RMAppAttemptState oldState = getAppAttemptState();\n      try {\n        /* keep the master in sync with the state machine */\n        this.stateMachine.doTransition(event.getType(), event);\n      } catch (InvalidStateTransitionException e) {\n        LOG.error(\"Can't handle this event at current state\", e);\n        /* TODO fail the application on the failed transition */\n      }\n\n      // Log at INFO if we're not recovering or not in a terminal state.\n      // Log at DEBUG otherwise.\n      if ((oldState != getAppAttemptState()) &&\n          ((recoveredFinalState == null) ||\n            (event.getType() != RMAppAttemptEventType.RECOVER))) {\n        LOG.info(String.format(STATE_CHANGE_MESSAGE, appAttemptID, oldState,\n            getAppAttemptState(), event.getType()));\n      } else if ((oldState != getAppAttemptState()) && LOG.isDebugEnabled()) {\n        LOG.debug(String.format(STATE_CHANGE_MESSAGE, appAttemptID, oldState,\n            getAppAttemptState(), event.getType()));\n      }\n    } finally {\n      this.writeLock.unlock();\n    }\n  }",
                "code_after_change": "  public void handle(RMAppAttemptEvent event) {\n\n    this.writeLock.lock();\n\n    try {\n      ApplicationAttemptId appAttemptID = event.getApplicationAttemptId();\n      LOG.debug(\"Processing event for \" + appAttemptID + \" of type \"\n          + event.getType());\n      final RMAppAttemptState oldState = getAppAttemptState();\n      try {\n        /* keep the master in sync with the state machine */\n        this.stateMachine.doTransition(event.getType(), event);\n      } catch (InvalidStateTransitionException e) {\n        LOG.error(\"App attempt: \" + appAttemptID\n            + \" can't handle this event at current state\", e);\n        onInvalidTranstion(event.getType(), oldState);\n      }\n\n      // Log at INFO if we're not recovering or not in a terminal state.\n      // Log at DEBUG otherwise.\n      if ((oldState != getAppAttemptState()) &&\n          ((recoveredFinalState == null) ||\n            (event.getType() != RMAppAttemptEventType.RECOVER))) {\n        LOG.info(String.format(STATE_CHANGE_MESSAGE, appAttemptID, oldState,\n            getAppAttemptState(), event.getType()));\n      } else if ((oldState != getAppAttemptState()) && LOG.isDebugEnabled()) {\n        LOG.debug(String.format(STATE_CHANGE_MESSAGE, appAttemptID, oldState,\n            getAppAttemptState(), event.getType()));\n      }\n    } finally {\n      this.writeLock.unlock();\n    }\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.getBlacklistedNodes": {
                "code_before_change": "  public Set<String> getBlacklistedNodes() {\n    if (scheduler instanceof AbstractYarnScheduler) {\n      AbstractYarnScheduler ayScheduler =\n          (AbstractYarnScheduler) scheduler;\n      SchedulerApplicationAttempt attempt =\n          ayScheduler.getApplicationAttempt(applicationAttemptId);\n      if (attempt != null) {\n        return attempt.getBlacklistedNodes();\n      }\n    }\n    return Collections.EMPTY_SET;\n  }",
                "code_after_change": "  public Set<String> getBlacklistedNodes() {\n    if (scheduler instanceof AbstractYarnScheduler) {\n      AbstractYarnScheduler ayScheduler =\n          (AbstractYarnScheduler) scheduler;\n      SchedulerApplicationAttempt attempt =\n          ayScheduler.getApplicationAttempt(applicationAttemptId);\n      if (attempt != null) {\n        return attempt.getBlacklistedNodes();\n      }\n    }\n    return Collections.EMPTY_SET;\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Correct",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report precisely identifies the root cause by mentioning the 'handle' method in the 'RMAppAttemptImpl' class, which is part of the ground truth methods. The fix suggestion is correct as it aligns with the developer's fix, which involves handling the InvalidStateTransitionException in the 'handle' method. The problem location identification is precise because the 'problem_location' field directly mentions the 'handle' method, which is in the ground truth list. There is no wrong information in the bug report as all details are consistent with the context of the bug."
        }
    },
    {
        "filename": "YARN-1409.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.NonAggregatingLogHandler.handle": {
                "code_before_change": "  public void handle(LogHandlerEvent event) {\n    switch (event.getType()) {\n      case APPLICATION_STARTED:\n        LogHandlerAppStartedEvent appStartedEvent =\n            (LogHandlerAppStartedEvent) event;\n        this.appOwners.put(appStartedEvent.getApplicationId(),\n            appStartedEvent.getUser());\n        this.dispatcher.getEventHandler().handle(\n            new ApplicationEvent(appStartedEvent.getApplicationId(),\n                ApplicationEventType.APPLICATION_LOG_HANDLING_INITED));\n        break;\n      case CONTAINER_FINISHED:\n        // Ignore\n        break;\n      case APPLICATION_FINISHED:\n        LogHandlerAppFinishedEvent appFinishedEvent =\n            (LogHandlerAppFinishedEvent) event;\n        // Schedule - so that logs are available on the UI till they're deleted.\n        LOG.info(\"Scheduling Log Deletion for application: \"\n            + appFinishedEvent.getApplicationId() + \", with delay of \"\n            + this.deleteDelaySeconds + \" seconds\");\n        sched.schedule(\n            new LogDeleterRunnable(appOwners.remove(appFinishedEvent\n                .getApplicationId()), appFinishedEvent.getApplicationId()),\n            this.deleteDelaySeconds, TimeUnit.SECONDS);\n        break;\n      default:\n        ; // Ignore\n    }\n  }",
                "code_after_change": "  public void handle(LogHandlerEvent event) {\n    switch (event.getType()) {\n      case APPLICATION_STARTED:\n        LogHandlerAppStartedEvent appStartedEvent =\n            (LogHandlerAppStartedEvent) event;\n        this.appOwners.put(appStartedEvent.getApplicationId(),\n            appStartedEvent.getUser());\n        this.dispatcher.getEventHandler().handle(\n            new ApplicationEvent(appStartedEvent.getApplicationId(),\n                ApplicationEventType.APPLICATION_LOG_HANDLING_INITED));\n        break;\n      case CONTAINER_FINISHED:\n        // Ignore\n        break;\n      case APPLICATION_FINISHED:\n        LogHandlerAppFinishedEvent appFinishedEvent =\n            (LogHandlerAppFinishedEvent) event;\n        // Schedule - so that logs are available on the UI till they're deleted.\n        LOG.info(\"Scheduling Log Deletion for application: \"\n            + appFinishedEvent.getApplicationId() + \", with delay of \"\n            + this.deleteDelaySeconds + \" seconds\");\n        LogDeleterRunnable logDeleter =\n            new LogDeleterRunnable(appOwners.remove(appFinishedEvent\n                  .getApplicationId()), appFinishedEvent.getApplicationId());\n        try {\n          sched.schedule(logDeleter, this.deleteDelaySeconds,\n              TimeUnit.SECONDS);\n        } catch (RejectedExecutionException e) {\n          // Handling this event in local thread before starting threads\n          // or after calling sched.shutdownNow().\n          logDeleter.run();\n        }\n        break;\n      default:\n        ; // Ignore\n    }\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Correct",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report precisely identifies the root cause as the ScheduledThreadPoolExecutor shutting down when NonAggregatingLogHandler attempts to submit a new task, which matches the ground truth method. The fix suggestion in the report is to check the state of the executor before submitting tasks, which aligns with the developer's fix of catching the RejectedExecutionException and handling it. The problem location is precisely identified as the NonAggregatingLogHandler.handle method, which is the ground truth method. There is no wrong information in the bug report as all details are consistent with the context of the bug."
        }
    },
    {
        "filename": "YARN-5545.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.addApplication": {
                "code_before_change": "  private synchronized void addApplication(ApplicationId applicationId,\n      String queueName, String user, Priority priority) {\n    // Sanity checks.\n    CSQueue queue = getQueue(queueName);\n    if (queue == null) {\n      String message = \"Application \" + applicationId +\n      \" submitted by user \" + user + \" to unknown queue: \" + queueName;\n      this.rmContext.getDispatcher().getEventHandler()\n          .handle(new RMAppEvent(applicationId,\n              RMAppEventType.APP_REJECTED, message));\n      return;\n    }\n    if (!(queue instanceof LeafQueue)) {\n      String message = \"Application \" + applicationId + \n          \" submitted by user \" + user + \" to non-leaf queue: \" + queueName;\n      this.rmContext.getDispatcher().getEventHandler()\n          .handle(new RMAppEvent(applicationId,\n              RMAppEventType.APP_REJECTED, message));\n      return;\n    }\n    // Submit to the queue\n    try {\n      queue.submitApplication(applicationId, user, queueName);\n    } catch (AccessControlException ace) {\n      LOG.info(\"Failed to submit application \" + applicationId + \" to queue \"\n          + queueName + \" from user \" + user, ace);\n      this.rmContext.getDispatcher().getEventHandler()\n          .handle(new RMAppEvent(applicationId,\n              RMAppEventType.APP_REJECTED, ace.toString()));\n      return;\n    }\n    // update the metrics\n    queue.getMetrics().submitApp(user);\n    SchedulerApplication<FiCaSchedulerApp> application =\n        new SchedulerApplication<FiCaSchedulerApp>(queue, user, priority);\n    applications.put(applicationId, application);\n    LOG.info(\"Accepted application \" + applicationId + \" from user: \" + user\n        + \", in queue: \" + queueName);\n    rmContext.getDispatcher().getEventHandler()\n        .handle(new RMAppEvent(applicationId, RMAppEventType.APP_ACCEPTED));\n  }",
                "code_after_change": "  private void addApplication(ApplicationId applicationId,\n      String queueName, String user, Priority priority) {\n    try {\n      writeLock.lock();\n      if (isSystemAppsLimitReached()) {\n        String message = \"Maximum system application limit reached,\"\n            + \"cannot accept submission of application: \" + applicationId;\n        this.rmContext.getDispatcher().getEventHandler().handle(new RMAppEvent(\n            applicationId, RMAppEventType.APP_REJECTED, message));\n        return;\n      }\n      // Sanity checks.\n      CSQueue queue = getQueue(queueName);\n      if (queue == null) {\n        String message =\n            \"Application \" + applicationId + \" submitted by user \" + user\n                + \" to unknown queue: \" + queueName;\n        this.rmContext.getDispatcher().getEventHandler().handle(\n            new RMAppEvent(applicationId, RMAppEventType.APP_REJECTED,\n                message));\n        return;\n      }\n      if (!(queue instanceof LeafQueue)) {\n        String message =\n            \"Application \" + applicationId + \" submitted by user \" + user\n                + \" to non-leaf queue: \" + queueName;\n        this.rmContext.getDispatcher().getEventHandler().handle(\n            new RMAppEvent(applicationId, RMAppEventType.APP_REJECTED,\n                message));\n        return;\n      }\n      // Submit to the queue\n      try {\n        queue.submitApplication(applicationId, user, queueName);\n      } catch (AccessControlException ace) {\n        LOG.info(\"Failed to submit application \" + applicationId + \" to queue \"\n            + queueName + \" from user \" + user, ace);\n        this.rmContext.getDispatcher().getEventHandler().handle(\n            new RMAppEvent(applicationId, RMAppEventType.APP_REJECTED,\n                ace.toString()));\n        return;\n      }\n      // update the metrics\n      queue.getMetrics().submitApp(user);\n      SchedulerApplication<FiCaSchedulerApp> application =\n          new SchedulerApplication<FiCaSchedulerApp>(queue, user, priority);\n      applications.put(applicationId, application);\n      LOG.info(\"Accepted application \" + applicationId + \" from user: \" + user\n          + \", in queue: \" + queueName);\n      rmContext.getDispatcher().getEventHandler().handle(\n          new RMAppEvent(applicationId, RMAppEventType.APP_ACCEPTED));\n    } finally {\n      writeLock.unlock();\n    }\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration.getLazyPreemptionEnabled": {
                "code_before_change": "  public boolean getLazyPreemptionEnabled() {\n    return getBoolean(LAZY_PREEMPTION_ENALBED, DEFAULT_LAZY_PREEMPTION_ENABLED);\n  }",
                "code_after_change": "  public boolean getLazyPreemptionEnabled() {\n    return getBoolean(LAZY_PREEMPTION_ENALBED, DEFAULT_LAZY_PREEMPTION_ENABLED);\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.getAppsInQueue": {
                "code_before_change": "  public List<ApplicationAttemptId> getAppsInQueue(String queueName) {\n    CSQueue queue = queues.get(queueName);\n    if (queue == null) {\n      return null;\n    }\n    List<ApplicationAttemptId> apps = new ArrayList<ApplicationAttemptId>();\n    queue.collectSchedulerApplications(apps);\n    return apps;\n  }",
                "code_after_change": "  public List<ApplicationAttemptId> getAppsInQueue(String queueName) {\n    CSQueue queue = queues.get(queueName);\n    if (queue == null) {\n      return null;\n    }\n    List<ApplicationAttemptId> apps = new ArrayList<ApplicationAttemptId>();\n    queue.collectSchedulerApplications(apps);\n    return apps;\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.setupQueueConfigs": {
                "code_before_change": "  protected synchronized void setupQueueConfigs(Resource clusterResource)\n      throws IOException {\n    super.setupQueueConfigs(clusterResource);\n    \n    this.lastClusterResource = clusterResource;\n    \n    this.cachedResourceLimitsForHeadroom = new ResourceLimits(clusterResource);\n    \n    // Initialize headroom info, also used for calculating application \n    // master resource limits.  Since this happens during queue initialization\n    // and all queues may not be realized yet, we'll use (optimistic) \n    // absoluteMaxCapacity (it will be replaced with the more accurate \n    // absoluteMaxAvailCapacity during headroom/userlimit/allocation events)\n    setQueueResourceLimitsInfo(clusterResource);\n\n    CapacitySchedulerConfiguration conf = csContext.getConfiguration();\n\n    setOrderingPolicy(conf.<FiCaSchedulerApp>getOrderingPolicy(getQueuePath()));\n\n    userLimit = conf.getUserLimit(getQueuePath());\n    userLimitFactor = conf.getUserLimitFactor(getQueuePath());\n\n    maxApplications = conf.getMaximumApplicationsPerQueue(getQueuePath());\n    if (maxApplications < 0) {\n      int maxSystemApps = conf.getMaximumSystemApplications();\n      maxApplications =\n          (int) (maxSystemApps * queueCapacities.getAbsoluteCapacity());\n    }\n    maxApplicationsPerUser = Math.min(maxApplications,\n        (int)(maxApplications * (userLimit / 100.0f) * userLimitFactor));\n    \n    maxAMResourcePerQueuePercent =\n        conf.getMaximumApplicationMasterResourcePerQueuePercent(getQueuePath());\n\n    if (!SchedulerUtils.checkQueueLabelExpression(\n        this.accessibleLabels, this.defaultLabelExpression, null)) {\n      throw new IOException(\"Invalid default label expression of \"\n          + \" queue=\"\n          + getQueueName()\n          + \" doesn't have permission to access all labels \"\n          + \"in default label expression. labelExpression of resource request=\"\n          + (this.defaultLabelExpression == null ? \"\"\n              : this.defaultLabelExpression)\n          + \". Queue labels=\"\n          + (getAccessibleNodeLabels() == null ? \"\" : StringUtils.join(\n              getAccessibleNodeLabels().iterator(), ',')));\n    }\n    \n    nodeLocalityDelay = conf.getNodeLocalityDelay();\n    rackLocalityFullReset = conf.getRackLocalityFullReset();\n\n    // re-init this since max allocation could have changed\n    this.minimumAllocationFactor =\n        Resources.ratio(resourceCalculator,\n            Resources.subtract(maximumAllocation, minimumAllocation),\n            maximumAllocation);\n\n    StringBuilder aclsString = new StringBuilder();\n    for (Map.Entry<AccessType, AccessControlList> e : acls.entrySet()) {\n      aclsString.append(e.getKey() + \":\" + e.getValue().getAclString());\n    }\n\n    StringBuilder labelStrBuilder = new StringBuilder(); \n    if (accessibleLabels != null) {\n      for (String s : accessibleLabels) {\n        labelStrBuilder.append(s);\n        labelStrBuilder.append(\",\");\n      }\n    }\n\n    defaultAppPriorityPerQueue = Priority.newInstance(conf\n        .getDefaultApplicationPriorityConfPerQueue(getQueuePath()));\n\n    LOG.info(\"Initializing \" + queueName + \"\\n\" +\n        \"capacity = \" + queueCapacities.getCapacity() +\n        \" [= (float) configuredCapacity / 100 ]\" + \"\\n\" + \n        \"asboluteCapacity = \" + queueCapacities.getAbsoluteCapacity() +\n        \" [= parentAbsoluteCapacity * capacity ]\" + \"\\n\" +\n        \"maxCapacity = \" + queueCapacities.getMaximumCapacity() +\n        \" [= configuredMaxCapacity ]\" + \"\\n\" +\n        \"absoluteMaxCapacity = \" + queueCapacities.getAbsoluteMaximumCapacity() +\n        \" [= 1.0 maximumCapacity undefined, \" +\n        \"(parentAbsoluteMaxCapacity * maximumCapacity) / 100 otherwise ]\" + \n        \"\\n\" +\n        \"userLimit = \" + userLimit +\n        \" [= configuredUserLimit ]\" + \"\\n\" +\n        \"userLimitFactor = \" + userLimitFactor +\n        \" [= configuredUserLimitFactor ]\" + \"\\n\" +\n        \"maxApplications = \" + maxApplications +\n        \" [= configuredMaximumSystemApplicationsPerQueue or\" + \n        \" (int)(configuredMaximumSystemApplications * absoluteCapacity)]\" + \n        \"\\n\" +\n        \"maxApplicationsPerUser = \" + maxApplicationsPerUser +\n        \" [= (int)(maxApplications * (userLimit / 100.0f) * \" +\n        \"userLimitFactor) ]\" + \"\\n\" +\n        \"usedCapacity = \" + queueCapacities.getUsedCapacity() +\n        \" [= usedResourcesMemory / \" +\n        \"(clusterResourceMemory * absoluteCapacity)]\" + \"\\n\" +\n        \"absoluteUsedCapacity = \" + absoluteUsedCapacity +\n        \" [= usedResourcesMemory / clusterResourceMemory]\" + \"\\n\" +\n        \"maxAMResourcePerQueuePercent = \" + maxAMResourcePerQueuePercent +\n        \" [= configuredMaximumAMResourcePercent ]\" + \"\\n\" +\n        \"minimumAllocationFactor = \" + minimumAllocationFactor +\n        \" [= (float)(maximumAllocationMemory - minimumAllocationMemory) / \" +\n        \"maximumAllocationMemory ]\" + \"\\n\" +\n        \"maximumAllocation = \" + maximumAllocation +\n        \" [= configuredMaxAllocation ]\" + \"\\n\" +\n        \"numContainers = \" + numContainers +\n        \" [= currentNumContainers ]\" + \"\\n\" +\n        \"state = \" + state +\n        \" [= configuredState ]\" + \"\\n\" +\n        \"acls = \" + aclsString +\n        \" [= configuredAcls ]\" + \"\\n\" + \n        \"nodeLocalityDelay = \" + nodeLocalityDelay + \"\\n\" +\n        \"labels=\" + labelStrBuilder.toString() + \"\\n\" +\n        \"reservationsContinueLooking = \" +\n        reservationsContinueLooking + \"\\n\" +\n        \"preemptionDisabled = \" + getPreemptionDisabled() + \"\\n\" +\n        \"defaultAppPriorityPerQueue = \" + defaultAppPriorityPerQueue);\n  }",
                "code_after_change": "  protected void setupQueueConfigs(Resource clusterResource)\n      throws IOException {\n    try {\n      writeLock.lock();\n      super.setupQueueConfigs(clusterResource);\n\n      this.lastClusterResource = clusterResource;\n\n      this.cachedResourceLimitsForHeadroom = new ResourceLimits(\n          clusterResource);\n\n      // Initialize headroom info, also used for calculating application\n      // master resource limits.  Since this happens during queue initialization\n      // and all queues may not be realized yet, we'll use (optimistic)\n      // absoluteMaxCapacity (it will be replaced with the more accurate\n      // absoluteMaxAvailCapacity during headroom/userlimit/allocation events)\n      setQueueResourceLimitsInfo(clusterResource);\n\n      CapacitySchedulerConfiguration conf = csContext.getConfiguration();\n\n      setOrderingPolicy(\n          conf.<FiCaSchedulerApp>getOrderingPolicy(getQueuePath()));\n\n      userLimit = conf.getUserLimit(getQueuePath());\n      userLimitFactor = conf.getUserLimitFactor(getQueuePath());\n\n      maxApplications = conf.getMaximumApplicationsPerQueue(getQueuePath());\n      if (maxApplications < 0) {\n        int maxGlobalPerQueueApps = conf.getGlobalMaximumApplicationsPerQueue();\n        if (maxGlobalPerQueueApps > 0) {\n          maxApplications = maxGlobalPerQueueApps;\n        } else {\n          int maxSystemApps = conf.getMaximumSystemApplications();\n          maxApplications =\n              (int) (maxSystemApps * queueCapacities.getAbsoluteCapacity());\n        }\n      }\n      maxApplicationsPerUser = Math.min(maxApplications,\n          (int) (maxApplications * (userLimit / 100.0f) * userLimitFactor));\n\n      maxAMResourcePerQueuePercent =\n          conf.getMaximumApplicationMasterResourcePerQueuePercent(\n              getQueuePath());\n\n      if (!SchedulerUtils.checkQueueLabelExpression(this.accessibleLabels,\n          this.defaultLabelExpression, null)) {\n        throw new IOException(\n            \"Invalid default label expression of \" + \" queue=\" + getQueueName()\n                + \" doesn't have permission to access all labels \"\n                + \"in default label expression. labelExpression of resource request=\"\n                + (this.defaultLabelExpression == null ?\n                \"\" :\n                this.defaultLabelExpression) + \". Queue labels=\" + (\n                getAccessibleNodeLabels() == null ?\n                    \"\" :\n                    StringUtils\n                        .join(getAccessibleNodeLabels().iterator(), ',')));\n      }\n\n      nodeLocalityDelay = conf.getNodeLocalityDelay();\n      rackLocalityFullReset = conf.getRackLocalityFullReset();\n\n      // re-init this since max allocation could have changed\n      this.minimumAllocationFactor = Resources.ratio(resourceCalculator,\n          Resources.subtract(maximumAllocation, minimumAllocation),\n          maximumAllocation);\n\n      StringBuilder aclsString = new StringBuilder();\n      for (Map.Entry<AccessType, AccessControlList> e : acls.entrySet()) {\n        aclsString.append(e.getKey() + \":\" + e.getValue().getAclString());\n      }\n\n      StringBuilder labelStrBuilder = new StringBuilder();\n      if (accessibleLabels != null) {\n        for (String s : accessibleLabels) {\n          labelStrBuilder.append(s);\n          labelStrBuilder.append(\",\");\n        }\n      }\n\n      defaultAppPriorityPerQueue = Priority.newInstance(\n          conf.getDefaultApplicationPriorityConfPerQueue(getQueuePath()));\n\n      LOG.info(\n          \"Initializing \" + queueName + \"\\n\" + \"capacity = \" + queueCapacities\n              .getCapacity() + \" [= (float) configuredCapacity / 100 ]\" + \"\\n\"\n              + \"absoluteCapacity = \" + queueCapacities.getAbsoluteCapacity()\n              + \" [= parentAbsoluteCapacity * capacity ]\" + \"\\n\"\n              + \"maxCapacity = \" + queueCapacities.getMaximumCapacity()\n              + \" [= configuredMaxCapacity ]\" + \"\\n\" + \"absoluteMaxCapacity = \"\n              + queueCapacities.getAbsoluteMaximumCapacity()\n              + \" [= 1.0 maximumCapacity undefined, \"\n              + \"(parentAbsoluteMaxCapacity * maximumCapacity) / 100 otherwise ]\"\n              + \"\\n\" + \"userLimit = \" + userLimit + \" [= configuredUserLimit ]\"\n              + \"\\n\" + \"userLimitFactor = \" + userLimitFactor\n              + \" [= configuredUserLimitFactor ]\" + \"\\n\" + \"maxApplications = \"\n              + maxApplications\n              + \" [= configuredMaximumSystemApplicationsPerQueue or\"\n              + \" (int)(configuredMaximumSystemApplications * absoluteCapacity)]\"\n              + \"\\n\" + \"maxApplicationsPerUser = \" + maxApplicationsPerUser\n              + \" [= (int)(maxApplications * (userLimit / 100.0f) * \"\n              + \"userLimitFactor) ]\" + \"\\n\" + \"usedCapacity = \"\n              + queueCapacities.getUsedCapacity() + \" [= usedResourcesMemory / \"\n              + \"(clusterResourceMemory * absoluteCapacity)]\" + \"\\n\"\n              + \"absoluteUsedCapacity = \" + absoluteUsedCapacity\n              + \" [= usedResourcesMemory / clusterResourceMemory]\" + \"\\n\"\n              + \"maxAMResourcePerQueuePercent = \" + maxAMResourcePerQueuePercent\n              + \" [= configuredMaximumAMResourcePercent ]\" + \"\\n\"\n              + \"minimumAllocationFactor = \" + minimumAllocationFactor\n              + \" [= (float)(maximumAllocationMemory - minimumAllocationMemory) / \"\n              + \"maximumAllocationMemory ]\" + \"\\n\" + \"maximumAllocation = \"\n              + maximumAllocation + \" [= configuredMaxAllocation ]\" + \"\\n\"\n              + \"numContainers = \" + numContainers\n              + \" [= currentNumContainers ]\" + \"\\n\" + \"state = \" + state\n              + \" [= configuredState ]\" + \"\\n\" + \"acls = \" + aclsString\n              + \" [= configuredAcls ]\" + \"\\n\" + \"nodeLocalityDelay = \"\n              + nodeLocalityDelay + \"\\n\" + \"labels=\" + labelStrBuilder\n              .toString() + \"\\n\" + \"reservationsContinueLooking = \"\n              + reservationsContinueLooking + \"\\n\" + \"preemptionDisabled = \"\n              + getPreemptionDisabled() + \"\\n\" + \"defaultAppPriorityPerQueue = \"\n              + defaultAppPriorityPerQueue);\n    } finally {\n      writeLock.unlock();\n    }\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Buggy Method"
            },
            "fix_suggestion": "Preventive",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Same Class or Module"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the root cause as a configuration issue with the `root.default` queue having a capacity of zero, which is related to the problem but not the precise root cause in the ground truth methods. It points to a configuration issue rather than a specific method, which is why it is categorized as 'Buggy Method'. The fix suggestion is preventive as it suggests checking and adjusting the configuration to prevent the issue, which aligns with the nature of the problem but does not match the developer's code fix. The problem location identification is partial because it mentions the class `org.apache.hadoop.yarn.conf.YarnConfiguration`, which is in the same module as the ground truth methods but does not specify the exact methods. There is no wrong information in the bug report as all the details are relevant to the context of the bug."
        }
    },
    {
        "filename": "YARN-301.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AppSchedulable.assignContainer": {
                "code_before_change": "  private Resource assignContainer(FSSchedulerNode node,\n      FSSchedulerApp application, Priority priority,\n      ResourceRequest request, NodeType type, boolean reserved) {\n\n    // How much does this request need?\n    Resource capability = request.getCapability();\n\n    // How much does the node have?\n    Resource available = node.getAvailableResource();\n\n    Container container = null;\n    if (reserved) {\n      container = node.getReservedContainer().getContainer();\n    } else {\n      container = createContainer(application, node, capability, priority);\n    }\n\n    // Can we allocate a container on this node?\n    int availableContainers =\n        available.getMemory() / capability.getMemory();\n\n    if (availableContainers > 0) {\n      // Inform the application of the new container for this request\n      RMContainer allocatedContainer =\n          application.allocate(type, node, priority, request, container);\n      if (allocatedContainer == null) {\n        // Did the application need this resource?\n        return Resources.none();\n      }\n      else {\n        // TODO this should subtract resource just assigned\n        // TEMPROARY\n        getMetrics().setAvailableResourcesToQueue(\n            scheduler.getClusterCapacity());\n      }\n\n\n      // If we had previously made a reservation, delete it\n      if (reserved) {\n        unreserve(application, priority, node);\n      }\n\n      // Inform the node\n      node.allocateContainer(application.getApplicationId(),\n          allocatedContainer);\n\n      return container.getResource();\n    } else {\n      // The desired container won't fit here, so reserve\n      reserve(application, priority, node, container, reserved);\n\n      return Resources.none();\n    }\n  }",
                "code_after_change": "  private Resource assignContainer(FSSchedulerNode node,\n      FSSchedulerApp application, Priority priority,\n      ResourceRequest request, NodeType type, boolean reserved) {\n\n    // How much does this request need?\n    Resource capability = request.getCapability();\n\n    // How much does the node have?\n    Resource available = node.getAvailableResource();\n\n    Container container = null;\n    if (reserved) {\n      container = node.getReservedContainer().getContainer();\n    } else {\n      container = createContainer(application, node, capability, priority);\n    }\n\n    // Can we allocate a container on this node?\n    int availableContainers =\n        available.getMemory() / capability.getMemory();\n\n    if (availableContainers > 0) {\n      // Inform the application of the new container for this request\n      RMContainer allocatedContainer =\n          application.allocate(type, node, priority, request, container);\n      if (allocatedContainer == null) {\n        // Did the application need this resource?\n        return Resources.none();\n      }\n      else {\n        // TODO this should subtract resource just assigned\n        // TEMPROARY\n        getMetrics().setAvailableResourcesToQueue(\n            scheduler.getClusterCapacity());\n      }\n\n\n      // If we had previously made a reservation, delete it\n      if (reserved) {\n        unreserve(application, priority, node);\n      }\n\n      // Inform the node\n      node.allocateContainer(application.getApplicationId(),\n          allocatedContainer);\n\n      return container.getResource();\n    } else {\n      // The desired container won't fit here, so reserve\n      reserve(application, priority, node, container, reserved);\n\n      return Resources.none();\n    }\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Preventive",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report precisely identifies the root cause by mentioning the 'assignContainer' method in the 'AppSchedulable' class, which is part of the ground truth methods. The fix suggestion is preventive as it suggests using concurrent collections like 'CopyOnWriteArrayList' to prevent the 'ConcurrentModificationException', which is a valid approach to mitigate the issue, although it doesn't match the developer's fix. The problem location identification is precise as it correctly mentions the 'assignContainer' method in the 'AppSchedulable' class, which is the ground truth method. There is no wrong information in the bug report as all the details are relevant and grounded in the context of the bug."
        }
    },
    {
        "filename": "YARN-7942.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-registry.src.main.java.org.apache.hadoop.registry.client.impl.zk.RegistrySecurity.applySecurityEnvironment": {
                "code_before_change": "  public void applySecurityEnvironment(CuratorFrameworkFactory.Builder builder) {\n\n    if (isSecureRegistry()) {\n      switch (access) {\n        case anon:\n          clearZKSaslClientProperties();\n          break;\n\n        case digest:\n          // no SASL\n          clearZKSaslClientProperties();\n          builder.authorization(SCHEME_DIGEST, digestAuthData);\n          break;\n\n        case sasl:\n          JaasConfiguration jconf =\n              new JaasConfiguration(jaasClientEntry, principal, keytab);\n          javax.security.auth.login.Configuration.setConfiguration(jconf);\n          setSystemPropertyIfUnset(ZooKeeperSaslClient.ENABLE_CLIENT_SASL_KEY,\n              \"true\");\n          setSystemPropertyIfUnset(ZooKeeperSaslClient.LOGIN_CONTEXT_NAME_KEY,\n              jaasClientEntry);\n          LOG.info(\n              \"Enabling ZK sasl client: jaasClientEntry = \" + jaasClientEntry\n                  + \", principal = \" + principal + \", keytab = \" + keytab);\n        default:\n          clearZKSaslClientProperties();\n          break;\n      }\n    }\n  }",
                "code_after_change": "  public void applySecurityEnvironment(CuratorFrameworkFactory.Builder\n      builder) throws IOException {\n\n    if (isSecureRegistry()) {\n      switch (access) {\n        case anon:\n          clearZKSaslClientProperties();\n          break;\n\n        case digest:\n          // no SASL\n          clearZKSaslClientProperties();\n          builder.authorization(SCHEME_DIGEST, digestAuthData);\n          break;\n\n        case sasl:\n          String existingJaasConf = System.getProperty(\n              \"java.security.auth.login.config\");\n          if (existingJaasConf == null || existingJaasConf.isEmpty()) {\n            if (principal == null || keytab == null) {\n              throw new IOException(\"SASL is configured for registry, \" +\n                  \"but neither keytab/principal nor java.security.auth.login\" +\n                  \".config system property are specified\");\n            }\n            // in this case, keytab and principal are specified and no jaas\n            // config is specified, so we will create one\n            LOG.info(\n                \"Enabling ZK sasl client: jaasClientEntry = \" + jaasClientEntry\n                    + \", principal = \" + principal + \", keytab = \" + keytab);\n            JaasConfiguration jconf =\n                new JaasConfiguration(jaasClientEntry, principal, keytab);\n            javax.security.auth.login.Configuration.setConfiguration(jconf);\n            setSystemPropertyIfUnset(ZooKeeperSaslClient.ENABLE_CLIENT_SASL_KEY,\n                \"true\");\n            setSystemPropertyIfUnset(ZooKeeperSaslClient.LOGIN_CONTEXT_NAME_KEY,\n                jaasClientEntry);\n          } else {\n            // in this case, jaas config is specified so we will not change it\n            LOG.info(\"Using existing ZK sasl configuration: \" +\n                \"jaasClientEntry = \" + System.getProperty(\n                    ZooKeeperSaslClient.LOGIN_CONTEXT_NAME_KEY, \"Client\") +\n                \", sasl client = \" + System.getProperty(\n                    ZooKeeperSaslClient.ENABLE_CLIENT_SASL_KEY,\n                    ZooKeeperSaslClient.ENABLE_CLIENT_SASL_DEFAULT) +\n                \", jaas = \" + existingJaasConf);\n          }\n          break;\n\n        default:\n          clearZKSaslClientProperties();\n          break;\n      }\n    }\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-registry.src.main.java.org.apache.hadoop.registry.client.impl.zk.RegistrySecurity.validateContext": {
                "code_before_change": "  public static AppConfigurationEntry[] validateContext(String context)  {\n    if (context == null) {\n      throw new RuntimeException(\"Null context argument\");\n    }\n    if (context.isEmpty()) {\n      throw new RuntimeException(\"Empty context argument\");\n    }\n    javax.security.auth.login.Configuration configuration =\n        javax.security.auth.login.Configuration.getConfiguration();\n    AppConfigurationEntry[] entries =\n        configuration.getAppConfigurationEntry(context);\n    if (entries == null) {\n      throw new RuntimeException(\n          String.format(\"Entry \\\"%s\\\" not found; \" +\n                        \"JAAS config = %s\",\n              context,\n              describeProperty(Environment.JAAS_CONF_KEY) ));\n    }\n    return entries;\n  }",
                "code_after_change": "  public static AppConfigurationEntry[] validateContext(String context)  {\n    if (context == null) {\n      throw new RuntimeException(\"Null context argument\");\n    }\n    if (context.isEmpty()) {\n      throw new RuntimeException(\"Empty context argument\");\n    }\n    javax.security.auth.login.Configuration configuration =\n        javax.security.auth.login.Configuration.getConfiguration();\n    AppConfigurationEntry[] entries =\n        configuration.getAppConfigurationEntry(context);\n    if (entries == null) {\n      throw new RuntimeException(\n          String.format(\"Entry \\\"%s\\\" not found; \" +\n                        \"JAAS config = %s\",\n              context,\n              describeProperty(Environment.JAAS_CONF_KEY) ));\n    }\n    return entries;\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Preventive",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the issue as a lack of proper authorization for the user or service attempting to delete the znode in ZooKeeper, which is related to the ground truth methods that deal with security and authorization. However, it does not precisely identify the root cause in the ground truth methods, but it is in the shared stack trace context. The fix suggestion involves verifying and updating ACLs, which is preventive as it addresses the authorization issue but does not match the exact code changes made in the ground truth methods. The problem location identification mentions methods in the stack trace context but not the exact ground truth methods. There is no wrong information in the bug report as it accurately describes the observed behavior and potential cause."
        }
    },
    {
        "filename": "YARN-7692.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.createAndPopulateNewRMApp": {
                "code_before_change": "  private RMAppImpl createAndPopulateNewRMApp(\n      ApplicationSubmissionContext submissionContext, long submitTime,\n      String user, boolean isRecovery, long startTime) throws YarnException {\n\n    ApplicationPlacementContext placementContext = null;\n    try {\n      placementContext = placeApplication(rmContext, submissionContext, user);\n    } catch (YarnException e) {\n      String msg =\n          \"Failed to place application \" + submissionContext.getApplicationId()\n              + \" to queue and specified \" + \"queue is invalid : \"\n              + submissionContext.getQueue();\n      LOG.error(msg, e);\n      throw e;\n    }\n\n    // We only replace the queue when it's a new application\n    if (!isRecovery) {\n      replaceQueueFromPlacementContext(placementContext, submissionContext);\n\n      // fail the submission if configured application timeout value is invalid\n      RMServerUtils.validateApplicationTimeouts(\n          submissionContext.getApplicationTimeouts());\n    }\n\n    ApplicationId applicationId = submissionContext.getApplicationId();\n    List<ResourceRequest> amReqs = validateAndCreateResourceRequest(\n        submissionContext, isRecovery);\n\n    // Verify and get the update application priority and set back to\n    // submissionContext\n    UserGroupInformation userUgi = UserGroupInformation.createRemoteUser(user);\n    Priority appPriority = scheduler.checkAndGetApplicationPriority(\n        submissionContext.getPriority(), userUgi, submissionContext.getQueue(),\n        applicationId);\n    submissionContext.setPriority(appPriority);\n\n    // Since FairScheduler queue mapping is done inside scheduler,\n    // if FairScheduler is used and the queue doesn't exist, we should not\n    // fail here because queue will be created inside FS. Ideally, FS queue\n    // mapping should be done outside scheduler too like CS.\n    // For now, exclude FS for the acl check.\n    if (!isRecovery && YarnConfiguration.isAclEnabled(conf)\n        && scheduler instanceof CapacityScheduler) {\n      String queueName = submissionContext.getQueue();\n      String appName = submissionContext.getApplicationName();\n      CSQueue csqueue = ((CapacityScheduler) scheduler).getQueue(queueName);\n\n      if (csqueue == null && placementContext != null) {\n        //could be an auto created queue through queue mapping. Validate\n        // parent queue exists and has valid acls\n        String parentQueueName = placementContext.getParentQueue();\n        csqueue = ((CapacityScheduler) scheduler).getQueue(parentQueueName);\n      }\n\n      if (csqueue != null\n          && !authorizer.checkPermission(\n              new AccessRequest(csqueue.getPrivilegedEntity(), userUgi,\n                  SchedulerUtils.toAccessType(QueueACL.SUBMIT_APPLICATIONS),\n                  applicationId.toString(), appName, Server.getRemoteAddress(),\n                  null))\n          && !authorizer.checkPermission(\n              new AccessRequest(csqueue.getPrivilegedEntity(), userUgi,\n                  SchedulerUtils.toAccessType(QueueACL.ADMINISTER_QUEUE),\n                  applicationId.toString(), appName, Server.getRemoteAddress(),\n                  null))) {\n        throw RPCUtil.getRemoteException(new AccessControlException(\n            \"User \" + user + \" does not have permission to submit \"\n                + applicationId + \" to queue \" + submissionContext.getQueue()));\n      }\n    }\n\n    // Create RMApp\n    RMAppImpl application =\n        new RMAppImpl(applicationId, rmContext, this.conf,\n            submissionContext.getApplicationName(), user,\n            submissionContext.getQueue(),\n            submissionContext, this.scheduler, this.masterService,\n            submitTime, submissionContext.getApplicationType(),\n            submissionContext.getApplicationTags(), amReqs, placementContext,\n            startTime);\n    // Concurrent app submissions with same applicationId will fail here\n    // Concurrent app submissions with different applicationIds will not\n    // influence each other\n    if (rmContext.getRMApps().putIfAbsent(applicationId, application) !=\n        null) {\n      String message = \"Application with id \" + applicationId\n          + \" is already present! Cannot add a duplicate!\";\n      LOG.warn(message);\n      throw new YarnException(message);\n    }\n\n    if (YarnConfiguration.timelineServiceV2Enabled(conf)) {\n      // Start timeline collector for the submitted app\n      application.startTimelineCollector();\n    }\n    // Inform the ACLs Manager\n    this.applicationACLsManager.addApplication(applicationId,\n        submissionContext.getAMContainerSpec().getApplicationACLs());\n    String appViewACLs = submissionContext.getAMContainerSpec()\n        .getApplicationACLs().get(ApplicationAccessType.VIEW_APP);\n    rmContext.getSystemMetricsPublisher().appACLsUpdated(\n        application, appViewACLs, System.currentTimeMillis());\n    return application;\n  }",
                "code_after_change": "  private RMAppImpl createAndPopulateNewRMApp(\n      ApplicationSubmissionContext submissionContext, long submitTime,\n      String user, boolean isRecovery, long startTime) throws YarnException {\n    if (!isRecovery) {\n      // Do queue mapping\n      if (rmContext.getQueuePlacementManager() != null) {\n        // We only do queue mapping when it's a new application\n        rmContext.getQueuePlacementManager().placeApplication(\n            submissionContext, user);\n      }\n      // fail the submission if configured application timeout value is invalid\n      RMServerUtils.validateApplicationTimeouts(\n          submissionContext.getApplicationTimeouts());\n    }\n\n    ApplicationId applicationId = submissionContext.getApplicationId();\n    List<ResourceRequest> amReqs = validateAndCreateResourceRequest(\n        submissionContext, isRecovery);\n\n    // Verify and get the update application priority and set back to\n    // submissionContext\n    UserGroupInformation userUgi = UserGroupInformation.createRemoteUser(user);\n\n    // Application priority needed to be validated only while submitting. During\n    // recovery, validated priority could be recovered from submission context.\n    if (!isRecovery) {\n      Priority appPriority = scheduler.checkAndGetApplicationPriority(\n          submissionContext.getPriority(), userUgi,\n          submissionContext.getQueue(),\n          applicationId);\n      submissionContext.setPriority(appPriority);\n    }\n\n    // Since FairScheduler queue mapping is done inside scheduler,\n    // if FairScheduler is used and the queue doesn't exist, we should not\n    // fail here because queue will be created inside FS. Ideally, FS queue\n    // mapping should be done outside scheduler too like CS.\n    // For now, exclude FS for the acl check.\n    if (!isRecovery && YarnConfiguration.isAclEnabled(conf)\n        && scheduler instanceof CapacityScheduler) {\n      String queueName = submissionContext.getQueue();\n      String appName = submissionContext.getApplicationName();\n      CSQueue csqueue = ((CapacityScheduler) scheduler).getQueue(queueName);\n      if (null != csqueue\n          && !authorizer.checkPermission(\n              new AccessRequest(csqueue.getPrivilegedEntity(), userUgi,\n                  SchedulerUtils.toAccessType(QueueACL.SUBMIT_APPLICATIONS),\n                  applicationId.toString(), appName, Server.getRemoteAddress(),\n                  null))\n          && !authorizer.checkPermission(\n              new AccessRequest(csqueue.getPrivilegedEntity(), userUgi,\n                  SchedulerUtils.toAccessType(QueueACL.ADMINISTER_QUEUE),\n                  applicationId.toString(), appName, Server.getRemoteAddress(),\n                  null))) {\n        throw RPCUtil.getRemoteException(new AccessControlException(\n            \"User \" + user + \" does not have permission to submit \"\n                + applicationId + \" to queue \" + submissionContext.getQueue()));\n      }\n    }\n\n    // Create RMApp\n    RMAppImpl application =\n        new RMAppImpl(applicationId, rmContext, this.conf,\n            submissionContext.getApplicationName(), user,\n            submissionContext.getQueue(),\n            submissionContext, this.scheduler, this.masterService,\n            submitTime, submissionContext.getApplicationType(),\n            submissionContext.getApplicationTags(), amReqs, startTime);\n    // Concurrent app submissions with same applicationId will fail here\n    // Concurrent app submissions with different applicationIds will not\n    // influence each other\n    if (rmContext.getRMApps().putIfAbsent(applicationId, application) !=\n        null) {\n      String message = \"Application with id \" + applicationId\n          + \" is already present! Cannot add a duplicate!\";\n      LOG.warn(message);\n      throw new YarnException(message);\n    }\n\n    if (YarnConfiguration.timelineServiceV2Enabled(conf)) {\n      // Start timeline collector for the submitted app\n      application.startTimelineCollector();\n    }\n    // Inform the ACLs Manager\n    this.applicationACLsManager.addApplication(applicationId,\n        submissionContext.getAMContainerSpec().getApplicationACLs());\n    String appViewACLs = submissionContext.getAMContainerSpec()\n        .getApplicationACLs().get(ApplicationAccessType.VIEW_APP);\n    rmContext.getSystemMetricsPublisher().appACLsUpdated(\n        application, appViewACLs, System.currentTimeMillis());\n    return application;\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Direct Caller/Callee"
            },
            "fix_suggestion": "Preventive",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Direct Caller/Callee"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the method `checkAndGetApplicationPriority` in the `CapacityScheduler` class as the root cause, which is a direct callee of the ground truth method `createAndPopulateNewRMApp`. Therefore, the root cause identification is 'Partial' with the sub-category 'Direct Caller/Callee'. The fix suggestion is 'Preventive' as it suggests modifying the recovery logic to handle permission checks more gracefully, which would prevent the crash but does not match the exact developer's fix. The problem location identification is 'Partial' with the sub-category 'Direct Caller/Callee' because it mentions `checkAndGetApplicationPriority`, which is directly called by the ground truth method. There is no wrong information in the bug report as all the details are relevant to the context of the bug."
        }
    },
    {
        "filename": "YARN-3917.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.util.ResourceCalculatorPlugin.getResourceCalculatorPlugin": {
                "code_before_change": "  public static ResourceCalculatorPlugin getResourceCalculatorPlugin(\n      Class<? extends ResourceCalculatorPlugin> clazz, Configuration conf) {\n\n    if (clazz != null) {\n      return ReflectionUtils.newInstance(clazz, conf);\n    }\n    try {\n      return new ResourceCalculatorPlugin();\n    } catch (SecurityException e) {\n      return null;\n    }\n  }",
                "code_after_change": "  public static ResourceCalculatorPlugin getResourceCalculatorPlugin(\n      Class<? extends ResourceCalculatorPlugin> clazz, Configuration conf) {\n\n    if (clazz != null) {\n      return ReflectionUtils.newInstance(clazz, conf);\n    }\n    try {\n      return new ResourceCalculatorPlugin();\n    } catch (Throwable t) {\n      LOG.warn(t + \": Failed to instantiate default resource calculator.\", t);\n    }\n    return null;\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Direct Caller/Callee"
            },
            "fix_suggestion": "Preventive",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Direct Caller/Callee"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the root cause as the `UnsupportedOperationException` thrown by the `SysInfo.newInstance()` method, which is a direct callee of the ground truth method `ResourceCalculatorPlugin.getResourceCalculatorPlugin`. The fix suggestion involves implementing exception handling in the `ResourceCalculatorPlugin`, which is preventive as it suggests handling exceptions to prevent the bug. The problem location identification is partial because it mentions `ResourceCalculatorPlugin.init`, which is closely related to the ground truth method as it is in the same class and directly calls the ground truth method. There is no wrong information in the bug report as all details are relevant to the context of the bug."
        }
    },
    {
        "filename": "YARN-3537.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.NodeManager.initAndStartRecoveryStore": {
                "code_before_change": "  private void initAndStartRecoveryStore(Configuration conf)\n      throws IOException {\n    boolean recoveryEnabled = conf.getBoolean(\n        YarnConfiguration.NM_RECOVERY_ENABLED,\n        YarnConfiguration.DEFAULT_NM_RECOVERY_ENABLED);\n    if (recoveryEnabled) {\n      FileSystem recoveryFs = FileSystem.getLocal(conf);\n      String recoveryDirName = conf.get(YarnConfiguration.NM_RECOVERY_DIR);\n      if (recoveryDirName == null) {\n        throw new IllegalArgumentException(\"Recovery is enabled but \" +\n            YarnConfiguration.NM_RECOVERY_DIR + \" is not set.\");\n      }\n      Path recoveryRoot = new Path(recoveryDirName);\n      recoveryFs.mkdirs(recoveryRoot, new FsPermission((short)0700));\n      nmStore = new NMLeveldbStateStoreService();\n    } else {\n      nmStore = new NMNullStateStoreService();\n    }\n    nmStore.init(conf);\n    nmStore.start();\n  }",
                "code_after_change": "  private void initAndStartRecoveryStore(Configuration conf)\n      throws IOException {\n    boolean recoveryEnabled = conf.getBoolean(\n        YarnConfiguration.NM_RECOVERY_ENABLED,\n        YarnConfiguration.DEFAULT_NM_RECOVERY_ENABLED);\n    if (recoveryEnabled) {\n      FileSystem recoveryFs = FileSystem.getLocal(conf);\n      String recoveryDirName = conf.get(YarnConfiguration.NM_RECOVERY_DIR);\n      if (recoveryDirName == null) {\n        throw new IllegalArgumentException(\"Recovery is enabled but \" +\n            YarnConfiguration.NM_RECOVERY_DIR + \" is not set.\");\n      }\n      Path recoveryRoot = new Path(recoveryDirName);\n      recoveryFs.mkdirs(recoveryRoot, new FsPermission((short)0700));\n      nmStore = new NMLeveldbStateStoreService();\n    } else {\n      nmStore = new NMNullStateStoreService();\n    }\n    nmStore.init(conf);\n    nmStore.start();\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.NodeManager.stopRecoveryStore": {
                "code_before_change": "  private void stopRecoveryStore() throws IOException {\n    nmStore.stop();\n    if (null != context) {\n      if (context.getDecommissioned() && nmStore.canRecover()) {\n        LOG.info(\"Removing state store due to decommission\");\n        Configuration conf = getConfig();\n        Path recoveryRoot =\n            new Path(conf.get(YarnConfiguration.NM_RECOVERY_DIR));\n        LOG.info(\"Removing state store at \" + recoveryRoot\n            + \" due to decommission\");\n        FileSystem recoveryFs = FileSystem.getLocal(conf);\n        if (!recoveryFs.delete(recoveryRoot, true)) {\n          LOG.warn(\"Unable to delete \" + recoveryRoot);\n        }\n      }\n    }\n  }",
                "code_after_change": "  private void stopRecoveryStore() throws IOException {\n    if (null != nmStore) {\n      nmStore.stop();\n      if (null != context) {\n        if (context.getDecommissioned() && nmStore.canRecover()) {\n          LOG.info(\"Removing state store due to decommission\");\n          Configuration conf = getConfig();\n          Path recoveryRoot =\n              new Path(conf.get(YarnConfiguration.NM_RECOVERY_DIR));\n          LOG.info(\"Removing state store at \" + recoveryRoot\n              + \" due to decommission\");\n          FileSystem recoveryFs = FileSystem.getLocal(conf);\n          if (!recoveryFs.delete(recoveryRoot, true)) {\n            LOG.warn(\"Unable to delete \" + recoveryRoot);\n          }\n        }\n      }\n    }\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Correct",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report precisely identifies the root cause of the NullPointerException as the nmStore not being initialized when recovery is disabled, which directly relates to the ground truth method 'NodeManager.initAndStartRecoveryStore'. The fix suggestion in the bug report matches the developer's fix, which involves adding a null check in the 'stopRecoveryStore' method. The problem location is also precise as it mentions the ground truth methods 'NodeManager.stopRecoveryStore' and 'NodeManager.initAndStartRecoveryStore'. There is no wrong information in the bug report as all details are consistent with the context of the bug."
        }
    },
    {
        "filename": "YARN-7962.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.security.DelegationTokenRenewer.serviceStop": {
                "code_before_change": "  protected void serviceStop() {\n    if (renewalTimer != null) {\n      renewalTimer.cancel();\n    }\n    appTokens.clear();\n    allTokens.clear();\n    this.renewerService.shutdown();\n    dtCancelThread.interrupt();\n    try {\n      dtCancelThread.join(1000);\n    } catch (InterruptedException e) {\n      e.printStackTrace();\n    }\n    if (tokenKeepAliveEnabled && delayedRemovalThread != null) {\n      delayedRemovalThread.interrupt();\n      try {\n        delayedRemovalThread.join(1000);\n      } catch (InterruptedException e) {\n        LOG.info(\"Interrupted while joining on delayed removal thread.\", e);\n      }\n    }\n  }",
                "code_after_change": "  protected void serviceStop() {\n    if (renewalTimer != null) {\n      renewalTimer.cancel();\n    }\n    appTokens.clear();\n    allTokens.clear();\n\n    serviceStateLock.writeLock().lock();\n    try {\n      isServiceStarted = false;\n      this.renewerService.shutdown();\n    } finally {\n      serviceStateLock.writeLock().unlock();\n    }\n\n    dtCancelThread.interrupt();\n    try {\n      dtCancelThread.join(1000);\n    } catch (InterruptedException e) {\n      e.printStackTrace();\n    }\n    if (tokenKeepAliveEnabled && delayedRemovalThread != null) {\n      delayedRemovalThread.interrupt();\n      try {\n        delayedRemovalThread.join(1000);\n      } catch (InterruptedException e) {\n        LOG.info(\"Interrupted while joining on delayed removal thread.\", e);\n      }\n    }\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Correct",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report precisely identifies the root cause as the `serviceStop` method not setting the `isServiceStarted` flag to false before shutting down the `ThreadPoolExecutor`, which matches the ground truth method. The fix suggestion in the bug report is correct as it matches the developer's fix by acquiring a lock and setting `isServiceStarted` to false before shutting down the executor. The problem location is also precise as it mentions the `serviceStop` method, which is the ground truth method. There is no wrong information in the bug report as all details are consistent with the context of the bug."
        }
    },
    {
        "filename": "YARN-8357.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-applications.hadoop-yarn-services.hadoop-yarn-services-core.src.main.java.org.apache.hadoop.yarn.service.client.ServiceClient.getStatus": {
                "code_before_change": "  public Service getStatus(String serviceName)\n      throws IOException, YarnException {\n    ServiceApiUtil.validateNameFormat(serviceName, getConfig());\n    Service appSpec = new Service();\n    appSpec.setName(serviceName);\n    ApplicationId currentAppId = getAppId(serviceName);\n    if (currentAppId == null) {\n      LOG.info(\"Service {} does not have an application ID\", serviceName);\n      return appSpec;\n    }\n    ApplicationReport appReport = yarnClient.getApplicationReport(currentAppId);\n    appSpec.setState(convertState(appReport.getYarnApplicationState()));\n    ApplicationTimeout lifetime =\n        appReport.getApplicationTimeouts().get(ApplicationTimeoutType.LIFETIME);\n    if (lifetime != null) {\n      appSpec.setLifetime(lifetime.getRemainingTime());\n    }\n\n    if (appReport.getYarnApplicationState() != RUNNING) {\n      LOG.info(\"Service {} is at {} state\", serviceName,\n          appReport.getYarnApplicationState());\n      return appSpec;\n    }\n    if (StringUtils.isEmpty(appReport.getHost())) {\n      LOG.warn(serviceName + \" AM hostname is empty\");\n      return appSpec;\n    }\n    ClientAMProtocol amProxy =\n        createAMProxy(serviceName, appReport);\n    GetStatusResponseProto response =\n        amProxy.getStatus(GetStatusRequestProto.newBuilder().build());\n    appSpec = jsonSerDeser.fromJson(response.getStatus());\n    if (lifetime != null) {\n      appSpec.setLifetime(lifetime.getRemainingTime());\n    }\n    return appSpec;\n  }",
                "code_after_change": "  public Service getStatus(String serviceName)\n      throws IOException, YarnException {\n    ServiceApiUtil.validateNameFormat(serviceName, getConfig());\n    Service appSpec = new Service();\n    appSpec.setName(serviceName);\n    appSpec.setState(ServiceState.STOPPED);\n    ApplicationId currentAppId = getAppId(serviceName);\n    if (currentAppId == null) {\n      LOG.info(\"Service {} does not have an application ID\", serviceName);\n      return appSpec;\n    }\n    ApplicationReport appReport = yarnClient.getApplicationReport(currentAppId);\n    appSpec.setState(convertState(appReport.getYarnApplicationState()));\n    ApplicationTimeout lifetime =\n        appReport.getApplicationTimeouts().get(ApplicationTimeoutType.LIFETIME);\n    if (lifetime != null) {\n      appSpec.setLifetime(lifetime.getRemainingTime());\n    }\n\n    if (appReport.getYarnApplicationState() != RUNNING) {\n      LOG.info(\"Service {} is at {} state\", serviceName,\n          appReport.getYarnApplicationState());\n      return appSpec;\n    }\n    if (StringUtils.isEmpty(appReport.getHost())) {\n      LOG.warn(serviceName + \" AM hostname is empty\");\n      return appSpec;\n    }\n    ClientAMProtocol amProxy =\n        createAMProxy(serviceName, appReport);\n    GetStatusResponseProto response =\n        amProxy.getStatus(GetStatusRequestProto.newBuilder().build());\n    appSpec = jsonSerDeser.fromJson(response.getStatus());\n    if (lifetime != null) {\n      appSpec.setLifetime(lifetime.getRemainingTime());\n    }\n    return appSpec;\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Correct",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report accurately identifies the root cause of the NullPointerException as being related to the `getStatus` method in `ServiceClient`, which is consistent with the ground truth method. The fix suggestion in the bug report matches the developer's fix, which involves ensuring that the service is properly initialized and has a valid application ID, as seen in the 'after' version of the `getStatus` method where the state is set to `ServiceState.STOPPED` initially. The problem location is precisely identified, as the `problem_location` field mentions the `ServiceClient.getStatus` method, which is part of the ground truth list. There is no wrong information in the bug report; all details are relevant and correctly describe the issue."
        }
    },
    {
        "filename": "YARN-6534.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.client.api.impl.TimelineConnector.serviceInit": {
                "code_before_change": "  protected void serviceInit(Configuration conf) throws Exception {\n    super.serviceInit(conf);\n    ClientConfig cc = new DefaultClientConfig();\n    cc.getClasses().add(YarnJacksonJaxbJsonProvider.class);\n\n    sslFactory = getSSLFactory(conf);\n    connConfigurator = getConnConfigurator(sslFactory);\n\n    if (UserGroupInformation.isSecurityEnabled()) {\n      authenticator = new KerberosDelegationTokenAuthenticator();\n    } else {\n      authenticator = new PseudoDelegationTokenAuthenticator();\n    }\n    authenticator.setConnectionConfigurator(connConfigurator);\n\n    connectionRetry = new TimelineClientConnectionRetry(conf);\n    client =\n        new Client(\n            new URLConnectionClientHandler(new TimelineURLConnectionFactory(\n                authUgi, authenticator, connConfigurator, token, doAsUser)),\n            cc);\n    if (requireConnectionRetry) {\n      TimelineJerseyRetryFilter retryFilter =\n          new TimelineJerseyRetryFilter(connectionRetry);\n      client.addFilter(retryFilter);\n    }\n  }",
                "code_after_change": "  protected void serviceInit(Configuration conf) throws Exception {\n    super.serviceInit(conf);\n    ClientConfig cc = new DefaultClientConfig();\n    cc.getClasses().add(YarnJacksonJaxbJsonProvider.class);\n\n    if (YarnConfiguration.useHttps(conf)) {\n      // If https is chosen, configures SSL client.\n      sslFactory = getSSLFactory(conf);\n      connConfigurator = getConnConfigurator(sslFactory);\n    } else {\n      connConfigurator = DEFAULT_TIMEOUT_CONN_CONFIGURATOR;\n    }\n\n    if (UserGroupInformation.isSecurityEnabled()) {\n      authenticator = new KerberosDelegationTokenAuthenticator();\n    } else {\n      authenticator = new PseudoDelegationTokenAuthenticator();\n    }\n    authenticator.setConnectionConfigurator(connConfigurator);\n\n    connectionRetry = new TimelineClientConnectionRetry(conf);\n    client =\n        new Client(\n            new URLConnectionClientHandler(new TimelineURLConnectionFactory(\n                authUgi, authenticator, connConfigurator, token, doAsUser)),\n            cc);\n    if (requireConnectionRetry) {\n      TimelineJerseyRetryFilter retryFilter =\n          new TimelineJerseyRetryFilter(connectionRetry);\n      client.addFilter(retryFilter);\n    }\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Correct",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report accurately identifies the root cause as the initialization of SSL components without checking if SSL is required, which matches the issue in the ground truth method 'TimelineConnector.serviceInit'. The fix suggestion in the report aligns with the developer's fix, which involves checking if HTTPS is enabled before initializing SSL components. The problem location identification is partial because it mentions methods like TimelineClientImpl.serviceInit, which are in the same stack trace context but not the exact ground truth method. There is no wrong information in the bug report as all details are relevant to the context of the bug."
        }
    },
    {
        "filename": "YARN-4227.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.completedContainerInternal": {
                "code_before_change": [],
                "code_after_change": "  protected void completedContainerInternal(\n      RMContainer rmContainer, ContainerStatus containerStatus,\n      RMContainerEventType event) {\n    try {\n      writeLock.lock();\n      Container container = rmContainer.getContainer();\n\n      // Get the application for the finished container\n      FSAppAttempt application = getCurrentAttemptForContainer(\n          container.getId());\n      ApplicationId appId =\n          container.getId().getApplicationAttemptId().getApplicationId();\n      if (application == null) {\n        LOG.info(\"Container \" + container + \" of finished application \" +\n            appId + \" completed with event \" + event);\n        return;\n      }\n\n      // Get the node on which the container was allocated\n      NodeId nodeID = container.getNodeId();\n      FSSchedulerNode node = getFSSchedulerNode(nodeID);\n      // node could be null if the thread was waiting for the lock and the node\n      // was removed in another thread\n      if (rmContainer.getState() == RMContainerState.RESERVED) {\n        if (node != null) {\n          application.unreserve(rmContainer.getReservedSchedulerKey(), node);\n        } else if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Skipping unreserve on removed node: \" + nodeID);\n        }\n      } else {\n        application.containerCompleted(rmContainer, containerStatus, event);\n        if (node != null) {\n          node.releaseContainer(rmContainer.getContainerId(), false);\n        } else if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Skipping container release on removed node: \" + nodeID);\n        }\n        updateRootQueueMetrics();\n      }\n\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Application attempt \" + application.getApplicationAttemptId()\n            + \" released container \" + container.getId() + \" on node: \" +\n            (node == null ? nodeID : node) + \" with event: \" + event);\n      }\n    } finally {\n      writeLock.unlock();\n    }\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Buggy Method"
            },
            "fix_suggestion": "Correct",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the root cause as a NullPointerException in the 'completedContainer' method of the FairScheduler class, which is the method where the error occurs but not where the actual fix was made. The actual fix was made in 'completedContainerInternal', making it a 'Buggy Method' sub-category. The suggestion to implement a null check for `rmContainer` in the `completedContainer` method matches the developer's fix, which involves handling null cases in the `completedContainerInternal` method. Therefore, it is marked as Correct. The `problem_location` field correctly mentions the `completedContainer` method in `FairScheduler`, which is directly related to the ground truth method `completedContainerInternal`. Thus, it is marked as Precise. There is no wrong information in the bug report as all details are relevant to the context of the bug."
        }
    },
    {
        "filename": "YARN-4288.json",
        "code_diff": {
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.io.retry.RetryPolicies.retryByRemoteException": {
                "code_before_change": "  public static final RetryPolicy retryByRemoteException(\n      RetryPolicy defaultPolicy,\n      Map<Class<? extends Exception>, RetryPolicy> exceptionToPolicyMap) {\n    return new RemoteExceptionDependentRetry(defaultPolicy, exceptionToPolicyMap);\n  }",
                "code_after_change": "  public static final RetryPolicy retryByRemoteException(\n      RetryPolicy defaultPolicy,\n      Map<Class<? extends Exception>, RetryPolicy> exceptionToPolicyMap) {\n    return new RemoteExceptionDependentRetry(defaultPolicy, exceptionToPolicyMap);\n  }"
            },
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.io.retry.RetryPolicies.shouldRetry": {
                "code_before_change": "    public RetryAction shouldRetry(Exception e, int retries, int failovers,\n        boolean isIdempotentOrAtMostOnce) throws Exception {\n      return RetryAction.FAIL;\n    }",
                "code_after_change": "    public RetryAction shouldRetry(Exception e, int retries, int failovers,\n        boolean isIdempotentOrAtMostOnce) throws Exception {\n      return RetryAction.FAIL;\n    }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.client.RMProxy.createRetryPolicy": {
                "code_before_change": "  public static RetryPolicy createRetryPolicy(Configuration conf) {\n    long rmConnectWaitMS =\n        conf.getLong(\n            YarnConfiguration.RESOURCEMANAGER_CONNECT_MAX_WAIT_MS,\n            YarnConfiguration.DEFAULT_RESOURCEMANAGER_CONNECT_MAX_WAIT_MS);\n    long rmConnectionRetryIntervalMS =\n        conf.getLong(\n            YarnConfiguration.RESOURCEMANAGER_CONNECT_RETRY_INTERVAL_MS,\n            YarnConfiguration\n                .DEFAULT_RESOURCEMANAGER_CONNECT_RETRY_INTERVAL_MS);\n\n    boolean waitForEver = (rmConnectWaitMS == -1);\n    if (!waitForEver) {\n      if (rmConnectWaitMS < 0) {\n        throw new YarnRuntimeException(\"Invalid Configuration. \"\n            + YarnConfiguration.RESOURCEMANAGER_CONNECT_MAX_WAIT_MS\n            + \" can be -1, but can not be other negative numbers\");\n      }\n\n      // try connect once\n      if (rmConnectWaitMS < rmConnectionRetryIntervalMS) {\n        LOG.warn(YarnConfiguration.RESOURCEMANAGER_CONNECT_MAX_WAIT_MS\n            + \" is smaller than \"\n            + YarnConfiguration.RESOURCEMANAGER_CONNECT_RETRY_INTERVAL_MS\n            + \". Only try connect once.\");\n        rmConnectWaitMS = 0;\n      }\n    }\n\n    // Handle HA case first\n    if (HAUtil.isHAEnabled(conf)) {\n      final long failoverSleepBaseMs = conf.getLong(\n          YarnConfiguration.CLIENT_FAILOVER_SLEEPTIME_BASE_MS,\n          rmConnectionRetryIntervalMS);\n\n      final long failoverSleepMaxMs = conf.getLong(\n          YarnConfiguration.CLIENT_FAILOVER_SLEEPTIME_MAX_MS,\n          rmConnectionRetryIntervalMS);\n\n      int maxFailoverAttempts = conf.getInt(\n          YarnConfiguration.CLIENT_FAILOVER_MAX_ATTEMPTS, -1);\n\n      if (maxFailoverAttempts == -1) {\n        if (waitForEver) {\n          maxFailoverAttempts = Integer.MAX_VALUE;\n        } else {\n          maxFailoverAttempts = (int) (rmConnectWaitMS / failoverSleepBaseMs);\n        }\n      }\n\n      return RetryPolicies.failoverOnNetworkException(\n          RetryPolicies.TRY_ONCE_THEN_FAIL, maxFailoverAttempts,\n          failoverSleepBaseMs, failoverSleepMaxMs);\n    }\n\n    if (rmConnectionRetryIntervalMS < 0) {\n      throw new YarnRuntimeException(\"Invalid Configuration. \" +\n          YarnConfiguration.RESOURCEMANAGER_CONNECT_RETRY_INTERVAL_MS +\n          \" should not be negative.\");\n    }\n\n    RetryPolicy retryPolicy = null;\n    if (waitForEver) {\n      retryPolicy = RetryPolicies.retryForeverWithFixedSleep(\n          rmConnectionRetryIntervalMS, TimeUnit.MILLISECONDS);\n    } else {\n      retryPolicy =\n          RetryPolicies.retryUpToMaximumTimeWithFixedSleep(rmConnectWaitMS,\n              rmConnectionRetryIntervalMS, TimeUnit.MILLISECONDS);\n    }\n\n    Map<Class<? extends Exception>, RetryPolicy> exceptionToPolicyMap =\n        new HashMap<Class<? extends Exception>, RetryPolicy>();\n\n    exceptionToPolicyMap.put(EOFException.class, retryPolicy);\n    exceptionToPolicyMap.put(ConnectException.class, retryPolicy);\n    exceptionToPolicyMap.put(NoRouteToHostException.class, retryPolicy);\n    exceptionToPolicyMap.put(UnknownHostException.class, retryPolicy);\n    exceptionToPolicyMap.put(ConnectTimeoutException.class, retryPolicy);\n    exceptionToPolicyMap.put(RetriableException.class, retryPolicy);\n    exceptionToPolicyMap.put(SocketException.class, retryPolicy);\n\n    return RetryPolicies.retryByException(\n        RetryPolicies.TRY_ONCE_THEN_FAIL, exceptionToPolicyMap);\n  }",
                "code_after_change": "  public static RetryPolicy createRetryPolicy(Configuration conf) {\n    long rmConnectWaitMS =\n        conf.getLong(\n            YarnConfiguration.RESOURCEMANAGER_CONNECT_MAX_WAIT_MS,\n            YarnConfiguration.DEFAULT_RESOURCEMANAGER_CONNECT_MAX_WAIT_MS);\n    long rmConnectionRetryIntervalMS =\n        conf.getLong(\n            YarnConfiguration.RESOURCEMANAGER_CONNECT_RETRY_INTERVAL_MS,\n            YarnConfiguration\n                .DEFAULT_RESOURCEMANAGER_CONNECT_RETRY_INTERVAL_MS);\n\n    boolean waitForEver = (rmConnectWaitMS == -1);\n    if (!waitForEver) {\n      if (rmConnectWaitMS < 0) {\n        throw new YarnRuntimeException(\"Invalid Configuration. \"\n            + YarnConfiguration.RESOURCEMANAGER_CONNECT_MAX_WAIT_MS\n            + \" can be -1, but can not be other negative numbers\");\n      }\n\n      // try connect once\n      if (rmConnectWaitMS < rmConnectionRetryIntervalMS) {\n        LOG.warn(YarnConfiguration.RESOURCEMANAGER_CONNECT_MAX_WAIT_MS\n            + \" is smaller than \"\n            + YarnConfiguration.RESOURCEMANAGER_CONNECT_RETRY_INTERVAL_MS\n            + \". Only try connect once.\");\n        rmConnectWaitMS = 0;\n      }\n    }\n\n    // Handle HA case first\n    if (HAUtil.isHAEnabled(conf)) {\n      final long failoverSleepBaseMs = conf.getLong(\n          YarnConfiguration.CLIENT_FAILOVER_SLEEPTIME_BASE_MS,\n          rmConnectionRetryIntervalMS);\n\n      final long failoverSleepMaxMs = conf.getLong(\n          YarnConfiguration.CLIENT_FAILOVER_SLEEPTIME_MAX_MS,\n          rmConnectionRetryIntervalMS);\n\n      int maxFailoverAttempts = conf.getInt(\n          YarnConfiguration.CLIENT_FAILOVER_MAX_ATTEMPTS, -1);\n\n      if (maxFailoverAttempts == -1) {\n        if (waitForEver) {\n          maxFailoverAttempts = Integer.MAX_VALUE;\n        } else {\n          maxFailoverAttempts = (int) (rmConnectWaitMS / failoverSleepBaseMs);\n        }\n      }\n\n      return RetryPolicies.failoverOnNetworkException(\n          RetryPolicies.TRY_ONCE_THEN_FAIL, maxFailoverAttempts,\n          failoverSleepBaseMs, failoverSleepMaxMs);\n    }\n\n    if (rmConnectionRetryIntervalMS < 0) {\n      throw new YarnRuntimeException(\"Invalid Configuration. \" +\n          YarnConfiguration.RESOURCEMANAGER_CONNECT_RETRY_INTERVAL_MS +\n          \" should not be negative.\");\n    }\n\n    RetryPolicy retryPolicy = null;\n    if (waitForEver) {\n      retryPolicy = RetryPolicies.retryForeverWithFixedSleep(\n          rmConnectionRetryIntervalMS, TimeUnit.MILLISECONDS);\n    } else {\n      retryPolicy =\n          RetryPolicies.retryUpToMaximumTimeWithFixedSleep(rmConnectWaitMS,\n              rmConnectionRetryIntervalMS, TimeUnit.MILLISECONDS);\n    }\n\n    Map<Class<? extends Exception>, RetryPolicy> exceptionToPolicyMap =\n        new HashMap<Class<? extends Exception>, RetryPolicy>();\n\n    exceptionToPolicyMap.put(EOFException.class, retryPolicy);\n    exceptionToPolicyMap.put(ConnectException.class, retryPolicy);\n    exceptionToPolicyMap.put(NoRouteToHostException.class, retryPolicy);\n    exceptionToPolicyMap.put(UnknownHostException.class, retryPolicy);\n    exceptionToPolicyMap.put(ConnectTimeoutException.class, retryPolicy);\n    exceptionToPolicyMap.put(RetriableException.class, retryPolicy);\n    exceptionToPolicyMap.put(SocketException.class, retryPolicy);\n    // YARN-4288: local IOException is also possible.\n    exceptionToPolicyMap.put(IOException.class, retryPolicy);\n    // Not retry on remote IO exception.\n    return RetryPolicies.retryOtherThanRemoteException(\n        RetryPolicies.TRY_ONCE_THEN_FAIL, exceptionToPolicyMap);\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Preventive",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the issue in the `rebootNodeStatusUpdaterAndRegisterWithRM` method, which is part of the stack trace but not the ground truth methods. The root cause is partially identified as it mentions the lack of retry mechanism, which is related to the retry policies in the ground truth methods. The fix suggestion is preventive as it suggests implementing a retry mechanism, which aligns with the concept of retry policies but does not match the exact developer's fix. The problem location is partially identified as it points to methods in the stack trace context but not the exact ground truth methods. There is no wrong information in the bug report as all details are relevant to the context of the bug."
        }
    },
    {
        "filename": "YARN-1032.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.util.RackResolver.coreResolve": {
                "code_before_change": "  private static Node coreResolve(String hostName) {\n    List <String> tmpList = new ArrayList<String>(1);\n    tmpList.add(hostName);\n    List <String> rNameList = dnsToSwitchMapping.resolve(tmpList);\n    String rName = rNameList.get(0);\n    LOG.info(\"Resolved \" + hostName + \" to \" + rName);\n    return new NodeBase(hostName, rName);\n  }",
                "code_after_change": "  private static Node coreResolve(String hostName) {\n    List <String> tmpList = new ArrayList<String>(1);\n    tmpList.add(hostName);\n    List <String> rNameList = dnsToSwitchMapping.resolve(tmpList);\n    String rName = null;\n    if (rNameList == null || rNameList.get(0) == null) {\n      rName = NetworkTopology.DEFAULT_RACK;\n      LOG.info(\"Couldn't resolve \" + hostName + \". Falling back to \"\n          + NetworkTopology.DEFAULT_RACK);\n    } else {\n      rName = rNameList.get(0);\n      LOG.info(\"Resolved \" + hostName + \" to \" + rName);\n    }\n    return new NodeBase(hostName, rName);\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Preventive",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report precisely identifies the root cause of the NullPointerException as the uninitialized dnsToSwitchMapping object in the coreResolve method, which matches the ground truth method. The fix suggestion is preventive because it suggests ensuring the RackResolver.init method is called before any hostname resolution, which would prevent the NPE by initializing dnsToSwitchMapping. The problem location identification is precise as it correctly mentions the coreResolve method, which is part of the ground truth. There is no wrong information in the bug report as all details are relevant and accurate."
        }
    },
    {
        "filename": "YARN-5837.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.NodesListManager.setDecomissionedNMs": {
                "code_before_change": "  private void setDecomissionedNMs() {\n    Set<String> excludeList = hostsReader.getExcludedHosts();\n    for (final String host : excludeList) {\n      NodeId nodeId = createUnknownNodeId(host);\n      RMNodeImpl rmNode = new RMNodeImpl(nodeId,\n          rmContext, host, -1, -1, new UnknownNode(host), null, null);\n      rmContext.getInactiveRMNodes().put(nodeId, rmNode);\n      rmNode.handle(new RMNodeEvent(nodeId, RMNodeEventType.DECOMMISSION));\n    }\n  }",
                "code_after_change": "  private void setDecomissionedNMs() {\n    Set<String> excludeList = hostsReader.getExcludedHosts();\n    for (final String host : excludeList) {\n      NodeId nodeId = createUnknownNodeId(host);\n      RMNodeImpl rmNode = new RMNodeImpl(nodeId,\n          rmContext, host, -1, -1, new UnknownNode(host),\n          Resource.newInstance(0, 0), \"unknown\");\n      rmContext.getInactiveRMNodes().put(nodeId, rmNode);\n      rmNode.handle(new RMNodeEvent(nodeId, RMNodeEventType.DECOMMISSION));\n    }\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Preventive",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the root cause as a NullPointerException in the `printNodeStatus` method due to an empty or null `NodeReport`, which is related to the ground truth method in the sense that it is part of the same stack trace context. However, it does not pinpoint the exact root cause in the ground truth method `setDecomissionedNMs`. The fix suggestion to implement null checks in `printNodeStatus` is preventive, as it would prevent the NPE but does not match the actual fix in the ground truth method. The problem location is precise as it correctly identifies the `printNodeStatus` method where the issue manifests. There is no wrong information in the bug report as all details are relevant to the context of the bug."
        }
    },
    {
        "filename": "YARN-6827.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart": {
                "code_before_change": "    protected void serviceStart() throws Exception {\n      RMStateStore rmStore = rmContext.getStateStore();\n      // The state store needs to start irrespective of recoveryEnabled as apps\n      // need events to move to further states.\n      rmStore.start();\n\n      if(recoveryEnabled) {\n        try {\n          LOG.info(\"Recovery started\");\n          rmStore.checkVersion();\n          if (rmContext.isWorkPreservingRecoveryEnabled()) {\n            rmContext.setEpoch(rmStore.getAndIncrementEpoch());\n          }\n          RMState state = rmStore.loadState();\n          recover(state);\n          LOG.info(\"Recovery ended\");\n        } catch (Exception e) {\n          // the Exception from loadState() needs to be handled for\n          // HA and we need to give up master status if we got fenced\n          LOG.error(\"Failed to load/recover state\", e);\n          throw e;\n        }\n      }\n\n      super.serviceStart();\n    }",
                "code_after_change": "    protected void serviceStart() throws Exception {\n      RMStateStore rmStore = rmContext.getStateStore();\n      // The state store needs to start irrespective of recoveryEnabled as apps\n      // need events to move to further states.\n      rmStore.start();\n\n      if(recoveryEnabled) {\n        try {\n          LOG.info(\"Recovery started\");\n          rmStore.checkVersion();\n          if (rmContext.isWorkPreservingRecoveryEnabled()) {\n            rmContext.setEpoch(rmStore.getAndIncrementEpoch());\n          }\n          RMState state = rmStore.loadState();\n          recover(state);\n          LOG.info(\"Recovery ended\");\n        } catch (Exception e) {\n          // the Exception from loadState() needs to be handled for\n          // HA and we need to give up master status if we got fenced\n          LOG.error(\"Failed to load/recover state\", e);\n          throw e;\n        }\n      } else {\n        if (HAUtil.isFederationEnabled(conf)) {\n          long epoch = conf.getLong(YarnConfiguration.RM_EPOCH,\n              YarnConfiguration.DEFAULT_RM_EPOCH);\n          rmContext.setEpoch(epoch);\n          LOG.info(\"Epoch set for Federation: \" + epoch);\n        }\n      }\n\n      super.serviceStart();\n    }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.transitionToStandby": {
                "code_before_change": "  synchronized void transitionToStandby(boolean initialize)\n      throws Exception {\n    if (rmContext.getHAServiceState() ==\n        HAServiceProtocol.HAServiceState.STANDBY) {\n      LOG.info(\"Already in standby state\");\n      return;\n    }\n\n    LOG.info(\"Transitioning to standby state\");\n    HAServiceState state = rmContext.getHAServiceState();\n    rmContext.setHAServiceState(HAServiceProtocol.HAServiceState.STANDBY);\n    if (state == HAServiceProtocol.HAServiceState.ACTIVE) {\n      stopActiveServices();\n      reinitialize(initialize);\n    }\n    LOG.info(\"Transitioned to standby state\");\n  }",
                "code_after_change": "  synchronized void transitionToStandby(boolean initialize)\n      throws Exception {\n    if (rmContext.getHAServiceState() ==\n        HAServiceProtocol.HAServiceState.STANDBY) {\n      LOG.info(\"Already in standby state\");\n      return;\n    }\n\n    LOG.info(\"Transitioning to standby state\");\n    HAServiceState state = rmContext.getHAServiceState();\n    rmContext.setHAServiceState(HAServiceProtocol.HAServiceState.STANDBY);\n    if (state == HAServiceProtocol.HAServiceState.ACTIVE) {\n      stopActiveServices();\n      reinitialize(initialize);\n    }\n    LOG.info(\"Transitioned to standby state\");\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Direct Caller/Callee"
            },
            "fix_suggestion": "Preventive",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Same Class or Module"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the root cause as the `timelineWriter` not being initialized before `putEntities` is called. This is partially correct as it points to a method (`TimelineClientImpl.putEntities`) that directly calls the ground truth method `ResourceManager.serviceStart`, which is responsible for initialization. The fix suggestion is preventive, as it suggests ensuring the `serviceStart` method is called before any publishing actions, which aligns with the need to initialize services properly. The problem location identification is partial because it mentions methods in the same class (`TimelineClientImpl`) as the ground truth method, but not the exact ground truth methods. There is no wrong information in the bug report, as all details are relevant to the context of the bug."
        }
    },
    {
        "filename": "YARN-3832.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService.initializeLogDir": {
                "code_before_change": "  private void initializeLogDir(FileContext lfs, String logDir) {\n    try {\n      lfs.mkdir(new Path(logDir), null, true);\n    } catch (FileAlreadyExistsException fe) {\n      // do nothing\n    } catch (IOException e) {\n      String msg = \"Could not initialize log dir \" + logDir;\n      LOG.warn(msg, e);\n      throw new YarnRuntimeException(msg, e);\n    }\n  }",
                "code_after_change": "  private void initializeLogDir(FileContext lfs, String logDir) {\n    try {\n      lfs.mkdir(new Path(logDir), null, true);\n    } catch (FileAlreadyExistsException fe) {\n      // do nothing\n    } catch (IOException e) {\n      String msg = \"Could not initialize log dir \" + logDir;\n      LOG.warn(msg, e);\n      throw new YarnRuntimeException(msg, e);\n    }\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Preventive",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the root cause as an issue with the rename operation in the Hadoop file system, which is related to the stack trace context but not the exact ground truth method. The fix suggestion is preventive, as it suggests checking and clearing the destination directory before renaming, which would prevent the issue. The problem location is partially identified, as it points to the FileContext.rename method, which is in the shared stack trace context but not the exact ground truth method. There is no wrong information in the bug report, as all details are consistent with the context of the bug."
        }
    },
    {
        "filename": "YARN-2409.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.resetDispatcher": {
                "code_before_change": "  private void resetDispatcher() {\n    Dispatcher dispatcher = setupDispatcher();\n    ((Service)dispatcher).init(this.conf);\n    ((Service)dispatcher).start();\n    removeService((Service)rmDispatcher);\n    rmDispatcher = dispatcher;\n    addIfService(rmDispatcher);\n    rmContext.setDispatcher(rmDispatcher);\n  }",
                "code_after_change": "  private void resetDispatcher() {\n    Dispatcher dispatcher = setupDispatcher();\n    ((Service)dispatcher).init(this.conf);\n    ((Service)dispatcher).start();\n    removeService((Service)rmDispatcher);\n    // Need to stop previous rmDispatcher before assigning new dispatcher\n    // otherwise causes \"AsyncDispatcher event handler\" thread leak\n    ((Service) rmDispatcher).stop();\n    rmDispatcher = dispatcher;\n    addIfService(rmDispatcher);\n    rmContext.setDispatcher(rmDispatcher);\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Preventive",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the root cause as the application attempting to process invalid events in the 'LAUNCHED' state, which is related to the stack trace context but not the exact root cause in the ground truth method. The fix suggestion is preventive as it suggests adding state checks to prevent the error, which would mitigate the issue but does not match the developer's fix. The problem location identification is partial because it mentions methods in the stack trace context but not the exact ground truth method. There is no wrong information in the bug report as all details are relevant to the context of the bug."
        }
    },
    {
        "filename": "YARN-8116.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl.removeDockerContainer": {
                "code_before_change": "  private static void removeDockerContainer(ContainerImpl container) {\n    DeletionService deletionService = container.context.getDeletionService();\n    DockerContainerDeletionTask deletionTask =\n        new DockerContainerDeletionTask(deletionService, container.user,\n            container.getContainerId().toString());\n    deletionService.delete(deletionTask);\n  }",
                "code_after_change": "  private static void removeDockerContainer(ContainerImpl container) {\n    DeletionService deletionService = container.context.getDeletionService();\n    DockerContainerDeletionTask deletionTask =\n        new DockerContainerDeletionTask(deletionService, container.user,\n            container.getContainerId().toString());\n    deletionService.delete(deletionTask);\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Preventive",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the root cause as being in the `loadContainerState` method, which is not the ground truth method but is in the same stack trace context as the ground truth method `removeDockerContainer`. The fix suggestion is preventive as it suggests adding a check to prevent the error, which is a valid approach to mitigate the issue. The problem location is identified as `loadContainerState`, which is not the ground truth method but is in the shared stack trace context. There is no wrong information in the bug report as all details are relevant to the context of the bug."
        }
    },
    {
        "filename": "YARN-8403.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService.run": {
                "code_before_change": "    public void run() {\n      try {\n        // TODO shutdown, better error handling esp. DU\n        while (!Thread.currentThread().isInterrupted()) {\n          try {\n            Future<Path> completed = queue.take();\n            LocalizerResourceRequestEvent assoc = pending.remove(completed);\n            try {\n              if (null == assoc) {\n                LOG.error(\"Localized unknown resource to \" + completed);\n                // TODO delete\n                return;\n              }\n              Path local = completed.get();\n              LocalResourceRequest key = assoc.getResource().getRequest();\n              publicRsrc.handle(new ResourceLocalizedEvent(key, local, FileUtil\n                .getDU(new File(local.toUri()))));\n              assoc.getResource().unlock();\n            } catch (ExecutionException e) {\n              LOG.info(\"Failed to download resource \" + assoc.getResource(),\n                  e.getCause());\n              LocalResourceRequest req = assoc.getResource().getRequest();\n              publicRsrc.handle(new ResourceFailedLocalizationEvent(req,\n                  e.getMessage()));\n              assoc.getResource().unlock();\n            } catch (CancellationException e) {\n              // ignore; shutting down\n            }\n          } catch (InterruptedException e) {\n            return;\n          }\n        }\n      } catch(Throwable t) {\n        LOG.error(\"Error: Shutting down\", t);\n      } finally {\n        LOG.info(\"Public cache exiting\");\n        threadPool.shutdownNow();\n      }\n    }",
                "code_after_change": "    public void run() {\n      try {\n        // TODO shutdown, better error handling esp. DU\n        while (!Thread.currentThread().isInterrupted()) {\n          try {\n            Future<Path> completed = queue.take();\n            LocalizerResourceRequestEvent assoc = pending.remove(completed);\n            try {\n              if (null == assoc) {\n                LOG.error(\"Localized unknown resource to \" + completed);\n                // TODO delete\n                return;\n              }\n              Path local = completed.get();\n              LocalResourceRequest key = assoc.getResource().getRequest();\n              publicRsrc.handle(new ResourceLocalizedEvent(key, local, FileUtil\n                .getDU(new File(local.toUri()))));\n              assoc.getResource().unlock();\n            } catch (ExecutionException e) {\n              String user = assoc.getContext().getUser();\n              ApplicationId applicationId = assoc.getContext().getContainerId().getApplicationAttemptId().getApplicationId();\n              LocalResourcesTracker tracker =\n                getLocalResourcesTracker(LocalResourceVisibility.APPLICATION, user, applicationId);\n              final String diagnostics = \"Failed to download resource \" +\n                  assoc.getResource() + \" \" + e.getCause();\n              tracker.handle(new ResourceFailedLocalizationEvent(\n                  assoc.getResource().getRequest(), diagnostics));\n              publicRsrc.handle(new ResourceFailedLocalizationEvent(\n                  assoc.getResource().getRequest(), diagnostics));\n              LOG.error(diagnostics);\n              assoc.getResource().unlock();\n            } catch (CancellationException e) {\n              // ignore; shutting down\n            }\n          } catch (InterruptedException e) {\n            return;\n          }\n        }\n      } catch(Throwable t) {\n        LOG.error(\"Error: Shutting down\", t);\n      } finally {\n        LOG.info(\"Public cache exiting\");\n        threadPool.shutdownNow();\n      }\n    }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Preventive",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the root cause as a `FileNotFoundException` due to permission issues, which is mentioned in the stack trace context of the ground truth method `ResourceLocalizationService.run`. However, it does not precisely identify the root cause within the ground truth method itself, hence it is classified as 'Partial' with 'Shared Stack Trace Context'. The fix suggestion involves changing permissions, which is preventive as it addresses the permission issue but does not match the developer's fix, which involved better error handling in the `run` method. The problem location mentions methods like `FSDownload.downloadAndUnpack` and `FileSystem.setPermission`, which are related to the stack trace context but not the ground truth method, so it is 'Partial' with 'Shared Stack Trace Context'. There is no wrong information in the bug report as all details are relevant to the context of the bug."
        }
    },
    {
        "filename": "YARN-1458.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.policies.ComputeFairShares.computeSteadyShares": {
                "code_before_change": [],
                "code_after_change": "  public static void computeSteadyShares(\n      Collection<? extends FSQueue> queues, Resource totalResources,\n      ResourceType type) {\n    computeSharesInternal(queues, totalResources, type, true);\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.policies.ComputeFairShares.computeSharesInternal": {
                "code_before_change": [],
                "code_after_change": "  private static void computeSharesInternal(\n      Collection<? extends Schedulable> allSchedulables,\n      Resource totalResources, ResourceType type, boolean isSteadyShare) {\n\n    Collection<Schedulable> schedulables = new ArrayList<Schedulable>();\n    int takenResources = handleFixedFairShares(\n        allSchedulables, schedulables, isSteadyShare, type);\n\n    if (schedulables.isEmpty()) {\n      return;\n    }\n    // Find an upper bound on R that we can use in our binary search. We start\n    // at R = 1 and double it until we have either used all the resources or we\n    // have met all Schedulables' max shares.\n    int totalMaxShare = 0;\n    for (Schedulable sched : schedulables) {\n      int maxShare = getResourceValue(sched.getMaxShare(), type);\n      if (maxShare == Integer.MAX_VALUE) {\n        totalMaxShare = Integer.MAX_VALUE;\n        break;\n      } else {\n        totalMaxShare += maxShare;\n      }\n    }\n\n    int totalResource = Math.max((getResourceValue(totalResources, type) -\n        takenResources), 0);\n    totalResource = Math.min(totalMaxShare, totalResource);\n\n    double rMax = 1.0;\n    while (resourceUsedWithWeightToResourceRatio(rMax, schedulables, type)\n        < totalResource) {\n      rMax *= 2.0;\n    }\n    // Perform the binary search for up to COMPUTE_FAIR_SHARES_ITERATIONS steps\n    double left = 0;\n    double right = rMax;\n    for (int i = 0; i < COMPUTE_FAIR_SHARES_ITERATIONS; i++) {\n      double mid = (left + right) / 2.0;\n      int plannedResourceUsed = resourceUsedWithWeightToResourceRatio(\n          mid, schedulables, type);\n      if (plannedResourceUsed == totalResource) {\n        right = mid;\n        break;\n      } else if (plannedResourceUsed < totalResource) {\n        left = mid;\n      } else {\n        right = mid;\n      }\n    }\n    // Set the fair shares based on the value of R we've converged to\n    for (Schedulable sched : schedulables) {\n      if (isSteadyShare) {\n        setResourceValue(computeShare(sched, right, type),\n            ((FSQueue) sched).getSteadyFairShare(), type);\n      } else {\n        setResourceValue(\n            computeShare(sched, right, type), sched.getFairShare(), type);\n      }\n    }\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.policies.ComputeFairShares.computeShare": {
                "code_before_change": "  private static int computeShare(Schedulable sched, double w2rRatio,\n      ResourceType type) {\n    double share = sched.getWeights().getWeight(type) * w2rRatio;\n    share = Math.max(share, getResourceValue(sched.getMinShare(), type));\n    share = Math.min(share, getResourceValue(sched.getMaxShare(), type));\n    return (int) share;\n  }",
                "code_after_change": "  private static int computeShare(Schedulable sched, double w2rRatio,\n      ResourceType type) {\n    double share = sched.getWeights().getWeight(type) * w2rRatio;\n    share = Math.max(share, getResourceValue(sched.getMinShare(), type));\n    share = Math.min(share, getResourceValue(sched.getMaxShare(), type));\n    return (int) share;\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Preventive",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies contention in the FairScheduler's methods, particularly during application removal and weight calculations, which are mentioned in the stack trace. However, it does not precisely identify the root cause in the ground truth methods, which are related to ComputeFairShares. The fix suggestion is preventive as it suggests implementing a timeout mechanism for acquiring locks, which would mitigate the livelock issue but does not match the developer's fix. The problem location is partially identified as it mentions methods in the same stack trace context as the ground truth methods. There is no wrong information in the bug report as all details are relevant to the context of the bug."
        }
    },
    {
        "filename": "YARN-8209.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DockerLinuxContainerRuntime.reapContainer": {
                "code_before_change": "  public void reapContainer(ContainerRuntimeContext ctx)\n      throws ContainerExecutionException {\n    // Clean up the Docker container\n    handleContainerRemove(ctx.getContainer().getContainerId().toString(),\n        ctx.getContainer().getLaunchContext().getEnvironment());\n\n    // Cleanup volumes when needed.\n    if (nmContext != null\n        && nmContext.getResourcePluginManager().getNameToPlugins() != null) {\n      for (ResourcePlugin plugin : nmContext.getResourcePluginManager()\n          .getNameToPlugins().values()) {\n        DockerCommandPlugin dockerCommandPlugin =\n            plugin.getDockerCommandPluginInstance();\n        if (dockerCommandPlugin != null) {\n          DockerVolumeCommand dockerVolumeCommand =\n              dockerCommandPlugin.getCleanupDockerVolumesCommand(\n                  ctx.getContainer());\n          if (dockerVolumeCommand != null) {\n            runDockerVolumeCommand(dockerVolumeCommand, ctx.getContainer());\n          }\n        }\n      }\n    }\n  }",
                "code_after_change": "  public void reapContainer(ContainerRuntimeContext ctx)\n      throws ContainerExecutionException {\n    // Clean up the Docker container\n    handleContainerRemove(ctx.getContainer().getContainerId().toString(),\n        ctx.getContainer().getLaunchContext().getEnvironment());\n\n    // Cleanup volumes when needed.\n    if (nmContext != null\n        && nmContext.getResourcePluginManager().getNameToPlugins() != null) {\n      for (ResourcePlugin plugin : nmContext.getResourcePluginManager()\n          .getNameToPlugins().values()) {\n        DockerCommandPlugin dockerCommandPlugin =\n            plugin.getDockerCommandPluginInstance();\n        if (dockerCommandPlugin != null) {\n          DockerVolumeCommand dockerVolumeCommand =\n              dockerCommandPlugin.getCleanupDockerVolumesCommand(\n                  ctx.getContainer());\n          if (dockerVolumeCommand != null) {\n            runDockerVolumeCommand(dockerVolumeCommand, ctx.getContainer());\n          }\n        }\n      }\n    }\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerCommand.setClientConfigDir": {
                "code_before_change": "  public void setClientConfigDir(String clientConfigDir) {\n    if (clientConfigDir != null) {\n      addCommandArguments(\"docker-config\", clientConfigDir);\n    }\n  }",
                "code_after_change": "  public void setClientConfigDir(String clientConfigDir) {\n    if (clientConfigDir != null) {\n      addCommandArguments(\"docker-config\", clientConfigDir);\n    }\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DockerLinuxContainerRuntime.getGroupIdInfo": {
                "code_before_change": "  private String[] getGroupIdInfo(String userName)\n      throws ContainerExecutionException {\n    String[] id = null;\n    Shell.ShellCommandExecutor shexec = new Shell.ShellCommandExecutor(\n        new String[]{\"id\", \"-G\", userName});\n    try {\n      shexec.execute();\n      id = shexec.getOutput().replace(\"\\n\", \"\").split(\" \");\n    } catch (Exception e) {\n      throw new ContainerExecutionException(e);\n    }\n    return id;\n  }",
                "code_after_change": "  private String[] getGroupIdInfo(String userName)\n      throws ContainerExecutionException {\n    String[] id = null;\n    Shell.ShellCommandExecutor shexec = new Shell.ShellCommandExecutor(\n        new String[]{\"id\", \"-G\", userName});\n    try {\n      shexec.execute();\n      id = shexec.getOutput().replace(\"\\n\", \"\").split(\" \");\n    } catch (Exception e) {\n      throw new ContainerExecutionException(e);\n    }\n    return id;\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerClient.writeCommandToTempFile": {
                "code_before_change": "  public String writeCommandToTempFile(DockerCommand cmd, String filePrefix)\n      throws ContainerExecutionException {\n    File dockerCommandFile = null;\n    try {\n      dockerCommandFile = File.createTempFile(TMP_FILE_PREFIX + filePrefix,\n          TMP_FILE_SUFFIX, new\n          File(tmpDirPath));\n\n      Writer writer = new OutputStreamWriter(\n          new FileOutputStream(dockerCommandFile), \"UTF-8\");\n      PrintWriter printWriter = new PrintWriter(writer);\n      printWriter.println(\"[docker-command-execution]\");\n      for (Map.Entry<String, List<String>> entry :\n          cmd.getDockerCommandWithArguments().entrySet()) {\n        if (entry.getKey().contains(\"=\")) {\n          throw new ContainerExecutionException(\n              \"'=' found in entry for docker command file, key = \" + entry\n                  .getKey() + \"; value = \" + entry.getValue());\n        }\n        if (entry.getValue().contains(\"\\n\")) {\n          throw new ContainerExecutionException(\n              \"'\\\\n' found in entry for docker command file, key = \" + entry\n                  .getKey() + \"; value = \" + entry.getValue());\n        }\n        printWriter.println(\"  \" + entry.getKey() + \"=\" + StringUtils\n            .join(\",\", entry.getValue()));\n      }\n      printWriter.close();\n\n      return dockerCommandFile.getAbsolutePath();\n    } catch (IOException e) {\n      LOG.warn(\"Unable to write docker command to temporary file!\");\n      throw new ContainerExecutionException(e);\n    }\n  }",
                "code_after_change": "  public String writeCommandToTempFile(DockerCommand cmd, String filePrefix)\n      throws ContainerExecutionException {\n    File dockerCommandFile = null;\n    try {\n      dockerCommandFile = File.createTempFile(TMP_FILE_PREFIX + filePrefix,\n          TMP_FILE_SUFFIX, new\n          File(tmpDirPath));\n\n      Writer writer = new OutputStreamWriter(\n          new FileOutputStream(dockerCommandFile), \"UTF-8\");\n      PrintWriter printWriter = new PrintWriter(writer);\n      printWriter.println(\"[docker-command-execution]\");\n      for (Map.Entry<String, List<String>> entry :\n          cmd.getDockerCommandWithArguments().entrySet()) {\n        if (entry.getKey().contains(\"=\")) {\n          throw new ContainerExecutionException(\n              \"'=' found in entry for docker command file, key = \" + entry\n                  .getKey() + \"; value = \" + entry.getValue());\n        }\n        if (entry.getValue().contains(\"\\n\")) {\n          throw new ContainerExecutionException(\n              \"'\\\\n' found in entry for docker command file, key = \" + entry\n                  .getKey() + \"; value = \" + entry.getValue());\n        }\n        printWriter.println(\"  \" + entry.getKey() + \"=\" + StringUtils\n            .join(\",\", entry.getValue()));\n      }\n      printWriter.close();\n\n      return dockerCommandFile.getAbsolutePath();\n    } catch (IOException e) {\n      LOG.warn(\"Unable to write docker command to temporary file!\");\n      throw new ContainerExecutionException(e);\n    }\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerRmCommand.DockerRmCommand": {
                "code_before_change": [],
                "code_after_change": []
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerInspectCommand.DockerInspectCommand": {
                "code_before_change": [],
                "code_after_change": []
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerInspectCommand.getIpAndHost": {
                "code_before_change": "  public DockerInspectCommand getIpAndHost() {\n    // Be sure to not use space in the argument, otherwise the\n    // extract_values_delim method in container-executor binary\n    // cannot parse the arguments correctly.\n    super.addCommandArguments(\"format\", \"{{range(.NetworkSettings.Networks)}}\"\n        + \"{{.IPAddress}},{{end}}{{.Config.Hostname}}\");\n    return this;\n  }",
                "code_after_change": "  public DockerInspectCommand getIpAndHost() {\n    // Be sure to not use space in the argument, otherwise the\n    // extract_values_delim method in container-executor binary\n    // cannot parse the arguments correctly.\n    super.addCommandArguments(\"format\", \"{{range(.NetworkSettings.Networks)}}\"\n        + \"{{.IPAddress}},{{end}}{{.Config.Hostname}}\");\n    this.commandArguments = \"--format={{range(.NetworkSettings.Networks)}}\"\n        + \"{{.IPAddress}},{{end}}{{.Config.Hostname}}\";\n    return this;\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DockerLinuxContainerRuntime.launchContainer": {
                "code_before_change": "  public void launchContainer(ContainerRuntimeContext ctx)\n      throws ContainerExecutionException {\n    Container container = ctx.getContainer();\n    Map<String, String> environment = container.getLaunchContext()\n        .getEnvironment();\n    String imageName = environment.get(ENV_DOCKER_CONTAINER_IMAGE);\n    String network = environment.get(ENV_DOCKER_CONTAINER_NETWORK);\n    String hostname = environment.get(ENV_DOCKER_CONTAINER_HOSTNAME);\n\n    if(network == null || network.isEmpty()) {\n      network = defaultNetwork;\n    }\n\n    validateContainerNetworkType(network);\n\n    validateHostname(hostname);\n\n    validateImageName(imageName);\n\n    String containerIdStr = container.getContainerId().toString();\n    String runAsUser = ctx.getExecutionAttribute(RUN_AS_USER);\n    String dockerRunAsUser = runAsUser;\n    Path containerWorkDir = ctx.getExecutionAttribute(CONTAINER_WORK_DIR);\n    String[] groups = null;\n\n    if (enableUserReMapping) {\n      String uid = getUserIdInfo(runAsUser);\n      groups = getGroupIdInfo(runAsUser);\n      String gid = groups[0];\n      if(Integer.parseInt(uid) < userRemappingUidThreshold) {\n        String message = \"uid: \" + uid + \" below threshold: \"\n            + userRemappingUidThreshold;\n        throw new ContainerExecutionException(message);\n      }\n      for(int i = 0; i < groups.length; i++) {\n        String group = groups[i];\n        if (Integer.parseInt(group) < userRemappingGidThreshold) {\n          String message = \"gid: \" + group\n              + \" below threshold: \" + userRemappingGidThreshold;\n          throw new ContainerExecutionException(message);\n        }\n      }\n      if (!allowPrivilegedContainerExecution(container)) {\n        dockerRunAsUser = uid + \":\" + gid;\n      } else {\n        dockerRunAsUser = ctx.getExecutionAttribute(USER);\n      }\n    }\n\n    //List<String> -> stored as List -> fetched/converted to List<String>\n    //we can't do better here thanks to type-erasure\n    @SuppressWarnings(\"unchecked\")\n    List<String> filecacheDirs = ctx.getExecutionAttribute(FILECACHE_DIRS);\n    @SuppressWarnings(\"unchecked\")\n    List<String> containerLogDirs = ctx.getExecutionAttribute(\n        CONTAINER_LOG_DIRS);\n    @SuppressWarnings(\"unchecked\")\n    List<String> userFilecacheDirs =\n        ctx.getExecutionAttribute(USER_FILECACHE_DIRS);\n    @SuppressWarnings(\"unchecked\")\n    List<String> applicationLocalDirs =\n        ctx.getExecutionAttribute(APPLICATION_LOCAL_DIRS);\n    @SuppressWarnings(\"unchecked\")\n    Map<Path, List<String>> localizedResources = ctx.getExecutionAttribute(\n        LOCALIZED_RESOURCES);\n\n    @SuppressWarnings(\"unchecked\")\n    DockerRunCommand runCommand = new DockerRunCommand(containerIdStr,\n        dockerRunAsUser, imageName)\n        .detachOnRun()\n        .setContainerWorkDir(containerWorkDir.toString())\n        .setNetworkType(network);\n    // Only add hostname if network is not host or if Registry DNS is enabled.\n    if (!network.equalsIgnoreCase(\"host\") ||\n        conf.getBoolean(RegistryConstants.KEY_DNS_ENABLED,\n            RegistryConstants.DEFAULT_DNS_ENABLED)) {\n      setHostname(runCommand, containerIdStr, hostname);\n    }\n    runCommand.setCapabilities(capabilities);\n\n    runCommand.addAllReadWriteMountLocations(containerLogDirs);\n    runCommand.addAllReadWriteMountLocations(applicationLocalDirs);\n    runCommand.addAllReadOnlyMountLocations(filecacheDirs);\n    runCommand.addAllReadOnlyMountLocations(userFilecacheDirs);\n\n    if (environment.containsKey(ENV_DOCKER_CONTAINER_LOCAL_RESOURCE_MOUNTS)) {\n      String mounts = environment.get(\n          ENV_DOCKER_CONTAINER_LOCAL_RESOURCE_MOUNTS);\n      if (!mounts.isEmpty()) {\n        for (String mount : StringUtils.split(mounts)) {\n          String[] dir = StringUtils.split(mount, ':');\n          if (dir.length != 2) {\n            throw new ContainerExecutionException(\"Invalid mount : \" +\n                mount);\n          }\n          String src = validateMount(dir[0], localizedResources);\n          String dst = dir[1];\n          runCommand.addReadOnlyMountLocation(src, dst, true);\n        }\n      }\n    }\n\n    if (environment.containsKey(ENV_DOCKER_CONTAINER_MOUNTS)) {\n      Matcher parsedMounts = USER_MOUNT_PATTERN.matcher(\n          environment.get(ENV_DOCKER_CONTAINER_MOUNTS));\n      if (!parsedMounts.find()) {\n        throw new ContainerExecutionException(\n            \"Unable to parse user supplied mount list: \"\n                + environment.get(ENV_DOCKER_CONTAINER_MOUNTS));\n      }\n      parsedMounts.reset();\n      while (parsedMounts.find()) {\n        String src = parsedMounts.group(1);\n        String dst = parsedMounts.group(2);\n        String mode = parsedMounts.group(3);\n        if (!mode.equals(\"ro\") && !mode.equals(\"rw\")) {\n          throw new ContainerExecutionException(\n              \"Invalid mount mode requested for mount: \"\n                  + parsedMounts.group());\n        }\n        if (mode.equals(\"ro\")) {\n          runCommand.addReadOnlyMountLocation(src, dst);\n        } else {\n          runCommand.addReadWriteMountLocation(src, dst);\n        }\n      }\n    }\n\n    if (allowHostPidNamespace(container)) {\n      runCommand.setPidNamespace(\"host\");\n    }\n\n    if (allowPrivilegedContainerExecution(container)) {\n      runCommand.setPrivileged();\n    }\n\n    addDockerClientConfigToRunCommand(ctx, runCommand);\n\n    String resourcesOpts = ctx.getExecutionAttribute(RESOURCES_OPTIONS);\n\n    addCGroupParentIfRequired(resourcesOpts, containerIdStr, runCommand);\n\n    String disableOverride = environment.get(\n        ENV_DOCKER_CONTAINER_RUN_OVERRIDE_DISABLE);\n\n    if (disableOverride != null && disableOverride.equals(\"true\")) {\n      LOG.info(\"command override disabled\");\n    } else {\n      List<String> overrideCommands = new ArrayList<>();\n      Path launchDst =\n          new Path(containerWorkDir, ContainerLaunch.CONTAINER_SCRIPT);\n\n      overrideCommands.add(\"bash\");\n      overrideCommands.add(launchDst.toUri().getPath());\n      runCommand.setOverrideCommandWithArgs(overrideCommands);\n    }\n\n    if(enableUserReMapping) {\n      if (!allowPrivilegedContainerExecution(container)) {\n        runCommand.groupAdd(groups);\n      }\n    }\n\n    // use plugins to update docker run command.\n    if (nmContext != null\n        && nmContext.getResourcePluginManager().getNameToPlugins() != null) {\n      for (ResourcePlugin plugin : nmContext.getResourcePluginManager()\n          .getNameToPlugins().values()) {\n        DockerCommandPlugin dockerCommandPlugin =\n            plugin.getDockerCommandPluginInstance();\n        if (dockerCommandPlugin != null) {\n          dockerCommandPlugin.updateDockerRunCommand(runCommand, container);\n        }\n      }\n    }\n\n    String commandFile = dockerClient.writeCommandToTempFile(runCommand,\n        container, nmContext);\n    PrivilegedOperation launchOp = buildLaunchOp(ctx,\n        commandFile, runCommand);\n\n    try {\n      privilegedOperationExecutor.executePrivilegedOperation(null,\n          launchOp, null, null, false, false);\n    } catch (PrivilegedOperationException e) {\n      LOG.warn(\"Launch container failed. Exception: \", e);\n      LOG.info(\"Docker command used: \" + runCommand);\n\n      throw new ContainerExecutionException(\"Launch container failed\", e\n          .getExitCode(), e.getOutput(), e.getErrorOutput());\n    }\n  }",
                "code_after_change": "  public void launchContainer(ContainerRuntimeContext ctx)\n      throws ContainerExecutionException {\n    Container container = ctx.getContainer();\n    ContainerId containerId = container.getContainerId();\n    Map<String, String> environment = container.getLaunchContext()\n        .getEnvironment();\n    String imageName = environment.get(ENV_DOCKER_CONTAINER_IMAGE);\n    String network = environment.get(ENV_DOCKER_CONTAINER_NETWORK);\n    String hostname = environment.get(ENV_DOCKER_CONTAINER_HOSTNAME);\n\n    if(network == null || network.isEmpty()) {\n      network = defaultNetwork;\n    }\n\n    validateContainerNetworkType(network);\n\n    validateHostname(hostname);\n\n    validateImageName(imageName);\n\n    String containerIdStr = containerId.toString();\n    String runAsUser = ctx.getExecutionAttribute(RUN_AS_USER);\n    String dockerRunAsUser = runAsUser;\n    Path containerWorkDir = ctx.getExecutionAttribute(CONTAINER_WORK_DIR);\n    String[] groups = null;\n\n    if (enableUserReMapping) {\n      String uid = getUserIdInfo(runAsUser);\n      groups = getGroupIdInfo(runAsUser);\n      String gid = groups[0];\n      if(Integer.parseInt(uid) < userRemappingUidThreshold) {\n        String message = \"uid: \" + uid + \" below threshold: \"\n            + userRemappingUidThreshold;\n        throw new ContainerExecutionException(message);\n      }\n      for(int i = 0; i < groups.length; i++) {\n        String group = groups[i];\n        if (Integer.parseInt(group) < userRemappingGidThreshold) {\n          String message = \"gid: \" + group\n              + \" below threshold: \" + userRemappingGidThreshold;\n          throw new ContainerExecutionException(message);\n        }\n      }\n      if (!allowPrivilegedContainerExecution(container)) {\n        dockerRunAsUser = uid + \":\" + gid;\n      } else {\n        dockerRunAsUser = ctx.getExecutionAttribute(USER);\n      }\n    }\n\n    //List<String> -> stored as List -> fetched/converted to List<String>\n    //we can't do better here thanks to type-erasure\n    @SuppressWarnings(\"unchecked\")\n    List<String> filecacheDirs = ctx.getExecutionAttribute(FILECACHE_DIRS);\n    @SuppressWarnings(\"unchecked\")\n    List<String> containerLogDirs = ctx.getExecutionAttribute(\n        CONTAINER_LOG_DIRS);\n    @SuppressWarnings(\"unchecked\")\n    List<String> userFilecacheDirs =\n        ctx.getExecutionAttribute(USER_FILECACHE_DIRS);\n    @SuppressWarnings(\"unchecked\")\n    List<String> applicationLocalDirs =\n        ctx.getExecutionAttribute(APPLICATION_LOCAL_DIRS);\n    @SuppressWarnings(\"unchecked\")\n    Map<Path, List<String>> localizedResources = ctx.getExecutionAttribute(\n        LOCALIZED_RESOURCES);\n\n    @SuppressWarnings(\"unchecked\")\n    DockerRunCommand runCommand = new DockerRunCommand(containerIdStr,\n        dockerRunAsUser, imageName)\n        .detachOnRun()\n        .setContainerWorkDir(containerWorkDir.toString())\n        .setNetworkType(network);\n    // Only add hostname if network is not host or if Registry DNS is enabled.\n    if (!network.equalsIgnoreCase(\"host\") ||\n        conf.getBoolean(RegistryConstants.KEY_DNS_ENABLED,\n            RegistryConstants.DEFAULT_DNS_ENABLED)) {\n      setHostname(runCommand, containerIdStr, hostname);\n    }\n    runCommand.setCapabilities(capabilities);\n\n    runCommand.addAllReadWriteMountLocations(containerLogDirs);\n    runCommand.addAllReadWriteMountLocations(applicationLocalDirs);\n    runCommand.addAllReadOnlyMountLocations(filecacheDirs);\n    runCommand.addAllReadOnlyMountLocations(userFilecacheDirs);\n\n    if (environment.containsKey(ENV_DOCKER_CONTAINER_LOCAL_RESOURCE_MOUNTS)) {\n      String mounts = environment.get(\n          ENV_DOCKER_CONTAINER_LOCAL_RESOURCE_MOUNTS);\n      if (!mounts.isEmpty()) {\n        for (String mount : StringUtils.split(mounts)) {\n          String[] dir = StringUtils.split(mount, ':');\n          if (dir.length != 2) {\n            throw new ContainerExecutionException(\"Invalid mount : \" +\n                mount);\n          }\n          String src = validateMount(dir[0], localizedResources);\n          String dst = dir[1];\n          runCommand.addReadOnlyMountLocation(src, dst, true);\n        }\n      }\n    }\n\n    if (environment.containsKey(ENV_DOCKER_CONTAINER_MOUNTS)) {\n      Matcher parsedMounts = USER_MOUNT_PATTERN.matcher(\n          environment.get(ENV_DOCKER_CONTAINER_MOUNTS));\n      if (!parsedMounts.find()) {\n        throw new ContainerExecutionException(\n            \"Unable to parse user supplied mount list: \"\n                + environment.get(ENV_DOCKER_CONTAINER_MOUNTS));\n      }\n      parsedMounts.reset();\n      while (parsedMounts.find()) {\n        String src = parsedMounts.group(1);\n        String dst = parsedMounts.group(2);\n        String mode = parsedMounts.group(3);\n        if (!mode.equals(\"ro\") && !mode.equals(\"rw\")) {\n          throw new ContainerExecutionException(\n              \"Invalid mount mode requested for mount: \"\n                  + parsedMounts.group());\n        }\n        if (mode.equals(\"ro\")) {\n          runCommand.addReadOnlyMountLocation(src, dst);\n        } else {\n          runCommand.addReadWriteMountLocation(src, dst);\n        }\n      }\n    }\n\n    if (allowHostPidNamespace(container)) {\n      runCommand.setPidNamespace(\"host\");\n    }\n\n    if (allowPrivilegedContainerExecution(container)) {\n      runCommand.setPrivileged();\n    }\n\n    addDockerClientConfigToRunCommand(ctx, runCommand);\n\n    String resourcesOpts = ctx.getExecutionAttribute(RESOURCES_OPTIONS);\n\n    addCGroupParentIfRequired(resourcesOpts, containerIdStr, runCommand);\n\n    String disableOverride = environment.get(\n        ENV_DOCKER_CONTAINER_RUN_OVERRIDE_DISABLE);\n\n    if (disableOverride != null && disableOverride.equals(\"true\")) {\n      LOG.info(\"command override disabled\");\n    } else {\n      List<String> overrideCommands = new ArrayList<>();\n      Path launchDst =\n          new Path(containerWorkDir, ContainerLaunch.CONTAINER_SCRIPT);\n\n      overrideCommands.add(\"bash\");\n      overrideCommands.add(launchDst.toUri().getPath());\n      runCommand.setOverrideCommandWithArgs(overrideCommands);\n    }\n\n    if(enableUserReMapping) {\n      if (!allowPrivilegedContainerExecution(container)) {\n        runCommand.groupAdd(groups);\n      }\n    }\n\n    // use plugins to update docker run command.\n    if (nmContext != null\n        && nmContext.getResourcePluginManager().getNameToPlugins() != null) {\n      for (ResourcePlugin plugin : nmContext.getResourcePluginManager()\n          .getNameToPlugins().values()) {\n        DockerCommandPlugin dockerCommandPlugin =\n            plugin.getDockerCommandPluginInstance();\n        if (dockerCommandPlugin != null) {\n          dockerCommandPlugin.updateDockerRunCommand(runCommand, container);\n        }\n      }\n    }\n\n    String commandFile = dockerClient.writeCommandToTempFile(runCommand,\n        containerId, nmContext);\n    PrivilegedOperation launchOp = buildLaunchOp(ctx,\n        commandFile, runCommand);\n\n    try {\n      privilegedOperationExecutor.executePrivilegedOperation(null,\n          launchOp, null, null, false, false);\n    } catch (PrivilegedOperationException e) {\n      LOG.warn(\"Launch container failed. Exception: \", e);\n      LOG.info(\"Docker command used: \" + runCommand);\n\n      throw new ContainerExecutionException(\"Launch container failed\", e\n          .getExitCode(), e.getOutput(), e.getErrorOutput());\n    }\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerCommandExecutor.executeDockerCommand": {
                "code_before_change": "  public static String executeDockerCommand(DockerCommand dockerCommand,\n      String containerId, Map<String, String> env, Configuration conf,\n      PrivilegedOperationExecutor privilegedOperationExecutor,\n      boolean disableFailureLogging, Context nmContext)\n      throws ContainerExecutionException {\n    DockerClient dockerClient = new DockerClient(conf);\n    String commandFile =\n        dockerClient.writeCommandToTempFile(dockerCommand,\n        nmContext.getContainers().get(ContainerId.fromString(containerId)),\n        nmContext);\n    PrivilegedOperation dockerOp = new PrivilegedOperation(\n        PrivilegedOperation.OperationType.RUN_DOCKER_CMD);\n    dockerOp.appendArgs(commandFile);\n    if (disableFailureLogging) {\n      dockerOp.disableFailureLogging();\n    }\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Running docker command: \" + dockerCommand);\n    }\n    try {\n      String result = privilegedOperationExecutor\n          .executePrivilegedOperation(null, dockerOp, null,\n              env, true, false);\n      if (result != null && !result.isEmpty()) {\n        result = result.trim();\n      }\n      return result;\n    } catch (PrivilegedOperationException e) {\n      throw new ContainerExecutionException(\"Docker operation failed\",\n          e.getExitCode(), e.getOutput(), e.getErrorOutput());\n    }\n  }",
                "code_after_change": "  public static String executeDockerCommand(DockerCommand dockerCommand,\n      String containerId, Map<String, String> env, Configuration conf,\n      PrivilegedOperationExecutor privilegedOperationExecutor,\n      boolean disableFailureLogging, Context nmContext)\n      throws ContainerExecutionException {\n    PrivilegedOperation dockerOp = dockerCommand.preparePrivilegedOperation(\n        dockerCommand, containerId, env, conf, nmContext);\n\n    if (disableFailureLogging) {\n      dockerOp.disableFailureLogging();\n    }\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Running docker command: \" + dockerCommand);\n    }\n    try {\n      String result = privilegedOperationExecutor\n          .executePrivilegedOperation(null, dockerOp, null,\n              env, true, false);\n      if (result != null && !result.isEmpty()) {\n        result = result.trim();\n      }\n      return result;\n    } catch (PrivilegedOperationException e) {\n      throw new ContainerExecutionException(\"Docker operation failed\",\n          e.getExitCode(), e.getOutput(), e.getErrorOutput());\n    }\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DockerLinuxContainerRuntime.runDockerVolumeCommand": {
                "code_before_change": "  private String runDockerVolumeCommand(DockerVolumeCommand dockerVolumeCommand,\n      Container container) throws ContainerExecutionException {\n    try {\n      String commandFile = dockerClient.writeCommandToTempFile(\n          dockerVolumeCommand, container, nmContext);\n      PrivilegedOperation privOp = new PrivilegedOperation(\n          PrivilegedOperation.OperationType.RUN_DOCKER_CMD);\n      privOp.appendArgs(commandFile);\n      String output = privilegedOperationExecutor\n          .executePrivilegedOperation(null, privOp, null,\n              null, true, false);\n      LOG.info(\"ContainerId=\" + container.getContainerId()\n          + \", docker volume output for \" + dockerVolumeCommand + \": \"\n          + output);\n      return output;\n    } catch (ContainerExecutionException e) {\n      LOG.error(\"Error when writing command to temp file, command=\"\n              + dockerVolumeCommand,\n          e);\n      throw e;\n    } catch (PrivilegedOperationException e) {\n      LOG.error(\"Error when executing command, command=\"\n          + dockerVolumeCommand, e);\n      throw new ContainerExecutionException(e);\n    }\n\n  }",
                "code_after_change": "  private String runDockerVolumeCommand(DockerVolumeCommand dockerVolumeCommand,\n      Container container) throws ContainerExecutionException {\n    try {\n      String commandFile = dockerClient.writeCommandToTempFile(\n          dockerVolumeCommand, container.getContainerId(), nmContext);\n      PrivilegedOperation privOp = new PrivilegedOperation(\n          PrivilegedOperation.OperationType.RUN_DOCKER_CMD);\n      privOp.appendArgs(commandFile);\n      String output = privilegedOperationExecutor\n          .executePrivilegedOperation(null, privOp, null,\n              null, true, false);\n      LOG.info(\"ContainerId=\" + container.getContainerId()\n          + \", docker volume output for \" + dockerVolumeCommand + \": \"\n          + output);\n      return output;\n    } catch (ContainerExecutionException e) {\n      LOG.error(\"Error when writing command to temp file, command=\"\n              + dockerVolumeCommand,\n          e);\n      throw e;\n    } catch (PrivilegedOperationException e) {\n      LOG.error(\"Error when executing command, command=\"\n          + dockerVolumeCommand, e);\n      throw new ContainerExecutionException(e);\n    }\n\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Correct",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report precisely identifies the root cause of the NullPointerException in the 'writeCommandToTempFile' method of the DockerClient class, which is part of the ground truth methods. The suggestion to add null checks for 'nmContext' and 'Container' objects before proceeding with the file writing logic matches the developer's fix, which involves ensuring these objects are not null. The problem location is precise as it correctly identifies the 'writeCommandToTempFile' method, which is part of the ground truth. There is no wrong information in the bug report as all details are relevant and accurate."
        }
    },
    {
        "filename": "YARN-3804.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.AdminService.serviceInit": {
                "code_before_change": "  public void serviceInit(Configuration conf) throws Exception {\n    if (rmContext.isHAEnabled()) {\n      autoFailoverEnabled = HAUtil.isAutomaticFailoverEnabled(conf);\n      if (autoFailoverEnabled) {\n        if (HAUtil.isAutomaticFailoverEmbedded(conf)) {\n          embeddedElector = createEmbeddedElectorService();\n          addIfService(embeddedElector);\n        }\n      }\n    }\n\n    masterServiceBindAddress = conf.getSocketAddr(\n        YarnConfiguration.RM_BIND_HOST,\n        YarnConfiguration.RM_ADMIN_ADDRESS,\n        YarnConfiguration.DEFAULT_RM_ADMIN_ADDRESS,\n        YarnConfiguration.DEFAULT_RM_ADMIN_PORT);\n    authorizer = YarnAuthorizationProvider.getInstance(conf);\n    authorizer.setAdmins(new AccessControlList(conf.get(\n      YarnConfiguration.YARN_ADMIN_ACL,\n        YarnConfiguration.DEFAULT_YARN_ADMIN_ACL)), UserGroupInformation\n        .getCurrentUser());\n    rmId = conf.get(YarnConfiguration.RM_HA_ID);\n\n    isDistributedNodeLabelConfiguration =\n        YarnConfiguration.isDistributedNodeLabelConfiguration(conf);\n\n    super.serviceInit(conf);\n  }",
                "code_after_change": "  public void serviceInit(Configuration conf) throws Exception {\n    if (rmContext.isHAEnabled()) {\n      autoFailoverEnabled = HAUtil.isAutomaticFailoverEnabled(conf);\n      if (autoFailoverEnabled) {\n        if (HAUtil.isAutomaticFailoverEmbedded(conf)) {\n          embeddedElector = createEmbeddedElectorService();\n          addIfService(embeddedElector);\n        }\n      }\n    }\n\n    masterServiceBindAddress = conf.getSocketAddr(\n        YarnConfiguration.RM_BIND_HOST,\n        YarnConfiguration.RM_ADMIN_ADDRESS,\n        YarnConfiguration.DEFAULT_RM_ADMIN_ADDRESS,\n        YarnConfiguration.DEFAULT_RM_ADMIN_PORT);\n    daemonUser = UserGroupInformation.getCurrentUser();\n    authorizer = YarnAuthorizationProvider.getInstance(conf);\n    authorizer.setAdmins(getAdminAclList(conf), UserGroupInformation\n        .getCurrentUser());\n    rmId = conf.get(YarnConfiguration.RM_HA_ID);\n\n    isDistributedNodeLabelConfiguration =\n        YarnConfiguration.isDistributedNodeLabelConfiguration(conf);\n\n    super.serviceInit(conf);\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.AdminService.refreshAdminAcls": {
                "code_before_change": "  public RefreshAdminAclsResponse refreshAdminAcls(\n      RefreshAdminAclsRequest request) throws YarnException, IOException {\n    return refreshAdminAcls(true);\n  }",
                "code_after_change": "  public RefreshAdminAclsResponse refreshAdminAcls(\n      RefreshAdminAclsRequest request) throws YarnException, IOException {\n    return refreshAdminAcls(true);\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Correct",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report precisely identifies the root cause as the lack of necessary permissions for the 'yarn' user to call the 'refreshAdminAcls' method, which is directly related to the ground truth method 'AdminService.refreshAdminAcls'. The fix suggestion to modify the 'yarn.admin.acl' property to include the 'yarn' user matches the developer's fix, which involves setting the correct admin ACLs. The problem location is precisely identified as it mentions the 'AdminService.refreshAdminAcls' method, which is part of the ground truth methods. There is no wrong information in the bug report as all details are consistent with the context of the bug."
        }
    },
    {
        "filename": "YARN-1839.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.transition": {
                "code_before_change": "    public void transition(RMAppAttemptImpl appAttempt,\n        RMAppAttemptEvent event) {\n    }",
                "code_after_change": "    public void transition(RMAppAttemptImpl appAttempt,\n        RMAppAttemptEvent event) {\n    }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM.createAndGetNMToken": {
                "code_before_change": "  public NMToken createAndGetNMToken(String applicationSubmitter,\n      ApplicationAttemptId appAttemptId, Container container) {\n    try {\n      this.readLock.lock();\n      HashSet<NodeId> nodeSet = this.appAttemptToNodeKeyMap.get(appAttemptId);\n      NMToken nmToken = null;\n      if (nodeSet != null) {\n        if (!nodeSet.contains(container.getNodeId())) {\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Sending NMToken for nodeId : \"\n                + container.getNodeId().toString()\n                + \" for application attempt : \" + appAttemptId.toString());\n          }\n          Token token =\n              createNMToken(container.getId().getApplicationAttemptId(),\n                container.getNodeId(), applicationSubmitter);\n          nmToken = NMToken.newInstance(container.getNodeId(), token);\n          // The node set here is used for differentiating whether the NMToken\n          // has been issued for this node from the client's perspective. If\n          // this is an AM container, the NMToken is issued only for RM and so\n          // we should not update the node set.\n          if (container.getId().getId() != 1) {\n            nodeSet.add(container.getNodeId());\n          }\n        }\n      }\n      return nmToken;\n    } finally {\n      this.readLock.unlock();\n    }\n  }",
                "code_after_change": "  public NMToken createAndGetNMToken(String applicationSubmitter,\n      ApplicationAttemptId appAttemptId, Container container) {\n    try {\n      this.readLock.lock();\n      HashSet<NodeId> nodeSet = this.appAttemptToNodeKeyMap.get(appAttemptId);\n      NMToken nmToken = null;\n      if (nodeSet != null) {\n        if (!nodeSet.contains(container.getNodeId())) {\n          LOG.info(\"Sending NMToken for nodeId : \" + container.getNodeId()\n              + \" for container : \" + container.getId());\n          Token token =\n              createNMToken(container.getId().getApplicationAttemptId(),\n                container.getNodeId(), applicationSubmitter);\n          nmToken = NMToken.newInstance(container.getNodeId(), token);\n          nodeSet.add(container.getNodeId());\n        }\n      }\n      return nmToken;\n    } finally {\n      this.readLock.unlock();\n    }\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-client.src.main.java.org.apache.hadoop.yarn.client.api.impl.AMRMClientImpl.populateNMTokens": {
                "code_before_change": "  protected void populateNMTokens(List<NMToken> nmTokens) {\n    for (NMToken token : nmTokens) {\n      String nodeId = token.getNodeId().toString();\n      if (getNMTokenCache().containsToken(nodeId)) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Replacing token for : \" + nodeId);\n        }\n      } else {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Received new token for : \" + nodeId);\n        }\n      }\n      getNMTokenCache().setToken(nodeId, token.getToken());\n    }\n  }",
                "code_after_change": "  protected void populateNMTokens(List<NMToken> nmTokens) {\n    for (NMToken token : nmTokens) {\n      String nodeId = token.getNodeId().toString();\n      if (getNMTokenCache().containsToken(nodeId)) {\n        LOG.info(\"Replacing token for : \" + nodeId);\n      } else {\n        LOG.info(\"Received new token for : \" + nodeId);\n      }\n      getNMTokenCache().setToken(nodeId, token.getToken());\n    }\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.completedContainer": {
                "code_before_change": "  private synchronized void completedContainer(RMContainer rmContainer,\n      ContainerStatus containerStatus, RMContainerEventType event) {\n    if (rmContainer == null) {\n      LOG.info(\"Null container completed...\");\n      return;\n    }\n    \n    Container container = rmContainer.getContainer();\n    \n    // Get the application for the finished container\n    FiCaSchedulerApp application =\n        getCurrentAttemptForContainer(container.getId());\n    ApplicationId appId =\n        container.getId().getApplicationAttemptId().getApplicationId();\n    if (application == null) {\n      LOG.info(\"Container \" + container + \" of\" + \" unknown application \"\n          + appId + \" completed with event \" + event);\n      return;\n    }\n    \n    // Get the node on which the container was allocated\n    FiCaSchedulerNode node = getNode(container.getNodeId());\n    \n    // Inform the queue\n    LeafQueue queue = (LeafQueue)application.getQueue();\n    queue.completedContainer(clusterResource, application, node, \n        rmContainer, containerStatus, event, null);\n\n    LOG.info(\"Application attempt \" + application.getApplicationAttemptId()\n        + \" released container \" + container.getId() + \" on node: \" + node\n        + \" with event: \" + event);\n  }",
                "code_after_change": "  private synchronized void completedContainer(RMContainer rmContainer,\n      ContainerStatus containerStatus, RMContainerEventType event) {\n    if (rmContainer == null) {\n      LOG.info(\"Null container completed...\");\n      return;\n    }\n    \n    Container container = rmContainer.getContainer();\n    \n    // Get the application for the finished container\n    FiCaSchedulerApp application =\n        getCurrentAttemptForContainer(container.getId());\n    ApplicationId appId =\n        container.getId().getApplicationAttemptId().getApplicationId();\n    if (application == null) {\n      LOG.info(\"Container \" + container + \" of\" + \" unknown application \"\n          + appId + \" completed with event \" + event);\n      return;\n    }\n    \n    // Get the node on which the container was allocated\n    FiCaSchedulerNode node = getNode(container.getNodeId());\n    \n    // Inform the queue\n    LeafQueue queue = (LeafQueue)application.getQueue();\n    queue.completedContainer(clusterResource, application, node, \n        rmContainer, containerStatus, event, null);\n\n    LOG.info(\"Application attempt \" + application.getApplicationAttemptId()\n        + \" released container \" + container.getId() + \" on node: \" + node\n        + \" with event: \" + event);\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.security.NMTokenSecretManagerInRM.activateNextMasterKey": {
                "code_before_change": "  public void activateNextMasterKey() {\n    super.writeLock.lock();\n    try {\n      LOG.info(\"Activating next master key with id: \"\n          + this.nextMasterKey.getMasterKey().getKeyId());\n      this.currentMasterKey = this.nextMasterKey;\n      this.nextMasterKey = null;\n      clearApplicationNMTokenKeys();\n    } finally {\n      super.writeLock.unlock();\n    }\n  }",
                "code_after_change": "  public void activateNextMasterKey() {\n    super.writeLock.lock();\n    try {\n      LOG.info(\"Activating next master key with id: \"\n          + this.nextMasterKey.getMasterKey().getKeyId());\n      this.currentMasterKey = this.nextMasterKey;\n      this.nextMasterKey = null;\n      clearApplicationNMTokenKeys();\n    } finally {\n      super.writeLock.unlock();\n    }\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Preventive",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the issue with NMToken not being sent or invalid, which is related to the ground truth methods dealing with NMToken management, but it does not precisely identify the root cause in those methods. The 'getProxy' method mentioned in the problem location is part of the stack trace, sharing context with the ground truth methods. The fix suggestion involves checking configurations and ensuring token validity, which is preventive but not directly matching the developer's fix. There is no incorrect information in the bug report."
        }
    },
    {
        "filename": "YARN-6714.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.tryCommit": {
                "code_before_change": "  public void tryCommit(Resource cluster, ResourceCommitRequest r) {\n    ResourceCommitRequest<FiCaSchedulerApp, FiCaSchedulerNode> request =\n        (ResourceCommitRequest<FiCaSchedulerApp, FiCaSchedulerNode>) r;\n\n    ApplicationAttemptId attemptId = null;\n\n    // We need to update unconfirmed allocated resource of application when\n    // any container allocated.\n    boolean updateUnconfirmedAllocatedResource =\n        request.getContainersToAllocate() != null && !request\n            .getContainersToAllocate().isEmpty();\n\n    // find the application to accept and apply the ResourceCommitRequest\n    if (request.anythingAllocatedOrReserved()) {\n      ContainerAllocationProposal<FiCaSchedulerApp, FiCaSchedulerNode> c =\n          request.getFirstAllocatedOrReservedContainer();\n      attemptId =\n          c.getAllocatedOrReservedContainer().getSchedulerApplicationAttempt()\n              .getApplicationAttemptId();\n    } else {\n      if (!request.getContainersToRelease().isEmpty()) {\n        attemptId = request.getContainersToRelease().get(0)\n            .getSchedulerApplicationAttempt().getApplicationAttemptId();\n      }\n    }\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Try to commit allocation proposal=\" + request);\n    }\n\n    if (attemptId != null) {\n      FiCaSchedulerApp app = getApplicationAttempt(attemptId);\n      if (app != null) {\n        if (app.accept(cluster, request)) {\n          app.apply(cluster, request);\n          LOG.info(\"Allocation proposal accepted\");\n        } else{\n          LOG.info(\"Failed to accept allocation proposal\");\n        }\n\n        // Update unconfirmed allocated resource.\n        if (updateUnconfirmedAllocatedResource) {\n          app.decUnconfirmedRes(request.getTotalAllocatedResource());\n        }\n      }\n    }\n  }",
                "code_after_change": "  public void tryCommit(Resource cluster, ResourceCommitRequest r) {\n    ResourceCommitRequest<FiCaSchedulerApp, FiCaSchedulerNode> request =\n        (ResourceCommitRequest<FiCaSchedulerApp, FiCaSchedulerNode>) r;\n\n    ApplicationAttemptId attemptId = null;\n\n    // We need to update unconfirmed allocated resource of application when\n    // any container allocated.\n    boolean updateUnconfirmedAllocatedResource =\n        request.getContainersToAllocate() != null && !request\n            .getContainersToAllocate().isEmpty();\n\n    // find the application to accept and apply the ResourceCommitRequest\n    if (request.anythingAllocatedOrReserved()) {\n      ContainerAllocationProposal<FiCaSchedulerApp, FiCaSchedulerNode> c =\n          request.getFirstAllocatedOrReservedContainer();\n      attemptId =\n          c.getAllocatedOrReservedContainer().getSchedulerApplicationAttempt()\n              .getApplicationAttemptId();\n    } else {\n      if (!request.getContainersToRelease().isEmpty()) {\n        attemptId = request.getContainersToRelease().get(0)\n            .getSchedulerApplicationAttempt().getApplicationAttemptId();\n      }\n    }\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Try to commit allocation proposal=\" + request);\n    }\n\n    if (attemptId != null) {\n      FiCaSchedulerApp app = getApplicationAttempt(attemptId);\n      // Required sanity check for attemptId - when async-scheduling enabled,\n      // proposal might be outdated if AM failover just finished\n      // and proposal queue was not be consumed in time\n      if (app != null && attemptId.equals(app.getApplicationAttemptId())) {\n        if (app.accept(cluster, request)) {\n          app.apply(cluster, request);\n          LOG.info(\"Allocation proposal accepted\");\n        } else{\n          LOG.info(\"Failed to accept allocation proposal\");\n        }\n\n        // Update unconfirmed allocated resource.\n        if (updateUnconfirmedAllocatedResource) {\n          app.decUnconfirmedRes(request.getTotalAllocatedResource());\n        }\n      }\n    }\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Alternative Fix",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the root cause as an issue with resource unreservation logic, which is related to the stack trace context but not the exact root cause in the 'tryCommit' method. The fix suggestion involves adding validation checks and locking mechanisms, which is an alternative approach to the developer's fix that added a sanity check for 'attemptId'. The problem location mentions methods in the stack trace but not the 'tryCommit' method, making it a partial identification with shared stack trace context. There is no wrong information as the report accurately describes the issue and its context."
        }
    },
    {
        "filename": "YARN-3351.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.webapp.util.WebAppUtils.getResolvedRMWebAppURLWithoutScheme": {
                "code_before_change": "  public static String getResolvedRMWebAppURLWithoutScheme(Configuration conf) {\n    return getResolvedRMWebAppURLWithoutScheme(conf,\n        YarnConfiguration.useHttps(conf) ? Policy.HTTPS_ONLY : Policy.HTTP_ONLY);\n  }",
                "code_after_change": "  public static String getResolvedRMWebAppURLWithoutScheme(Configuration conf) {\n    return getResolvedRMWebAppURLWithoutScheme(conf,\n        YarnConfiguration.useHttps(conf) ? Policy.HTTPS_ONLY : Policy.HTTP_ONLY);\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.webapp.util.WebAppUtils.getProxyHostAndPort": {
                "code_before_change": "  public static String getProxyHostAndPort(Configuration conf) {\n    String addr = conf.get(YarnConfiguration.PROXY_ADDRESS);\n    if(addr == null || addr.isEmpty()) {\n      addr = getResolvedRMWebAppURLWithoutScheme(conf);\n    }\n    return addr;\n  }",
                "code_after_change": "  public static String getProxyHostAndPort(Configuration conf) {\n    String addr = conf.get(YarnConfiguration.PROXY_ADDRESS);\n    if(addr == null || addr.isEmpty()) {\n      addr = getResolvedRMWebAppURLWithoutScheme(conf);\n    }\n    return addr;\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.webapp.util.WebAppUtils.getResolvedRemoteRMWebAppURLWithoutScheme": {
                "code_before_change": [],
                "code_after_change": "  public static String getResolvedRemoteRMWebAppURLWithoutScheme(\n      Configuration conf) {\n    return getResolvedRemoteRMWebAppURLWithoutScheme(conf,\n        YarnConfiguration.useHttps(conf) ? Policy.HTTPS_ONLY : Policy.HTTP_ONLY);\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.webapp.NavBlock.NavBlock": {
                "code_before_change": [],
                "code_after_change": []
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Preventive",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the root cause as a `java.net.BindException`, which is mentioned in the stack trace context but not directly related to the ground truth methods. The fix suggestion is preventive as it suggests checking network configurations and permissions, which could prevent the issue but does not match the developer's fix. The problem location is partially identified as it mentions methods in the same stack trace context as the ground truth methods, specifically `WebAppProxyServlet.proxyLink` and `WebAppProxyServlet.doGet`. There is no wrong information in the bug report as all details are relevant to the context of the bug."
        }
    },
    {
        "filename": "YARN-2813.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-applicationhistoryservice.src.main.java.org.apache.hadoop.yarn.server.timeline.MemoryTimelineStore.getDomain": {
                "code_before_change": "  public TimelineDomain getDomain(String domainId)\n      throws IOException {\n    TimelineDomain domain = domainsById.get(domainId);\n    if (domain == null) {\n      return null;\n    } else {\n      return createTimelineDomain(\n          domain.getId(),\n          domain.getDescription(),\n          domain.getOwner(),\n          domain.getReaders(),\n          domain.getWriters(),\n          domain.getCreatedTime(),\n          domain.getModifiedTime());\n    }\n  }",
                "code_after_change": "  public TimelineDomain getDomain(String domainId)\n      throws IOException {\n    TimelineDomain domain = domainsById.get(domainId);\n    if (domain == null) {\n      return null;\n    } else {\n      return createTimelineDomain(\n          domain.getId(),\n          domain.getDescription(),\n          domain.getOwner(),\n          domain.getReaders(),\n          domain.getWriters(),\n          domain.getCreatedTime(),\n          domain.getModifiedTime());\n    }\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-applicationhistoryservice.src.main.java.org.apache.hadoop.yarn.server.timeline.MemoryTimelineStore.getDomains": {
                "code_before_change": "  public TimelineDomains getDomains(String owner)\n      throws IOException {\n    List<TimelineDomain> domains = new ArrayList<TimelineDomain>();\n    for (TimelineDomain domain : domainsByOwner.get(owner)) {\n      TimelineDomain domainToReturn = createTimelineDomain(\n          domain.getId(),\n          domain.getDescription(),\n          domain.getOwner(),\n          domain.getReaders(),\n          domain.getWriters(),\n          domain.getCreatedTime(),\n          domain.getModifiedTime());\n      domains.add(domainToReturn);\n    }\n    Collections.sort(domains, new Comparator<TimelineDomain>() {\n      @Override\n      public int compare(\n          TimelineDomain domain1, TimelineDomain domain2) {\n         int result = domain2.getCreatedTime().compareTo(\n             domain1.getCreatedTime());\n         if (result == 0) {\n           return domain2.getModifiedTime().compareTo(\n               domain1.getModifiedTime());\n         } else {\n           return result;\n         }\n      }\n    });\n    TimelineDomains domainsToReturn = new TimelineDomains();\n    domainsToReturn.addDomains(domains);\n    return domainsToReturn;\n  }",
                "code_after_change": "  public TimelineDomains getDomains(String owner)\n      throws IOException {\n    List<TimelineDomain> domains = new ArrayList<TimelineDomain>();\n    Set<TimelineDomain> domainsOfOneOwner = domainsByOwner.get(owner);\n    if (domainsOfOneOwner == null) {\n      return new TimelineDomains();\n    }\n    for (TimelineDomain domain : domainsByOwner.get(owner)) {\n      TimelineDomain domainToReturn = createTimelineDomain(\n          domain.getId(),\n          domain.getDescription(),\n          domain.getOwner(),\n          domain.getReaders(),\n          domain.getWriters(),\n          domain.getCreatedTime(),\n          domain.getModifiedTime());\n      domains.add(domainToReturn);\n    }\n    Collections.sort(domains, new Comparator<TimelineDomain>() {\n      @Override\n      public int compare(\n          TimelineDomain domain1, TimelineDomain domain2) {\n         int result = domain2.getCreatedTime().compareTo(\n             domain1.getCreatedTime());\n         if (result == 0) {\n           return domain2.getModifiedTime().compareTo(\n               domain1.getModifiedTime());\n         } else {\n           return result;\n         }\n      }\n    });\n    TimelineDomains domainsToReturn = new TimelineDomains();\n    domainsToReturn.addDomains(domains);\n    return domainsToReturn;\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Correct",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report precisely identifies the root cause of the NullPointerException in the MemoryTimelineStore.getDomains method, which is part of the ground truth methods. The fix suggestion provided in the report matches the developer's fix, which involves checking if the domainsByOwner map contains the specified owner before attempting to access it. The problem location is also precisely identified as it mentions the MemoryTimelineStore.getDomains method, which is part of the ground truth. There is no wrong information in the bug report as all details are consistent with the context of the bug."
        }
    },
    {
        "filename": "YARN-5006.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppEvent.getApplicationId": {
                "code_before_change": "  public ApplicationId getApplicationId() {\n    return this.appId;\n  }",
                "code_after_change": "  public ApplicationId getApplicationId() {\n    return this.appId;\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.FinalSavingTransition": {
                "code_before_change": [],
                "code_after_change": []
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-api.src.main.java.org.apache.hadoop.yarn.conf.YarnConfiguration.isAclEnabled": {
                "code_before_change": [],
                "code_after_change": []
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppEvent.RMAppEvent": {
                "code_before_change": [],
                "code_after_change": []
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Preventive",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the issue with storing ApplicationStateData in ZooKeeper due to exceeding the znode size limit, which is related to the methods in the stack trace but not the ground truth methods. Therefore, the root cause identification is 'Partial' with 'Shared Stack Trace Context' as the sub-category. The fix suggestion is 'Preventive' as it suggests reducing the size of ApplicationStateData or increasing the znode size limit, which would mitigate the issue. The problem location identification is also 'Partial' with 'Shared Stack Trace Context' because the methods mentioned in the problem_location field are part of the stack trace but not the ground truth methods. There is no wrong information in the bug report as all details are relevant to the context of the bug."
        }
    },
    {
        "filename": "YARN-2805.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceInit": {
                "code_before_change": "  protected void serviceInit(Configuration conf) throws Exception {\n    this.conf = conf;\n    this.rmContext = new RMContextImpl();\n    \n    // Set UGI and do login\n    // If security is enabled, use login user\n    // If security is not enabled, use current user\n    this.rmLoginUGI = UserGroupInformation.getCurrentUser();\n    try {\n      doSecureLogin();\n    } catch(IOException ie) {\n      throw new YarnRuntimeException(\"Failed to login\", ie);\n    }\n    \n    this.configurationProvider =\n        ConfigurationProviderFactory.getConfigurationProvider(conf);\n    this.configurationProvider.init(this.conf);\n    rmContext.setConfigurationProvider(configurationProvider);\n\n    // load core-site.xml\n    InputStream coreSiteXMLInputStream =\n        this.configurationProvider.getConfigurationInputStream(this.conf,\n            YarnConfiguration.CORE_SITE_CONFIGURATION_FILE);\n    if (coreSiteXMLInputStream != null) {\n      this.conf.addResource(coreSiteXMLInputStream);\n    }\n\n    // Do refreshUserToGroupsMappings with loaded core-site.xml\n    Groups.getUserToGroupsMappingServiceWithLoadedConfiguration(this.conf)\n        .refresh();\n\n    // Do refreshSuperUserGroupsConfiguration with loaded core-site.xml\n    // Or use RM specific configurations to overwrite the common ones first\n    // if they exist\n    RMServerUtils.processRMProxyUsersConf(conf);\n    ProxyUsers.refreshSuperUserGroupsConfiguration(this.conf);\n\n    // load yarn-site.xml\n    InputStream yarnSiteXMLInputStream =\n        this.configurationProvider.getConfigurationInputStream(this.conf,\n            YarnConfiguration.YARN_SITE_CONFIGURATION_FILE);\n    if (yarnSiteXMLInputStream != null) {\n      this.conf.addResource(yarnSiteXMLInputStream);\n    }\n\n    validateConfigs(this.conf);\n\n    // register the handlers for all AlwaysOn services using setupDispatcher().\n    rmDispatcher = setupDispatcher();\n    addIfService(rmDispatcher);\n    rmContext.setDispatcher(rmDispatcher);\n\n    adminService = createAdminService();\n    addService(adminService);\n    rmContext.setRMAdminService(adminService);\n\n    this.rmContext.setHAEnabled(HAUtil.isHAEnabled(this.conf));\n    if (this.rmContext.isHAEnabled()) {\n      HAUtil.verifyAndSetConfiguration(this.conf);\n    }\n    \n    createAndInitActiveServices();\n\n    webAppAddress = WebAppUtils.getWebAppBindURL(this.conf,\n                      YarnConfiguration.RM_BIND_HOST,\n                      WebAppUtils.getRMWebAppURLWithoutScheme(this.conf));\n\n    super.serviceInit(this.conf);\n  }",
                "code_after_change": "  protected void serviceInit(Configuration conf) throws Exception {\n    this.conf = conf;\n    this.rmContext = new RMContextImpl();\n    \n    // Set HA configuration should be done before login\n    this.rmContext.setHAEnabled(HAUtil.isHAEnabled(this.conf));\n    if (this.rmContext.isHAEnabled()) {\n      HAUtil.verifyAndSetConfiguration(this.conf);\n    }\n    \n    // Set UGI and do login\n    // If security is enabled, use login user\n    // If security is not enabled, use current user\n    this.rmLoginUGI = UserGroupInformation.getCurrentUser();\n    try {\n      doSecureLogin();\n    } catch(IOException ie) {\n      throw new YarnRuntimeException(\"Failed to login\", ie);\n    }\n    \n    this.configurationProvider =\n        ConfigurationProviderFactory.getConfigurationProvider(conf);\n    this.configurationProvider.init(this.conf);\n    rmContext.setConfigurationProvider(configurationProvider);\n\n    // load core-site.xml\n    InputStream coreSiteXMLInputStream =\n        this.configurationProvider.getConfigurationInputStream(this.conf,\n            YarnConfiguration.CORE_SITE_CONFIGURATION_FILE);\n    if (coreSiteXMLInputStream != null) {\n      this.conf.addResource(coreSiteXMLInputStream);\n    }\n\n    // Do refreshUserToGroupsMappings with loaded core-site.xml\n    Groups.getUserToGroupsMappingServiceWithLoadedConfiguration(this.conf)\n        .refresh();\n\n    // Do refreshSuperUserGroupsConfiguration with loaded core-site.xml\n    // Or use RM specific configurations to overwrite the common ones first\n    // if they exist\n    RMServerUtils.processRMProxyUsersConf(conf);\n    ProxyUsers.refreshSuperUserGroupsConfiguration(this.conf);\n\n    // load yarn-site.xml\n    InputStream yarnSiteXMLInputStream =\n        this.configurationProvider.getConfigurationInputStream(this.conf,\n            YarnConfiguration.YARN_SITE_CONFIGURATION_FILE);\n    if (yarnSiteXMLInputStream != null) {\n      this.conf.addResource(yarnSiteXMLInputStream);\n    }\n\n    validateConfigs(this.conf);\n\n    // register the handlers for all AlwaysOn services using setupDispatcher().\n    rmDispatcher = setupDispatcher();\n    addIfService(rmDispatcher);\n    rmContext.setDispatcher(rmDispatcher);\n\n    adminService = createAdminService();\n    addService(adminService);\n    rmContext.setRMAdminService(adminService);\n    \n    createAndInitActiveServices();\n\n    webAppAddress = WebAppUtils.getWebAppBindURL(this.conf,\n                      YarnConfiguration.RM_BIND_HOST,\n                      WebAppUtils.getRMWebAppURLWithoutScheme(this.conf));\n\n    super.serviceInit(this.conf);\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Buggy Method"
            },
            "fix_suggestion": "Preventive",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the root cause as issues with the keytab file, which is related to the method 'ResourceManager.serviceInit' where the error occurred, but not where the actual fix was made. Therefore, it is classified as 'Partial' with the sub-category 'Buggy Method'. The fix suggestion is 'Preventive' as it suggests checking permissions and configurations, which would prevent the issue but does not match the developer's fix of setting HA configuration before login. The problem location identification is 'Precise' because the 'problem_location' field mentions 'ResourceManager.serviceInit', which is in the ground truth list. There is no wrong information in the bug report as all details are relevant to the context of the bug."
        }
    },
    {
        "filename": "YARN-4744.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.TrafficController.BatchBuilder": {
                "code_before_change": [],
                "code_after_change": []
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperation.appendArgs": {
                "code_before_change": "  public void appendArgs(String... args) {\n    for (String arg : args) {\n      this.args.add(arg);\n    }\n  }",
                "code_after_change": "  public void appendArgs(String... args) {\n    for (String arg : args) {\n      this.args.add(arg);\n    }\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DefaultLinuxContainerRuntime.signalContainer": {
                "code_before_change": "  public void signalContainer(ContainerRuntimeContext ctx)\n      throws ContainerExecutionException {\n    Container container = ctx.getContainer();\n    PrivilegedOperation signalOp = new PrivilegedOperation(\n        PrivilegedOperation.OperationType.SIGNAL_CONTAINER, (String) null);\n\n    signalOp.appendArgs(ctx.getExecutionAttribute(RUN_AS_USER),\n        ctx.getExecutionAttribute(USER),\n        Integer.toString(PrivilegedOperation.RunAsUserCommand\n            .SIGNAL_CONTAINER.getValue()),\n        ctx.getExecutionAttribute(PID),\n        Integer.toString(ctx.getExecutionAttribute(SIGNAL).getValue()));\n\n    try {\n      PrivilegedOperationExecutor executor = PrivilegedOperationExecutor\n          .getInstance(conf);\n\n      executor.executePrivilegedOperation(null,\n          signalOp, null, container.getLaunchContext().getEnvironment(),\n          false);\n    } catch (PrivilegedOperationException e) {\n      LOG.warn(\"Signal container failed. Exception: \", e);\n\n      throw new ContainerExecutionException(\"Signal container failed\", e\n          .getExitCode(), e.getOutput(), e.getErrorOutput());\n    }\n  }",
                "code_after_change": "  public void signalContainer(ContainerRuntimeContext ctx)\n      throws ContainerExecutionException {\n    Container container = ctx.getContainer();\n    PrivilegedOperation signalOp = new PrivilegedOperation(\n        PrivilegedOperation.OperationType.SIGNAL_CONTAINER);\n\n    signalOp.appendArgs(ctx.getExecutionAttribute(RUN_AS_USER),\n        ctx.getExecutionAttribute(USER),\n        Integer.toString(PrivilegedOperation.RunAsUserCommand\n            .SIGNAL_CONTAINER.getValue()),\n        ctx.getExecutionAttribute(PID),\n        Integer.toString(ctx.getExecutionAttribute(SIGNAL).getValue()));\n\n    //Some failures here are acceptable. Let the calling executor decide.\n    signalOp.disableFailureLogging();\n\n    try {\n      PrivilegedOperationExecutor executor = PrivilegedOperationExecutor\n          .getInstance(conf);\n\n      executor.executePrivilegedOperation(null,\n          signalOp, null, container.getLaunchContext().getEnvironment(),\n          false);\n    } catch (PrivilegedOperationException e) {\n      //Don't log the failure here. Some kinds of signaling failures are\n      // acceptable. Let the calling executor decide what to do.\n      throw new ContainerExecutionException(\"Signal container failed\", e\n          .getExitCode(), e.getOutput(), e.getErrorOutput());\n    }\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationExecutor.executePrivilegedOperation": {
                "code_before_change": "  public String executePrivilegedOperation(List<String> prefixCommands,\n      PrivilegedOperation operation, File workingDir,\n      Map<String, String> env, boolean grabOutput)\n      throws PrivilegedOperationException {\n    String[] fullCommandArray = getPrivilegedOperationExecutionCommand\n        (prefixCommands, operation);\n    ShellCommandExecutor exec = new ShellCommandExecutor(fullCommandArray,\n        workingDir, env);\n\n    try {\n      exec.execute();\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"command array:\");\n        LOG.debug(Arrays.toString(fullCommandArray));\n        LOG.debug(\"Privileged Execution Operation Output:\");\n        LOG.debug(exec.getOutput());\n      }\n    } catch (ExitCodeException e) {\n      StringBuilder logBuilder = new StringBuilder(\"Shell execution returned \"\n          + \"exit code: \")\n          .append(exec.getExitCode())\n          .append(\". Privileged Execution Operation Output: \")\n          .append(System.lineSeparator()).append(exec.getOutput());\n\n      logBuilder.append(\"Full command array for failed execution: \")\n          .append(System.lineSeparator());\n      logBuilder.append(Arrays.toString(fullCommandArray));\n\n      LOG.warn(logBuilder.toString());\n\n      //stderr from shell executor seems to be stuffed into the exception\n      //'message' - so, we have to extract it and set it as the error out\n      throw new PrivilegedOperationException(e, e.getExitCode(),\n          exec.getOutput(), e.getMessage());\n    } catch (IOException e) {\n      LOG.warn(\"IOException executing command: \", e);\n      throw new PrivilegedOperationException(e);\n    }\n\n    if (grabOutput) {\n      return exec.getOutput();\n    }\n\n    return null;\n  }",
                "code_after_change": "  public String executePrivilegedOperation(List<String> prefixCommands,\n      PrivilegedOperation operation, File workingDir,\n      Map<String, String> env, boolean grabOutput)\n      throws PrivilegedOperationException {\n    String[] fullCommandArray = getPrivilegedOperationExecutionCommand\n        (prefixCommands, operation);\n    ShellCommandExecutor exec = new ShellCommandExecutor(fullCommandArray,\n        workingDir, env);\n\n    try {\n      exec.execute();\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"command array:\");\n        LOG.debug(Arrays.toString(fullCommandArray));\n        LOG.debug(\"Privileged Execution Operation Output:\");\n        LOG.debug(exec.getOutput());\n      }\n    } catch (ExitCodeException e) {\n      if (operation.isFailureLoggingEnabled()) {\n\n        StringBuilder logBuilder = new StringBuilder(\"Shell execution returned \"\n            + \"exit code: \")\n            .append(exec.getExitCode())\n            .append(\". Privileged Execution Operation Output: \")\n            .append(System.lineSeparator()).append(exec.getOutput());\n\n        logBuilder.append(\"Full command array for failed execution: \")\n            .append(System.lineSeparator());\n        logBuilder.append(Arrays.toString(fullCommandArray));\n\n        LOG.warn(logBuilder.toString());\n      }\n\n      //stderr from shell executor seems to be stuffed into the exception\n      //'message' - so, we have to extract it and set it as the error out\n      throw new PrivilegedOperationException(e, e.getExitCode(),\n          exec.getOutput(), e.getMessage());\n    } catch (IOException e) {\n      LOG.warn(\"IOException executing command: \", e);\n      throw new PrivilegedOperationException(e);\n    }\n\n    if (grabOutput) {\n      return exec.getOutput();\n    }\n\n    return null;\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.resources.CGroupsHandlerImpl.mountCGroupController": {
                "code_before_change": "  public void mountCGroupController(CGroupController controller)\n      throws ResourceHandlerException {\n    if (!enableCGroupMount) {\n      LOG.warn(\"CGroup mounting is disabled - ignoring mount request for: \" +\n          controller.getName());\n      return;\n    }\n\n    String path = getControllerPath(controller);\n\n    if (path == null) {\n      try {\n        //lock out other readers/writers till we are done\n        rwLock.writeLock().lock();\n\n        String hierarchy = cGroupPrefix;\n        StringBuffer controllerPath = new StringBuffer()\n            .append(cGroupMountPath).append('/').append(controller.getName());\n        StringBuffer cGroupKV = new StringBuffer()\n            .append(controller.getName()).append('=').append(controllerPath);\n        PrivilegedOperation.OperationType opType = PrivilegedOperation\n            .OperationType.MOUNT_CGROUPS;\n        PrivilegedOperation op = new PrivilegedOperation(opType, (String) null);\n\n        op.appendArgs(hierarchy, cGroupKV.toString());\n        LOG.info(\"Mounting controller \" + controller.getName() + \" at \" +\n              controllerPath);\n        privilegedOperationExecutor.executePrivilegedOperation(op, false);\n\n        //if privileged operation succeeds, update controller paths\n        controllerPaths.put(controller, controllerPath.toString());\n\n        return;\n      } catch (PrivilegedOperationException e) {\n        LOG.error(\"Failed to mount controller: \" + controller.getName());\n        throw new ResourceHandlerException(\"Failed to mount controller: \"\n            + controller.getName());\n      } finally {\n        rwLock.writeLock().unlock();\n      }\n    } else {\n      LOG.info(\"CGroup controller already mounted at: \" + path);\n      return;\n    }\n  }",
                "code_after_change": "  public void mountCGroupController(CGroupController controller)\n      throws ResourceHandlerException {\n    if (!enableCGroupMount) {\n      LOG.warn(\"CGroup mounting is disabled - ignoring mount request for: \" +\n          controller.getName());\n      return;\n    }\n\n    String path = getControllerPath(controller);\n\n    if (path == null) {\n      try {\n        //lock out other readers/writers till we are done\n        rwLock.writeLock().lock();\n\n        String hierarchy = cGroupPrefix;\n        StringBuffer controllerPath = new StringBuffer()\n            .append(cGroupMountPath).append('/').append(controller.getName());\n        StringBuffer cGroupKV = new StringBuffer()\n            .append(controller.getName()).append('=').append(controllerPath);\n        PrivilegedOperation.OperationType opType = PrivilegedOperation\n            .OperationType.MOUNT_CGROUPS;\n        PrivilegedOperation op = new PrivilegedOperation(opType);\n\n        op.appendArgs(hierarchy, cGroupKV.toString());\n        LOG.info(\"Mounting controller \" + controller.getName() + \" at \" +\n              controllerPath);\n        privilegedOperationExecutor.executePrivilegedOperation(op, false);\n\n        //if privileged operation succeeds, update controller paths\n        controllerPaths.put(controller, controllerPath.toString());\n\n        return;\n      } catch (PrivilegedOperationException e) {\n        LOG.error(\"Failed to mount controller: \" + controller.getName());\n        throw new ResourceHandlerException(\"Failed to mount controller: \"\n            + controller.getName());\n      } finally {\n        rwLock.writeLock().unlock();\n      }\n    } else {\n      LOG.info(\"CGroup controller already mounted at: \" + path);\n      return;\n    }\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DefaultLinuxContainerRuntime.launchContainer": {
                "code_before_change": "  public void launchContainer(ContainerRuntimeContext ctx)\n      throws ContainerExecutionException {\n    Container container = ctx.getContainer();\n    PrivilegedOperation launchOp = new PrivilegedOperation(\n        PrivilegedOperation.OperationType.LAUNCH_CONTAINER, (String) null);\n\n    //All of these arguments are expected to be available in the runtime context\n    launchOp.appendArgs(ctx.getExecutionAttribute(RUN_AS_USER),\n        ctx.getExecutionAttribute(USER),\n        Integer.toString(PrivilegedOperation.\n            RunAsUserCommand.LAUNCH_CONTAINER.getValue()),\n        ctx.getExecutionAttribute(APPID),\n        ctx.getExecutionAttribute(CONTAINER_ID_STR),\n        ctx.getExecutionAttribute(CONTAINER_WORK_DIR).toString(),\n        ctx.getExecutionAttribute(NM_PRIVATE_CONTAINER_SCRIPT_PATH).toUri()\n            .getPath(),\n        ctx.getExecutionAttribute(NM_PRIVATE_TOKENS_PATH).toUri().getPath(),\n        ctx.getExecutionAttribute(PID_FILE_PATH).toString(),\n        StringUtils.join(PrivilegedOperation.LINUX_FILE_PATH_SEPARATOR,\n            ctx.getExecutionAttribute(LOCAL_DIRS)),\n        StringUtils.join(PrivilegedOperation.LINUX_FILE_PATH_SEPARATOR,\n            ctx.getExecutionAttribute(LOG_DIRS)),\n        ctx.getExecutionAttribute(RESOURCES_OPTIONS));\n\n    String tcCommandFile = ctx.getExecutionAttribute(TC_COMMAND_FILE);\n\n    if (tcCommandFile != null) {\n      launchOp.appendArgs(tcCommandFile);\n    }\n\n    //List<String> -> stored as List -> fetched/converted to List<String>\n    //we can't do better here thanks to type-erasure\n    @SuppressWarnings(\"unchecked\")\n    List<String> prefixCommands = (List<String>) ctx.getExecutionAttribute(\n        CONTAINER_LAUNCH_PREFIX_COMMANDS);\n\n    try {\n      privilegedOperationExecutor.executePrivilegedOperation(prefixCommands,\n            launchOp, null, container.getLaunchContext().getEnvironment(),\n            false);\n    } catch (PrivilegedOperationException e) {\n      LOG.warn(\"Launch container failed. Exception: \", e);\n\n      throw new ContainerExecutionException(\"Launch container failed\", e\n          .getExitCode(), e.getOutput(), e.getErrorOutput());\n    }\n  }",
                "code_after_change": "  public void launchContainer(ContainerRuntimeContext ctx)\n      throws ContainerExecutionException {\n    Container container = ctx.getContainer();\n    PrivilegedOperation launchOp = new PrivilegedOperation(\n        PrivilegedOperation.OperationType.LAUNCH_CONTAINER);\n\n    //All of these arguments are expected to be available in the runtime context\n    launchOp.appendArgs(ctx.getExecutionAttribute(RUN_AS_USER),\n        ctx.getExecutionAttribute(USER),\n        Integer.toString(PrivilegedOperation.\n            RunAsUserCommand.LAUNCH_CONTAINER.getValue()),\n        ctx.getExecutionAttribute(APPID),\n        ctx.getExecutionAttribute(CONTAINER_ID_STR),\n        ctx.getExecutionAttribute(CONTAINER_WORK_DIR).toString(),\n        ctx.getExecutionAttribute(NM_PRIVATE_CONTAINER_SCRIPT_PATH).toUri()\n            .getPath(),\n        ctx.getExecutionAttribute(NM_PRIVATE_TOKENS_PATH).toUri().getPath(),\n        ctx.getExecutionAttribute(PID_FILE_PATH).toString(),\n        StringUtils.join(PrivilegedOperation.LINUX_FILE_PATH_SEPARATOR,\n            ctx.getExecutionAttribute(LOCAL_DIRS)),\n        StringUtils.join(PrivilegedOperation.LINUX_FILE_PATH_SEPARATOR,\n            ctx.getExecutionAttribute(LOG_DIRS)),\n        ctx.getExecutionAttribute(RESOURCES_OPTIONS));\n\n    String tcCommandFile = ctx.getExecutionAttribute(TC_COMMAND_FILE);\n\n    if (tcCommandFile != null) {\n      launchOp.appendArgs(tcCommandFile);\n    }\n\n    //List<String> -> stored as List -> fetched/converted to List<String>\n    //we can't do better here thanks to type-erasure\n    @SuppressWarnings(\"unchecked\")\n    List<String> prefixCommands = (List<String>) ctx.getExecutionAttribute(\n        CONTAINER_LAUNCH_PREFIX_COMMANDS);\n\n    try {\n      privilegedOperationExecutor.executePrivilegedOperation(prefixCommands,\n            launchOp, null, container.getLaunchContext().getEnvironment(),\n            false);\n    } catch (PrivilegedOperationException e) {\n      LOG.warn(\"Launch container failed. Exception: \", e);\n\n      throw new ContainerExecutionException(\"Launch container failed\", e\n          .getExitCode(), e.getOutput(), e.getErrorOutput());\n    }\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DockerLinuxContainerRuntime.signalContainer": {
                "code_before_change": "  public void signalContainer(ContainerRuntimeContext ctx)\n      throws ContainerExecutionException {\n    Container container = ctx.getContainer();\n    PrivilegedOperation signalOp = new PrivilegedOperation(\n        PrivilegedOperation.OperationType.SIGNAL_CONTAINER, (String) null);\n\n    signalOp.appendArgs(ctx.getExecutionAttribute(RUN_AS_USER),\n        ctx.getExecutionAttribute(USER),\n        Integer.toString(PrivilegedOperation\n            .RunAsUserCommand.SIGNAL_CONTAINER.getValue()),\n        ctx.getExecutionAttribute(PID),\n        Integer.toString(ctx.getExecutionAttribute(SIGNAL).getValue()));\n\n    try {\n      PrivilegedOperationExecutor executor = PrivilegedOperationExecutor\n          .getInstance(conf);\n\n      executor.executePrivilegedOperation(null,\n          signalOp, null, container.getLaunchContext().getEnvironment(),\n          false);\n    } catch (PrivilegedOperationException e) {\n      LOG.warn(\"Signal container failed. Exception: \", e);\n\n      throw new ContainerExecutionException(\"Signal container failed\", e\n          .getExitCode(), e.getOutput(), e.getErrorOutput());\n    }\n  }",
                "code_after_change": "  public void signalContainer(ContainerRuntimeContext ctx)\n      throws ContainerExecutionException {\n    Container container = ctx.getContainer();\n    PrivilegedOperation signalOp = new PrivilegedOperation(\n        PrivilegedOperation.OperationType.SIGNAL_CONTAINER);\n\n    signalOp.appendArgs(ctx.getExecutionAttribute(RUN_AS_USER),\n        ctx.getExecutionAttribute(USER),\n        Integer.toString(PrivilegedOperation\n            .RunAsUserCommand.SIGNAL_CONTAINER.getValue()),\n        ctx.getExecutionAttribute(PID),\n        Integer.toString(ctx.getExecutionAttribute(SIGNAL).getValue()));\n\n    try {\n      PrivilegedOperationExecutor executor = PrivilegedOperationExecutor\n          .getInstance(conf);\n\n      executor.executePrivilegedOperation(null,\n          signalOp, null, container.getLaunchContext().getEnvironment(),\n          false);\n    } catch (PrivilegedOperationException e) {\n      LOG.warn(\"Signal container failed. Exception: \", e);\n\n      throw new ContainerExecutionException(\"Signal container failed\", e\n          .getExitCode(), e.getOutput(), e.getErrorOutput());\n    }\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DockerLinuxContainerRuntime.launchContainer": {
                "code_before_change": "  public void launchContainer(ContainerRuntimeContext ctx)\n      throws ContainerExecutionException {\n    Container container = ctx.getContainer();\n    Map<String, String> environment = container.getLaunchContext()\n        .getEnvironment();\n    String imageName = environment.get(ENV_DOCKER_CONTAINER_IMAGE);\n\n    if (imageName == null) {\n      throw new ContainerExecutionException(ENV_DOCKER_CONTAINER_IMAGE\n          + \" not set!\");\n    }\n\n    String containerIdStr = container.getContainerId().toString();\n    String runAsUser = ctx.getExecutionAttribute(RUN_AS_USER);\n    Path containerWorkDir = ctx.getExecutionAttribute(CONTAINER_WORK_DIR);\n    //List<String> -> stored as List -> fetched/converted to List<String>\n    //we can't do better here thanks to type-erasure\n    @SuppressWarnings(\"unchecked\")\n    List<String> localDirs = ctx.getExecutionAttribute(LOCAL_DIRS);\n    @SuppressWarnings(\"unchecked\")\n    List<String> logDirs = ctx.getExecutionAttribute(LOG_DIRS);\n    @SuppressWarnings(\"unchecked\")\n    List<String> containerLocalDirs = ctx.getExecutionAttribute(\n        CONTAINER_LOCAL_DIRS);\n    @SuppressWarnings(\"unchecked\")\n    List<String> containerLogDirs = ctx.getExecutionAttribute(\n        CONTAINER_LOG_DIRS);\n    Set<String> capabilities = new HashSet<>(Arrays.asList(conf.getStrings(\n        YarnConfiguration.NM_DOCKER_CONTAINER_CAPABILITIES,\n        YarnConfiguration.DEFAULT_NM_DOCKER_CONTAINER_CAPABILITIES)));\n\n    @SuppressWarnings(\"unchecked\")\n    DockerRunCommand runCommand = new DockerRunCommand(containerIdStr,\n        runAsUser, imageName)\n        .detachOnRun()\n        .setContainerWorkDir(containerWorkDir.toString())\n        .setNetworkType(\"host\")\n        .setCapabilities(capabilities)\n        .addMountLocation(\"/etc/passwd\", \"/etc/password:ro\");\n    List<String> allDirs = new ArrayList<>(containerLocalDirs);\n\n    allDirs.add(containerWorkDir.toString());\n    allDirs.addAll(containerLogDirs);\n    for (String dir: allDirs) {\n      runCommand.addMountLocation(dir, dir);\n    }\n\n    if (allowPrivilegedContainerExecution(container)) {\n      runCommand.setPrivileged();\n    }\n\n    String resourcesOpts = ctx.getExecutionAttribute(RESOURCES_OPTIONS);\n\n    addCGroupParentIfRequired(resourcesOpts, containerIdStr, runCommand);\n\n   Path nmPrivateContainerScriptPath = ctx.getExecutionAttribute(\n        NM_PRIVATE_CONTAINER_SCRIPT_PATH);\n\n    String disableOverride = environment.get(\n        ENV_DOCKER_CONTAINER_RUN_OVERRIDE_DISABLE);\n\n    if (disableOverride != null && disableOverride.equals(\"true\")) {\n      if (LOG.isInfoEnabled()) {\n        LOG.info(\"command override disabled\");\n      }\n    } else {\n      List<String> overrideCommands = new ArrayList<>();\n      Path launchDst =\n          new Path(containerWorkDir, ContainerLaunch.CONTAINER_SCRIPT);\n\n      overrideCommands.add(\"bash\");\n      overrideCommands.add(launchDst.toUri().getPath());\n      runCommand.setOverrideCommandWithArgs(overrideCommands);\n    }\n\n    String commandFile = dockerClient.writeCommandToTempFile(runCommand,\n        containerIdStr);\n    PrivilegedOperation launchOp = new PrivilegedOperation(\n        PrivilegedOperation.OperationType.LAUNCH_DOCKER_CONTAINER, (String)\n        null);\n\n    launchOp.appendArgs(runAsUser, ctx.getExecutionAttribute(USER),\n        Integer.toString(PrivilegedOperation\n            .RunAsUserCommand.LAUNCH_DOCKER_CONTAINER.getValue()),\n        ctx.getExecutionAttribute(APPID),\n        containerIdStr, containerWorkDir.toString(),\n        nmPrivateContainerScriptPath.toUri().getPath(),\n        ctx.getExecutionAttribute(NM_PRIVATE_TOKENS_PATH).toUri().getPath(),\n        ctx.getExecutionAttribute(PID_FILE_PATH).toString(),\n        StringUtils.join(PrivilegedOperation.LINUX_FILE_PATH_SEPARATOR,\n            localDirs),\n        StringUtils.join(PrivilegedOperation.LINUX_FILE_PATH_SEPARATOR,\n            logDirs),\n        commandFile,\n        resourcesOpts);\n\n    String tcCommandFile = ctx.getExecutionAttribute(TC_COMMAND_FILE);\n\n    if (tcCommandFile != null) {\n      launchOp.appendArgs(tcCommandFile);\n    }\n\n    try {\n      privilegedOperationExecutor.executePrivilegedOperation(null,\n          launchOp, null, container.getLaunchContext().getEnvironment(),\n          false);\n    } catch (PrivilegedOperationException e) {\n      LOG.warn(\"Launch container failed. Exception: \", e);\n\n      throw new ContainerExecutionException(\"Launch container failed\", e\n          .getExitCode(), e.getOutput(), e.getErrorOutput());\n    }\n  }",
                "code_after_change": "  public void launchContainer(ContainerRuntimeContext ctx)\n      throws ContainerExecutionException {\n    Container container = ctx.getContainer();\n    Map<String, String> environment = container.getLaunchContext()\n        .getEnvironment();\n    String imageName = environment.get(ENV_DOCKER_CONTAINER_IMAGE);\n\n    if (imageName == null) {\n      throw new ContainerExecutionException(ENV_DOCKER_CONTAINER_IMAGE\n          + \" not set!\");\n    }\n\n    String containerIdStr = container.getContainerId().toString();\n    String runAsUser = ctx.getExecutionAttribute(RUN_AS_USER);\n    Path containerWorkDir = ctx.getExecutionAttribute(CONTAINER_WORK_DIR);\n    //List<String> -> stored as List -> fetched/converted to List<String>\n    //we can't do better here thanks to type-erasure\n    @SuppressWarnings(\"unchecked\")\n    List<String> localDirs = ctx.getExecutionAttribute(LOCAL_DIRS);\n    @SuppressWarnings(\"unchecked\")\n    List<String> logDirs = ctx.getExecutionAttribute(LOG_DIRS);\n    @SuppressWarnings(\"unchecked\")\n    List<String> containerLocalDirs = ctx.getExecutionAttribute(\n        CONTAINER_LOCAL_DIRS);\n    @SuppressWarnings(\"unchecked\")\n    List<String> containerLogDirs = ctx.getExecutionAttribute(\n        CONTAINER_LOG_DIRS);\n    Set<String> capabilities = new HashSet<>(Arrays.asList(conf.getStrings(\n        YarnConfiguration.NM_DOCKER_CONTAINER_CAPABILITIES,\n        YarnConfiguration.DEFAULT_NM_DOCKER_CONTAINER_CAPABILITIES)));\n\n    @SuppressWarnings(\"unchecked\")\n    DockerRunCommand runCommand = new DockerRunCommand(containerIdStr,\n        runAsUser, imageName)\n        .detachOnRun()\n        .setContainerWorkDir(containerWorkDir.toString())\n        .setNetworkType(\"host\")\n        .setCapabilities(capabilities)\n        .addMountLocation(\"/etc/passwd\", \"/etc/password:ro\");\n    List<String> allDirs = new ArrayList<>(containerLocalDirs);\n\n    allDirs.add(containerWorkDir.toString());\n    allDirs.addAll(containerLogDirs);\n    for (String dir: allDirs) {\n      runCommand.addMountLocation(dir, dir);\n    }\n\n    if (allowPrivilegedContainerExecution(container)) {\n      runCommand.setPrivileged();\n    }\n\n    String resourcesOpts = ctx.getExecutionAttribute(RESOURCES_OPTIONS);\n\n    addCGroupParentIfRequired(resourcesOpts, containerIdStr, runCommand);\n\n   Path nmPrivateContainerScriptPath = ctx.getExecutionAttribute(\n        NM_PRIVATE_CONTAINER_SCRIPT_PATH);\n\n    String disableOverride = environment.get(\n        ENV_DOCKER_CONTAINER_RUN_OVERRIDE_DISABLE);\n\n    if (disableOverride != null && disableOverride.equals(\"true\")) {\n      if (LOG.isInfoEnabled()) {\n        LOG.info(\"command override disabled\");\n      }\n    } else {\n      List<String> overrideCommands = new ArrayList<>();\n      Path launchDst =\n          new Path(containerWorkDir, ContainerLaunch.CONTAINER_SCRIPT);\n\n      overrideCommands.add(\"bash\");\n      overrideCommands.add(launchDst.toUri().getPath());\n      runCommand.setOverrideCommandWithArgs(overrideCommands);\n    }\n\n    String commandFile = dockerClient.writeCommandToTempFile(runCommand,\n        containerIdStr);\n    PrivilegedOperation launchOp = new PrivilegedOperation(\n        PrivilegedOperation.OperationType.LAUNCH_DOCKER_CONTAINER);\n\n    launchOp.appendArgs(runAsUser, ctx.getExecutionAttribute(USER),\n        Integer.toString(PrivilegedOperation\n            .RunAsUserCommand.LAUNCH_DOCKER_CONTAINER.getValue()),\n        ctx.getExecutionAttribute(APPID),\n        containerIdStr, containerWorkDir.toString(),\n        nmPrivateContainerScriptPath.toUri().getPath(),\n        ctx.getExecutionAttribute(NM_PRIVATE_TOKENS_PATH).toUri().getPath(),\n        ctx.getExecutionAttribute(PID_FILE_PATH).toString(),\n        StringUtils.join(PrivilegedOperation.LINUX_FILE_PATH_SEPARATOR,\n            localDirs),\n        StringUtils.join(PrivilegedOperation.LINUX_FILE_PATH_SEPARATOR,\n            logDirs),\n        commandFile,\n        resourcesOpts);\n\n    String tcCommandFile = ctx.getExecutionAttribute(TC_COMMAND_FILE);\n\n    if (tcCommandFile != null) {\n      launchOp.appendArgs(tcCommandFile);\n    }\n\n    try {\n      privilegedOperationExecutor.executePrivilegedOperation(null,\n          launchOp, null, container.getLaunchContext().getEnvironment(),\n          false);\n    } catch (PrivilegedOperationException e) {\n      LOG.warn(\"Launch container failed. Exception: \", e);\n\n      throw new ContainerExecutionException(\"Launch container failed\", e\n          .getExitCode(), e.getOutput(), e.getErrorOutput());\n    }\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.startLocalizer": {
                "code_before_change": "  public void startLocalizer(LocalizerStartContext ctx)\n      throws IOException, InterruptedException {\n    Path nmPrivateContainerTokensPath = ctx.getNmPrivateContainerTokens();\n    InetSocketAddress nmAddr = ctx.getNmAddr();\n    String user = ctx.getUser();\n    String appId = ctx.getAppId();\n    String locId = ctx.getLocId();\n    LocalDirsHandlerService dirsHandler = ctx.getDirsHandler();\n    List<String> localDirs = dirsHandler.getLocalDirs();\n    List<String> logDirs = dirsHandler.getLogDirs();\n\n    verifyUsernamePattern(user);\n    String runAsUser = getRunAsUser(user);\n    PrivilegedOperation initializeContainerOp = new PrivilegedOperation(\n        PrivilegedOperation.OperationType.INITIALIZE_CONTAINER, (String) null);\n    List<String> prefixCommands = new ArrayList<>();\n\n    addSchedPriorityCommand(prefixCommands);\n    initializeContainerOp.appendArgs(\n        runAsUser,\n        user,\n        Integer.toString(\n            PrivilegedOperation.RunAsUserCommand.INITIALIZE_CONTAINER\n                .getValue()),\n        appId,\n        nmPrivateContainerTokensPath.toUri().getPath().toString(),\n        StringUtils.join(PrivilegedOperation.LINUX_FILE_PATH_SEPARATOR,\n            localDirs),\n        StringUtils.join(PrivilegedOperation.LINUX_FILE_PATH_SEPARATOR,\n            logDirs));\n\n    File jvm =                                  // use same jvm as parent\n        new File(new File(System.getProperty(\"java.home\"), \"bin\"), \"java\");\n    initializeContainerOp.appendArgs(jvm.toString());\n    initializeContainerOp.appendArgs(\"-classpath\");\n    initializeContainerOp.appendArgs(System.getProperty(\"java.class.path\"));\n    String javaLibPath = System.getProperty(\"java.library.path\");\n    if (javaLibPath != null) {\n      initializeContainerOp.appendArgs(\"-Djava.library.path=\" + javaLibPath);\n    }\n\n    initializeContainerOp.appendArgs(ContainerLocalizer.getJavaOpts(getConf()));\n\n    List<String> localizerArgs = new ArrayList<>();\n\n    buildMainArgs(localizerArgs, user, appId, locId, nmAddr, localDirs);\n    initializeContainerOp.appendArgs(localizerArgs);\n\n    try {\n      Configuration conf = super.getConf();\n      PrivilegedOperationExecutor privilegedOperationExecutor =\n          PrivilegedOperationExecutor.getInstance(conf);\n\n      privilegedOperationExecutor.executePrivilegedOperation(prefixCommands,\n          initializeContainerOp, null, null, false);\n\n    } catch (PrivilegedOperationException e) {\n      int exitCode = e.getExitCode();\n      LOG.warn(\"Exit code from container \" + locId + \" startLocalizer is : \"\n          + exitCode, e);\n\n      throw new IOException(\"Application \" + appId + \" initialization failed\" +\n          \" (exitCode=\" + exitCode + \") with output: \" + e.getOutput(), e);\n    }\n  }",
                "code_after_change": "  public void startLocalizer(LocalizerStartContext ctx)\n      throws IOException, InterruptedException {\n    Path nmPrivateContainerTokensPath = ctx.getNmPrivateContainerTokens();\n    InetSocketAddress nmAddr = ctx.getNmAddr();\n    String user = ctx.getUser();\n    String appId = ctx.getAppId();\n    String locId = ctx.getLocId();\n    LocalDirsHandlerService dirsHandler = ctx.getDirsHandler();\n    List<String> localDirs = dirsHandler.getLocalDirs();\n    List<String> logDirs = dirsHandler.getLogDirs();\n\n    verifyUsernamePattern(user);\n    String runAsUser = getRunAsUser(user);\n    PrivilegedOperation initializeContainerOp = new PrivilegedOperation(\n        PrivilegedOperation.OperationType.INITIALIZE_CONTAINER);\n    List<String> prefixCommands = new ArrayList<>();\n\n    addSchedPriorityCommand(prefixCommands);\n    initializeContainerOp.appendArgs(\n        runAsUser,\n        user,\n        Integer.toString(\n            PrivilegedOperation.RunAsUserCommand.INITIALIZE_CONTAINER\n                .getValue()),\n        appId,\n        nmPrivateContainerTokensPath.toUri().getPath().toString(),\n        StringUtils.join(PrivilegedOperation.LINUX_FILE_PATH_SEPARATOR,\n            localDirs),\n        StringUtils.join(PrivilegedOperation.LINUX_FILE_PATH_SEPARATOR,\n            logDirs));\n\n    File jvm =                                  // use same jvm as parent\n        new File(new File(System.getProperty(\"java.home\"), \"bin\"), \"java\");\n    initializeContainerOp.appendArgs(jvm.toString());\n    initializeContainerOp.appendArgs(\"-classpath\");\n    initializeContainerOp.appendArgs(System.getProperty(\"java.class.path\"));\n    String javaLibPath = System.getProperty(\"java.library.path\");\n    if (javaLibPath != null) {\n      initializeContainerOp.appendArgs(\"-Djava.library.path=\" + javaLibPath);\n    }\n\n    initializeContainerOp.appendArgs(ContainerLocalizer.getJavaOpts(getConf()));\n\n    List<String> localizerArgs = new ArrayList<>();\n\n    buildMainArgs(localizerArgs, user, appId, locId, nmAddr, localDirs);\n    initializeContainerOp.appendArgs(localizerArgs);\n\n    try {\n      Configuration conf = super.getConf();\n      PrivilegedOperationExecutor privilegedOperationExecutor =\n          PrivilegedOperationExecutor.getInstance(conf);\n\n      privilegedOperationExecutor.executePrivilegedOperation(prefixCommands,\n          initializeContainerOp, null, null, false);\n\n    } catch (PrivilegedOperationException e) {\n      int exitCode = e.getExitCode();\n      LOG.warn(\"Exit code from container \" + locId + \" startLocalizer is : \"\n          + exitCode, e);\n\n      throw new IOException(\"Application \" + appId + \" initialization failed\" +\n          \" (exitCode=\" + exitCode + \") with output: \" + e.getOutput(), e);\n    }\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.init": {
                "code_before_change": "  public void init() throws IOException {\n    Configuration conf = super.getConf();\n\n    // Send command to executor which will just start up,\n    // verify configuration/permissions and exit\n    try {\n      PrivilegedOperation checkSetupOp = new PrivilegedOperation(\n          PrivilegedOperation.OperationType.CHECK_SETUP, (String) null);\n      PrivilegedOperationExecutor privilegedOperationExecutor =\n          PrivilegedOperationExecutor.getInstance(conf);\n\n      privilegedOperationExecutor.executePrivilegedOperation(checkSetupOp,\n          false);\n    } catch (PrivilegedOperationException e) {\n      int exitCode = e.getExitCode();\n      LOG.warn(\"Exit code from container executor initialization is : \"\n          + exitCode, e);\n\n      throw new IOException(\"Linux container executor not configured properly\"\n          + \" (error=\" + exitCode + \")\", e);\n    }\n\n    try {\n      resourceHandlerChain = ResourceHandlerModule\n          .getConfiguredResourceHandlerChain(conf);\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Resource handler chain enabled = \" + (resourceHandlerChain\n            == null));\n      }\n      if (resourceHandlerChain != null) {\n        LOG.debug(\"Bootstrapping resource handler chain\");\n        resourceHandlerChain.bootstrap(conf);\n      }\n    } catch (ResourceHandlerException e) {\n      LOG.error(\"Failed to bootstrap configured resource subsystems! \", e);\n      throw new IOException(\n          \"Failed to bootstrap configured resource subsystems!\");\n    }\n\n    try {\n      if (linuxContainerRuntime == null) {\n        LinuxContainerRuntime runtime = new DelegatingLinuxContainerRuntime();\n\n        runtime.initialize(conf);\n        this.linuxContainerRuntime = runtime;\n      }\n    } catch (ContainerExecutionException e) {\n      throw new IOException(\"Failed to initialize linux container runtime(s)!\");\n    }\n\n    resourcesHandler.init(this);\n  }",
                "code_after_change": "  public void init() throws IOException {\n    Configuration conf = super.getConf();\n\n    // Send command to executor which will just start up,\n    // verify configuration/permissions and exit\n    try {\n      PrivilegedOperation checkSetupOp = new PrivilegedOperation(\n          PrivilegedOperation.OperationType.CHECK_SETUP);\n      PrivilegedOperationExecutor privilegedOperationExecutor =\n          PrivilegedOperationExecutor.getInstance(conf);\n\n      privilegedOperationExecutor.executePrivilegedOperation(checkSetupOp,\n          false);\n    } catch (PrivilegedOperationException e) {\n      int exitCode = e.getExitCode();\n      LOG.warn(\"Exit code from container executor initialization is : \"\n          + exitCode, e);\n\n      throw new IOException(\"Linux container executor not configured properly\"\n          + \" (error=\" + exitCode + \")\", e);\n    }\n\n    try {\n      resourceHandlerChain = ResourceHandlerModule\n          .getConfiguredResourceHandlerChain(conf);\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Resource handler chain enabled = \" + (resourceHandlerChain\n            == null));\n      }\n      if (resourceHandlerChain != null) {\n        LOG.debug(\"Bootstrapping resource handler chain\");\n        resourceHandlerChain.bootstrap(conf);\n      }\n    } catch (ResourceHandlerException e) {\n      LOG.error(\"Failed to bootstrap configured resource subsystems! \", e);\n      throw new IOException(\n          \"Failed to bootstrap configured resource subsystems!\");\n    }\n\n    try {\n      if (linuxContainerRuntime == null) {\n        LinuxContainerRuntime runtime = new DelegatingLinuxContainerRuntime();\n\n        runtime.initialize(conf);\n        this.linuxContainerRuntime = runtime;\n      }\n    } catch (ContainerExecutionException e) {\n      throw new IOException(\"Failed to initialize linux container runtime(s)!\");\n    }\n\n    resourcesHandler.init(this);\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperation.PrivilegedOperation": {
                "code_before_change": [],
                "code_after_change": []
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperation.getOption": {
                "code_before_change": "    public String getOption() {\n      return option;\n    }",
                "code_after_change": "    public String getOption() {\n      return option;\n    }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Correct",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report precisely identifies the root cause by mentioning the 'PrivilegedOperationException' and the methods 'PrivilegedOperationExecutor.executePrivilegedOperation' and 'DefaultLinuxContainerRuntime.signalContainer', which are part of the ground truth methods. The fix suggestion is correct as it aligns with the changes made in the 'executePrivilegedOperation' method, which involves handling failure logging. The problem location identification is precise because the 'problem_location' field lists methods that are part of the ground truth. There is no wrong information in the bug report as all details are relevant and accurate."
        }
    },
    {
        "filename": "YARN-1752.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService.finishApplicationMaster": {
                "code_before_change": "  public FinishApplicationMasterResponse finishApplicationMaster(\n      FinishApplicationMasterRequest request) throws YarnException,\n      IOException {\n\n    ApplicationAttemptId applicationAttemptId = authorizeRequest();\n\n    AllocateResponseLock lock = responseMap.get(applicationAttemptId);\n    if (lock == null) {\n      throwApplicationDoesNotExistInCacheException(applicationAttemptId);\n    }\n\n    // Allow only one thread in AM to do finishApp at a time.\n    synchronized (lock) {\n      \n      this.amLivelinessMonitor.receivedPing(applicationAttemptId);\n\n      RMApp rmApp =\n          rmContext.getRMApps().get(applicationAttemptId.getApplicationId());\n\n      if (rmApp.getApplicationSubmissionContext().getUnmanagedAM()) {\n        // No recovery supported yet for unmanaged AM. Send the unregister event\n        // and (falsely) acknowledge state-store write immediately.\n        rmContext.getDispatcher().getEventHandler().handle(\n          new RMAppAttemptUnregistrationEvent(applicationAttemptId, request\n              .getTrackingUrl(), request.getFinalApplicationStatus(), request\n              .getDiagnostics()));\n        return FinishApplicationMasterResponse.newInstance(true);\n      }\n\n      // Not an unmanaged-AM.\n      if (rmApp.isAppSafeToTerminate()) {\n        return FinishApplicationMasterResponse.newInstance(true);\n      } else {\n        // keep sending the unregister event as RM may crash in the meanwhile.\n        rmContext.getDispatcher().getEventHandler().handle(\n          new RMAppAttemptUnregistrationEvent(applicationAttemptId, request\n              .getTrackingUrl(), request.getFinalApplicationStatus(), request\n              .getDiagnostics()));\n        return FinishApplicationMasterResponse.newInstance(false);\n      }\n    }\n  }",
                "code_after_change": "  public FinishApplicationMasterResponse finishApplicationMaster(\n      FinishApplicationMasterRequest request) throws YarnException,\n      IOException {\n\n    ApplicationAttemptId applicationAttemptId = authorizeRequest();\n\n    AllocateResponseLock lock = responseMap.get(applicationAttemptId);\n    if (lock == null) {\n      throwApplicationDoesNotExistInCacheException(applicationAttemptId);\n    }\n\n    // Allow only one thread in AM to do finishApp at a time.\n    synchronized (lock) {\n      if (!hasApplicationMasterRegistered(applicationAttemptId)) {\n        String message =\n            \"Application Master is trying to unregister before registering for: \"\n                + applicationAttemptId.getApplicationId();\n        LOG.error(message);\n        RMAuditLogger.logFailure(\n            this.rmContext.getRMApps()\n                .get(applicationAttemptId.getApplicationId()).getUser(),\n            AuditConstants.UNREGISTER_AM, \"\", \"ApplicationMasterService\",\n            message, applicationAttemptId.getApplicationId(),\n            applicationAttemptId);\n        throw new InvalidApplicationMasterRequestException(message);\n      }\n      \n      this.amLivelinessMonitor.receivedPing(applicationAttemptId);\n\n      RMApp rmApp =\n          rmContext.getRMApps().get(applicationAttemptId.getApplicationId());\n\n      if (rmApp.getApplicationSubmissionContext().getUnmanagedAM()) {\n        // No recovery supported yet for unmanaged AM. Send the unregister event\n        // and (falsely) acknowledge state-store write immediately.\n        rmContext.getDispatcher().getEventHandler().handle(\n          new RMAppAttemptUnregistrationEvent(applicationAttemptId, request\n              .getTrackingUrl(), request.getFinalApplicationStatus(), request\n              .getDiagnostics()));\n        return FinishApplicationMasterResponse.newInstance(true);\n      }\n\n      // Not an unmanaged-AM.\n      if (rmApp.isAppSafeToTerminate()) {\n        return FinishApplicationMasterResponse.newInstance(true);\n      } else {\n        // keep sending the unregister event as RM may crash in the meanwhile.\n        rmContext.getDispatcher().getEventHandler().handle(\n          new RMAppAttemptUnregistrationEvent(applicationAttemptId, request\n              .getTrackingUrl(), request.getFinalApplicationStatus(), request\n              .getDiagnostics()));\n        return FinishApplicationMasterResponse.newInstance(false);\n      }\n    }\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Preventive",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the root cause as an invalid state transition from LAUNCHED to UNREGISTERED, which is related to the stack trace context but not the exact ground truth method. The fix suggestion involves adding checks in the RMAppAttemptImpl.handle method, which is preventive as it suggests ensuring the application does not attempt to unregister while in the LAUNCHED state. The problem location mentions methods in the stack trace context but not the exact ground truth method. There is no wrong information in the bug report as all details are relevant to the context of the bug."
        }
    },
    {
        "filename": "YARN-6629.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp.accept": {
                "code_before_change": [],
                "code_after_change": []
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.tryCommit": {
                "code_before_change": [],
                "code_after_change": []
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp.apply": {
                "code_before_change": [],
                "code_after_change": []
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Correct",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Buggy Method"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report precisely identifies the root cause as a NullPointerException occurring due to a missing schedulerKey in the schedulerKeyToPlacementSets map, which aligns with the issue in the `FiCaSchedulerApp.apply` method where a check for outstanding asks was added to prevent such a scenario. The suggestion to add a null check for the schedulerKey before accessing the map matches the developer's fix, which includes a sanity check for outstanding asks in the `FiCaSchedulerApp.apply` method. The problem location identification is 'Partial' under 'Buggy Method' for the same reason as the root cause identification. There is no wrong information in the bug report as all the details provided are relevant to the context of the bug."
        }
    },
    {
        "filename": "YARN-3493.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.transition": {
                "code_before_change": "    public void transition(RMAppAttemptImpl appAttempt,\n        RMAppAttemptEvent event) {\n    }",
                "code_after_change": "    public void transition(RMAppAttemptImpl appAttempt,\n        RMAppAttemptEvent event) {\n    }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils.normalizeAndvalidateRequest": {
                "code_before_change": [],
                "code_after_change": "  public static void normalizeAndvalidateRequest(ResourceRequest resReq,\n      Resource maximumResource, String queueName, YarnScheduler scheduler)\n      throws InvalidResourceRequestException {\n    normalizeAndValidateRequest(resReq, maximumResource, queueName, scheduler,\n        false);\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.createAndPopulateNewRMApp": {
                "code_before_change": "  private RMAppImpl createAndPopulateNewRMApp(\n      ApplicationSubmissionContext submissionContext,\n      long submitTime, String user)\n      throws YarnException {\n    ApplicationId applicationId = submissionContext.getApplicationId();\n    ResourceRequest amReq = validateAndCreateResourceRequest(submissionContext);\n    // Create RMApp\n    RMAppImpl application =\n        new RMAppImpl(applicationId, rmContext, this.conf,\n            submissionContext.getApplicationName(), user,\n            submissionContext.getQueue(),\n            submissionContext, this.scheduler, this.masterService,\n            submitTime, submissionContext.getApplicationType(),\n            submissionContext.getApplicationTags(), amReq);\n\n    // Concurrent app submissions with same applicationId will fail here\n    // Concurrent app submissions with different applicationIds will not\n    // influence each other\n    if (rmContext.getRMApps().putIfAbsent(applicationId, application) !=\n        null) {\n      String message = \"Application with id \" + applicationId\n          + \" is already present! Cannot add a duplicate!\";\n      LOG.warn(message);\n      throw RPCUtil.getRemoteException(message);\n    }\n    // Inform the ACLs Manager\n    this.applicationACLsManager.addApplication(applicationId,\n        submissionContext.getAMContainerSpec().getApplicationACLs());\n    String appViewACLs = submissionContext.getAMContainerSpec()\n        .getApplicationACLs().get(ApplicationAccessType.VIEW_APP);\n    rmContext.getSystemMetricsPublisher().appACLsUpdated(\n        application, appViewACLs, System.currentTimeMillis());\n    return application;\n  }",
                "code_after_change": "  private RMAppImpl createAndPopulateNewRMApp(\n      ApplicationSubmissionContext submissionContext, long submitTime,\n      String user, boolean isRecovery) throws YarnException {\n    ApplicationId applicationId = submissionContext.getApplicationId();\n    ResourceRequest amReq =\n        validateAndCreateResourceRequest(submissionContext, isRecovery);\n\n    // Create RMApp\n    RMAppImpl application =\n        new RMAppImpl(applicationId, rmContext, this.conf,\n            submissionContext.getApplicationName(), user,\n            submissionContext.getQueue(),\n            submissionContext, this.scheduler, this.masterService,\n            submitTime, submissionContext.getApplicationType(),\n            submissionContext.getApplicationTags(), amReq);\n\n    // Concurrent app submissions with same applicationId will fail here\n    // Concurrent app submissions with different applicationIds will not\n    // influence each other\n    if (rmContext.getRMApps().putIfAbsent(applicationId, application) !=\n        null) {\n      String message = \"Application with id \" + applicationId\n          + \" is already present! Cannot add a duplicate!\";\n      LOG.warn(message);\n      throw new YarnException(message);\n    }\n    // Inform the ACLs Manager\n    this.applicationACLsManager.addApplication(applicationId,\n        submissionContext.getAMContainerSpec().getApplicationACLs());\n    String appViewACLs = submissionContext.getAMContainerSpec()\n        .getApplicationACLs().get(ApplicationAccessType.VIEW_APP);\n    rmContext.getSystemMetricsPublisher().appACLsUpdated(\n        application, appViewACLs, System.currentTimeMillis());\n    return application;\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService.allocate": {
                "code_before_change": "  public AllocateResponse allocate(AllocateRequest request)\n      throws YarnException, IOException {\n\n    AMRMTokenIdentifier amrmTokenIdentifier = authorizeRequest();\n\n    ApplicationAttemptId appAttemptId =\n        amrmTokenIdentifier.getApplicationAttemptId();\n    ApplicationId applicationId = appAttemptId.getApplicationId();\n\n    this.amLivelinessMonitor.receivedPing(appAttemptId);\n\n    /* check if its in cache */\n    AllocateResponseLock lock = responseMap.get(appAttemptId);\n    if (lock == null) {\n      String message =\n          \"Application attempt \" + appAttemptId\n              + \" doesn't exist in ApplicationMasterService cache.\";\n      LOG.error(message);\n      throw new ApplicationAttemptNotFoundException(message);\n    }\n    synchronized (lock) {\n      AllocateResponse lastResponse = lock.getAllocateResponse();\n      if (!hasApplicationMasterRegistered(appAttemptId)) {\n        String message =\n            \"AM is not registered for known application attempt: \" + appAttemptId\n                + \" or RM had restarted after AM registered . AM should re-register.\";\n        LOG.info(message);\n        RMAuditLogger.logFailure(\n          this.rmContext.getRMApps().get(appAttemptId.getApplicationId())\n            .getUser(), AuditConstants.AM_ALLOCATE, \"\",\n          \"ApplicationMasterService\", message, applicationId, appAttemptId);\n        throw new ApplicationMasterNotRegisteredException(message);\n      }\n\n      if ((request.getResponseId() + 1) == lastResponse.getResponseId()) {\n        /* old heartbeat */\n        return lastResponse;\n      } else if (request.getResponseId() + 1 < lastResponse.getResponseId()) {\n        String message =\n            \"Invalid responseId in AllocateRequest from application attempt: \"\n                + appAttemptId + \", expect responseId to be \"\n                + (lastResponse.getResponseId() + 1);\n        throw new InvalidApplicationMasterRequestException(message);\n      }\n\n      //filter illegal progress values\n      float filteredProgress = request.getProgress();\n      if (Float.isNaN(filteredProgress) || filteredProgress == Float.NEGATIVE_INFINITY\n        || filteredProgress < 0) {\n         request.setProgress(0);\n      } else if (filteredProgress > 1 || filteredProgress == Float.POSITIVE_INFINITY) {\n        request.setProgress(1);\n      }\n\n      // Send the status update to the appAttempt.\n      this.rmContext.getDispatcher().getEventHandler().handle(\n          new RMAppAttemptStatusupdateEvent(appAttemptId, request\n              .getProgress()));\n\n      List<ResourceRequest> ask = request.getAskList();\n      List<ContainerId> release = request.getReleaseList();\n\n      ResourceBlacklistRequest blacklistRequest =\n          request.getResourceBlacklistRequest();\n      List<String> blacklistAdditions =\n          (blacklistRequest != null) ?\n              blacklistRequest.getBlacklistAdditions() : Collections.EMPTY_LIST;\n      List<String> blacklistRemovals =\n          (blacklistRequest != null) ?\n              blacklistRequest.getBlacklistRemovals() : Collections.EMPTY_LIST;\n      RMApp app =\n          this.rmContext.getRMApps().get(applicationId);\n      \n      // set label expression for Resource Requests if resourceName=ANY \n      ApplicationSubmissionContext asc = app.getApplicationSubmissionContext();\n      for (ResourceRequest req : ask) {\n        if (null == req.getNodeLabelExpression()\n            && ResourceRequest.ANY.equals(req.getResourceName())) {\n          req.setNodeLabelExpression(asc.getNodeLabelExpression());\n        }\n      }\n              \n      // sanity check\n      try {\n        RMServerUtils.validateResourceRequests(ask,\n            rScheduler.getMaximumResourceCapability(), app.getQueue(),\n            rScheduler);\n      } catch (InvalidResourceRequestException e) {\n        LOG.warn(\"Invalid resource ask by application \" + appAttemptId, e);\n        throw e;\n      }\n      \n      try {\n        RMServerUtils.validateBlacklistRequest(blacklistRequest);\n      }  catch (InvalidResourceBlacklistRequestException e) {\n        LOG.warn(\"Invalid blacklist request by application \" + appAttemptId, e);\n        throw e;\n      }\n\n      // In the case of work-preserving AM restart, it's possible for the\n      // AM to release containers from the earlier attempt.\n      if (!app.getApplicationSubmissionContext()\n        .getKeepContainersAcrossApplicationAttempts()) {\n        try {\n          RMServerUtils.validateContainerReleaseRequest(release, appAttemptId);\n        } catch (InvalidContainerReleaseException e) {\n          LOG.warn(\"Invalid container release by application \" + appAttemptId, e);\n          throw e;\n        }\n      }\n\n      // Send new requests to appAttempt.\n      Allocation allocation =\n          this.rScheduler.allocate(appAttemptId, ask, release, \n              blacklistAdditions, blacklistRemovals);\n\n      if (!blacklistAdditions.isEmpty() || !blacklistRemovals.isEmpty()) {\n        LOG.info(\"blacklist are updated in Scheduler.\" +\n            \"blacklistAdditions: \" + blacklistAdditions + \", \" +\n            \"blacklistRemovals: \" + blacklistRemovals);\n      }\n      RMAppAttempt appAttempt = app.getRMAppAttempt(appAttemptId);\n      AllocateResponse allocateResponse =\n          recordFactory.newRecordInstance(AllocateResponse.class);\n      if (!allocation.getContainers().isEmpty()) {\n        allocateResponse.setNMTokens(allocation.getNMTokens());\n      }\n\n      // update the response with the deltas of node status changes\n      List<RMNode> updatedNodes = new ArrayList<RMNode>();\n      if(app.pullRMNodeUpdates(updatedNodes) > 0) {\n        List<NodeReport> updatedNodeReports = new ArrayList<NodeReport>();\n        for(RMNode rmNode: updatedNodes) {\n          SchedulerNodeReport schedulerNodeReport =  \n              rScheduler.getNodeReport(rmNode.getNodeID());\n          Resource used = BuilderUtils.newResource(0, 0);\n          int numContainers = 0;\n          if (schedulerNodeReport != null) {\n            used = schedulerNodeReport.getUsedResource();\n            numContainers = schedulerNodeReport.getNumContainers();\n          }\n          NodeId nodeId = rmNode.getNodeID();\n          NodeReport report =\n              BuilderUtils.newNodeReport(nodeId, rmNode.getState(),\n                  rmNode.getHttpAddress(), rmNode.getRackName(), used,\n                  rmNode.getTotalCapability(), numContainers,\n                  rmNode.getHealthReport(), rmNode.getLastHealthReportTime(),\n                  rmNode.getNodeLabels());\n\n          updatedNodeReports.add(report);\n        }\n        allocateResponse.setUpdatedNodes(updatedNodeReports);\n      }\n\n      allocateResponse.setAllocatedContainers(allocation.getContainers());\n      allocateResponse.setCompletedContainersStatuses(appAttempt\n          .pullJustFinishedContainers());\n      allocateResponse.setResponseId(lastResponse.getResponseId() + 1);\n      allocateResponse.setAvailableResources(allocation.getResourceLimit());\n\n      allocateResponse.setNumClusterNodes(this.rScheduler.getNumClusterNodes());\n\n      // add preemption to the allocateResponse message (if any)\n      allocateResponse\n          .setPreemptionMessage(generatePreemptionMessage(allocation));\n\n      // update AMRMToken if the token is rolled-up\n      MasterKeyData nextMasterKey =\n          this.rmContext.getAMRMTokenSecretManager().getNextMasterKeyData();\n\n      if (nextMasterKey != null\n          && nextMasterKey.getMasterKey().getKeyId() != amrmTokenIdentifier\n            .getKeyId()) {\n        RMAppAttemptImpl appAttemptImpl = (RMAppAttemptImpl)appAttempt;\n        Token<AMRMTokenIdentifier> amrmToken = appAttempt.getAMRMToken();\n        if (nextMasterKey.getMasterKey().getKeyId() !=\n            appAttemptImpl.getAMRMTokenKeyId()) {\n          LOG.info(\"The AMRMToken has been rolled-over. Send new AMRMToken back\"\n              + \" to application: \" + applicationId);\n          amrmToken = rmContext.getAMRMTokenSecretManager()\n              .createAndGetAMRMToken(appAttemptId);\n          appAttemptImpl.setAMRMToken(amrmToken);\n        }\n        allocateResponse.setAMRMToken(org.apache.hadoop.yarn.api.records.Token\n          .newInstance(amrmToken.getIdentifier(), amrmToken.getKind()\n            .toString(), amrmToken.getPassword(), amrmToken.getService()\n            .toString()));\n      }\n\n      /*\n       * As we are updating the response inside the lock object so we don't\n       * need to worry about unregister call occurring in between (which\n       * removes the lock object).\n       */\n      lock.setAllocateResponse(allocateResponse);\n      return allocateResponse;\n    }    \n  }",
                "code_after_change": "  public AllocateResponse allocate(AllocateRequest request)\n      throws YarnException, IOException {\n\n    AMRMTokenIdentifier amrmTokenIdentifier = authorizeRequest();\n\n    ApplicationAttemptId appAttemptId =\n        amrmTokenIdentifier.getApplicationAttemptId();\n    ApplicationId applicationId = appAttemptId.getApplicationId();\n\n    this.amLivelinessMonitor.receivedPing(appAttemptId);\n\n    /* check if its in cache */\n    AllocateResponseLock lock = responseMap.get(appAttemptId);\n    if (lock == null) {\n      String message =\n          \"Application attempt \" + appAttemptId\n              + \" doesn't exist in ApplicationMasterService cache.\";\n      LOG.error(message);\n      throw new ApplicationAttemptNotFoundException(message);\n    }\n    synchronized (lock) {\n      AllocateResponse lastResponse = lock.getAllocateResponse();\n      if (!hasApplicationMasterRegistered(appAttemptId)) {\n        String message =\n            \"AM is not registered for known application attempt: \" + appAttemptId\n                + \" or RM had restarted after AM registered . AM should re-register.\";\n        LOG.info(message);\n        RMAuditLogger.logFailure(\n          this.rmContext.getRMApps().get(appAttemptId.getApplicationId())\n            .getUser(), AuditConstants.AM_ALLOCATE, \"\",\n          \"ApplicationMasterService\", message, applicationId, appAttemptId);\n        throw new ApplicationMasterNotRegisteredException(message);\n      }\n\n      if ((request.getResponseId() + 1) == lastResponse.getResponseId()) {\n        /* old heartbeat */\n        return lastResponse;\n      } else if (request.getResponseId() + 1 < lastResponse.getResponseId()) {\n        String message =\n            \"Invalid responseId in AllocateRequest from application attempt: \"\n                + appAttemptId + \", expect responseId to be \"\n                + (lastResponse.getResponseId() + 1);\n        throw new InvalidApplicationMasterRequestException(message);\n      }\n\n      //filter illegal progress values\n      float filteredProgress = request.getProgress();\n      if (Float.isNaN(filteredProgress) || filteredProgress == Float.NEGATIVE_INFINITY\n        || filteredProgress < 0) {\n         request.setProgress(0);\n      } else if (filteredProgress > 1 || filteredProgress == Float.POSITIVE_INFINITY) {\n        request.setProgress(1);\n      }\n\n      // Send the status update to the appAttempt.\n      this.rmContext.getDispatcher().getEventHandler().handle(\n          new RMAppAttemptStatusupdateEvent(appAttemptId, request\n              .getProgress()));\n\n      List<ResourceRequest> ask = request.getAskList();\n      List<ContainerId> release = request.getReleaseList();\n\n      ResourceBlacklistRequest blacklistRequest =\n          request.getResourceBlacklistRequest();\n      List<String> blacklistAdditions =\n          (blacklistRequest != null) ?\n              blacklistRequest.getBlacklistAdditions() : Collections.EMPTY_LIST;\n      List<String> blacklistRemovals =\n          (blacklistRequest != null) ?\n              blacklistRequest.getBlacklistRemovals() : Collections.EMPTY_LIST;\n      RMApp app =\n          this.rmContext.getRMApps().get(applicationId);\n      \n      // set label expression for Resource Requests\n      ApplicationSubmissionContext asc = app.getApplicationSubmissionContext();\n      for (ResourceRequest req : ask) {\n        if (null == req.getNodeLabelExpression()) {\n          req.setNodeLabelExpression(asc.getNodeLabelExpression());\n        }\n      }\n              \n      // sanity check\n      try {\n        RMServerUtils.normalizeAndValidateRequests(ask,\n            rScheduler.getMaximumResourceCapability(), app.getQueue(),\n            rScheduler);\n      } catch (InvalidResourceRequestException e) {\n        LOG.warn(\"Invalid resource ask by application \" + appAttemptId, e);\n        throw e;\n      }\n      \n      try {\n        RMServerUtils.validateBlacklistRequest(blacklistRequest);\n      }  catch (InvalidResourceBlacklistRequestException e) {\n        LOG.warn(\"Invalid blacklist request by application \" + appAttemptId, e);\n        throw e;\n      }\n\n      // In the case of work-preserving AM restart, it's possible for the\n      // AM to release containers from the earlier attempt.\n      if (!app.getApplicationSubmissionContext()\n        .getKeepContainersAcrossApplicationAttempts()) {\n        try {\n          RMServerUtils.validateContainerReleaseRequest(release, appAttemptId);\n        } catch (InvalidContainerReleaseException e) {\n          LOG.warn(\"Invalid container release by application \" + appAttemptId, e);\n          throw e;\n        }\n      }\n\n      // Send new requests to appAttempt.\n      Allocation allocation =\n          this.rScheduler.allocate(appAttemptId, ask, release, \n              blacklistAdditions, blacklistRemovals);\n\n      if (!blacklistAdditions.isEmpty() || !blacklistRemovals.isEmpty()) {\n        LOG.info(\"blacklist are updated in Scheduler.\" +\n            \"blacklistAdditions: \" + blacklistAdditions + \", \" +\n            \"blacklistRemovals: \" + blacklistRemovals);\n      }\n      RMAppAttempt appAttempt = app.getRMAppAttempt(appAttemptId);\n      AllocateResponse allocateResponse =\n          recordFactory.newRecordInstance(AllocateResponse.class);\n      if (!allocation.getContainers().isEmpty()) {\n        allocateResponse.setNMTokens(allocation.getNMTokens());\n      }\n\n      // update the response with the deltas of node status changes\n      List<RMNode> updatedNodes = new ArrayList<RMNode>();\n      if(app.pullRMNodeUpdates(updatedNodes) > 0) {\n        List<NodeReport> updatedNodeReports = new ArrayList<NodeReport>();\n        for(RMNode rmNode: updatedNodes) {\n          SchedulerNodeReport schedulerNodeReport =  \n              rScheduler.getNodeReport(rmNode.getNodeID());\n          Resource used = BuilderUtils.newResource(0, 0);\n          int numContainers = 0;\n          if (schedulerNodeReport != null) {\n            used = schedulerNodeReport.getUsedResource();\n            numContainers = schedulerNodeReport.getNumContainers();\n          }\n          NodeId nodeId = rmNode.getNodeID();\n          NodeReport report =\n              BuilderUtils.newNodeReport(nodeId, rmNode.getState(),\n                  rmNode.getHttpAddress(), rmNode.getRackName(), used,\n                  rmNode.getTotalCapability(), numContainers,\n                  rmNode.getHealthReport(), rmNode.getLastHealthReportTime(),\n                  rmNode.getNodeLabels());\n\n          updatedNodeReports.add(report);\n        }\n        allocateResponse.setUpdatedNodes(updatedNodeReports);\n      }\n\n      allocateResponse.setAllocatedContainers(allocation.getContainers());\n      allocateResponse.setCompletedContainersStatuses(appAttempt\n          .pullJustFinishedContainers());\n      allocateResponse.setResponseId(lastResponse.getResponseId() + 1);\n      allocateResponse.setAvailableResources(allocation.getResourceLimit());\n\n      allocateResponse.setNumClusterNodes(this.rScheduler.getNumClusterNodes());\n\n      // add preemption to the allocateResponse message (if any)\n      allocateResponse\n          .setPreemptionMessage(generatePreemptionMessage(allocation));\n\n      // update AMRMToken if the token is rolled-up\n      MasterKeyData nextMasterKey =\n          this.rmContext.getAMRMTokenSecretManager().getNextMasterKeyData();\n\n      if (nextMasterKey != null\n          && nextMasterKey.getMasterKey().getKeyId() != amrmTokenIdentifier\n            .getKeyId()) {\n        Token<AMRMTokenIdentifier> amrmToken =\n            rmContext.getAMRMTokenSecretManager().createAndGetAMRMToken(\n              appAttemptId);\n        ((RMAppAttemptImpl)appAttempt).setAMRMToken(amrmToken);\n        allocateResponse.setAMRMToken(org.apache.hadoop.yarn.api.records.Token\n          .newInstance(amrmToken.getIdentifier(), amrmToken.getKind()\n            .toString(), amrmToken.getPassword(), amrmToken.getService()\n            .toString()));\n        LOG.info(\"The AMRMToken has been rolled-over. Send new AMRMToken back\"\n            + \" to application: \" + applicationId);\n      }\n\n      /*\n       * As we are updating the response inside the lock object so we don't\n       * need to worry about unregister call occurring in between (which\n       * removes the lock object).\n       */\n      lock.setAllocateResponse(allocateResponse);\n      return allocateResponse;\n    }    \n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.recoverApplication": {
                "code_before_change": "  protected void recoverApplication(ApplicationStateData appState,\n      RMState rmState) throws Exception {\n    ApplicationSubmissionContext appContext =\n        appState.getApplicationSubmissionContext();\n    ApplicationId appId = appContext.getApplicationId();\n\n    // create and recover app.\n    RMAppImpl application =\n        createAndPopulateNewRMApp(appContext, appState.getSubmitTime(),\n          appState.getUser());\n    application.handle(new RMAppRecoverEvent(appId, rmState));\n  }",
                "code_after_change": "  protected void recoverApplication(ApplicationState appState, RMState rmState)\n      throws Exception {\n    ApplicationSubmissionContext appContext =\n        appState.getApplicationSubmissionContext();\n    ApplicationId appId = appState.getAppId();\n\n    // create and recover app.\n    RMAppImpl application =\n        createAndPopulateNewRMApp(appContext, appState.getSubmitTime(),\n            appState.getUser(), true);\n\n    application.handle(new RMAppRecoverEvent(appId, rmState));\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.validateAndCreateResourceRequest": {
                "code_before_change": "  private ResourceRequest validateAndCreateResourceRequest(\n      ApplicationSubmissionContext submissionContext)\n      throws InvalidResourceRequestException {\n    // Validation of the ApplicationSubmissionContext needs to be completed\n    // here. Only those fields that are dependent on RM's configuration are\n    // checked here as they have to be validated whether they are part of new\n    // submission or just being recovered.\n\n    // Check whether AM resource requirements are within required limits\n    if (!submissionContext.getUnmanagedAM()) {\n      ResourceRequest amReq;\n      if (submissionContext.getAMContainerResourceRequest() != null) {\n        amReq = submissionContext.getAMContainerResourceRequest();\n      } else {\n        amReq =\n            BuilderUtils.newResourceRequest(\n                RMAppAttemptImpl.AM_CONTAINER_PRIORITY, ResourceRequest.ANY,\n                submissionContext.getResource(), 1);\n      }\n      \n      // set label expression for AM container\n      if (null == amReq.getNodeLabelExpression()) {\n        amReq.setNodeLabelExpression(submissionContext\n            .getNodeLabelExpression());\n      }\n\n      try {\n        SchedulerUtils.validateResourceRequest(amReq,\n            scheduler.getMaximumResourceCapability(),\n            submissionContext.getQueue(), scheduler);\n      } catch (InvalidResourceRequestException e) {\n        LOG.warn(\"RM app submission failed in validating AM resource request\"\n            + \" for application \" + submissionContext.getApplicationId(), e);\n        throw e;\n      }\n      SchedulerUtils.normalizeRequest(amReq, scheduler.getResourceCalculator(),\n          scheduler.getClusterResource(),\n          scheduler.getMinimumResourceCapability(),\n          scheduler.getMaximumResourceCapability(),\n          scheduler.getMinimumResourceCapability());\n      return amReq;\n    }\n    \n    return null;\n  }",
                "code_after_change": "  private ResourceRequest validateAndCreateResourceRequest(\n      ApplicationSubmissionContext submissionContext, boolean isRecovery)\n      throws InvalidResourceRequestException {\n    // Validation of the ApplicationSubmissionContext needs to be completed\n    // here. Only those fields that are dependent on RM's configuration are\n    // checked here as they have to be validated whether they are part of new\n    // submission or just being recovered.\n\n    // Check whether AM resource requirements are within required limits\n    if (!submissionContext.getUnmanagedAM()) {\n      ResourceRequest amReq = submissionContext.getAMContainerResourceRequest();\n      if (amReq == null) {\n        amReq = BuilderUtils\n            .newResourceRequest(RMAppAttemptImpl.AM_CONTAINER_PRIORITY,\n                ResourceRequest.ANY, submissionContext.getResource(), 1);\n      }\n\n      // set label expression for AM container\n      if (null == amReq.getNodeLabelExpression()) {\n        amReq.setNodeLabelExpression(submissionContext\n            .getNodeLabelExpression());\n      }\n\n      try {\n        SchedulerUtils.normalizeAndValidateRequest(amReq,\n            scheduler.getMaximumResourceCapability(),\n            submissionContext.getQueue(), scheduler, isRecovery);\n      } catch (InvalidResourceRequestException e) {\n        LOG.warn(\"RM app submission failed in validating AM resource request\"\n            + \" for application \" + submissionContext.getApplicationId(), e);\n        throw e;\n      }\n      return amReq;\n    }\n    \n    return null;\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils.normalizeRequest": {
                "code_before_change": "  public static void normalizeRequest(\n    ResourceRequest ask,\n    ResourceCalculator resourceCalculator,\n    Resource clusterResource,\n    Resource minimumResource,\n    Resource maximumResource) {\n    Resource normalized =\n      Resources.normalize(\n        resourceCalculator, ask.getCapability(), minimumResource,\n        maximumResource, minimumResource);\n    ask.setCapability(normalized);\n  }",
                "code_after_change": "  public static void normalizeRequest(\n    ResourceRequest ask,\n    ResourceCalculator resourceCalculator,\n    Resource clusterResource,\n    Resource minimumResource,\n    Resource maximumResource) {\n    Resource normalized =\n      Resources.normalize(\n        resourceCalculator, ask.getCapability(), minimumResource,\n        maximumResource, minimumResource);\n    ask.setCapability(normalized);\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.RMServerUtils.queryRMNodes": {
                "code_before_change": "  public static List<RMNode> queryRMNodes(RMContext context,\n      EnumSet<NodeState> acceptedStates) {\n    // nodes contains nodes that are NEW, RUNNING OR UNHEALTHY\n    ArrayList<RMNode> results = new ArrayList<RMNode>();\n    if (acceptedStates.contains(NodeState.NEW) ||\n        acceptedStates.contains(NodeState.RUNNING) ||\n        acceptedStates.contains(NodeState.UNHEALTHY)) {\n      for (RMNode rmNode : context.getRMNodes().values()) {\n        if (acceptedStates.contains(rmNode.getState())) {\n          results.add(rmNode);\n        }\n      }\n    }\n\n    // inactiveNodes contains nodes that are DECOMMISSIONED, LOST, OR REBOOTED\n    if (acceptedStates.contains(NodeState.DECOMMISSIONED) ||\n        acceptedStates.contains(NodeState.LOST) ||\n        acceptedStates.contains(NodeState.REBOOTED)) {\n      for (RMNode rmNode : context.getInactiveRMNodes().values()) {\n        if (acceptedStates.contains(rmNode.getState())) {\n          results.add(rmNode);\n        }\n      }\n    }\n    return results;\n  }",
                "code_after_change": "  public static List<RMNode> queryRMNodes(RMContext context,\n      EnumSet<NodeState> acceptedStates) {\n    // nodes contains nodes that are NEW, RUNNING OR UNHEALTHY\n    ArrayList<RMNode> results = new ArrayList<RMNode>();\n    if (acceptedStates.contains(NodeState.NEW) ||\n        acceptedStates.contains(NodeState.RUNNING) ||\n        acceptedStates.contains(NodeState.UNHEALTHY)) {\n      for (RMNode rmNode : context.getRMNodes().values()) {\n        if (acceptedStates.contains(rmNode.getState())) {\n          results.add(rmNode);\n        }\n      }\n    }\n\n    // inactiveNodes contains nodes that are DECOMMISSIONED, LOST, OR REBOOTED\n    if (acceptedStates.contains(NodeState.DECOMMISSIONED) ||\n        acceptedStates.contains(NodeState.LOST) ||\n        acceptedStates.contains(NodeState.REBOOTED)) {\n      for (RMNode rmNode : context.getInactiveRMNodes().values()) {\n        if (acceptedStates.contains(rmNode.getState())) {\n          results.add(rmNode);\n        }\n      }\n    }\n    return results;\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.submitApplication": {
                "code_before_change": "  protected void submitApplication(\n      ApplicationSubmissionContext submissionContext, long submitTime,\n      String user) throws YarnException {\n    ApplicationId applicationId = submissionContext.getApplicationId();\n\n    RMAppImpl application =\n        createAndPopulateNewRMApp(submissionContext, submitTime, user);\n    ApplicationId appId = submissionContext.getApplicationId();\n\n    if (UserGroupInformation.isSecurityEnabled()) {\n      try {\n        this.rmContext.getDelegationTokenRenewer().addApplicationAsync(appId,\n            parseCredentials(submissionContext),\n            submissionContext.getCancelTokensWhenComplete(),\n            application.getUser());\n      } catch (Exception e) {\n        LOG.warn(\"Unable to parse credentials.\", e);\n        // Sending APP_REJECTED is fine, since we assume that the\n        // RMApp is in NEW state and thus we haven't yet informed the\n        // scheduler about the existence of the application\n        assert application.getState() == RMAppState.NEW;\n        this.rmContext.getDispatcher().getEventHandler()\n          .handle(new RMAppRejectedEvent(applicationId, e.getMessage()));\n        throw RPCUtil.getRemoteException(e);\n      }\n    } else {\n      // Dispatcher is not yet started at this time, so these START events\n      // enqueued should be guaranteed to be first processed when dispatcher\n      // gets started.\n      this.rmContext.getDispatcher().getEventHandler()\n        .handle(new RMAppEvent(applicationId, RMAppEventType.START));\n    }\n  }",
                "code_after_change": "  protected void submitApplication(\n      ApplicationSubmissionContext submissionContext, long submitTime,\n      String user) throws YarnException {\n    ApplicationId applicationId = submissionContext.getApplicationId();\n\n    RMAppImpl application =\n        createAndPopulateNewRMApp(submissionContext, submitTime, user, false);\n    ApplicationId appId = submissionContext.getApplicationId();\n\n    if (UserGroupInformation.isSecurityEnabled()) {\n      try {\n        this.rmContext.getDelegationTokenRenewer().addApplicationAsync(appId,\n            parseCredentials(submissionContext),\n            submissionContext.getCancelTokensWhenComplete(),\n            application.getUser());\n      } catch (Exception e) {\n        LOG.warn(\"Unable to parse credentials.\", e);\n        // Sending APP_REJECTED is fine, since we assume that the\n        // RMApp is in NEW state and thus we haven't yet informed the\n        // scheduler about the existence of the application\n        assert application.getState() == RMAppState.NEW;\n        this.rmContext.getDispatcher().getEventHandler()\n          .handle(new RMAppRejectedEvent(applicationId, e.getMessage()));\n        throw RPCUtil.getRemoteException(e);\n      }\n    } else {\n      // Dispatcher is not yet started at this time, so these START events\n      // enqueued should be guaranteed to be first processed when dispatcher\n      // gets started.\n      this.rmContext.getDispatcher().getEventHandler()\n        .handle(new RMAppEvent(applicationId, RMAppEventType.START));\n    }\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils.validateResourceRequest": {
                "code_before_change": "  public static void validateResourceRequest(ResourceRequest resReq,\n      Resource maximumResource, String queueName, YarnScheduler scheduler)\n      throws InvalidResourceRequestException {\n    if (resReq.getCapability().getMemory() < 0 ||\n        resReq.getCapability().getMemory() > maximumResource.getMemory()) {\n      throw new InvalidResourceRequestException(\"Invalid resource request\"\n          + \", requested memory < 0\"\n          + \", or requested memory > max configured\"\n          + \", requestedMemory=\" + resReq.getCapability().getMemory()\n          + \", maxMemory=\" + maximumResource.getMemory());\n    }\n    if (resReq.getCapability().getVirtualCores() < 0 ||\n        resReq.getCapability().getVirtualCores() >\n        maximumResource.getVirtualCores()) {\n      throw new InvalidResourceRequestException(\"Invalid resource request\"\n          + \", requested virtual cores < 0\"\n          + \", or requested virtual cores > max configured\"\n          + \", requestedVirtualCores=\"\n          + resReq.getCapability().getVirtualCores()\n          + \", maxVirtualCores=\" + maximumResource.getVirtualCores());\n    }\n    \n    // Get queue from scheduler\n    QueueInfo queueInfo = null;\n    try {\n      queueInfo = scheduler.getQueueInfo(queueName, false, false);\n    } catch (IOException e) {\n      // it is possible queue cannot get when queue mapping is set, just ignore\n      // the queueInfo here, and move forward\n    }\n\n    // check labels in the resource request.\n    String labelExp = resReq.getNodeLabelExpression();\n    \n    // if queue has default label expression, and RR doesn't have, use the\n    // default label expression of queue\n    if (labelExp == null && queueInfo != null\n        && ResourceRequest.ANY.equals(resReq.getResourceName())) {\n      labelExp = queueInfo.getDefaultNodeLabelExpression();\n    }\n    \n    // If labelExp still equals to null, set it to be NO_LABEL\n    resReq\n        .setNodeLabelExpression(labelExp == null ? RMNodeLabelsManager.NO_LABEL\n            : labelExp);\n    \n    // we don't allow specify label expression other than resourceName=ANY now\n    if (!ResourceRequest.ANY.equals(resReq.getResourceName())\n        && labelExp != null && !labelExp.trim().isEmpty()) {\n      throw new InvalidResourceRequestException(\n          \"Invailid resource request, queue=\" + queueInfo.getQueueName()\n              + \" specified node label expression in a \"\n              + \"resource request has resource name = \"\n              + resReq.getResourceName());\n    }\n    \n    // we don't allow specify label expression with more than one node labels now\n    if (labelExp != null && labelExp.contains(\"&&\")) {\n      throw new InvalidResourceRequestException(\n          \"Invailid resource request, queue=\" + queueInfo.getQueueName()\n              + \" specified more than one node label \"\n              + \"in a node label expression, node label expression = \"\n              + labelExp);\n    }\n    \n    if (labelExp != null && !labelExp.trim().isEmpty() && queueInfo != null) {\n      if (!checkQueueLabelExpression(queueInfo.getAccessibleNodeLabels(),\n          labelExp)) {\n        throw new InvalidResourceRequestException(\"Invalid resource request\"\n            + \", queue=\"\n            + queueInfo.getQueueName()\n            + \" doesn't have permission to access all labels \"\n            + \"in resource request. labelExpression of resource request=\"\n            + labelExp\n            + \". Queue labels=\"\n            + (queueInfo.getAccessibleNodeLabels() == null ? \"\" : StringUtils.join(queueInfo\n                .getAccessibleNodeLabels().iterator(), ',')));\n      }\n    }\n  }",
                "code_after_change": "  public static void validateResourceRequest(ResourceRequest resReq,\n      Resource maximumResource, QueueInfo queueInfo)\n      throws InvalidResourceRequestException {\n    if (resReq.getCapability().getMemory() < 0 ||\n        resReq.getCapability().getMemory() > maximumResource.getMemory()) {\n      throw new InvalidResourceRequestException(\"Invalid resource request\"\n          + \", requested memory < 0\"\n          + \", or requested memory > max configured\"\n          + \", requestedMemory=\" + resReq.getCapability().getMemory()\n          + \", maxMemory=\" + maximumResource.getMemory());\n    }\n    if (resReq.getCapability().getVirtualCores() < 0 ||\n        resReq.getCapability().getVirtualCores() >\n        maximumResource.getVirtualCores()) {\n      throw new InvalidResourceRequestException(\"Invalid resource request\"\n          + \", requested virtual cores < 0\"\n          + \", or requested virtual cores > max configured\"\n          + \", requestedVirtualCores=\"\n          + resReq.getCapability().getVirtualCores()\n          + \", maxVirtualCores=\" + maximumResource.getVirtualCores());\n    }\n    String labelExp = resReq.getNodeLabelExpression();\n    \n    if (labelExp != null && !labelExp.trim().isEmpty() && queueInfo != null) {\n      if (!checkQueueLabelExpression(queueInfo.getAccessibleNodeLabels(),\n          labelExp)) {\n        throw new InvalidResourceRequestException(\"Invalid resource request\"\n            + \", queue=\"\n            + queueInfo.getQueueName()\n            + \" doesn't have permission to access all labels \"\n            + \"in resource request. labelExpression of resource request=\"\n            + labelExp\n            + \". Queue labels=\"\n            + (queueInfo.getAccessibleNodeLabels() == null ? \"\" : StringUtils.join(queueInfo\n                .getAccessibleNodeLabels().iterator(), ',')));\n      }\n    }\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Alternative Fix",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report precisely identifies the root cause as the 'InvalidResourceRequestException' thrown by 'SchedulerUtils.validateResourceRequest', which is a ground truth method. The fix suggestion is an alternative fix because it suggests adjusting memory settings in the configuration file, which is different from the code changes made by the developer but would likely resolve the issue. The problem location identification is precise as it mentions 'SchedulerUtils.validateResourceRequest' and 'RMAppManager.validateAndCreateResourceRequest', both of which are ground truth methods. There is no wrong information in the bug report as all details are relevant and accurate."
        }
    },
    {
        "filename": "YARN-6054.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-applicationhistoryservice.src.main.java.org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore.LeveldbTimelineStore": {
                "code_before_change": [],
                "code_after_change": []
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-applicationhistoryservice.src.main.java.org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore.serviceInit": {
                "code_before_change": "  protected void serviceInit(Configuration conf) throws Exception {\n    Preconditions.checkArgument(conf.getLong(\n        YarnConfiguration.TIMELINE_SERVICE_TTL_MS,\n        YarnConfiguration.DEFAULT_TIMELINE_SERVICE_TTL_MS) > 0,\n        \"%s property value should be greater than zero\",\n        YarnConfiguration.TIMELINE_SERVICE_TTL_MS);\n    Preconditions.checkArgument(conf.getLong(\n        YarnConfiguration.TIMELINE_SERVICE_LEVELDB_TTL_INTERVAL_MS,\n        YarnConfiguration.DEFAULT_TIMELINE_SERVICE_LEVELDB_TTL_INTERVAL_MS) > 0,\n        \"%s property value should be greater than zero\",\n        YarnConfiguration.TIMELINE_SERVICE_LEVELDB_TTL_INTERVAL_MS);\n    Preconditions.checkArgument(conf.getLong(\n        YarnConfiguration.TIMELINE_SERVICE_LEVELDB_READ_CACHE_SIZE,\n        YarnConfiguration.DEFAULT_TIMELINE_SERVICE_LEVELDB_READ_CACHE_SIZE) >= 0,\n        \"%s property value should be greater than or equal to zero\",\n        YarnConfiguration.TIMELINE_SERVICE_LEVELDB_READ_CACHE_SIZE);\n    Preconditions.checkArgument(conf.getLong(\n        YarnConfiguration.TIMELINE_SERVICE_LEVELDB_START_TIME_READ_CACHE_SIZE,\n        YarnConfiguration.DEFAULT_TIMELINE_SERVICE_LEVELDB_START_TIME_READ_CACHE_SIZE) > 0,\n        \" %s property value should be greater than zero\",\n        YarnConfiguration.TIMELINE_SERVICE_LEVELDB_START_TIME_READ_CACHE_SIZE);\n    Preconditions.checkArgument(conf.getLong(\n        YarnConfiguration.TIMELINE_SERVICE_LEVELDB_START_TIME_WRITE_CACHE_SIZE,\n        YarnConfiguration.DEFAULT_TIMELINE_SERVICE_LEVELDB_START_TIME_WRITE_CACHE_SIZE) > 0,\n        \"%s property value should be greater than zero\",\n        YarnConfiguration.TIMELINE_SERVICE_LEVELDB_START_TIME_WRITE_CACHE_SIZE);\n\n    Options options = new Options();\n    options.createIfMissing(true);\n    options.cacheSize(conf.getLong(\n        YarnConfiguration.TIMELINE_SERVICE_LEVELDB_READ_CACHE_SIZE,\n        YarnConfiguration.DEFAULT_TIMELINE_SERVICE_LEVELDB_READ_CACHE_SIZE));\n    JniDBFactory factory = new JniDBFactory();\n    Path dbPath = new Path(\n        conf.get(YarnConfiguration.TIMELINE_SERVICE_LEVELDB_PATH), FILENAME);\n    FileSystem localFS = null;\n    try {\n      localFS = FileSystem.getLocal(conf);\n      if (!localFS.exists(dbPath)) {\n        if (!localFS.mkdirs(dbPath)) {\n          throw new IOException(\"Couldn't create directory for leveldb \" +\n              \"timeline store \" + dbPath);\n        }\n        localFS.setPermission(dbPath, LEVELDB_DIR_UMASK);\n      }\n    } finally {\n      IOUtils.cleanup(LOG, localFS);\n    }\n    LOG.info(\"Using leveldb path \" + dbPath);\n    db = factory.open(new File(dbPath.toString()), options);\n    checkVersion();\n    startTimeWriteCache =\n        Collections.synchronizedMap(new LRUMap(getStartTimeWriteCacheSize(\n            conf)));\n    startTimeReadCache =\n        Collections.synchronizedMap(new LRUMap(getStartTimeReadCacheSize(\n            conf)));\n\n    if (conf.getBoolean(YarnConfiguration.TIMELINE_SERVICE_TTL_ENABLE, true)) {\n      deletionThread = new EntityDeletionThread(conf);\n      deletionThread.start();\n    }\n\n    super.serviceInit(conf);\n  }",
                "code_after_change": "  protected void serviceInit(Configuration conf) throws Exception {\n    Preconditions.checkArgument(conf.getLong(\n        YarnConfiguration.TIMELINE_SERVICE_TTL_MS,\n        YarnConfiguration.DEFAULT_TIMELINE_SERVICE_TTL_MS) > 0,\n        \"%s property value should be greater than zero\",\n        YarnConfiguration.TIMELINE_SERVICE_TTL_MS);\n    Preconditions.checkArgument(conf.getLong(\n        YarnConfiguration.TIMELINE_SERVICE_LEVELDB_TTL_INTERVAL_MS,\n        YarnConfiguration.DEFAULT_TIMELINE_SERVICE_LEVELDB_TTL_INTERVAL_MS) > 0,\n        \"%s property value should be greater than zero\",\n        YarnConfiguration.TIMELINE_SERVICE_LEVELDB_TTL_INTERVAL_MS);\n    Preconditions.checkArgument(conf.getLong(\n        YarnConfiguration.TIMELINE_SERVICE_LEVELDB_READ_CACHE_SIZE,\n        YarnConfiguration.DEFAULT_TIMELINE_SERVICE_LEVELDB_READ_CACHE_SIZE) >= 0,\n        \"%s property value should be greater than or equal to zero\",\n        YarnConfiguration.TIMELINE_SERVICE_LEVELDB_READ_CACHE_SIZE);\n    Preconditions.checkArgument(conf.getLong(\n        YarnConfiguration.TIMELINE_SERVICE_LEVELDB_START_TIME_READ_CACHE_SIZE,\n        YarnConfiguration.DEFAULT_TIMELINE_SERVICE_LEVELDB_START_TIME_READ_CACHE_SIZE) > 0,\n        \" %s property value should be greater than zero\",\n        YarnConfiguration.TIMELINE_SERVICE_LEVELDB_START_TIME_READ_CACHE_SIZE);\n    Preconditions.checkArgument(conf.getLong(\n        YarnConfiguration.TIMELINE_SERVICE_LEVELDB_START_TIME_WRITE_CACHE_SIZE,\n        YarnConfiguration.DEFAULT_TIMELINE_SERVICE_LEVELDB_START_TIME_WRITE_CACHE_SIZE) > 0,\n        \"%s property value should be greater than zero\",\n        YarnConfiguration.TIMELINE_SERVICE_LEVELDB_START_TIME_WRITE_CACHE_SIZE);\n\n    Options options = new Options();\n    options.createIfMissing(true);\n    options.cacheSize(conf.getLong(\n        YarnConfiguration.TIMELINE_SERVICE_LEVELDB_READ_CACHE_SIZE,\n        YarnConfiguration.DEFAULT_TIMELINE_SERVICE_LEVELDB_READ_CACHE_SIZE));\n    if(factory == null) {\n      factory = new JniDBFactory();\n    }\n\n    Path dbPath = new Path(\n        conf.get(YarnConfiguration.TIMELINE_SERVICE_LEVELDB_PATH), FILENAME);\n    FileSystem localFS = null;\n    try {\n      localFS = FileSystem.getLocal(conf);\n      if (!localFS.exists(dbPath)) {\n        if (!localFS.mkdirs(dbPath)) {\n          throw new IOException(\"Couldn't create directory for leveldb \" +\n              \"timeline store \" + dbPath);\n        }\n        localFS.setPermission(dbPath, LEVELDB_DIR_UMASK);\n      }\n    } finally {\n      IOUtils.cleanup(LOG, localFS);\n    }\n    LOG.info(\"Using leveldb path \" + dbPath);\n    try {\n      db = factory.open(new File(dbPath.toString()), options);\n    } catch (IOException ioe) {\n      File dbFile = new File(dbPath.toString());\n      File backupPath = new File(\n          dbPath.toString() + BACKUP_EXT + Time.monotonicNow());\n      LOG.warn(\"Incurred exception while loading LevelDb database. Backing \" +\n          \"up at \"+ backupPath, ioe);\n      FileUtils.copyDirectory(dbFile, backupPath);\n      LOG.warn(\"Going to try repair\");\n      factory.repair(dbFile, options);\n      db = factory.open(dbFile, options);\n    }\n    checkVersion();\n    startTimeWriteCache =\n        Collections.synchronizedMap(new LRUMap(getStartTimeWriteCacheSize(\n            conf)));\n    startTimeReadCache =\n        Collections.synchronizedMap(new LRUMap(getStartTimeReadCacheSize(\n            conf)));\n\n    if (conf.getBoolean(YarnConfiguration.TIMELINE_SERVICE_TTL_ENABLE, true)) {\n      deletionThread = new EntityDeletionThread(conf);\n      deletionThread.start();\n    }\n\n    super.serviceInit(conf);\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Alternative Fix",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report precisely identifies the root cause as the corruption of the LevelDB database due to missing state files, which aligns with the issue in the 'serviceInit' method of 'LeveldbTimelineStore'. The fix suggestion involves implementing a recovery mechanism in the 'serviceInit' method, which is an alternative approach to the developer's fix that involved handling exceptions and repairing the database. The problem location is precisely identified as the 'serviceInit' method in the 'LeveldbTimelineStore' class, which is part of the ground truth methods. There is no wrong information in the bug report as all details are relevant to the context of the bug."
        }
    },
    {
        "filename": "YARN-196.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.getRMClient": {
                "code_before_change": [],
                "code_after_change": "  protected ResourceTracker getRMClient() {\n    Configuration conf = getConfig();\n    YarnRPC rpc = YarnRPC.create(conf);\n    return (ResourceTracker) rpc.getProxy(ResourceTracker.class, rmAddress,\n        conf);\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.start": {
                "code_before_change": [],
                "code_after_change": "  public void start() {\n\n    // NodeManager is the last service to start, so NodeId is available.\n    this.nodeId = this.context.getNodeId();\n\n    InetSocketAddress httpBindAddress = getConfig().getSocketAddr(\n        YarnConfiguration.NM_WEBAPP_ADDRESS,\n        YarnConfiguration.DEFAULT_NM_WEBAPP_ADDRESS,\n        YarnConfiguration.DEFAULT_NM_WEBAPP_PORT);\n    try {\n      this.httpPort = httpBindAddress.getPort();\n      // Registration has to be in start so that ContainerManager can get the\n      // perNM tokens needed to authenticate ContainerTokens.\n      registerWithRM();\n      super.start();\n      startStatusUpdater();\n    } catch (Exception e) {\n      throw new AvroRuntimeException(e);\n    }\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Buggy Method"
            },
            "fix_suggestion": "Alternative Fix",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the root cause as a connection issue between NodeManager and ResourceManager, which is related to the ground truth method 'NodeStatusUpdaterImpl.start', but it does not precisely identify the root cause in 'getRMClient'. Therefore, it is classified as 'Partial' with 'Buggy Method' as the sub-category. The fix suggestion involves implementing a retry mechanism, which is an alternative approach to the developer's fix that involved changes in 'getRMClient'. The problem location identification is 'Precise' as it mentions 'NodeStatusUpdaterImpl.start', which is a ground truth method. There is no wrong information in the bug report as all details are relevant to the context of the bug."
        }
    },
    {
        "filename": "YARN-8508.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.reacquireContainer": {
                "code_before_change": "  public int reacquireContainer(ContainerReacquisitionContext ctx)\n      throws IOException, InterruptedException {\n    ContainerId containerId = ctx.getContainerId();\n\n    try {\n      //Resource handler chain needs to reacquire container state\n      //as well\n      if (resourceHandlerChain != null) {\n        try {\n          resourceHandlerChain.reacquireContainer(containerId);\n        } catch (ResourceHandlerException e) {\n          LOG.warn(\"ResourceHandlerChain.reacquireContainer failed for \" +\n              \"containerId: \" + containerId + \" Exception: \" + e);\n        }\n      }\n\n      return super.reacquireContainer(ctx);\n    } finally {\n      resourcesHandler.postExecute(containerId);\n      if (resourceHandlerChain != null) {\n        try {\n          resourceHandlerChain.postComplete(containerId);\n        } catch (ResourceHandlerException e) {\n          LOG.warn(\"ResourceHandlerChain.postComplete failed for \" +\n              \"containerId: \" + containerId + \" Exception: \" + e);\n        }\n      }\n    }\n  }",
                "code_after_change": "  public int reacquireContainer(ContainerReacquisitionContext ctx)\n      throws IOException, InterruptedException {\n    ContainerId containerId = ctx.getContainerId();\n\n    try {\n      //Resource handler chain needs to reacquire container state\n      //as well\n      if (resourceHandlerChain != null) {\n        try {\n          resourceHandlerChain.reacquireContainer(containerId);\n        } catch (ResourceHandlerException e) {\n          LOG.warn(\"ResourceHandlerChain.reacquireContainer failed for \" +\n              \"containerId: \" + containerId + \" Exception: \" + e);\n        }\n      }\n\n      return super.reacquireContainer(ctx);\n    } finally {\n      resourcesHandler.postExecute(containerId);\n      postComplete(containerId);\n    }\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.reapContainer": {
                "code_before_change": "  public boolean reapContainer(ContainerReapContext ctx) throws IOException {\n    Container container = ctx.getContainer();\n    String user = ctx.getUser();\n    String runAsUser = getRunAsUser(user);\n    ContainerRuntimeContext runtimeContext = new ContainerRuntimeContext\n        .Builder(container)\n        .setExecutionAttribute(RUN_AS_USER, runAsUser)\n        .setExecutionAttribute(USER, user)\n        .build();\n    try {\n      linuxContainerRuntime.reapContainer(runtimeContext);\n    } catch (ContainerExecutionException e) {\n      int retCode = e.getExitCode();\n      if (retCode != 0) {\n        return false;\n      }\n      LOG.warn(\"Error in reaping container \"\n          + container.getContainerId().toString() + \" exit = \" + retCode, e);\n      logOutput(e.getOutput());\n      throw new IOException(\"Error in reaping container \"\n          + container.getContainerId().toString() + \" exit = \" + retCode, e);\n    }\n    return true;\n  }",
                "code_after_change": "  public boolean reapContainer(ContainerReapContext ctx) throws IOException {\n    Container container = ctx.getContainer();\n    String user = ctx.getUser();\n    String runAsUser = getRunAsUser(user);\n    ContainerRuntimeContext runtimeContext = new ContainerRuntimeContext\n        .Builder(container)\n        .setExecutionAttribute(RUN_AS_USER, runAsUser)\n        .setExecutionAttribute(USER, user)\n        .build();\n    try {\n      linuxContainerRuntime.reapContainer(runtimeContext);\n    } catch (ContainerExecutionException e) {\n      int retCode = e.getExitCode();\n      if (retCode != 0) {\n        return false;\n      }\n      LOG.warn(\"Error in reaping container \"\n          + container.getContainerId().toString() + \" exit = \" + retCode, e);\n      logOutput(e.getOutput());\n      throw new IOException(\"Error in reaping container \"\n          + container.getContainerId().toString() + \" exit = \" + retCode, e);\n    } finally {\n      postComplete(container.getContainerId());\n    }\n    return true;\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.removeDockerContainer": {
                "code_before_change": "  public void removeDockerContainer(String containerId) {\n    try {\n      PrivilegedOperationExecutor privOpExecutor =\n          PrivilegedOperationExecutor.getInstance(super.getConf());\n      if (DockerCommandExecutor.isRemovable(\n          DockerCommandExecutor.getContainerStatus(containerId,\n              super.getConf(), privOpExecutor, nmContext))) {\n        LOG.info(\"Removing Docker container : \" + containerId);\n        DockerRmCommand dockerRmCommand = new DockerRmCommand(containerId);\n        DockerCommandExecutor.executeDockerCommand(dockerRmCommand, containerId,\n            null, super.getConf(), privOpExecutor, false, nmContext);\n      }\n    } catch (ContainerExecutionException e) {\n      LOG.warn(\"Unable to remove docker container: \" + containerId);\n    }\n  }",
                "code_after_change": "  public void removeDockerContainer(String containerId) {\n    try {\n      PrivilegedOperationExecutor privOpExecutor =\n          PrivilegedOperationExecutor.getInstance(super.getConf());\n      if (DockerCommandExecutor.isRemovable(\n          DockerCommandExecutor.getContainerStatus(containerId,\n              super.getConf(), privOpExecutor, nmContext))) {\n        LOG.info(\"Removing Docker container : \" + containerId);\n        DockerRmCommand dockerRmCommand = new DockerRmCommand(containerId);\n        DockerCommandExecutor.executeDockerCommand(dockerRmCommand, containerId,\n            null, super.getConf(), privOpExecutor, false, nmContext);\n      }\n    } catch (ContainerExecutionException e) {\n      LOG.warn(\"Unable to remove docker container: \" + containerId);\n    }\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.handleLaunchForLaunchType": {
                "code_before_change": "  private int handleLaunchForLaunchType(ContainerStartContext ctx,\n      ApplicationConstants.ContainerLaunchType type) throws IOException,\n      ConfigurationException {\n    Container container = ctx.getContainer();\n    String user = ctx.getUser();\n\n    verifyUsernamePattern(user);\n\n    ContainerId containerId = container.getContainerId();\n\n    resourcesHandler.preExecute(containerId,\n            container.getResource());\n    String resourcesOptions = resourcesHandler.getResourcesOption(containerId);\n    String tcCommandFile = null;\n    List<String> numaArgs = null;\n\n    try {\n      if (resourceHandlerChain != null) {\n        List<PrivilegedOperation> ops = resourceHandlerChain\n            .preStart(container);\n\n        if (ops != null) {\n          List<PrivilegedOperation> resourceOps = new ArrayList<>();\n\n          resourceOps.add(new PrivilegedOperation(\n              PrivilegedOperation.OperationType.ADD_PID_TO_CGROUP,\n                  resourcesOptions));\n\n          for (PrivilegedOperation op : ops) {\n            switch (op.getOperationType()) {\n            case ADD_PID_TO_CGROUP:\n              resourceOps.add(op);\n              break;\n            case TC_MODIFY_STATE:\n              tcCommandFile = op.getArguments().get(0);\n              break;\n            case ADD_NUMA_PARAMS:\n              numaArgs = op.getArguments();\n              break;\n            default:\n              LOG.warn(\"PrivilegedOperation type unsupported in launch: \"\n                  + op.getOperationType());\n            }\n          }\n\n          if (resourceOps.size() > 1) {\n            //squash resource operations\n            try {\n              PrivilegedOperation operation = PrivilegedOperationExecutor\n                  .squashCGroupOperations(resourceOps);\n              resourcesOptions = operation.getArguments().get(0);\n            } catch (PrivilegedOperationException e) {\n              LOG.error(\"Failed to squash cgroup operations!\", e);\n              throw new ResourceHandlerException(\n                  \"Failed to squash cgroup operations!\");\n            }\n          }\n        }\n      }\n    } catch (ResourceHandlerException e) {\n      LOG.error(\"ResourceHandlerChain.preStart() failed!\", e);\n      throw new IOException(\"ResourceHandlerChain.preStart() failed!\", e);\n    }\n\n    try {\n      Path pidFilePath = getPidFilePath(containerId);\n      if (pidFilePath != null) {\n\n        ContainerRuntimeContext runtimeContext = buildContainerRuntimeContext(\n            ctx, pidFilePath, resourcesOptions, tcCommandFile, numaArgs);\n\n        if (type.equals(ApplicationConstants.ContainerLaunchType.RELAUNCH)) {\n          linuxContainerRuntime.relaunchContainer(runtimeContext);\n        } else {\n          linuxContainerRuntime.launchContainer(runtimeContext);\n        }\n\n      } else {\n        LOG.info(\n            \"Container was marked as inactive. Returning terminated error\");\n        return ContainerExecutor.ExitCode.TERMINATED.getExitCode();\n      }\n    } catch (ContainerExecutionException e) {\n      return handleExitCode(e, container, containerId);\n    } finally {\n      resourcesHandler.postExecute(containerId);\n\n      try {\n        if (resourceHandlerChain != null) {\n          resourceHandlerChain.postComplete(containerId);\n        }\n      } catch (ResourceHandlerException e) {\n        LOG.warn(\"ResourceHandlerChain.postComplete failed for \" +\n            \"containerId: \" + containerId + \". Exception: \" + e);\n      }\n    }\n\n    return 0;\n  }",
                "code_after_change": "  private int handleLaunchForLaunchType(ContainerStartContext ctx,\n      ApplicationConstants.ContainerLaunchType type) throws IOException,\n      ConfigurationException {\n    Container container = ctx.getContainer();\n    String user = ctx.getUser();\n\n    verifyUsernamePattern(user);\n\n    ContainerId containerId = container.getContainerId();\n\n    resourcesHandler.preExecute(containerId,\n            container.getResource());\n    String resourcesOptions = resourcesHandler.getResourcesOption(containerId);\n    String tcCommandFile = null;\n    List<String> numaArgs = null;\n\n    try {\n      if (resourceHandlerChain != null) {\n        List<PrivilegedOperation> ops = resourceHandlerChain\n            .preStart(container);\n\n        if (ops != null) {\n          List<PrivilegedOperation> resourceOps = new ArrayList<>();\n\n          resourceOps.add(new PrivilegedOperation(\n              PrivilegedOperation.OperationType.ADD_PID_TO_CGROUP,\n                  resourcesOptions));\n\n          for (PrivilegedOperation op : ops) {\n            switch (op.getOperationType()) {\n            case ADD_PID_TO_CGROUP:\n              resourceOps.add(op);\n              break;\n            case TC_MODIFY_STATE:\n              tcCommandFile = op.getArguments().get(0);\n              break;\n            case ADD_NUMA_PARAMS:\n              numaArgs = op.getArguments();\n              break;\n            default:\n              LOG.warn(\"PrivilegedOperation type unsupported in launch: \"\n                  + op.getOperationType());\n            }\n          }\n\n          if (resourceOps.size() > 1) {\n            //squash resource operations\n            try {\n              PrivilegedOperation operation = PrivilegedOperationExecutor\n                  .squashCGroupOperations(resourceOps);\n              resourcesOptions = operation.getArguments().get(0);\n            } catch (PrivilegedOperationException e) {\n              LOG.error(\"Failed to squash cgroup operations!\", e);\n              throw new ResourceHandlerException(\n                  \"Failed to squash cgroup operations!\");\n            }\n          }\n        }\n      }\n    } catch (ResourceHandlerException e) {\n      LOG.error(\"ResourceHandlerChain.preStart() failed!\", e);\n      throw new IOException(\"ResourceHandlerChain.preStart() failed!\", e);\n    }\n\n    try {\n      Path pidFilePath = getPidFilePath(containerId);\n      if (pidFilePath != null) {\n\n        ContainerRuntimeContext runtimeContext = buildContainerRuntimeContext(\n            ctx, pidFilePath, resourcesOptions, tcCommandFile, numaArgs);\n\n        if (type.equals(ApplicationConstants.ContainerLaunchType.RELAUNCH)) {\n          linuxContainerRuntime.relaunchContainer(runtimeContext);\n        } else {\n          linuxContainerRuntime.launchContainer(runtimeContext);\n        }\n\n      } else {\n        LOG.info(\n            \"Container was marked as inactive. Returning terminated error\");\n        return ContainerExecutor.ExitCode.TERMINATED.getExitCode();\n      }\n    } catch (ContainerExecutionException e) {\n      return handleExitCode(e, container, containerId);\n    } finally {\n      resourcesHandler.postExecute(containerId);\n      postComplete(containerId);\n    }\n\n    return 0;\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Preventive",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the root cause as a failure in GPU resource allocation due to a mismatch between requested and available GPUs, which is related to the methods in the stack trace but not the exact ground truth methods. The fix suggestion is preventive as it suggests checking GPU availability and configuration, which could prevent the issue but does not match the developer's fix. The problem location identification is partial because it mentions methods in the stack trace context but not the exact ground truth methods. There is no wrong information in the bug report as all details are relevant to the context of the bug."
        }
    },
    {
        "filename": "YARN-2308.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.addApplication": {
                "code_before_change": "  private synchronized void addApplication(ApplicationId applicationId,\n      String queueName, String user, boolean isAppRecovering) {\n    // santiy checks.\n    CSQueue queue = getQueue(queueName);\n    if (queue == null) {\n      String message = \"Application \" + applicationId + \n      \" submitted by user \" + user + \" to unknown queue: \" + queueName;\n      this.rmContext.getDispatcher().getEventHandler()\n          .handle(new RMAppRejectedEvent(applicationId, message));\n      return;\n    }\n    if (!(queue instanceof LeafQueue)) {\n      String message = \"Application \" + applicationId + \n          \" submitted by user \" + user + \" to non-leaf queue: \" + queueName;\n      this.rmContext.getDispatcher().getEventHandler()\n          .handle(new RMAppRejectedEvent(applicationId, message));\n      return;\n    }\n    // Submit to the queue\n    try {\n      queue.submitApplication(applicationId, user, queueName);\n    } catch (AccessControlException ace) {\n      LOG.info(\"Failed to submit application \" + applicationId + \" to queue \"\n          + queueName + \" from user \" + user, ace);\n      this.rmContext.getDispatcher().getEventHandler()\n          .handle(new RMAppRejectedEvent(applicationId, ace.toString()));\n      return;\n    }\n    SchedulerApplication<FiCaSchedulerApp> application =\n        new SchedulerApplication<FiCaSchedulerApp>(queue, user);\n    applications.put(applicationId, application);\n    LOG.info(\"Accepted application \" + applicationId + \" from user: \" + user\n        + \", in queue: \" + queueName);\n    if (isAppRecovering) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(applicationId + \" is recovering. Skip notifying APP_ACCEPTED\");\n      }\n    } else {\n      rmContext.getDispatcher().getEventHandler()\n        .handle(new RMAppEvent(applicationId, RMAppEventType.APP_ACCEPTED));\n    }\n  }",
                "code_after_change": "  private synchronized void addApplication(ApplicationId applicationId,\n    String queueName, String user, boolean isAppRecovering) {\n\n    if (mappings != null && mappings.size() > 0) {\n      try {\n        String mappedQueue = getMappedQueue(user);\n        if (mappedQueue != null) {\n          // We have a mapping, should we use it?\n          if (queueName.equals(YarnConfiguration.DEFAULT_QUEUE_NAME)\n              || overrideWithQueueMappings) {\n            LOG.info(\"Application \" + applicationId + \" user \" + user\n                + \" mapping [\" + queueName + \"] to [\" + mappedQueue\n                + \"] override \" + overrideWithQueueMappings);\n            queueName = mappedQueue;\n            RMApp rmApp = rmContext.getRMApps().get(applicationId);\n            rmApp.setQueue(queueName);\n          }\n        }\n      } catch (IOException ioex) {\n        String message = \"Failed to submit application \" + applicationId +\n            \" submitted by user \" + user + \" reason: \" + ioex.getMessage();\n        this.rmContext.getDispatcher().getEventHandler()\n            .handle(new RMAppRejectedEvent(applicationId, message));\n        return;\n      }\n    }\n\n    // sanity checks.\n    CSQueue queue = getQueue(queueName);\n    if (queue == null) {\n      //During a restart, this indicates a queue was removed, which is\n      //not presently supported\n      if (isAppRecovering) {\n        //throwing RuntimeException because some other exceptions are caught\n        //(including YarnRuntimeException) and we want this to force an exit\n        String queueErrorMsg = \"Queue named \" + queueName \n           + \" missing during application recovery.\"\n           + \" Queue removal during recovery is not presently supported by the\"\n           + \" capacity scheduler, please restart with all queues configured\"\n           + \" which were present before shutdown/restart.\";\n        LOG.fatal(queueErrorMsg);\n        throw new RuntimeException(queueErrorMsg);\n      }\n      String message = \"Application \" + applicationId + \n      \" submitted by user \" + user + \" to unknown queue: \" + queueName;\n      this.rmContext.getDispatcher().getEventHandler()\n          .handle(new RMAppRejectedEvent(applicationId, message));\n      return;\n    }\n    if (!(queue instanceof LeafQueue)) {\n      String message = \"Application \" + applicationId + \n          \" submitted by user \" + user + \" to non-leaf queue: \" + queueName;\n      this.rmContext.getDispatcher().getEventHandler()\n          .handle(new RMAppRejectedEvent(applicationId, message));\n      return;\n    }\n    // Submit to the queue\n    try {\n      queue.submitApplication(applicationId, user, queueName);\n    } catch (AccessControlException ace) {\n      LOG.info(\"Failed to submit application \" + applicationId + \" to queue \"\n          + queueName + \" from user \" + user, ace);\n      this.rmContext.getDispatcher().getEventHandler()\n          .handle(new RMAppRejectedEvent(applicationId, ace.toString()));\n      return;\n    }\n    // update the metrics\n    queue.getMetrics().submitApp(user);\n    SchedulerApplication<FiCaSchedulerApp> application =\n        new SchedulerApplication<FiCaSchedulerApp>(queue, user);\n    applications.put(applicationId, application);\n    LOG.info(\"Accepted application \" + applicationId + \" from user: \" + user\n        + \", in queue: \" + queueName);\n    if (isAppRecovering) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(applicationId + \" is recovering. Skip notifying APP_ACCEPTED\");\n      }\n    } else {\n      rmContext.getDispatcher().getEventHandler()\n        .handle(new RMAppEvent(applicationId, RMAppEventType.APP_ACCEPTED));\n    }\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Buggy Method"
            },
            "fix_suggestion": "Preventive",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the root cause as being in the `addApplicationAttempt` method, which is not where the actual fix was made, but it is where the error occurred, hence 'Partial' with 'Buggy Method'. The fix suggestion involves adding a null check, which is preventive as it would mitigate the NPE but does not match the actual fix. The problem location is precise as it correctly identifies the `addApplicationAttempt` method in the `problem_location` field. There is no wrong information in the bug report as all details are relevant to the context of the bug."
        }
    },
    {
        "filename": "YARN-933.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.StatusUpdateTransition": {
                "code_before_change": [],
                "code_after_change": []
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Buggy Method"
            },
            "fix_suggestion": "Preventive",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Buggy Method"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the root cause as a `ConnectTimeoutException` leading to an invalid state transition, which is related to the method `RMAppAttemptImpl.handle` where the error occurs, but not where the actual fix was made. Thus, it is classified as 'Partial' under 'Buggy Method'. The fix suggestion involves checking network connectivity and adjusting timeout settings, which are preventive measures to mitigate the issue, hence classified as 'Preventive'. The problem location mentions `RMAppAttemptImpl.handle`, which is where the error occurs, but not where the fix was made, so it is 'Partial' under 'Buggy Method'. There is no wrong information in the bug report as all details are relevant to the context of the bug."
        }
    },
    {
        "filename": "YARN-1374.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.createApplicationMasterService": {
                "code_before_change": "  protected ApplicationMasterService createApplicationMasterService() {\n    return new ApplicationMasterService(this.rmContext, scheduler);\n  }",
                "code_after_change": "  protected ApplicationMasterService createApplicationMasterService() {\n    return new ApplicationMasterService(this.rmContext, scheduler);\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStop": {
                "code_before_change": "    protected void serviceStop() throws Exception {\n      if (webApp != null) {\n        webApp.stop();\n      }\n      if (rmDTSecretManager != null) {\n        rmDTSecretManager.stopThreads();\n      }\n\n      if (amRmTokenSecretManager != null) {\n        amRmTokenSecretManager.stop();\n      }\n      if (containerTokenSecretManager != null) {\n        containerTokenSecretManager.stop();\n      }\n      if(nmTokenSecretManager != null) {\n        nmTokenSecretManager.stop();\n      }\n\n      DefaultMetricsSystem.shutdown();\n\n      if (rmContext != null) {\n        RMStateStore store = rmContext.getStateStore();\n        try {\n          store.close();\n        } catch (Exception e) {\n          LOG.error(\"Error closing store.\", e);\n        }\n      }\n\n      super.serviceStop();\n    }",
                "code_after_change": "    protected void serviceStop() throws Exception {\n      if (webApp != null) {\n        webApp.stop();\n      }\n      if (rmDTSecretManager != null) {\n        rmDTSecretManager.stopThreads();\n      }\n\n      if (amRmTokenSecretManager != null) {\n        amRmTokenSecretManager.stop();\n      }\n      if (containerTokenSecretManager != null) {\n        containerTokenSecretManager.stop();\n      }\n      if(nmTokenSecretManager != null) {\n        nmTokenSecretManager.stop();\n      }\n\n      DefaultMetricsSystem.shutdown();\n\n      if (rmContext != null) {\n        RMStateStore store = rmContext.getStateStore();\n        try {\n          store.close();\n        } catch (Exception e) {\n          LOG.error(\"Error closing store.\", e);\n        }\n      }\n\n      super.serviceStop();\n    }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Preventive",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the root cause as a ConcurrentModificationException in the CompositeService.serviceInit method, which is not the exact root cause but is in the same stack trace context as the ground truth methods. The fix suggestion to synchronize modifications to the serviceList is preventive, as it addresses the concurrency issue but does not match the developer's fix, which involved changes in ResourceManager.serviceStop. The problem location is partially identified as it mentions methods in the same stack trace context as the ground truth methods. There is no wrong information in the bug report as all details are relevant to the context of the bug."
        }
    },
    {
        "filename": "YARN-174.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.NodeManager.stateChanged": {
                "code_before_change": "  public void stateChanged(Service service) {\n    if (NodeStatusUpdaterImpl.class.getName().equals(service.getName())\n        && STATE.STOPPED.equals(service.getServiceState())) {\n\n      boolean hasToReboot = ((NodeStatusUpdaterImpl) service).hasToRebootNode();\n\n      // Shutdown the Nodemanager when the NodeStatusUpdater is stopped.      \n      stop();\n\n      // Reboot the whole node-manager if NodeStatusUpdater got a reboot command\n      // from the RM.\n      if (hasToReboot) {\n        LOG.info(\"Rebooting the node manager.\");\n        NodeManager nodeManager = createNewNodeManager();\n        nodeManager.initAndStartNodeManager(hasToReboot);\n      }\n    }\n  }",
                "code_after_change": "  public void stateChanged(Service service) {\n    if (NodeStatusUpdaterImpl.class.getName().equals(service.getName())\n        && STATE.STOPPED.equals(service.getServiceState())) {\n\n      boolean hasToReboot = ((NodeStatusUpdaterImpl) service).hasToRebootNode();\n\n      // Shutdown the Nodemanager when the NodeStatusUpdater is stopped.      \n      stop();\n\n      // Reboot the whole node-manager if NodeStatusUpdater got a reboot command\n      // from the RM.\n      if (hasToReboot) {\n        LOG.info(\"Rebooting the node manager.\");\n        NodeManager nodeManager = createNewNodeManager();\n        nodeManager.initAndStartNodeManager(this.getConfig(), hasToReboot);\n      }\n    }\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.NodeManager.main": {
                "code_before_change": "  public static void main(String[] args) {\n    Thread.setDefaultUncaughtExceptionHandler(new YarnUncaughtExceptionHandler());\n    StringUtils.startupShutdownMessage(NodeManager.class, args, LOG);\n    NodeManager nodeManager = new NodeManager();\n    nodeManager.initAndStartNodeManager(false);\n  }",
                "code_after_change": "  public static void main(String[] args) {\n    Thread.setDefaultUncaughtExceptionHandler(new YarnUncaughtExceptionHandler());\n    StringUtils.startupShutdownMessage(NodeManager.class, args, LOG);\n    NodeManager nodeManager = new NodeManager();\n    Configuration conf = new YarnConfiguration();\n    nodeManager.initAndStartNodeManager(conf, false);\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.NodeManager.initAndStartNodeManager": {
                "code_before_change": "  private void initAndStartNodeManager(boolean hasToReboot) {\n    try {\n\n      // Remove the old hook if we are rebooting.\n      if (hasToReboot && null != nodeManagerShutdownHook) {\n        ShutdownHookManager.get().removeShutdownHook(nodeManagerShutdownHook);\n      }\n\n      nodeManagerShutdownHook = new CompositeServiceShutdownHook(this);\n      ShutdownHookManager.get().addShutdownHook(nodeManagerShutdownHook,\n                                                SHUTDOWN_HOOK_PRIORITY);\n\n      YarnConfiguration conf = new YarnConfiguration();\n      this.init(conf);\n      this.start();\n    } catch (Throwable t) {\n      LOG.fatal(\"Error starting NodeManager\", t);\n      System.exit(-1);\n    }\n  }",
                "code_after_change": "  private void initAndStartNodeManager(Configuration conf, boolean hasToReboot) {\n    try {\n\n      // Remove the old hook if we are rebooting.\n      if (hasToReboot && null != nodeManagerShutdownHook) {\n        ShutdownHookManager.get().removeShutdownHook(nodeManagerShutdownHook);\n      }\n\n      nodeManagerShutdownHook = new CompositeServiceShutdownHook(this);\n      ShutdownHookManager.get().addShutdownHook(nodeManagerShutdownHook,\n                                                SHUTDOWN_HOOK_PRIORITY);\n\n      this.init(conf);\n      this.start();\n    } catch (Throwable t) {\n      LOG.fatal(\"Error starting NodeManager\", t);\n      System.exit(-1);\n    }\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Preventive",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the issue with the Yarn log directory path, which is related to the error but not the exact root cause in the ground truth methods. The methods mentioned in the report are part of the stack trace, hence 'Shared Stack Trace Context'. The fix suggestion is preventive as it suggests configuration changes to avoid the error, which is different from the code changes made by the developer. The `problem_location` field in the bug report mentions `NodeManager.initAndStartNodeManager`, which is one of the ground truth methods. Therefore, it is rated as 'Precise'. There is no wrong information in the report as all details are relevant to the context of the bug."
        }
    },
    {
        "filename": "YARN-6448.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode.addUnallocatedResource": {
                "code_before_change": "  private synchronized void addUnallocatedResource(Resource resource) {\n    if (resource == null) {\n      LOG.error(\"Invalid resource addition of null resource for \"\n          + rmNode.getNodeAddress());\n      return;\n    }\n    Resources.addTo(unallocatedResource, resource);\n    Resources.subtractFrom(allocatedResource, resource);\n  }",
                "code_after_change": "  private synchronized void addUnallocatedResource(Resource resource) {\n    if (resource == null) {\n      LOG.error(\"Invalid resource addition of null resource for \"\n          + rmNode.getNodeAddress());\n      return;\n    }\n    Resources.addTo(unallocatedResource, resource);\n    Resources.subtractFrom(allocatedResource, resource);\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.nodeUpdate": {
                "code_before_change": "  protected void nodeUpdate(RMNode nm) {\n    try {\n      writeLock.lock();\n      long start = getClock().getTime();\n      eventLog.log(\"HEARTBEAT\", nm.getHostName());\n      super.nodeUpdate(nm);\n\n      FSSchedulerNode fsNode = getFSSchedulerNode(nm.getNodeID());\n      attemptScheduling(fsNode);\n\n      long duration = getClock().getTime() - start;\n      fsOpDurations.addNodeUpdateDuration(duration);\n    } finally {\n      writeLock.unlock();\n    }\n  }",
                "code_after_change": "  protected void nodeUpdate(RMNode nm) {\n    try {\n      writeLock.lock();\n      long start = getClock().getTime();\n      eventLog.log(\"HEARTBEAT\", nm.getHostName());\n      super.nodeUpdate(nm);\n\n      FSSchedulerNode fsNode = getFSSchedulerNode(nm.getNodeID());\n      attemptScheduling(fsNode);\n\n      long duration = getClock().getTime() - start;\n      fsOpDurations.addNodeUpdateDuration(duration);\n    } finally {\n      writeLock.unlock();\n    }\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Preventive",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the root cause as an `IllegalArgumentException` in the `sortedNodeList` method, which is part of the stack trace but not the ground truth method. This places it in the 'Shared Stack Trace Context' sub-category. The fix suggestion is preventive as it suggests ensuring proper initialization of `Resource` objects and reviewing the comparator, which could prevent the issue but does not match the actual fix. The problem location is also partial, as it mentions methods in the stack trace but not the ground truth methods. There is no wrong information in the bug report as all details are relevant to the context of the bug."
        }
    },
    {
        "filename": "YARN-4530.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService.run": {
                "code_before_change": "    public void run() {\n      try {\n        // TODO shutdown, better error handling esp. DU\n        while (!Thread.currentThread().isInterrupted()) {\n          try {\n            Future<Path> completed = queue.take();\n            LocalizerResourceRequestEvent assoc = pending.remove(completed);\n            try {\n              Path local = completed.get();\n              if (null == assoc) {\n                LOG.error(\"Localized unknown resource to \" + completed);\n                // TODO delete\n                return;\n              }\n              LocalResourceRequest key = assoc.getResource().getRequest();\n              publicRsrc.handle(new ResourceLocalizedEvent(key, local, FileUtil\n                .getDU(new File(local.toUri()))));\n              assoc.getResource().unlock();\n            } catch (ExecutionException e) {\n              LOG.info(\"Failed to download resource \" + assoc.getResource(),\n                  e.getCause());\n              LocalResourceRequest req = assoc.getResource().getRequest();\n              publicRsrc.handle(new ResourceFailedLocalizationEvent(req,\n                  e.getMessage()));\n              assoc.getResource().unlock();\n            } catch (CancellationException e) {\n              // ignore; shutting down\n            }\n          } catch (InterruptedException e) {\n            return;\n          }\n        }\n      } catch(Throwable t) {\n        LOG.fatal(\"Error: Shutting down\", t);\n      } finally {\n        LOG.info(\"Public cache exiting\");\n        threadPool.shutdownNow();\n      }\n    }",
                "code_after_change": "    public void run() {\n      try {\n        // TODO shutdown, better error handling esp. DU\n        while (!Thread.currentThread().isInterrupted()) {\n          try {\n            Future<Path> completed = queue.take();\n            LocalizerResourceRequestEvent assoc = pending.remove(completed);\n            try {\n              if (null == assoc) {\n                LOG.error(\"Localized unknown resource to \" + completed);\n                // TODO delete\n                return;\n              }\n              Path local = completed.get();\n              LocalResourceRequest key = assoc.getResource().getRequest();\n              publicRsrc.handle(new ResourceLocalizedEvent(key, local, FileUtil\n                .getDU(new File(local.toUri()))));\n              assoc.getResource().unlock();\n            } catch (ExecutionException e) {\n              LOG.info(\"Failed to download resource \" + assoc.getResource(),\n                  e.getCause());\n              LocalResourceRequest req = assoc.getResource().getRequest();\n              publicRsrc.handle(new ResourceFailedLocalizationEvent(req,\n                  e.getMessage()));\n              assoc.getResource().unlock();\n            } catch (CancellationException e) {\n              // ignore; shutting down\n            }\n          } catch (InterruptedException e) {\n            return;\n          }\n        }\n      } catch(Throwable t) {\n        LOG.fatal(\"Error: Shutting down\", t);\n      } finally {\n        LOG.info(\"Public cache exiting\");\n        threadPool.shutdownNow();\n      }\n    }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Preventive",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report precisely identifies the root cause as a NullPointerException in the ResourceLocalizationService, which is the ground truth method. The fix suggestion is preventive as it suggests ensuring resources are not modified after scheduling and reviewing initialization processes, which would prevent the issue but does not match the exact code change. The problem location identification is precise as it mentions the ResourceLocalizationService.run method, which is the ground truth method. There is no wrong information in the bug report as all details are relevant to the context of the bug."
        }
    },
    {
        "filename": "YARN-7737.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call": {
                "code_before_change": "  public Integer call() {\n    if (!validateContainerState()) {\n      return 0;\n    }\n\n    final ContainerLaunchContext launchContext = container.getLaunchContext();\n    ContainerId containerID = container.getContainerId();\n    String containerIdStr = containerID.toString();\n    final List<String> command = launchContext.getCommands();\n    int ret = -1;\n\n    Path containerLogDir;\n    try {\n      Map<Path, List<String>> localResources = getLocalizedResources();\n\n      final String user = container.getUser();\n      // /////////////////////////// Variable expansion\n      // Before the container script gets written out.\n      List<String> newCmds = new ArrayList<String>(command.size());\n      String appIdStr = app.getAppId().toString();\n      String relativeContainerLogDir = ContainerLaunch\n          .getRelativeContainerLogDir(appIdStr, containerIdStr);\n      containerLogDir =\n          dirsHandler.getLogPathForWrite(relativeContainerLogDir, false);\n      recordContainerLogDir(containerID, containerLogDir.toString());\n      for (String str : command) {\n        // TODO: Should we instead work via symlinks without this grammar?\n        newCmds.add(expandEnvironment(str, containerLogDir));\n      }\n      launchContext.setCommands(newCmds);\n\n      Map<String, String> environment = launchContext.getEnvironment();\n      // Make a copy of env to iterate & do variable expansion\n      for (Entry<String, String> entry : environment.entrySet()) {\n        String value = entry.getValue();\n        value = expandEnvironment(value, containerLogDir);\n        entry.setValue(value);\n      }\n      // /////////////////////////// End of variable expansion\n\n      FileContext lfs = FileContext.getLocalFSFileContext();\n\n      Path nmPrivateContainerScriptPath = dirsHandler.getLocalPathForWrite(\n          getContainerPrivateDir(appIdStr, containerIdStr) + Path.SEPARATOR\n              + CONTAINER_SCRIPT);\n      Path nmPrivateTokensPath = dirsHandler.getLocalPathForWrite(\n          getContainerPrivateDir(appIdStr, containerIdStr) + Path.SEPARATOR\n              + String.format(ContainerLocalizer.TOKEN_FILE_NAME_FMT,\n              containerIdStr));\n      Path nmPrivateClasspathJarDir = dirsHandler.getLocalPathForWrite(\n          getContainerPrivateDir(appIdStr, containerIdStr));\n\n      // Select the working directory for the container\n      Path containerWorkDir = deriveContainerWorkDir();\n      recordContainerWorkDir(containerID, containerWorkDir.toString());\n\n      String pidFileSubpath = getPidFileSubpath(appIdStr, containerIdStr);\n      // pid file should be in nm private dir so that it is not\n      // accessible by users\n      pidFilePath = dirsHandler.getLocalPathForWrite(pidFileSubpath);\n      List<String> localDirs = dirsHandler.getLocalDirs();\n      List<String> logDirs = dirsHandler.getLogDirs();\n      List<String> filecacheDirs = getNMFilecacheDirs(localDirs);\n      List<String> userLocalDirs = getUserLocalDirs(localDirs);\n      List<String> containerLocalDirs = getContainerLocalDirs(localDirs);\n      List<String> containerLogDirs = getContainerLogDirs(logDirs);\n\n      if (!dirsHandler.areDisksHealthy()) {\n        ret = ContainerExitStatus.DISKS_FAILED;\n        throw new IOException(\"Most of the disks failed. \"\n            + dirsHandler.getDisksHealthReport(false));\n      }\n      List<Path> appDirs = new ArrayList<Path>(localDirs.size());\n      for (String localDir : localDirs) {\n        Path usersdir = new Path(localDir, ContainerLocalizer.USERCACHE);\n        Path userdir = new Path(usersdir, user);\n        Path appsdir = new Path(userdir, ContainerLocalizer.APPCACHE);\n        appDirs.add(new Path(appsdir, appIdStr));\n      }\n\n      // Set the token location too.\n      environment.put(\n          ApplicationConstants.CONTAINER_TOKEN_FILE_ENV_NAME,\n          new Path(containerWorkDir,\n              FINAL_CONTAINER_TOKENS_FILE).toUri().getPath());\n\n      // /////////// Write out the container-script in the nmPrivate space.\n      try (DataOutputStream containerScriptOutStream =\n               lfs.create(nmPrivateContainerScriptPath,\n                   EnumSet.of(CREATE, OVERWRITE))) {\n        // Sanitize the container's environment\n        sanitizeEnv(environment, containerWorkDir, appDirs, userLocalDirs,\n            containerLogDirs, localResources, nmPrivateClasspathJarDir);\n\n        prepareContainer(localResources, containerLocalDirs);\n\n        // Write out the environment\n        exec.writeLaunchEnv(containerScriptOutStream, environment,\n            localResources, launchContext.getCommands(),\n            new Path(containerLogDirs.get(0)), user);\n      }\n      // /////////// End of writing out container-script\n\n      // /////////// Write out the container-tokens in the nmPrivate space.\n      try (DataOutputStream tokensOutStream =\n               lfs.create(nmPrivateTokensPath, EnumSet.of(CREATE, OVERWRITE))) {\n        Credentials creds = container.getCredentials();\n        creds.writeTokenStorageToStream(tokensOutStream);\n      }\n      // /////////// End of writing out container-tokens\n\n      ret = launchContainer(new ContainerStartContext.Builder()\n          .setContainer(container)\n          .setLocalizedResources(localResources)\n          .setNmPrivateContainerScriptPath(nmPrivateContainerScriptPath)\n          .setNmPrivateTokensPath(nmPrivateTokensPath)\n          .setUser(user)\n          .setAppId(appIdStr)\n          .setContainerWorkDir(containerWorkDir)\n          .setLocalDirs(localDirs)\n          .setLogDirs(logDirs)\n          .setFilecacheDirs(filecacheDirs)\n          .setUserLocalDirs(userLocalDirs)\n          .setContainerLocalDirs(containerLocalDirs)\n          .setContainerLogDirs(containerLogDirs).build());\n    } catch (ConfigurationException e) {\n      LOG.error(\"Failed to launch container due to configuration error.\", e);\n      dispatcher.getEventHandler().handle(new ContainerExitEvent(\n          containerID, ContainerEventType.CONTAINER_EXITED_WITH_FAILURE, ret,\n          e.getMessage()));\n      // Mark the node as unhealthy\n      context.getNodeStatusUpdater().reportException(e);\n      return ret;\n    } catch (Throwable e) {\n      LOG.warn(\"Failed to launch container.\", e);\n      dispatcher.getEventHandler().handle(new ContainerExitEvent(\n          containerID, ContainerEventType.CONTAINER_EXITED_WITH_FAILURE, ret,\n          e.getMessage()));\n      return ret;\n    } finally {\n      setContainerCompletedStatus(ret);\n    }\n\n    handleContainerExitCode(ret, containerLogDir);\n    return ret;\n  }",
                "code_after_change": "  public Integer call() {\n    if (!validateContainerState()) {\n      return 0;\n    }\n\n    final ContainerLaunchContext launchContext = container.getLaunchContext();\n    ContainerId containerID = container.getContainerId();\n    String containerIdStr = containerID.toString();\n    final List<String> command = launchContext.getCommands();\n    int ret = -1;\n\n    Path containerLogDir;\n    try {\n      Map<Path, List<String>> localResources = getLocalizedResources();\n\n      final String user = container.getUser();\n      // /////////////////////////// Variable expansion\n      // Before the container script gets written out.\n      List<String> newCmds = new ArrayList<String>(command.size());\n      String appIdStr = app.getAppId().toString();\n      String relativeContainerLogDir = ContainerLaunch\n          .getRelativeContainerLogDir(appIdStr, containerIdStr);\n      containerLogDir =\n          dirsHandler.getLogPathForWrite(relativeContainerLogDir, false);\n      recordContainerLogDir(containerID, containerLogDir.toString());\n      for (String str : command) {\n        // TODO: Should we instead work via symlinks without this grammar?\n        newCmds.add(expandEnvironment(str, containerLogDir));\n      }\n      launchContext.setCommands(newCmds);\n\n      Map<String, String> environment = launchContext.getEnvironment();\n      // Make a copy of env to iterate & do variable expansion\n      for (Entry<String, String> entry : environment.entrySet()) {\n        String value = entry.getValue();\n        value = expandEnvironment(value, containerLogDir);\n        entry.setValue(value);\n      }\n      // /////////////////////////// End of variable expansion\n\n      FileContext lfs = FileContext.getLocalFSFileContext();\n\n      Path nmPrivateContainerScriptPath = dirsHandler.getLocalPathForWrite(\n          getContainerPrivateDir(appIdStr, containerIdStr) + Path.SEPARATOR\n              + CONTAINER_SCRIPT);\n      Path nmPrivateTokensPath = dirsHandler.getLocalPathForWrite(\n          getContainerPrivateDir(appIdStr, containerIdStr) + Path.SEPARATOR\n              + String.format(ContainerLocalizer.TOKEN_FILE_NAME_FMT,\n              containerIdStr));\n      Path nmPrivateClasspathJarDir = dirsHandler.getLocalPathForWrite(\n          getContainerPrivateDir(appIdStr, containerIdStr));\n\n      // Select the working directory for the container\n      Path containerWorkDir = deriveContainerWorkDir();\n      recordContainerWorkDir(containerID, containerWorkDir.toString());\n\n      String pidFileSubpath = getPidFileSubpath(appIdStr, containerIdStr);\n      // pid file should be in nm private dir so that it is not\n      // accessible by users\n      pidFilePath = dirsHandler.getLocalPathForWrite(pidFileSubpath);\n      List<String> localDirs = dirsHandler.getLocalDirs();\n      List<String> logDirs = dirsHandler.getLogDirs();\n      List<String> filecacheDirs = getNMFilecacheDirs(localDirs);\n      List<String> userLocalDirs = getUserLocalDirs(localDirs);\n      List<String> containerLocalDirs = getContainerLocalDirs(localDirs);\n      List<String> containerLogDirs = getContainerLogDirs(logDirs);\n\n      if (!dirsHandler.areDisksHealthy()) {\n        ret = ContainerExitStatus.DISKS_FAILED;\n        throw new IOException(\"Most of the disks failed. \"\n            + dirsHandler.getDisksHealthReport(false));\n      }\n      List<Path> appDirs = new ArrayList<Path>(localDirs.size());\n      for (String localDir : localDirs) {\n        Path usersdir = new Path(localDir, ContainerLocalizer.USERCACHE);\n        Path userdir = new Path(usersdir, user);\n        Path appsdir = new Path(userdir, ContainerLocalizer.APPCACHE);\n        appDirs.add(new Path(appsdir, appIdStr));\n      }\n\n      // Set the token location too.\n      environment.put(\n          ApplicationConstants.CONTAINER_TOKEN_FILE_ENV_NAME,\n          new Path(containerWorkDir,\n              FINAL_CONTAINER_TOKENS_FILE).toUri().getPath());\n\n      // /////////// Write out the container-script in the nmPrivate space.\n      try (DataOutputStream containerScriptOutStream =\n               lfs.create(nmPrivateContainerScriptPath,\n                   EnumSet.of(CREATE, OVERWRITE))) {\n        // Sanitize the container's environment\n        sanitizeEnv(environment, containerWorkDir, appDirs, userLocalDirs,\n            containerLogDirs, localResources, nmPrivateClasspathJarDir);\n\n        prepareContainer(localResources, containerLocalDirs);\n\n        // Write out the environment\n        exec.writeLaunchEnv(containerScriptOutStream, environment,\n            localResources, launchContext.getCommands(),\n            containerLogDir, user);\n      }\n      // /////////// End of writing out container-script\n\n      // /////////// Write out the container-tokens in the nmPrivate space.\n      try (DataOutputStream tokensOutStream =\n               lfs.create(nmPrivateTokensPath, EnumSet.of(CREATE, OVERWRITE))) {\n        Credentials creds = container.getCredentials();\n        creds.writeTokenStorageToStream(tokensOutStream);\n      }\n      // /////////// End of writing out container-tokens\n\n      ret = launchContainer(new ContainerStartContext.Builder()\n          .setContainer(container)\n          .setLocalizedResources(localResources)\n          .setNmPrivateContainerScriptPath(nmPrivateContainerScriptPath)\n          .setNmPrivateTokensPath(nmPrivateTokensPath)\n          .setUser(user)\n          .setAppId(appIdStr)\n          .setContainerWorkDir(containerWorkDir)\n          .setLocalDirs(localDirs)\n          .setLogDirs(logDirs)\n          .setFilecacheDirs(filecacheDirs)\n          .setUserLocalDirs(userLocalDirs)\n          .setContainerLocalDirs(containerLocalDirs)\n          .setContainerLogDirs(containerLogDirs).build());\n    } catch (ConfigurationException e) {\n      LOG.error(\"Failed to launch container due to configuration error.\", e);\n      dispatcher.getEventHandler().handle(new ContainerExitEvent(\n          containerID, ContainerEventType.CONTAINER_EXITED_WITH_FAILURE, ret,\n          e.getMessage()));\n      // Mark the node as unhealthy\n      context.getNodeStatusUpdater().reportException(e);\n      return ret;\n    } catch (Throwable e) {\n      LOG.warn(\"Failed to launch container.\", e);\n      dispatcher.getEventHandler().handle(new ContainerExitEvent(\n          containerID, ContainerEventType.CONTAINER_EXITED_WITH_FAILURE, ret,\n          e.getMessage()));\n      return ret;\n    } finally {\n      setContainerCompletedStatus(ret);\n    }\n\n    handleContainerExitCode(ret, containerLogDir);\n    return ret;\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Same Class or Module"
            },
            "fix_suggestion": "Preventive",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the root cause as the missing prelaunch error log file, which is related to the container launch process. However, it does not precisely pinpoint the method 'ContainerLaunch.call' where the fix was made, but it does mention 'ContainerLaunch.handleContainerExitWithFailure' and 'ContainerLaunch.call', which are in the same class as the ground truth method. The fix suggestion involves verifying file paths, process completion, and permissions, which are preventive measures that could mitigate the issue. The `problem_location` field in the bug report correctly identifies `ContainerLaunch.call`, which is part of the ground truth method list. Therefore, it is rated as 'Precise'. There is no wrong information in the bug report as all details are relevant to the context of the bug."
        }
    },
    {
        "filename": "YARN-5136.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.moveApplication": {
                "code_before_change": "  public synchronized String moveApplication(ApplicationId appId,\n      String queueName) throws YarnException {\n    SchedulerApplication<FSAppAttempt> app = applications.get(appId);\n    if (app == null) {\n      throw new YarnException(\"App to be moved \" + appId + \" not found.\");\n    }\n    FSAppAttempt attempt = (FSAppAttempt) app.getCurrentAppAttempt();\n    // To serialize with FairScheduler#allocate, synchronize on app attempt\n    synchronized (attempt) {\n      FSLeafQueue oldQueue = (FSLeafQueue) app.getQueue();\n      String destQueueName = handleMoveToPlanQueue(queueName);\n      FSLeafQueue targetQueue = queueMgr.getLeafQueue(destQueueName, false);\n      if (targetQueue == null) {\n        throw new YarnException(\"Target queue \" + queueName\n            + \" not found or is not a leaf queue.\");\n      }\n      if (targetQueue == oldQueue) {\n        return oldQueue.getQueueName();\n      }\n      \n      if (oldQueue.isRunnableApp(attempt)) {\n        verifyMoveDoesNotViolateConstraints(attempt, oldQueue, targetQueue);\n      }\n      \n      executeMove(app, attempt, oldQueue, targetQueue);\n      return targetQueue.getQueueName();\n    }\n  }",
                "code_after_change": "  public String moveApplication(ApplicationId appId,\n      String queueName) throws YarnException {\n    try {\n      writeLock.lock();\n      SchedulerApplication<FSAppAttempt> app = applications.get(appId);\n      if (app == null) {\n        throw new YarnException(\"App to be moved \" + appId + \" not found.\");\n      }\n      FSAppAttempt attempt = (FSAppAttempt) app.getCurrentAppAttempt();\n      // To serialize with FairScheduler#allocate, synchronize on app attempt\n\n      try {\n        attempt.getWriteLock().lock();\n        FSLeafQueue oldQueue = (FSLeafQueue) app.getQueue();\n        // Check if the attempt is already stopped: don't move stopped app\n        // attempt. The attempt has already been removed from all queues.\n        if (attempt.isStopped()) {\n          LOG.info(\"Application \" + appId + \" is stopped and can't be moved!\");\n          throw new YarnException(\"Application \" + appId\n              + \" is stopped and can't be moved!\");\n        }\n        String destQueueName = handleMoveToPlanQueue(queueName);\n        FSLeafQueue targetQueue = queueMgr.getLeafQueue(destQueueName, false);\n        if (targetQueue == null) {\n          throw new YarnException(\"Target queue \" + queueName\n              + \" not found or is not a leaf queue.\");\n        }\n        if (targetQueue == oldQueue) {\n          return oldQueue.getQueueName();\n        }\n\n        if (oldQueue.isRunnableApp(attempt)) {\n          verifyMoveDoesNotViolateConstraints(attempt, oldQueue, targetQueue);\n        }\n\n        executeMove(app, attempt, oldQueue, targetQueue);\n        return targetQueue.getQueueName();\n      } finally {\n        attempt.getWriteLock().unlock();\n      }\n    } finally {\n      writeLock.unlock();\n    }\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.removeApplicationAttempt": {
                "code_before_change": "  private synchronized void removeApplicationAttempt(\n      ApplicationAttemptId applicationAttemptId,\n      RMAppAttemptState rmAppAttemptFinalState, boolean keepContainers) {\n    LOG.info(\"Application \" + applicationAttemptId + \" is done.\" +\n        \" finalState=\" + rmAppAttemptFinalState);\n    SchedulerApplication<FSAppAttempt> application =\n        applications.get(applicationAttemptId.getApplicationId());\n    FSAppAttempt attempt = getSchedulerApp(applicationAttemptId);\n\n    if (attempt == null || application == null) {\n      LOG.info(\"Unknown application \" + applicationAttemptId + \" has completed!\");\n      return;\n    }\n\n    // Release all the running containers\n    for (RMContainer rmContainer : attempt.getLiveContainers()) {\n      if (keepContainers\n          && rmContainer.getState().equals(RMContainerState.RUNNING)) {\n        // do not kill the running container in the case of work-preserving AM\n        // restart.\n        LOG.info(\"Skip killing \" + rmContainer.getContainerId());\n        continue;\n      }\n      super.completedContainer(rmContainer,\n          SchedulerUtils.createAbnormalContainerStatus(\n              rmContainer.getContainerId(),\n              SchedulerUtils.COMPLETED_APPLICATION),\n              RMContainerEventType.KILL);\n    }\n\n    // Release all reserved containers\n    for (RMContainer rmContainer : attempt.getReservedContainers()) {\n      super.completedContainer(rmContainer,\n          SchedulerUtils.createAbnormalContainerStatus(\n              rmContainer.getContainerId(),\n              \"Application Complete\"),\n              RMContainerEventType.KILL);\n    }\n    // Clean up pending requests, metrics etc.\n    attempt.stop(rmAppAttemptFinalState);\n\n    // Inform the queue\n    FSLeafQueue queue = queueMgr.getLeafQueue(attempt.getQueue()\n        .getQueueName(), false);\n    boolean wasRunnable = queue.removeApp(attempt);\n\n    if (wasRunnable) {\n      maxRunningEnforcer.untrackRunnableApp(attempt);\n      maxRunningEnforcer.updateRunnabilityOnAppRemoval(attempt,\n          attempt.getQueue());\n    } else {\n      maxRunningEnforcer.untrackNonRunnableApp(attempt);\n    }\n  }",
                "code_after_change": "  private void removeApplicationAttempt(\n      ApplicationAttemptId applicationAttemptId,\n      RMAppAttemptState rmAppAttemptFinalState, boolean keepContainers) {\n    try {\n      writeLock.lock();\n      LOG.info(\"Application \" + applicationAttemptId + \" is done. finalState=\"\n              + rmAppAttemptFinalState);\n      FSAppAttempt attempt = getApplicationAttempt(applicationAttemptId);\n\n      if (attempt == null) {\n        LOG.info(\n            \"Unknown application \" + applicationAttemptId + \" has completed!\");\n        return;\n      }\n\n      // Check if the attempt is already stopped and don't stop it twice.\n      if (attempt.isStopped()) {\n        LOG.info(\"Application \" + applicationAttemptId + \" has already been \"\n            + \"stopped!\");\n        return;\n      }\n\n      // Release all the running containers\n      for (RMContainer rmContainer : attempt.getLiveContainers()) {\n        if (keepContainers && rmContainer.getState().equals(\n            RMContainerState.RUNNING)) {\n          // do not kill the running container in the case of work-preserving AM\n          // restart.\n          LOG.info(\"Skip killing \" + rmContainer.getContainerId());\n          continue;\n        }\n        super.completedContainer(rmContainer, SchedulerUtils\n                .createAbnormalContainerStatus(rmContainer.getContainerId(),\n                    SchedulerUtils.COMPLETED_APPLICATION),\n            RMContainerEventType.KILL);\n      }\n\n      // Release all reserved containers\n      for (RMContainer rmContainer : attempt.getReservedContainers()) {\n        super.completedContainer(rmContainer, SchedulerUtils\n            .createAbnormalContainerStatus(rmContainer.getContainerId(),\n                \"Application Complete\"), RMContainerEventType.KILL);\n      }\n      // Clean up pending requests, metrics etc.\n      attempt.stop(rmAppAttemptFinalState);\n\n      // Inform the queue\n      FSLeafQueue queue = queueMgr.getLeafQueue(\n          attempt.getQueue().getQueueName(), false);\n      boolean wasRunnable = queue.removeApp(attempt);\n\n      if (wasRunnable) {\n        maxRunningEnforcer.untrackRunnableApp(attempt);\n        maxRunningEnforcer.updateRunnabilityOnAppRemoval(attempt,\n            attempt.getQueue());\n      } else{\n        maxRunningEnforcer.untrackNonRunnableApp(attempt);\n      }\n    } finally {\n      writeLock.unlock();\n    }\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.verifyMoveDoesNotViolateConstraints": {
                "code_before_change": "  private void verifyMoveDoesNotViolateConstraints(FSAppAttempt app,\n      FSLeafQueue oldQueue, FSLeafQueue targetQueue) throws YarnException {\n    String queueName = targetQueue.getQueueName();\n    ApplicationAttemptId appAttId = app.getApplicationAttemptId();\n    // When checking maxResources and maxRunningApps, only need to consider\n    // queues before the lowest common ancestor of the two queues because the\n    // total running apps in queues above will not be changed.\n    FSQueue lowestCommonAncestor = findLowestCommonAncestorQueue(oldQueue,\n        targetQueue);\n    Resource consumption = app.getCurrentConsumption();\n    \n    // Check whether the move would go over maxRunningApps or maxShare\n    FSQueue cur = targetQueue;\n    while (cur != lowestCommonAncestor) {\n      // maxRunningApps\n      if (cur.getNumRunnableApps() == allocConf.getQueueMaxApps(cur.getQueueName())) {\n        throw new YarnException(\"Moving app attempt \" + appAttId + \" to queue \"\n            + queueName + \" would violate queue maxRunningApps constraints on\"\n            + \" queue \" + cur.getQueueName());\n      }\n      \n      // maxShare\n      if (!Resources.fitsIn(Resources.add(cur.getResourceUsage(), consumption),\n          cur.getMaxShare())) {\n        throw new YarnException(\"Moving app attempt \" + appAttId + \" to queue \"\n            + queueName + \" would violate queue maxShare constraints on\"\n            + \" queue \" + cur.getQueueName());\n      }\n      \n      cur = cur.getParent();\n    }\n  }",
                "code_after_change": "  private void verifyMoveDoesNotViolateConstraints(FSAppAttempt app,\n      FSLeafQueue oldQueue, FSLeafQueue targetQueue) throws YarnException {\n    String queueName = targetQueue.getQueueName();\n    ApplicationAttemptId appAttId = app.getApplicationAttemptId();\n    // When checking maxResources and maxRunningApps, only need to consider\n    // queues before the lowest common ancestor of the two queues because the\n    // total running apps in queues above will not be changed.\n    FSQueue lowestCommonAncestor = findLowestCommonAncestorQueue(oldQueue,\n        targetQueue);\n    Resource consumption = app.getCurrentConsumption();\n    \n    // Check whether the move would go over maxRunningApps or maxShare\n    FSQueue cur = targetQueue;\n    while (cur != lowestCommonAncestor) {\n      // maxRunningApps\n      if (cur.getNumRunnableApps() == cur.getMaxRunningApps()) {\n        throw new YarnException(\"Moving app attempt \" + appAttId + \" to queue \"\n            + queueName + \" would violate queue maxRunningApps constraints on\"\n            + \" queue \" + cur.getQueueName());\n      }\n      \n      // maxShare\n      if (!Resources.fitsIn(Resources.add(cur.getResourceUsage(), consumption),\n          cur.getMaxShare())) {\n        throw new YarnException(\"Moving app attempt \" + appAttId + \" to queue \"\n            + queueName + \" would violate queue maxShare constraints on\"\n            + \" queue \" + cur.getQueueName());\n      }\n      \n      cur = cur.getParent();\n    }\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Direct Caller/Callee"
            },
            "fix_suggestion": "Alternative Fix",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": ""
            },
            "wrong_information": "No",
            "explanation_of_judgement": "1. **Root Cause Identification**: The bug report identifies the issue in `FSLeafQueue.removeApp` and `FairScheduler.removeApplicationAttempt`, which are directly involved in the call stack leading to the error. However, the actual root cause involves the lifecycle management in `FairScheduler.moveApplication` and `FairScheduler.verifyMoveDoesNotViolateConstraints`, which are not mentioned. Thus, it is classified as 'Partial' with 'Direct Caller/Callee' as the sub-category.\n\n2. **Fix Suggestion**: The suggestion to ensure proper tracking of applications and checking their existence before removal is an alternative approach to resolving the issue, as it addresses the lifecycle management problem indirectly. The actual fix involved adding synchronization and state checks in `FairScheduler.moveApplication` and `FairScheduler.removeApplicationAttempt`.\n\n3. **Problem Location Identification**: The bug report precisely identifies `FSLeafQueue.removeApp` and `FairScheduler.removeApplicationAttempt` as problem locations, which are part of the ground truth methods.\n\n4. **Wrong Information**: The bug report does not contain any incorrect or unrelated information. All details are relevant to the context of the bug."
        }
    },
    {
        "filename": "YARN-8211.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-registry.src.main.java.org.apache.hadoop.registry.server.dns.RegistryDNS.nioTCPClient": {
                "code_before_change": "  public void nioTCPClient(SocketChannel ch) throws IOException {\n    try {\n      // query sizes are small, so the following two lines should work\n      // in all instances\n      ByteBuffer buf = ByteBuffer.allocate(1024);\n      ch.read(buf);\n      buf.flip();\n      int messageLength = getMessgeLength(buf);\n\n      byte[] in = new byte[messageLength];\n\n      buf.get(in, 0, messageLength);\n\n      Message query;\n      byte[] response;\n      try {\n        query = new Message(in);\n        LOG.info(\"received TCP query {}\", query.getQuestion());\n        response = generateReply(query, ch.socket());\n        if (response == null) {\n          return;\n        }\n      } catch (IOException e) {\n        response = formErrorMessage(in);\n      }\n\n      ByteBuffer out = ByteBuffer.allocate(response.length + 2);\n      out.clear();\n      byte[] data = new byte[2];\n\n      data[1] = (byte)(response.length & 0xFF);\n      data[0] = (byte)((response.length >> 8) & 0xFF);\n      out.put(data);\n      out.put(response);\n      out.flip();\n\n      while(out.hasRemaining()) {\n        ch.write(out);\n      }\n\n    } catch (IOException e) {\n      throw NetUtils.wrapException(ch.socket().getInetAddress().getHostName(),\n          ch.socket().getPort(),\n          ch.socket().getLocalAddress().getHostName(),\n          ch.socket().getLocalPort(), e);\n    } finally {\n      IOUtils.closeStream(ch);\n    }\n\n  }",
                "code_after_change": "  public void nioTCPClient(SocketChannel ch) throws IOException {\n    try {\n      // query sizes are small, so the following two lines should work\n      // in all instances\n      ByteBuffer buf = ByteBuffer.allocate(1024);\n      ch.read(buf);\n      buf.flip();\n      int messageLength = getMessgeLength(buf);\n\n      byte[] in = new byte[messageLength];\n\n      buf.get(in, 0, messageLength);\n\n      Message query;\n      byte[] response;\n      try {\n        query = new Message(in);\n        LOG.info(\"received TCP query {}\", query.getQuestion());\n        response = generateReply(query, ch.socket());\n        if (response == null) {\n          return;\n        }\n      } catch (IOException e) {\n        response = formErrorMessage(in);\n      }\n\n      ByteBuffer out = ByteBuffer.allocate(response.length + 2);\n      out.clear();\n      byte[] data = new byte[2];\n\n      data[1] = (byte)(response.length & 0xFF);\n      data[0] = (byte)((response.length >> 8) & 0xFF);\n      out.put(data);\n      out.put(response);\n      out.flip();\n\n      while(out.hasRemaining()) {\n        ch.write(out);\n      }\n\n    } catch (IOException e) {\n      throw NetUtils.wrapException(ch.socket().getInetAddress().getHostName(),\n          ch.socket().getPort(),\n          ch.socket().getLocalAddress().getHostName(),\n          ch.socket().getLocalPort(), e);\n    } catch (BufferUnderflowException e) {\n      // Ignore system monitor ping packets\n    } finally {\n      IOUtils.closeStream(ch);\n    }\n\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Alternative Fix",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report precisely identifies the root cause as the `nioTCPClient` method not ensuring enough data is read into the `ByteBuffer` before calling `getMessgeLength`, which matches the ground truth method. The fix suggestion in the bug report suggests checking the number of bytes read before proceeding, which is an alternative fix to the developer's approach of catching the `BufferUnderflowException`. The problem location is precisely identified as it mentions the `nioTCPClient` method, which is the ground truth method. There is no wrong information in the bug report as all details are relevant and accurate."
        }
    },
    {
        "filename": "YARN-2124.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.validateConfigs": {
                "code_before_change": "  protected static void validateConfigs(Configuration conf) {\n    // validate max-attempts\n    int globalMaxAppAttempts =\n        conf.getInt(YarnConfiguration.RM_AM_MAX_ATTEMPTS,\n        YarnConfiguration.DEFAULT_RM_AM_MAX_ATTEMPTS);\n    if (globalMaxAppAttempts <= 0) {\n      throw new YarnRuntimeException(\"Invalid global max attempts configuration\"\n          + \", \" + YarnConfiguration.RM_AM_MAX_ATTEMPTS\n          + \"=\" + globalMaxAppAttempts + \", it should be a positive integer.\");\n    }\n\n    // validate expireIntvl >= heartbeatIntvl\n    long expireIntvl = conf.getLong(YarnConfiguration.RM_NM_EXPIRY_INTERVAL_MS,\n        YarnConfiguration.DEFAULT_RM_NM_EXPIRY_INTERVAL_MS);\n    long heartbeatIntvl =\n        conf.getLong(YarnConfiguration.RM_NM_HEARTBEAT_INTERVAL_MS,\n            YarnConfiguration.DEFAULT_RM_NM_HEARTBEAT_INTERVAL_MS);\n    if (expireIntvl < heartbeatIntvl) {\n      throw new YarnRuntimeException(\"Nodemanager expiry interval should be no\"\n          + \" less than heartbeat interval, \"\n          + YarnConfiguration.RM_NM_EXPIRY_INTERVAL_MS + \"=\" + expireIntvl\n          + \", \" + YarnConfiguration.RM_NM_HEARTBEAT_INTERVAL_MS + \"=\"\n          + heartbeatIntvl);\n    }\n  }",
                "code_after_change": "  protected static void validateConfigs(Configuration conf) {\n    // validate max-attempts\n    int globalMaxAppAttempts =\n        conf.getInt(YarnConfiguration.RM_AM_MAX_ATTEMPTS,\n        YarnConfiguration.DEFAULT_RM_AM_MAX_ATTEMPTS);\n    if (globalMaxAppAttempts <= 0) {\n      throw new YarnRuntimeException(\"Invalid global max attempts configuration\"\n          + \", \" + YarnConfiguration.RM_AM_MAX_ATTEMPTS\n          + \"=\" + globalMaxAppAttempts + \", it should be a positive integer.\");\n    }\n\n    // validate expireIntvl >= heartbeatIntvl\n    long expireIntvl = conf.getLong(YarnConfiguration.RM_NM_EXPIRY_INTERVAL_MS,\n        YarnConfiguration.DEFAULT_RM_NM_EXPIRY_INTERVAL_MS);\n    long heartbeatIntvl =\n        conf.getLong(YarnConfiguration.RM_NM_HEARTBEAT_INTERVAL_MS,\n            YarnConfiguration.DEFAULT_RM_NM_HEARTBEAT_INTERVAL_MS);\n    if (expireIntvl < heartbeatIntvl) {\n      throw new YarnRuntimeException(\"Nodemanager expiry interval should be no\"\n          + \" less than heartbeat interval, \"\n          + YarnConfiguration.RM_NM_EXPIRY_INTERVAL_MS + \"=\" + expireIntvl\n          + \", \" + YarnConfiguration.RM_NM_HEARTBEAT_INTERVAL_MS + \"=\"\n          + heartbeatIntvl);\n    }\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.createPolicyMonitors": {
                "code_before_change": "    protected void createPolicyMonitors() {\n      if (scheduler instanceof PreemptableResourceScheduler\n          && conf.getBoolean(YarnConfiguration.RM_SCHEDULER_ENABLE_MONITORS,\n          YarnConfiguration.DEFAULT_RM_SCHEDULER_ENABLE_MONITORS)) {\n        LOG.info(\"Loading policy monitors\");\n        List<SchedulingEditPolicy> policies = conf.getInstances(\n            YarnConfiguration.RM_SCHEDULER_MONITOR_POLICIES,\n            SchedulingEditPolicy.class);\n        if (policies.size() > 0) {\n          rmDispatcher.register(ContainerPreemptEventType.class,\n              new RMContainerPreemptEventDispatcher(\n                  (PreemptableResourceScheduler) scheduler));\n          for (SchedulingEditPolicy policy : policies) {\n            LOG.info(\"LOADING SchedulingEditPolicy:\" + policy.getPolicyName());\n            policy.init(conf, rmContext.getDispatcher().getEventHandler(),\n                (PreemptableResourceScheduler) scheduler);\n            // periodically check whether we need to take action to guarantee\n            // constraints\n            SchedulingMonitor mon = new SchedulingMonitor(policy);\n            addService(mon);\n          }\n        } else {\n          LOG.warn(\"Policy monitors configured (\" +\n              YarnConfiguration.RM_SCHEDULER_ENABLE_MONITORS +\n              \") but none specified (\" +\n              YarnConfiguration.RM_SCHEDULER_MONITOR_POLICIES + \")\");\n        }\n      }\n    }",
                "code_after_change": "    protected void createPolicyMonitors() {\n      if (scheduler instanceof PreemptableResourceScheduler\n          && conf.getBoolean(YarnConfiguration.RM_SCHEDULER_ENABLE_MONITORS,\n          YarnConfiguration.DEFAULT_RM_SCHEDULER_ENABLE_MONITORS)) {\n        LOG.info(\"Loading policy monitors\");\n        List<SchedulingEditPolicy> policies = conf.getInstances(\n            YarnConfiguration.RM_SCHEDULER_MONITOR_POLICIES,\n            SchedulingEditPolicy.class);\n        if (policies.size() > 0) {\n          rmDispatcher.register(ContainerPreemptEventType.class,\n              new RMContainerPreemptEventDispatcher(\n                  (PreemptableResourceScheduler) scheduler));\n          for (SchedulingEditPolicy policy : policies) {\n            LOG.info(\"LOADING SchedulingEditPolicy:\" + policy.getPolicyName());\n            // periodically check whether we need to take action to guarantee\n            // constraints\n            SchedulingMonitor mon = new SchedulingMonitor(rmContext, policy);\n            addService(mon);\n          }\n        } else {\n          LOG.warn(\"Policy monitors configured (\" +\n              YarnConfiguration.RM_SCHEDULER_ENABLE_MONITORS +\n              \") but none specified (\" +\n              YarnConfiguration.RM_SCHEDULER_MONITOR_POLICIES + \")\");\n        }\n      }\n    }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy.init": {
                "code_before_change": "  public void init(Configuration config,\n      EventHandler<ContainerPreemptEvent> disp,\n      PreemptableResourceScheduler sched) {\n    LOG.info(\"Preemption monitor:\" + this.getClass().getCanonicalName());\n    assert null == scheduler : \"Unexpected duplicate call to init\";\n    if (!(sched instanceof CapacityScheduler)) {\n      throw new YarnRuntimeException(\"Class \" +\n          sched.getClass().getCanonicalName() + \" not instance of \" +\n          CapacityScheduler.class.getCanonicalName());\n    }\n    dispatcher = disp;\n    scheduler = (CapacityScheduler) sched;\n    maxIgnoredOverCapacity = config.getDouble(MAX_IGNORED_OVER_CAPACITY, 0.1);\n    naturalTerminationFactor =\n      config.getDouble(NATURAL_TERMINATION_FACTOR, 0.2);\n    maxWaitTime = config.getLong(WAIT_TIME_BEFORE_KILL, 15000);\n    monitoringInterval = config.getLong(MONITORING_INTERVAL, 3000);\n    percentageClusterPreemptionAllowed =\n      config.getFloat(TOTAL_PREEMPTION_PER_ROUND, (float) 0.1);\n    observeOnly = config.getBoolean(OBSERVE_ONLY, false);\n    rc = scheduler.getResourceCalculator();\n  }",
                "code_after_change": "  public void init(Configuration config,\n      EventHandler<ContainerPreemptEvent> disp,\n      PreemptableResourceScheduler sched) {\n    LOG.info(\"Preemption monitor:\" + this.getClass().getCanonicalName());\n    assert null == scheduler : \"Unexpected duplicate call to init\";\n    if (!(sched instanceof CapacityScheduler)) {\n      throw new YarnRuntimeException(\"Class \" +\n          sched.getClass().getCanonicalName() + \" not instance of \" +\n          CapacityScheduler.class.getCanonicalName());\n    }\n    dispatcher = disp;\n    scheduler = (CapacityScheduler) sched;\n    maxIgnoredOverCapacity = config.getDouble(MAX_IGNORED_OVER_CAPACITY, 0.1);\n    naturalTerminationFactor =\n      config.getDouble(NATURAL_TERMINATION_FACTOR, 0.2);\n    maxWaitTime = config.getLong(WAIT_TIME_BEFORE_KILL, 15000);\n    monitoringInterval = config.getLong(MONITORING_INTERVAL, 3000);\n    percentageClusterPreemptionAllowed =\n      config.getFloat(TOTAL_PREEMPTION_PER_ROUND, (float) 0.1);\n    observeOnly = config.getBoolean(OBSERVE_ONLY, false);\n    rc = scheduler.getResourceCalculator();\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Same Class or Module"
            },
            "fix_suggestion": "Alternative Fix",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Same Class or Module"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the root cause as the initialization order of ProportionalCapacityPreemptionPolicy and CapacityScheduler, which is related to the ground truth method ProportionalCapacityPreemptionPolicy.init, but not precise. The fix suggestion to modify the initialization sequence and add null checks is an alternative fix that could resolve the issue. The problem location mentions methods in the same class or module as the ground truth method, but not the exact methods. There is no wrong information in the bug report as all details are relevant to the context of the bug."
        }
    },
    {
        "filename": "YARN-8591.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-timelineservice.src.main.java.org.apache.hadoop.yarn.server.timelineservice.reader.TimelineReaderWebServices.validateAuthUserWithEntityUser": {
                "code_before_change": "  static boolean validateAuthUserWithEntityUser(\n      TimelineReaderManager readerManager, UserGroupInformation ugi,\n      String entityUser) {\n    String authUser = TimelineReaderWebServicesUtils.getUserName(ugi);\n    String requestedUser = TimelineReaderWebServicesUtils.parseStr(entityUser);\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\n          \"Authenticated User: \" + authUser + \" Requested User:\" + entityUser);\n    }\n    return (readerManager.checkAccess(ugi) || authUser.equals(requestedUser));\n  }",
                "code_after_change": "  static boolean validateAuthUserWithEntityUser(\n      TimelineReaderManager readerManager, UserGroupInformation ugi,\n      String entityUser) {\n    String authUser = TimelineReaderWebServicesUtils.getUserName(ugi);\n    String requestedUser = TimelineReaderWebServicesUtils.parseStr(entityUser);\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\n          \"Authenticated User: \" + authUser + \" Requested User:\" + entityUser);\n    }\n    return (readerManager.checkAccess(ugi) || authUser.equals(requestedUser));\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Preventive",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the root cause as a NullPointerException due to a null 'info' field in the 'checkAccess' method, which is part of the stack trace but not the ground truth method. Therefore, it is classified as 'Partial' with 'Shared Stack Trace Context'. The fix suggestion involves adding null checks, which is preventive as it would mitigate the issue but does not match the actual fix. The problem location mentions methods in the stack trace but not the ground truth method, so it is 'Partial' with 'Shared Stack Trace Context'. There is no wrong information as all details are relevant to the context of the bug."
        }
    },
    {
        "filename": "YARN-6649.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-applicationhistoryservice.src.main.java.org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore.getEntity": {
                "code_before_change": "  public TimelineEntity getEntity(String entityId, String entityType,\n      EnumSet<Field> fields) throws IOException {\n    Long revStartTime = getStartTimeLong(entityId, entityType);\n    if (revStartTime == null) {\n      return null;\n    }\n    byte[] prefix = KeyBuilder.newInstance().add(entityType)\n        .add(writeReverseOrderedLong(revStartTime)).add(entityId)\n        .getBytesForLookup();\n\n    DB db = entitydb.getDBForStartTime(revStartTime);\n    if (db == null) {\n      return null;\n    }\n    try (DBIterator iterator = db.iterator()) {\n      iterator.seek(prefix);\n\n      return getEntity(entityId, entityType, revStartTime, fields, iterator,\n          prefix, prefix.length);\n    }\n  }",
                "code_after_change": "  public TimelineEntity getEntity(String entityId, String entityType,\n      EnumSet<Field> fields) throws IOException {\n    Long revStartTime = getStartTimeLong(entityId, entityType);\n    if (revStartTime == null) {\n      return null;\n    }\n    byte[] prefix = KeyBuilder.newInstance().add(entityType)\n        .add(writeReverseOrderedLong(revStartTime)).add(entityId)\n        .getBytesForLookup();\n\n    DB db = entitydb.getDBForStartTime(revStartTime);\n    if (db == null) {\n      return null;\n    }\n    try (DBIterator iterator = db.iterator()) {\n      iterator.seek(prefix);\n\n      return getEntity(entityId, entityType, revStartTime, fields, iterator,\n          prefix, prefix.length);\n    }\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-applicationhistoryservice.src.main.java.org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore.addPrimaryFilter": {
                "code_before_change": "  private static void addPrimaryFilter(TimelineEntity entity, byte[] key,\n      int offset) throws IOException {\n    KeyParser kp = new KeyParser(key, offset);\n    String name = kp.getNextString();\n    byte[] bytes = kp.getRemainingBytes();\n    Object value = fstConf.asObject(bytes);\n    entity.addPrimaryFilter(name, value);\n  }",
                "code_after_change": "  private static void addPrimaryFilter(TimelineEntity entity, byte[] key,\n      int offset) throws IOException {\n    KeyParser kp = new KeyParser(key, offset);\n    String name = kp.getNextString();\n    byte[] bytes = kp.getRemainingBytes();\n    Object value = null;\n    try {\n      value = fstConf.asObject(bytes);\n      entity.addPrimaryFilter(name, value);\n    } catch (Exception e) {\n      LOG.warn(\"Error while decoding \" + name, e);\n    }\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-applicationhistoryservice.src.main.java.org.apache.hadoop.yarn.server.timeline.RollingLevelDBTimelineStore.getEntityEvent": {
                "code_before_change": "  private static TimelineEvent getEntityEvent(Set<String> eventTypes,\n      byte[] key, int offset, byte[] value) throws IOException {\n    KeyParser kp = new KeyParser(key, offset);\n    long ts = kp.getNextLong();\n    String tstype = kp.getNextString();\n    if (eventTypes == null || eventTypes.contains(tstype)) {\n      TimelineEvent event = new TimelineEvent();\n      event.setTimestamp(ts);\n      event.setEventType(tstype);\n      Object o = fstConf.asObject(value);\n      if (o == null) {\n        event.setEventInfo(null);\n      } else if (o instanceof Map) {\n        @SuppressWarnings(\"unchecked\")\n        Map<String, Object> m = (Map<String, Object>) o;\n        event.setEventInfo(m);\n      } else {\n        throw new IOException(\"Couldn't deserialize event info map\");\n      }\n      return event;\n    }\n    return null;\n  }",
                "code_after_change": "  private static TimelineEvent getEntityEvent(Set<String> eventTypes,\n      byte[] key, int offset, byte[] value) throws IOException {\n    KeyParser kp = new KeyParser(key, offset);\n    long ts = kp.getNextLong();\n    String tstype = kp.getNextString();\n    if (eventTypes == null || eventTypes.contains(tstype)) {\n      TimelineEvent event = new TimelineEvent();\n      event.setTimestamp(ts);\n      event.setEventType(tstype);\n      Object o = null;\n      try {\n        o = fstConf.asObject(value);\n      } catch (Exception e) {\n        LOG.warn(\"Error while decoding \" + tstype, e);\n      }\n      if (o == null) {\n        event.setEventInfo(null);\n      } else if (o instanceof Map) {\n        @SuppressWarnings(\"unchecked\")\n        Map<String, Object> m = (Map<String, Object>) o;\n        event.setEventInfo(m);\n      } else {\n        throw new IOException(\"Couldn't deserialize event info map\");\n      }\n      return event;\n    }\n    return null;\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Buggy Method"
            },
            "fix_suggestion": "Preventive",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": ""
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the root cause as a failure in the FST serialization framework, which is related to the methods mentioned in the ground truth, but it does not pinpoint the exact root cause in the ground truth methods. Therefore, it is classified as 'Partial' with the sub-category 'Buggy Method' because it points to the method where the error occurred but not where the actual fix was made. The fix suggestion is 'Preventive' as it suggests verifying class definitions and serialization configurations, which would prevent the bug. The problem location identification is 'Precise' because the 'problem_location' field mentions the 'RollingLevelDBTimelineStore.getEntity' method, which is in the ground truth list. There is no wrong information in the bug report as all the information is relevant to the context of the bug."
        }
    },
    {
        "filename": "YARN-3742.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.AdminService.transitionToActive": {
                "code_before_change": "  public synchronized void transitionToActive(\n      HAServiceProtocol.StateChangeRequestInfo reqInfo) throws IOException {\n    // call refreshAdminAcls before HA state transition\n    // for the case that adminAcls have been updated in previous active RM\n    try {\n      refreshAdminAcls(false);\n    } catch (YarnException ex) {\n      throw new ServiceFailedException(\"Can not execute refreshAdminAcls\", ex);\n    }\n\n    UserGroupInformation user = checkAccess(\"transitionToActive\");\n    checkHaStateChange(reqInfo);\n    try {\n      rm.transitionToActive();\n      // call all refresh*s for active RM to get the updated configurations.\n      refreshAll();\n      RMAuditLogger.logSuccess(user.getShortUserName(),\n          \"transitionToActive\", \"RMHAProtocolService\");\n    } catch (Exception e) {\n      RMAuditLogger.logFailure(user.getShortUserName(), \"transitionToActive\",\n          \"\", \"RMHAProtocolService\",\n          \"Exception transitioning to active\");\n      throw new ServiceFailedException(\n          \"Error when transitioning to Active mode\", e);\n    }\n  }",
                "code_after_change": "  public synchronized void transitionToActive(\n      HAServiceProtocol.StateChangeRequestInfo reqInfo) throws IOException {\n    if (isRMActive()) {\n      return;\n    }\n    // call refreshAdminAcls before HA state transition\n    // for the case that adminAcls have been updated in previous active RM\n    try {\n      refreshAdminAcls(false);\n    } catch (YarnException ex) {\n      throw new ServiceFailedException(\"Can not execute refreshAdminAcls\", ex);\n    }\n\n    UserGroupInformation user = checkAccess(\"transitionToActive\");\n    checkHaStateChange(reqInfo);\n\n    try {\n      // call all refresh*s for active RM to get the updated configurations.\n      refreshAll();\n    } catch (Exception e) {\n      rmContext\n          .getDispatcher()\n          .getEventHandler()\n          .handle(\n              new RMFatalEvent(RMFatalEventType.TRANSITION_TO_ACTIVE_FAILED,\n                  e, \"failure to refresh configuration settings\"));\n      throw new ServiceFailedException(\n          \"Error on refreshAll during transition to Active\", e);\n    }\n\n    try {\n      rm.transitionToActive();\n    } catch (Exception e) {\n      RMAuditLogger.logFailure(user.getShortUserName(), \"transitionToActive\",\n          \"\", \"RM\",\n          \"Exception transitioning to active\");\n      throw new ServiceFailedException(\n          \"Error when transitioning to Active mode\", e);\n    }\n\n    RMAuditLogger.logSuccess(user.getShortUserName(), \"transitionToActive\",\n        \"RM\");\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.handle": {
                "code_before_change": "    public void handle(SchedulerEvent event) {\n      try {\n        int qSize = eventQueue.size();\n        if (qSize !=0 && qSize %1000 == 0) {\n          LOG.info(\"Size of scheduler event-queue is \" + qSize);\n        }\n        int remCapacity = eventQueue.remainingCapacity();\n        if (remCapacity < 1000) {\n          LOG.info(\"Very low remaining capacity on scheduler event queue: \"\n              + remCapacity);\n        }\n        this.eventQueue.put(event);\n      } catch (InterruptedException e) {\n        LOG.info(\"Interrupted. Trying to exit gracefully.\");\n      }\n    }",
                "code_after_change": "    public void handle(RMFatalEvent event) {\n      LOG.error(\"Received \" + event);\n\n      if (HAUtil.isHAEnabled(getConfig())) {\n        // If we're in an HA config, the right answer is always to go into\n        // standby.\n        LOG.warn(\"Transitioning the resource manager to standby.\");\n        handleTransitionToStandByInNewThread();\n      } else {\n        // If we're stand-alone, we probably want to shut down, but the if and\n        // how depends on the event.\n        switch(event.getType()) {\n        case STATE_STORE_FENCED:\n          LOG.fatal(\"State store fenced even though the resource manager \" +\n              \"is not configured for high availability. Shutting down this \" +\n              \"resource manager to protect the integrity of the state store.\");\n          ExitUtil.terminate(1, event.getExplanation());\n          break;\n        case STATE_STORE_OP_FAILED:\n          if (YarnConfiguration.shouldRMFailFast(getConfig())) {\n            LOG.fatal(\"Shutting down the resource manager because a state \" +\n                \"store operation failed, and the resource manager is \" +\n                \"configured to fail fast. See the yarn.fail-fast and \" +\n                \"yarn.resourcemanager.fail-fast properties.\");\n            ExitUtil.terminate(1, event.getExplanation());\n          } else {\n            LOG.warn(\"Ignoring state store operation failure because the \" +\n                \"resource manager is not configured to fail fast. See the \" +\n                \"yarn.fail-fast and yarn.resourcemanager.fail-fast \" +\n                \"properties.\");\n          }\n          break;\n        default:\n          LOG.fatal(\"Shutting down the resource manager.\");\n          ExitUtil.terminate(1, event.getExplanation());\n        }\n      }\n    }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.ActiveStandbyElectorBasedElectorService.serviceInit": {
                "code_before_change": [],
                "code_after_change": "  protected void serviceInit(Configuration conf)\n      throws Exception {\n    conf = conf instanceof YarnConfiguration\n        ? conf\n        : new YarnConfiguration(conf);\n\n    String zkQuorum = conf.get(YarnConfiguration.RM_ZK_ADDRESS);\n    if (zkQuorum == null) {\n      throw new YarnRuntimeException(\"Embedded automatic failover \" +\n          \"is enabled, but \" + YarnConfiguration.RM_ZK_ADDRESS +\n          \" is not set\");\n    }\n\n    String rmId = HAUtil.getRMHAId(conf);\n    String clusterId = YarnConfiguration.getClusterId(conf);\n    localActiveNodeInfo = createActiveNodeInfo(clusterId, rmId);\n\n    String zkBasePath = conf.get(YarnConfiguration.AUTO_FAILOVER_ZK_BASE_PATH,\n        YarnConfiguration.DEFAULT_AUTO_FAILOVER_ZK_BASE_PATH);\n    String electionZNode = zkBasePath + \"/\" + clusterId;\n\n    zkSessionTimeout = conf.getLong(YarnConfiguration.RM_ZK_TIMEOUT_MS,\n        YarnConfiguration.DEFAULT_RM_ZK_TIMEOUT_MS);\n\n    List<ACL> zkAcls = RMZKUtils.getZKAcls(conf);\n    List<ZKUtil.ZKAuthInfo> zkAuths = RMZKUtils.getZKAuths(conf);\n\n    int maxRetryNum =\n        conf.getInt(YarnConfiguration.RM_HA_FC_ELECTOR_ZK_RETRIES_KEY, conf\n          .getInt(CommonConfigurationKeys.HA_FC_ELECTOR_ZK_OP_RETRIES_KEY,\n            CommonConfigurationKeys.HA_FC_ELECTOR_ZK_OP_RETRIES_DEFAULT));\n    elector = new ActiveStandbyElector(zkQuorum, (int) zkSessionTimeout,\n        electionZNode, zkAcls, zkAuths, this, maxRetryNum, false);\n\n    elector.ensureParentZNode();\n    if (!isParentZnodeSafe(clusterId)) {\n      notifyFatalError(String.format(\"invalid data in znode, %s, \" +\n          \"which may require the state store to be reformatted\",\n          electionZNode));\n    }\n\n    super.serviceInit(conf);\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.RMCriticalThreadUncaughtExceptionHandler.RMCriticalThreadUncaughtExceptionHandler": {
                "code_before_change": [],
                "code_after_change": []
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.createPolicyMonitors": {
                "code_before_change": "    protected void createPolicyMonitors() {\n      if (scheduler instanceof PreemptableResourceScheduler\n          && conf.getBoolean(YarnConfiguration.RM_SCHEDULER_ENABLE_MONITORS,\n          YarnConfiguration.DEFAULT_RM_SCHEDULER_ENABLE_MONITORS)) {\n        LOG.info(\"Loading policy monitors\");\n        List<SchedulingEditPolicy> policies = conf.getInstances(\n            YarnConfiguration.RM_SCHEDULER_MONITOR_POLICIES,\n            SchedulingEditPolicy.class);\n        if (policies.size() > 0) {\n          rmDispatcher.register(ContainerPreemptEventType.class,\n              new RMContainerPreemptEventDispatcher(\n                  (PreemptableResourceScheduler) scheduler));\n          for (SchedulingEditPolicy policy : policies) {\n            LOG.info(\"LOADING SchedulingEditPolicy:\" + policy.getPolicyName());\n            // periodically check whether we need to take action to guarantee\n            // constraints\n            SchedulingMonitor mon = new SchedulingMonitor(rmContext, policy);\n            addService(mon);\n          }\n        } else {\n          LOG.warn(\"Policy monitors configured (\" +\n              YarnConfiguration.RM_SCHEDULER_ENABLE_MONITORS +\n              \") but none specified (\" +\n              YarnConfiguration.RM_SCHEDULER_MONITOR_POLICIES + \")\");\n        }\n      }\n    }",
                "code_after_change": "    protected void createPolicyMonitors() {\n      if (scheduler instanceof PreemptableResourceScheduler\n          && conf.getBoolean(YarnConfiguration.RM_SCHEDULER_ENABLE_MONITORS,\n          YarnConfiguration.DEFAULT_RM_SCHEDULER_ENABLE_MONITORS)) {\n        LOG.info(\"Loading policy monitors\");\n        List<SchedulingEditPolicy> policies = conf.getInstances(\n            YarnConfiguration.RM_SCHEDULER_MONITOR_POLICIES,\n            SchedulingEditPolicy.class);\n        if (policies.size() > 0) {\n          for (SchedulingEditPolicy policy : policies) {\n            LOG.info(\"LOADING SchedulingEditPolicy:\" + policy.getPolicyName());\n            // periodically check whether we need to take action to guarantee\n            // constraints\n            SchedulingMonitor mon = new SchedulingMonitor(rmContext, policy);\n            addService(mon);\n          }\n        } else {\n          LOG.warn(\"Policy monitors configured (\" +\n              YarnConfiguration.RM_SCHEDULER_ENABLE_MONITORS +\n              \") but none specified (\" +\n              YarnConfiguration.RM_SCHEDULER_MONITOR_POLICIES + \")\");\n        }\n      }\n    }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Preventive",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the issue as a timeout during the creation of the ZKClient, which is related to the methods in the stack trace but not the exact root cause in the ground truth methods. Therefore, it is classified as 'Partial' with 'Shared Stack Trace Context'. The fix suggestion involves adjusting configuration settings, which is preventive as it aims to mitigate the issue by increasing the timeout value. The problem location identification is 'Partial' with 'Shared Stack Trace Context' because it mentions methods in the stack trace but not the ground truth methods. There is no wrong information in the bug report as all details are relevant to the context of the bug."
        }
    },
    {
        "filename": "YARN-4984.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService.initAppAggregator": {
                "code_before_change": "  protected void initAppAggregator(final ApplicationId appId, String user,\n      Credentials credentials, Map<ApplicationAccessType, String> appAcls,\n      LogAggregationContext logAggregationContext) {\n\n    // Get user's FileSystem credentials\n    final UserGroupInformation userUgi =\n        UserGroupInformation.createRemoteUser(user);\n    if (credentials != null) {\n      userUgi.addCredentials(credentials);\n    }\n\n    // New application\n    final AppLogAggregator appLogAggregator =\n        new AppLogAggregatorImpl(this.dispatcher, this.deletionService,\n            getConfig(), appId, userUgi, this.nodeId, dirsHandler,\n            getRemoteNodeLogFileForApp(appId, user),\n            appAcls, logAggregationContext, this.context,\n            getLocalFileContext(getConfig()));\n    if (this.appLogAggregators.putIfAbsent(appId, appLogAggregator) != null) {\n      throw new YarnRuntimeException(\"Duplicate initApp for \" + appId);\n    }\n    // wait until check for existing aggregator to create dirs\n    YarnRuntimeException appDirException = null;\n    try {\n      // Create the app dir\n      createAppDir(user, appId, userUgi);\n    } catch (Exception e) {\n      appLogAggregator.disableLogAggregation();\n      if (!(e instanceof YarnRuntimeException)) {\n        appDirException = new YarnRuntimeException(e);\n      } else {\n        appDirException = (YarnRuntimeException)e;\n      }\n    }\n\n    // TODO Get the user configuration for the list of containers that need log\n    // aggregation.\n\n    // Schedule the aggregator.\n    Runnable aggregatorWrapper = new Runnable() {\n      public void run() {\n        try {\n          appLogAggregator.run();\n        } finally {\n          appLogAggregators.remove(appId);\n          closeFileSystems(userUgi);\n        }\n      }\n    };\n    this.threadPool.execute(aggregatorWrapper);\n\n    if (appDirException != null) {\n      throw appDirException;\n    }\n  }",
                "code_after_change": "  protected void initAppAggregator(final ApplicationId appId, String user,\n      Credentials credentials, Map<ApplicationAccessType, String> appAcls,\n      LogAggregationContext logAggregationContext) {\n\n    // Get user's FileSystem credentials\n    final UserGroupInformation userUgi =\n        UserGroupInformation.createRemoteUser(user);\n    if (credentials != null) {\n      userUgi.addCredentials(credentials);\n    }\n\n    // New application\n    final AppLogAggregator appLogAggregator =\n        new AppLogAggregatorImpl(this.dispatcher, this.deletionService,\n            getConfig(), appId, userUgi, this.nodeId, dirsHandler,\n            getRemoteNodeLogFileForApp(appId, user),\n            appAcls, logAggregationContext, this.context,\n            getLocalFileContext(getConfig()));\n    if (this.appLogAggregators.putIfAbsent(appId, appLogAggregator) != null) {\n      throw new YarnRuntimeException(\"Duplicate initApp for \" + appId);\n    }\n    // wait until check for existing aggregator to create dirs\n    YarnRuntimeException appDirException = null;\n    try {\n      // Create the app dir\n      createAppDir(user, appId, userUgi);\n    } catch (Exception e) {\n      appLogAggregator.disableLogAggregation();\n      if (!(e instanceof YarnRuntimeException)) {\n        appDirException = new YarnRuntimeException(e);\n      } else {\n        appDirException = (YarnRuntimeException)e;\n      }\n      appLogAggregators.remove(appId);\n      closeFileSystems(userUgi);\n      throw appDirException;\n    }\n\n    // TODO Get the user configuration for the list of containers that need log\n    // aggregation.\n\n    // Schedule the aggregator.\n    Runnable aggregatorWrapper = new Runnable() {\n      public void run() {\n        try {\n          appLogAggregator.run();\n        } finally {\n          appLogAggregators.remove(appId);\n          closeFileSystems(userUgi);\n        }\n      }\n    };\n    this.threadPool.execute(aggregatorWrapper);\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService.run": {
                "code_before_change": "        public Object run() throws Exception {\n          try {\n            // TODO: Reuse FS for user?\n            FileSystem remoteFS = getFileSystem(getConfig());\n\n            // Only creating directories if they are missing to avoid\n            // unnecessary load on the filesystem from all of the nodes\n            Path appDir = LogAggregationUtils.getRemoteAppLogDir(\n                LogAggregationService.this.remoteRootLogDir, appId, user,\n                LogAggregationService.this.remoteRootLogDirSuffix);\n            appDir = appDir.makeQualified(remoteFS.getUri(),\n                remoteFS.getWorkingDirectory());\n\n            if (!checkExists(remoteFS, appDir, APP_DIR_PERMISSIONS)) {\n              Path suffixDir = LogAggregationUtils.getRemoteLogSuffixedDir(\n                  LogAggregationService.this.remoteRootLogDir, user,\n                  LogAggregationService.this.remoteRootLogDirSuffix);\n              suffixDir = suffixDir.makeQualified(remoteFS.getUri(),\n                  remoteFS.getWorkingDirectory());\n\n              if (!checkExists(remoteFS, suffixDir, APP_DIR_PERMISSIONS)) {\n                Path userDir = LogAggregationUtils.getRemoteLogUserDir(\n                    LogAggregationService.this.remoteRootLogDir, user);\n                userDir = userDir.makeQualified(remoteFS.getUri(),\n                    remoteFS.getWorkingDirectory());\n\n                if (!checkExists(remoteFS, userDir, APP_DIR_PERMISSIONS)) {\n                  createDir(remoteFS, userDir, APP_DIR_PERMISSIONS);\n                }\n\n                createDir(remoteFS, suffixDir, APP_DIR_PERMISSIONS);\n              }\n\n              createDir(remoteFS, appDir, APP_DIR_PERMISSIONS);\n            }\n\n          } catch (IOException e) {\n            LOG.error(\"Failed to setup application log directory for \"\n                + appId, e);\n            throw e;\n          }\n          return null;\n        }",
                "code_after_change": "        public Object run() throws Exception {\n          try {\n            // TODO: Reuse FS for user?\n            FileSystem remoteFS = getFileSystem(getConfig());\n\n            // Only creating directories if they are missing to avoid\n            // unnecessary load on the filesystem from all of the nodes\n            Path appDir = LogAggregationUtils.getRemoteAppLogDir(\n                LogAggregationService.this.remoteRootLogDir, appId, user,\n                LogAggregationService.this.remoteRootLogDirSuffix);\n            appDir = appDir.makeQualified(remoteFS.getUri(),\n                remoteFS.getWorkingDirectory());\n\n            if (!checkExists(remoteFS, appDir, APP_DIR_PERMISSIONS)) {\n              Path suffixDir = LogAggregationUtils.getRemoteLogSuffixedDir(\n                  LogAggregationService.this.remoteRootLogDir, user,\n                  LogAggregationService.this.remoteRootLogDirSuffix);\n              suffixDir = suffixDir.makeQualified(remoteFS.getUri(),\n                  remoteFS.getWorkingDirectory());\n\n              if (!checkExists(remoteFS, suffixDir, APP_DIR_PERMISSIONS)) {\n                Path userDir = LogAggregationUtils.getRemoteLogUserDir(\n                    LogAggregationService.this.remoteRootLogDir, user);\n                userDir = userDir.makeQualified(remoteFS.getUri(),\n                    remoteFS.getWorkingDirectory());\n\n                if (!checkExists(remoteFS, userDir, APP_DIR_PERMISSIONS)) {\n                  createDir(remoteFS, userDir, APP_DIR_PERMISSIONS);\n                }\n\n                createDir(remoteFS, suffixDir, APP_DIR_PERMISSIONS);\n              }\n\n              createDir(remoteFS, appDir, APP_DIR_PERMISSIONS);\n            }\n\n          } catch (IOException e) {\n            LOG.error(\"Failed to setup application log directory for \"\n                + appId, e);\n            throw e;\n          }\n          return null;\n        }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Correct",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report accurately identifies the root cause as the LogAggregationService swallowing exceptions related to invalid Hadoop delegation tokens, which aligns with the ground truth methods. The fix suggestion in the report matches the developer's fix, which involves handling exceptions properly in the LogAggregationService methods, particularly in 'initAppAggregator'. The problem location is precisely identified as it mentions 'initAppAggregator', which is one of the ground truth methods. There is no wrong information in the bug report as all details are consistent with the context of the bug and the provided source code."
        }
    },
    {
        "filename": "YARN-4584.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.recover": {
                "code_before_change": "  public void recover(RMState state) {\n    ApplicationStateData appState =\n        state.getApplicationState().get(getApplicationId());\n    this.recoveredFinalState = appState.getState();\n    LOG.info(\"Recovering app: \" + getApplicationId() + \" with \" + \n        + appState.getAttemptCount() + \" attempts and final state = \"\n        + this.recoveredFinalState );\n    this.diagnostics.append(null == appState.getDiagnostics() ? \"\" : appState\n        .getDiagnostics());\n    this.storedFinishTime = appState.getFinishTime();\n    this.startTime = appState.getStartTime();\n    this.callerContext = appState.getCallerContext();\n    // If interval > 0, some attempts might have been deleted.\n    if (submissionContext.getAttemptFailuresValidityInterval() > 0) {\n      this.firstAttemptIdInStateStore = appState.getFirstAttemptId();\n      this.nextAttemptId = firstAttemptIdInStateStore;\n    }\n\n    // send the ATS create Event\n    sendATSCreateEvent(this, this.startTime);\n\n    for(int i=0; i<appState.getAttemptCount(); ++i) {\n      // create attempt\n      createNewAttempt();\n      ((RMAppAttemptImpl)this.currentAttempt).recover(state);\n    }\n  }",
                "code_after_change": "  public void recover(RMState state) {\n    ApplicationStateData appState =\n        state.getApplicationState().get(getApplicationId());\n    this.recoveredFinalState = appState.getState();\n    LOG.info(\"Recovering app: \" + getApplicationId() + \" with \" + \n        + appState.getAttemptCount() + \" attempts and final state = \"\n        + this.recoveredFinalState );\n    this.diagnostics.append(null == appState.getDiagnostics() ? \"\" : appState\n        .getDiagnostics());\n    this.storedFinishTime = appState.getFinishTime();\n    this.startTime = appState.getStartTime();\n    this.callerContext = appState.getCallerContext();\n    // If interval > 0, some attempts might have been deleted.\n    if (this.attemptFailuresValidityInterval > 0) {\n      this.firstAttemptIdInStateStore = appState.getFirstAttemptId();\n      this.nextAttemptId = firstAttemptIdInStateStore;\n    }\n\n    // send the ATS create Event\n    sendATSCreateEvent(this, this.startTime);\n\n    for(int i=0; i<appState.getAttemptCount(); ++i) {\n      // create attempt\n      createNewAttempt();\n      ((RMAppAttemptImpl)this.currentAttempt).recover(state);\n    }\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.transition": {
                "code_before_change": "    public void transition(RMAppImpl app, RMAppEvent event) {\n    };",
                "code_after_change": "    public void transition(RMAppImpl app, RMAppEvent event) {\n    };"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Preventive",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report precisely identifies the root cause by pointing out the NullPointerException in the recover methods of RMAppImpl and RMAppAttemptImpl, which aligns with the ground truth methods. The fix suggestion is preventive as it suggests adding null checks and logging, which would mitigate the issue but does not match the exact developer's fix. The problem location identification is precise as it correctly mentions the methods RMAppImpl.recover and RMAppAttemptImpl.recover, which are part of the ground truth. There is no wrong information in the bug report as all details are relevant and accurate."
        }
    },
    {
        "filename": "YARN-2846.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.ContainerExecutor.reacquireContainer": {
                "code_before_change": "  public int reacquireContainer(String user, ContainerId containerId)\n      throws IOException {\n    Path pidPath = getPidFilePath(containerId);\n    if (pidPath == null) {\n      LOG.warn(containerId + \" is not active, returning terminated error\");\n      return ExitCode.TERMINATED.getExitCode();\n    }\n\n    String pid = null;\n    pid = ProcessIdFileReader.getProcessId(pidPath);\n    if (pid == null) {\n      throw new IOException(\"Unable to determine pid for \" + containerId);\n    }\n\n    LOG.info(\"Reacquiring \" + containerId + \" with pid \" + pid);\n    try {\n      while(isContainerProcessAlive(user, pid)) {\n        Thread.sleep(1000);\n      }\n    } catch (InterruptedException e) {\n      throw new IOException(\"Interrupted while waiting for process \" + pid\n          + \" to exit\", e);\n    }\n\n    // wait for exit code file to appear\n    String exitCodeFile = ContainerLaunch.getExitCodeFile(pidPath.toString());\n    File file = new File(exitCodeFile);\n    final int sleepMsec = 100;\n    int msecLeft = 2000;\n    while (!file.exists() && msecLeft >= 0) {\n      if (!isContainerActive(containerId)) {\n        LOG.info(containerId + \" was deactivated\");\n        return ExitCode.TERMINATED.getExitCode();\n      }\n      try {\n        Thread.sleep(sleepMsec);\n      } catch (InterruptedException e) {\n        throw new IOException(\n            \"Interrupted while waiting for exit code from \" + containerId, e);\n      }\n      msecLeft -= sleepMsec;\n    }\n    if (msecLeft < 0) {\n      throw new IOException(\"Timeout while waiting for exit code from \"\n          + containerId);\n    }\n\n    try {\n      return Integer.parseInt(FileUtils.readFileToString(file).trim());\n    } catch (NumberFormatException e) {\n      throw new IOException(\"Error parsing exit code from pid \" + pid, e);\n    }\n  }",
                "code_after_change": "  public int reacquireContainer(String user, ContainerId containerId)\n      throws IOException, InterruptedException {\n    Path pidPath = getPidFilePath(containerId);\n    if (pidPath == null) {\n      LOG.warn(containerId + \" is not active, returning terminated error\");\n      return ExitCode.TERMINATED.getExitCode();\n    }\n\n    String pid = null;\n    pid = ProcessIdFileReader.getProcessId(pidPath);\n    if (pid == null) {\n      throw new IOException(\"Unable to determine pid for \" + containerId);\n    }\n\n    LOG.info(\"Reacquiring \" + containerId + \" with pid \" + pid);\n    while(isContainerProcessAlive(user, pid)) {\n      Thread.sleep(1000);\n    }\n\n    // wait for exit code file to appear\n    String exitCodeFile = ContainerLaunch.getExitCodeFile(pidPath.toString());\n    File file = new File(exitCodeFile);\n    final int sleepMsec = 100;\n    int msecLeft = 2000;\n    while (!file.exists() && msecLeft >= 0) {\n      if (!isContainerActive(containerId)) {\n        LOG.info(containerId + \" was deactivated\");\n        return ExitCode.TERMINATED.getExitCode();\n      }\n      \n      Thread.sleep(sleepMsec);\n      \n      msecLeft -= sleepMsec;\n    }\n    if (msecLeft < 0) {\n      throw new IOException(\"Timeout while waiting for exit code from \"\n          + containerId);\n    }\n\n    try {\n      return Integer.parseInt(FileUtils.readFileToString(file).trim());\n    } catch (NumberFormatException e) {\n      throw new IOException(\"Error parsing exit code from pid \" + pid, e);\n    }\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.ContainerExecutor.isContainerProcessAlive": {
                "code_before_change": "  public abstract boolean isContainerProcessAlive(String user, String pid)\n      throws IOException;\n\n  /**\n   * Recover an already existing container. This is a blocking call and returns\n   * only when the container exits.  Note that the container must have been\n   * activated prior to this call.\n   * @param user the user of the container\n   * @param containerId The ID of the container to reacquire\n   * @return The exit code of the pre-existing container\n   * @throws IOException\n   */\n  public int reacquireContainer(String user, ContainerId containerId)\n      throws IOException {\n    Path pidPath = getPidFilePath(containerId);\n    if (pidPath == null) {\n      LOG.warn(containerId + \" is not active, returning terminated error\");\n      return ExitCode.TERMINATED.getExitCode();\n    }\n\n    String pid = null;\n    pid = ProcessIdFileReader.getProcessId(pidPath);\n    if (pid == null) {\n      throw new IOException(\"Unable to determine pid for \" + containerId);\n    }\n\n    LOG.info(\"Reacquiring \" + containerId + \" with pid \" + pid);\n    try {\n      while(isContainerProcessAlive(user, pid)) {\n        Thread.sleep(1000);\n      }\n    } catch (InterruptedException e) {\n      throw new IOException(\"Interrupted while waiting for process \" + pid\n          + \" to exit\", e);\n    }\n\n    // wait for exit code file to appear\n    String exitCodeFile = ContainerLaunch.getExitCodeFile(pidPath.toString());\n    File file = new File(exitCodeFile);\n    final int sleepMsec = 100;\n    int msecLeft = 2000;\n    while (!file.exists() && msecLeft >= 0) {\n      if (!isContainerActive(containerId)) {\n        LOG.info(containerId + \" was deactivated\");\n        return ExitCode.TERMINATED.getExitCode();\n      }\n      try {\n        Thread.sleep(sleepMsec);\n      } catch (InterruptedException e) {\n        throw new IOException(\n            \"Interrupted while waiting for exit code from \" + containerId, e);\n      }\n      msecLeft -= sleepMsec;\n    }\n    if (msecLeft < 0) {\n      throw new IOException(\"Timeout while waiting for exit code from \"\n          + containerId);\n    }\n\n    try {\n      return Integer.parseInt(FileUtils.readFileToString(file).trim());\n    } catch (NumberFormatException e) {\n      throw new IOException(\"Error parsing exit code from pid \" + pid, e);\n    }\n  }",
                "code_after_change": "  public abstract boolean isContainerProcessAlive(String user, String pid)\n      throws IOException;\n\n  /**\n   * Recover an already existing container. This is a blocking call and returns\n   * only when the container exits.  Note that the container must have been\n   * activated prior to this call.\n   * @param user the user of the container\n   * @param containerId The ID of the container to reacquire\n   * @return The exit code of the pre-existing container\n   * @throws IOException\n   * @throws InterruptedException \n   */\n  public int reacquireContainer(String user, ContainerId containerId)\n      throws IOException, InterruptedException {\n    Path pidPath = getPidFilePath(containerId);\n    if (pidPath == null) {\n      LOG.warn(containerId + \" is not active, returning terminated error\");\n      return ExitCode.TERMINATED.getExitCode();\n    }\n\n    String pid = null;\n    pid = ProcessIdFileReader.getProcessId(pidPath);\n    if (pid == null) {\n      throw new IOException(\"Unable to determine pid for \" + containerId);\n    }\n\n    LOG.info(\"Reacquiring \" + containerId + \" with pid \" + pid);\n    while(isContainerProcessAlive(user, pid)) {\n      Thread.sleep(1000);\n    }\n\n    // wait for exit code file to appear\n    String exitCodeFile = ContainerLaunch.getExitCodeFile(pidPath.toString());\n    File file = new File(exitCodeFile);\n    final int sleepMsec = 100;\n    int msecLeft = 2000;\n    while (!file.exists() && msecLeft >= 0) {\n      if (!isContainerActive(containerId)) {\n        LOG.info(containerId + \" was deactivated\");\n        return ExitCode.TERMINATED.getExitCode();\n      }\n      \n      Thread.sleep(sleepMsec);\n      \n      msecLeft -= sleepMsec;\n    }\n    if (msecLeft < 0) {\n      throw new IOException(\"Timeout while waiting for exit code from \"\n          + containerId);\n    }\n\n    try {\n      return Integer.parseInt(FileUtils.readFileToString(file).trim());\n    } catch (NumberFormatException e) {\n      throw new IOException(\"Error parsing exit code from pid \" + pid, e);\n    }\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.RecoveredContainerLaunch.call": {
                "code_before_change": "  public Integer call() {\n    int retCode = ExitCode.LOST.getExitCode();\n    ContainerId containerId = container.getContainerId();\n    String appIdStr = ConverterUtils.toString(\n        containerId.getApplicationAttemptId().getApplicationId());\n    String containerIdStr = ConverterUtils.toString(containerId);\n\n    dispatcher.getEventHandler().handle(new ContainerEvent(containerId,\n        ContainerEventType.CONTAINER_LAUNCHED));\n\n    try {\n      File pidFile = locatePidFile(appIdStr, containerIdStr);\n      if (pidFile != null) {\n        String pidPathStr = pidFile.getPath();\n        pidFilePath = new Path(pidPathStr);\n        exec.activateContainer(containerId, pidFilePath);\n        retCode = exec.reacquireContainer(container.getUser(), containerId);\n      } else {\n        LOG.warn(\"Unable to locate pid file for container \" + containerIdStr);\n      }\n    } catch (IOException e) {\n        LOG.error(\"Unable to recover container \" + containerIdStr, e);\n    } finally {\n      this.completed.set(true);\n      exec.deactivateContainer(containerId);\n      try {\n        getContext().getNMStateStore().storeContainerCompleted(containerId,\n            retCode);\n      } catch (IOException e) {\n        LOG.error(\"Unable to set exit code for container \" + containerId);\n      }\n    }\n\n    if (retCode != 0) {\n      LOG.warn(\"Recovered container exited with a non-zero exit code \"\n          + retCode);\n      this.dispatcher.getEventHandler().handle(new ContainerExitEvent(\n          containerId,\n          ContainerEventType.CONTAINER_EXITED_WITH_FAILURE, retCode,\n          \"Container exited with a non-zero exit code \" + retCode));\n      return retCode;\n    }\n\n    LOG.info(\"Recovered container \" + containerId + \" succeeded\");\n    dispatcher.getEventHandler().handle(\n        new ContainerEvent(containerId,\n            ContainerEventType.CONTAINER_EXITED_WITH_SUCCESS));\n    return 0;\n  }",
                "code_after_change": "  public Integer call() {\n    int retCode = ExitCode.LOST.getExitCode();\n    ContainerId containerId = container.getContainerId();\n    String appIdStr = ConverterUtils.toString(\n        containerId.getApplicationAttemptId().getApplicationId());\n    String containerIdStr = ConverterUtils.toString(containerId);\n\n    dispatcher.getEventHandler().handle(new ContainerEvent(containerId,\n        ContainerEventType.CONTAINER_LAUNCHED));\n\n    boolean notInterrupted = true;\n    try {\n      File pidFile = locatePidFile(appIdStr, containerIdStr);\n      if (pidFile != null) {\n        String pidPathStr = pidFile.getPath();\n        pidFilePath = new Path(pidPathStr);\n        exec.activateContainer(containerId, pidFilePath);\n        retCode = exec.reacquireContainer(container.getUser(), containerId);\n      } else {\n        LOG.warn(\"Unable to locate pid file for container \" + containerIdStr);\n      }\n    } catch (IOException e) {\n        LOG.error(\"Unable to recover container \" + containerIdStr, e);\n    } catch (InterruptedException e) {\n      LOG.warn(\"Interrupted while waiting for exit code from \" + containerId);\n      notInterrupted = false;\n    } finally {\n      if (notInterrupted) {\n        this.completed.set(true);\n        exec.deactivateContainer(containerId);\n        try {\n          getContext().getNMStateStore().storeContainerCompleted(containerId,\n              retCode);\n        } catch (IOException e) {\n          LOG.error(\"Unable to set exit code for container \" + containerId);\n        }\n      }\n    }\n\n    if (retCode != 0) {\n      LOG.warn(\"Recovered container exited with a non-zero exit code \"\n          + retCode);\n      this.dispatcher.getEventHandler().handle(new ContainerExitEvent(\n          containerId,\n          ContainerEventType.CONTAINER_EXITED_WITH_FAILURE, retCode,\n          \"Container exited with a non-zero exit code \" + retCode));\n      return retCode;\n    }\n\n    LOG.info(\"Recovered container \" + containerId + \" succeeded\");\n    dispatcher.getEventHandler().handle(\n        new ContainerEvent(containerId,\n            ContainerEventType.CONTAINER_EXITED_WITH_SUCCESS));\n    return 0;\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer": {
                "code_before_change": "  public int launchContainer(Container container,\n      Path nmPrivateCotainerScriptPath, Path nmPrivateTokensPath,\n      String user, String appId, Path containerWorkDir,\n      List<String> localDirs, List<String> logDirs) throws IOException {\n\n    verifyUsernamePattern(user);\n    String runAsUser = getRunAsUser(user);\n\n    ContainerId containerId = container.getContainerId();\n    String containerIdStr = ConverterUtils.toString(containerId);\n    \n    resourcesHandler.preExecute(containerId,\n            container.getResource());\n    String resourcesOptions = resourcesHandler.getResourcesOption(\n            containerId);\n\n    ShellCommandExecutor shExec = null;\n\n    try {\n      Path pidFilePath = getPidFilePath(containerId);\n      if (pidFilePath != null) {\n        List<String> command = new ArrayList<String>();\n        addSchedPriorityCommand(command);\n        command.addAll(Arrays.asList(\n            containerExecutorExe, runAsUser, user, Integer\n                .toString(Commands.LAUNCH_CONTAINER.getValue()), appId,\n            containerIdStr, containerWorkDir.toString(),\n            nmPrivateCotainerScriptPath.toUri().getPath().toString(),\n            nmPrivateTokensPath.toUri().getPath().toString(),\n            pidFilePath.toString(),\n            StringUtils.join(\",\", localDirs),\n            StringUtils.join(\",\", logDirs),\n            resourcesOptions));\n        String[] commandArray = command.toArray(new String[command.size()]);\n        shExec = new ShellCommandExecutor(commandArray, null, // NM's cwd\n            container.getLaunchContext().getEnvironment()); // sanitized env\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"launchContainer: \" + Arrays.toString(commandArray));\n        }\n        shExec.execute();\n        if (LOG.isDebugEnabled()) {\n          logOutput(shExec.getOutput());\n        }\n      } else {\n        LOG.info(\"Container was marked as inactive. Returning terminated error\");\n        return ExitCode.TERMINATED.getExitCode();\n      }\n    } catch (ExitCodeException e) {\n      if (null == shExec) {\n        return -1;\n      }\n      int exitCode = shExec.getExitCode();\n      LOG.warn(\"Exit code from container \" + containerId + \" is : \" + exitCode);\n      // 143 (SIGTERM) and 137 (SIGKILL) exit codes means the container was\n      // terminated/killed forcefully. In all other cases, log the\n      // container-executor's output\n      if (exitCode != ExitCode.FORCE_KILLED.getExitCode()\n          && exitCode != ExitCode.TERMINATED.getExitCode()) {\n        LOG.warn(\"Exception from container-launch with container ID: \"\n            + containerId + \" and exit code: \" + exitCode , e);\n\n        StringBuilder builder = new StringBuilder();\n        builder.append(\"Exception from container-launch.\\n\");\n        builder.append(\"Container id: \" + containerId + \"\\n\");\n        builder.append(\"Exit code: \" + exitCode + \"\\n\");\n        if (!Optional.fromNullable(e.getMessage()).or(\"\").isEmpty()) {\n          builder.append(\"Exception message: \" + e.getMessage() + \"\\n\");\n        }\n        builder.append(\"Stack trace: \"\n            + StringUtils.stringifyException(e) + \"\\n\");\n        if (!shExec.getOutput().isEmpty()) {\n          builder.append(\"Shell output: \" + shExec.getOutput() + \"\\n\");\n        }\n        String diagnostics = builder.toString();\n        logOutput(diagnostics);\n        container.handle(new ContainerDiagnosticsUpdateEvent(containerId,\n            diagnostics));\n      } else {\n        container.handle(new ContainerDiagnosticsUpdateEvent(containerId,\n            \"Container killed on request. Exit code is \" + exitCode));\n      }\n      return exitCode;\n    } finally {\n      resourcesHandler.postExecute(containerId);\n    }\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Output from LinuxContainerExecutor's launchContainer follows:\");\n      logOutput(shExec.getOutput());\n    }\n    return 0;\n  }",
                "code_after_change": "  public int launchContainer(Container container,\n      Path nmPrivateCotainerScriptPath, Path nmPrivateTokensPath,\n      String user, String appId, Path containerWorkDir,\n      List<String> localDirs, List<String> logDirs) throws IOException {\n\n    verifyUsernamePattern(user);\n    String runAsUser = getRunAsUser(user);\n\n    ContainerId containerId = container.getContainerId();\n    String containerIdStr = ConverterUtils.toString(containerId);\n    \n    resourcesHandler.preExecute(containerId,\n            container.getResource());\n    String resourcesOptions = resourcesHandler.getResourcesOption(\n            containerId);\n\n    ShellCommandExecutor shExec = null;\n\n    try {\n      Path pidFilePath = getPidFilePath(containerId);\n      if (pidFilePath != null) {\n        List<String> command = new ArrayList<String>();\n        addSchedPriorityCommand(command);\n        command.addAll(Arrays.asList(\n            containerExecutorExe, runAsUser, user, Integer\n                .toString(Commands.LAUNCH_CONTAINER.getValue()), appId,\n            containerIdStr, containerWorkDir.toString(),\n            nmPrivateCotainerScriptPath.toUri().getPath().toString(),\n            nmPrivateTokensPath.toUri().getPath().toString(),\n            pidFilePath.toString(),\n            StringUtils.join(\",\", localDirs),\n            StringUtils.join(\",\", logDirs),\n            resourcesOptions));\n        String[] commandArray = command.toArray(new String[command.size()]);\n        shExec = new ShellCommandExecutor(commandArray, null, // NM's cwd\n            container.getLaunchContext().getEnvironment()); // sanitized env\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"launchContainer: \" + Arrays.toString(commandArray));\n        }\n        shExec.execute();\n        if (LOG.isDebugEnabled()) {\n          logOutput(shExec.getOutput());\n        }\n      } else {\n        LOG.info(\"Container was marked as inactive. Returning terminated error\");\n        return ExitCode.TERMINATED.getExitCode();\n      }\n    } catch (ExitCodeException e) {\n      if (null == shExec) {\n        return -1;\n      }\n      int exitCode = shExec.getExitCode();\n      LOG.warn(\"Exit code from container \" + containerId + \" is : \" + exitCode);\n      // 143 (SIGTERM) and 137 (SIGKILL) exit codes means the container was\n      // terminated/killed forcefully. In all other cases, log the\n      // container-executor's output\n      if (exitCode != ExitCode.FORCE_KILLED.getExitCode()\n          && exitCode != ExitCode.TERMINATED.getExitCode()) {\n        LOG.warn(\"Exception from container-launch with container ID: \"\n            + containerId + \" and exit code: \" + exitCode , e);\n\n        StringBuilder builder = new StringBuilder();\n        builder.append(\"Exception from container-launch.\\n\");\n        builder.append(\"Container id: \" + containerId + \"\\n\");\n        builder.append(\"Exit code: \" + exitCode + \"\\n\");\n        if (!Optional.fromNullable(e.getMessage()).or(\"\").isEmpty()) {\n          builder.append(\"Exception message: \" + e.getMessage() + \"\\n\");\n        }\n        builder.append(\"Stack trace: \"\n            + StringUtils.stringifyException(e) + \"\\n\");\n        if (!shExec.getOutput().isEmpty()) {\n          builder.append(\"Shell output: \" + shExec.getOutput() + \"\\n\");\n        }\n        String diagnostics = builder.toString();\n        logOutput(diagnostics);\n        container.handle(new ContainerDiagnosticsUpdateEvent(containerId,\n            diagnostics));\n      } else {\n        container.handle(new ContainerDiagnosticsUpdateEvent(containerId,\n            \"Container killed on request. Exit code is \" + exitCode));\n      }\n      return exitCode;\n    } finally {\n      resourcesHandler.postExecute(containerId);\n    }\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Output from LinuxContainerExecutor's launchContainer follows:\");\n      logOutput(shExec.getOutput());\n    }\n    return 0;\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Correct",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report precisely identifies the root cause as an `InterruptedException` in the `reacquireContainer()` method of the `ContainerExecutor` class, which matches the ground truth method. The fix suggestion in the report is to modify the `reacquireContainer()` method to handle interruptions, which aligns with the developer's fix of removing the catch block for `InterruptedException` and allowing it to propagate. The problem location is precisely identified as it mentions the `reacquireContainer` method, which is part of the ground truth methods. There is no wrong information in the bug report as all details are consistent with the context of the bug."
        }
    },
    {
        "filename": "YARN-7890.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerRelaunch.call": {
                "code_before_change": "  public Integer call() {\n    if (!validateContainerState()) {\n      return 0;\n    }\n\n    ContainerId containerId = container.getContainerId();\n    String containerIdStr = containerId.toString();\n    int ret = -1;\n    Path containerLogDir;\n    try {\n      Path containerWorkDir = getContainerWorkDir();\n      cleanupPreviousContainerFiles(containerWorkDir);\n\n      containerLogDir = getContainerLogDir();\n\n      Map<Path, List<String>> localResources = getLocalizedResources();\n\n      String appIdStr = app.getAppId().toString();\n      Path nmPrivateContainerScriptPath =\n          getNmPrivateContainerScriptPath(appIdStr, containerIdStr);\n      Path nmPrivateTokensPath =\n          getNmPrivateTokensPath(appIdStr, containerIdStr);\n      pidFilePath = getPidFilePath(appIdStr, containerIdStr);\n\n      LOG.info(\"Relaunch container with \"\n          + \"workDir = \" + containerWorkDir.toString()\n          + \", logDir = \" + containerLogDir.toString()\n          + \", nmPrivateContainerScriptPath = \"\n          + nmPrivateContainerScriptPath.toString()\n          + \", nmPrivateTokensPath = \" + nmPrivateTokensPath.toString()\n          + \", pidFilePath = \" + pidFilePath.toString());\n\n      List<String> localDirs = dirsHandler.getLocalDirs();\n      List<String> logDirs = dirsHandler.getLogDirs();\n      List<String> containerLocalDirs = getContainerLocalDirs(localDirs);\n      List<String> containerLogDirs = getContainerLogDirs(logDirs);\n\n      if (!dirsHandler.areDisksHealthy()) {\n        ret = ContainerExitStatus.DISKS_FAILED;\n        throw new IOException(\"Most of the disks failed. \"\n            + dirsHandler.getDisksHealthReport(false));\n      }\n\n      ret = launchContainer(new ContainerStartContext.Builder()\n          .setContainer(container)\n          .setLocalizedResources(localResources)\n          .setNmPrivateContainerScriptPath(nmPrivateContainerScriptPath)\n          .setNmPrivateTokensPath(nmPrivateTokensPath)\n          .setUser(container.getUser())\n          .setAppId(appIdStr)\n          .setContainerWorkDir(containerWorkDir)\n          .setLocalDirs(localDirs)\n          .setLogDirs(logDirs)\n          .setContainerLocalDirs(containerLocalDirs)\n          .setContainerLogDirs(containerLogDirs)\n          .build());\n    } catch (ConfigurationException e) {\n      LOG.error(\"Failed to launch container due to configuration error.\", e);\n      dispatcher.getEventHandler().handle(new ContainerExitEvent(\n          containerId, ContainerEventType.CONTAINER_EXITED_WITH_FAILURE, ret,\n          e.getMessage()));\n      // Mark the node as unhealthy\n      getContext().getNodeStatusUpdater().reportException(e);\n      return ret;\n    } catch (Throwable e) {\n      LOG.warn(\"Failed to relaunch container.\", e);\n      dispatcher.getEventHandler().handle(new ContainerExitEvent(\n          containerId, ContainerEventType.CONTAINER_EXITED_WITH_FAILURE, ret,\n          e.getMessage()));\n      return ret;\n    } finally {\n      setContainerCompletedStatus(ret);\n    }\n\n    handleContainerExitCode(ret, containerLogDir);\n\n    return ret;\n  }",
                "code_after_change": "  public Integer call() {\n    if (!validateContainerState()) {\n      return 0;\n    }\n\n    ContainerId containerId = container.getContainerId();\n    String containerIdStr = containerId.toString();\n    int ret = -1;\n    Path containerLogDir;\n    try {\n      Path containerWorkDir = getContainerWorkDir();\n      cleanupPreviousContainerFiles(containerWorkDir);\n\n      containerLogDir = getContainerLogDir();\n\n      Map<Path, List<String>> localResources = getLocalizedResources();\n\n      String appIdStr = app.getAppId().toString();\n      Path nmPrivateContainerScriptPath =\n          getNmPrivateContainerScriptPath(appIdStr, containerIdStr);\n      Path nmPrivateTokensPath =\n          getNmPrivateTokensPath(appIdStr, containerIdStr);\n      pidFilePath = getPidFilePath(appIdStr, containerIdStr);\n\n      LOG.info(\"Relaunch container with \"\n          + \"workDir = \" + containerWorkDir.toString()\n          + \", logDir = \" + containerLogDir.toString()\n          + \", nmPrivateContainerScriptPath = \"\n          + nmPrivateContainerScriptPath.toString()\n          + \", nmPrivateTokensPath = \" + nmPrivateTokensPath.toString()\n          + \", pidFilePath = \" + pidFilePath.toString());\n\n      List<String> localDirs = dirsHandler.getLocalDirs();\n      List<String> logDirs = dirsHandler.getLogDirs();\n      List<String> containerLocalDirs = getContainerLocalDirs(localDirs);\n      List<String> containerLogDirs = getContainerLogDirs(logDirs);\n      List<String> filecacheDirs = getNMFilecacheDirs(localDirs);\n      List<String> userLocalDirs = getUserLocalDirs(localDirs);\n\n      if (!dirsHandler.areDisksHealthy()) {\n        ret = ContainerExitStatus.DISKS_FAILED;\n        throw new IOException(\"Most of the disks failed. \"\n            + dirsHandler.getDisksHealthReport(false));\n      }\n\n      ret = launchContainer(new ContainerStartContext.Builder()\n          .setContainer(container)\n          .setLocalizedResources(localResources)\n          .setNmPrivateContainerScriptPath(nmPrivateContainerScriptPath)\n          .setNmPrivateTokensPath(nmPrivateTokensPath)\n          .setUser(container.getUser())\n          .setAppId(appIdStr)\n          .setContainerWorkDir(containerWorkDir)\n          .setLocalDirs(localDirs)\n          .setLogDirs(logDirs)\n          .setFilecacheDirs(filecacheDirs)\n          .setUserLocalDirs(userLocalDirs)\n          .setContainerLocalDirs(containerLocalDirs)\n          .setContainerLogDirs(containerLogDirs)\n          .build());\n    } catch (ConfigurationException e) {\n      LOG.error(\"Failed to launch container due to configuration error.\", e);\n      dispatcher.getEventHandler().handle(new ContainerExitEvent(\n          containerId, ContainerEventType.CONTAINER_EXITED_WITH_FAILURE, ret,\n          e.getMessage()));\n      // Mark the node as unhealthy\n      getContext().getNodeStatusUpdater().reportException(e);\n      return ret;\n    } catch (Throwable e) {\n      LOG.warn(\"Failed to relaunch container.\", e);\n      dispatcher.getEventHandler().handle(new ContainerExitEvent(\n          containerId, ContainerEventType.CONTAINER_EXITED_WITH_FAILURE, ret,\n          e.getMessage()));\n      return ret;\n    } finally {\n      setContainerCompletedStatus(ret);\n    }\n\n    handleContainerExitCode(ret, containerLogDir);\n\n    return ret;\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Alternative Fix",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the root cause as the `filecacheDirs` field being null in the `ContainerStartContext` class, which is mentioned in the stack trace but not the actual ground truth method. Therefore, it is classified as 'Partial' with 'Shared Stack Trace Context'. The fix suggestion involves ensuring the `setFilecacheDirs` method is called, which is an alternative approach to the actual fix that involved setting `filecacheDirs` in the `ContainerRelaunch.call` method. The problem location identification is also 'Partial' with 'Shared Stack Trace Context' because it mentions methods in the stack trace but not the ground truth method. There is no wrong information in the bug report as all details are relevant to the context of the bug."
        }
    },
    {
        "filename": "YARN-139.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.event.AsyncDispatcher.handle": {
                "code_before_change": "    public void handle(Event event) {\n      /* all this method does is enqueue all the events onto the queue */\n      int qSize = eventQueue.size();\n      if (qSize !=0 && qSize %1000 == 0) {\n        LOG.info(\"Size of event-queue is \" + qSize);\n      }\n      int remCapacity = eventQueue.remainingCapacity();\n      if (remCapacity < 1000) {\n        LOG.warn(\"Very low remaining capacity in the event-queue: \"\n            + remCapacity);\n      }\n      try {\n        eventQueue.put(event);\n      } catch (InterruptedException e) {\n        LOG.warn(\"AsyncDispatcher thread interrupted\", e);\n        throw new YarnException(e);\n      }\n    };",
                "code_after_change": "    public void handle(Event event) {\n      /* all this method does is enqueue all the events onto the queue */\n      int qSize = eventQueue.size();\n      if (qSize !=0 && qSize %1000 == 0) {\n        LOG.info(\"Size of event-queue is \" + qSize);\n      }\n      int remCapacity = eventQueue.remainingCapacity();\n      if (remCapacity < 1000) {\n        LOG.warn(\"Very low remaining capacity in the event-queue: \"\n            + remCapacity);\n      }\n      try {\n        eventQueue.put(event);\n      } catch (InterruptedException e) {\n        if (!stopped) {\n          LOG.warn(\"AsyncDispatcher thread interrupted\", e);\n        }\n        throw new YarnException(e);\n      }\n    };"
            },
            "hadoop-mapreduce-project.hadoop-mapreduce-client.hadoop-mapreduce-client-app.src.main.java.org.apache.hadoop.mapreduce.v2.app.MRAppMaster.cleanupStagingDir": {
                "code_before_change": "  public void cleanupStagingDir() throws IOException {\n    /* make sure we clean the staging files */\n    String jobTempDir = null;\n    FileSystem fs = getFileSystem(getConfig());\n    try {\n      if (!keepJobFiles(new JobConf(getConfig()))) {\n        jobTempDir = getConfig().get(MRJobConfig.MAPREDUCE_JOB_DIR);\n        if (jobTempDir == null) {\n          LOG.warn(\"Job Staging directory is null\");\n          return;\n        }\n        Path jobTempDirPath = new Path(jobTempDir);\n        LOG.info(\"Deleting staging directory \" + FileSystem.getDefaultUri(getConfig()) +\n            \" \" + jobTempDir);\n        fs.delete(jobTempDirPath, true);\n      }\n    } catch(IOException io) {\n      LOG.error(\"Failed to cleanup staging dir \" + jobTempDir, io);\n    }\n  }",
                "code_after_change": "  public void cleanupStagingDir() throws IOException {\n    /* make sure we clean the staging files */\n    String jobTempDir = null;\n    FileSystem fs = getFileSystem(getConfig());\n    try {\n      if (!keepJobFiles(new JobConf(getConfig()))) {\n        jobTempDir = getConfig().get(MRJobConfig.MAPREDUCE_JOB_DIR);\n        if (jobTempDir == null) {\n          LOG.warn(\"Job Staging directory is null\");\n          return;\n        }\n        Path jobTempDirPath = new Path(jobTempDir);\n        LOG.info(\"Deleting staging directory \" + FileSystem.getDefaultUri(getConfig()) +\n            \" \" + jobTempDir);\n        fs.delete(jobTempDirPath, true);\n      }\n    } catch(IOException io) {\n      LOG.error(\"Failed to cleanup staging dir \" + jobTempDir, io);\n    }\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.event.AsyncDispatcher.run": {
                "code_before_change": "      public void run() {\n        while (!stopped && !Thread.currentThread().isInterrupted()) {\n          Event event;\n          try {\n            event = eventQueue.take();\n          } catch(InterruptedException ie) {\n            LOG.warn(\"AsyncDispatcher thread interrupted\", ie);\n            return;\n          }\n          if (event != null) {\n            dispatch(event);\n          }\n        }\n      }",
                "code_after_change": "      public void run() {\n        while (!stopped && !Thread.currentThread().isInterrupted()) {\n          Event event;\n          try {\n            event = eventQueue.take();\n          } catch(InterruptedException ie) {\n            if (!stopped) {\n              LOG.warn(\"AsyncDispatcher thread interrupted\", ie);\n            }\n            return;\n          }\n          if (event != null) {\n            dispatch(event);\n          }\n        }\n      }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Direct Caller/Callee"
            },
            "fix_suggestion": "Preventive",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Direct Caller/Callee"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the root cause as the `InterruptedException` in the `AsyncDispatcher.stop()` method, which is a direct caller of the ground truth method `AsyncDispatcher.run`. The fix suggestion is preventive as it suggests handling the exception more gracefully, which aligns with the developer's fix of checking the `stopped` flag before logging. The problem location is partially identified as it mentions `AsyncDispatcher.stop`, which is a direct caller of the ground truth method `AsyncDispatcher.run`. There is no wrong information in the bug report as all details are relevant to the context of the bug."
        }
    },
    {
        "filename": "YARN-42.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.NonAggregatingLogHandler.stop": {
                "code_before_change": [],
                "code_after_change": []
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Preventive",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the IOException related to directory creation as the root cause, which is mentioned in the stack trace but not the actual root cause in the ground truth method. Therefore, it is classified as 'Partial' with 'Shared Stack Trace Context'. The fix suggestion is preventive as it suggests ensuring permissions and directory existence, which would prevent the IOException but does not directly address the NPE in the ground truth method. The problem location identification is 'Partial' with 'Shared Stack Trace Context' because it mentions methods in the stack trace but not the ground truth method. There is no wrong information in the bug report as all details are relevant to the context of the bug."
        }
    },
    {
        "filename": "YARN-7453.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.createAndStartZKManager": {
                "code_before_change": [],
                "code_after_change": "  public ZKCuratorManager createAndStartZKManager(Configuration\n      config) throws IOException {\n    ZKCuratorManager manager = new ZKCuratorManager(config);\n\n    // Get authentication\n    List<AuthInfo> authInfos = new ArrayList<>();\n    if (HAUtil.isHAEnabled(config) && HAUtil.getConfValueForRMInstance(\n        YarnConfiguration.ZK_RM_STATE_STORE_ROOT_NODE_ACL, config) == null) {\n      String zkRootNodeUsername = HAUtil.getConfValueForRMInstance(\n          YarnConfiguration.RM_ADDRESS,\n          YarnConfiguration.DEFAULT_RM_ADDRESS, config);\n      String defaultFencingAuth =\n          zkRootNodeUsername + \":\" + zkRootNodePassword;\n      byte[] defaultFencingAuthData =\n          defaultFencingAuth.getBytes(Charset.forName(\"UTF-8\"));\n      String scheme = new DigestAuthenticationProvider().getScheme();\n      AuthInfo authInfo = new AuthInfo(scheme, defaultFencingAuthData);\n      authInfos.add(authInfo);\n    }\n\n    manager.start(authInfos);\n    return manager;\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.conf.ZKConfigurationStore.initialize": {
                "code_before_change": "  public void initialize(Configuration config, Configuration schedConf,\n      RMContext rmContext) throws Exception {\n    this.conf = config;\n    this.maxLogs = conf.getLong(YarnConfiguration.RM_SCHEDCONF_MAX_LOGS,\n        YarnConfiguration.DEFAULT_RM_SCHEDCONF_ZK_MAX_LOGS);\n    this.znodeParentPath =\n        conf.get(YarnConfiguration.RM_SCHEDCONF_STORE_ZK_PARENT_PATH,\n            YarnConfiguration.DEFAULT_RM_SCHEDCONF_STORE_ZK_PARENT_PATH);\n    this.zkManager = rmContext.getResourceManager().getAndStartZKManager(conf);\n    this.zkAcl = ZKCuratorManager.getZKAcls(conf);\n\n    this.zkVersionPath = getNodePath(znodeParentPath, ZK_VERSION_PATH);\n    this.logsPath = getNodePath(znodeParentPath, LOGS_PATH);\n    this.confStorePath = getNodePath(znodeParentPath, CONF_STORE_PATH);\n    this.fencingNodePath = getNodePath(znodeParentPath, FENCING_PATH);\n\n    zkManager.createRootDirRecursively(znodeParentPath);\n    zkManager.delete(fencingNodePath);\n\n    if (!zkManager.exists(logsPath)) {\n      zkManager.create(logsPath);\n      zkManager.setData(logsPath,\n          serializeObject(new LinkedList<LogMutation>()), -1);\n    }\n\n    if (!zkManager.exists(confStorePath)) {\n      zkManager.create(confStorePath);\n      HashMap<String, String> mapSchedConf = new HashMap<>();\n      for (Map.Entry<String, String> entry : schedConf) {\n        mapSchedConf.put(entry.getKey(), entry.getValue());\n      }\n      zkManager.setData(confStorePath, serializeObject(mapSchedConf), -1);\n    }\n  }",
                "code_after_change": "  public void initialize(Configuration config, Configuration schedConf,\n      RMContext rmContext) throws Exception {\n    this.conf = config;\n    this.maxLogs = conf.getLong(YarnConfiguration.RM_SCHEDCONF_MAX_LOGS,\n        YarnConfiguration.DEFAULT_RM_SCHEDCONF_ZK_MAX_LOGS);\n    this.znodeParentPath =\n        conf.get(YarnConfiguration.RM_SCHEDCONF_STORE_ZK_PARENT_PATH,\n            YarnConfiguration.DEFAULT_RM_SCHEDCONF_STORE_ZK_PARENT_PATH);\n    this.zkManager =\n        rmContext.getResourceManager().createAndStartZKManager(conf);\n    this.zkAcl = ZKCuratorManager.getZKAcls(conf);\n\n    this.zkVersionPath = getNodePath(znodeParentPath, ZK_VERSION_PATH);\n    this.logsPath = getNodePath(znodeParentPath, LOGS_PATH);\n    this.confStorePath = getNodePath(znodeParentPath, CONF_STORE_PATH);\n    this.fencingNodePath = getNodePath(znodeParentPath, FENCING_PATH);\n\n    zkManager.createRootDirRecursively(znodeParentPath);\n    zkManager.delete(fencingNodePath);\n\n    if (!zkManager.exists(logsPath)) {\n      zkManager.create(logsPath);\n      zkManager.setData(logsPath,\n          serializeObject(new LinkedList<LogMutation>()), -1);\n    }\n\n    if (!zkManager.exists(confStorePath)) {\n      zkManager.create(confStorePath);\n      HashMap<String, String> mapSchedConf = new HashMap<>();\n      for (Map.Entry<String, String> entry : schedConf) {\n        mapSchedConf.put(entry.getKey(), entry.getValue());\n      }\n      zkManager.setData(confStorePath, serializeObject(mapSchedConf), -1);\n    }\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.createEmbeddedElector": {
                "code_before_change": "  protected EmbeddedElector createEmbeddedElector() throws IOException {\n    EmbeddedElector elector;\n    curatorEnabled =\n        conf.getBoolean(YarnConfiguration.CURATOR_LEADER_ELECTOR,\n            YarnConfiguration.DEFAULT_CURATOR_LEADER_ELECTOR_ENABLED);\n    if (curatorEnabled) {\n      this.zkManager = getAndStartZKManager(conf);\n      elector = new CuratorBasedElectorService(this);\n    } else {\n      elector = new ActiveStandbyElectorBasedElectorService(this);\n    }\n    return elector;\n  }",
                "code_after_change": "  protected EmbeddedElector createEmbeddedElector() throws IOException {\n    EmbeddedElector elector;\n    curatorEnabled =\n        conf.getBoolean(YarnConfiguration.CURATOR_LEADER_ELECTOR,\n            YarnConfiguration.DEFAULT_CURATOR_LEADER_ELECTOR_ENABLED);\n    if (curatorEnabled) {\n      this.zkManager = createAndStartZKManager(conf);\n      elector = new CuratorBasedElectorService(this);\n    } else {\n      elector = new ActiveStandbyElectorBasedElectorService(this);\n    }\n    return elector;\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Correct",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the root cause as a `NoAuthException` from ZooKeeper, which is related to the authentication issue but does not precisely identify the ground truth methods where the fix was applied. The methods mentioned in the `problem_location` field are part of the stack trace but are not the exact methods where the fix was made, hence classified as 'Shared Stack Trace Context'. The suggestion to ensure that the ResourceManager is configured with the correct authentication credentials for ZooKeeper matches the developer's fix, which involves setting up authentication in `createAndStartZKManager`. Thus, it is marked as 'Correct'. There is no wrong information in the bug report as all details are relevant to the context of the bug."
        }
    },
    {
        "filename": "YARN-3369.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.getResourceRequest": {
                "code_before_change": "  synchronized public ResourceRequest getResourceRequest(Priority priority,\n      String resourceName) {\n    Map<String, ResourceRequest> nodeRequests = requests.get(priority);\n    return (nodeRequests == null) ? null : nodeRequests.get(resourceName);\n  }",
                "code_after_change": "  synchronized public ResourceRequest getResourceRequest(Priority priority,\n      String resourceName) {\n    Map<String, ResourceRequest> nodeRequests = requests.get(priority);\n    return (nodeRequests == null) ? null : nodeRequests.get(resourceName);\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.checkForDeactivation": {
                "code_before_change": "  synchronized private void checkForDeactivation() {\n    boolean deactivate = true;\n    for (Priority priority : getPriorities()) {\n      ResourceRequest request = getResourceRequest(priority, ResourceRequest.ANY);\n      if (request.getNumContainers() > 0) {\n        deactivate = false;\n        break;\n      }\n    }\n    if (deactivate) {\n      activeUsersManager.deactivateApplication(user, applicationId);\n    }\n  }",
                "code_after_change": "  synchronized private void checkForDeactivation() {\n    boolean deactivate = true;\n    for (Priority priority : getPriorities()) {\n      ResourceRequest request = getResourceRequest(priority, ResourceRequest.ANY);\n      if (request != null) {\n        if (request.getNumContainers() > 0) {\n          deactivate = false;\n          break;\n        }\n      }\n    }\n    if (deactivate) {\n      activeUsersManager.deactivateApplication(user, applicationId);\n    }\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.getResource": {
                "code_before_change": "  public synchronized Resource getResource(Priority priority) {\n    ResourceRequest request = getResourceRequest(priority, ResourceRequest.ANY);\n    return request.getCapability();\n  }",
                "code_after_change": "  public synchronized Resource getResource(Priority priority) {\n    ResourceRequest request = getResourceRequest(priority, ResourceRequest.ANY);\n    return (request == null) ? null : request.getCapability();\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Correct",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report precisely identifies the root cause of the issue as a `NullPointerException` in the `checkForDeactivation()` method due to a null `ResourceRequest` object returned by `getResourceRequest()`. This matches the ground truth methods. The fix suggestion in the bug report is correct as it matches the developer's fix, which involves adding a null check in the `checkForDeactivation()` method. The problem location is also precisely identified, as the report mentions the `checkForDeactivation`, `getResourceRequest`, and `getPriorities` methods, which are part of the ground truth. There is no wrong information in the bug report; all details are accurate and relevant to the bug."
        }
    },
    {
        "filename": "YARN-945.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.transition": {
                "code_before_change": "    public void transition(RMAppAttemptImpl appAttempt,\n        RMAppAttemptEvent event) {\n    }",
                "code_after_change": "    public void transition(RMAppAttemptImpl appAttempt,\n        RMAppAttemptEvent event) {\n    }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-client.src.main.java.org.apache.hadoop.yarn.client.ClientRMProxy.getRMAddress": {
                "code_before_change": "  private static InetSocketAddress getRMAddress(Configuration conf, Class<?> protocol) {\n    if (protocol == ApplicationClientProtocol.class) {\n      return conf.getSocketAddr(YarnConfiguration.RM_ADDRESS,\n          YarnConfiguration.DEFAULT_RM_ADDRESS,\n          YarnConfiguration.DEFAULT_RM_PORT);\n    } else if (protocol == ResourceManagerAdministrationProtocol.class) {\n      return conf.getSocketAddr(\n          YarnConfiguration.RM_ADMIN_ADDRESS,\n          YarnConfiguration.DEFAULT_RM_ADMIN_ADDRESS,\n          YarnConfiguration.DEFAULT_RM_ADMIN_PORT);\n    } else if (protocol == ApplicationMasterProtocol.class) {\n      return conf.getSocketAddr(\n          YarnConfiguration.RM_SCHEDULER_ADDRESS,\n          YarnConfiguration.DEFAULT_RM_SCHEDULER_ADDRESS,\n          YarnConfiguration.DEFAULT_RM_SCHEDULER_PORT);\n    } else {\n      String message = \"Unsupported protocol found when creating the proxy \" +\n          \"connection to ResourceManager: \" +\n          ((protocol != null) ? protocol.getClass().getName() : \"null\");\n      LOG.error(message);\n      throw new IllegalStateException(message);\n    }\n  }",
                "code_after_change": "  private static InetSocketAddress getRMAddress(Configuration conf,\n      Class<?> protocol) throws IOException {\n    if (protocol == ApplicationClientProtocol.class) {\n      return conf.getSocketAddr(YarnConfiguration.RM_ADDRESS,\n          YarnConfiguration.DEFAULT_RM_ADDRESS,\n          YarnConfiguration.DEFAULT_RM_PORT);\n    } else if (protocol == ResourceManagerAdministrationProtocol.class) {\n      return conf.getSocketAddr(\n          YarnConfiguration.RM_ADMIN_ADDRESS,\n          YarnConfiguration.DEFAULT_RM_ADMIN_ADDRESS,\n          YarnConfiguration.DEFAULT_RM_ADMIN_PORT);\n    } else if (protocol == ApplicationMasterProtocol.class) {\n      InetSocketAddress serviceAddr =\n          conf.getSocketAddr(YarnConfiguration.RM_SCHEDULER_ADDRESS,\n            YarnConfiguration.DEFAULT_RM_SCHEDULER_ADDRESS,\n            YarnConfiguration.DEFAULT_RM_SCHEDULER_PORT);\n      setupTokens(serviceAddr);\n      return serviceAddr;\n    } else {\n      String message = \"Unsupported protocol found when creating the proxy \" +\n          \"connection to ResourceManager: \" +\n          ((protocol != null) ? protocol.getClass().getName() : \"null\");\n      LOG.error(message);\n      throw new IllegalStateException(message);\n    }\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-common.src.main.java.org.apache.hadoop.yarn.server.api.ServerRMProxy.getRMAddress": {
                "code_before_change": "  private static InetSocketAddress getRMAddress(Configuration conf, Class<?> protocol) {\n    if (protocol == ResourceTracker.class) {\n      return conf.getSocketAddr(\n        YarnConfiguration.RM_RESOURCE_TRACKER_ADDRESS,\n        YarnConfiguration.DEFAULT_RM_RESOURCE_TRACKER_ADDRESS,\n        YarnConfiguration.DEFAULT_RM_RESOURCE_TRACKER_PORT);\n    }\n    else {\n      String message = \"Unsupported protocol found when creating the proxy \" +\n          \"connection to ResourceManager: \" +\n          ((protocol != null) ? protocol.getClass().getName() : \"null\");\n      LOG.error(message);\n      throw new IllegalStateException(message);\n    }\n  }",
                "code_after_change": "  private static InetSocketAddress getRMAddress(Configuration conf, Class<?> protocol) {\n    if (protocol == ResourceTracker.class) {\n      return conf.getSocketAddr(\n        YarnConfiguration.RM_RESOURCE_TRACKER_ADDRESS,\n        YarnConfiguration.DEFAULT_RM_RESOURCE_TRACKER_ADDRESS,\n        YarnConfiguration.DEFAULT_RM_RESOURCE_TRACKER_PORT);\n    } else {\n      String message = \"Unsupported protocol found when creating the proxy \" +\n          \"connection to ResourceManager: \" +\n          ((protocol != null) ? protocol.getClass().getName() : \"null\");\n      LOG.error(message);\n      throw new IllegalStateException(message);\n    }\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-applications.hadoop-yarn-applications-unmanaged-am-launcher.src.main.java.org.apache.hadoop.yarn.applications.unmanagedamlauncher.UnmanagedAMLauncher.launchAM": {
                "code_before_change": "  public void launchAM(ApplicationAttemptId attemptId) throws IOException {\n    Map<String, String> env = System.getenv();\n    ArrayList<String> envAMList = new ArrayList<String>();\n    boolean setClasspath = false;\n    for (Map.Entry<String, String> entry : env.entrySet()) {\n      String key = entry.getKey();\n      String value = entry.getValue();\n      if(key.equals(\"CLASSPATH\")) {\n        setClasspath = true;\n        if(classpath != null) {\n          value = value + File.pathSeparator + classpath;\n        }\n      }\n      envAMList.add(key + \"=\" + value);\n    }\n    \n    if(!setClasspath && classpath!=null) {\n      envAMList.add(\"CLASSPATH=\"+classpath);\n    }\n    ContainerId containerId = ContainerId.newInstance(attemptId, 0);\n\n    String hostname = InetAddress.getLocalHost().getHostName();\n    envAMList.add(Environment.CONTAINER_ID.name() + \"=\" + containerId);\n    envAMList.add(Environment.NM_HOST.name() + \"=\" + hostname);\n    envAMList.add(Environment.NM_HTTP_PORT.name() + \"=0\");\n    envAMList.add(Environment.NM_PORT.name() + \"=0\");\n    envAMList.add(Environment.LOCAL_DIRS.name() + \"= /tmp\");\n    envAMList.add(ApplicationConstants.APP_SUBMIT_TIME_ENV + \"=\"\n        + System.currentTimeMillis());\n\n    String[] envAM = new String[envAMList.size()];\n    Process amProc = Runtime.getRuntime().exec(amCmd, envAMList.toArray(envAM));\n\n    final BufferedReader errReader = \n        new BufferedReader(new InputStreamReader(amProc\n                                                 .getErrorStream()));\n    final BufferedReader inReader = \n        new BufferedReader(new InputStreamReader(amProc\n                                                 .getInputStream()));\n    \n    // read error and input streams as this would free up the buffers\n    // free the error stream buffer\n    Thread errThread = new Thread() {\n      @Override\n      public void run() {\n        try {\n          String line = errReader.readLine();\n          while((line != null) && !isInterrupted()) {\n            System.err.println(line);\n            line = errReader.readLine();\n          }\n        } catch(IOException ioe) {\n          LOG.warn(\"Error reading the error stream\", ioe);\n        }\n      }\n    };\n    Thread outThread = new Thread() {\n      @Override\n      public void run() {\n        try {\n          String line = inReader.readLine();\n          while((line != null) && !isInterrupted()) {\n            System.out.println(line);\n            line = inReader.readLine();\n          }\n        } catch(IOException ioe) {\n          LOG.warn(\"Error reading the out stream\", ioe);\n        }\n      }\n    };\n    try {\n      errThread.start();\n      outThread.start();\n    } catch (IllegalStateException ise) { }\n\n    // wait for the process to finish and check the exit code\n    try {\n      int exitCode = amProc.waitFor();\n      LOG.info(\"AM process exited with value: \" + exitCode);\n    } catch (InterruptedException e) {\n      e.printStackTrace();\n    } finally {\n      amCompleted = true;\n    }\n    \n    try {\n      // make sure that the error thread exits\n      // on Windows these threads sometimes get stuck and hang the execution\n      // timeout and join later after destroying the process.\n      errThread.join();\n      outThread.join();\n      errReader.close();\n      inReader.close();\n    } catch (InterruptedException ie) {\n      LOG.info(\"ShellExecutor: Interrupted while reading the error/out stream\",\n          ie);\n    } catch (IOException ioe) {\n      LOG.warn(\"Error while closing the error/out stream\", ioe);\n    }\n    amProc.destroy();\n  }",
                "code_after_change": "  public void launchAM(ApplicationAttemptId attemptId) \n    throws IOException, YarnException {\n    ApplicationReport report = \n      rmClient.getApplicationReport(attemptId.getApplicationId());\n    if (report.getYarnApplicationState() != YarnApplicationState.ACCEPTED) {\n      throw new YarnException(\n          \"Umanaged AM must be in ACCEPTED state before launching\");\n    }\n    Credentials credentials = new Credentials();\n    Token<AMRMTokenIdentifier> token = \n        rmClient.getAMRMToken(attemptId.getApplicationId());\n    // Service will be empty but that's okay, we are just passing down only\n    // AMRMToken down to the real AM which eventually sets the correct\n    // service-address.\n    credentials.addToken(token.getService(), token);\n    File tokenFile = File.createTempFile(\"unmanagedAMRMToken\",\"\", \n        new File(System.getProperty(\"user.dir\")));\n    try {\n      FileUtil.chmod(tokenFile.getAbsolutePath(), \"600\");\n    } catch (InterruptedException ex) {\n      throw new RuntimeException(ex);\n    }\n    tokenFile.deleteOnExit();\n    DataOutputStream os = new DataOutputStream(new FileOutputStream(tokenFile, \n        true));\n    credentials.writeTokenStorageToStream(os);\n    os.close();\n    \n    Map<String, String> env = System.getenv();\n    ArrayList<String> envAMList = new ArrayList<String>();\n    boolean setClasspath = false;\n    for (Map.Entry<String, String> entry : env.entrySet()) {\n      String key = entry.getKey();\n      String value = entry.getValue();\n      if(key.equals(\"CLASSPATH\")) {\n        setClasspath = true;\n        if(classpath != null) {\n          value = value + File.pathSeparator + classpath;\n        }\n      }\n      envAMList.add(key + \"=\" + value);\n    }\n    \n    if(!setClasspath && classpath!=null) {\n      envAMList.add(\"CLASSPATH=\"+classpath);\n    }\n    ContainerId containerId = ContainerId.newInstance(attemptId, 0);\n\n    String hostname = InetAddress.getLocalHost().getHostName();\n    envAMList.add(Environment.CONTAINER_ID.name() + \"=\" + containerId);\n    envAMList.add(Environment.NM_HOST.name() + \"=\" + hostname);\n    envAMList.add(Environment.NM_HTTP_PORT.name() + \"=0\");\n    envAMList.add(Environment.NM_PORT.name() + \"=0\");\n    envAMList.add(Environment.LOCAL_DIRS.name() + \"= /tmp\");\n    envAMList.add(ApplicationConstants.APP_SUBMIT_TIME_ENV + \"=\"\n        + System.currentTimeMillis());\n\n    envAMList.add(ApplicationConstants.CONTAINER_TOKEN_FILE_ENV_NAME + \"=\" + \n      tokenFile.getAbsolutePath());\n    \n    String[] envAM = new String[envAMList.size()];\n    Process amProc = Runtime.getRuntime().exec(amCmd, envAMList.toArray(envAM));\n\n    final BufferedReader errReader = \n        new BufferedReader(new InputStreamReader(amProc\n                                                 .getErrorStream()));\n    final BufferedReader inReader = \n        new BufferedReader(new InputStreamReader(amProc\n                                                 .getInputStream()));\n    \n    // read error and input streams as this would free up the buffers\n    // free the error stream buffer\n    Thread errThread = new Thread() {\n      @Override\n      public void run() {\n        try {\n          String line = errReader.readLine();\n          while((line != null) && !isInterrupted()) {\n            System.err.println(line);\n            line = errReader.readLine();\n          }\n        } catch(IOException ioe) {\n          LOG.warn(\"Error reading the error stream\", ioe);\n        }\n      }\n    };\n    Thread outThread = new Thread() {\n      @Override\n      public void run() {\n        try {\n          String line = inReader.readLine();\n          while((line != null) && !isInterrupted()) {\n            System.out.println(line);\n            line = inReader.readLine();\n          }\n        } catch(IOException ioe) {\n          LOG.warn(\"Error reading the out stream\", ioe);\n        }\n      }\n    };\n    try {\n      errThread.start();\n      outThread.start();\n    } catch (IllegalStateException ise) { }\n\n    // wait for the process to finish and check the exit code\n    try {\n      int exitCode = amProc.waitFor();\n      LOG.info(\"AM process exited with value: \" + exitCode);\n    } catch (InterruptedException e) {\n      e.printStackTrace();\n    } finally {\n      amCompleted = true;\n    }\n    \n    try {\n      // make sure that the error thread exits\n      // on Windows these threads sometimes get stuck and hang the execution\n      // timeout and join later after destroying the process.\n      errThread.join();\n      outThread.join();\n      errReader.close();\n      inReader.close();\n    } catch (InterruptedException ie) {\n      LOG.info(\"ShellExecutor: Interrupted while reading the error/out stream\",\n          ie);\n    } catch (IOException ioe) {\n      LOG.warn(\"Error while closing the error/out stream\", ioe);\n    }\n    amProc.destroy();\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.amlauncher.AMLauncher.setupTokens": {
                "code_before_change": "  private void setupTokens(\n      ContainerLaunchContext container, ContainerId containerID)\n      throws IOException {\n    Map<String, String> environment = container.getEnvironment();\n    environment.put(ApplicationConstants.APPLICATION_WEB_PROXY_BASE_ENV,\n        application.getWebProxyBase());\n    // Set AppSubmitTime and MaxAppAttempts to be consumable by the AM.\n    ApplicationId applicationId =\n        application.getAppAttemptId().getApplicationId();\n    environment.put(\n        ApplicationConstants.APP_SUBMIT_TIME_ENV,\n        String.valueOf(rmContext.getRMApps()\n            .get(applicationId)\n            .getSubmitTime()));\n    environment.put(ApplicationConstants.MAX_APP_ATTEMPTS_ENV,\n        String.valueOf(rmContext.getRMApps().get(\n            applicationId).getMaxAppAttempts()));\n\n    Credentials credentials = new Credentials();\n    \n    if (UserGroupInformation.isSecurityEnabled()) {\n      // TODO: Security enabled/disabled info should come from RM.\n\n      DataInputByteBuffer dibb = new DataInputByteBuffer();\n      if (container.getTokens() != null) {\n        // TODO: Don't do this kind of checks everywhere.\n        dibb.reset(container.getTokens());\n        credentials.readTokenStorageStream(dibb);\n      }\n    }\n\n    // Add AMRMToken\n    Token<AMRMTokenIdentifier> amrmToken = application.getAMRMToken();\n    if (amrmToken != null) {\n      credentials.addToken(amrmToken.getService(), amrmToken);\n    }\n    DataOutputBuffer dob = new DataOutputBuffer();\n    credentials.writeTokenStorageToStream(dob);\n    container.setTokens(ByteBuffer.wrap(dob.getData(), 0, dob.getLength()));\n  }",
                "code_after_change": "  private void setupTokens(\n      ContainerLaunchContext container, ContainerId containerID)\n      throws IOException {\n    Map<String, String> environment = container.getEnvironment();\n    environment.put(ApplicationConstants.APPLICATION_WEB_PROXY_BASE_ENV,\n        application.getWebProxyBase());\n    // Set AppSubmitTime and MaxAppAttempts to be consumable by the AM.\n    ApplicationId applicationId =\n        application.getAppAttemptId().getApplicationId();\n    environment.put(\n        ApplicationConstants.APP_SUBMIT_TIME_ENV,\n        String.valueOf(rmContext.getRMApps()\n            .get(applicationId)\n            .getSubmitTime()));\n    environment.put(ApplicationConstants.MAX_APP_ATTEMPTS_ENV,\n        String.valueOf(rmContext.getRMApps().get(\n            applicationId).getMaxAppAttempts()));\n\n    Credentials credentials = new Credentials();\n    DataInputByteBuffer dibb = new DataInputByteBuffer();\n    if (container.getTokens() != null) {\n      // TODO: Don't do this kind of checks everywhere.\n      dibb.reset(container.getTokens());\n      credentials.readTokenStorageStream(dibb);\n    }\n\n    // Add AMRMToken\n    Token<AMRMTokenIdentifier> amrmToken = getAMRMToken();\n    if (amrmToken != null) {\n      credentials.addToken(amrmToken.getService(), amrmToken);\n    }\n    DataOutputBuffer dob = new DataOutputBuffer();\n    credentials.writeTokenStorageToStream(dob);\n    container.setTokens(ByteBuffer.wrap(dob.getData(), 0, dob.getLength()));\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.client.RMProxy.createRMProxy": {
                "code_before_change": "  public static <T> T createRMProxy(final Configuration conf,\n      final Class<T> protocol, InetSocketAddress rmAddress) throws IOException {\n    RetryPolicy retryPolicy = createRetryPolicy(conf);\n    T proxy = RMProxy.<T>getProxy(conf, protocol, rmAddress);\n    LOG.info(\"Connecting to ResourceManager at \" + rmAddress);\n    return (T) RetryProxy.create(protocol, proxy, retryPolicy);\n  }",
                "code_after_change": "  public static <T> T createRMProxy(final Configuration conf,\n      final Class<T> protocol, InetSocketAddress rmAddress) throws IOException {\n    RetryPolicy retryPolicy = createRetryPolicy(conf);\n    T proxy = RMProxy.<T>getProxy(conf, protocol, rmAddress);\n    LOG.info(\"Connecting to ResourceManager at \" + rmAddress);\n    return (T) RetryProxy.create(protocol, proxy, retryPolicy);\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-client.src.main.java.org.apache.hadoop.yarn.client.ClientRMProxy.createRMProxy": {
                "code_before_change": "  public static <T> T createRMProxy(final Configuration conf,\n      final Class<T> protocol) throws IOException {\n    InetSocketAddress rmAddress = getRMAddress(conf, protocol);\n    return createRMProxy(conf, protocol, rmAddress);\n  }",
                "code_after_change": "  public static <T> T createRMProxy(final Configuration conf,\n      final Class<T> protocol) throws IOException {\n    InetSocketAddress rmAddress = getRMAddress(conf, protocol);\n    return createRMProxy(conf, protocol, rmAddress);\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.recoverAppAttemptTokens": {
                "code_before_change": "  private void recoverAppAttemptTokens(Credentials appAttemptTokens) {\n    if (appAttemptTokens == null) {\n      return;\n    }\n    if (UserGroupInformation.isSecurityEnabled()) {\n\n      ClientToAMTokenSelector clientToAMTokenSelector =\n          new ClientToAMTokenSelector();\n      this.clientToAMToken =\n          clientToAMTokenSelector.selectToken(new Text(),\n            appAttemptTokens.getAllTokens());\n    }\n\n    InetSocketAddress serviceAddr =\n        conf.getSocketAddr(YarnConfiguration.RM_SCHEDULER_ADDRESS,\n          YarnConfiguration.DEFAULT_RM_SCHEDULER_ADDRESS,\n          YarnConfiguration.DEFAULT_RM_SCHEDULER_PORT);\n    AMRMTokenSelector appTokenSelector = new AMRMTokenSelector();\n    this.amrmToken =\n        appTokenSelector.selectToken(\n          SecurityUtil.buildTokenService(serviceAddr),\n          appAttemptTokens.getAllTokens());\n\n    // For now, no need to populate tokens back to AMRMTokenSecretManager,\n    // because running attempts are rebooted. Later in work-preserve restart,\n    // we'll create NEW->RUNNING transition in which the restored tokens will be\n    // added to the secret manager\n  }",
                "code_after_change": "  private void recoverAppAttemptTokens(Credentials appAttemptTokens) {\n    if (appAttemptTokens == null) {\n      return;\n    }\n    if (UserGroupInformation.isSecurityEnabled()) {\n\n      ClientToAMTokenSelector clientToAMTokenSelector =\n          new ClientToAMTokenSelector();\n      this.clientToAMToken =\n          clientToAMTokenSelector.selectToken(new Text(),\n            appAttemptTokens.getAllTokens());\n    }\n\n    // Only one AMRMToken is stored per-attempt, so this should be fine. Can't\n    // use TokenSelector as service may change - think fail-over.\n    this.amrmToken =\n        (Token<AMRMTokenIdentifier>) appAttemptTokens\n          .getToken(RMStateStore.AM_RM_TOKEN_SERVICE);\n\n    // For now, no need to populate tokens back to AMRMTokenSecretManager,\n    // because running attempts are rebooted. Later in work-preserve restart,\n    // we'll create NEW->RUNNING transition in which the restored tokens will be\n    // added to the secret manager\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.client.RMProxy.run": {
                "code_before_change": "        public T run() {\n          return (T) YarnRPC.create(conf).getProxy(protocol, rmAddress, conf);\n        }",
                "code_after_change": "        public T run() {\n          return (T) YarnRPC.create(conf).getProxy(protocol, rmAddress, conf);\n        }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-client.src.main.java.org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.getApplicationReport": {
                "code_before_change": "  public ApplicationReport getApplicationReport(ApplicationId appId)\n      throws YarnException, IOException {\n    GetApplicationReportRequest request =\n        Records.newRecord(GetApplicationReportRequest.class);\n    request.setApplicationId(appId);\n    GetApplicationReportResponse response =\n        rmClient.getApplicationReport(request);\n    return response.getApplicationReport();\n  }",
                "code_after_change": "  public ApplicationReport getApplicationReport(ApplicationId appId)\n      throws YarnException, IOException {\n    GetApplicationReportRequest request =\n        Records.newRecord(GetApplicationReportRequest.class);\n    request.setApplicationId(appId);\n    GetApplicationReportResponse response =\n        rmClient.getApplicationReport(request);\n    return response.getApplicationReport();\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-client.src.main.java.org.apache.hadoop.yarn.client.api.YarnClient.getApplicationReport": {
                "code_before_change": "  public abstract ApplicationReport getApplicationReport(ApplicationId appId)\n      throws YarnException, IOException;\n\n  /**\n   * <p>\n   * Get a report (ApplicationReport) of all Applications in the cluster.\n   * </p>\n   * \n   * <p>\n   * If the user does not have <code>VIEW_APP</code> access for an application\n   * then the corresponding report will be filtered as described in\n   * {@link #getApplicationReport(ApplicationId)}.",
                "code_after_change": "  public abstract ApplicationReport getApplicationReport(ApplicationId appId)\n      throws YarnException, IOException;\n\n  /**\n   * Get the AMRM token of the application.\n   * <p/>\n   * The AMRM token is required for AM to RM scheduling operations. For \n   * managed Application Masters Yarn takes care of injecting it. For unmanaged\n   * Applications Masters, the token must be obtained via this method and set\n   * in the {@link org.apache.hadoop.security.UserGroupInformation} of the"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Preventive",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the root cause as a misconfiguration in the Hadoop authentication settings, which is related to the stack trace context but not directly to the ground truth methods. The fix suggestion to change the authentication method in the configuration files is preventive, as it addresses the misconfiguration issue. The problem location identification is partial because it points to configuration files and methods related to authentication, which are in the shared stack trace context but not the exact ground truth methods. There is no wrong information in the bug report as it accurately describes the issue and provides a valid preventive measure."
        }
    },
    {
        "filename": "YARN-6072.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceInit": {
                "code_before_change": "  protected void serviceInit(Configuration conf) throws Exception {\n    this.conf = conf;\n    this.rmContext = new RMContextImpl();\n    \n    this.configurationProvider =\n        ConfigurationProviderFactory.getConfigurationProvider(conf);\n    this.configurationProvider.init(this.conf);\n    rmContext.setConfigurationProvider(configurationProvider);\n\n    // load core-site.xml\n    InputStream coreSiteXMLInputStream =\n        this.configurationProvider.getConfigurationInputStream(this.conf,\n            YarnConfiguration.CORE_SITE_CONFIGURATION_FILE);\n    if (coreSiteXMLInputStream != null) {\n      this.conf.addResource(coreSiteXMLInputStream,\n          YarnConfiguration.CORE_SITE_CONFIGURATION_FILE);\n    }\n\n    // Do refreshUserToGroupsMappings with loaded core-site.xml\n    Groups.getUserToGroupsMappingServiceWithLoadedConfiguration(this.conf)\n        .refresh();\n\n    // Do refreshSuperUserGroupsConfiguration with loaded core-site.xml\n    // Or use RM specific configurations to overwrite the common ones first\n    // if they exist\n    RMServerUtils.processRMProxyUsersConf(conf);\n    ProxyUsers.refreshSuperUserGroupsConfiguration(this.conf);\n\n    // load yarn-site.xml\n    InputStream yarnSiteXMLInputStream =\n        this.configurationProvider.getConfigurationInputStream(this.conf,\n            YarnConfiguration.YARN_SITE_CONFIGURATION_FILE);\n    if (yarnSiteXMLInputStream != null) {\n      this.conf.addResource(yarnSiteXMLInputStream,\n          YarnConfiguration.YARN_SITE_CONFIGURATION_FILE);\n    }\n\n    validateConfigs(this.conf);\n    \n    // Set HA configuration should be done before login\n    this.rmContext.setHAEnabled(HAUtil.isHAEnabled(this.conf));\n    if (this.rmContext.isHAEnabled()) {\n      HAUtil.verifyAndSetConfiguration(this.conf);\n\n      // If the RM is configured to use an embedded leader elector,\n      // initialize the leader elector.\n      if (HAUtil.isAutomaticFailoverEnabled(conf) &&\n          HAUtil.isAutomaticFailoverEmbedded(conf)) {\n        EmbeddedElector elector = createEmbeddedElector();\n        addIfService(elector);\n        rmContext.setLeaderElectorService(elector);\n      }\n    }\n\n    // Set UGI and do login\n    // If security is enabled, use login user\n    // If security is not enabled, use current user\n    this.rmLoginUGI = UserGroupInformation.getCurrentUser();\n    try {\n      doSecureLogin();\n    } catch(IOException ie) {\n      throw new YarnRuntimeException(\"Failed to login\", ie);\n    }\n\n    // register the handlers for all AlwaysOn services using setupDispatcher().\n    rmDispatcher = setupDispatcher();\n    addIfService(rmDispatcher);\n    rmContext.setDispatcher(rmDispatcher);\n\n    adminService = createAdminService();\n    addService(adminService);\n    rmContext.setRMAdminService(adminService);\n\n    rmContext.setYarnConfiguration(conf);\n    \n    createAndInitActiveServices(false);\n\n    webAppAddress = WebAppUtils.getWebAppBindURL(this.conf,\n                      YarnConfiguration.RM_BIND_HOST,\n                      WebAppUtils.getRMWebAppURLWithoutScheme(this.conf));\n\n    RMApplicationHistoryWriter rmApplicationHistoryWriter =\n        createRMApplicationHistoryWriter();\n    addService(rmApplicationHistoryWriter);\n    rmContext.setRMApplicationHistoryWriter(rmApplicationHistoryWriter);\n\n    // initialize the RM timeline collector first so that the system metrics\n    // publisher can bind to it\n    if (YarnConfiguration.timelineServiceV2Enabled(this.conf)) {\n      RMTimelineCollectorManager timelineCollectorManager =\n          createRMTimelineCollectorManager();\n      addService(timelineCollectorManager);\n      rmContext.setRMTimelineCollectorManager(timelineCollectorManager);\n    }\n\n    SystemMetricsPublisher systemMetricsPublisher =\n        createSystemMetricsPublisher();\n    addIfService(systemMetricsPublisher);\n    rmContext.setSystemMetricsPublisher(systemMetricsPublisher);\n\n    super.serviceInit(this.conf);\n  }",
                "code_after_change": "  protected void serviceInit(Configuration conf) throws Exception {\n    this.conf = conf;\n    this.rmContext = new RMContextImpl();\n    \n    this.configurationProvider =\n        ConfigurationProviderFactory.getConfigurationProvider(conf);\n    this.configurationProvider.init(this.conf);\n    rmContext.setConfigurationProvider(configurationProvider);\n\n    // load core-site.xml\n    InputStream coreSiteXMLInputStream =\n        this.configurationProvider.getConfigurationInputStream(this.conf,\n            YarnConfiguration.CORE_SITE_CONFIGURATION_FILE);\n    if (coreSiteXMLInputStream != null) {\n      this.conf.addResource(coreSiteXMLInputStream,\n          YarnConfiguration.CORE_SITE_CONFIGURATION_FILE);\n    }\n\n    // Do refreshUserToGroupsMappings with loaded core-site.xml\n    Groups.getUserToGroupsMappingServiceWithLoadedConfiguration(this.conf)\n        .refresh();\n\n    // Do refreshSuperUserGroupsConfiguration with loaded core-site.xml\n    // Or use RM specific configurations to overwrite the common ones first\n    // if they exist\n    RMServerUtils.processRMProxyUsersConf(conf);\n    ProxyUsers.refreshSuperUserGroupsConfiguration(this.conf);\n\n    // load yarn-site.xml\n    InputStream yarnSiteXMLInputStream =\n        this.configurationProvider.getConfigurationInputStream(this.conf,\n            YarnConfiguration.YARN_SITE_CONFIGURATION_FILE);\n    if (yarnSiteXMLInputStream != null) {\n      this.conf.addResource(yarnSiteXMLInputStream,\n          YarnConfiguration.YARN_SITE_CONFIGURATION_FILE);\n    }\n\n    validateConfigs(this.conf);\n    \n    // Set HA configuration should be done before login\n    this.rmContext.setHAEnabled(HAUtil.isHAEnabled(this.conf));\n    if (this.rmContext.isHAEnabled()) {\n      HAUtil.verifyAndSetConfiguration(this.conf);\n    }\n\n    // Set UGI and do login\n    // If security is enabled, use login user\n    // If security is not enabled, use current user\n    this.rmLoginUGI = UserGroupInformation.getCurrentUser();\n    try {\n      doSecureLogin();\n    } catch(IOException ie) {\n      throw new YarnRuntimeException(\"Failed to login\", ie);\n    }\n\n    // register the handlers for all AlwaysOn services using setupDispatcher().\n    rmDispatcher = setupDispatcher();\n    addIfService(rmDispatcher);\n    rmContext.setDispatcher(rmDispatcher);\n\n    // The order of services below should not be changed as services will be\n    // started in same order\n    // As elector service needs admin service to be initialized and started,\n    // first we add admin service then elector service\n\n    adminService = createAdminService();\n    addService(adminService);\n    rmContext.setRMAdminService(adminService);\n\n    // elector must be added post adminservice\n    if (this.rmContext.isHAEnabled()) {\n      // If the RM is configured to use an embedded leader elector,\n      // initialize the leader elector.\n      if (HAUtil.isAutomaticFailoverEnabled(conf)\n          && HAUtil.isAutomaticFailoverEmbedded(conf)) {\n        EmbeddedElector elector = createEmbeddedElector();\n        addIfService(elector);\n        rmContext.setLeaderElectorService(elector);\n      }\n    }\n\n    rmContext.setYarnConfiguration(conf);\n    \n    createAndInitActiveServices(false);\n\n    webAppAddress = WebAppUtils.getWebAppBindURL(this.conf,\n                      YarnConfiguration.RM_BIND_HOST,\n                      WebAppUtils.getRMWebAppURLWithoutScheme(this.conf));\n\n    RMApplicationHistoryWriter rmApplicationHistoryWriter =\n        createRMApplicationHistoryWriter();\n    addService(rmApplicationHistoryWriter);\n    rmContext.setRMApplicationHistoryWriter(rmApplicationHistoryWriter);\n\n    SystemMetricsPublisher systemMetricsPublisher = createSystemMetricsPublisher();\n    addService(systemMetricsPublisher);\n    rmContext.setSystemMetricsPublisher(systemMetricsPublisher);\n\n    super.serviceInit(this.conf);\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.AdminService.refreshAll": {
                "code_before_change": "  void refreshAll() throws ServiceFailedException {\n    try {\n      checkAcls(\"refreshAll\");\n      refreshQueues();\n      refreshNodes();\n      refreshSuperUserGroupsConfiguration();\n      refreshUserToGroupsMappings();\n      if (getConfig().getBoolean(\n          CommonConfigurationKeysPublic.HADOOP_SECURITY_AUTHORIZATION,\n          false)) {\n        refreshServiceAcls();\n      }\n      refreshClusterMaxPriority();\n    } catch (Exception ex) {\n      throw new ServiceFailedException(ex.getMessage());\n    }\n  }",
                "code_after_change": "  void refreshAll() throws ServiceFailedException {\n    try {\n      checkAcls(\"refreshAll\");\n      refreshQueues();\n      refreshNodes();\n      refreshSuperUserGroupsConfiguration();\n      refreshUserToGroupsMappings();\n      if (getConfig().getBoolean(\n          CommonConfigurationKeysPublic.HADOOP_SECURITY_AUTHORIZATION,\n          false)) {\n        refreshServiceAcls();\n      }\n      refreshClusterMaxPriority();\n    } catch (Exception ex) {\n      throw new ServiceFailedException(\"RefreshAll operation failed\", ex);\n    }\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Buggy Method"
            },
            "fix_suggestion": "Preventive",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Buggy Method"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the root cause as a NullPointerException in the 'refreshServiceAcls' method, which is not the exact location of the fix but is where the error occurs, hence 'Partial' with 'Buggy Method'. The fix suggestion to ensure proper initialization of Configuration and PolicyProvider is preventive as it would mitigate the issue but does not match the actual fix. The problem location identification is 'Partial' with 'Buggy Method' as it points to 'refreshServiceAcls', where the error occurs, but not where the fix was made. There is no wrong information in the bug report as all details are relevant to the context of the bug."
        }
    },
    {
        "filename": "YARN-7663.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.sendATSCreateEvent": {
                "code_before_change": "  private void sendATSCreateEvent() {\n    rmContext.getRMApplicationHistoryWriter().applicationStarted(this);\n    rmContext.getSystemMetricsPublisher().appCreated(this, this.startTime);\n  }",
                "code_after_change": "  private void sendATSCreateEvent() {\n    rmContext.getRMApplicationHistoryWriter().applicationStarted(this);\n    rmContext.getSystemMetricsPublisher().appCreated(this, this.startTime);\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.handle": {
                "code_before_change": "  public void handle(RMAppEvent event) {\n\n    this.writeLock.lock();\n\n    try {\n      ApplicationId appID = event.getApplicationId();\n      LOG.debug(\"Processing event for \" + appID + \" of type \"\n          + event.getType());\n      final RMAppState oldState = getState();\n      try {\n        /* keep the master in sync with the state machine */\n        this.stateMachine.doTransition(event.getType(), event);\n      } catch (InvalidStateTransitionException e) {\n        LOG.error(\"App: \" + appID\n            + \" can't handle this event at current state\", e);\n        /* TODO fail the application on the failed transition */\n      }\n\n      // Log at INFO if we're not recovering or not in a terminal state.\n      // Log at DEBUG otherwise.\n      if ((oldState != getState()) &&\n          (((recoveredFinalState == null)) ||\n            (event.getType() != RMAppEventType.RECOVER))) {\n        LOG.info(String.format(STATE_CHANGE_MESSAGE, appID, oldState,\n            getState(), event.getType()));\n      } else if ((oldState != getState()) && LOG.isDebugEnabled()) {\n        LOG.debug(String.format(STATE_CHANGE_MESSAGE, appID, oldState,\n            getState(), event.getType()));\n      }\n    } finally {\n      this.writeLock.unlock();\n    }\n  }",
                "code_after_change": "  public void handle(RMAppEvent event) {\n\n    this.writeLock.lock();\n\n    try {\n      ApplicationId appID = event.getApplicationId();\n      LOG.debug(\"Processing event for \" + appID + \" of type \"\n          + event.getType());\n      final RMAppState oldState = getState();\n      try {\n        /* keep the master in sync with the state machine */\n        this.stateMachine.doTransition(event.getType(), event);\n      } catch (InvalidStateTransitionException e) {\n        LOG.error(\"App: \" + appID\n            + \" can't handle this event at current state\", e);\n        onInvalidStateTransition(event.getType(), oldState);\n      }\n\n      if (oldState != getState()) {\n        LOG.info(appID + \" State change from \" + oldState + \" to \"\n            + getState() + \" on event=\" + event.getType());\n      }\n    } finally {\n      this.writeLock.unlock();\n    }\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.KillAttemptTransition": {
                "code_before_change": [],
                "code_after_change": []
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Correct",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report precisely identifies the root cause as an invalid state transition from 'KILLED' to 'START' in the RMAppImpl.handle method, which is one of the ground truth methods. The fix suggestion to implement a check in the RMAppImpl.handle method to prevent processing of START events when the application is in the KILLED state matches the developer's fix. The problem location is precisely identified as the RMAppImpl.handle method, which is part of the ground truth methods. There is no wrong information in the bug report as all details are consistent with the context of the bug."
        }
    },
    {
        "filename": "YARN-5873.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl.RMContainerImpl": {
                "code_before_change": [],
                "code_after_change": []
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl.setContainerId": {
                "code_before_change": "  public void setContainerId(ContainerId containerId) {\n    // In some cases, for example, global scheduling. It is possible that\n    // container created without container-id assigned, so we will publish\n    // container creation event to timeline service when id assigned.\n    container.setId(containerId);\n\n    // If saveNonAMContainerMetaInfo is true, store system metrics for all\n    // containers. If false, and if this container is marked as the AM, metrics\n    // will still be published for this container, but that calculation happens\n    // later.\n    if (saveNonAMContainerMetaInfo && null != container.getId()) {\n      rmContext.getSystemMetricsPublisher().containerCreated(\n          this, this.creationTime);\n    }\n  }",
                "code_after_change": "  public void setContainerId(ContainerId containerId) {\n    // In some cases, for example, global scheduling. It is possible that\n    // container created without container-id assigned, so we will publish\n    // container creation event to timeline service when id assigned.\n    container.setId(containerId);\n\n    if (containerId != null) {\n      rmContext.getRMApplicationHistoryWriter().containerStarted(this);\n    }\n    // If saveNonAMContainerMetaInfo is true, store system metrics for all\n    // containers. If false, and if this container is marked as the AM, metrics\n    // will still be published for this container, but that calculation happens\n    // later.\n    if (saveNonAMContainerMetaInfo && null != container.getId()) {\n      rmContext.getSystemMetricsPublisher().containerCreated(\n          this, this.creationTime);\n    }\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Correct",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Buggy Method"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report accurately identifies the root cause of the NullPointerException as the `containerId` in the `WritingContainerStartEvent` class being null, which aligns with the ground truth methods related to `RMContainerImpl` and `setContainerId`. The fix suggestion in the report matches the developer's fix, which involves ensuring the `ContainerId` is properly initialized before invoking `containerStarted`, so it is 'Correct'. The problem location mentions `WritingContainerStartEvent.hashCode`, which is where the error occurs, but not where the fix was made, so it is 'Partial' under 'Buggy Method'. There is no wrong information in the report as all details are relevant to the context of the bug."
        }
    },
    {
        "filename": "YARN-3227.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl.getHttpURLConnection": {
                "code_before_change": "    public HttpURLConnection getHttpURLConnection(final URL url) throws IOException {\n      boolean isProxyAccess =\n          UserGroupInformation.getCurrentUser().getAuthenticationMethod()\n          == UserGroupInformation.AuthenticationMethod.PROXY;\n      UserGroupInformation callerUGI = isProxyAccess ?\n          UserGroupInformation.getCurrentUser().getRealUser()\n          : UserGroupInformation.getCurrentUser();\n      final String doAsUser = isProxyAccess ?\n          UserGroupInformation.getCurrentUser().getShortUserName() : null;\n      try {\n        return callerUGI.doAs(new PrivilegedExceptionAction<HttpURLConnection>() {\n          @Override\n          public HttpURLConnection run() throws Exception {\n            return new DelegationTokenAuthenticatedURL(\n                authenticator, connConfigurator).openConnection(url, token,\n                doAsUser);\n          }\n        });\n      } catch (UndeclaredThrowableException e) {\n        throw new IOException(e.getCause());\n      } catch (InterruptedException e) {\n        throw new IOException(e);\n      }\n    }",
                "code_after_change": "    public HttpURLConnection getHttpURLConnection(final URL url) throws IOException {\n      boolean isProxyAccess =\n          UserGroupInformation.getCurrentUser().getAuthenticationMethod()\n          == UserGroupInformation.AuthenticationMethod.PROXY;\n      UserGroupInformation callerUGI = isProxyAccess ?\n          UserGroupInformation.getCurrentUser().getRealUser()\n          : UserGroupInformation.getCurrentUser();\n      final String doAsUser = isProxyAccess ?\n          UserGroupInformation.getCurrentUser().getShortUserName() : null;\n      callerUGI.checkTGTAndReloginFromKeytab();\n      try {\n        return callerUGI.doAs(new PrivilegedExceptionAction<HttpURLConnection>() {\n          @Override\n          public HttpURLConnection run() throws Exception {\n            return new DelegationTokenAuthenticatedURL(\n                authenticator, connConfigurator).openConnection(url, token,\n                doAsUser);\n          }\n        });\n      } catch (UndeclaredThrowableException e) {\n        throw new IOException(e.getCause());\n      } catch (InterruptedException e) {\n        throw new IOException(e);\n      }\n    }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl.run": {
                "code_before_change": "    public abstract Object run() throws IOException;\n    // The method to indicate if we should retry given the incoming exception\n    public abstract boolean shouldRetryOn(Exception e);\n  }\n\n  // Class to handle retry\n  // Outside this class, only visible to tests\n  @Private\n  @VisibleForTesting\n  static class TimelineClientConnectionRetry {",
                "code_after_change": "    public abstract Object run() throws IOException;\n    // The method to indicate if we should retry given the incoming exception\n    public abstract boolean shouldRetryOn(Exception e);\n  }\n\n  // Class to handle retry\n  // Outside this class, only visible to tests\n  @Private\n  @VisibleForTesting\n  static class TimelineClientConnectionRetry {"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Preventive",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the root cause as the expiration of the RM user's TGT, which is related to the issue but not the precise root cause in the ground truth methods. The methods mentioned in the stack trace, such as 'DelegationTokenRenewer.handleAppSubmitEvent', are in the same context as the ground truth methods, hence 'Shared Stack Trace Context'. The fix suggestion is preventive as it suggests ensuring valid credentials and permissions, which aligns with the need to handle expired TGTs but does not directly match the developer's fix. The problem location identification is partial because it mentions methods like 'DelegationTokenRenewer.renew' and 'UserGroupInformation.getCurrentUser', which are related but not the exact ground truth methods. There is no wrong information in the bug report as all details are relevant to the context of the bug."
        }
    },
    {
        "filename": "YARN-4235.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.QueuePlacementRule.getQueueForApp": {
                "code_before_change": "  protected abstract String getQueueForApp(String requestedQueue, String user,\n      Groups groups, Map<FSQueueType, Set<String>> configuredQueues)\n      throws IOException;\n\n  /**\n   * Places apps in queues by username of the submitter\n   */\n  public static class User extends QueuePlacementRule {\n    @Override\n    protected String getQueueForApp(String requestedQueue, String user,\n        Groups groups, Map<FSQueueType, Set<String>> configuredQueues) {\n      return \"root.\" + cleanName(user);\n    }\n    \n    @Override\n    public boolean isTerminal() {\n      return create;\n    }\n  }",
                "code_after_change": "  protected abstract String getQueueForApp(String requestedQueue, String user,\n      Groups groups, Map<FSQueueType, Set<String>> configuredQueues)\n      throws IOException;\n\n  /**\n   * Places apps in queues by username of the submitter\n   */\n  public static class User extends QueuePlacementRule {\n    @Override\n    protected String getQueueForApp(String requestedQueue, String user,\n        Groups groups, Map<FSQueueType, Set<String>> configuredQueues) {\n      return \"root.\" + cleanName(user);\n    }\n    \n    @Override\n    public boolean isTerminal() {\n      return create;\n    }\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Alternative Fix",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report precisely identifies the root cause as the `getQueueForApp` method in the `QueuePlacementRule` class, which matches the ground truth method. The fix suggestion is an alternative fix because it suggests adding a check for empty groups and returning a default queue, which is a valid approach but not exactly what was implemented in the developer's fix. The problem location identification is precise as it correctly mentions the `getQueueForApp` method in the `problem_location` field. There is no wrong information in the bug report as all details are consistent with the context of the bug."
        }
    },
    {
        "filename": "YARN-4833.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.createAndPopulateNewRMApp": {
                "code_before_change": "  private RMAppImpl createAndPopulateNewRMApp(\n      ApplicationSubmissionContext submissionContext, long submitTime,\n      String user, boolean isRecovery)\n      throws YarnException, AccessControlException {\n    // Do queue mapping\n    if (!isRecovery) {\n      if (rmContext.getQueuePlacementManager() != null) {\n        // We only do queue mapping when it's a new application\n        rmContext.getQueuePlacementManager().placeApplication(\n            submissionContext, user);\n      }\n    }\n    \n    ApplicationId applicationId = submissionContext.getApplicationId();\n    ResourceRequest amReq =\n        validateAndCreateResourceRequest(submissionContext, isRecovery);\n\n    // Verify and get the update application priority and set back to\n    // submissionContext\n    Priority appPriority = rmContext.getScheduler()\n        .checkAndGetApplicationPriority(submissionContext.getPriority(), user,\n            submissionContext.getQueue(), applicationId);\n    submissionContext.setPriority(appPriority);\n\n    UserGroupInformation userUgi = UserGroupInformation.createRemoteUser(user);\n    // Since FairScheduler queue mapping is done inside scheduler,\n    // if FairScheduler is used and the queue doesn't exist, we should not\n    // fail here because queue will be created inside FS. Ideally, FS queue\n    // mapping should be done outside scheduler too like CS.\n    // For now, exclude FS for the acl check.\n    if (!isRecovery && YarnConfiguration.isAclEnabled(conf)\n        && scheduler instanceof CapacityScheduler) {\n      String queueName = submissionContext.getQueue();\n      String appName = submissionContext.getApplicationName();\n      CSQueue csqueue = ((CapacityScheduler) scheduler).getQueue(queueName);\n      if (null != csqueue\n          && !authorizer.checkPermission(\n              new AccessRequest(csqueue.getPrivilegedEntity(), userUgi,\n                  SchedulerUtils.toAccessType(QueueACL.SUBMIT_APPLICATIONS),\n                  applicationId.toString(), appName))\n          && !authorizer.checkPermission(\n              new AccessRequest(csqueue.getPrivilegedEntity(), userUgi,\n                  SchedulerUtils.toAccessType(QueueACL.ADMINISTER_QUEUE),\n                  applicationId.toString(), appName))) {\n        throw new AccessControlException(\n            \"User \" + user + \" does not have permission to submit \"\n                + applicationId + \" to queue \" + submissionContext.getQueue());\n      }\n    }\n\n    // Create RMApp\n    RMAppImpl application = new RMAppImpl(applicationId, rmContext, this.conf,\n        submissionContext.getApplicationName(), user,\n        submissionContext.getQueue(), submissionContext, this.scheduler,\n        this.masterService, submitTime, submissionContext.getApplicationType(),\n        submissionContext.getApplicationTags(), amReq);\n\n    // Concurrent app submissions with same applicationId will fail here\n    // Concurrent app submissions with different applicationIds will not\n    // influence each other\n    if (rmContext.getRMApps().putIfAbsent(applicationId, application) !=\n        null) {\n      String message = \"Application with id \" + applicationId\n          + \" is already present! Cannot add a duplicate!\";\n      LOG.warn(message);\n      throw new YarnException(message);\n    }\n    // Inform the ACLs Manager\n    this.applicationACLsManager.addApplication(applicationId,\n        submissionContext.getAMContainerSpec().getApplicationACLs());\n    String appViewACLs = submissionContext.getAMContainerSpec()\n        .getApplicationACLs().get(ApplicationAccessType.VIEW_APP);\n    rmContext.getSystemMetricsPublisher().appACLsUpdated(\n        application, appViewACLs, System.currentTimeMillis());\n    return application;\n  }",
                "code_after_change": "  private RMAppImpl createAndPopulateNewRMApp(\n      ApplicationSubmissionContext submissionContext, long submitTime,\n      String user, boolean isRecovery, long startTime) throws YarnException {\n    // Do queue mapping\n    if (!isRecovery) {\n      if (rmContext.getQueuePlacementManager() != null) {\n        // We only do queue mapping when it's a new application\n        rmContext.getQueuePlacementManager().placeApplication(\n            submissionContext, user);\n      }\n    }\n    \n    ApplicationId applicationId = submissionContext.getApplicationId();\n    ResourceRequest amReq =\n        validateAndCreateResourceRequest(submissionContext, isRecovery);\n\n    // Verify and get the update application priority and set back to\n    // submissionContext\n    Priority appPriority = rmContext.getScheduler()\n        .checkAndGetApplicationPriority(submissionContext.getPriority(), user,\n            submissionContext.getQueue(), applicationId);\n    submissionContext.setPriority(appPriority);\n\n    UserGroupInformation userUgi = UserGroupInformation.createRemoteUser(user);\n    // Since FairScheduler queue mapping is done inside scheduler,\n    // if FairScheduler is used and the queue doesn't exist, we should not\n    // fail here because queue will be created inside FS. Ideally, FS queue\n    // mapping should be done outside scheduler too like CS.\n    // For now, exclude FS for the acl check.\n    if (!isRecovery && YarnConfiguration.isAclEnabled(conf)\n        && scheduler instanceof CapacityScheduler) {\n      String queueName = submissionContext.getQueue();\n      String appName = submissionContext.getApplicationName();\n      CSQueue csqueue = ((CapacityScheduler) scheduler).getQueue(queueName);\n      if (null != csqueue\n          && !authorizer.checkPermission(\n              new AccessRequest(csqueue.getPrivilegedEntity(), userUgi,\n                  SchedulerUtils.toAccessType(QueueACL.SUBMIT_APPLICATIONS),\n                  applicationId.toString(), appName, Server.getRemoteAddress(),\n                  null))\n          && !authorizer.checkPermission(\n              new AccessRequest(csqueue.getPrivilegedEntity(), userUgi,\n                  SchedulerUtils.toAccessType(QueueACL.ADMINISTER_QUEUE),\n                  applicationId.toString(), appName, Server.getRemoteAddress(),\n                  null))) {\n        throw RPCUtil.getRemoteException(new AccessControlException(\n            \"User \" + user + \" does not have permission to submit \"\n                + applicationId + \" to queue \" + submissionContext.getQueue()));\n      }\n    }\n\n    // Create RMApp\n    RMAppImpl application =\n        new RMAppImpl(applicationId, rmContext, this.conf,\n            submissionContext.getApplicationName(), user,\n            submissionContext.getQueue(),\n            submissionContext, this.scheduler, this.masterService,\n            submitTime, submissionContext.getApplicationType(),\n            submissionContext.getApplicationTags(), amReq, startTime);\n    // Concurrent app submissions with same applicationId will fail here\n    // Concurrent app submissions with different applicationIds will not\n    // influence each other\n    if (rmContext.getRMApps().putIfAbsent(applicationId, application) !=\n        null) {\n      String message = \"Application with id \" + applicationId\n          + \" is already present! Cannot add a duplicate!\";\n      LOG.warn(message);\n      throw new YarnException(message);\n    }\n\n    if (YarnConfiguration.timelineServiceV2Enabled(conf)) {\n      // Start timeline collector for the submitted app\n      application.startTimelineCollector();\n    }\n    // Inform the ACLs Manager\n    this.applicationACLsManager.addApplication(applicationId,\n        submissionContext.getAMContainerSpec().getApplicationACLs());\n    String appViewACLs = submissionContext.getAMContainerSpec()\n        .getApplicationACLs().get(ApplicationAccessType.VIEW_APP);\n    rmContext.getSystemMetricsPublisher().appACLsUpdated(\n        application, appViewACLs, System.currentTimeMillis());\n    return application;\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.recoverApplication": {
                "code_before_change": "  protected void recoverApplication(ApplicationStateData appState,\n      RMState rmState) throws Exception {\n    ApplicationSubmissionContext appContext =\n        appState.getApplicationSubmissionContext();\n    ApplicationId appId = appContext.getApplicationId();\n\n    // create and recover app.\n    RMAppImpl application =\n        createAndPopulateNewRMApp(appContext, appState.getSubmitTime(),\n            appState.getUser(), true);\n\n    application.handle(new RMAppRecoverEvent(appId, rmState));\n  }",
                "code_after_change": "  protected void recoverApplication(ApplicationStateData appState,\n      RMState rmState) throws Exception {\n    ApplicationSubmissionContext appContext =\n        appState.getApplicationSubmissionContext();\n    ApplicationId appId = appContext.getApplicationId();\n\n    // create and recover app.\n    RMAppImpl application =\n        createAndPopulateNewRMApp(appContext, appState.getSubmitTime(),\n            appState.getUser(), true, appState.getStartTime());\n\n    application.handle(new RMAppRecoverEvent(appId, rmState));\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.RMAppManager.checkAppNumCompletedLimit": {
                "code_before_change": "  protected synchronized void checkAppNumCompletedLimit() {\n    // check apps kept in state store.\n    while (completedAppsInStateStore > this.maxCompletedAppsInStateStore) {\n      ApplicationId removeId =\n          completedApps.get(completedApps.size() - completedAppsInStateStore);\n      RMApp removeApp = rmContext.getRMApps().get(removeId);\n      LOG.info(\"Max number of completed apps kept in state store met:\"\n          + \" maxCompletedAppsInStateStore = \" + maxCompletedAppsInStateStore\n          + \", removing app \" + removeApp.getApplicationId()\n          + \" from state store.\");\n      rmContext.getStateStore().removeApplication(removeApp);\n      completedAppsInStateStore--;\n    }\n\n    // check apps kept in memorty.\n    while (completedApps.size() > this.maxCompletedAppsInMemory) {\n      ApplicationId removeId = completedApps.remove();\n      LOG.info(\"Application should be expired, max number of completed apps\"\n          + \" kept in memory met: maxCompletedAppsInMemory = \"\n          + this.maxCompletedAppsInMemory + \", removing app \" + removeId\n          + \" from memory: \");\n      rmContext.getRMApps().remove(removeId);\n      this.applicationACLsManager.removeApplication(removeId);\n    }\n  }",
                "code_after_change": "  protected synchronized void checkAppNumCompletedLimit() {\n    // check apps kept in state store.\n    while (completedAppsInStateStore > this.maxCompletedAppsInStateStore) {\n      ApplicationId removeId =\n          completedApps.get(completedApps.size() - completedAppsInStateStore);\n      RMApp removeApp = rmContext.getRMApps().get(removeId);\n      LOG.info(\"Max number of completed apps kept in state store met:\"\n          + \" maxCompletedAppsInStateStore = \" + maxCompletedAppsInStateStore\n          + \", removing app \" + removeApp.getApplicationId()\n          + \" from state store.\");\n      rmContext.getStateStore().removeApplication(removeApp);\n      completedAppsInStateStore--;\n    }\n\n    // check apps kept in memorty.\n    while (completedApps.size() > this.maxCompletedAppsInMemory) {\n      ApplicationId removeId = completedApps.remove();\n      LOG.info(\"Application should be expired, max number of completed apps\"\n          + \" kept in memory met: maxCompletedAppsInMemory = \"\n          + this.maxCompletedAppsInMemory + \", removing app \" + removeId\n          + \" from memory: \");\n      rmContext.getRMApps().remove(removeId);\n      this.applicationACLsManager.removeApplication(removeId);\n    }\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Preventive",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report precisely identifies the root cause as a permission issue in the 'createAndPopulateNewRMApp' method, which is one of the ground truth methods. The fix suggestion is preventive as it suggests modifying ACLs in the configuration files to prevent the issue, which aligns with the nature of the problem but does not directly match the code changes. The problem location identification is precise as it correctly mentions the 'createAndPopulateNewRMApp' method, which is part of the ground truth. There is no wrong information in the bug report as all details are relevant and accurate."
        }
    },
    {
        "filename": "YARN-1689.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.assignToQueue": {
                "code_before_change": "  FSLeafQueue assignToQueue(RMApp rmApp, String queueName, String user) {\n    FSLeafQueue queue = null;\n    try {\n      QueuePlacementPolicy placementPolicy = allocConf.getPlacementPolicy();\n      queueName = placementPolicy.assignAppToQueue(queueName, user);\n      if (queueName == null) {\n        return null;\n      }\n      queue = queueMgr.getLeafQueue(queueName, true);\n    } catch (IOException ex) {\n      LOG.error(\"Error assigning app to queue, rejecting\", ex);\n    }\n    \n    if (rmApp != null) {\n      rmApp.setQueue(queue.getName());\n    } else {\n      LOG.warn(\"Couldn't find RM app to set queue name on\");\n    }\n    \n    return queue;\n  }",
                "code_after_change": "  FSLeafQueue assignToQueue(RMApp rmApp, String queueName, String user) {\n    FSLeafQueue queue = null;\n    try {\n      QueuePlacementPolicy placementPolicy = allocConf.getPlacementPolicy();\n      queueName = placementPolicy.assignAppToQueue(queueName, user);\n      if (queueName == null) {\n        return null;\n      }\n      queue = queueMgr.getLeafQueue(queueName, true);\n    } catch (IOException ex) {\n      LOG.error(\"Error assigning app to queue, rejecting\", ex);\n    }\n    \n    if (rmApp != null) {\n      rmApp.setQueue(queue.getName());\n    } else {\n      LOG.warn(\"Couldn't find RM app to set queue name on\");\n    }\n    \n    return queue;\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler.containerCompleted": {
                "code_before_change": "  private synchronized void containerCompleted(RMContainer rmContainer,\n      ContainerStatus containerStatus, RMContainerEventType event) {\n    if (rmContainer == null) {\n      LOG.info(\"Null container completed...\");\n      return;\n    }\n\n    // Get the application for the finished container\n    Container container = rmContainer.getContainer();\n    FiCaSchedulerApp application =\n        getCurrentAttemptForContainer(container.getId());\n    ApplicationId appId =\n        container.getId().getApplicationAttemptId().getApplicationId();\n    \n    // Get the node on which the container was allocated\n    FiCaSchedulerNode node = getNode(container.getNodeId());\n    \n    if (application == null) {\n      LOG.info(\"Unknown application: \" + appId + \n          \" released container \" + container.getId() +\n          \" on node: \" + node + \n          \" with event: \" + event);\n      return;\n    }\n\n    // Inform the application\n    application.containerCompleted(rmContainer, containerStatus, event);\n\n    // Inform the node\n    node.releaseContainer(container);\n    \n    // Update total usage\n    Resources.subtractFrom(usedResource, container.getResource());\n\n    LOG.info(\"Application attempt \" + application.getApplicationAttemptId() + \n        \" released container \" + container.getId() +\n        \" on node: \" + node + \n        \" with event: \" + event);\n     \n  }",
                "code_after_change": "  private synchronized void containerCompleted(RMContainer rmContainer,\n      ContainerStatus containerStatus, RMContainerEventType event) {\n    if (rmContainer == null) {\n      LOG.info(\"Null container completed...\");\n      return;\n    }\n\n    // Get the application for the finished container\n    Container container = rmContainer.getContainer();\n    ApplicationAttemptId applicationAttemptId = container.getId().getApplicationAttemptId();\n    FiCaSchedulerApp application = getApplication(applicationAttemptId);\n    \n    // Get the node on which the container was allocated\n    FiCaSchedulerNode node = getNode(container.getNodeId());\n    \n    if (application == null) {\n      LOG.info(\"Unknown application: \" + applicationAttemptId + \n          \" released container \" + container.getId() +\n          \" on node: \" + node + \n          \" with event: \" + event);\n      return;\n    }\n\n    // Inform the application\n    application.containerCompleted(rmContainer, containerStatus, event);\n\n    // Inform the node\n    node.releaseContainer(container);\n    \n    // Update total usage\n    Resources.subtractFrom(usedResource, container.getResource());\n\n    LOG.info(\"Application \" + applicationAttemptId + \n        \" released container \" + container.getId() +\n        \" on node: \" + node + \n        \" with event: \" + event);\n     \n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.ActiveUsersManager.deactivateApplication": {
                "code_before_change": "  synchronized public void deactivateApplication(\n      String user, ApplicationId applicationId) {\n    Set<ApplicationId> userApps = usersApplications.get(user);\n    if (userApps != null) {\n      if (userApps.remove(applicationId)) {\n        metrics.deactivateApp(user);\n      }\n      if (userApps.isEmpty()) {\n        usersApplications.remove(user);\n        --activeUsers;\n        metrics.decrActiveUsers();\n        LOG.debug(\"User \" + user + \" removed from activeUsers, currently: \" + \n            activeUsers);\n      }\n    }\n  }",
                "code_after_change": "  synchronized public void deactivateApplication(\n      String user, ApplicationId applicationId) {\n    Set<ApplicationId> userApps = usersApplications.get(user);\n    if (userApps != null) {\n      if (userApps.remove(applicationId)) {\n        metrics.deactivateApp(user);\n      }\n      if (userApps.isEmpty()) {\n        usersApplications.remove(user);\n        --activeUsers;\n        metrics.decrActiveUsers();\n        LOG.debug(\"User \" + user + \" removed from activeUsers, currently: \" + \n            activeUsers);\n      }\n    }\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.reinitialize": {
                "code_before_change": "  public synchronized void reinitialize(Configuration conf, RMContext rmContext)\n      throws IOException {\n    if (!initialized) {\n      this.conf = new FairSchedulerConfiguration(conf);\n      validateConf(this.conf);\n      minimumAllocation = this.conf.getMinimumAllocation();\n      maximumAllocation = this.conf.getMaximumAllocation();\n      incrAllocation = this.conf.getIncrementAllocation();\n      continuousSchedulingEnabled = this.conf.isContinuousSchedulingEnabled();\n      continuousSchedulingSleepMs =\n              this.conf.getContinuousSchedulingSleepMs();\n      nodeLocalityThreshold = this.conf.getLocalityThresholdNode();\n      rackLocalityThreshold = this.conf.getLocalityThresholdRack();\n      nodeLocalityDelayMs = this.conf.getLocalityDelayNodeMs();\n      rackLocalityDelayMs = this.conf.getLocalityDelayRackMs();\n      preemptionEnabled = this.conf.getPreemptionEnabled();\n      assignMultiple = this.conf.getAssignMultiple();\n      maxAssign = this.conf.getMaxAssign();\n      sizeBasedWeight = this.conf.getSizeBasedWeight();\n      preemptionInterval = this.conf.getPreemptionInterval();\n      waitTimeBeforeKill = this.conf.getWaitTimeBeforeKill();\n      usePortForNodeName = this.conf.getUsePortForNodeName();\n      \n      rootMetrics = FSQueueMetrics.forQueue(\"root\", null, true, conf);\n      this.rmContext = rmContext;\n      // This stores per-application scheduling information\n      this.applications =\n          new ConcurrentHashMap<ApplicationId, SchedulerApplication>();\n      this.eventLog = new FairSchedulerEventLog();\n      eventLog.init(this.conf);\n\n      initialized = true;\n\n      allocConf = new AllocationConfiguration(conf);\n      try {\n        queueMgr.initialize(conf);\n      } catch (Exception e) {\n        throw new IOException(\"Failed to start FairScheduler\", e);\n      }\n\n      Thread updateThread = new Thread(new UpdateThread());\n      updateThread.setName(\"FairSchedulerUpdateThread\");\n      updateThread.setDaemon(true);\n      updateThread.start();\n\n      if (continuousSchedulingEnabled) {\n        // start continuous scheduling thread\n        Thread schedulingThread = new Thread(\n          new Runnable() {\n            @Override\n            public void run() {\n              continuousScheduling();\n            }\n          }\n        );\n        schedulingThread.setName(\"ContinuousScheduling\");\n        schedulingThread.setDaemon(true);\n        schedulingThread.start();\n      }\n      \n      allocsLoader.init(conf);\n      allocsLoader.setReloadListener(new AllocationReloadListener());\n      // If we fail to load allocations file on initialize, we want to fail\n      // immediately.  After a successful load, exceptions on future reloads\n      // will just result in leaving things as they are.\n      try {\n        allocsLoader.reloadAllocations();\n      } catch (Exception e) {\n        throw new IOException(\"Failed to initialize FairScheduler\", e);\n      }\n      allocsLoader.start();\n    } else {\n      try {\n        allocsLoader.reloadAllocations();\n      } catch (Exception e) {\n        LOG.error(\"Failed to reload allocations file\", e);\n      }\n    }\n  }",
                "code_after_change": "  public synchronized void reinitialize(Configuration conf, RMContext rmContext)\n      throws IOException {\n    if (!initialized) {\n      this.conf = new FairSchedulerConfiguration(conf);\n      validateConf(this.conf);\n      minimumAllocation = this.conf.getMinimumAllocation();\n      maximumAllocation = this.conf.getMaximumAllocation();\n      incrAllocation = this.conf.getIncrementAllocation();\n      continuousSchedulingEnabled = this.conf.isContinuousSchedulingEnabled();\n      continuousSchedulingSleepMs =\n              this.conf.getContinuousSchedulingSleepMs();\n      nodeLocalityThreshold = this.conf.getLocalityThresholdNode();\n      rackLocalityThreshold = this.conf.getLocalityThresholdRack();\n      nodeLocalityDelayMs = this.conf.getLocalityDelayNodeMs();\n      rackLocalityDelayMs = this.conf.getLocalityDelayRackMs();\n      preemptionEnabled = this.conf.getPreemptionEnabled();\n      assignMultiple = this.conf.getAssignMultiple();\n      maxAssign = this.conf.getMaxAssign();\n      sizeBasedWeight = this.conf.getSizeBasedWeight();\n      preemptionInterval = this.conf.getPreemptionInterval();\n      waitTimeBeforeKill = this.conf.getWaitTimeBeforeKill();\n      usePortForNodeName = this.conf.getUsePortForNodeName();\n      \n      rootMetrics = FSQueueMetrics.forQueue(\"root\", null, true, conf);\n      this.rmContext = rmContext;\n      this.eventLog = new FairSchedulerEventLog();\n      eventLog.init(this.conf);\n\n      initialized = true;\n\n      allocConf = new AllocationConfiguration(conf);\n      try {\n        queueMgr.initialize(conf);\n      } catch (Exception e) {\n        throw new IOException(\"Failed to start FairScheduler\", e);\n      }\n\n      Thread updateThread = new Thread(new UpdateThread());\n      updateThread.setName(\"FairSchedulerUpdateThread\");\n      updateThread.setDaemon(true);\n      updateThread.start();\n\n      if (continuousSchedulingEnabled) {\n        // start continuous scheduling thread\n        Thread schedulingThread = new Thread(\n          new Runnable() {\n            @Override\n            public void run() {\n              continuousScheduling();\n            }\n          }\n        );\n        schedulingThread.setName(\"ContinuousScheduling\");\n        schedulingThread.setDaemon(true);\n        schedulingThread.start();\n      }\n      \n      allocsLoader.init(conf);\n      allocsLoader.setReloadListener(new AllocationReloadListener());\n      // If we fail to load allocations file on initialize, we want to fail\n      // immediately.  After a successful load, exceptions on future reloads\n      // will just result in leaving things as they are.\n      try {\n        allocsLoader.reloadAllocations();\n      } catch (Exception e) {\n        throw new IOException(\"Failed to initialize FairScheduler\", e);\n      }\n      allocsLoader.start();\n    } else {\n      try {\n        allocsLoader.reloadAllocations();\n      } catch (Exception e) {\n        LOG.error(\"Failed to reload allocations file\", e);\n      }\n    }\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.nodeUpdate": {
                "code_before_change": "  private synchronized void nodeUpdate(RMNode nm) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"nodeUpdate: \" + nm + \" clusterResources: \" + clusterResource);\n    }\n\n    FiCaSchedulerNode node = getNode(nm.getNodeID());\n    \n    // Update resource if any change\n    SchedulerUtils.updateResourceIfChanged(node, nm, clusterResource, LOG);\n    \n    List<UpdatedContainerInfo> containerInfoList = nm.pullContainerUpdates();\n    List<ContainerStatus> newlyLaunchedContainers = new ArrayList<ContainerStatus>();\n    List<ContainerStatus> completedContainers = new ArrayList<ContainerStatus>();\n    for(UpdatedContainerInfo containerInfo : containerInfoList) {\n      newlyLaunchedContainers.addAll(containerInfo.getNewlyLaunchedContainers());\n      completedContainers.addAll(containerInfo.getCompletedContainers());\n    }\n    \n    // Processing the newly launched containers\n    for (ContainerStatus launchedContainer : newlyLaunchedContainers) {\n      containerLaunchedOnNode(launchedContainer.getContainerId(), node);\n    }\n\n    // Process completed containers\n    for (ContainerStatus completedContainer : completedContainers) {\n      ContainerId containerId = completedContainer.getContainerId();\n      LOG.debug(\"Container FINISHED: \" + containerId);\n      completedContainer(getRMContainer(containerId), \n          completedContainer, RMContainerEventType.FINISHED);\n    }\n\n    // Now node data structures are upto date and ready for scheduling.\n    if(LOG.isDebugEnabled()) {\n      LOG.debug(\"Node being looked for scheduling \" + nm\n        + \" availableResource: \" + node.getAvailableResource());\n    }\n\n    // Assign new containers...\n    // 1. Check for reserved applications\n    // 2. Schedule if there are no reservations\n\n    RMContainer reservedContainer = node.getReservedContainer();\n    if (reservedContainer != null) {\n      FiCaSchedulerApp reservedApplication =\n          getCurrentAttemptForContainer(reservedContainer.getContainerId());\n      \n      // Try to fulfill the reservation\n      LOG.info(\"Trying to fulfill reservation for application \" + \n          reservedApplication.getApplicationId() + \" on node: \" + nm);\n      \n      LeafQueue queue = ((LeafQueue)reservedApplication.getQueue());\n      CSAssignment assignment = queue.assignContainers(clusterResource, node);\n      \n      RMContainer excessReservation = assignment.getExcessReservation();\n      if (excessReservation != null) {\n      Container container = excessReservation.getContainer();\n      queue.completedContainer(\n          clusterResource, assignment.getApplication(), node, \n          excessReservation, \n          SchedulerUtils.createAbnormalContainerStatus(\n              container.getId(), \n              SchedulerUtils.UNRESERVED_CONTAINER), \n          RMContainerEventType.RELEASED, null);\n      }\n\n    }\n\n    // Try to schedule more if there are no reservations to fulfill\n    if (node.getReservedContainer() == null) {\n      root.assignContainers(clusterResource, node);\n    } else {\n      LOG.info(\"Skipping scheduling since node \" + nm + \n          \" is reserved by application \" + \n          node.getReservedContainer().getContainerId().getApplicationAttemptId()\n          );\n    }\n  \n  }",
                "code_after_change": "  private synchronized void nodeUpdate(RMNode nm) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"nodeUpdate: \" + nm + \" clusterResources: \" + clusterResource);\n    }\n\n    FiCaSchedulerNode node = getNode(nm.getNodeID());\n    \n    // Update resource if any change\n    SchedulerUtils.updateResourceIfChanged(node, nm, clusterResource, LOG);\n    \n    List<UpdatedContainerInfo> containerInfoList = nm.pullContainerUpdates();\n    List<ContainerStatus> newlyLaunchedContainers = new ArrayList<ContainerStatus>();\n    List<ContainerStatus> completedContainers = new ArrayList<ContainerStatus>();\n    for(UpdatedContainerInfo containerInfo : containerInfoList) {\n      newlyLaunchedContainers.addAll(containerInfo.getNewlyLaunchedContainers());\n      completedContainers.addAll(containerInfo.getCompletedContainers());\n    }\n    \n    // Processing the newly launched containers\n    for (ContainerStatus launchedContainer : newlyLaunchedContainers) {\n      containerLaunchedOnNode(launchedContainer.getContainerId(), node);\n    }\n\n    // Process completed containers\n    for (ContainerStatus completedContainer : completedContainers) {\n      ContainerId containerId = completedContainer.getContainerId();\n      LOG.debug(\"Container FINISHED: \" + containerId);\n      completedContainer(getRMContainer(containerId), \n          completedContainer, RMContainerEventType.FINISHED);\n    }\n\n    // Now node data structures are upto date and ready for scheduling.\n    if(LOG.isDebugEnabled()) {\n      LOG.debug(\"Node being looked for scheduling \" + nm\n        + \" availableResource: \" + node.getAvailableResource());\n    }\n\n    // Assign new containers...\n    // 1. Check for reserved applications\n    // 2. Schedule if there are no reservations\n\n    RMContainer reservedContainer = node.getReservedContainer();\n    if (reservedContainer != null) {\n      FiCaSchedulerApp reservedApplication = \n          getApplication(reservedContainer.getApplicationAttemptId());\n      \n      // Try to fulfill the reservation\n      LOG.info(\"Trying to fulfill reservation for application \" + \n          reservedApplication.getApplicationId() + \" on node: \" + nm);\n      \n      LeafQueue queue = ((LeafQueue)reservedApplication.getQueue());\n      CSAssignment assignment = queue.assignContainers(clusterResource, node);\n      \n      RMContainer excessReservation = assignment.getExcessReservation();\n      if (excessReservation != null) {\n      Container container = excessReservation.getContainer();\n      queue.completedContainer(\n          clusterResource, assignment.getApplication(), node, \n          excessReservation, \n          SchedulerUtils.createAbnormalContainerStatus(\n              container.getId(), \n              SchedulerUtils.UNRESERVED_CONTAINER), \n          RMContainerEventType.RELEASED, null);\n      }\n\n    }\n\n    // Try to schedule more if there are no reservations to fulfill\n    if (node.getReservedContainer() == null) {\n      root.assignContainers(clusterResource, node);\n    } else {\n      LOG.info(\"Skipping scheduling since node \" + nm + \n          \" is reserved by application \" + \n          node.getReservedContainer().getContainerId().getApplicationAttemptId()\n          );\n    }\n  \n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-applications.hadoop-yarn-applications-distributedshell.src.main.java.org.apache.hadoop.yarn.applications.distributedshell.Client.run": {
                "code_before_change": "  public boolean run() throws IOException, YarnException {\n\n    LOG.info(\"Running Client\");\n    yarnClient.start();\n\n    YarnClusterMetrics clusterMetrics = yarnClient.getYarnClusterMetrics();\n    LOG.info(\"Got Cluster metric info from ASM\" \n        + \", numNodeManagers=\" + clusterMetrics.getNumNodeManagers());\n\n    List<NodeReport> clusterNodeReports = yarnClient.getNodeReports(\n        NodeState.RUNNING);\n    LOG.info(\"Got Cluster node info from ASM\");\n    for (NodeReport node : clusterNodeReports) {\n      LOG.info(\"Got node report from ASM for\"\n          + \", nodeId=\" + node.getNodeId() \n          + \", nodeAddress\" + node.getHttpAddress()\n          + \", nodeRackName\" + node.getRackName()\n          + \", nodeNumContainers\" + node.getNumContainers());\n    }\n\n    QueueInfo queueInfo = yarnClient.getQueueInfo(this.amQueue);\n    LOG.info(\"Queue info\"\n        + \", queueName=\" + queueInfo.getQueueName()\n        + \", queueCurrentCapacity=\" + queueInfo.getCurrentCapacity()\n        + \", queueMaxCapacity=\" + queueInfo.getMaximumCapacity()\n        + \", queueApplicationCount=\" + queueInfo.getApplications().size()\n        + \", queueChildQueueCount=\" + queueInfo.getChildQueues().size());\t\t\n\n    List<QueueUserACLInfo> listAclInfo = yarnClient.getQueueAclsInfo();\n    for (QueueUserACLInfo aclInfo : listAclInfo) {\n      for (QueueACL userAcl : aclInfo.getUserAcls()) {\n        LOG.info(\"User ACL Info for Queue\"\n            + \", queueName=\" + aclInfo.getQueueName()\t\t\t\n            + \", userAcl=\" + userAcl.name());\n      }\n    }\t\t\n\n    // Get a new application id\n    YarnClientApplication app = yarnClient.createApplication();\n    GetNewApplicationResponse appResponse = app.getNewApplicationResponse();\n    // TODO get min/max resource capabilities from RM and change memory ask if needed\n    // If we do not have min/max, we may not be able to correctly request \n    // the required resources from the RM for the app master\n    // Memory ask has to be a multiple of min and less than max. \n    // Dump out information about cluster capability as seen by the resource manager\n    int maxMem = appResponse.getMaximumResourceCapability().getMemory();\n    LOG.info(\"Max mem capabililty of resources in this cluster \" + maxMem);\n\n    // A resource ask cannot exceed the max. \n    if (amMemory > maxMem) {\n      LOG.info(\"AM memory specified above max threshold of cluster. Using max value.\"\n          + \", specified=\" + amMemory\n          + \", max=\" + maxMem);\n      amMemory = maxMem;\n    }\t\t\t\t\n\n    int maxVCores = appResponse.getMaximumResourceCapability().getVirtualCores();\n    LOG.info(\"Max virtual cores capabililty of resources in this cluster \" + maxVCores);\n    \n    if (amVCores > maxVCores) {\n      LOG.info(\"AM virtual cores specified above max threshold of cluster. \" \n          + \"Using max value.\" + \", specified=\" + amVCores \n          + \", max=\" + maxVCores);\n      amVCores = maxVCores;\n    }\n    \n    // set the application name\n    ApplicationSubmissionContext appContext = app.getApplicationSubmissionContext();\n    ApplicationId appId = appContext.getApplicationId();\n\n    appContext.setKeepContainersAcrossApplicationAttempts(keepContainers);\n    appContext.setApplicationName(appName);\n\n    // Set up the container launch context for the application master\n    ContainerLaunchContext amContainer = Records.newRecord(ContainerLaunchContext.class);\n\n    // set local resources for the application master\n    // local files or archives as needed\n    // In this scenario, the jar file for the application master is part of the local resources\t\t\t\n    Map<String, LocalResource> localResources = new HashMap<String, LocalResource>();\n\n    LOG.info(\"Copy App Master jar from local filesystem and add to local environment\");\n    // Copy the application master jar to the filesystem \n    // Create a local resource to point to the destination jar path \n    FileSystem fs = FileSystem.get(conf);\n    addToLocalResources(fs, appMasterJar, appMasterJarPath, appId.getId(),\n        localResources, null);\n\n    // Set the log4j properties if needed \n    if (!log4jPropFile.isEmpty()) {\n      addToLocalResources(fs, log4jPropFile, log4jPath, appId.getId(),\n          localResources, null);\n    }\t\t\t\n\n    // The shell script has to be made available on the final container(s)\n    // where it will be executed. \n    // To do this, we need to first copy into the filesystem that is visible \n    // to the yarn framework. \n    // We do not need to set this as a local resource for the application \n    // master as the application master does not need it. \t\t\n    String hdfsShellScriptLocation = \"\"; \n    long hdfsShellScriptLen = 0;\n    long hdfsShellScriptTimestamp = 0;\n    if (!shellScriptPath.isEmpty()) {\n      Path shellSrc = new Path(shellScriptPath);\n      String shellPathSuffix =\n          appName + \"/\" + appId.getId() + \"/\"\n              + (Shell.WINDOWS ? windowBatPath : linuxShellPath);\n      Path shellDst =\n          new Path(fs.getHomeDirectory(), shellPathSuffix);\n      fs.copyFromLocalFile(false, true, shellSrc, shellDst);\n      hdfsShellScriptLocation = shellDst.toUri().toString(); \n      FileStatus shellFileStatus = fs.getFileStatus(shellDst);\n      hdfsShellScriptLen = shellFileStatus.getLen();\n      hdfsShellScriptTimestamp = shellFileStatus.getModificationTime();\n    }\n\n    if (!shellCommand.isEmpty()) {\n      addToLocalResources(fs, null, shellCommandPath, appId.getId(),\n          localResources, shellCommand);\n    }\n\n    if (shellArgs.length > 0) {\n      addToLocalResources(fs, null, shellArgsPath, appId.getId(),\n          localResources, StringUtils.join(shellArgs, \" \"));\n    }\n    // Set local resource info into app master container launch context\n    amContainer.setLocalResources(localResources);\n\n    // Set the necessary security tokens as needed\n    //amContainer.setContainerTokens(containerToken);\n\n    // Set the env variables to be setup in the env where the application master will be run\n    LOG.info(\"Set the environment for the application master\");\n    Map<String, String> env = new HashMap<String, String>();\n\n    // put location of shell script into env\n    // using the env info, the application master will create the correct local resource for the \n    // eventual containers that will be launched to execute the shell scripts\n    env.put(DSConstants.DISTRIBUTEDSHELLSCRIPTLOCATION, hdfsShellScriptLocation);\n    env.put(DSConstants.DISTRIBUTEDSHELLSCRIPTTIMESTAMP, Long.toString(hdfsShellScriptTimestamp));\n    env.put(DSConstants.DISTRIBUTEDSHELLSCRIPTLEN, Long.toString(hdfsShellScriptLen));\n\n    // Add AppMaster.jar location to classpath \t\t\n    // At some point we should not be required to add \n    // the hadoop specific classpaths to the env. \n    // It should be provided out of the box. \n    // For now setting all required classpaths including\n    // the classpath to \".\" for the application jar\n    StringBuilder classPathEnv = new StringBuilder(Environment.CLASSPATH.$())\n      .append(File.pathSeparatorChar).append(\"./*\");\n    for (String c : conf.getStrings(\n        YarnConfiguration.YARN_APPLICATION_CLASSPATH,\n        YarnConfiguration.DEFAULT_YARN_APPLICATION_CLASSPATH)) {\n      classPathEnv.append(File.pathSeparatorChar);\n      classPathEnv.append(c.trim());\n    }\n    classPathEnv.append(File.pathSeparatorChar).append(\"./log4j.properties\");\n\n    // add the runtime classpath needed for tests to work\n    if (conf.getBoolean(YarnConfiguration.IS_MINI_YARN_CLUSTER, false)) {\n      classPathEnv.append(':');\n      classPathEnv.append(System.getProperty(\"java.class.path\"));\n    }\n\n    env.put(\"CLASSPATH\", classPathEnv.toString());\n\n    amContainer.setEnvironment(env);\n\n    // Set the necessary command to execute the application master \n    Vector<CharSequence> vargs = new Vector<CharSequence>(30);\n\n    // Set java executable command \n    LOG.info(\"Setting up app master command\");\n    vargs.add(Environment.JAVA_HOME.$() + \"/bin/java\");\n    // Set Xmx based on am memory size\n    vargs.add(\"-Xmx\" + amMemory + \"m\");\n    // Set class name \n    vargs.add(appMasterMainClass);\n    // Set params for Application Master\n    vargs.add(\"--container_memory \" + String.valueOf(containerMemory));\n    vargs.add(\"--container_vcores \" + String.valueOf(containerVirtualCores));\n    vargs.add(\"--num_containers \" + String.valueOf(numContainers));\n    vargs.add(\"--priority \" + String.valueOf(shellCmdPriority));\n\n    for (Map.Entry<String, String> entry : shellEnv.entrySet()) {\n      vargs.add(\"--shell_env \" + entry.getKey() + \"=\" + entry.getValue());\n    }\t\t\t\n    if (debugFlag) {\n      vargs.add(\"--debug\");\n    }\n\n    vargs.add(\"1>\" + ApplicationConstants.LOG_DIR_EXPANSION_VAR + \"/AppMaster.stdout\");\n    vargs.add(\"2>\" + ApplicationConstants.LOG_DIR_EXPANSION_VAR + \"/AppMaster.stderr\");\n\n    // Get final commmand\n    StringBuilder command = new StringBuilder();\n    for (CharSequence str : vargs) {\n      command.append(str).append(\" \");\n    }\n\n    LOG.info(\"Completed setting up app master command \" + command.toString());\t   \n    List<String> commands = new ArrayList<String>();\n    commands.add(command.toString());\t\t\n    amContainer.setCommands(commands);\n\n    // Set up resource type requirements\n    // For now, both memory and vcores are supported, so we set memory and \n    // vcores requirements\n    Resource capability = Records.newRecord(Resource.class);\n    capability.setMemory(amMemory);\n    capability.setVirtualCores(amVCores);\n    appContext.setResource(capability);\n\n    // Service data is a binary blob that can be passed to the application\n    // Not needed in this scenario\n    // amContainer.setServiceData(serviceData);\n\n    // Setup security tokens\n    if (UserGroupInformation.isSecurityEnabled()) {\n      Credentials credentials = new Credentials();\n      String tokenRenewer = conf.get(YarnConfiguration.RM_PRINCIPAL);\n      if (tokenRenewer == null || tokenRenewer.length() == 0) {\n        throw new IOException(\n          \"Can't get Master Kerberos principal for the RM to use as renewer\");\n      }\n\n      // For now, only getting tokens for the default file-system.\n      final Token<?> tokens[] =\n          fs.addDelegationTokens(tokenRenewer, credentials);\n      if (tokens != null) {\n        for (Token<?> token : tokens) {\n          LOG.info(\"Got dt for \" + fs.getUri() + \"; \" + token);\n        }\n      }\n      DataOutputBuffer dob = new DataOutputBuffer();\n      credentials.writeTokenStorageToStream(dob);\n      ByteBuffer fsTokens = ByteBuffer.wrap(dob.getData(), 0, dob.getLength());\n      amContainer.setTokens(fsTokens);\n    }\n\n    appContext.setAMContainerSpec(amContainer);\n\n    // Set the priority for the application master\n    Priority pri = Records.newRecord(Priority.class);\n    // TODO - what is the range for priority? how to decide? \n    pri.setPriority(amPriority);\n    appContext.setPriority(pri);\n\n    // Set the queue to which this application is to be submitted in the RM\n    appContext.setQueue(amQueue);\n\n    // Submit the application to the applications manager\n    // SubmitApplicationResponse submitResp = applicationsManager.submitApplication(appRequest);\n    // Ignore the response as either a valid response object is returned on success \n    // or an exception thrown to denote some form of a failure\n    LOG.info(\"Submitting application to ASM\");\n\n    yarnClient.submitApplication(appContext);\n\n    // TODO\n    // Try submitting the same request again\n    // app submission failure?\n\n    // Monitor the application\n    return monitorApplication(appId);\n\n  }",
                "code_after_change": "  public boolean run() throws IOException, YarnException {\n\n    LOG.info(\"Running Client\");\n    yarnClient.start();\n\n    YarnClusterMetrics clusterMetrics = yarnClient.getYarnClusterMetrics();\n    LOG.info(\"Got Cluster metric info from ASM\" \n        + \", numNodeManagers=\" + clusterMetrics.getNumNodeManagers());\n\n    List<NodeReport> clusterNodeReports = yarnClient.getNodeReports(\n        NodeState.RUNNING);\n    LOG.info(\"Got Cluster node info from ASM\");\n    for (NodeReport node : clusterNodeReports) {\n      LOG.info(\"Got node report from ASM for\"\n          + \", nodeId=\" + node.getNodeId() \n          + \", nodeAddress\" + node.getHttpAddress()\n          + \", nodeRackName\" + node.getRackName()\n          + \", nodeNumContainers\" + node.getNumContainers());\n    }\n\n    QueueInfo queueInfo = yarnClient.getQueueInfo(this.amQueue);\n    LOG.info(\"Queue info\"\n        + \", queueName=\" + queueInfo.getQueueName()\n        + \", queueCurrentCapacity=\" + queueInfo.getCurrentCapacity()\n        + \", queueMaxCapacity=\" + queueInfo.getMaximumCapacity()\n        + \", queueApplicationCount=\" + queueInfo.getApplications().size()\n        + \", queueChildQueueCount=\" + queueInfo.getChildQueues().size());\t\t\n\n    List<QueueUserACLInfo> listAclInfo = yarnClient.getQueueAclsInfo();\n    for (QueueUserACLInfo aclInfo : listAclInfo) {\n      for (QueueACL userAcl : aclInfo.getUserAcls()) {\n        LOG.info(\"User ACL Info for Queue\"\n            + \", queueName=\" + aclInfo.getQueueName()\t\t\t\n            + \", userAcl=\" + userAcl.name());\n      }\n    }\t\t\n\n    // Get a new application id\n    YarnClientApplication app = yarnClient.createApplication();\n    GetNewApplicationResponse appResponse = app.getNewApplicationResponse();\n    // TODO get min/max resource capabilities from RM and change memory ask if needed\n    // If we do not have min/max, we may not be able to correctly request \n    // the required resources from the RM for the app master\n    // Memory ask has to be a multiple of min and less than max. \n    // Dump out information about cluster capability as seen by the resource manager\n    int maxMem = appResponse.getMaximumResourceCapability().getMemory();\n    LOG.info(\"Max mem capabililty of resources in this cluster \" + maxMem);\n\n    // A resource ask cannot exceed the max. \n    if (amMemory > maxMem) {\n      LOG.info(\"AM memory specified above max threshold of cluster. Using max value.\"\n          + \", specified=\" + amMemory\n          + \", max=\" + maxMem);\n      amMemory = maxMem;\n    }\t\t\t\t\n\n    int maxVCores = appResponse.getMaximumResourceCapability().getVirtualCores();\n    LOG.info(\"Max virtual cores capabililty of resources in this cluster \" + maxVCores);\n    \n    if (amVCores > maxVCores) {\n      LOG.info(\"AM virtual cores specified above max threshold of cluster. \" \n          + \"Using max value.\" + \", specified=\" + amVCores \n          + \", max=\" + maxVCores);\n      amVCores = maxVCores;\n    }\n    \n    // set the application name\n    ApplicationSubmissionContext appContext = app.getApplicationSubmissionContext();\n    ApplicationId appId = appContext.getApplicationId();\n    appContext.setApplicationName(appName);\n\n    // Set up the container launch context for the application master\n    ContainerLaunchContext amContainer = Records.newRecord(ContainerLaunchContext.class);\n\n    // set local resources for the application master\n    // local files or archives as needed\n    // In this scenario, the jar file for the application master is part of the local resources\t\t\t\n    Map<String, LocalResource> localResources = new HashMap<String, LocalResource>();\n\n    LOG.info(\"Copy App Master jar from local filesystem and add to local environment\");\n    // Copy the application master jar to the filesystem \n    // Create a local resource to point to the destination jar path \n    FileSystem fs = FileSystem.get(conf);\n    addToLocalResources(fs, appMasterJar, appMasterJarPath, appId.getId(),\n        localResources, null);\n\n    // Set the log4j properties if needed \n    if (!log4jPropFile.isEmpty()) {\n      addToLocalResources(fs, log4jPropFile, log4jPath, appId.getId(),\n          localResources, null);\n    }\t\t\t\n\n    // The shell script has to be made available on the final container(s)\n    // where it will be executed. \n    // To do this, we need to first copy into the filesystem that is visible \n    // to the yarn framework. \n    // We do not need to set this as a local resource for the application \n    // master as the application master does not need it. \t\t\n    String hdfsShellScriptLocation = \"\"; \n    long hdfsShellScriptLen = 0;\n    long hdfsShellScriptTimestamp = 0;\n    if (!shellScriptPath.isEmpty()) {\n      Path shellSrc = new Path(shellScriptPath);\n      String shellPathSuffix =\n          appName + \"/\" + appId.getId() + \"/\"\n              + (Shell.WINDOWS ? windowBatPath : linuxShellPath);\n      Path shellDst =\n          new Path(fs.getHomeDirectory(), shellPathSuffix);\n      fs.copyFromLocalFile(false, true, shellSrc, shellDst);\n      hdfsShellScriptLocation = shellDst.toUri().toString(); \n      FileStatus shellFileStatus = fs.getFileStatus(shellDst);\n      hdfsShellScriptLen = shellFileStatus.getLen();\n      hdfsShellScriptTimestamp = shellFileStatus.getModificationTime();\n    }\n\n    if (!shellCommand.isEmpty()) {\n      addToLocalResources(fs, null, shellCommandPath, appId.getId(),\n          localResources, shellCommand);\n    }\n\n    if (shellArgs.length > 0) {\n      addToLocalResources(fs, null, shellArgsPath, appId.getId(),\n          localResources, StringUtils.join(shellArgs, \" \"));\n    }\n    // Set local resource info into app master container launch context\n    amContainer.setLocalResources(localResources);\n\n    // Set the necessary security tokens as needed\n    //amContainer.setContainerTokens(containerToken);\n\n    // Set the env variables to be setup in the env where the application master will be run\n    LOG.info(\"Set the environment for the application master\");\n    Map<String, String> env = new HashMap<String, String>();\n\n    // put location of shell script into env\n    // using the env info, the application master will create the correct local resource for the \n    // eventual containers that will be launched to execute the shell scripts\n    env.put(DSConstants.DISTRIBUTEDSHELLSCRIPTLOCATION, hdfsShellScriptLocation);\n    env.put(DSConstants.DISTRIBUTEDSHELLSCRIPTTIMESTAMP, Long.toString(hdfsShellScriptTimestamp));\n    env.put(DSConstants.DISTRIBUTEDSHELLSCRIPTLEN, Long.toString(hdfsShellScriptLen));\n\n    // Add AppMaster.jar location to classpath \t\t\n    // At some point we should not be required to add \n    // the hadoop specific classpaths to the env. \n    // It should be provided out of the box. \n    // For now setting all required classpaths including\n    // the classpath to \".\" for the application jar\n    StringBuilder classPathEnv = new StringBuilder(Environment.CLASSPATH.$())\n      .append(File.pathSeparatorChar).append(\"./*\");\n    for (String c : conf.getStrings(\n        YarnConfiguration.YARN_APPLICATION_CLASSPATH,\n        YarnConfiguration.DEFAULT_YARN_APPLICATION_CLASSPATH)) {\n      classPathEnv.append(File.pathSeparatorChar);\n      classPathEnv.append(c.trim());\n    }\n    classPathEnv.append(File.pathSeparatorChar).append(\"./log4j.properties\");\n\n    // add the runtime classpath needed for tests to work\n    if (conf.getBoolean(YarnConfiguration.IS_MINI_YARN_CLUSTER, false)) {\n      classPathEnv.append(':');\n      classPathEnv.append(System.getProperty(\"java.class.path\"));\n    }\n\n    env.put(\"CLASSPATH\", classPathEnv.toString());\n\n    amContainer.setEnvironment(env);\n\n    // Set the necessary command to execute the application master \n    Vector<CharSequence> vargs = new Vector<CharSequence>(30);\n\n    // Set java executable command \n    LOG.info(\"Setting up app master command\");\n    vargs.add(Environment.JAVA_HOME.$() + \"/bin/java\");\n    // Set Xmx based on am memory size\n    vargs.add(\"-Xmx\" + amMemory + \"m\");\n    // Set class name \n    vargs.add(appMasterMainClass);\n    // Set params for Application Master\n    vargs.add(\"--container_memory \" + String.valueOf(containerMemory));\n    vargs.add(\"--container_vcores \" + String.valueOf(containerVirtualCores));\n    vargs.add(\"--num_containers \" + String.valueOf(numContainers));\n    vargs.add(\"--priority \" + String.valueOf(shellCmdPriority));\n\n    for (Map.Entry<String, String> entry : shellEnv.entrySet()) {\n      vargs.add(\"--shell_env \" + entry.getKey() + \"=\" + entry.getValue());\n    }\t\t\t\n    if (debugFlag) {\n      vargs.add(\"--debug\");\n    }\n\n    vargs.add(\"1>\" + ApplicationConstants.LOG_DIR_EXPANSION_VAR + \"/AppMaster.stdout\");\n    vargs.add(\"2>\" + ApplicationConstants.LOG_DIR_EXPANSION_VAR + \"/AppMaster.stderr\");\n\n    // Get final commmand\n    StringBuilder command = new StringBuilder();\n    for (CharSequence str : vargs) {\n      command.append(str).append(\" \");\n    }\n\n    LOG.info(\"Completed setting up app master command \" + command.toString());\t   \n    List<String> commands = new ArrayList<String>();\n    commands.add(command.toString());\t\t\n    amContainer.setCommands(commands);\n\n    // Set up resource type requirements\n    // For now, both memory and vcores are supported, so we set memory and \n    // vcores requirements\n    Resource capability = Records.newRecord(Resource.class);\n    capability.setMemory(amMemory);\n    capability.setVirtualCores(amVCores);\n    appContext.setResource(capability);\n\n    // Service data is a binary blob that can be passed to the application\n    // Not needed in this scenario\n    // amContainer.setServiceData(serviceData);\n\n    // Setup security tokens\n    if (UserGroupInformation.isSecurityEnabled()) {\n      Credentials credentials = new Credentials();\n      String tokenRenewer = conf.get(YarnConfiguration.RM_PRINCIPAL);\n      if (tokenRenewer == null || tokenRenewer.length() == 0) {\n        throw new IOException(\n          \"Can't get Master Kerberos principal for the RM to use as renewer\");\n      }\n\n      // For now, only getting tokens for the default file-system.\n      final Token<?> tokens[] =\n          fs.addDelegationTokens(tokenRenewer, credentials);\n      if (tokens != null) {\n        for (Token<?> token : tokens) {\n          LOG.info(\"Got dt for \" + fs.getUri() + \"; \" + token);\n        }\n      }\n      DataOutputBuffer dob = new DataOutputBuffer();\n      credentials.writeTokenStorageToStream(dob);\n      ByteBuffer fsTokens = ByteBuffer.wrap(dob.getData(), 0, dob.getLength());\n      amContainer.setTokens(fsTokens);\n    }\n\n    appContext.setAMContainerSpec(amContainer);\n\n    // Set the priority for the application master\n    Priority pri = Records.newRecord(Priority.class);\n    // TODO - what is the range for priority? how to decide? \n    pri.setPriority(amPriority);\n    appContext.setPriority(pri);\n\n    // Set the queue to which this application is to be submitted in the RM\n    appContext.setQueue(amQueue);\n\n    // Submit the application to the applications manager\n    // SubmitApplicationResponse submitResp = applicationsManager.submitApplication(appRequest);\n    // Ignore the response as either a valid response object is returned on success \n    // or an exception thrown to denote some form of a failure\n    LOG.info(\"Submitting application to ASM\");\n\n    yarnClient.submitApplication(appContext);\n\n    // TODO\n    // Try submitting the same request again\n    // app submission failure?\n\n    // Monitor the application\n    return monitorApplication(appId);\n\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.AppRejectedTransition": {
                "code_before_change": [],
                "code_after_change": []
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-applications.hadoop-yarn-applications-distributedshell.src.main.java.org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster.run": {
                "code_before_change": "  public boolean run() throws YarnException, IOException {\n    LOG.info(\"Starting ApplicationMaster\");\n\n    Credentials credentials =\n        UserGroupInformation.getCurrentUser().getCredentials();\n    DataOutputBuffer dob = new DataOutputBuffer();\n    credentials.writeTokenStorageToStream(dob);\n    // Now remove the AM->RM token so that containers cannot access it.\n    Iterator<Token<?>> iter = credentials.getAllTokens().iterator();\n    while (iter.hasNext()) {\n      Token<?> token = iter.next();\n      if (token.getKind().equals(AMRMTokenIdentifier.KIND_NAME)) {\n        iter.remove();\n      }\n    }\n    allTokens = ByteBuffer.wrap(dob.getData(), 0, dob.getLength());\n\n    AMRMClientAsync.CallbackHandler allocListener = new RMCallbackHandler();\n    amRMClient = AMRMClientAsync.createAMRMClientAsync(1000, allocListener);\n    amRMClient.init(conf);\n    amRMClient.start();\n\n    containerListener = createNMCallbackHandler();\n    nmClientAsync = new NMClientAsyncImpl(containerListener);\n    nmClientAsync.init(conf);\n    nmClientAsync.start();\n\n    // Setup local RPC Server to accept status requests directly from clients\n    // TODO need to setup a protocol for client to be able to communicate to\n    // the RPC server\n    // TODO use the rpc port info to register with the RM for the client to\n    // send requests to this app master\n\n    // Register self with ResourceManager\n    // This will start heartbeating to the RM\n    appMasterHostname = NetUtils.getHostname();\n    RegisterApplicationMasterResponse response = amRMClient\n        .registerApplicationMaster(appMasterHostname, appMasterRpcPort,\n            appMasterTrackingUrl);\n    // Dump out information about cluster capability as seen by the\n    // resource manager\n    int maxMem = response.getMaximumResourceCapability().getMemory();\n    LOG.info(\"Max mem capabililty of resources in this cluster \" + maxMem);\n    \n    int maxVCores = response.getMaximumResourceCapability().getVirtualCores();\n    LOG.info(\"Max vcores capabililty of resources in this cluster \" + maxVCores);\n\n    // A resource ask cannot exceed the max.\n    if (containerMemory > maxMem) {\n      LOG.info(\"Container memory specified above max threshold of cluster.\"\n          + \" Using max value.\" + \", specified=\" + containerMemory + \", max=\"\n          + maxMem);\n      containerMemory = maxMem;\n    }\n\n    if (containerVirtualCores > maxVCores) {\n      LOG.info(\"Container virtual cores specified above max threshold of cluster.\"\n          + \" Using max value.\" + \", specified=\" + containerVirtualCores + \", max=\"\n          + maxVCores);\n      containerVirtualCores = maxVCores;\n    }\n\n    List<Container> previousAMRunningContainers =\n        response.getContainersFromPreviousAttempt();\n    LOG.info(\"Received \" + previousAMRunningContainers.size()\n        + \" previous AM's running containers on AM registration.\");\n    numAllocatedContainers.addAndGet(previousAMRunningContainers.size());\n\n    int numTotalContainersToRequest =\n        numTotalContainers - previousAMRunningContainers.size();\n    // Setup ask for containers from RM\n    // Send request for containers to RM\n    // Until we get our fully allocated quota, we keep on polling RM for\n    // containers\n    // Keep looping until all the containers are launched and shell script\n    // executed on them ( regardless of success/failure).\n    for (int i = 0; i < numTotalContainersToRequest; ++i) {\n      ContainerRequest containerAsk = setupContainerAskForRM();\n      amRMClient.addContainerRequest(containerAsk);\n    }\n    numRequestedContainers.set(numTotalContainersToRequest);\n    return success;\n  }",
                "code_after_change": "  public boolean run() throws YarnException, IOException {\n    LOG.info(\"Starting ApplicationMaster\");\n\n    Credentials credentials =\n        UserGroupInformation.getCurrentUser().getCredentials();\n    DataOutputBuffer dob = new DataOutputBuffer();\n    credentials.writeTokenStorageToStream(dob);\n    // Now remove the AM->RM token so that containers cannot access it.\n    Iterator<Token<?>> iter = credentials.getAllTokens().iterator();\n    while (iter.hasNext()) {\n      Token<?> token = iter.next();\n      if (token.getKind().equals(AMRMTokenIdentifier.KIND_NAME)) {\n        iter.remove();\n      }\n    }\n    allTokens = ByteBuffer.wrap(dob.getData(), 0, dob.getLength());\n\n    AMRMClientAsync.CallbackHandler allocListener = new RMCallbackHandler();\n    amRMClient = AMRMClientAsync.createAMRMClientAsync(1000, allocListener);\n    amRMClient.init(conf);\n    amRMClient.start();\n\n    containerListener = createNMCallbackHandler();\n    nmClientAsync = new NMClientAsyncImpl(containerListener);\n    nmClientAsync.init(conf);\n    nmClientAsync.start();\n\n    // Setup local RPC Server to accept status requests directly from clients\n    // TODO need to setup a protocol for client to be able to communicate to\n    // the RPC server\n    // TODO use the rpc port info to register with the RM for the client to\n    // send requests to this app master\n\n    // Register self with ResourceManager\n    // This will start heartbeating to the RM\n    appMasterHostname = NetUtils.getHostname();\n    RegisterApplicationMasterResponse response = amRMClient\n        .registerApplicationMaster(appMasterHostname, appMasterRpcPort,\n            appMasterTrackingUrl);\n    // Dump out information about cluster capability as seen by the\n    // resource manager\n    int maxMem = response.getMaximumResourceCapability().getMemory();\n    LOG.info(\"Max mem capabililty of resources in this cluster \" + maxMem);\n    \n    int maxVCores = response.getMaximumResourceCapability().getVirtualCores();\n    LOG.info(\"Max vcores capabililty of resources in this cluster \" + maxVCores);\n\n    // A resource ask cannot exceed the max.\n    if (containerMemory > maxMem) {\n      LOG.info(\"Container memory specified above max threshold of cluster.\"\n          + \" Using max value.\" + \", specified=\" + containerMemory + \", max=\"\n          + maxMem);\n      containerMemory = maxMem;\n    }\n\n    if (containerVirtualCores > maxVCores) {\n      LOG.info(\"Container virtual cores specified above max threshold of cluster.\"\n          + \" Using max value.\" + \", specified=\" + containerVirtualCores + \", max=\"\n          + maxVCores);\n      containerVirtualCores = maxVCores;\n    }\n\n    // Setup ask for containers from RM\n    // Send request for containers to RM\n    // Until we get our fully allocated quota, we keep on polling RM for\n    // containers\n    // Keep looping until all the containers are launched and shell script\n    // executed on them ( regardless of success/failure).\n    for (int i = 0; i < numTotalContainers; ++i) {\n      ContainerRequest containerAsk = setupContainerAskForRM();\n      amRMClient.addContainerRequest(containerAsk);\n    }\n    numRequestedContainers.set(numTotalContainers);\n\n    while (!done\n        && (numCompletedContainers.get() != numTotalContainers)) {\n      try {\n        Thread.sleep(200);\n      } catch (InterruptedException ex) {}\n    }\n    finish();\n    \n    return success;\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.RMAppNodeUpdateTransition": {
                "code_before_change": [],
                "code_after_change": []
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService.registerApplicationMaster": {
                "code_before_change": "  public RegisterApplicationMasterResponse registerApplicationMaster(\n      RegisterApplicationMasterRequest request) throws YarnException,\n      IOException {\n\n    ApplicationAttemptId applicationAttemptId = authorizeRequest();\n\n    ApplicationId appID = applicationAttemptId.getApplicationId();\n    AllocateResponseLock lock = responseMap.get(applicationAttemptId);\n    if (lock == null) {\n      RMAuditLogger.logFailure(this.rmContext.getRMApps().get(appID).getUser(),\n          AuditConstants.REGISTER_AM, \"Application doesn't exist in cache \"\n              + applicationAttemptId, \"ApplicationMasterService\",\n          \"Error in registering application master\", appID,\n          applicationAttemptId);\n      throwApplicationDoesNotExistInCacheException(applicationAttemptId);\n    }\n\n    // Allow only one thread in AM to do registerApp at a time.\n    synchronized (lock) {\n      AllocateResponse lastResponse = lock.getAllocateResponse();\n      if (hasApplicationMasterRegistered(applicationAttemptId)) {\n        String message =\n            \"Application Master is already registered : \"\n                + applicationAttemptId.getApplicationId();\n        LOG.warn(message);\n        RMAuditLogger.logFailure(\n          this.rmContext.getRMApps()\n            .get(applicationAttemptId.getApplicationId()).getUser(),\n          AuditConstants.REGISTER_AM, \"\", \"ApplicationMasterService\", message,\n          applicationAttemptId.getApplicationId(), applicationAttemptId);\n        throw new InvalidApplicationMasterRequestException(message);\n      }\n      \n      this.amLivelinessMonitor.receivedPing(applicationAttemptId);\n      RMApp app = this.rmContext.getRMApps().get(appID);\n      \n      // Setting the response id to 0 to identify if the\n      // application master is register for the respective attemptid\n      lastResponse.setResponseId(0);\n      lock.setAllocateResponse(lastResponse);\n      LOG.info(\"AM registration \" + applicationAttemptId);\n      this.rmContext\n        .getDispatcher()\n        .getEventHandler()\n        .handle(\n          new RMAppAttemptRegistrationEvent(applicationAttemptId, request\n            .getHost(), request.getRpcPort(), request.getTrackingUrl()));\n      RMAuditLogger.logSuccess(app.getUser(), AuditConstants.REGISTER_AM,\n        \"ApplicationMasterService\", appID, applicationAttemptId);\n\n      // Pick up min/max resource from scheduler...\n      RegisterApplicationMasterResponse response = recordFactory\n          .newRecordInstance(RegisterApplicationMasterResponse.class);\n      response.setMaximumResourceCapability(rScheduler\n          .getMaximumResourceCapability());\n      response.setApplicationACLs(app.getRMAppAttempt(applicationAttemptId)\n          .getSubmissionContext().getAMContainerSpec().getApplicationACLs());\n      response.setQueue(app.getQueue());\n      if (UserGroupInformation.isSecurityEnabled()) {\n        LOG.info(\"Setting client token master key\");\n        response.setClientToAMTokenMasterKey(java.nio.ByteBuffer.wrap(rmContext\n            .getClientToAMTokenSecretManager()\n            .getMasterKey(applicationAttemptId).getEncoded()));        \n      }\n\n      List<Container> containerList =\n          ((AbstractYarnScheduler) rScheduler)\n            .getTransferredContainers(applicationAttemptId);\n      response.setContainersFromPreviousAttempt(containerList);\n      return response;\n    }\n  }",
                "code_after_change": "  public RegisterApplicationMasterResponse registerApplicationMaster(\n      RegisterApplicationMasterRequest request) throws YarnException,\n      IOException {\n\n    ApplicationAttemptId applicationAttemptId = authorizeRequest();\n\n    ApplicationId appID = applicationAttemptId.getApplicationId();\n    AllocateResponseLock lock = responseMap.get(applicationAttemptId);\n    if (lock == null) {\n      RMAuditLogger.logFailure(this.rmContext.getRMApps().get(appID).getUser(),\n          AuditConstants.REGISTER_AM, \"Application doesn't exist in cache \"\n              + applicationAttemptId, \"ApplicationMasterService\",\n          \"Error in registering application master\", appID,\n          applicationAttemptId);\n      throwApplicationDoesNotExistInCacheException(applicationAttemptId);\n    }\n\n    // Allow only one thread in AM to do registerApp at a time.\n    synchronized (lock) {\n      AllocateResponse lastResponse = lock.getAllocateResponse();\n      if (hasApplicationMasterRegistered(applicationAttemptId)) {\n        String message =\n            \"Application Master is already registered : \"\n                + applicationAttemptId.getApplicationId();\n        LOG.warn(message);\n        RMAuditLogger.logFailure(\n          this.rmContext.getRMApps()\n            .get(applicationAttemptId.getApplicationId()).getUser(),\n          AuditConstants.REGISTER_AM, \"\", \"ApplicationMasterService\", message,\n          applicationAttemptId.getApplicationId(), applicationAttemptId);\n        throw new InvalidApplicationMasterRequestException(message);\n      }\n      \n      this.amLivelinessMonitor.receivedPing(applicationAttemptId);\n      RMApp app = this.rmContext.getRMApps().get(appID);\n      \n      // Setting the response id to 0 to identify if the\n      // application master is register for the respective attemptid\n      lastResponse.setResponseId(0);\n      lock.setAllocateResponse(lastResponse);\n      LOG.info(\"AM registration \" + applicationAttemptId);\n      this.rmContext\n        .getDispatcher()\n        .getEventHandler()\n        .handle(\n          new RMAppAttemptRegistrationEvent(applicationAttemptId, request\n            .getHost(), request.getRpcPort(), request.getTrackingUrl()));\n      RMAuditLogger.logSuccess(app.getUser(), AuditConstants.REGISTER_AM,\n        \"ApplicationMasterService\", appID, applicationAttemptId);\n\n      // Pick up min/max resource from scheduler...\n      RegisterApplicationMasterResponse response = recordFactory\n          .newRecordInstance(RegisterApplicationMasterResponse.class);\n      response.setMaximumResourceCapability(rScheduler\n          .getMaximumResourceCapability());\n      response.setApplicationACLs(app.getRMAppAttempt(applicationAttemptId)\n          .getSubmissionContext().getAMContainerSpec().getApplicationACLs());\n      response.setQueue(app.getQueue());\n      if (UserGroupInformation.isSecurityEnabled()) {\n        LOG.info(\"Setting client token master key\");\n        response.setClientToAMTokenMasterKey(java.nio.ByteBuffer.wrap(rmContext\n            .getClientToAMTokenSecretManager()\n            .getMasterKey(applicationAttemptId).getEncoded()));        \n      }\n      return response;\n    }\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.activateApplications": {
                "code_before_change": "  private synchronized void activateApplications() {\n    for (Iterator<FiCaSchedulerApp> i=pendingApplications.iterator(); \n         i.hasNext(); ) {\n      FiCaSchedulerApp application = i.next();\n      \n      // Check queue limit\n      if (getNumActiveApplications() >= getMaximumActiveApplications()) {\n        break;\n      }\n      \n      // Check user limit\n      User user = getUser(application.getUser());\n      if (user.getActiveApplications() < getMaximumActiveApplicationsPerUser()) {\n        user.activateApplication();\n        activeApplications.add(application);\n        i.remove();\n        LOG.info(\"Application \" + application.getApplicationId() +\n            \" from user: \" + application.getUser() + \n            \" activated in queue: \" + getQueueName());\n      }\n    }\n  }",
                "code_after_change": "  private synchronized void activateApplications() {\n    for (Iterator<FiCaSchedulerApp> i=pendingApplications.iterator(); \n         i.hasNext(); ) {\n      FiCaSchedulerApp application = i.next();\n      \n      // Check queue limit\n      if (getNumActiveApplications() >= getMaximumActiveApplications()) {\n        break;\n      }\n      \n      // Check user limit\n      User user = getUser(application.getUser());\n      if (user.getActiveApplications() < getMaximumActiveApplicationsPerUser()) {\n        user.activateApplication();\n        activeApplications.add(application);\n        i.remove();\n        LOG.info(\"Application \" + application.getApplicationId() +\n            \" from user: \" + application.getUser() + \n            \" activated in queue: \" + getQueueName());\n      }\n    }\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.allocate": {
                "code_before_change": "  public Allocation allocate(ApplicationAttemptId applicationAttemptId,\n      List<ResourceRequest> ask, List<ContainerId> release, \n      List<String> blacklistAdditions, List<String> blacklistRemovals) {\n\n    FiCaSchedulerApp application = getApplicationAttempt(applicationAttemptId);\n    if (application == null) {\n      LOG.info(\"Calling allocate on removed \" +\n          \"or non existant application \" + applicationAttemptId);\n      return EMPTY_ALLOCATION;\n    }\n    \n    // Sanity check\n    SchedulerUtils.normalizeRequests(\n        ask, getResourceCalculator(), getClusterResources(),\n        getMinimumResourceCapability(), maximumAllocation);\n\n    // Release containers\n    for (ContainerId releasedContainerId : release) {\n      RMContainer rmContainer = getRMContainer(releasedContainerId);\n      if (rmContainer == null) {\n         RMAuditLogger.logFailure(application.getUser(),\n             AuditConstants.RELEASE_CONTAINER, \n             \"Unauthorized access or invalid container\", \"CapacityScheduler\",\n             \"Trying to release container not owned by app or with invalid id\",\n             application.getApplicationId(), releasedContainerId);\n      }\n      completedContainer(rmContainer,\n          SchedulerUtils.createAbnormalContainerStatus(\n              releasedContainerId, \n              SchedulerUtils.RELEASED_CONTAINER),\n          RMContainerEventType.RELEASED);\n    }\n\n    synchronized (application) {\n\n      // make sure we aren't stopping/removing the application\n      // when the allocate comes in\n      if (application.isStopped()) {\n        LOG.info(\"Calling allocate on a stopped \" +\n            \"application \" + applicationAttemptId);\n        return EMPTY_ALLOCATION;\n      }\n\n      if (!ask.isEmpty()) {\n\n        if(LOG.isDebugEnabled()) {\n          LOG.debug(\"allocate: pre-update\" +\n            \" applicationAttemptId=\" + applicationAttemptId + \n            \" application=\" + application);\n        }\n        application.showRequests();\n  \n        // Update application requests\n        application.updateResourceRequests(ask);\n  \n        LOG.debug(\"allocate: post-update\");\n        application.showRequests();\n      }\n\n      if(LOG.isDebugEnabled()) {\n        LOG.debug(\"allocate:\" +\n          \" applicationAttemptId=\" + applicationAttemptId + \n          \" #ask=\" + ask.size());\n      }\n\n      application.updateBlacklist(blacklistAdditions, blacklistRemovals);\n\n      return application.getAllocation(getResourceCalculator(),\n                   clusterResource, getMinimumResourceCapability());\n    }\n  }",
                "code_after_change": "  public Allocation allocate(ApplicationAttemptId applicationAttemptId,\n      List<ResourceRequest> ask, List<ContainerId> release, \n      List<String> blacklistAdditions, List<String> blacklistRemovals) {\n\n    FiCaSchedulerApp application = getApplication(applicationAttemptId);\n    if (application == null) {\n      LOG.info(\"Calling allocate on removed \" +\n          \"or non existant application \" + applicationAttemptId);\n      return EMPTY_ALLOCATION;\n    }\n    \n    // Sanity check\n    SchedulerUtils.normalizeRequests(\n        ask, getResourceCalculator(), getClusterResources(),\n        getMinimumResourceCapability(), maximumAllocation);\n\n    // Release containers\n    for (ContainerId releasedContainerId : release) {\n      RMContainer rmContainer = getRMContainer(releasedContainerId);\n      if (rmContainer == null) {\n         RMAuditLogger.logFailure(application.getUser(),\n             AuditConstants.RELEASE_CONTAINER, \n             \"Unauthorized access or invalid container\", \"CapacityScheduler\",\n             \"Trying to release container not owned by app or with invalid id\",\n             application.getApplicationId(), releasedContainerId);\n      }\n      completedContainer(rmContainer,\n          SchedulerUtils.createAbnormalContainerStatus(\n              releasedContainerId, \n              SchedulerUtils.RELEASED_CONTAINER),\n          RMContainerEventType.RELEASED);\n    }\n\n    synchronized (application) {\n\n      // make sure we aren't stopping/removing the application\n      // when the allocate comes in\n      if (application.isStopped()) {\n        LOG.info(\"Calling allocate on a stopped \" +\n            \"application \" + applicationAttemptId);\n        return EMPTY_ALLOCATION;\n      }\n\n      if (!ask.isEmpty()) {\n\n        if(LOG.isDebugEnabled()) {\n          LOG.debug(\"allocate: pre-update\" +\n            \" applicationAttemptId=\" + applicationAttemptId + \n            \" application=\" + application);\n        }\n        application.showRequests();\n  \n        // Update application requests\n        application.updateResourceRequests(ask);\n  \n        LOG.debug(\"allocate: post-update\");\n        application.showRequests();\n      }\n\n      if(LOG.isDebugEnabled()) {\n        LOG.debug(\"allocate:\" +\n          \" applicationAttemptId=\" + applicationAttemptId + \n          \" #ask=\" + ask.size());\n      }\n\n      application.updateBlacklist(blacklistAdditions, blacklistRemovals);\n\n      return application.getAllocation(getResourceCalculator(),\n                   clusterResource, getMinimumResourceCapability());\n    }\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler.checkAccess": {
                "code_before_change": "  public synchronized boolean checkAccess(UserGroupInformation callerUGI,\n      QueueACL acl, String queueName) {\n    return DEFAULT_QUEUE.hasAccess(acl, callerUGI);\n  }",
                "code_after_change": "  public synchronized boolean checkAccess(UserGroupInformation callerUGI,\n      QueueACL acl, String queueName) {\n    return DEFAULT_QUEUE.hasAccess(acl, callerUGI);\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler.getMaximumResourceCapability": {
                "code_before_change": "  public Resource getMaximumResourceCapability() {\n    return maximumAllocation;\n  }",
                "code_after_change": "  public Resource getMaximumResourceCapability() {\n    return maximumAllocation;\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.allocate": {
                "code_before_change": "  public Allocation allocate(ApplicationAttemptId appAttemptId,\n      List<ResourceRequest> ask, List<ContainerId> release, List<String> blacklistAdditions, List<String> blacklistRemovals) {\n\n    // Make sure this application exists\n    FSSchedulerApp application = getSchedulerApp(appAttemptId);\n    if (application == null) {\n      LOG.info(\"Calling allocate on removed \" +\n          \"or non existant application \" + appAttemptId);\n      return EMPTY_ALLOCATION;\n    }\n\n    // Sanity check\n    SchedulerUtils.normalizeRequests(ask, new DominantResourceCalculator(),\n        clusterCapacity, minimumAllocation, maximumAllocation, incrAllocation);\n\n    // Release containers\n    for (ContainerId releasedContainerId : release) {\n      RMContainer rmContainer = getRMContainer(releasedContainerId);\n      if (rmContainer == null) {\n        RMAuditLogger.logFailure(application.getUser(),\n            AuditConstants.RELEASE_CONTAINER,\n            \"Unauthorized access or invalid container\", \"FairScheduler\",\n            \"Trying to release container not owned by app or with invalid id\",\n            application.getApplicationId(), releasedContainerId);\n      }\n      completedContainer(rmContainer,\n          SchedulerUtils.createAbnormalContainerStatus(\n              releasedContainerId,\n              SchedulerUtils.RELEASED_CONTAINER),\n          RMContainerEventType.RELEASED);\n    }\n\n    synchronized (application) {\n      if (!ask.isEmpty()) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"allocate: pre-update\" +\n              \" applicationAttemptId=\" + appAttemptId +\n              \" application=\" + application.getApplicationId());\n        }\n        application.showRequests();\n\n        // Update application requests\n        application.updateResourceRequests(ask);\n\n        LOG.debug(\"allocate: post-update\");\n        application.showRequests();\n      }\n\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"allocate:\" +\n            \" applicationAttemptId=\" + appAttemptId +\n            \" #ask=\" + ask.size());\n\n        LOG.debug(\"Preempting \" + application.getPreemptionContainers().size()\n            + \" container(s)\");\n      }\n      \n      Set<ContainerId> preemptionContainerIds = new HashSet<ContainerId>();\n      for (RMContainer container : application.getPreemptionContainers()) {\n        preemptionContainerIds.add(container.getContainerId());\n      }\n\n      application.updateBlacklist(blacklistAdditions, blacklistRemovals);\n      \n      return new Allocation(application.pullNewlyAllocatedContainers(),\n          application.getHeadroom(), preemptionContainerIds);\n    }\n  }",
                "code_after_change": "  public Allocation allocate(ApplicationAttemptId appAttemptId,\n      List<ResourceRequest> ask, List<ContainerId> release, List<String> blacklistAdditions, List<String> blacklistRemovals) {\n\n    // Make sure this application exists\n    FSSchedulerApp application = applications.get(appAttemptId);\n    if (application == null) {\n      LOG.info(\"Calling allocate on removed \" +\n          \"or non existant application \" + appAttemptId);\n      return EMPTY_ALLOCATION;\n    }\n\n    // Sanity check\n    SchedulerUtils.normalizeRequests(ask, new DominantResourceCalculator(),\n        clusterCapacity, minimumAllocation, maximumAllocation, incrAllocation);\n\n    // Release containers\n    for (ContainerId releasedContainerId : release) {\n      RMContainer rmContainer = getRMContainer(releasedContainerId);\n      if (rmContainer == null) {\n        RMAuditLogger.logFailure(application.getUser(),\n            AuditConstants.RELEASE_CONTAINER,\n            \"Unauthorized access or invalid container\", \"FairScheduler\",\n            \"Trying to release container not owned by app or with invalid id\",\n            application.getApplicationId(), releasedContainerId);\n      }\n      completedContainer(rmContainer,\n          SchedulerUtils.createAbnormalContainerStatus(\n              releasedContainerId,\n              SchedulerUtils.RELEASED_CONTAINER),\n          RMContainerEventType.RELEASED);\n    }\n\n    synchronized (application) {\n      if (!ask.isEmpty()) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"allocate: pre-update\" +\n              \" applicationAttemptId=\" + appAttemptId +\n              \" application=\" + application.getApplicationId());\n        }\n        application.showRequests();\n\n        // Update application requests\n        application.updateResourceRequests(ask);\n\n        LOG.debug(\"allocate: post-update\");\n        application.showRequests();\n      }\n\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"allocate:\" +\n            \" applicationAttemptId=\" + appAttemptId +\n            \" #ask=\" + ask.size());\n\n        LOG.debug(\"Preempting \" + application.getPreemptionContainers().size()\n            + \" container(s)\");\n      }\n      \n      Set<ContainerId> preemptionContainerIds = new HashSet<ContainerId>();\n      for (RMContainer container : application.getPreemptionContainers()) {\n        preemptionContainerIds.add(container.getContainerId());\n      }\n\n      application.updateBlacklist(blacklistAdditions, blacklistRemovals);\n      \n      return new Allocation(application.pullNewlyAllocatedContainers(),\n          application.getHeadroom(), preemptionContainerIds);\n    }\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.AttemptStartedTransition": {
                "code_before_change": [],
                "code_after_change": []
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-applications.hadoop-yarn-applications-distributedshell.src.main.java.org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster.finish": {
                "code_before_change": "  protected void finish() {\n    // wait for completion.\n    while (!done\n        && (numCompletedContainers.get() != numTotalContainers)) {\n      try {\n        Thread.sleep(200);\n      } catch (InterruptedException ex) {}\n    }\n\n    // Join all launched threads\n    // needed for when we time out\n    // and we need to release containers\n    for (Thread launchThread : launchThreads) {\n      try {\n        launchThread.join(10000);\n      } catch (InterruptedException e) {\n        LOG.info(\"Exception thrown in thread join: \" + e.getMessage());\n        e.printStackTrace();\n      }\n    }\n\n    // When the application completes, it should stop all running containers\n    LOG.info(\"Application completed. Stopping running containers\");\n    nmClientAsync.stop();\n\n    // When the application completes, it should send a finish application\n    // signal to the RM\n    LOG.info(\"Application completed. Signalling finish to RM\");\n\n    FinalApplicationStatus appStatus;\n    String appMessage = null;\n    success = true;\n    if (numFailedContainers.get() == 0 && \n        numCompletedContainers.get() == numTotalContainers) {\n      appStatus = FinalApplicationStatus.SUCCEEDED;\n    } else {\n      appStatus = FinalApplicationStatus.FAILED;\n      appMessage = \"Diagnostics.\" + \", total=\" + numTotalContainers\n          + \", completed=\" + numCompletedContainers.get() + \", allocated=\"\n          + numAllocatedContainers.get() + \", failed=\"\n          + numFailedContainers.get();\n      success = false;\n    }\n    try {\n      amRMClient.unregisterApplicationMaster(appStatus, appMessage, null);\n    } catch (YarnException ex) {\n      LOG.error(\"Failed to unregister application\", ex);\n    } catch (IOException e) {\n      LOG.error(\"Failed to unregister application\", e);\n    }\n    \n    amRMClient.stop();\n  }",
                "code_after_change": "  private void finish() {\n    // Join all launched threads\n    // needed for when we time out\n    // and we need to release containers\n    for (Thread launchThread : launchThreads) {\n      try {\n        launchThread.join(10000);\n      } catch (InterruptedException e) {\n        LOG.info(\"Exception thrown in thread join: \" + e.getMessage());\n        e.printStackTrace();\n      }\n    }\n\n    // When the application completes, it should stop all running containers\n    LOG.info(\"Application completed. Stopping running containers\");\n    nmClientAsync.stop();\n\n    // When the application completes, it should send a finish application\n    // signal to the RM\n    LOG.info(\"Application completed. Signalling finish to RM\");\n\n    FinalApplicationStatus appStatus;\n    String appMessage = null;\n    success = true;\n    if (numFailedContainers.get() == 0 && \n        numCompletedContainers.get() == numTotalContainers) {\n      appStatus = FinalApplicationStatus.SUCCEEDED;\n    } else {\n      appStatus = FinalApplicationStatus.FAILED;\n      appMessage = \"Diagnostics.\" + \", total=\" + numTotalContainers\n          + \", completed=\" + numCompletedContainers.get() + \", allocated=\"\n          + numAllocatedContainers.get() + \", failed=\"\n          + numFailedContainers.get();\n      success = false;\n    }\n    try {\n      amRMClient.unregisterApplicationMaster(appStatus, appMessage, null);\n    } catch (YarnException ex) {\n      LOG.error(\"Failed to unregister application\", ex);\n    } catch (IOException e) {\n      LOG.error(\"Failed to unregister application\", e);\n    }\n    \n    amRMClient.stop();\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle": {
                "code_before_change": "  public void handle(SchedulerEvent event) {\n    switch(event.getType()) {\n    case NODE_ADDED:\n    {\n      NodeAddedSchedulerEvent nodeAddedEvent = (NodeAddedSchedulerEvent)event;\n      addNode(nodeAddedEvent.getAddedRMNode());\n    }\n    break;\n    case NODE_REMOVED:\n    {\n      NodeRemovedSchedulerEvent nodeRemovedEvent = (NodeRemovedSchedulerEvent)event;\n      removeNode(nodeRemovedEvent.getRemovedRMNode());\n    }\n    break;\n    case NODE_UPDATE:\n    {\n      NodeUpdateSchedulerEvent nodeUpdatedEvent = (NodeUpdateSchedulerEvent)event;\n      nodeUpdate(nodeUpdatedEvent.getRMNode());\n    }\n    break;\n    case APP_ADDED:\n    {\n      AppAddedSchedulerEvent appAddedEvent = (AppAddedSchedulerEvent) event;\n      addApplication(appAddedEvent.getApplicationId(),\n        appAddedEvent.getQueue(), appAddedEvent.getUser());\n    }\n    break;\n    case APP_REMOVED:\n    {\n      AppRemovedSchedulerEvent appRemovedEvent = (AppRemovedSchedulerEvent)event;\n      doneApplication(appRemovedEvent.getApplicationID(),\n        appRemovedEvent.getFinalState());\n    }\n    break;\n    case APP_ATTEMPT_ADDED:\n    {\n      AppAttemptAddedSchedulerEvent appAttemptAddedEvent =\n          (AppAttemptAddedSchedulerEvent) event;\n      addApplicationAttempt(appAttemptAddedEvent.getApplicationAttemptId(),\n        appAttemptAddedEvent.getTransferStateFromPreviousAttempt());\n    }\n    break;\n    case APP_ATTEMPT_REMOVED:\n    {\n      AppAttemptRemovedSchedulerEvent appAttemptRemovedEvent =\n          (AppAttemptRemovedSchedulerEvent) event;\n      doneApplicationAttempt(appAttemptRemovedEvent.getApplicationAttemptID(),\n        appAttemptRemovedEvent.getFinalAttemptState(),\n        appAttemptRemovedEvent.getKeepContainersAcrossAppAttempts());\n    }\n    break;\n    case CONTAINER_EXPIRED:\n    {\n      ContainerExpiredSchedulerEvent containerExpiredEvent = \n          (ContainerExpiredSchedulerEvent) event;\n      ContainerId containerId = containerExpiredEvent.getContainerId();\n      completedContainer(getRMContainer(containerId), \n          SchedulerUtils.createAbnormalContainerStatus(\n              containerId, \n              SchedulerUtils.EXPIRED_CONTAINER), \n          RMContainerEventType.EXPIRE);\n    }\n    break;\n    default:\n      LOG.error(\"Invalid eventtype \" + event.getType() + \". Ignoring!\");\n    }\n  }",
                "code_after_change": "  public void handle(SchedulerEvent event) {\n    switch(event.getType()) {\n    case NODE_ADDED:\n    {\n      NodeAddedSchedulerEvent nodeAddedEvent = (NodeAddedSchedulerEvent)event;\n      addNode(nodeAddedEvent.getAddedRMNode());\n    }\n    break;\n    case NODE_REMOVED:\n    {\n      NodeRemovedSchedulerEvent nodeRemovedEvent = (NodeRemovedSchedulerEvent)event;\n      removeNode(nodeRemovedEvent.getRemovedRMNode());\n    }\n    break;\n    case NODE_UPDATE:\n    {\n      NodeUpdateSchedulerEvent nodeUpdatedEvent = (NodeUpdateSchedulerEvent)event;\n      nodeUpdate(nodeUpdatedEvent.getRMNode());\n    }\n    break;\n    case APP_ATTEMPT_ADDED:\n    {\n      AppAttemptAddedSchedulerEvent appAttemptAddedEvent =\n          (AppAttemptAddedSchedulerEvent) event;\n      addApplicationAttempt(appAttemptAddedEvent.getApplicationAttemptId(),\n        appAttemptAddedEvent.getQueue(), appAttemptAddedEvent.getUser());\n    }\n    break;\n    case APP_ATTEMPT_REMOVED:\n    {\n      AppAttemptRemovedSchedulerEvent appAttemptRemovedEvent =\n          (AppAttemptRemovedSchedulerEvent) event;\n      doneApplicationAttempt(appAttemptRemovedEvent.getApplicationAttemptID(),\n        appAttemptRemovedEvent.getFinalAttemptState());\n    }\n    break;\n    case CONTAINER_EXPIRED:\n    {\n      ContainerExpiredSchedulerEvent containerExpiredEvent = \n          (ContainerExpiredSchedulerEvent) event;\n      ContainerId containerId = containerExpiredEvent.getContainerId();\n      completedContainer(getRMContainer(containerId), \n          SchedulerUtils.createAbnormalContainerStatus(\n              containerId, \n              SchedulerUtils.EXPIRED_CONTAINER), \n          RMContainerEventType.EXPIRE);\n    }\n    break;\n    default:\n      LOG.error(\"Invalid eventtype \" + event.getType() + \". Ignoring!\");\n    }\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler.getNodeReport": {
                "code_before_change": "  public synchronized SchedulerNodeReport getNodeReport(NodeId nodeId) {\n    FiCaSchedulerNode node = getNode(nodeId);\n    return node == null ? null : new SchedulerNodeReport(node);\n  }",
                "code_after_change": "  public synchronized SchedulerNodeReport getNodeReport(NodeId nodeId) {\n    FiCaSchedulerNode node = getNode(nodeId);\n    return node == null ? null : new SchedulerNodeReport(node);\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.api.protocolrecords.impl.pb.RegisterApplicationMasterResponsePBImpl.convertToProtoFormat": {
                "code_before_change": "  private ResourceProto convertToProtoFormat(Resource resource) {\n    return ((ResourcePBImpl)resource).getProto();\n  }",
                "code_after_change": "  private ResourceProto convertToProtoFormat(Resource resource) {\n    return ((ResourcePBImpl)resource).getProto();\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.allocate": {
                "code_before_change": "  synchronized public void allocate(NodeType type, SchedulerNode node,\n      Priority priority, ResourceRequest request, Container container) {\n    if (type == NodeType.NODE_LOCAL) {\n      allocateNodeLocal(node, priority, request, container);\n    } else if (type == NodeType.RACK_LOCAL) {\n      allocateRackLocal(node, priority, request, container);\n    } else {\n      allocateOffSwitch(node, priority, request, container);\n    }\n    QueueMetrics metrics = queue.getMetrics();\n    if (pending) {\n      // once an allocation is done we assume the application is\n      // running from scheduler's POV.\n      pending = false;\n      metrics.runAppAttempt(applicationId, user);\n    }\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"allocate: applicationId=\" + applicationId\n          + \" container=\" + container.getId()\n          + \" host=\" + container.getNodeId().toString()\n          + \" user=\" + user\n          + \" resource=\" + request.getCapability());\n    }\n    metrics.allocateResources(user, 1, request.getCapability(), true);\n  }",
                "code_after_change": "  synchronized public void allocate(NodeType type, SchedulerNode node,\n      Priority priority, ResourceRequest request, Container container) {\n    if (type == NodeType.NODE_LOCAL) {\n      allocateNodeLocal(node, priority, request, container);\n    } else if (type == NodeType.RACK_LOCAL) {\n      allocateRackLocal(node, priority, request, container);\n    } else {\n      allocateOffSwitch(node, priority, request, container);\n    }\n    QueueMetrics metrics = queue.getMetrics();\n    if (pending) {\n      // once an allocation is done we assume the application is\n      // running from scheduler's POV.\n      pending = false;\n      metrics.incrAppsRunning(this, user);\n    }\n    LOG.debug(\"allocate: user: \" + user + \", memory: \"\n        + request.getCapability());\n    metrics.allocateResources(user, 1, request.getCapability());\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.StatusUpdateTransition": {
                "code_before_change": [],
                "code_after_change": []
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.QueueMetrics.getMetrics": {
                "code_before_change": "  public void getMetrics(MetricsCollector collector, boolean all) {\n    updateRunningTime();\n    registry.snapshot(collector.addRecord(registry.info()), all);\n  }",
                "code_after_change": "  public void getMetrics(MetricsCollector collector, boolean all) {\n    updateRunningTime();\n    registry.snapshot(collector.addRecord(registry.info()), all);\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.completedContainer": {
                "code_before_change": "  private synchronized void completedContainer(RMContainer rmContainer,\n      ContainerStatus containerStatus, RMContainerEventType event) {\n    if (rmContainer == null) {\n      LOG.info(\"Null container completed...\");\n      return;\n    }\n\n    Container container = rmContainer.getContainer();\n\n    // Get the application for the finished container\n    FSSchedulerApp application =\n        getCurrentAttemptForContainer(container.getId());\n    ApplicationId appId =\n        container.getId().getApplicationAttemptId().getApplicationId();\n    if (application == null) {\n      LOG.info(\"Container \" + container + \" of\" +\n          \" unknown application attempt \" + appId +\n          \" completed with event \" + event);\n      return;\n    }\n\n    // Get the node on which the container was allocated\n    FSSchedulerNode node = nodes.get(container.getNodeId());\n\n    if (rmContainer.getState() == RMContainerState.RESERVED) {\n      application.unreserve(node, rmContainer.getReservedPriority());\n      node.unreserveResource(application);\n    } else {\n      application.containerCompleted(rmContainer, containerStatus, event);\n      node.releaseContainer(container);\n      updateRootQueueMetrics();\n    }\n\n    LOG.info(\"Application attempt \" + application.getApplicationAttemptId()\n        + \" released container \" + container.getId() + \" on node: \" + node\n        + \" with event: \" + event);\n  }",
                "code_after_change": "  private synchronized void completedContainer(RMContainer rmContainer,\n      ContainerStatus containerStatus, RMContainerEventType event) {\n    if (rmContainer == null) {\n      LOG.info(\"Null container completed...\");\n      return;\n    }\n\n    Container container = rmContainer.getContainer();\n\n    // Get the application for the finished container\n    ApplicationAttemptId applicationAttemptId = container.getId().getApplicationAttemptId();\n    FSSchedulerApp application = applications.get(applicationAttemptId);\n    if (application == null) {\n      LOG.info(\"Container \" + container + \" of\" +\n          \" unknown application \" + applicationAttemptId +\n          \" completed with event \" + event);\n      return;\n    }\n\n    // Get the node on which the container was allocated\n    FSSchedulerNode node = nodes.get(container.getNodeId());\n\n    if (rmContainer.getState() == RMContainerState.RESERVED) {\n      application.unreserve(node, rmContainer.getReservedPriority());\n      node.unreserveResource(application);\n    } else {\n      application.containerCompleted(rmContainer, containerStatus, event);\n      node.releaseContainer(container);\n      updateRootQueueMetrics();\n    }\n\n    LOG.info(\"Application \" + applicationAttemptId +\n        \" released container \" + container.getId() +\n        \" on node: \" + node +\n        \" with event: \" + event);\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.addApplication": {
                "code_before_change": [],
                "code_after_change": "  private synchronized void addApplication(FiCaSchedulerApp application, User user) {\n    // Accept \n    user.submitApplication();\n    pendingApplications.add(application);\n    applicationsMap.put(application.getApplicationAttemptId(), application);\n\n    // Activate applications\n    activateApplications();\n    \n    LOG.info(\"Application added -\" +\n        \" appId: \" + application.getApplicationId() +\n        \" user: \" + user + \",\" + \" leaf-queue: \" + getQueueName() +\n        \" #user-pending-applications: \" + user.getPendingApplications() +\n        \" #user-active-applications: \" + user.getActiveApplications() +\n        \" #queue-pending-applications: \" + getNumPendingApplications() +\n        \" #queue-active-applications: \" + getNumActiveApplications()\n        );\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-api.src.main.java.org.apache.hadoop.yarn.api.records.ApplicationSubmissionContext.newInstance": {
                "code_before_change": "  public static ApplicationSubmissionContext newInstance(\n      ApplicationId applicationId, String applicationName, String queue,\n      Priority priority, ContainerLaunchContext amContainer,\n      boolean isUnmanagedAM, boolean cancelTokensWhenComplete,\n      int maxAppAttempts, Resource resource, String applicationType,\n      boolean keepContainers) {\n    ApplicationSubmissionContext context =\n        Records.newRecord(ApplicationSubmissionContext.class);\n    context.setApplicationId(applicationId);\n    context.setApplicationName(applicationName);\n    context.setQueue(queue);\n    context.setPriority(priority);\n    context.setAMContainerSpec(amContainer);\n    context.setUnmanagedAM(isUnmanagedAM);\n    context.setCancelTokensWhenComplete(cancelTokensWhenComplete);\n    context.setMaxAppAttempts(maxAppAttempts);\n    context.setResource(resource);\n    context.setApplicationType(applicationType);\n    context.setKeepContainersAcrossApplicationAttempts(keepContainers);\n    return context;\n  }",
                "code_after_change": "  public static ApplicationSubmissionContext newInstance(\n      ApplicationId applicationId, String applicationName, String queue,\n      Priority priority, ContainerLaunchContext amContainer,\n      boolean isUnmanagedAM, boolean cancelTokensWhenComplete,\n      int maxAppAttempts, Resource resource, String applicationType) {\n    ApplicationSubmissionContext context =\n        Records.newRecord(ApplicationSubmissionContext.class);\n    context.setApplicationId(applicationId);\n    context.setApplicationName(applicationName);\n    context.setQueue(queue);\n    context.setPriority(priority);\n    context.setAMContainerSpec(amContainer);\n    context.setUnmanagedAM(isUnmanagedAM);\n    context.setCancelTokensWhenComplete(cancelTokensWhenComplete);\n    context.setMaxAppAttempts(maxAppAttempts);\n    context.setResource(resource);\n    context.setApplicationType(applicationType);\n    return context;\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.getNodesOnWhichAttemptRan": {
                "code_before_change": "    private Set<NodeId> getNodesOnWhichAttemptRan(RMAppImpl app) {\n      Set<NodeId> nodes = new HashSet<NodeId>();\n      for (RMAppAttempt attempt : app.attempts.values()) {\n        nodes.addAll(attempt.getRanNodes());\n      }\n      return nodes;\n    }",
                "code_after_change": "    private Set<NodeId> getNodesOnWhichAttemptRan(RMAppImpl app) {\n      Set<NodeId> nodes = new HashSet<NodeId>();\n      for (RMAppAttempt attempt : app.attempts.values()) {\n        nodes.addAll(attempt.getRanNodes());\n      }\n      return nodes;\n    }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode.reserveResource": {
                "code_before_change": "  public synchronized void reserveResource(\n      SchedulerApplicationAttempt application, Priority priority, \n      RMContainer reservedContainer) {\n    // Check if it's already reserved\n    if (this.reservedContainer != null) {\n      // Sanity check\n      if (!reservedContainer.getContainer().getNodeId().equals(getNodeID())) {\n        throw new IllegalStateException(\"Trying to reserve\" +\n            \" container \" + reservedContainer +\n            \" on node \" + reservedContainer.getReservedNode() + \n            \" when currently\" + \" reserved resource \" + this.reservedContainer +\n            \" on node \" + this.reservedContainer.getReservedNode());\n      }\n      \n      // Cannot reserve more than one application attempt on a given node!\n      // Reservation is still against attempt.\n      if (!this.reservedContainer.getContainer().getId().getApplicationAttemptId().equals(\n          reservedContainer.getContainer().getId().getApplicationAttemptId())) {\n        throw new IllegalStateException(\"Trying to reserve\" +\n        \t\t\" container \" + reservedContainer + \n            \" for application \" + application.getApplicationAttemptId() + \n            \" when currently\" +\n            \" reserved container \" + this.reservedContainer +\n            \" on node \" + this);\n      }\n\n      LOG.info(\"Updated reserved container \" + \n          reservedContainer.getContainer().getId() + \" on node \" + \n          this + \" for application \" + application);\n    } else {\n      LOG.info(\"Reserved container \" + reservedContainer.getContainer().getId() + \n          \" on node \" + this + \" for application \" + application);\n    }\n    this.reservedContainer = reservedContainer;\n  }",
                "code_after_change": "  public synchronized void reserveResource(\n      SchedulerApplication application, Priority priority, \n      RMContainer reservedContainer) {\n    // Check if it's already reserved\n    if (this.reservedContainer != null) {\n      // Sanity check\n      if (!reservedContainer.getContainer().getNodeId().equals(getNodeID())) {\n        throw new IllegalStateException(\"Trying to reserve\" +\n            \" container \" + reservedContainer +\n            \" on node \" + reservedContainer.getReservedNode() + \n            \" when currently\" + \" reserved resource \" + this.reservedContainer +\n            \" on node \" + this.reservedContainer.getReservedNode());\n      }\n      \n      // Cannot reserve more than one application on a given node!\n      if (!this.reservedContainer.getContainer().getId().getApplicationAttemptId().equals(\n          reservedContainer.getContainer().getId().getApplicationAttemptId())) {\n        throw new IllegalStateException(\"Trying to reserve\" +\n        \t\t\" container \" + reservedContainer + \n            \" for application \" + application.getApplicationAttemptId() + \n            \" when currently\" +\n            \" reserved container \" + this.reservedContainer +\n            \" on node \" + this);\n      }\n\n      LOG.info(\"Updated reserved container \" + \n          reservedContainer.getContainer().getId() + \" on node \" + \n          this + \" for application \" + application);\n    } else {\n      LOG.info(\"Reserved container \" + reservedContainer.getContainer().getId() + \n          \" on node \" + this + \" for application \" + application);\n    }\n    this.reservedContainer = reservedContainer;\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.getAppsInQueue": {
                "code_before_change": "  public List<ApplicationAttemptId> getAppsInQueue(String queueName) {\n    FSQueue queue = queueMgr.getQueue(queueName);\n    if (queue == null) {\n      return null;\n    }\n    List<ApplicationAttemptId> apps = new ArrayList<ApplicationAttemptId>();\n    queue.collectSchedulerApplications(apps);\n    return apps;\n  }",
                "code_after_change": "  public List<ApplicationAttemptId> getAppsInQueue(String queueName) {\n    FSQueue queue = queueMgr.getQueue(queueName);\n    if (queue == null) {\n      return null;\n    }\n    List<ApplicationAttemptId> apps = new ArrayList<ApplicationAttemptId>();\n    queue.collectSchedulerApplications(apps);\n    return apps;\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.api.records.impl.pb.ApplicationSubmissionContextPBImpl.setResource": {
                "code_before_change": "  public void setResource(Resource resource) {\n    maybeInitBuilder();\n    if (resource == null) {\n      builder.clearResource();\n    }\n    this.resource = resource;\n  }",
                "code_after_change": "  public void setResource(Resource resource) {\n    maybeInitBuilder();\n    if (resource == null) {\n      builder.clearResource();\n    }\n    this.resource = resource;\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.stop": {
                "code_before_change": "  synchronized public void stop(RMAppAttemptState rmAppAttemptFinalState) {\n    // clear pending resources metrics for the application\n    QueueMetrics metrics = queue.getMetrics();\n    for (Map<String, ResourceRequest> asks : requests.values()) {\n      ResourceRequest request = asks.get(ResourceRequest.ANY);\n      if (request != null) {\n        metrics.decrPendingResources(user, request.getNumContainers(),\n            request.getCapability());\n      }\n    }\n    metrics.finishAppAttempt(applicationId, pending, user);\n    \n    // Clear requests themselves\n    clearRequests();\n  }",
                "code_after_change": "  synchronized public void stop(RMAppAttemptState rmAppAttemptFinalState) {\n    // clear pending resources metrics for the application\n    QueueMetrics metrics = queue.getMetrics();\n    for (Map<String, ResourceRequest> asks : requests.values()) {\n      ResourceRequest request = asks.get(ResourceRequest.ANY);\n      if (request != null) {\n        metrics.decrPendingResources(user, request.getNumContainers(),\n            Resources.multiply(request.getCapability(), request\n                .getNumContainers()));\n      }\n    }\n    metrics.finishApp(this, rmAppAttemptFinalState);\n    \n    // Clear requests themselves\n    clearRequests();\n  }"
            },
            "hadoop-tools.hadoop-sls.src.main.java.org.apache.hadoop.yarn.sls.scheduler.ResourceSchedulerWrapper.getAppResourceUsageReport": {
                "code_before_change": "  public ApplicationResourceUsageReport getAppResourceUsageReport(\n      ApplicationAttemptId appAttemptId) {\n    return scheduler.getAppResourceUsageReport(appAttemptId);\n  }",
                "code_after_change": "  public ApplicationResourceUsageReport getAppResourceUsageReport(\n      ApplicationAttemptId appAttemptId) {\n    return scheduler.getAppResourceUsageReport(appAttemptId);\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.parseQueue": {
                "code_before_change": "  static CSQueue parseQueue(\n      CapacitySchedulerContext csContext, \n      CapacitySchedulerConfiguration conf, \n      CSQueue parent, String queueName, Map<String, CSQueue> queues,\n      Map<String, CSQueue> oldQueues, \n      QueueHook hook) throws IOException {\n    CSQueue queue;\n    String[] childQueueNames = \n      conf.getQueues((parent == null) ? \n          queueName : (parent.getQueuePath()+\".\"+queueName));\n    if (childQueueNames == null || childQueueNames.length == 0) {\n      if (null == parent) {\n        throw new IllegalStateException(\n            \"Queue configuration missing child queue names for \" + queueName);\n      }\n      queue = \n          new LeafQueue(csContext, queueName, parent,oldQueues.get(queueName));\n      \n      // Used only for unit tests\n      queue = hook.hook(queue);\n    } else {\n      ParentQueue parentQueue = \n        new ParentQueue(csContext, queueName, parent,oldQueues.get(queueName));\n\n      // Used only for unit tests\n      queue = hook.hook(parentQueue);\n      \n      List<CSQueue> childQueues = new ArrayList<CSQueue>();\n      for (String childQueueName : childQueueNames) {\n        CSQueue childQueue = \n          parseQueue(csContext, conf, queue, childQueueName, \n              queues, oldQueues, hook);\n        childQueues.add(childQueue);\n      }\n      parentQueue.setChildQueues(childQueues);\n    }\n\n    if(queue instanceof LeafQueue == true && queues.containsKey(queueName)\n      && queues.get(queueName) instanceof LeafQueue == true) {\n      throw new IOException(\"Two leaf queues were named \" + queueName\n        + \". Leaf queue names must be distinct\");\n    }\n    queues.put(queueName, queue);\n\n    LOG.info(\"Initialized queue: \" + queue);\n    return queue;\n  }",
                "code_after_change": "  static CSQueue parseQueue(\n      CapacitySchedulerContext csContext, \n      CapacitySchedulerConfiguration conf, \n      CSQueue parent, String queueName, Map<String, CSQueue> queues,\n      Map<String, CSQueue> oldQueues, \n      QueueHook hook) throws IOException {\n    CSQueue queue;\n    String[] childQueueNames = \n      conf.getQueues((parent == null) ? \n          queueName : (parent.getQueuePath()+\".\"+queueName));\n    if (childQueueNames == null || childQueueNames.length == 0) {\n      if (null == parent) {\n        throw new IllegalStateException(\n            \"Queue configuration missing child queue names for \" + queueName);\n      }\n      queue = \n          new LeafQueue(csContext, queueName, parent,oldQueues.get(queueName));\n      \n      // Used only for unit tests\n      queue = hook.hook(queue);\n    } else {\n      ParentQueue parentQueue = \n        new ParentQueue(csContext, queueName, parent,oldQueues.get(queueName));\n\n      // Used only for unit tests\n      queue = hook.hook(parentQueue);\n      \n      List<CSQueue> childQueues = new ArrayList<CSQueue>();\n      for (String childQueueName : childQueueNames) {\n        CSQueue childQueue = \n          parseQueue(csContext, conf, queue, childQueueName, \n              queues, oldQueues, hook);\n        childQueues.add(childQueue);\n      }\n      parentQueue.setChildQueues(childQueues);\n    }\n\n    if(queue instanceof LeafQueue == true && queues.containsKey(queueName)\n      && queues.get(queueName) instanceof LeafQueue == true) {\n      throw new IOException(\"Two leaf queues were named \" + queueName\n        + \". Leaf queue names must be distinct\");\n    }\n    queues.put(queueName, queue);\n\n    LOG.info(\"Initialized queue: \" + queue);\n    return queue;\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.hasAccess": {
                "code_before_change": "  public boolean hasAccess(QueueACL acl, UserGroupInformation user) {\n    // Check if the leaf-queue allows access\n    synchronized (this) {\n      if (acls.get(acl).isUserAllowed(user)) {\n        return true;\n      }\n    }\n\n    // Check if parent-queue allows access\n    return getParent().hasAccess(acl, user);\n  }",
                "code_after_change": "  public boolean hasAccess(QueueACL acl, UserGroupInformation user) {\n    // Check if the leaf-queue allows access\n    synchronized (this) {\n      if (acls.get(acl).isUserAllowed(user)) {\n        return true;\n      }\n    }\n\n    // Check if parent-queue allows access\n    return getParent().hasAccess(acl, user);\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.transition": {
                "code_before_change": "    public void transition(RMAppAttemptImpl appAttempt,\n        RMAppAttemptEvent event) {\n    }",
                "code_after_change": "    public void transition(RMAppAttemptImpl appAttempt,\n        RMAppAttemptEvent event) {\n    }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.recover": {
                "code_before_change": "  public void recover(RMState state) throws Exception {\n    ApplicationState appState =\n        state.getApplicationState().get(getAppAttemptId().getApplicationId());\n    ApplicationAttemptState attemptState =\n        appState.getAttempt(getAppAttemptId());\n    assert attemptState != null;\n    LOG.info(\"Recovering attempt: \" + getAppAttemptId() + \" with final state: \"\n        + attemptState.getState());\n    diagnostics.append(\"Attempt recovered after RM restart\");\n    diagnostics.append(attemptState.getDiagnostics());\n    setMasterContainer(attemptState.getMasterContainer());\n    recoverAppAttemptCredentials(attemptState.getAppAttemptCredentials());\n    this.recoveredFinalState = attemptState.getState();\n    this.originalTrackingUrl = attemptState.getFinalTrackingUrl();\n    this.proxiedTrackingUrl = generateProxyUriWithScheme(originalTrackingUrl);\n    this.finalStatus = attemptState.getFinalApplicationStatus();\n    this.startTime = attemptState.getStartTime();\n  }",
                "code_after_change": "  public void recover(RMState state) throws Exception {\n    ApplicationState appState =\n        state.getApplicationState().get(getAppAttemptId().getApplicationId());\n    ApplicationAttemptState attemptState =\n        appState.getAttempt(getAppAttemptId());\n    assert attemptState != null;\n    LOG.info(\"Recovering attempt: \" + getAppAttemptId() + \" with final state: \"\n        + attemptState.getState());\n    diagnostics.append(\"Attempt recovered after RM restart\");\n    diagnostics.append(attemptState.getDiagnostics());\n    setMasterContainer(attemptState.getMasterContainer());\n    recoverAppAttemptCredentials(attemptState.getAppAttemptCredentials());\n    this.recoveredFinalState = attemptState.getState();\n    this.originalTrackingUrl = attemptState.getFinalTrackingUrl();\n    this.proxiedTrackingUrl = generateProxyUriWithScheme(originalTrackingUrl);\n    this.finalStatus = attemptState.getFinalApplicationStatus();\n    this.startTime = attemptState.getStartTime();\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-api.src.main.java.org.apache.hadoop.yarn.api.records.ContainerId.newInstance": {
                "code_before_change": "  public static ContainerId newInstance(ApplicationAttemptId appAttemptId,\n      int containerId) {\n    ContainerId id = Records.newRecord(ContainerId.class);\n    id.setId(containerId);\n    id.setApplicationAttemptId(appAttemptId);\n    id.build();\n    return id;\n  }",
                "code_after_change": "  public static ContainerId newInstance(ApplicationAttemptId appAttemptId,\n      int containerId) {\n    ContainerId id = Records.newRecord(ContainerId.class);\n    id.setId(containerId);\n    id.setApplicationAttemptId(appAttemptId);\n    id.build();\n    return id;\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.ActiveUsersManager.activateApplication": {
                "code_before_change": "  synchronized public void activateApplication(\n      String user, ApplicationId applicationId) {\n    Set<ApplicationId> userApps = usersApplications.get(user);\n    if (userApps == null) {\n      userApps = new HashSet<ApplicationId>();\n      usersApplications.put(user, userApps);\n      ++activeUsers;\n      metrics.incrActiveUsers();\n      LOG.debug(\"User \" + user + \" added to activeUsers, currently: \" + \n          activeUsers);\n    }\n    if (userApps.add(applicationId)) {\n      metrics.activateApp(user);\n    }\n  }",
                "code_after_change": "  synchronized public void activateApplication(\n      String user, ApplicationId applicationId) {\n    Set<ApplicationId> userApps = usersApplications.get(user);\n    if (userApps == null) {\n      userApps = new HashSet<ApplicationId>();\n      usersApplications.put(user, userApps);\n      ++activeUsers;\n      metrics.incrActiveUsers();\n      LOG.debug(\"User \" + user + \" added to activeUsers, currently: \" + \n          activeUsers);\n    }\n    if (userApps.add(applicationId)) {\n      metrics.activateApp(user);\n    }\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.AppAttemptRemovedSchedulerEvent.getApplicationAttemptID": {
                "code_before_change": "  public ApplicationAttemptId getApplicationAttemptID() {\n    return this.applicationAttemptId;\n  }",
                "code_after_change": "  public ApplicationAttemptId getApplicationAttemptID() {\n    return this.applicationAttemptId;\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.getEventLog": {
                "code_before_change": "  public FairSchedulerEventLog getEventLog() {\n    return eventLog;\n  }",
                "code_after_change": "  public FairSchedulerEventLog getEventLog() {\n    return eventLog;\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.getClusterResources": {
                "code_before_change": "  public Resource getClusterResources() {\n    return clusterResource;\n  }",
                "code_after_change": "  public Resource getClusterResources() {\n    return clusterResource;\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.ActiveUsersManager.ActiveUsersManager": {
                "code_before_change": [],
                "code_after_change": []
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.submitApplication": {
                "code_before_change": "  public void submitApplication(ApplicationId applicationId, String user,\n      String queue) throws AccessControlException {\n    \n    synchronized (this) {\n      // Sanity check\n      if (queue.equals(queueName)) {\n        throw new AccessControlException(\"Cannot submit application \" +\n            \"to non-leaf queue: \" + queueName);\n      }\n      \n      if (state != QueueState.RUNNING) {\n        throw new AccessControlException(\"Queue \" + getQueuePath() +\n            \" is STOPPED. Cannot accept submission of application: \" +\n            applicationId);\n      }\n\n      addApplication(applicationId, user);\n    }\n    \n    // Inform the parent queue\n    if (parent != null) {\n      try {\n        parent.submitApplication(applicationId, user, queue);\n      } catch (AccessControlException ace) {\n        LOG.info(\"Failed to submit application to parent-queue: \" + \n            parent.getQueuePath(), ace);\n        removeApplication(applicationId, user);\n        throw ace;\n      }\n    }\n  }",
                "code_after_change": "  public void submitApplication(FiCaSchedulerApp application, String user,\n      String queue) throws AccessControlException {\n    \n    synchronized (this) {\n      // Sanity check\n      if (queue.equals(queueName)) {\n        throw new AccessControlException(\"Cannot submit application \" +\n            \"to non-leaf queue: \" + queueName);\n      }\n      \n      if (state != QueueState.RUNNING) {\n        throw new AccessControlException(\"Queue \" + getQueuePath() +\n            \" is STOPPED. Cannot accept submission of application: \" +\n            application.getApplicationId());\n      }\n\n      addApplication(application, user);\n    }\n    \n    // Inform the parent queue\n    if (parent != null) {\n      try {\n        parent.submitApplication(application, user, queue);\n      } catch (AccessControlException ace) {\n        LOG.info(\"Failed to submit application to parent-queue: \" + \n            parent.getQueuePath(), ace);\n        removeApplication(application, user);\n        throw ace;\n      }\n    }\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-api.src.main.java.org.apache.hadoop.yarn.api.protocolrecords.RegisterApplicationMasterResponse.newInstance": {
                "code_before_change": "  public static RegisterApplicationMasterResponse newInstance(\n      Resource minCapability, Resource maxCapability,\n      Map<ApplicationAccessType, String> acls, ByteBuffer key,\n      List<Container> containersFromPreviousAttempt, String queue) {\n    RegisterApplicationMasterResponse response =\n        Records.newRecord(RegisterApplicationMasterResponse.class);\n    response.setMaximumResourceCapability(maxCapability);\n    response.setApplicationACLs(acls);\n    response.setClientToAMTokenMasterKey(key);\n    response.setContainersFromPreviousAttempt(containersFromPreviousAttempt);\n    response.setQueue(queue);\n    return response;\n  }",
                "code_after_change": "  public static RegisterApplicationMasterResponse newInstance(\n      Resource minCapability, Resource maxCapability,\n      Map<ApplicationAccessType, String> acls, ByteBuffer key,\n      String queue) {\n    RegisterApplicationMasterResponse response =\n        Records.newRecord(RegisterApplicationMasterResponse.class);\n    response.setMaximumResourceCapability(maxCapability);\n    response.setApplicationACLs(acls);\n    response.setClientToAMTokenMasterKey(key);\n    response.setQueue(queue);\n    return response;\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.handle": {
                "code_before_change": "  public void handle(SchedulerEvent event) {\n    switch (event.getType()) {\n    case NODE_ADDED:\n      if (!(event instanceof NodeAddedSchedulerEvent)) {\n        throw new RuntimeException(\"Unexpected event type: \" + event);\n      }\n      NodeAddedSchedulerEvent nodeAddedEvent = (NodeAddedSchedulerEvent)event;\n      addNode(nodeAddedEvent.getAddedRMNode());\n      break;\n    case NODE_REMOVED:\n      if (!(event instanceof NodeRemovedSchedulerEvent)) {\n        throw new RuntimeException(\"Unexpected event type: \" + event);\n      }\n      NodeRemovedSchedulerEvent nodeRemovedEvent = (NodeRemovedSchedulerEvent)event;\n      removeNode(nodeRemovedEvent.getRemovedRMNode());\n      break;\n    case NODE_UPDATE:\n      if (!(event instanceof NodeUpdateSchedulerEvent)) {\n        throw new RuntimeException(\"Unexpected event type: \" + event);\n      }\n      NodeUpdateSchedulerEvent nodeUpdatedEvent = (NodeUpdateSchedulerEvent)event;\n      nodeUpdate(nodeUpdatedEvent.getRMNode());\n      break;\n    case APP_ADDED:\n      if (!(event instanceof AppAddedSchedulerEvent)) {\n        throw new RuntimeException(\"Unexpected event type: \" + event);\n      }\n      AppAddedSchedulerEvent appAddedEvent = (AppAddedSchedulerEvent) event;\n      addApplication(appAddedEvent.getApplicationId(),\n        appAddedEvent.getQueue(), appAddedEvent.getUser());\n      break;\n    case APP_REMOVED:\n      if (!(event instanceof AppRemovedSchedulerEvent)) {\n        throw new RuntimeException(\"Unexpected event type: \" + event);\n      }\n      AppRemovedSchedulerEvent appRemovedEvent = (AppRemovedSchedulerEvent)event;\n      removeApplication(appRemovedEvent.getApplicationID(),\n        appRemovedEvent.getFinalState());\n      break;\n    case APP_ATTEMPT_ADDED:\n      if (!(event instanceof AppAttemptAddedSchedulerEvent)) {\n        throw new RuntimeException(\"Unexpected event type: \" + event);\n      }\n      AppAttemptAddedSchedulerEvent appAttemptAddedEvent =\n          (AppAttemptAddedSchedulerEvent) event;\n      addApplicationAttempt(appAttemptAddedEvent.getApplicationAttemptId(),\n        appAttemptAddedEvent.getTransferStateFromPreviousAttempt());\n      break;\n    case APP_ATTEMPT_REMOVED:\n      if (!(event instanceof AppAttemptRemovedSchedulerEvent)) {\n        throw new RuntimeException(\"Unexpected event type: \" + event);\n      }\n      AppAttemptRemovedSchedulerEvent appAttemptRemovedEvent =\n          (AppAttemptRemovedSchedulerEvent) event;\n      removeApplicationAttempt(\n          appAttemptRemovedEvent.getApplicationAttemptID(),\n          appAttemptRemovedEvent.getFinalAttemptState(),\n          appAttemptRemovedEvent.getKeepContainersAcrossAppAttempts());\n      break;\n    case CONTAINER_EXPIRED:\n      if (!(event instanceof ContainerExpiredSchedulerEvent)) {\n        throw new RuntimeException(\"Unexpected event type: \" + event);\n      }\n      ContainerExpiredSchedulerEvent containerExpiredEvent =\n          (ContainerExpiredSchedulerEvent)event;\n      ContainerId containerId = containerExpiredEvent.getContainerId();\n      completedContainer(getRMContainer(containerId),\n          SchedulerUtils.createAbnormalContainerStatus(\n              containerId,\n              SchedulerUtils.EXPIRED_CONTAINER),\n          RMContainerEventType.EXPIRE);\n      break;\n    default:\n      LOG.error(\"Unknown event arrived at FairScheduler: \" + event.toString());\n    }\n  }",
                "code_after_change": "  public void handle(SchedulerEvent event) {\n    switch (event.getType()) {\n    case NODE_ADDED:\n      if (!(event instanceof NodeAddedSchedulerEvent)) {\n        throw new RuntimeException(\"Unexpected event type: \" + event);\n      }\n      NodeAddedSchedulerEvent nodeAddedEvent = (NodeAddedSchedulerEvent)event;\n      addNode(nodeAddedEvent.getAddedRMNode());\n      break;\n    case NODE_REMOVED:\n      if (!(event instanceof NodeRemovedSchedulerEvent)) {\n        throw new RuntimeException(\"Unexpected event type: \" + event);\n      }\n      NodeRemovedSchedulerEvent nodeRemovedEvent = (NodeRemovedSchedulerEvent)event;\n      removeNode(nodeRemovedEvent.getRemovedRMNode());\n      break;\n    case NODE_UPDATE:\n      if (!(event instanceof NodeUpdateSchedulerEvent)) {\n        throw new RuntimeException(\"Unexpected event type: \" + event);\n      }\n      NodeUpdateSchedulerEvent nodeUpdatedEvent = (NodeUpdateSchedulerEvent)event;\n      nodeUpdate(nodeUpdatedEvent.getRMNode());\n      break;\n    case APP_ATTEMPT_ADDED:\n      if (!(event instanceof AppAttemptAddedSchedulerEvent)) {\n        throw new RuntimeException(\"Unexpected event type: \" + event);\n      }\n      AppAttemptAddedSchedulerEvent appAttemptAddedEvent =\n          (AppAttemptAddedSchedulerEvent) event;\n      String queue = appAttemptAddedEvent.getQueue();\n      addApplicationAttempt(appAttemptAddedEvent.getApplicationAttemptId(),\n        queue, appAttemptAddedEvent.getUser());\n      break;\n    case APP_ATTEMPT_REMOVED:\n      if (!(event instanceof AppAttemptRemovedSchedulerEvent)) {\n        throw new RuntimeException(\"Unexpected event type: \" + event);\n      }\n      AppAttemptRemovedSchedulerEvent appAttemptRemovedEvent =\n          (AppAttemptRemovedSchedulerEvent) event;\n      removeApplicationAttempt(appAttemptRemovedEvent.getApplicationAttemptID(),\n        appAttemptRemovedEvent.getFinalAttemptState());\n      break;\n    case CONTAINER_EXPIRED:\n      if (!(event instanceof ContainerExpiredSchedulerEvent)) {\n        throw new RuntimeException(\"Unexpected event type: \" + event);\n      }\n      ContainerExpiredSchedulerEvent containerExpiredEvent =\n          (ContainerExpiredSchedulerEvent)event;\n      ContainerId containerId = containerExpiredEvent.getContainerId();\n      completedContainer(getRMContainer(containerId),\n          SchedulerUtils.createAbnormalContainerStatus(\n              containerId,\n              SchedulerUtils.EXPIRED_CONTAINER),\n          RMContainerEventType.EXPIRE);\n      break;\n    default:\n      LOG.error(\"Unknown event arrived at FairScheduler: \" + event.toString());\n    }\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-applications.hadoop-yarn-applications-distributedshell.src.main.java.org.apache.hadoop.yarn.applications.distributedshell.Client.init": {
                "code_before_change": "  public boolean init(String[] args) throws ParseException {\n\n    CommandLine cliParser = new GnuParser().parse(opts, args);\n\n    if (args.length == 0) {\n      throw new IllegalArgumentException(\"No args specified for client to initialize\");\n    }\n\n    if (cliParser.hasOption(\"log_properties\")) {\n      String log4jPath = cliParser.getOptionValue(\"log_properties\");\n      try {\n        Log4jPropertyHelper.updateLog4jConfiguration(Client.class, log4jPath);\n      } catch (Exception e) {\n        LOG.warn(\"Can not set up custom log4j properties. \" + e);\n      }\n    }\n\n    if (cliParser.hasOption(\"help\")) {\n      printUsage();\n      return false;\n    }\n\n    if (cliParser.hasOption(\"debug\")) {\n      debugFlag = true;\n\n    }\n\n    if (cliParser.hasOption(\"keep_containers_across_application_attempts\")) {\n      LOG.info(\"keep_containers_across_application_attempts\");\n      keepContainers = true;\n    }\n\n    appName = cliParser.getOptionValue(\"appname\", \"DistributedShell\");\n    amPriority = Integer.parseInt(cliParser.getOptionValue(\"priority\", \"0\"));\n    amQueue = cliParser.getOptionValue(\"queue\", \"default\");\n    amMemory = Integer.parseInt(cliParser.getOptionValue(\"master_memory\", \"10\"));\t\t\n    amVCores = Integer.parseInt(cliParser.getOptionValue(\"master_vcores\", \"1\"));\n\n    if (amMemory < 0) {\n      throw new IllegalArgumentException(\"Invalid memory specified for application master, exiting.\"\n          + \" Specified memory=\" + amMemory);\n    }\n    if (amVCores < 0) {\n      throw new IllegalArgumentException(\"Invalid virtual cores specified for application master, exiting.\"\n          + \" Specified virtual cores=\" + amVCores);\n    }\n\n    if (!cliParser.hasOption(\"jar\")) {\n      throw new IllegalArgumentException(\"No jar file specified for application master\");\n    }\t\t\n\n    appMasterJar = cliParser.getOptionValue(\"jar\");\n\n    if (!cliParser.hasOption(\"shell_command\") && !cliParser.hasOption(\"shell_script\")) {\n      throw new IllegalArgumentException(\n          \"No shell command or shell script specified to be executed by application master\");\n    } else if (cliParser.hasOption(\"shell_command\") && cliParser.hasOption(\"shell_script\")) {\n      throw new IllegalArgumentException(\"Can not specify shell_command option \" +\n          \"and shell_script option at the same time\");\n    } else if (cliParser.hasOption(\"shell_command\")) {\n      shellCommand = cliParser.getOptionValue(\"shell_command\");\n    } else {\n      shellScriptPath = cliParser.getOptionValue(\"shell_script\");\n    }\n    if (cliParser.hasOption(\"shell_args\")) {\n      shellArgs = cliParser.getOptionValues(\"shell_args\");\n    }\n    if (cliParser.hasOption(\"shell_env\")) { \n      String envs[] = cliParser.getOptionValues(\"shell_env\");\n      for (String env : envs) {\n        env = env.trim();\n        int index = env.indexOf('=');\n        if (index == -1) {\n          shellEnv.put(env, \"\");\n          continue;\n        }\n        String key = env.substring(0, index);\n        String val = \"\";\n        if (index < (env.length()-1)) {\n          val = env.substring(index+1);\n        }\n        shellEnv.put(key, val);\n      }\n    }\n    shellCmdPriority = Integer.parseInt(cliParser.getOptionValue(\"shell_cmd_priority\", \"0\"));\n\n    containerMemory = Integer.parseInt(cliParser.getOptionValue(\"container_memory\", \"10\"));\n    containerVirtualCores = Integer.parseInt(cliParser.getOptionValue(\"container_vcores\", \"1\"));\n    numContainers = Integer.parseInt(cliParser.getOptionValue(\"num_containers\", \"1\"));\n\n    if (containerMemory < 0 || containerVirtualCores < 0 || numContainers < 1) {\n      throw new IllegalArgumentException(\"Invalid no. of containers or container memory/vcores specified,\"\n          + \" exiting.\"\n          + \" Specified containerMemory=\" + containerMemory\n          + \", containerVirtualCores=\" + containerVirtualCores\n          + \", numContainer=\" + numContainers);\n    }\n\n    clientTimeout = Integer.parseInt(cliParser.getOptionValue(\"timeout\", \"600000\"));\n\n    log4jPropFile = cliParser.getOptionValue(\"log_properties\", \"\");\n\n    return true;\n  }",
                "code_after_change": "  public boolean init(String[] args) throws ParseException {\n\n    CommandLine cliParser = new GnuParser().parse(opts, args);\n\n    if (args.length == 0) {\n      throw new IllegalArgumentException(\"No args specified for client to initialize\");\n    }\n\n    if (cliParser.hasOption(\"log_properties\")) {\n      String log4jPath = cliParser.getOptionValue(\"log_properties\");\n      try {\n        Log4jPropertyHelper.updateLog4jConfiguration(Client.class, log4jPath);\n      } catch (Exception e) {\n        LOG.warn(\"Can not set up custom log4j properties. \" + e);\n      }\n    }\n\n    if (cliParser.hasOption(\"help\")) {\n      printUsage();\n      return false;\n    }\n\n    if (cliParser.hasOption(\"debug\")) {\n      debugFlag = true;\n\n    }\n\n    appName = cliParser.getOptionValue(\"appname\", \"DistributedShell\");\n    amPriority = Integer.parseInt(cliParser.getOptionValue(\"priority\", \"0\"));\n    amQueue = cliParser.getOptionValue(\"queue\", \"default\");\n    amMemory = Integer.parseInt(cliParser.getOptionValue(\"master_memory\", \"10\"));\t\t\n    amVCores = Integer.parseInt(cliParser.getOptionValue(\"master_vcores\", \"1\"));\n    \n    if (amMemory < 0) {\n      throw new IllegalArgumentException(\"Invalid memory specified for application master, exiting.\"\n          + \" Specified memory=\" + amMemory);\n    }\n    if (amVCores < 0) {\n      throw new IllegalArgumentException(\"Invalid virtual cores specified for application master, exiting.\"\n          + \" Specified virtual cores=\" + amVCores);\n    }\n\n    if (!cliParser.hasOption(\"jar\")) {\n      throw new IllegalArgumentException(\"No jar file specified for application master\");\n    }\t\t\n\n    appMasterJar = cliParser.getOptionValue(\"jar\");\n\n    if (!cliParser.hasOption(\"shell_command\") && !cliParser.hasOption(\"shell_script\")) {\n      throw new IllegalArgumentException(\n          \"No shell command or shell script specified to be executed by application master\");\n    } else if (cliParser.hasOption(\"shell_command\") && cliParser.hasOption(\"shell_script\")) {\n      throw new IllegalArgumentException(\"Can not specify shell_command option \" +\n          \"and shell_script option at the same time\");\n    } else if (cliParser.hasOption(\"shell_command\")) {\n      shellCommand = cliParser.getOptionValue(\"shell_command\");\n    } else {\n      shellScriptPath = cliParser.getOptionValue(\"shell_script\");\n    }\n    if (cliParser.hasOption(\"shell_args\")) {\n      shellArgs = cliParser.getOptionValues(\"shell_args\");\n    }\n    if (cliParser.hasOption(\"shell_env\")) { \n      String envs[] = cliParser.getOptionValues(\"shell_env\");\n      for (String env : envs) {\n        env = env.trim();\n        int index = env.indexOf('=');\n        if (index == -1) {\n          shellEnv.put(env, \"\");\n          continue;\n        }\n        String key = env.substring(0, index);\n        String val = \"\";\n        if (index < (env.length()-1)) {\n          val = env.substring(index+1);\n        }\n        shellEnv.put(key, val);\n      }\n    }\n    shellCmdPriority = Integer.parseInt(cliParser.getOptionValue(\"shell_cmd_priority\", \"0\"));\n\n    containerMemory = Integer.parseInt(cliParser.getOptionValue(\"container_memory\", \"10\"));\n    containerVirtualCores = Integer.parseInt(cliParser.getOptionValue(\"container_vcores\", \"1\"));\n    numContainers = Integer.parseInt(cliParser.getOptionValue(\"num_containers\", \"1\"));\n\n    if (containerMemory < 0 || containerVirtualCores < 0 || numContainers < 1) {\n      throw new IllegalArgumentException(\"Invalid no. of containers or container memory/vcores specified,\"\n          + \" exiting.\"\n          + \" Specified containerMemory=\" + containerMemory\n          + \", containerVirtualCores=\" + containerVirtualCores\n          + \", numContainer=\" + numContainers);\n    }\n\n    clientTimeout = Integer.parseInt(cliParser.getOptionValue(\"timeout\", \"600000\"));\n\n    log4jPropFile = cliParser.getOptionValue(\"log_properties\", \"\");\n\n    return true;\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.getConf": {
                "code_before_change": "  public Configuration getConf() {\n    return yarnConf;\n  }",
                "code_after_change": "  public Configuration getConf() {\n    return yarnConf;\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.rememberTargetTransitionsAndStoreState": {
                "code_before_change": "  private void rememberTargetTransitionsAndStoreState(RMAppAttemptEvent event,\n      Object transitionToDo, RMAppAttemptState targetFinalState,\n      RMAppAttemptState stateToBeStored) {\n\n    rememberTargetTransitions(event, transitionToDo, targetFinalState);\n\n    // As of today, finalState, diagnostics, final-tracking-url and\n    // finalAppStatus are the only things that we store into the StateStore\n    // AFTER the initial saving on app-attempt-start\n    // These fields can be visible from outside only after they are saved in\n    // StateStore\n    String diags = null;\n    String finalTrackingUrl = null;\n    FinalApplicationStatus finalStatus = null;\n\n    switch (event.getType()) {\n    case LAUNCH_FAILED:\n      RMAppAttemptLaunchFailedEvent launchFaileEvent =\n          (RMAppAttemptLaunchFailedEvent) event;\n      diags = launchFaileEvent.getMessage();\n      break;\n    case REGISTERED:\n      diags = getUnexpectedAMRegisteredDiagnostics();\n      break;\n    case UNREGISTERED:\n      RMAppAttemptUnregistrationEvent unregisterEvent =\n          (RMAppAttemptUnregistrationEvent) event;\n      diags = unregisterEvent.getDiagnostics();\n      finalTrackingUrl = sanitizeTrackingUrl(unregisterEvent.getFinalTrackingUrl());\n      finalStatus = unregisterEvent.getFinalApplicationStatus();\n      break;\n    case CONTAINER_FINISHED:\n      RMAppAttemptContainerFinishedEvent finishEvent =\n          (RMAppAttemptContainerFinishedEvent) event;\n      diags = getAMContainerCrashedDiagnostics(finishEvent);\n      break;\n    case KILL:\n      break;\n    case EXPIRE:\n      diags = getAMExpiredDiagnostics(event);\n      break;\n    default:\n      break;\n    }\n\n    RMStateStore rmStore = rmContext.getStateStore();\n    ApplicationAttemptState attemptState =\n        new ApplicationAttemptState(applicationAttemptId, getMasterContainer(),\n          rmStore.getCredentialsFromAppAttempt(this), startTime,\n          stateToBeStored, finalTrackingUrl, diags, finalStatus);\n    LOG.info(\"Updating application attempt \" + applicationAttemptId\n        + \" with final state: \" + targetedFinalState);\n    rmStore.updateApplicationAttemptState(attemptState);\n  }",
                "code_after_change": "  private void rememberTargetTransitionsAndStoreState(RMAppAttemptEvent event,\n      Object transitionToDo, RMAppAttemptState targetFinalState,\n      RMAppAttemptState stateToBeStored) {\n\n    rememberTargetTransitions(event, transitionToDo, targetFinalState);\n\n    // As of today, finalState, diagnostics, final-tracking-url and\n    // finalAppStatus are the only things that we store into the StateStore\n    // AFTER the initial saving on app-attempt-start\n    // These fields can be visible from outside only after they are saved in\n    // StateStore\n    String diags = null;\n    String finalTrackingUrl = null;\n    FinalApplicationStatus finalStatus = null;\n\n    switch (event.getType()) {\n    case APP_REJECTED:\n      RMAppAttemptRejectedEvent rejectedEvent =\n          (RMAppAttemptRejectedEvent) event;\n      diags = rejectedEvent.getMessage();\n      break;\n    case LAUNCH_FAILED:\n      RMAppAttemptLaunchFailedEvent launchFaileEvent =\n          (RMAppAttemptLaunchFailedEvent) event;\n      diags = launchFaileEvent.getMessage();\n      break;\n    case REGISTERED:\n      diags = getUnexpectedAMRegisteredDiagnostics();\n      break;\n    case UNREGISTERED:\n      RMAppAttemptUnregistrationEvent unregisterEvent =\n          (RMAppAttemptUnregistrationEvent) event;\n      diags = unregisterEvent.getDiagnostics();\n      finalTrackingUrl = sanitizeTrackingUrl(unregisterEvent.getFinalTrackingUrl());\n      finalStatus = unregisterEvent.getFinalApplicationStatus();\n      break;\n    case CONTAINER_FINISHED:\n      RMAppAttemptContainerFinishedEvent finishEvent =\n          (RMAppAttemptContainerFinishedEvent) event;\n      diags = getAMContainerCrashedDiagnostics(finishEvent);\n      break;\n    case KILL:\n      break;\n    case EXPIRE:\n      diags = getAMExpiredDiagnostics(event);\n      break;\n    default:\n      break;\n    }\n\n    RMStateStore rmStore = rmContext.getStateStore();\n    ApplicationAttemptState attemptState =\n        new ApplicationAttemptState(applicationAttemptId, getMasterContainer(),\n          rmStore.getCredentialsFromAppAttempt(this), startTime,\n          stateToBeStored, finalTrackingUrl, diags, finalStatus);\n    LOG.info(\"Updating application attempt \" + applicationAttemptId\n        + \" with final state: \" + targetedFinalState);\n    rmStore.updateApplicationAttemptState(attemptState);\n  }"
            },
            "hadoop-tools.hadoop-sls.src.main.java.org.apache.hadoop.yarn.sls.scheduler.ResourceSchedulerWrapper.updateQueueWithNodeUpdate": {
                "code_before_change": "  private void updateQueueWithNodeUpdate(\n          NodeUpdateSchedulerEventWrapper eventWrapper) {\n    RMNodeWrapper node = (RMNodeWrapper) eventWrapper.getRMNode();\n    List<UpdatedContainerInfo> containerList = node.getContainerUpdates();\n    for (UpdatedContainerInfo info : containerList) {\n      for (ContainerStatus status : info.getCompletedContainers()) {\n        ContainerId containerId = status.getContainerId();\n        SchedulerAppReport app = scheduler.getSchedulerAppInfo(\n                containerId.getApplicationAttemptId());\n\n        if (app == null) {\n          // this happens for the AM container\n          // The app have already removed when the NM sends the release\n          // information.\n          continue;\n        }\n\n        String queue =\n            appQueueMap.get(containerId.getApplicationAttemptId()\n              .getApplicationId());\n        int releasedMemory = 0, releasedVCores = 0;\n        if (status.getExitStatus() == ContainerExitStatus.SUCCESS) {\n          for (RMContainer rmc : app.getLiveContainers()) {\n            if (rmc.getContainerId() == containerId) {\n              releasedMemory += rmc.getContainer().getResource().getMemory();\n              releasedVCores += rmc.getContainer()\n                      .getResource().getVirtualCores();\n              break;\n            }\n          }\n        } else if (status.getExitStatus() == ContainerExitStatus.ABORTED) {\n          if (preemptionContainerMap.containsKey(containerId)) {\n            Resource preResource = preemptionContainerMap.get(containerId);\n            releasedMemory += preResource.getMemory();\n            releasedVCores += preResource.getVirtualCores();\n            preemptionContainerMap.remove(containerId);\n          }\n        }\n        // update queue counters\n        updateQueueMetrics(queue, releasedMemory, releasedVCores);\n      }\n    }\n  }",
                "code_after_change": "  private void updateQueueWithNodeUpdate(\n          NodeUpdateSchedulerEventWrapper eventWrapper) {\n    RMNodeWrapper node = (RMNodeWrapper) eventWrapper.getRMNode();\n    List<UpdatedContainerInfo> containerList = node.getContainerUpdates();\n    for (UpdatedContainerInfo info : containerList) {\n      for (ContainerStatus status : info.getCompletedContainers()) {\n        ContainerId containerId = status.getContainerId();\n        SchedulerAppReport app = scheduler.getSchedulerAppInfo(\n                containerId.getApplicationAttemptId());\n\n        if (app == null) {\n          // this happens for the AM container\n          // The app have already removed when the NM sends the release\n          // information.\n          continue;\n        }\n\n        String queue = appQueueMap.get(containerId.getApplicationAttemptId());\n        int releasedMemory = 0, releasedVCores = 0;\n        if (status.getExitStatus() == ContainerExitStatus.SUCCESS) {\n          for (RMContainer rmc : app.getLiveContainers()) {\n            if (rmc.getContainerId() == containerId) {\n              releasedMemory += rmc.getContainer().getResource().getMemory();\n              releasedVCores += rmc.getContainer()\n                      .getResource().getVirtualCores();\n              break;\n            }\n          }\n        } else if (status.getExitStatus() == ContainerExitStatus.ABORTED) {\n          if (preemptionContainerMap.containsKey(containerId)) {\n            Resource preResource = preemptionContainerMap.get(containerId);\n            releasedMemory += preResource.getMemory();\n            releasedVCores += preResource.getVirtualCores();\n            preemptionContainerMap.remove(containerId);\n          }\n        }\n        // update queue counters\n        updateQueueMetrics(queue, releasedMemory, releasedVCores);\n      }\n    }\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.getQueueManager": {
                "code_before_change": "  public QueueManager getQueueManager() {\n    return queueMgr;\n  }",
                "code_after_change": "  public QueueManager getQueueManager() {\n    return queueMgr;\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.transition": {
                "code_before_change": "    public void transition(RMAppImpl app, RMAppEvent event) {\n    };",
                "code_after_change": "    public void transition(RMAppImpl app, RMAppEvent event) {\n    };"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.api.protocolrecords.impl.pb.RegisterApplicationMasterResponsePBImpl.setQueue": {
                "code_before_change": "  public void setQueue(String queue) {\n    maybeInitBuilder();\n    if (queue == null) {\n      builder.clearQueue();\n    } else {\n      builder.setQueue(queue);\n    }\n  }",
                "code_after_change": "  public void setQueue(String queue) {\n    maybeInitBuilder();\n    if (queue == null) {\n      builder.clearQueue();\n    } else {\n      builder.setQueue(queue);\n    }\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService.allocate": {
                "code_before_change": "  public AllocateResponse allocate(AllocateRequest request)\n      throws YarnException, IOException {\n\n    ApplicationAttemptId appAttemptId = authorizeRequest();\n\n    this.amLivelinessMonitor.receivedPing(appAttemptId);\n\n    /* check if its in cache */\n    AllocateResponseLock lock = responseMap.get(appAttemptId);\n    if (lock == null) {\n      LOG.error(\"AppAttemptId doesnt exist in cache \" + appAttemptId);\n      return resync;\n    }\n    synchronized (lock) {\n      AllocateResponse lastResponse = lock.getAllocateResponse();\n      if (!hasApplicationMasterRegistered(appAttemptId)) {\n        String message =\n            \"Application Master is trying to allocate before registering for: \"\n                + appAttemptId.getApplicationId();\n        LOG.error(message);\n        RMAuditLogger.logFailure(\n            this.rmContext.getRMApps().get(appAttemptId.getApplicationId())\n                .getUser(), AuditConstants.REGISTER_AM, \"\",\n            \"ApplicationMasterService\", message,\n            appAttemptId.getApplicationId(),\n            appAttemptId);\n        throw new InvalidApplicationMasterRequestException(message);\n      }\n\n      if ((request.getResponseId() + 1) == lastResponse.getResponseId()) {\n        /* old heartbeat */\n        return lastResponse;\n      } else if (request.getResponseId() + 1 < lastResponse.getResponseId()) {\n        LOG.error(\"Invalid responseid from appAttemptId \" + appAttemptId);\n        // Oh damn! Sending reboot isn't enough. RM state is corrupted. TODO:\n        // Reboot is not useful since after AM reboots, it will send register\n        // and\n        // get an exception. Might as well throw an exception here.\n        return resync;\n      }\n\n      // Send the status update to the appAttempt.\n      this.rmContext.getDispatcher().getEventHandler().handle(\n          new RMAppAttemptStatusupdateEvent(appAttemptId, request\n              .getProgress()));\n\n      List<ResourceRequest> ask = request.getAskList();\n      List<ContainerId> release = request.getReleaseList();\n\n      ResourceBlacklistRequest blacklistRequest =\n          request.getResourceBlacklistRequest();\n      List<String> blacklistAdditions =\n          (blacklistRequest != null) ?\n              blacklistRequest.getBlacklistAdditions() : null;\n      List<String> blacklistRemovals =\n          (blacklistRequest != null) ?\n              blacklistRequest.getBlacklistRemovals() : null;\n\n      // sanity check\n      try {\n        RMServerUtils.validateResourceRequests(ask,\n            rScheduler.getMaximumResourceCapability());\n      } catch (InvalidResourceRequestException e) {\n        LOG.warn(\"Invalid resource ask by application \" + appAttemptId, e);\n        throw e;\n      }\n      \n      try {\n        RMServerUtils.validateBlacklistRequest(blacklistRequest);\n      }  catch (InvalidResourceBlacklistRequestException e) {\n        LOG.warn(\"Invalid blacklist request by application \" + appAttemptId, e);\n        throw e;\n      }\n\n      RMApp app =\n          this.rmContext.getRMApps().get(appAttemptId.getApplicationId());\n      // In the case of work-preserving AM restart, it's possible for the\n      // AM to release containers from the earlier attempt.\n      if (!app.getApplicationSubmissionContext()\n        .getKeepContainersAcrossApplicationAttempts()) {\n        try {\n          RMServerUtils.validateContainerReleaseRequest(release, appAttemptId);\n        } catch (InvalidContainerReleaseException e) {\n          LOG.warn(\"Invalid container release by application \" + appAttemptId, e);\n          throw e;\n        }\n      }\n\n      // Send new requests to appAttempt.\n      Allocation allocation =\n          this.rScheduler.allocate(appAttemptId, ask, release, \n              blacklistAdditions, blacklistRemovals);\n\n      RMAppAttempt appAttempt = app.getRMAppAttempt(appAttemptId);\n      \n      AllocateResponse allocateResponse =\n          recordFactory.newRecordInstance(AllocateResponse.class);\n\n      // update the response with the deltas of node status changes\n      List<RMNode> updatedNodes = new ArrayList<RMNode>();\n      if(app.pullRMNodeUpdates(updatedNodes) > 0) {\n        List<NodeReport> updatedNodeReports = new ArrayList<NodeReport>();\n        for(RMNode rmNode: updatedNodes) {\n          SchedulerNodeReport schedulerNodeReport =  \n              rScheduler.getNodeReport(rmNode.getNodeID());\n          Resource used = BuilderUtils.newResource(0, 0);\n          int numContainers = 0;\n          if (schedulerNodeReport != null) {\n            used = schedulerNodeReport.getUsedResource();\n            numContainers = schedulerNodeReport.getNumContainers();\n          }\n          NodeReport report = BuilderUtils.newNodeReport(rmNode.getNodeID(),\n              rmNode.getState(),\n              rmNode.getHttpAddress(), rmNode.getRackName(), used,\n              rmNode.getTotalCapability(), numContainers,\n              rmNode.getHealthReport(),\n              rmNode.getLastHealthReportTime());\n\n          updatedNodeReports.add(report);\n        }\n        allocateResponse.setUpdatedNodes(updatedNodeReports);\n      }\n\n      allocateResponse.setAllocatedContainers(allocation.getContainers());\n      allocateResponse.setCompletedContainersStatuses(appAttempt\n          .pullJustFinishedContainers());\n      allocateResponse.setResponseId(lastResponse.getResponseId() + 1);\n      allocateResponse.setAvailableResources(allocation.getResourceLimit());\n\n      allocateResponse.setNumClusterNodes(this.rScheduler.getNumClusterNodes());\n\n      // add preemption to the allocateResponse message (if any)\n      allocateResponse\n          .setPreemptionMessage(generatePreemptionMessage(allocation));\n\n      // Adding NMTokens for allocated containers.\n      if (!allocation.getContainers().isEmpty()) {\n        allocateResponse.setNMTokens(rmContext.getNMTokenSecretManager()\n            .createAndGetNMTokens(app.getUser(), appAttemptId,\n                allocation.getContainers()));\n      }\n      /*\n       * As we are updating the response inside the lock object so we don't\n       * need to worry about unregister call occurring in between (which\n       * removes the lock object).\n       */\n      lock.setAllocateResponse(allocateResponse);\n      return allocateResponse;\n    }    \n  }",
                "code_after_change": "  public AllocateResponse allocate(AllocateRequest request)\n      throws YarnException, IOException {\n\n    ApplicationAttemptId appAttemptId = authorizeRequest();\n\n    this.amLivelinessMonitor.receivedPing(appAttemptId);\n\n    /* check if its in cache */\n    AllocateResponseLock lock = responseMap.get(appAttemptId);\n    if (lock == null) {\n      LOG.error(\"AppAttemptId doesnt exist in cache \" + appAttemptId);\n      return resync;\n    }\n    synchronized (lock) {\n      AllocateResponse lastResponse = lock.getAllocateResponse();\n      if (!hasApplicationMasterRegistered(appAttemptId)) {\n        String message =\n            \"Application Master is trying to allocate before registering for: \"\n                + appAttemptId.getApplicationId();\n        LOG.error(message);\n        RMAuditLogger.logFailure(\n            this.rmContext.getRMApps().get(appAttemptId.getApplicationId())\n                .getUser(), AuditConstants.REGISTER_AM, \"\",\n            \"ApplicationMasterService\", message,\n            appAttemptId.getApplicationId(),\n            appAttemptId);\n        throw new InvalidApplicationMasterRequestException(message);\n      }\n\n      if ((request.getResponseId() + 1) == lastResponse.getResponseId()) {\n        /* old heartbeat */\n        return lastResponse;\n      } else if (request.getResponseId() + 1 < lastResponse.getResponseId()) {\n        LOG.error(\"Invalid responseid from appAttemptId \" + appAttemptId);\n        // Oh damn! Sending reboot isn't enough. RM state is corrupted. TODO:\n        // Reboot is not useful since after AM reboots, it will send register\n        // and\n        // get an exception. Might as well throw an exception here.\n        return resync;\n      }\n\n      // Send the status update to the appAttempt.\n      this.rmContext.getDispatcher().getEventHandler().handle(\n          new RMAppAttemptStatusupdateEvent(appAttemptId, request\n              .getProgress()));\n\n      List<ResourceRequest> ask = request.getAskList();\n      List<ContainerId> release = request.getReleaseList();\n\n      ResourceBlacklistRequest blacklistRequest =\n          request.getResourceBlacklistRequest();\n      List<String> blacklistAdditions =\n          (blacklistRequest != null) ?\n              blacklistRequest.getBlacklistAdditions() : null;\n      List<String> blacklistRemovals =\n          (blacklistRequest != null) ?\n              blacklistRequest.getBlacklistRemovals() : null;\n\n      // sanity check\n      try {\n        RMServerUtils.validateResourceRequests(ask,\n            rScheduler.getMaximumResourceCapability());\n      } catch (InvalidResourceRequestException e) {\n        LOG.warn(\"Invalid resource ask by application \" + appAttemptId, e);\n        throw e;\n      }\n      \n      try {\n        RMServerUtils.validateBlacklistRequest(blacklistRequest);\n      }  catch (InvalidResourceBlacklistRequestException e) {\n        LOG.warn(\"Invalid blacklist request by application \" + appAttemptId, e);\n        throw e;\n      }\n      \n      try {\n        RMServerUtils.validateContainerReleaseRequest(release, appAttemptId);\n      } catch (InvalidContainerReleaseException e) {\n        LOG.warn(\"Invalid container release by application \" + appAttemptId, e);\n        throw e;\n      }\n      \n      // Send new requests to appAttempt.\n      Allocation allocation =\n          this.rScheduler.allocate(appAttemptId, ask, release, \n              blacklistAdditions, blacklistRemovals);\n\n      RMApp app = this.rmContext.getRMApps().get(\n          appAttemptId.getApplicationId());\n      RMAppAttempt appAttempt = app.getRMAppAttempt(appAttemptId);\n      \n      AllocateResponse allocateResponse =\n          recordFactory.newRecordInstance(AllocateResponse.class);\n\n      // update the response with the deltas of node status changes\n      List<RMNode> updatedNodes = new ArrayList<RMNode>();\n      if(app.pullRMNodeUpdates(updatedNodes) > 0) {\n        List<NodeReport> updatedNodeReports = new ArrayList<NodeReport>();\n        for(RMNode rmNode: updatedNodes) {\n          SchedulerNodeReport schedulerNodeReport =  \n              rScheduler.getNodeReport(rmNode.getNodeID());\n          Resource used = BuilderUtils.newResource(0, 0);\n          int numContainers = 0;\n          if (schedulerNodeReport != null) {\n            used = schedulerNodeReport.getUsedResource();\n            numContainers = schedulerNodeReport.getNumContainers();\n          }\n          NodeReport report = BuilderUtils.newNodeReport(rmNode.getNodeID(),\n              rmNode.getState(),\n              rmNode.getHttpAddress(), rmNode.getRackName(), used,\n              rmNode.getTotalCapability(), numContainers,\n              rmNode.getHealthReport(),\n              rmNode.getLastHealthReportTime());\n\n          updatedNodeReports.add(report);\n        }\n        allocateResponse.setUpdatedNodes(updatedNodeReports);\n      }\n\n      allocateResponse.setAllocatedContainers(allocation.getContainers());\n      allocateResponse.setCompletedContainersStatuses(appAttempt\n          .pullJustFinishedContainers());\n      allocateResponse.setResponseId(lastResponse.getResponseId() + 1);\n      allocateResponse.setAvailableResources(allocation.getResourceLimit());\n\n      allocateResponse.setNumClusterNodes(this.rScheduler.getNumClusterNodes());\n\n      // add preemption to the allocateResponse message (if any)\n      allocateResponse\n          .setPreemptionMessage(generatePreemptionMessage(allocation));\n\n      // Adding NMTokens for allocated containers.\n      if (!allocation.getContainers().isEmpty()) {\n        allocateResponse.setNMTokens(rmContext.getNMTokenSecretManager()\n            .createAndGetNMTokens(app.getUser(), appAttemptId,\n                allocation.getContainers()));\n      }\n      /*\n       * As we are updating the response inside the lock object so we don't\n       * need to worry about unregister call occurring in between (which\n       * removes the lock object).\n       */\n      lock.setAllocateResponse(allocateResponse);\n      return allocateResponse;\n    }    \n  }"
            },
            "hadoop-tools.hadoop-sls.src.main.java.org.apache.hadoop.yarn.sls.scheduler.ResourceSchedulerWrapper.updateQueueWithAllocateRequest": {
                "code_before_change": "  private void updateQueueWithAllocateRequest(Allocation allocation,\n                        ApplicationAttemptId attemptId,\n                        List<ResourceRequest> resourceRequests,\n                        List<ContainerId> containerIds) throws IOException {\n    // update queue information\n    Resource pendingResource = Resources.createResource(0, 0);\n    Resource allocatedResource = Resources.createResource(0, 0);\n    String queueName = appQueueMap.get(attemptId.getApplicationId());\n    // container requested\n    for (ResourceRequest request : resourceRequests) {\n      if (request.getResourceName().equals(ResourceRequest.ANY)) {\n        Resources.addTo(pendingResource,\n                Resources.multiply(request.getCapability(),\n                        request.getNumContainers()));\n      }\n    }\n    // container allocated\n    for (Container container : allocation.getContainers()) {\n      Resources.addTo(allocatedResource, container.getResource());\n      Resources.subtractFrom(pendingResource, container.getResource());\n    }\n    // container released from AM\n    SchedulerAppReport report = scheduler.getSchedulerAppInfo(attemptId);\n    for (ContainerId containerId : containerIds) {\n      Container container = null;\n      for (RMContainer c : report.getLiveContainers()) {\n        if (c.getContainerId().equals(containerId)) {\n          container = c.getContainer();\n          break;\n        }\n      }\n      if (container != null) {\n        // released allocated containers\n        Resources.subtractFrom(allocatedResource, container.getResource());\n      } else {\n        for (RMContainer c : report.getReservedContainers()) {\n          if (c.getContainerId().equals(containerId)) {\n            container = c.getContainer();\n            break;\n          }\n        }\n        if (container != null) {\n          // released reserved containers\n          Resources.subtractFrom(pendingResource, container.getResource());\n        }\n      }\n    }\n    // containers released/preemption from scheduler\n    Set<ContainerId> preemptionContainers = new HashSet<ContainerId>();\n    if (allocation.getContainerPreemptions() != null) {\n      preemptionContainers.addAll(allocation.getContainerPreemptions());\n    }\n    if (allocation.getStrictContainerPreemptions() != null) {\n      preemptionContainers.addAll(allocation.getStrictContainerPreemptions());\n    }\n    if (! preemptionContainers.isEmpty()) {\n      for (ContainerId containerId : preemptionContainers) {\n        if (! preemptionContainerMap.containsKey(containerId)) {\n          Container container = null;\n          for (RMContainer c : report.getLiveContainers()) {\n            if (c.getContainerId().equals(containerId)) {\n              container = c.getContainer();\n              break;\n            }\n          }\n          if (container != null) {\n            preemptionContainerMap.put(containerId, container.getResource());\n          }\n        }\n\n      }\n    }\n\n    // update metrics\n    SortedMap<String, Counter> counterMap = metrics.getCounters();\n    String names[] = new String[]{\n            \"counter.queue.\" + queueName + \".pending.memory\",\n            \"counter.queue.\" + queueName + \".pending.cores\",\n            \"counter.queue.\" + queueName + \".allocated.memory\",\n            \"counter.queue.\" + queueName + \".allocated.cores\"};\n    int values[] = new int[]{pendingResource.getMemory(),\n            pendingResource.getVirtualCores(),\n            allocatedResource.getMemory(), allocatedResource.getVirtualCores()};\n    for (int i = names.length - 1; i >= 0; i --) {\n      if (! counterMap.containsKey(names[i])) {\n        metrics.counter(names[i]);\n        counterMap = metrics.getCounters();\n      }\n      counterMap.get(names[i]).inc(values[i]);\n    }\n\n    queueLock.lock();\n    try {\n      if (! schedulerMetrics.isTracked(queueName)) {\n        schedulerMetrics.trackQueue(queueName);\n      }\n    } finally {\n      queueLock.unlock();\n    }\n  }",
                "code_after_change": "  private void updateQueueWithAllocateRequest(Allocation allocation,\n                        ApplicationAttemptId attemptId,\n                        List<ResourceRequest> resourceRequests,\n                        List<ContainerId> containerIds) throws IOException {\n    // update queue information\n    Resource pendingResource = Resources.createResource(0, 0);\n    Resource allocatedResource = Resources.createResource(0, 0);\n    String queueName = appQueueMap.get(attemptId);\n    // container requested\n    for (ResourceRequest request : resourceRequests) {\n      if (request.getResourceName().equals(ResourceRequest.ANY)) {\n        Resources.addTo(pendingResource,\n                Resources.multiply(request.getCapability(),\n                        request.getNumContainers()));\n      }\n    }\n    // container allocated\n    for (Container container : allocation.getContainers()) {\n      Resources.addTo(allocatedResource, container.getResource());\n      Resources.subtractFrom(pendingResource, container.getResource());\n    }\n    // container released from AM\n    SchedulerAppReport report = scheduler.getSchedulerAppInfo(attemptId);\n    for (ContainerId containerId : containerIds) {\n      Container container = null;\n      for (RMContainer c : report.getLiveContainers()) {\n        if (c.getContainerId().equals(containerId)) {\n          container = c.getContainer();\n          break;\n        }\n      }\n      if (container != null) {\n        // released allocated containers\n        Resources.subtractFrom(allocatedResource, container.getResource());\n      } else {\n        for (RMContainer c : report.getReservedContainers()) {\n          if (c.getContainerId().equals(containerId)) {\n            container = c.getContainer();\n            break;\n          }\n        }\n        if (container != null) {\n          // released reserved containers\n          Resources.subtractFrom(pendingResource, container.getResource());\n        }\n      }\n    }\n    // containers released/preemption from scheduler\n    Set<ContainerId> preemptionContainers = new HashSet<ContainerId>();\n    if (allocation.getContainerPreemptions() != null) {\n      preemptionContainers.addAll(allocation.getContainerPreemptions());\n    }\n    if (allocation.getStrictContainerPreemptions() != null) {\n      preemptionContainers.addAll(allocation.getStrictContainerPreemptions());\n    }\n    if (! preemptionContainers.isEmpty()) {\n      for (ContainerId containerId : preemptionContainers) {\n        if (! preemptionContainerMap.containsKey(containerId)) {\n          Container container = null;\n          for (RMContainer c : report.getLiveContainers()) {\n            if (c.getContainerId().equals(containerId)) {\n              container = c.getContainer();\n              break;\n            }\n          }\n          if (container != null) {\n            preemptionContainerMap.put(containerId, container.getResource());\n          }\n        }\n\n      }\n    }\n\n    // update metrics\n    SortedMap<String, Counter> counterMap = metrics.getCounters();\n    String names[] = new String[]{\n            \"counter.queue.\" + queueName + \".pending.memory\",\n            \"counter.queue.\" + queueName + \".pending.cores\",\n            \"counter.queue.\" + queueName + \".allocated.memory\",\n            \"counter.queue.\" + queueName + \".allocated.cores\"};\n    int values[] = new int[]{pendingResource.getMemory(),\n            pendingResource.getVirtualCores(),\n            allocatedResource.getMemory(), allocatedResource.getVirtualCores()};\n    for (int i = names.length - 1; i >= 0; i --) {\n      if (! counterMap.containsKey(names[i])) {\n        metrics.counter(names[i]);\n        counterMap = metrics.getCounters();\n      }\n      counterMap.get(names[i]).inc(values[i]);\n    }\n\n    queueLock.lock();\n    try {\n      if (! schedulerMetrics.isTracked(queueName)) {\n        schedulerMetrics.trackQueue(queueName);\n      }\n    } finally {\n      queueLock.unlock();\n    }\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.handle": {
                "code_before_change": "  public void handle(RMAppEvent event) {\n\n    this.writeLock.lock();\n\n    try {\n      ApplicationId appID = event.getApplicationId();\n      LOG.debug(\"Processing event for \" + appID + \" of type \"\n          + event.getType());\n      final RMAppState oldState = getState();\n      try {\n        /* keep the master in sync with the state machine */\n        this.stateMachine.doTransition(event.getType(), event);\n      } catch (InvalidStateTransitonException e) {\n        LOG.error(\"Can't handle this event at current state\", e);\n        /* TODO fail the application on the failed transition */\n      }\n\n      if (oldState != getState()) {\n        LOG.info(appID + \" State change from \" + oldState + \" to \"\n            + getState());\n      }\n    } finally {\n      this.writeLock.unlock();\n    }\n  }",
                "code_after_change": "  public void handle(RMAppEvent event) {\n\n    this.writeLock.lock();\n\n    try {\n      ApplicationId appID = event.getApplicationId();\n      LOG.debug(\"Processing event for \" + appID + \" of type \"\n          + event.getType());\n      final RMAppState oldState = getState();\n      try {\n        /* keep the master in sync with the state machine */\n        this.stateMachine.doTransition(event.getType(), event);\n      } catch (InvalidStateTransitonException e) {\n        LOG.error(\"Can't handle this event at current state\", e);\n        /* TODO fail the application on the failed transition */\n      }\n\n      if (oldState != getState()) {\n        LOG.info(appID + \" State change from \" + oldState + \" to \"\n            + getState());\n      }\n    } finally {\n      this.writeLock.unlock();\n    }\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.AttemptFailedTransition": {
                "code_before_change": [],
                "code_after_change": []
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.submitApplication": {
                "code_before_change": "  public void submitApplication(ApplicationId applicationId, String userName,\n      String queue)  throws AccessControlException {\n    // Careful! Locking order is important!\n\n    // Check queue ACLs\n    UserGroupInformation userUgi = UserGroupInformation.createRemoteUser(userName);\n    if (!hasAccess(QueueACL.SUBMIT_APPLICATIONS, userUgi)\n        && !hasAccess(QueueACL.ADMINISTER_QUEUE, userUgi)) {\n      throw new AccessControlException(\"User \" + userName + \" cannot submit\" +\n          \" applications to queue \" + getQueuePath());\n    }\n\n    User user = null;\n    synchronized (this) {\n\n      // Check if the queue is accepting jobs\n      if (getState() != QueueState.RUNNING) {\n        String msg = \"Queue \" + getQueuePath() +\n        \" is STOPPED. Cannot accept submission of application: \" + applicationId;\n        LOG.info(msg);\n        throw new AccessControlException(msg);\n      }\n\n      // Check submission limits for queues\n      if (getNumApplications() >= getMaxApplications()) {\n        String msg = \"Queue \" + getQueuePath() + \n        \" already has \" + getNumApplications() + \" applications,\" +\n        \" cannot accept submission of application: \" + applicationId;\n        LOG.info(msg);\n        throw new AccessControlException(msg);\n      }\n\n      // Check submission limits for the user on this queue\n      user = getUser(userName);\n      if (user.getTotalApplications() >= getMaxApplicationsPerUser()) {\n        String msg = \"Queue \" + getQueuePath() + \n        \" already has \" + user.getTotalApplications() + \n        \" applications from user \" + userName + \n        \" cannot accept submission of application: \" + applicationId;\n        LOG.info(msg);\n        throw new AccessControlException(msg);\n      }\n    }\n\n    // Inform the parent queue\n    try {\n      getParent().submitApplication(applicationId, userName, queue);\n    } catch (AccessControlException ace) {\n      LOG.info(\"Failed to submit application to parent-queue: \" + \n          getParent().getQueuePath(), ace);\n      throw ace;\n    }\n\n    metrics.submitApp(userName);\n  }",
                "code_after_change": "  public void submitApplication(FiCaSchedulerApp application, String userName,\n      String queue)  throws AccessControlException {\n    // Careful! Locking order is important!\n\n    // Check queue ACLs\n    UserGroupInformation userUgi = UserGroupInformation.createRemoteUser(userName);\n    if (!hasAccess(QueueACL.SUBMIT_APPLICATIONS, userUgi)\n        && !hasAccess(QueueACL.ADMINISTER_QUEUE, userUgi)) {\n      throw new AccessControlException(\"User \" + userName + \" cannot submit\" +\n          \" applications to queue \" + getQueuePath());\n    }\n\n    User user = null;\n    synchronized (this) {\n\n      // Check if the queue is accepting jobs\n      if (getState() != QueueState.RUNNING) {\n        String msg = \"Queue \" + getQueuePath() +\n        \" is STOPPED. Cannot accept submission of application: \" +\n        application.getApplicationId();\n        LOG.info(msg);\n        throw new AccessControlException(msg);\n      }\n\n      // Check submission limits for queues\n      if (getNumApplications() >= getMaxApplications()) {\n        String msg = \"Queue \" + getQueuePath() + \n        \" already has \" + getNumApplications() + \" applications,\" +\n        \" cannot accept submission of application: \" + \n        application.getApplicationId();\n        LOG.info(msg);\n        throw new AccessControlException(msg);\n      }\n\n      // Check submission limits for the user on this queue\n      user = getUser(userName);\n      if (user.getTotalApplications() >= getMaxApplicationsPerUser()) {\n        String msg = \"Queue \" + getQueuePath() + \n        \" already has \" + user.getTotalApplications() + \n        \" applications from user \" + userName + \n        \" cannot accept submission of application: \" + \n        application.getApplicationId();\n        LOG.info(msg);\n        throw new AccessControlException(msg);\n      }\n\n      // Add the application to our data-structures\n      addApplication(application, user);\n    }\n\n    int attemptId = application.getApplicationAttemptId().getAttemptId();\n    metrics.submitApp(userName, attemptId);\n\n    // Inform the parent queue\n    try {\n      getParent().submitApplication(application, userName, queue);\n    } catch (AccessControlException ace) {\n      LOG.info(\"Failed to submit application to parent-queue: \" + \n          getParent().getQueuePath(), ace);\n      removeApplication(application, user);\n      throw ace;\n    }\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.recover": {
                "code_before_change": "  public void recover(RMState state) throws Exception{\n    ApplicationState appState = state.getApplicationState().get(getApplicationId());\n    this.recoveredFinalState = appState.getState();\n    LOG.info(\"Recovering app: \" + getApplicationId() + \" with \" + \n        + appState.getAttemptCount() + \" attempts and final state = \" + this.recoveredFinalState );\n    this.diagnostics.append(appState.getDiagnostics());\n    this.storedFinishTime = appState.getFinishTime();\n    this.startTime = appState.getStartTime();\n\n    for(int i=0; i<appState.getAttemptCount(); ++i) {\n      // create attempt\n      createNewAttempt();\n      ((RMAppAttemptImpl)this.currentAttempt).recover(state);\n    }\n  }",
                "code_after_change": "  public void recover(RMState state) throws Exception{\n    ApplicationState appState = state.getApplicationState().get(getApplicationId());\n    this.recoveredFinalState = appState.getState();\n    LOG.info(\"Recovering app: \" + getApplicationId() + \" with \" + \n        + appState.getAttemptCount() + \" attempts and final state = \" + this.recoveredFinalState );\n    this.diagnostics.append(appState.getDiagnostics());\n    this.storedFinishTime = appState.getFinishTime();\n    this.startTime = appState.getStartTime();\n\n    for(int i=0; i<appState.getAttemptCount(); ++i) {\n      // create attempt\n      createNewAttempt(false);\n      ((RMAppAttemptImpl)this.currentAttempt).recover(state);\n    }\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.hasAccess": {
                "code_before_change": "  public boolean hasAccess(QueueACL acl, UserGroupInformation user) {\n    synchronized (this) {\n      if (acls.get(acl).isUserAllowed(user)) {\n        return true;\n      }\n    }\n    \n    if (parent != null) {\n      return parent.hasAccess(acl, user);\n    }\n    \n    return false;\n  }",
                "code_after_change": "  public boolean hasAccess(QueueACL acl, UserGroupInformation user) {\n    synchronized (this) {\n      if (acls.get(acl).isUserAllowed(user)) {\n        return true;\n      }\n    }\n    \n    if (parent != null) {\n      return parent.hasAccess(acl, user);\n    }\n    \n    return false;\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler.assignContainers": {
                "code_before_change": "  private void assignContainers(FiCaSchedulerNode node) {\n    LOG.debug(\"assignContainers:\" +\n        \" node=\" + node.getRMNode().getNodeAddress() + \n        \" #applications=\" + applications.size());\n\n    // Try to assign containers to applications in fifo order\n    for (Map.Entry<ApplicationId, SchedulerApplication> e : applications\n        .entrySet()) {\n      FiCaSchedulerApp application =\n          (FiCaSchedulerApp) e.getValue().getCurrentAppAttempt();\n      LOG.debug(\"pre-assignContainers\");\n      application.showRequests();\n      synchronized (application) {\n        // Check if this resource is on the blacklist\n        if (SchedulerAppUtils.isBlacklisted(application, node, LOG)) {\n          continue;\n        }\n        \n        for (Priority priority : application.getPriorities()) {\n          int maxContainers = \n            getMaxAllocatableContainers(application, priority, node, \n                NodeType.OFF_SWITCH); \n          // Ensure the application needs containers of this priority\n          if (maxContainers > 0) {\n            int assignedContainers = \n              assignContainersOnNode(node, application, priority);\n            // Do not assign out of order w.r.t priorities\n            if (assignedContainers == 0) {\n              break;\n            }\n          }\n        }\n      }\n      \n      LOG.debug(\"post-assignContainers\");\n      application.showRequests();\n\n      // Done\n      if (Resources.lessThan(resourceCalculator, clusterResource,\n              node.getAvailableResource(), minimumAllocation)) {\n        break;\n      }\n    }\n\n    // Update the applications' headroom to correctly take into\n    // account the containers assigned in this update.\n    for (SchedulerApplication application : applications.values()) {\n      FiCaSchedulerApp attempt =\n          (FiCaSchedulerApp) application.getCurrentAppAttempt();\n      attempt.setHeadroom(Resources.subtract(clusterResource, usedResource));\n    }\n  }",
                "code_after_change": "  private void assignContainers(FiCaSchedulerNode node) {\n    LOG.debug(\"assignContainers:\" +\n        \" node=\" + node.getRMNode().getNodeAddress() + \n        \" #applications=\" + applications.size());\n\n    // Try to assign containers to applications in fifo order\n    for (Map.Entry<ApplicationAttemptId, FiCaSchedulerApp> e : applications\n        .entrySet()) {\n      FiCaSchedulerApp application = e.getValue();\n      LOG.debug(\"pre-assignContainers\");\n      application.showRequests();\n      synchronized (application) {\n        // Check if this resource is on the blacklist\n        if (SchedulerAppUtils.isBlacklisted(application, node, LOG)) {\n          continue;\n        }\n        \n        for (Priority priority : application.getPriorities()) {\n          int maxContainers = \n            getMaxAllocatableContainers(application, priority, node, \n                NodeType.OFF_SWITCH); \n          // Ensure the application needs containers of this priority\n          if (maxContainers > 0) {\n            int assignedContainers = \n              assignContainersOnNode(node, application, priority);\n            // Do not assign out of order w.r.t priorities\n            if (assignedContainers == 0) {\n              break;\n            }\n          }\n        }\n      }\n      \n      LOG.debug(\"post-assignContainers\");\n      application.showRequests();\n\n      // Done\n      if (Resources.lessThan(resourceCalculator, clusterResource,\n              node.getAvailableResource(), minimumAllocation)) {\n        break;\n      }\n    }\n\n    // Update the applications' headroom to correctly take into\n    // account the containers assigned in this update.\n    for (FiCaSchedulerApp application : applications.values()) {\n      application.setHeadroom(Resources.subtract(clusterResource, usedResource));\n    }\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.api.protocolrecords.impl.pb.RegisterApplicationMasterResponsePBImpl.mergeLocalToBuilder": {
                "code_before_change": "  private void mergeLocalToBuilder() {\n    if (this.maximumResourceCapability != null) {\n      builder.setMaximumCapability(\n          convertToProtoFormat(this.maximumResourceCapability));\n    }\n    if (this.applicationACLS != null) {\n      addApplicationACLs();\n    }\n    if (this.containersFromPreviousAttempt != null) {\n      addRunningContainersToProto();\n    }\n  }",
                "code_after_change": "  private void mergeLocalToBuilder() {\n    if (this.maximumResourceCapability != null) {\n      builder.setMaximumCapability(\n          convertToProtoFormat(this.maximumResourceCapability));\n    }\n    if (this.applicationACLS != null) {\n      addApplicationACLs();\n    }\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.getNodeReport": {
                "code_before_change": "  public SchedulerNodeReport getNodeReport(NodeId nodeId) {\n    FSSchedulerNode node = nodes.get(nodeId);\n    return node == null ? null : new SchedulerNodeReport(node);\n  }",
                "code_after_change": "  public SchedulerNodeReport getNodeReport(NodeId nodeId) {\n    FSSchedulerNode node = nodes.get(nodeId);\n    return node == null ? null : new SchedulerNodeReport(node);\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.getAppAttemptId": {
                "code_before_change": "  public ApplicationAttemptId getAppAttemptId() {\n    return this.applicationAttemptId;\n  }",
                "code_after_change": "  public ApplicationAttemptId getAppAttemptId() {\n    return this.applicationAttemptId;\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler.removeApplicationAttempt": {
                "code_before_change": "  private synchronized void removeApplicationAttempt(\n      ApplicationAttemptId applicationAttemptId,\n      RMAppAttemptState rmAppAttemptFinalState, boolean keepContainers) {\n    LOG.info(\"Application \" + applicationAttemptId + \" is done.\" +\n        \" finalState=\" + rmAppAttemptFinalState);\n    SchedulerApplication application =\n        applications.get(applicationAttemptId.getApplicationId());\n    FSSchedulerApp attempt = getSchedulerApp(applicationAttemptId);\n\n    if (attempt == null || application == null) {\n      LOG.info(\"Unknown application \" + applicationAttemptId + \" has completed!\");\n      return;\n    }\n\n    // Release all the running containers\n    for (RMContainer rmContainer : attempt.getLiveContainers()) {\n      if (keepContainers\n          && rmContainer.getState().equals(RMContainerState.RUNNING)) {\n        // do not kill the running container in the case of work-preserving AM\n        // restart.\n        LOG.info(\"Skip killing \" + rmContainer.getContainerId());\n        continue;\n      }\n      completedContainer(rmContainer,\n          SchedulerUtils.createAbnormalContainerStatus(\n              rmContainer.getContainerId(),\n              SchedulerUtils.COMPLETED_APPLICATION),\n              RMContainerEventType.KILL);\n    }\n\n    // Release all reserved containers\n    for (RMContainer rmContainer : attempt.getReservedContainers()) {\n      completedContainer(rmContainer,\n          SchedulerUtils.createAbnormalContainerStatus(\n              rmContainer.getContainerId(),\n              \"Application Complete\"),\n              RMContainerEventType.KILL);\n    }\n    // Clean up pending requests, metrics etc.\n    attempt.stop(rmAppAttemptFinalState);\n\n    // Inform the queue\n    FSLeafQueue queue = queueMgr.getLeafQueue(attempt.getQueue()\n        .getQueueName(), false);\n    boolean wasRunnable = queue.removeApp(attempt);\n\n    if (wasRunnable) {\n      maxRunningEnforcer.untrackRunnableApp(attempt);\n      maxRunningEnforcer.updateRunnabilityOnAppRemoval(attempt,\n          attempt.getQueue());\n    } else {\n      maxRunningEnforcer.untrackNonRunnableApp(attempt);\n    }\n  }",
                "code_after_change": "  private synchronized void removeApplicationAttempt(\n      ApplicationAttemptId applicationAttemptId,\n      RMAppAttemptState rmAppAttemptFinalState) {\n    LOG.info(\"Application \" + applicationAttemptId + \" is done.\" +\n        \" finalState=\" + rmAppAttemptFinalState);\n\n    FSSchedulerApp application = applications.get(applicationAttemptId);\n\n    if (application == null) {\n      LOG.info(\"Unknown application \" + applicationAttemptId + \" has completed!\");\n      return;\n    }\n\n    // Release all the running containers\n    for (RMContainer rmContainer : application.getLiveContainers()) {\n      completedContainer(rmContainer,\n          SchedulerUtils.createAbnormalContainerStatus(\n              rmContainer.getContainerId(),\n              SchedulerUtils.COMPLETED_APPLICATION),\n              RMContainerEventType.KILL);\n    }\n\n    // Release all reserved containers\n    for (RMContainer rmContainer : application.getReservedContainers()) {\n      completedContainer(rmContainer,\n          SchedulerUtils.createAbnormalContainerStatus(\n              rmContainer.getContainerId(),\n              \"Application Complete\"),\n          RMContainerEventType.KILL);\n    }\n\n    // Clean up pending requests, metrics etc.\n    application.stop(rmAppAttemptFinalState);\n\n    // Inform the queue\n    FSLeafQueue queue = queueMgr.getLeafQueue(application.getQueue()\n        .getQueueName(), false);\n    boolean wasRunnable = queue.removeApp(application);\n\n    if (wasRunnable) {\n      maxRunningEnforcer.updateRunnabilityOnAppRemoval(application);\n    } else {\n      maxRunningEnforcer.untrackNonRunnableApp(application);\n    }\n    \n    // Remove from our data-structure\n    applications.remove(applicationAttemptId);\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.api.protocolrecords.impl.pb.RegisterApplicationMasterResponsePBImpl.getClientToAMTokenMasterKey": {
                "code_before_change": "  public ByteBuffer getClientToAMTokenMasterKey() {\n    maybeInitBuilder();\n    ByteBuffer key =\n        ByteBuffer.wrap(builder.getClientToAmTokenMasterKey().toByteArray());\n    return key;\n  }",
                "code_after_change": "  public ByteBuffer getClientToAMTokenMasterKey() {\n    maybeInitBuilder();\n    ByteBuffer key =\n        ByteBuffer.wrap(builder.getClientToAmTokenMasterKey().toByteArray());\n    return key;\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.completedContainer": {
                "code_before_change": "  private synchronized void completedContainer(RMContainer rmContainer,\n      ContainerStatus containerStatus, RMContainerEventType event) {\n    if (rmContainer == null) {\n      LOG.info(\"Null container completed...\");\n      return;\n    }\n    \n    Container container = rmContainer.getContainer();\n    \n    // Get the application for the finished container\n    FiCaSchedulerApp application =\n        getCurrentAttemptForContainer(container.getId());\n    ApplicationId appId =\n        container.getId().getApplicationAttemptId().getApplicationId();\n    if (application == null) {\n      LOG.info(\"Container \" + container + \" of\" + \" unknown application \"\n          + appId + \" completed with event \" + event);\n      return;\n    }\n    \n    // Get the node on which the container was allocated\n    FiCaSchedulerNode node = getNode(container.getNodeId());\n    \n    // Inform the queue\n    LeafQueue queue = (LeafQueue)application.getQueue();\n    queue.completedContainer(clusterResource, application, node, \n        rmContainer, containerStatus, event, null);\n\n    LOG.info(\"Application attempt \" + application.getApplicationAttemptId()\n        + \" released container \" + container.getId() + \" on node: \" + node\n        + \" with event: \" + event);\n  }",
                "code_after_change": "  private synchronized void completedContainer(RMContainer rmContainer,\n      ContainerStatus containerStatus, RMContainerEventType event) {\n    if (rmContainer == null) {\n      LOG.info(\"Null container completed...\");\n      return;\n    }\n    \n    Container container = rmContainer.getContainer();\n    \n    // Get the application for the finished container\n    ApplicationAttemptId applicationAttemptId =\n      container.getId().getApplicationAttemptId();\n    FiCaSchedulerApp application = getApplication(applicationAttemptId);\n    if (application == null) {\n      LOG.info(\"Container \" + container + \" of\" +\n      \t\t\" unknown application \" + applicationAttemptId + \n          \" completed with event \" + event);\n      return;\n    }\n    \n    // Get the node on which the container was allocated\n    FiCaSchedulerNode node = getNode(container.getNodeId());\n    \n    // Inform the queue\n    LeafQueue queue = (LeafQueue)application.getQueue();\n    queue.completedContainer(clusterResource, application, node, \n        rmContainer, containerStatus, event, null);\n\n    LOG.info(\"Application \" + applicationAttemptId + \n        \" released container \" + container.getId() +\n        \" on node: \" + node + \n        \" with event: \" + event);\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-applications.hadoop-yarn-applications-distributedshell.src.main.java.org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster.main": {
                "code_before_change": "  public static void main(String[] args) {\n    boolean result = false;\n    try {\n      ApplicationMaster appMaster = new ApplicationMaster();\n      LOG.info(\"Initializing ApplicationMaster\");\n      boolean doRun = appMaster.init(args);\n      if (!doRun) {\n        System.exit(0);\n      }\n      result = appMaster.run();\n      appMaster.finish();\n    } catch (Throwable t) {\n      LOG.fatal(\"Error running ApplicationMaster\", t);\n      System.exit(1);\n    }\n    if (result) {\n      LOG.info(\"Application Master completed successfully. exiting\");\n      System.exit(0);\n    } else {\n      LOG.info(\"Application Master failed. exiting\");\n      System.exit(2);\n    }\n  }",
                "code_after_change": "  public static void main(String[] args) {\n    boolean result = false;\n    try {\n      ApplicationMaster appMaster = new ApplicationMaster();\n      LOG.info(\"Initializing ApplicationMaster\");\n      boolean doRun = appMaster.init(args);\n      if (!doRun) {\n        System.exit(0);\n      }\n      result = appMaster.run();\n    } catch (Throwable t) {\n      LOG.fatal(\"Error running ApplicationMaster\", t);\n      System.exit(1);\n    }\n    if (result) {\n      LOG.info(\"Application Master completed successfully. exiting\");\n      System.exit(0);\n    } else {\n      LOG.info(\"Application Master failed. exiting\");\n      System.exit(2);\n    }\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.YarnScheduler.checkAccess": {
                "code_before_change": "  boolean checkAccess(UserGroupInformation callerUGI,\n      QueueACL acl, String queueName);\n  \n  /**\n   * Gets the apps under a given queue\n   * @param queueName the name of the queue.\n   * @return a collection of app attempt ids in the given queue.\n   */\n  @LimitedPrivate(\"yarn\")\n  @Stable\n  public List<ApplicationAttemptId> getAppsInQueue(String queueName);\n\n  /**\n   * Get the container for the given containerId.\n   * @param containerId\n   * @return the container for the given containerId.\n   */\n  @LimitedPrivate(\"yarn\")\n  @Unstable\n  public RMContainer getRMContainer(ContainerId containerId);\n  \n  /**\n   * Moves the given application to the given queue\n   * @param appId\n   * @param newQueue\n   * @return the name of the queue the application was placed into\n   * @throws YarnException if the move cannot be carried out\n   */\n  @LimitedPrivate(\"yarn\")\n  @Evolving\n  public String moveApplication(ApplicationId appId, String newQueue)\n      throws YarnException;\n}",
                "code_after_change": "  boolean checkAccess(UserGroupInformation callerUGI,\n      QueueACL acl, String queueName);\n  \n  /**\n   * Gets the apps under a given queue\n   * @param queueName the name of the queue.\n   * @return a collection of app attempt ids in the given queue.\n   */\n  @LimitedPrivate(\"yarn\")\n  @Stable\n  public List<ApplicationAttemptId> getAppsInQueue(String queueName);\n}"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.preemptContainer": {
                "code_before_change": "  public void preemptContainer(ApplicationAttemptId aid, RMContainer cont) {\n    if(LOG.isDebugEnabled()){\n      LOG.debug(\"PREEMPT_CONTAINER: application:\" + aid.toString() +\n          \" container: \" + cont.toString());\n    }\n    FiCaSchedulerApp app = getApplicationAttempt(aid);\n    if (app != null) {\n      app.addPreemptContainer(cont.getContainerId());\n    }\n  }",
                "code_after_change": "  public void preemptContainer(ApplicationAttemptId aid, RMContainer cont) {\n    if(LOG.isDebugEnabled()){\n      LOG.debug(\"PREEMPT_CONTAINER: application:\" + aid.toString() +\n          \" container: \" + cont.toString());\n    }\n    FiCaSchedulerApp app = applications.get(aid);\n    if (app != null) {\n      app.addPreemptContainer(cont.getContainerId());\n    }\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.getAppKilledDiagnostics": {
                "code_before_change": "  private static String getAppKilledDiagnostics() {\n    return \"Application killed by user.\";\n  }",
                "code_after_change": "  private static String getAppKilledDiagnostics() {\n    return \"Application killed by user.\";\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-applications.hadoop-yarn-applications-distributedshell.src.main.java.org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster.printUsage": {
                "code_before_change": "  private void printUsage(Options opts) {\n    new HelpFormatter().printHelp(\"ApplicationMaster\", opts);\n  }",
                "code_after_change": "  private void printUsage(Options opts) {\n    new HelpFormatter().printHelp(\"ApplicationMaster\", opts);\n  }"
            },
            "hadoop-tools.hadoop-sls.src.main.java.org.apache.hadoop.yarn.sls.scheduler.ResourceSchedulerWrapper.handle": {
                "code_before_change": "  public void handle(SchedulerEvent schedulerEvent) {\n    // metrics off\n    if (! metricsON) {\n      scheduler.handle(schedulerEvent);\n      return;\n    }\n    if(!running)    running = true;\n\n    // metrics on\n    Timer.Context handlerTimer = null;\n    Timer.Context operationTimer = null;\n\n    NodeUpdateSchedulerEventWrapper eventWrapper;\n    try {\n      //if (schedulerEvent instanceof NodeUpdateSchedulerEvent) {\n      if (schedulerEvent.getType() == SchedulerEventType.NODE_UPDATE\n              && schedulerEvent instanceof NodeUpdateSchedulerEvent) {\n        eventWrapper = new NodeUpdateSchedulerEventWrapper(\n                (NodeUpdateSchedulerEvent)schedulerEvent);\n        schedulerEvent = eventWrapper;\n        updateQueueWithNodeUpdate(eventWrapper);\n      } else if (schedulerEvent.getType() == SchedulerEventType.APP_ATTEMPT_REMOVED\n          && schedulerEvent instanceof AppAttemptRemovedSchedulerEvent) {\n        // check if having AM Container, update resource usage information\n        AppAttemptRemovedSchedulerEvent appRemoveEvent =\n            (AppAttemptRemovedSchedulerEvent) schedulerEvent;\n        ApplicationAttemptId appAttemptId =\n                appRemoveEvent.getApplicationAttemptID();\n        String queue = appQueueMap.get(appAttemptId.getApplicationId());\n        SchedulerAppReport app = scheduler.getSchedulerAppInfo(appAttemptId);\n        if (! app.getLiveContainers().isEmpty()) {  // have 0 or 1\n          // should have one container which is AM container\n          RMContainer rmc = app.getLiveContainers().iterator().next();\n          updateQueueMetrics(queue,\n                  rmc.getContainer().getResource().getMemory(),\n                  rmc.getContainer().getResource().getVirtualCores());\n        }\n      }\n\n      handlerTimer = schedulerHandleTimer.time();\n      operationTimer = schedulerHandleTimerMap\n              .get(schedulerEvent.getType()).time();\n\n      scheduler.handle(schedulerEvent);\n    } finally {\n      if (handlerTimer != null)     handlerTimer.stop();\n      if (operationTimer != null)   operationTimer.stop();\n      schedulerHandleCounter.inc();\n      schedulerHandleCounterMap.get(schedulerEvent.getType()).inc();\n\n      if (schedulerEvent.getType() == SchedulerEventType.APP_REMOVED\n          && schedulerEvent instanceof AppRemovedSchedulerEvent) {\n        SLSRunner.decreaseRemainingApps();\n        AppRemovedSchedulerEvent appRemoveEvent =\n                (AppRemovedSchedulerEvent) schedulerEvent;\n        appQueueMap.remove(appRemoveEvent.getApplicationID());\n      } else if (schedulerEvent.getType() == SchedulerEventType.APP_ADDED\n          && schedulerEvent instanceof AppAddedSchedulerEvent) {\n        AppAddedSchedulerEvent appAddEvent =\n                (AppAddedSchedulerEvent) schedulerEvent;\n        String queueName = appAddEvent.getQueue();\n        appQueueMap.put(appAddEvent.getApplicationId(), queueName);\n      }\n    }\n  }\n\n  private void updateQueueWithNodeUpdate(\n          NodeUpdateSchedulerEventWrapper eventWrapper) {\n    RMNodeWrapper node = (RMNodeWrapper) eventWrapper.getRMNode();\n    List<UpdatedContainerInfo> containerList = node.getContainerUpdates();\n    for (UpdatedContainerInfo info : containerList) {\n      for (ContainerStatus status : info.getCompletedContainers()) {\n        ContainerId containerId = status.getContainerId();\n        SchedulerAppReport app = scheduler.getSchedulerAppInfo(\n                containerId.getApplicationAttemptId());\n\n        if (app == null) {\n          // this happens for the AM container\n          // The app have already removed when the NM sends the release\n          // information.\n          continue;\n        }\n\n        String queue =\n            appQueueMap.get(containerId.getApplicationAttemptId()\n              .getApplicationId());\n        int releasedMemory = 0, releasedVCores = 0;\n        if (status.getExitStatus() == ContainerExitStatus.SUCCESS) {\n          for (RMContainer rmc : app.getLiveContainers()) {\n            if (rmc.getContainerId() == containerId) {\n              releasedMemory += rmc.getContainer().getResource().getMemory();\n              releasedVCores += rmc.getContainer()\n                      .getResource().getVirtualCores();\n              break;\n            }\n          }\n        } else if (status.getExitStatus() == ContainerExitStatus.ABORTED) {\n          if (preemptionContainerMap.containsKey(containerId)) {\n            Resource preResource = preemptionContainerMap.get(containerId);\n            releasedMemory += preResource.getMemory();\n            releasedVCores += preResource.getVirtualCores();\n            preemptionContainerMap.remove(containerId);\n          }\n        }\n        // update queue counters\n        updateQueueMetrics(queue, releasedMemory, releasedVCores);\n      }\n    }\n  }\n\n  private void updateQueueWithAllocateRequest(Allocation allocation,\n                        ApplicationAttemptId attemptId,\n                        List<ResourceRequest> resourceRequests,\n                        List<ContainerId> containerIds) throws IOException {\n    // update queue information\n    Resource pendingResource = Resources.createResource(0, 0);\n    Resource allocatedResource = Resources.createResource(0, 0);\n    String queueName = appQueueMap.get(attemptId.getApplicationId());\n    // container requested\n    for (ResourceRequest request : resourceRequests) {\n      if (request.getResourceName().equals(ResourceRequest.ANY)) {\n        Resources.addTo(pendingResource,\n                Resources.multiply(request.getCapability(),\n                        request.getNumContainers()));\n      }\n    }\n    // container allocated\n    for (Container container : allocation.getContainers()) {\n      Resources.addTo(allocatedResource, container.getResource());\n      Resources.subtractFrom(pendingResource, container.getResource());\n    }\n    // container released from AM\n    SchedulerAppReport report = scheduler.getSchedulerAppInfo(attemptId);\n    for (ContainerId containerId : containerIds) {\n      Container container = null;\n      for (RMContainer c : report.getLiveContainers()) {\n        if (c.getContainerId().equals(containerId)) {\n          container = c.getContainer();\n          break;\n        }\n      }\n      if (container != null) {\n        // released allocated containers\n        Resources.subtractFrom(allocatedResource, container.getResource());\n      } else {\n        for (RMContainer c : report.getReservedContainers()) {\n          if (c.getContainerId().equals(containerId)) {\n            container = c.getContainer();\n            break;\n          }\n        }\n        if (container != null) {\n          // released reserved containers\n          Resources.subtractFrom(pendingResource, container.getResource());\n        }\n      }\n    }\n    // containers released/preemption from scheduler\n    Set<ContainerId> preemptionContainers = new HashSet<ContainerId>();\n    if (allocation.getContainerPreemptions() != null) {\n      preemptionContainers.addAll(allocation.getContainerPreemptions());\n    }\n    if (allocation.getStrictContainerPreemptions() != null) {\n      preemptionContainers.addAll(allocation.getStrictContainerPreemptions());\n    }\n    if (! preemptionContainers.isEmpty()) {\n      for (ContainerId containerId : preemptionContainers) {\n        if (! preemptionContainerMap.containsKey(containerId)) {\n          Container container = null;\n          for (RMContainer c : report.getLiveContainers()) {\n            if (c.getContainerId().equals(containerId)) {\n              container = c.getContainer();\n              break;\n            }\n          }\n          if (container != null) {\n            preemptionContainerMap.put(containerId, container.getResource());\n          }\n        }\n\n      }\n    }\n\n    // update metrics\n    SortedMap<String, Counter> counterMap = metrics.getCounters();\n    String names[] = new String[]{\n            \"counter.queue.\" + queueName + \".pending.memory\",\n            \"counter.queue.\" + queueName + \".pending.cores\",\n            \"counter.queue.\" + queueName + \".allocated.memory\",\n            \"counter.queue.\" + queueName + \".allocated.cores\"};\n    int values[] = new int[]{pendingResource.getMemory(),\n            pendingResource.getVirtualCores(),\n            allocatedResource.getMemory(), allocatedResource.getVirtualCores()};\n    for (int i = names.length - 1; i >= 0; i --) {\n      if (! counterMap.containsKey(names[i])) {\n        metrics.counter(names[i]);\n        counterMap = metrics.getCounters();\n      }\n      counterMap.get(names[i]).inc(values[i]);\n    }\n\n    queueLock.lock();\n    try {\n      if (! schedulerMetrics.isTracked(queueName)) {\n        schedulerMetrics.trackQueue(queueName);\n      }\n    } finally {\n      queueLock.unlock();\n    }\n  }\n\n  private void tearDown() throws IOException {\n    // close job runtime writer\n    if (jobRuntimeLogBW != null) {\n      jobRuntimeLogBW.close();\n    }\n    // shut pool\n    if (pool != null)  pool.shutdown();\n  }\n\n  @SuppressWarnings(\"unchecked\")\n  private void initMetrics() throws Exception {\n    metrics = new MetricRegistry();\n    // configuration\n    metricsOutputDir = conf.get(SLSConfiguration.METRICS_OUTPUT_DIR);\n    int metricsWebAddressPort = conf.getInt(\n            SLSConfiguration.METRICS_WEB_ADDRESS_PORT,\n            SLSConfiguration.METRICS_WEB_ADDRESS_PORT_DEFAULT);\n    // create SchedulerMetrics for current scheduler\n    String schedulerMetricsType = conf.get(scheduler.getClass().getName());\n    Class schedulerMetricsClass = schedulerMetricsType == null?\n            defaultSchedulerMetricsMap.get(scheduler.getClass()) :\n            Class.forName(schedulerMetricsType);\n    schedulerMetrics = (SchedulerMetrics)ReflectionUtils\n            .newInstance(schedulerMetricsClass, new Configuration());\n    schedulerMetrics.init(scheduler, metrics);\n\n    // register various metrics\n    registerJvmMetrics();\n    registerClusterResourceMetrics();\n    registerContainerAppNumMetrics();\n    registerSchedulerMetrics();\n\n    // .csv output\n    initMetricsCSVOutput();\n\n    // start web app to provide real-time tracking\n    web = new SLSWebApp(this, metricsWebAddressPort);\n    web.start();\n\n    // a thread to update histogram timer\n    pool = new ScheduledThreadPoolExecutor(2);\n    pool.scheduleAtFixedRate(new HistogramsRunnable(), 0, 1000,\n            TimeUnit.MILLISECONDS);\n\n    // a thread to output metrics for real-tiem tracking\n    pool.scheduleAtFixedRate(new MetricsLogRunnable(), 0, 1000,\n            TimeUnit.MILLISECONDS);\n\n    // application running information\n    jobRuntimeLogBW = new BufferedWriter(\n            new FileWriter(metricsOutputDir + \"/jobruntime.csv\"));\n    jobRuntimeLogBW.write(\"JobID,real_start_time,real_end_time,\" +\n            \"simulate_start_time,simulate_end_time\" + EOL);\n    jobRuntimeLogBW.flush();\n  }\n\n  private void registerJvmMetrics() {\n    // add JVM gauges\n    metrics.register(\"variable.jvm.free.memory\",\n      new Gauge<Long>() {\n        @Override\n        public Long getValue() {\n          return Runtime.getRuntime().freeMemory();\n        }\n      }\n    );\n    metrics.register(\"variable.jvm.max.memory\",\n      new Gauge<Long>() {\n        @Override\n        public Long getValue() {\n          return Runtime.getRuntime().maxMemory();\n        }\n      }\n    );\n    metrics.register(\"variable.jvm.total.memory\",\n      new Gauge<Long>() {\n        @Override\n        public Long getValue() {\n          return Runtime.getRuntime().totalMemory();\n        }\n      }\n    );\n  }\n\n  private void registerClusterResourceMetrics() {\n    metrics.register(\"variable.cluster.allocated.memory\",\n      new Gauge<Integer>() {\n        @Override\n        public Integer getValue() {\n          if(scheduler == null || scheduler.getRootQueueMetrics() == null) {\n            return 0;\n          } else {\n            return scheduler.getRootQueueMetrics().getAllocatedMB();\n          }\n        }\n      }\n    );\n    metrics.register(\"variable.cluster.allocated.vcores\",\n      new Gauge<Integer>() {\n        @Override\n        public Integer getValue() {\n          if(scheduler == null || scheduler.getRootQueueMetrics() == null) {\n            return 0;\n          } else {\n            return scheduler.getRootQueueMetrics().getAllocatedVirtualCores();\n          }\n        }\n      }\n    );\n    metrics.register(\"variable.cluster.available.memory\",\n      new Gauge<Integer>() {\n        @Override\n        public Integer getValue() {\n          if(scheduler == null || scheduler.getRootQueueMetrics() == null) {\n            return 0;\n          } else {\n            return scheduler.getRootQueueMetrics().getAvailableMB();\n          }\n        }\n      }\n    );\n    metrics.register(\"variable.cluster.available.vcores\",\n      new Gauge<Integer>() {\n        @Override\n        public Integer getValue() {\n          if(scheduler == null || scheduler.getRootQueueMetrics() == null) {\n            return 0;\n          } else {\n            return scheduler.getRootQueueMetrics().getAvailableVirtualCores();\n          }\n        }\n      }\n    );\n  }\n\n  private void registerContainerAppNumMetrics() {\n    metrics.register(\"variable.running.application\",\n      new Gauge<Integer>() {\n        @Override\n        public Integer getValue() {\n          if(scheduler == null || scheduler.getRootQueueMetrics() == null) {\n            return 0;\n          } else {\n            return scheduler.getRootQueueMetrics().getAppsRunning();\n          }\n        }\n      }\n    );\n    metrics.register(\"variable.running.container\",\n      new Gauge<Integer>() {\n        @Override\n        public Integer getValue() {\n          if(scheduler == null || scheduler.getRootQueueMetrics() == null) {\n            return 0;\n          } else {\n            return scheduler.getRootQueueMetrics().getAllocatedContainers();\n          }\n        }\n      }\n    );\n  }\n\n  private void registerSchedulerMetrics() {\n    samplerLock.lock();\n    try {\n      // counters for scheduler operations\n      schedulerAllocateCounter = metrics.counter(\n              \"counter.scheduler.operation.allocate\");\n      schedulerHandleCounter = metrics.counter(\n              \"counter.scheduler.operation.handle\");\n      schedulerHandleCounterMap = new HashMap<SchedulerEventType, Counter>();\n      for (SchedulerEventType e : SchedulerEventType.values()) {\n        Counter counter = metrics.counter(\n                \"counter.scheduler.operation.handle.\" + e);\n        schedulerHandleCounterMap.put(e, counter);\n      }\n      // timers for scheduler operations\n      int timeWindowSize = conf.getInt(\n              SLSConfiguration.METRICS_TIMER_WINDOW_SIZE,\n              SLSConfiguration.METRICS_TIMER_WINDOW_SIZE_DEFAULT);\n      schedulerAllocateTimer = new Timer(\n              new SlidingWindowReservoir(timeWindowSize));\n      schedulerHandleTimer = new Timer(\n              new SlidingWindowReservoir(timeWindowSize));\n      schedulerHandleTimerMap = new HashMap<SchedulerEventType, Timer>();\n      for (SchedulerEventType e : SchedulerEventType.values()) {\n        Timer timer = new Timer(new SlidingWindowReservoir(timeWindowSize));\n        schedulerHandleTimerMap.put(e, timer);\n      }\n      // histogram for scheduler operations (Samplers)\n      schedulerHistogramList = new ArrayList<Histogram>();\n      histogramTimerMap = new HashMap<Histogram, Timer>();\n      Histogram schedulerAllocateHistogram = new Histogram(\n              new SlidingWindowReservoir(SAMPLING_SIZE));\n      metrics.register(\"sampler.scheduler.operation.allocate.timecost\",\n              schedulerAllocateHistogram);\n      schedulerHistogramList.add(schedulerAllocateHistogram);\n      histogramTimerMap.put(schedulerAllocateHistogram, schedulerAllocateTimer);\n      Histogram schedulerHandleHistogram = new Histogram(\n              new SlidingWindowReservoir(SAMPLING_SIZE));\n      metrics.register(\"sampler.scheduler.operation.handle.timecost\",\n              schedulerHandleHistogram);\n      schedulerHistogramList.add(schedulerHandleHistogram);\n      histogramTimerMap.put(schedulerHandleHistogram, schedulerHandleTimer);\n      for (SchedulerEventType e : SchedulerEventType.values()) {\n        Histogram histogram = new Histogram(\n                new SlidingWindowReservoir(SAMPLING_SIZE));\n        metrics.register(\n                \"sampler.scheduler.operation.handle.\" + e + \".timecost\",\n                histogram);\n        schedulerHistogramList.add(histogram);\n        histogramTimerMap.put(histogram, schedulerHandleTimerMap.get(e));\n      }\n    } finally {\n      samplerLock.unlock();\n    }\n  }\n\n  private void initMetricsCSVOutput() {\n    int timeIntervalMS = conf.getInt(\n            SLSConfiguration.METRICS_RECORD_INTERVAL_MS,\n            SLSConfiguration.METRICS_RECORD_INTERVAL_MS_DEFAULT);\n    File dir = new File(metricsOutputDir + \"/metrics\");\n    if(! dir.exists()\n            && ! dir.mkdirs()) {\n      LOG.error(\"Cannot create directory \" + dir.getAbsoluteFile());\n    }\n    final CsvReporter reporter = CsvReporter.forRegistry(metrics)\n            .formatFor(Locale.US)\n            .convertRatesTo(TimeUnit.SECONDS)\n            .convertDurationsTo(TimeUnit.MILLISECONDS)\n            .build(new File(metricsOutputDir + \"/metrics\"));\n    reporter.start(timeIntervalMS, TimeUnit.MILLISECONDS);\n  }\n\n  class HistogramsRunnable implements Runnable {\n    @Override\n    public void run() {\n      samplerLock.lock();\n      try {\n        for (Histogram histogram : schedulerHistogramList) {\n          Timer timer = histogramTimerMap.get(histogram);\n          histogram.update((int) timer.getSnapshot().getMean());\n        }\n      } finally {\n        samplerLock.unlock();\n      }\n    }\n  }\n\n  class MetricsLogRunnable implements Runnable {\n    private boolean firstLine = true;\n    public MetricsLogRunnable() {\n      try {\n        metricsLogBW = new BufferedWriter(\n                new FileWriter(metricsOutputDir + \"/realtimetrack.json\"));\n        metricsLogBW.write(\"[\");\n      } catch (IOException e) {\n        e.printStackTrace();\n      }\n    }\n\n    @Override\n    public void run() {\n      if(running) {\n        // all WebApp to get real tracking json\n        String metrics = web.generateRealTimeTrackingMetrics();\n        // output\n        try {\n          if(firstLine) {\n            metricsLogBW.write(metrics + EOL);\n            firstLine = false;\n          } else {\n            metricsLogBW.write(\",\" + metrics + EOL);\n          }\n          metricsLogBW.flush();\n        } catch (IOException e) {\n          e.printStackTrace();\n        }\n      }\n    }\n  }\n\n  // the following functions are used by AMSimulator\n  public void addAMRuntime(ApplicationId appId,\n                           long traceStartTimeMS, long traceEndTimeMS,\n                           long simulateStartTimeMS, long simulateEndTimeMS) {\n\n    try {\n      // write job runtime information\n      StringBuilder sb = new StringBuilder();\n      sb.append(appId).append(\",\").append(traceStartTimeMS).append(\",\")\n              .append(traceEndTimeMS).append(\",\").append(simulateStartTimeMS)\n              .append(\",\").append(simulateEndTimeMS);\n      jobRuntimeLogBW.write(sb.toString() + EOL);\n      jobRuntimeLogBW.flush();\n    } catch (IOException e) {\n      e.printStackTrace();\n    }\n  }\n\n  private void updateQueueMetrics(String queue,\n                                  int releasedMemory, int releasedVCores) {\n    // update queue counters\n    SortedMap<String, Counter> counterMap = metrics.getCounters();\n    if (releasedMemory != 0) {\n      String name = \"counter.queue.\" + queue + \".allocated.memory\";\n      if (! counterMap.containsKey(name)) {\n        metrics.counter(name);\n        counterMap = metrics.getCounters();\n      }\n      counterMap.get(name).inc(-releasedMemory);\n    }\n    if (releasedVCores != 0) {\n      String name = \"counter.queue.\" + queue + \".allocated.cores\";\n      if (! counterMap.containsKey(name)) {\n        metrics.counter(name);\n        counterMap = metrics.getCounters();\n      }\n      counterMap.get(name).inc(-releasedVCores);\n    }\n  }\n\n  public void setQueueSet(Set<String> queues) {\n    this.queueSet = queues;\n  }\n\n  public Set<String> getQueueSet() {\n    return this.queueSet;\n  }\n\n  public void setTrackedAppSet(Set<String> apps) {\n    this.trackedAppSet = apps;\n  }\n\n  public Set<String> getTrackedAppSet() {\n    return this.trackedAppSet;\n  }\n\n  public MetricRegistry getMetrics() {\n    return metrics;\n  }\n\n  public SchedulerMetrics getSchedulerMetrics() {\n    return schedulerMetrics;\n  }\n\n  // API open to out classes\n  public void addTrackedApp(ApplicationAttemptId appAttemptId,\n                            String oldAppId) {\n    if (metricsON) {\n      schedulerMetrics.trackApp(appAttemptId, oldAppId);\n    }\n  }\n\n  public void removeTrackedApp(ApplicationAttemptId appAttemptId,\n                               String oldAppId) {\n    if (metricsON) {\n      schedulerMetrics.untrackApp(appAttemptId, oldAppId);\n    }\n  }\n\n  @Override\n  public Configuration getConf() {\n    return conf;\n  }\n\n  @Override\n  public void reinitialize(Configuration entries, RMContext rmContext)\n          throws IOException {\n    scheduler.reinitialize(entries, rmContext);\n  }\n\n  @Override\n  public void recover(RMStateStore.RMState rmState) throws Exception {\n    scheduler.recover(rmState);\n  }\n\n  @Override\n  public QueueInfo getQueueInfo(String s, boolean b, boolean b2)\n          throws IOException {\n    return scheduler.getQueueInfo(s, b, b2);\n  }\n\n  @Override\n  public List<QueueUserACLInfo> getQueueUserAclInfo() {\n    return scheduler.getQueueUserAclInfo();\n  }\n\n  @Override\n  public Resource getMinimumResourceCapability() {\n    return scheduler.getMinimumResourceCapability();\n  }\n\n  @Override\n  public Resource getMaximumResourceCapability() {\n    return scheduler.getMaximumResourceCapability();\n  }\n\n  @Override\n  public int getNumClusterNodes() {\n    return scheduler.getNumClusterNodes();\n  }\n\n  @Override\n  public SchedulerNodeReport getNodeReport(NodeId nodeId) {\n    return scheduler.getNodeReport(nodeId);\n  }\n\n  @Override\n  public SchedulerAppReport getSchedulerAppInfo(\n          ApplicationAttemptId attemptId) {\n    return scheduler.getSchedulerAppInfo(attemptId);\n  }\n\n  @Override\n  public QueueMetrics getRootQueueMetrics() {\n    return scheduler.getRootQueueMetrics();\n  }\n\n  @Override\n  public synchronized boolean checkAccess(UserGroupInformation callerUGI,\n      QueueACL acl, String queueName) {\n    return scheduler.checkAccess(callerUGI, acl, queueName);\n  }\n\n  @Override\n  public ApplicationResourceUsageReport getAppResourceUsageReport(\n      ApplicationAttemptId appAttemptId) {\n    return scheduler.getAppResourceUsageReport(appAttemptId);\n  }\n\n  @Override\n  public List<ApplicationAttemptId> getAppsInQueue(String queue) {\n    return scheduler.getAppsInQueue(queue);\n  }\n\n  @Override\n  public RMContainer getRMContainer(ContainerId containerId) {\n    return null;\n  }\n\n  @Override\n  public String moveApplication(ApplicationId appId, String newQueue)\n      throws YarnException {\n    return scheduler.moveApplication(appId, newQueue);\n  }\n}",
                "code_after_change": "  public void handle(SchedulerEvent schedulerEvent) {\n    // metrics off\n    if (! metricsON) {\n      scheduler.handle(schedulerEvent);\n      return;\n    }\n    if(!running)    running = true;\n\n    // metrics on\n    Timer.Context handlerTimer = null;\n    Timer.Context operationTimer = null;\n\n    NodeUpdateSchedulerEventWrapper eventWrapper;\n    try {\n      //if (schedulerEvent instanceof NodeUpdateSchedulerEvent) {\n      if (schedulerEvent.getType() == SchedulerEventType.NODE_UPDATE\n              && schedulerEvent instanceof NodeUpdateSchedulerEvent) {\n        eventWrapper = new NodeUpdateSchedulerEventWrapper(\n                (NodeUpdateSchedulerEvent)schedulerEvent);\n        schedulerEvent = eventWrapper;\n        updateQueueWithNodeUpdate(eventWrapper);\n      } else if (schedulerEvent.getType() == SchedulerEventType.APP_ATTEMPT_REMOVED\n          && schedulerEvent instanceof AppAttemptRemovedSchedulerEvent) {\n        // check if having AM Container, update resource usage information\n        AppAttemptRemovedSchedulerEvent appRemoveEvent =\n            (AppAttemptRemovedSchedulerEvent) schedulerEvent;\n        ApplicationAttemptId appAttemptId =\n                appRemoveEvent.getApplicationAttemptID();\n        String queue = appQueueMap.get(appAttemptId);\n        SchedulerAppReport app = scheduler.getSchedulerAppInfo(appAttemptId);\n        if (! app.getLiveContainers().isEmpty()) {  // have 0 or 1\n          // should have one container which is AM container\n          RMContainer rmc = app.getLiveContainers().iterator().next();\n          updateQueueMetrics(queue,\n                  rmc.getContainer().getResource().getMemory(),\n                  rmc.getContainer().getResource().getVirtualCores());\n        }\n      }\n\n      handlerTimer = schedulerHandleTimer.time();\n      operationTimer = schedulerHandleTimerMap\n              .get(schedulerEvent.getType()).time();\n\n      scheduler.handle(schedulerEvent);\n    } finally {\n      if (handlerTimer != null)     handlerTimer.stop();\n      if (operationTimer != null)   operationTimer.stop();\n      schedulerHandleCounter.inc();\n      schedulerHandleCounterMap.get(schedulerEvent.getType()).inc();\n\n      if (schedulerEvent.getType() == SchedulerEventType.APP_ATTEMPT_REMOVED\n          && schedulerEvent instanceof AppAttemptRemovedSchedulerEvent) {\n        SLSRunner.decreaseRemainingApps();\n        AppAttemptRemovedSchedulerEvent appRemoveEvent =\n                (AppAttemptRemovedSchedulerEvent) schedulerEvent;\n        ApplicationAttemptId appAttemptId =\n                appRemoveEvent.getApplicationAttemptID();\n        appQueueMap.remove(appRemoveEvent.getApplicationAttemptID());\n      } else if (schedulerEvent.getType() == SchedulerEventType.APP_ATTEMPT_ADDED\n          && schedulerEvent instanceof AppAttemptAddedSchedulerEvent) {\n        AppAttemptAddedSchedulerEvent appAddEvent =\n                (AppAttemptAddedSchedulerEvent) schedulerEvent;\n        String queueName = appAddEvent.getQueue();\n        appQueueMap.put(appAddEvent.getApplicationAttemptId(), queueName);\n      }\n    }\n  }\n\n  private void updateQueueWithNodeUpdate(\n          NodeUpdateSchedulerEventWrapper eventWrapper) {\n    RMNodeWrapper node = (RMNodeWrapper) eventWrapper.getRMNode();\n    List<UpdatedContainerInfo> containerList = node.getContainerUpdates();\n    for (UpdatedContainerInfo info : containerList) {\n      for (ContainerStatus status : info.getCompletedContainers()) {\n        ContainerId containerId = status.getContainerId();\n        SchedulerAppReport app = scheduler.getSchedulerAppInfo(\n                containerId.getApplicationAttemptId());\n\n        if (app == null) {\n          // this happens for the AM container\n          // The app have already removed when the NM sends the release\n          // information.\n          continue;\n        }\n\n        String queue = appQueueMap.get(containerId.getApplicationAttemptId());\n        int releasedMemory = 0, releasedVCores = 0;\n        if (status.getExitStatus() == ContainerExitStatus.SUCCESS) {\n          for (RMContainer rmc : app.getLiveContainers()) {\n            if (rmc.getContainerId() == containerId) {\n              releasedMemory += rmc.getContainer().getResource().getMemory();\n              releasedVCores += rmc.getContainer()\n                      .getResource().getVirtualCores();\n              break;\n            }\n          }\n        } else if (status.getExitStatus() == ContainerExitStatus.ABORTED) {\n          if (preemptionContainerMap.containsKey(containerId)) {\n            Resource preResource = preemptionContainerMap.get(containerId);\n            releasedMemory += preResource.getMemory();\n            releasedVCores += preResource.getVirtualCores();\n            preemptionContainerMap.remove(containerId);\n          }\n        }\n        // update queue counters\n        updateQueueMetrics(queue, releasedMemory, releasedVCores);\n      }\n    }\n  }\n\n  private void updateQueueWithAllocateRequest(Allocation allocation,\n                        ApplicationAttemptId attemptId,\n                        List<ResourceRequest> resourceRequests,\n                        List<ContainerId> containerIds) throws IOException {\n    // update queue information\n    Resource pendingResource = Resources.createResource(0, 0);\n    Resource allocatedResource = Resources.createResource(0, 0);\n    String queueName = appQueueMap.get(attemptId);\n    // container requested\n    for (ResourceRequest request : resourceRequests) {\n      if (request.getResourceName().equals(ResourceRequest.ANY)) {\n        Resources.addTo(pendingResource,\n                Resources.multiply(request.getCapability(),\n                        request.getNumContainers()));\n      }\n    }\n    // container allocated\n    for (Container container : allocation.getContainers()) {\n      Resources.addTo(allocatedResource, container.getResource());\n      Resources.subtractFrom(pendingResource, container.getResource());\n    }\n    // container released from AM\n    SchedulerAppReport report = scheduler.getSchedulerAppInfo(attemptId);\n    for (ContainerId containerId : containerIds) {\n      Container container = null;\n      for (RMContainer c : report.getLiveContainers()) {\n        if (c.getContainerId().equals(containerId)) {\n          container = c.getContainer();\n          break;\n        }\n      }\n      if (container != null) {\n        // released allocated containers\n        Resources.subtractFrom(allocatedResource, container.getResource());\n      } else {\n        for (RMContainer c : report.getReservedContainers()) {\n          if (c.getContainerId().equals(containerId)) {\n            container = c.getContainer();\n            break;\n          }\n        }\n        if (container != null) {\n          // released reserved containers\n          Resources.subtractFrom(pendingResource, container.getResource());\n        }\n      }\n    }\n    // containers released/preemption from scheduler\n    Set<ContainerId> preemptionContainers = new HashSet<ContainerId>();\n    if (allocation.getContainerPreemptions() != null) {\n      preemptionContainers.addAll(allocation.getContainerPreemptions());\n    }\n    if (allocation.getStrictContainerPreemptions() != null) {\n      preemptionContainers.addAll(allocation.getStrictContainerPreemptions());\n    }\n    if (! preemptionContainers.isEmpty()) {\n      for (ContainerId containerId : preemptionContainers) {\n        if (! preemptionContainerMap.containsKey(containerId)) {\n          Container container = null;\n          for (RMContainer c : report.getLiveContainers()) {\n            if (c.getContainerId().equals(containerId)) {\n              container = c.getContainer();\n              break;\n            }\n          }\n          if (container != null) {\n            preemptionContainerMap.put(containerId, container.getResource());\n          }\n        }\n\n      }\n    }\n\n    // update metrics\n    SortedMap<String, Counter> counterMap = metrics.getCounters();\n    String names[] = new String[]{\n            \"counter.queue.\" + queueName + \".pending.memory\",\n            \"counter.queue.\" + queueName + \".pending.cores\",\n            \"counter.queue.\" + queueName + \".allocated.memory\",\n            \"counter.queue.\" + queueName + \".allocated.cores\"};\n    int values[] = new int[]{pendingResource.getMemory(),\n            pendingResource.getVirtualCores(),\n            allocatedResource.getMemory(), allocatedResource.getVirtualCores()};\n    for (int i = names.length - 1; i >= 0; i --) {\n      if (! counterMap.containsKey(names[i])) {\n        metrics.counter(names[i]);\n        counterMap = metrics.getCounters();\n      }\n      counterMap.get(names[i]).inc(values[i]);\n    }\n\n    queueLock.lock();\n    try {\n      if (! schedulerMetrics.isTracked(queueName)) {\n        schedulerMetrics.trackQueue(queueName);\n      }\n    } finally {\n      queueLock.unlock();\n    }\n  }\n\n  private void tearDown() throws IOException {\n    // close job runtime writer\n    if (jobRuntimeLogBW != null) {\n      jobRuntimeLogBW.close();\n    }\n    // shut pool\n    if (pool != null)  pool.shutdown();\n  }\n\n  @SuppressWarnings(\"unchecked\")\n  private void initMetrics() throws Exception {\n    metrics = new MetricRegistry();\n    // configuration\n    metricsOutputDir = conf.get(SLSConfiguration.METRICS_OUTPUT_DIR);\n    int metricsWebAddressPort = conf.getInt(\n            SLSConfiguration.METRICS_WEB_ADDRESS_PORT,\n            SLSConfiguration.METRICS_WEB_ADDRESS_PORT_DEFAULT);\n    // create SchedulerMetrics for current scheduler\n    String schedulerMetricsType = conf.get(scheduler.getClass().getName());\n    Class schedulerMetricsClass = schedulerMetricsType == null?\n            defaultSchedulerMetricsMap.get(scheduler.getClass()) :\n            Class.forName(schedulerMetricsType);\n    schedulerMetrics = (SchedulerMetrics)ReflectionUtils\n            .newInstance(schedulerMetricsClass, new Configuration());\n    schedulerMetrics.init(scheduler, metrics);\n\n    // register various metrics\n    registerJvmMetrics();\n    registerClusterResourceMetrics();\n    registerContainerAppNumMetrics();\n    registerSchedulerMetrics();\n\n    // .csv output\n    initMetricsCSVOutput();\n\n    // start web app to provide real-time tracking\n    web = new SLSWebApp(this, metricsWebAddressPort);\n    web.start();\n\n    // a thread to update histogram timer\n    pool = new ScheduledThreadPoolExecutor(2);\n    pool.scheduleAtFixedRate(new HistogramsRunnable(), 0, 1000,\n            TimeUnit.MILLISECONDS);\n\n    // a thread to output metrics for real-tiem tracking\n    pool.scheduleAtFixedRate(new MetricsLogRunnable(), 0, 1000,\n            TimeUnit.MILLISECONDS);\n\n    // application running information\n    jobRuntimeLogBW = new BufferedWriter(\n            new FileWriter(metricsOutputDir + \"/jobruntime.csv\"));\n    jobRuntimeLogBW.write(\"JobID,real_start_time,real_end_time,\" +\n            \"simulate_start_time,simulate_end_time\" + EOL);\n    jobRuntimeLogBW.flush();\n  }\n\n  private void registerJvmMetrics() {\n    // add JVM gauges\n    metrics.register(\"variable.jvm.free.memory\",\n      new Gauge<Long>() {\n        @Override\n        public Long getValue() {\n          return Runtime.getRuntime().freeMemory();\n        }\n      }\n    );\n    metrics.register(\"variable.jvm.max.memory\",\n      new Gauge<Long>() {\n        @Override\n        public Long getValue() {\n          return Runtime.getRuntime().maxMemory();\n        }\n      }\n    );\n    metrics.register(\"variable.jvm.total.memory\",\n      new Gauge<Long>() {\n        @Override\n        public Long getValue() {\n          return Runtime.getRuntime().totalMemory();\n        }\n      }\n    );\n  }\n\n  private void registerClusterResourceMetrics() {\n    metrics.register(\"variable.cluster.allocated.memory\",\n      new Gauge<Integer>() {\n        @Override\n        public Integer getValue() {\n          if(scheduler == null || scheduler.getRootQueueMetrics() == null) {\n            return 0;\n          } else {\n            return scheduler.getRootQueueMetrics().getAllocatedMB();\n          }\n        }\n      }\n    );\n    metrics.register(\"variable.cluster.allocated.vcores\",\n      new Gauge<Integer>() {\n        @Override\n        public Integer getValue() {\n          if(scheduler == null || scheduler.getRootQueueMetrics() == null) {\n            return 0;\n          } else {\n            return scheduler.getRootQueueMetrics().getAllocatedVirtualCores();\n          }\n        }\n      }\n    );\n    metrics.register(\"variable.cluster.available.memory\",\n      new Gauge<Integer>() {\n        @Override\n        public Integer getValue() {\n          if(scheduler == null || scheduler.getRootQueueMetrics() == null) {\n            return 0;\n          } else {\n            return scheduler.getRootQueueMetrics().getAvailableMB();\n          }\n        }\n      }\n    );\n    metrics.register(\"variable.cluster.available.vcores\",\n      new Gauge<Integer>() {\n        @Override\n        public Integer getValue() {\n          if(scheduler == null || scheduler.getRootQueueMetrics() == null) {\n            return 0;\n          } else {\n            return scheduler.getRootQueueMetrics().getAvailableVirtualCores();\n          }\n        }\n      }\n    );\n  }\n\n  private void registerContainerAppNumMetrics() {\n    metrics.register(\"variable.running.application\",\n      new Gauge<Integer>() {\n        @Override\n        public Integer getValue() {\n          if(scheduler == null || scheduler.getRootQueueMetrics() == null) {\n            return 0;\n          } else {\n            return scheduler.getRootQueueMetrics().getAppsRunning();\n          }\n        }\n      }\n    );\n    metrics.register(\"variable.running.container\",\n      new Gauge<Integer>() {\n        @Override\n        public Integer getValue() {\n          if(scheduler == null || scheduler.getRootQueueMetrics() == null) {\n            return 0;\n          } else {\n            return scheduler.getRootQueueMetrics().getAllocatedContainers();\n          }\n        }\n      }\n    );\n  }\n\n  private void registerSchedulerMetrics() {\n    samplerLock.lock();\n    try {\n      // counters for scheduler operations\n      schedulerAllocateCounter = metrics.counter(\n              \"counter.scheduler.operation.allocate\");\n      schedulerHandleCounter = metrics.counter(\n              \"counter.scheduler.operation.handle\");\n      schedulerHandleCounterMap = new HashMap<SchedulerEventType, Counter>();\n      for (SchedulerEventType e : SchedulerEventType.values()) {\n        Counter counter = metrics.counter(\n                \"counter.scheduler.operation.handle.\" + e);\n        schedulerHandleCounterMap.put(e, counter);\n      }\n      // timers for scheduler operations\n      int timeWindowSize = conf.getInt(\n              SLSConfiguration.METRICS_TIMER_WINDOW_SIZE,\n              SLSConfiguration.METRICS_TIMER_WINDOW_SIZE_DEFAULT);\n      schedulerAllocateTimer = new Timer(\n              new SlidingWindowReservoir(timeWindowSize));\n      schedulerHandleTimer = new Timer(\n              new SlidingWindowReservoir(timeWindowSize));\n      schedulerHandleTimerMap = new HashMap<SchedulerEventType, Timer>();\n      for (SchedulerEventType e : SchedulerEventType.values()) {\n        Timer timer = new Timer(new SlidingWindowReservoir(timeWindowSize));\n        schedulerHandleTimerMap.put(e, timer);\n      }\n      // histogram for scheduler operations (Samplers)\n      schedulerHistogramList = new ArrayList<Histogram>();\n      histogramTimerMap = new HashMap<Histogram, Timer>();\n      Histogram schedulerAllocateHistogram = new Histogram(\n              new SlidingWindowReservoir(SAMPLING_SIZE));\n      metrics.register(\"sampler.scheduler.operation.allocate.timecost\",\n              schedulerAllocateHistogram);\n      schedulerHistogramList.add(schedulerAllocateHistogram);\n      histogramTimerMap.put(schedulerAllocateHistogram, schedulerAllocateTimer);\n      Histogram schedulerHandleHistogram = new Histogram(\n              new SlidingWindowReservoir(SAMPLING_SIZE));\n      metrics.register(\"sampler.scheduler.operation.handle.timecost\",\n              schedulerHandleHistogram);\n      schedulerHistogramList.add(schedulerHandleHistogram);\n      histogramTimerMap.put(schedulerHandleHistogram, schedulerHandleTimer);\n      for (SchedulerEventType e : SchedulerEventType.values()) {\n        Histogram histogram = new Histogram(\n                new SlidingWindowReservoir(SAMPLING_SIZE));\n        metrics.register(\n                \"sampler.scheduler.operation.handle.\" + e + \".timecost\",\n                histogram);\n        schedulerHistogramList.add(histogram);\n        histogramTimerMap.put(histogram, schedulerHandleTimerMap.get(e));\n      }\n    } finally {\n      samplerLock.unlock();\n    }\n  }\n\n  private void initMetricsCSVOutput() {\n    int timeIntervalMS = conf.getInt(\n            SLSConfiguration.METRICS_RECORD_INTERVAL_MS,\n            SLSConfiguration.METRICS_RECORD_INTERVAL_MS_DEFAULT);\n    File dir = new File(metricsOutputDir + \"/metrics\");\n    if(! dir.exists()\n            && ! dir.mkdirs()) {\n      LOG.error(\"Cannot create directory \" + dir.getAbsoluteFile());\n    }\n    final CsvReporter reporter = CsvReporter.forRegistry(metrics)\n            .formatFor(Locale.US)\n            .convertRatesTo(TimeUnit.SECONDS)\n            .convertDurationsTo(TimeUnit.MILLISECONDS)\n            .build(new File(metricsOutputDir + \"/metrics\"));\n    reporter.start(timeIntervalMS, TimeUnit.MILLISECONDS);\n  }\n\n  class HistogramsRunnable implements Runnable {\n    @Override\n    public void run() {\n      samplerLock.lock();\n      try {\n        for (Histogram histogram : schedulerHistogramList) {\n          Timer timer = histogramTimerMap.get(histogram);\n          histogram.update((int) timer.getSnapshot().getMean());\n        }\n      } finally {\n        samplerLock.unlock();\n      }\n    }\n  }\n\n  class MetricsLogRunnable implements Runnable {\n    private boolean firstLine = true;\n    public MetricsLogRunnable() {\n      try {\n        metricsLogBW = new BufferedWriter(\n                new FileWriter(metricsOutputDir + \"/realtimetrack.json\"));\n        metricsLogBW.write(\"[\");\n      } catch (IOException e) {\n        e.printStackTrace();\n      }\n    }\n\n    @Override\n    public void run() {\n      if(running) {\n        // all WebApp to get real tracking json\n        String metrics = web.generateRealTimeTrackingMetrics();\n        // output\n        try {\n          if(firstLine) {\n            metricsLogBW.write(metrics + EOL);\n            firstLine = false;\n          } else {\n            metricsLogBW.write(\",\" + metrics + EOL);\n          }\n          metricsLogBW.flush();\n        } catch (IOException e) {\n          e.printStackTrace();\n        }\n      }\n    }\n  }\n\n  // the following functions are used by AMSimulator\n  public void addAMRuntime(ApplicationId appId,\n                           long traceStartTimeMS, long traceEndTimeMS,\n                           long simulateStartTimeMS, long simulateEndTimeMS) {\n\n    try {\n      // write job runtime information\n      StringBuilder sb = new StringBuilder();\n      sb.append(appId).append(\",\").append(traceStartTimeMS).append(\",\")\n              .append(traceEndTimeMS).append(\",\").append(simulateStartTimeMS)\n              .append(\",\").append(simulateEndTimeMS);\n      jobRuntimeLogBW.write(sb.toString() + EOL);\n      jobRuntimeLogBW.flush();\n    } catch (IOException e) {\n      e.printStackTrace();\n    }\n  }\n\n  private void updateQueueMetrics(String queue,\n                                  int releasedMemory, int releasedVCores) {\n    // update queue counters\n    SortedMap<String, Counter> counterMap = metrics.getCounters();\n    if (releasedMemory != 0) {\n      String name = \"counter.queue.\" + queue + \".allocated.memory\";\n      if (! counterMap.containsKey(name)) {\n        metrics.counter(name);\n        counterMap = metrics.getCounters();\n      }\n      counterMap.get(name).inc(-releasedMemory);\n    }\n    if (releasedVCores != 0) {\n      String name = \"counter.queue.\" + queue + \".allocated.cores\";\n      if (! counterMap.containsKey(name)) {\n        metrics.counter(name);\n        counterMap = metrics.getCounters();\n      }\n      counterMap.get(name).inc(-releasedVCores);\n    }\n  }\n\n  public void setQueueSet(Set<String> queues) {\n    this.queueSet = queues;\n  }\n\n  public Set<String> getQueueSet() {\n    return this.queueSet;\n  }\n\n  public void setTrackedAppSet(Set<String> apps) {\n    this.trackedAppSet = apps;\n  }\n\n  public Set<String> getTrackedAppSet() {\n    return this.trackedAppSet;\n  }\n\n  public MetricRegistry getMetrics() {\n    return metrics;\n  }\n\n  public SchedulerMetrics getSchedulerMetrics() {\n    return schedulerMetrics;\n  }\n\n  // API open to out classes\n  public void addTrackedApp(ApplicationAttemptId appAttemptId,\n                            String oldAppId) {\n    if (metricsON) {\n      schedulerMetrics.trackApp(appAttemptId, oldAppId);\n    }\n  }\n\n  public void removeTrackedApp(ApplicationAttemptId appAttemptId,\n                               String oldAppId) {\n    if (metricsON) {\n      schedulerMetrics.untrackApp(appAttemptId, oldAppId);\n    }\n  }\n\n  @Override\n  public Configuration getConf() {\n    return conf;\n  }\n\n  @Override\n  public void reinitialize(Configuration entries, RMContext rmContext)\n          throws IOException {\n    scheduler.reinitialize(entries, rmContext);\n  }\n\n  @Override\n  public void recover(RMStateStore.RMState rmState) throws Exception {\n    scheduler.recover(rmState);\n  }\n\n  @Override\n  public QueueInfo getQueueInfo(String s, boolean b, boolean b2)\n          throws IOException {\n    return scheduler.getQueueInfo(s, b, b2);\n  }\n\n  @Override\n  public List<QueueUserACLInfo> getQueueUserAclInfo() {\n    return scheduler.getQueueUserAclInfo();\n  }\n\n  @Override\n  public Resource getMinimumResourceCapability() {\n    return scheduler.getMinimumResourceCapability();\n  }\n\n  @Override\n  public Resource getMaximumResourceCapability() {\n    return scheduler.getMaximumResourceCapability();\n  }\n\n  @Override\n  public int getNumClusterNodes() {\n    return scheduler.getNumClusterNodes();\n  }\n\n  @Override\n  public SchedulerNodeReport getNodeReport(NodeId nodeId) {\n    return scheduler.getNodeReport(nodeId);\n  }\n\n  @Override\n  public SchedulerAppReport getSchedulerAppInfo(\n          ApplicationAttemptId attemptId) {\n    return scheduler.getSchedulerAppInfo(attemptId);\n  }\n\n  @Override\n  public QueueMetrics getRootQueueMetrics() {\n    return scheduler.getRootQueueMetrics();\n  }\n\n  @Override\n  public synchronized boolean checkAccess(UserGroupInformation callerUGI,\n      QueueACL acl, String queueName) {\n    return scheduler.checkAccess(callerUGI, acl, queueName);\n  }\n\n  @Override\n  public ApplicationResourceUsageReport getAppResourceUsageReport(\n      ApplicationAttemptId appAttemptId) {\n    return scheduler.getAppResourceUsageReport(appAttemptId);\n  }\n\n  @Override\n  public List<ApplicationAttemptId> getAppsInQueue(String queue) {\n    return scheduler.getAppsInQueue(queue);\n  }\n}"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.removeApplication": {
                "code_before_change": [],
                "code_after_change": "  public synchronized void removeApplication(FiCaSchedulerApp application, User user) {\n    boolean wasActive = activeApplications.remove(application);\n    if (!wasActive) {\n      pendingApplications.remove(application);\n    }\n    applicationsMap.remove(application.getApplicationAttemptId());\n\n    user.finishApplication(wasActive);\n    if (user.getTotalApplications() == 0) {\n      users.remove(application.getUser());\n    }\n\n    // Check if we can activate more applications\n    activateApplications();\n    \n    // Inform the activeUsersManager\n    synchronized (application) {\n      activeUsersManager.deactivateApplication(\n          application.getUser(), application.getApplicationId());\n    }\n    \n    LOG.info(\"Application removed -\" +\n        \" appId: \" + application.getApplicationId() + \n        \" user: \" + application.getUser() + \n        \" queue: \" + getQueueName() +\n        \" #user-pending-applications: \" + user.getPendingApplications() +\n        \" #user-active-applications: \" + user.getActiveApplications() +\n        \" #queue-pending-applications: \" + getNumPendingApplications() +\n        \" #queue-active-applications: \" + getNumActiveApplications()\n        );\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.getNode": {
                "code_before_change": "  FiCaSchedulerNode getNode(NodeId nodeId) {\n    return nodes.get(nodeId);\n  }",
                "code_after_change": "  FiCaSchedulerNode getNode(NodeId nodeId) {\n    return nodes.get(nodeId);\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode.getRunningContainers": {
                "code_before_change": "  public synchronized List<RMContainer> getRunningContainers() {\n    return new ArrayList<RMContainer>(launchedContainers.values());\n  }",
                "code_after_change": "  public synchronized List<RMContainer> getRunningContainers() {\n    return new ArrayList<RMContainer>(launchedContainers.values());\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler.allocate": {
                "code_before_change": "  public Allocation allocate(\n      ApplicationAttemptId applicationAttemptId, List<ResourceRequest> ask,\n      List<ContainerId> release, List<String> blacklistAdditions, List<String> blacklistRemovals) {\n    FiCaSchedulerApp application = getApplicationAttempt(applicationAttemptId);\n    if (application == null) {\n      LOG.error(\"Calling allocate on removed \" +\n          \"or non existant application \" + applicationAttemptId);\n      return EMPTY_ALLOCATION;\n    }\n\n    // Sanity check\n    SchedulerUtils.normalizeRequests(ask, resourceCalculator, \n        clusterResource, minimumAllocation, maximumAllocation);\n\n    // Release containers\n    for (ContainerId releasedContainer : release) {\n      RMContainer rmContainer = getRMContainer(releasedContainer);\n      if (rmContainer == null) {\n         RMAuditLogger.logFailure(application.getUser(),\n             AuditConstants.RELEASE_CONTAINER, \n             \"Unauthorized access or invalid container\", \"FifoScheduler\", \n             \"Trying to release container not owned by app or with invalid id\",\n             application.getApplicationId(), releasedContainer);\n      }\n      containerCompleted(rmContainer,\n          SchedulerUtils.createAbnormalContainerStatus(\n              releasedContainer, \n              SchedulerUtils.RELEASED_CONTAINER),\n          RMContainerEventType.RELEASED);\n    }\n\n    synchronized (application) {\n\n      // make sure we aren't stopping/removing the application\n      // when the allocate comes in\n      if (application.isStopped()) {\n        LOG.info(\"Calling allocate on a stopped \" +\n            \"application \" + applicationAttemptId);\n        return EMPTY_ALLOCATION;\n      }\n\n      if (!ask.isEmpty()) {\n        LOG.debug(\"allocate: pre-update\" +\n            \" applicationId=\" + applicationAttemptId + \n            \" application=\" + application);\n        application.showRequests();\n\n        // Update application requests\n        application.updateResourceRequests(ask);\n\n        LOG.debug(\"allocate: post-update\" +\n            \" applicationId=\" + applicationAttemptId + \n            \" application=\" + application);\n        application.showRequests();\n\n        LOG.debug(\"allocate:\" +\n            \" applicationId=\" + applicationAttemptId + \n            \" #ask=\" + ask.size());\n      }\n\n      application.updateBlacklist(blacklistAdditions, blacklistRemovals);\n\n      return new Allocation(\n          application.pullNewlyAllocatedContainers(), \n          application.getHeadroom());\n    }\n  }",
                "code_after_change": "  public Allocation allocate(\n      ApplicationAttemptId applicationAttemptId, List<ResourceRequest> ask,\n      List<ContainerId> release, List<String> blacklistAdditions, List<String> blacklistRemovals) {\n    FiCaSchedulerApp application = getApplication(applicationAttemptId);\n    if (application == null) {\n      LOG.error(\"Calling allocate on removed \" +\n          \"or non existant application \" + applicationAttemptId);\n      return EMPTY_ALLOCATION;\n    }\n\n    // Sanity check\n    SchedulerUtils.normalizeRequests(ask, resourceCalculator, \n        clusterResource, minimumAllocation, maximumAllocation);\n\n    // Release containers\n    for (ContainerId releasedContainer : release) {\n      RMContainer rmContainer = getRMContainer(releasedContainer);\n      if (rmContainer == null) {\n         RMAuditLogger.logFailure(application.getUser(),\n             AuditConstants.RELEASE_CONTAINER, \n             \"Unauthorized access or invalid container\", \"FifoScheduler\", \n             \"Trying to release container not owned by app or with invalid id\",\n             application.getApplicationId(), releasedContainer);\n      }\n      containerCompleted(rmContainer,\n          SchedulerUtils.createAbnormalContainerStatus(\n              releasedContainer, \n              SchedulerUtils.RELEASED_CONTAINER),\n          RMContainerEventType.RELEASED);\n    }\n\n    synchronized (application) {\n\n      // make sure we aren't stopping/removing the application\n      // when the allocate comes in\n      if (application.isStopped()) {\n        LOG.info(\"Calling allocate on a stopped \" +\n            \"application \" + applicationAttemptId);\n        return EMPTY_ALLOCATION;\n      }\n\n      if (!ask.isEmpty()) {\n        LOG.debug(\"allocate: pre-update\" +\n            \" applicationId=\" + applicationAttemptId + \n            \" application=\" + application);\n        application.showRequests();\n\n        // Update application requests\n        application.updateResourceRequests(ask);\n\n        LOG.debug(\"allocate: post-update\" +\n            \" applicationId=\" + applicationAttemptId + \n            \" application=\" + application);\n        application.showRequests();\n\n        LOG.debug(\"allocate:\" +\n            \" applicationId=\" + applicationAttemptId + \n            \" #ask=\" + ask.size());\n      }\n\n      application.updateBlacklist(blacklistAdditions, blacklistRemovals);\n\n      return new Allocation(\n          application.pullNewlyAllocatedContainers(), \n          application.getHeadroom());\n    }\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler.handle": {
                "code_before_change": "  public void handle(SchedulerEvent event) {\n    switch(event.getType()) {\n    case NODE_ADDED:\n    {\n      NodeAddedSchedulerEvent nodeAddedEvent = (NodeAddedSchedulerEvent)event;\n      addNode(nodeAddedEvent.getAddedRMNode());\n    }\n    break;\n    case NODE_REMOVED:\n    {\n      NodeRemovedSchedulerEvent nodeRemovedEvent = (NodeRemovedSchedulerEvent)event;\n      removeNode(nodeRemovedEvent.getRemovedRMNode());\n    }\n    break;\n    case NODE_UPDATE:\n    {\n      NodeUpdateSchedulerEvent nodeUpdatedEvent = \n      (NodeUpdateSchedulerEvent)event;\n      nodeUpdate(nodeUpdatedEvent.getRMNode());\n    }\n    break;\n    case APP_ADDED:\n    {\n      AppAddedSchedulerEvent appAddedEvent = (AppAddedSchedulerEvent) event;\n      addApplication(appAddedEvent.getApplicationId(),\n        appAddedEvent.getQueue(), appAddedEvent.getUser());\n    }\n    break;\n    case APP_REMOVED:\n    {\n      AppRemovedSchedulerEvent appRemovedEvent = (AppRemovedSchedulerEvent)event;\n      doneApplication(appRemovedEvent.getApplicationID(),\n        appRemovedEvent.getFinalState());\n    }\n    break;\n    case APP_ATTEMPT_ADDED:\n    {\n      AppAttemptAddedSchedulerEvent appAttemptAddedEvent =\n          (AppAttemptAddedSchedulerEvent) event;\n      addApplicationAttempt(appAttemptAddedEvent.getApplicationAttemptId(),\n        appAttemptAddedEvent.getTransferStateFromPreviousAttempt());\n    }\n    break;\n    case APP_ATTEMPT_REMOVED:\n    {\n      AppAttemptRemovedSchedulerEvent appAttemptRemovedEvent =\n          (AppAttemptRemovedSchedulerEvent) event;\n      try {\n        doneApplicationAttempt(\n          appAttemptRemovedEvent.getApplicationAttemptID(),\n          appAttemptRemovedEvent.getFinalAttemptState(),\n          appAttemptRemovedEvent.getKeepContainersAcrossAppAttempts());\n      } catch(IOException ie) {\n        LOG.error(\"Unable to remove application \"\n            + appAttemptRemovedEvent.getApplicationAttemptID(), ie);\n      }\n    }\n    break;\n    case CONTAINER_EXPIRED:\n    {\n      ContainerExpiredSchedulerEvent containerExpiredEvent = \n          (ContainerExpiredSchedulerEvent) event;\n      ContainerId containerid = containerExpiredEvent.getContainerId();\n      containerCompleted(getRMContainer(containerid), \n          SchedulerUtils.createAbnormalContainerStatus(\n              containerid, \n              SchedulerUtils.EXPIRED_CONTAINER),\n          RMContainerEventType.EXPIRE);\n    }\n    break;\n    default:\n      LOG.error(\"Invalid eventtype \" + event.getType() + \". Ignoring!\");\n    }\n  }",
                "code_after_change": "  public void handle(SchedulerEvent event) {\n    switch(event.getType()) {\n    case NODE_ADDED:\n    {\n      NodeAddedSchedulerEvent nodeAddedEvent = (NodeAddedSchedulerEvent)event;\n      addNode(nodeAddedEvent.getAddedRMNode());\n    }\n    break;\n    case NODE_REMOVED:\n    {\n      NodeRemovedSchedulerEvent nodeRemovedEvent = (NodeRemovedSchedulerEvent)event;\n      removeNode(nodeRemovedEvent.getRemovedRMNode());\n    }\n    break;\n    case NODE_UPDATE:\n    {\n      NodeUpdateSchedulerEvent nodeUpdatedEvent = \n      (NodeUpdateSchedulerEvent)event;\n      nodeUpdate(nodeUpdatedEvent.getRMNode());\n    }\n    break;\n    case APP_ATTEMPT_ADDED:\n    {\n      AppAttemptAddedSchedulerEvent appAttemptAddedEvent =\n          (AppAttemptAddedSchedulerEvent) event;\n      addApplicationAttempt(appAttemptAddedEvent.getApplicationAttemptId(),\n        appAttemptAddedEvent.getUser());\n    }\n    break;\n    case APP_ATTEMPT_REMOVED:\n    {\n      AppAttemptRemovedSchedulerEvent appAttemptRemovedEvent =\n          (AppAttemptRemovedSchedulerEvent) event;\n      try {\n        doneApplicationAttempt(\n          appAttemptRemovedEvent.getApplicationAttemptID(),\n          appAttemptRemovedEvent.getFinalAttemptState());\n      } catch(IOException ie) {\n        LOG.error(\"Unable to remove application \"\n            + appAttemptRemovedEvent.getApplicationAttemptID(), ie);\n      }\n    }\n    break;\n    case CONTAINER_EXPIRED:\n    {\n      ContainerExpiredSchedulerEvent containerExpiredEvent = \n          (ContainerExpiredSchedulerEvent) event;\n      ContainerId containerid = containerExpiredEvent.getContainerId();\n      containerCompleted(getRMContainer(containerid), \n          SchedulerUtils.createAbnormalContainerStatus(\n              containerid, \n              SchedulerUtils.EXPIRED_CONTAINER),\n          RMContainerEventType.EXPIRE);\n    }\n    break;\n    default:\n      LOG.error(\"Invalid eventtype \" + event.getType() + \". Ignoring!\");\n    }\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-applications.hadoop-yarn-applications-distributedshell.src.main.java.org.apache.hadoop.yarn.applications.distributedshell.Client.Client": {
                "code_before_change": [],
                "code_after_change": []
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.RMAppAttemptImpl": {
                "code_before_change": [],
                "code_after_change": []
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Preventive",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies issues in the 'getTransferredContainers' and 'RMAppImpl.handle' methods, which are mentioned in the stack trace but are not the ground truth methods. Thus, the root cause identification is partial with a 'Shared Stack Trace Context' sub-category. The fix suggestion is preventive as it suggests adding checks to prevent the error, which aligns with the nature of the bug but does not match the exact developer's fix. The problem location identification is also partial with a 'Shared Stack Trace Context' sub-category, as it points to methods in the stack trace but not the exact ground truth methods. There is no wrong information in the bug report as all details are relevant to the context of the bug."
        }
    },
    {
        "filename": "YARN-5594.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.security.client.YARNDelegationTokenIdentifier.readFields": {
                "code_before_change": "  public synchronized void readFields(DataInput in) throws IOException {\n    builder.mergeFrom((DataInputStream) in);\n    if (builder.getOwner() != null) {\n      setOwner(new Text(builder.getOwner()));\n    }\n    if (builder.getRenewer() != null) {\n      setRenewer(new Text(builder.getRenewer()));\n    }\n    if (builder.getRealUser() != null) {\n      setRealUser(new Text(builder.getRealUser()));\n    }\n    setIssueDate(builder.getIssueDate());\n    setMaxDate(builder.getMaxDate());\n    setSequenceNumber(builder.getSequenceNumber());\n    setMasterKeyId(builder.getMasterKeyId());\n  }",
                "code_after_change": "  public synchronized void readFields(DataInput in) throws IOException {\n    builder.mergeFrom((DataInputStream) in);\n    if (builder.getOwner() != null) {\n      setOwner(new Text(builder.getOwner()));\n    }\n    if (builder.getRenewer() != null) {\n      setRenewer(new Text(builder.getRenewer()));\n    }\n    if (builder.getRealUser() != null) {\n      setRealUser(new Text(builder.getRealUser()));\n    }\n    setIssueDate(builder.getIssueDate());\n    setMaxDate(builder.getMaxDate());\n    setSequenceNumber(builder.getSequenceNumber());\n    setMasterKeyId(builder.getMasterKeyId());\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.recovery.records.RMDelegationTokenIdentifierData.getTokenIdentifier": {
                "code_before_change": "  public RMDelegationTokenIdentifier getTokenIdentifier() throws IOException {\n    ByteArrayInputStream in =\n        new ByteArrayInputStream(builder.getTokenIdentifier().toByteArray());\n    RMDelegationTokenIdentifier identifer = new RMDelegationTokenIdentifier();\n    identifer.readFields(new DataInputStream(in));\n    return identifer;\n  }",
                "code_after_change": "  public RMDelegationTokenIdentifier getTokenIdentifier() throws IOException {\n    ByteArrayInputStream in =\n        new ByteArrayInputStream(builder.getTokenIdentifier().toByteArray());\n    RMDelegationTokenIdentifier identifer = new RMDelegationTokenIdentifier();\n    identifer.readFields(new DataInputStream(in));\n    return identifer;\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.security.client.YARNDelegationTokenIdentifier.write": {
                "code_before_change": "  public synchronized void write(DataOutput out) throws IOException {\n    setBuilderFields();\n    builder.build().writeTo((DataOutputStream) out);\n  }",
                "code_after_change": "  public synchronized void write(DataOutput out) throws IOException {\n    setBuilderFields();\n    builder.build().writeTo((DataOutputStream) out);\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Alternative Fix",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the root cause as a mismatch in the RMDelegationToken file format between Hadoop versions, which is related to the ground truth methods as they deal with token identifiers. However, it does not precisely identify the exact root cause in the ground truth methods, but it is in the shared stack trace context. The fix suggestion involves handling the old data format during recovery, which is an alternative approach to the developer's fix, as it suggests a more general solution rather than specific code changes. The problem location mentions methods in the stack trace context but not the exact ground truth methods, hence it is partial. There is no wrong information in the bug report as all details are relevant to the context of the bug."
        }
    },
    {
        "filename": "YARN-7511.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceSet.resourceLocalized": {
                "code_before_change": "  public Set<String> resourceLocalized(LocalResourceRequest request,\n      Path location) {\n    Set<String> symlinks = pendingResources.remove(request);\n    if (symlinks == null) {\n      return null;\n    } else {\n      for (String symlink : symlinks) {\n        localizedResources.put(symlink, location);\n      }\n      return symlinks;\n    }\n  }",
                "code_after_change": "  public Set<String> resourceLocalized(LocalResourceRequest request,\n      Path location) {\n    Set<String> symlinks = pendingResources.remove(request);\n    if (symlinks == null) {\n      return null;\n    } else {\n      for (String symlink : symlinks) {\n        localizedResources.put(symlink, location);\n      }\n      return symlinks;\n    }\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceSet.resourceLocalizationFailed": {
                "code_before_change": "  public void resourceLocalizationFailed(LocalResourceRequest request) {\n    pendingResources.remove(request);\n    resourcesFailedToBeLocalized.add(request);\n  }",
                "code_after_change": "  public void resourceLocalizationFailed(LocalResourceRequest request) {\n    // Skip null request when localization failed for running container\n    if (request == null) {\n      return;\n    }\n    pendingResources.remove(request);\n    resourcesFailedToBeLocalized.add(request);\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Correct",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report precisely identifies the root cause as a NullPointerException occurring in the 'resourceLocalizationFailed' method of the 'ResourceSet' class, which matches the ground truth method. The fix suggestion in the bug report is correct as it matches the developer's fix, which involves adding a null check for the 'request' parameter in the 'resourceLocalizationFailed' method. The problem location identification is precise as the 'problem_location' field mentions the 'ResourceSet.resourceLocalizationFailed' method, which is in the ground truth list. There is no wrong information in the bug report as all details are consistent with the context of the bug."
        }
    },
    {
        "filename": "YARN-6068.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.AppLogAggregatorImpl.run": {
                "code_before_change": "          public Object run() throws Exception {\n            FileSystem remoteFS = remoteNodeLogFileForApp.getFileSystem(conf);\n            if (rename) {\n              remoteFS.rename(remoteNodeTmpLogFileForApp, renamedPath);\n            } else {\n              remoteFS.delete(remoteNodeTmpLogFileForApp, false);\n            }\n            return null;\n          }",
                "code_after_change": "          public Object run() throws Exception {\n            FileSystem remoteFS = remoteNodeLogFileForApp.getFileSystem(conf);\n            if (rename) {\n              remoteFS.rename(remoteNodeTmpLogFileForApp, renamedPath);\n            } else {\n              remoteFS.delete(remoteNodeTmpLogFileForApp, false);\n            }\n            return null;\n          }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Alternative Fix",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the root cause as an invalid state transition in the `ApplicationImpl` class, which is mentioned in the stack trace, but the actual fix was made in the `AppLogAggregatorImpl.run` method. Therefore, the root cause identification is 'Partial' with 'Shared Stack Trace Context' as the sub-category. The fix suggestion involves modifying state transitions, which is an alternative approach to resolving the issue, hence 'Alternative Fix'. The problem location identification is 'Partial' with 'Shared Stack Trace Context' because it mentions methods in the stack trace but not the exact ground truth method. There is no wrong information in the bug report as all details are relevant to the context of the bug."
        }
    },
    {
        "filename": "YARN-903.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.authorizeGetAndStopContainerRequest": {
                "code_before_change": "  protected void authorizeGetAndStopContainerRequest(ContainerId containerId,\n      Container container, boolean stopRequest) throws YarnException {\n\n    UserGroupInformation remoteUgi = getRemoteUgi();\n    NMTokenIdentifier identifier = selectNMTokenIdentifier(remoteUgi);\n\n    /*\n     * For get/stop container status; we need to verify that 1) User (NMToken)\n     * application attempt only has started container. 2) Requested containerId\n     * belongs to the same application attempt (NMToken) which was used. (Note:-\n     * This will prevent user in knowing another application's containers).\n     */\n\n    if ((!identifier.getApplicationAttemptId().equals(\n      containerId.getApplicationAttemptId()))\n        || (container != null && !identifier.getApplicationAttemptId().equals(\n          container.getContainerId().getApplicationAttemptId()))) {\n      if (stopRequest) {\n        LOG.warn(identifier.getApplicationAttemptId()\n            + \" attempted to stop non-application container : \"\n            + container.getContainerId().toString());\n        NMAuditLogger.logFailure(\"UnknownUser\", AuditConstants.STOP_CONTAINER,\n          \"ContainerManagerImpl\", \"Trying to stop unknown container!\",\n          identifier.getApplicationAttemptId().getApplicationId(),\n          container.getContainerId());\n      } else {\n        LOG.warn(identifier.getApplicationAttemptId()\n            + \" attempted to get get status for non-application container : \"\n            + container.getContainerId().toString());\n      }\n      throw RPCUtil.getRemoteException(\"Container \" + containerId.toString()\n          + \" is not started by this application attempt.\");\n    }\n\n    if (container == null) {\n      throw RPCUtil.getRemoteException(\"Container \" + containerId.toString()\n          + \" is not handled by this NodeManager\");\n    }\n  }",
                "code_after_change": "  protected void authorizeGetAndStopContainerRequest(ContainerId containerId,\n      Container container, boolean stopRequest, NMTokenIdentifier identifier)\n      throws YarnException {\n    /*\n     * For get/stop container status; we need to verify that 1) User (NMToken)\n     * application attempt only has started container. 2) Requested containerId\n     * belongs to the same application attempt (NMToken) which was used. (Note:-\n     * This will prevent user in knowing another application's containers).\n     */\n\n    if ((!identifier.getApplicationAttemptId().equals(\n      containerId.getApplicationAttemptId()))\n        || (container != null && !identifier.getApplicationAttemptId().equals(\n          container.getContainerId().getApplicationAttemptId()))) {\n      if (stopRequest) {\n        LOG.warn(identifier.getApplicationAttemptId()\n            + \" attempted to stop non-application container : \"\n            + container.getContainerId().toString());\n        NMAuditLogger.logFailure(\"UnknownUser\", AuditConstants.STOP_CONTAINER,\n          \"ContainerManagerImpl\", \"Trying to stop unknown container!\",\n          identifier.getApplicationAttemptId().getApplicationId(),\n          container.getContainerId());\n      } else {\n        LOG.warn(identifier.getApplicationAttemptId()\n            + \" attempted to get status for non-application container : \"\n            + container.getContainerId().toString());\n      }\n    }\n\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.sendOutofBandHeartBeat": {
                "code_before_change": "  public void sendOutofBandHeartBeat() {\n    synchronized (this.heartbeatMonitor) {\n      this.heartbeatMonitor.notify();\n    }\n  }",
                "code_after_change": "  public void sendOutofBandHeartBeat() {\n    synchronized (this.heartbeatMonitor) {\n      this.heartbeatMonitor.notify();\n    }\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.getNodeStatusAndUpdateContainersInContext": {
                "code_before_change": "  public NodeStatus getNodeStatusAndUpdateContainersInContext() {\n\n    NodeStatus nodeStatus = recordFactory.newRecordInstance(NodeStatus.class);\n    nodeStatus.setNodeId(this.nodeId);\n\n    int numActiveContainers = 0;\n    List<ContainerStatus> containersStatuses = new ArrayList<ContainerStatus>();\n    for (Iterator<Entry<ContainerId, Container>> i =\n        this.context.getContainers().entrySet().iterator(); i.hasNext();) {\n      Entry<ContainerId, Container> e = i.next();\n      ContainerId containerId = e.getKey();\n      Container container = e.getValue();\n\n      // Clone the container to send it to the RM\n      org.apache.hadoop.yarn.api.records.ContainerStatus containerStatus = \n          container.cloneAndGetContainerStatus();\n      containersStatuses.add(containerStatus);\n      ++numActiveContainers;\n      LOG.info(\"Sending out status for container: \" + containerStatus);\n\n      if (containerStatus.getState() == ContainerState.COMPLETE) {\n        // Remove\n        i.remove();\n\n        LOG.info(\"Removed completed container \" + containerId);\n      }\n    }\n    nodeStatus.setContainersStatuses(containersStatuses);\n\n    LOG.debug(this.nodeId + \" sending out status for \"\n        + numActiveContainers + \" containers\");\n\n    NodeHealthStatus nodeHealthStatus = this.context.getNodeHealthStatus();\n    nodeHealthStatus.setHealthReport(healthChecker.getHealthReport());\n    nodeHealthStatus.setIsNodeHealthy(healthChecker.isHealthy());\n    nodeHealthStatus.setLastHealthReportTime(\n        healthChecker.getLastHealthReportTime());\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Node's health-status : \" + nodeHealthStatus.getIsNodeHealthy()\n                + \", \" + nodeHealthStatus.getHealthReport());\n    }\n    nodeStatus.setNodeHealthStatus(nodeHealthStatus);\n\n    List<ApplicationId> keepAliveAppIds = createKeepAliveApplicationList();\n    nodeStatus.setKeepAliveApplications(keepAliveAppIds);\n    \n    return nodeStatus;\n  }",
                "code_after_change": "  public NodeStatus getNodeStatusAndUpdateContainersInContext() {\n\n    NodeStatus nodeStatus = recordFactory.newRecordInstance(NodeStatus.class);\n    nodeStatus.setNodeId(this.nodeId);\n\n    int numActiveContainers = 0;\n    List<ContainerStatus> containersStatuses = new ArrayList<ContainerStatus>();\n    for (Iterator<Entry<ContainerId, Container>> i =\n        this.context.getContainers().entrySet().iterator(); i.hasNext();) {\n      Entry<ContainerId, Container> e = i.next();\n      ContainerId containerId = e.getKey();\n      Container container = e.getValue();\n\n      // Clone the container to send it to the RM\n      org.apache.hadoop.yarn.api.records.ContainerStatus containerStatus = \n          container.cloneAndGetContainerStatus();\n      containersStatuses.add(containerStatus);\n      ++numActiveContainers;\n      LOG.info(\"Sending out status for container: \" + containerStatus);\n\n      if (containerStatus.getState() == ContainerState.COMPLETE) {\n        // Remove\n        i.remove();\n        // Adding to finished containers cache. Cache will keep it around at\n        // least for #durationToTrackStoppedContainers duration. In the\n        // subsequent call to stop container it will get removed from cache.\n        addStoppedContainersToCache(containerId);\n        \n        LOG.info(\"Removed completed container \" + containerId);\n      }\n    }\n    nodeStatus.setContainersStatuses(containersStatuses);\n\n    LOG.debug(this.nodeId + \" sending out status for \"\n        + numActiveContainers + \" containers\");\n\n    NodeHealthStatus nodeHealthStatus = this.context.getNodeHealthStatus();\n    nodeHealthStatus.setHealthReport(healthChecker.getHealthReport());\n    nodeHealthStatus.setIsNodeHealthy(healthChecker.isHealthy());\n    nodeHealthStatus.setLastHealthReportTime(\n        healthChecker.getLastHealthReportTime());\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Node's health-status : \" + nodeHealthStatus.getIsNodeHealthy()\n                + \", \" + nodeHealthStatus.getHealthReport());\n    }\n    nodeStatus.setNodeHealthStatus(nodeHealthStatus);\n\n    List<ApplicationId> keepAliveAppIds = createKeepAliveApplicationList();\n    nodeStatus.setKeepAliveApplications(keepAliveAppIds);\n    \n    return nodeStatus;\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.updateMasterKeys": {
                "code_before_change": "      private void updateMasterKeys(NodeHeartbeatResponse response) {\n        // See if the master-key has rolled over\n        MasterKey updatedMasterKey = response.getContainerTokenMasterKey();\n        if (updatedMasterKey != null) {\n          // Will be non-null only on roll-over on RM side\n          context.getContainerTokenSecretManager().setMasterKey(updatedMasterKey);\n        }\n        \n        updatedMasterKey = response.getNMTokenMasterKey();\n        if (updatedMasterKey != null) {\n          context.getNMTokenSecretManager().setMasterKey(updatedMasterKey);\n        }\n      }",
                "code_after_change": "      private void updateMasterKeys(NodeHeartbeatResponse response) {\n        // See if the master-key has rolled over\n        MasterKey updatedMasterKey = response.getContainerTokenMasterKey();\n        if (updatedMasterKey != null) {\n          // Will be non-null only on roll-over on RM side\n          context.getContainerTokenSecretManager().setMasterKey(updatedMasterKey);\n        }\n        \n        updatedMasterKey = response.getNMTokenMasterKey();\n        if (updatedMasterKey != null) {\n          context.getNMTokenSecretManager().setMasterKey(updatedMasterKey);\n        }\n      }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.stopContainerInternal": {
                "code_before_change": [],
                "code_after_change": "  private void stopContainerInternal(NMTokenIdentifier nmTokenIdentifier,\n      ContainerId containerID) throws YarnException {\n    String containerIDStr = containerID.toString();\n    Container container = this.context.getContainers().get(containerID);\n    LOG.info(\"Stopping container with container Id: \" + containerIDStr);\n    authorizeGetAndStopContainerRequest(containerID, container, true,\n      nmTokenIdentifier);\n\n    if (container == null) {\n      if (!nodeStatusUpdater.isContainerRecentlyStopped(containerID)) {\n        throw RPCUtil.getRemoteException(\"Container \" + containerIDStr\n          + \" is not handled by this NodeManager\");\n      }\n    } else {\n      dispatcher.getEventHandler().handle(\n        new ContainerKillEvent(containerID,\n          \"Container killed by the ApplicationMaster.\"));\n\n      NMAuditLogger.logSuccess(container.getUser(),    \n        AuditConstants.STOP_CONTAINER, \"ContainerManageImpl\", containerID\n          .getApplicationAttemptId().getApplicationId(), containerID);\n\n      // TODO: Move this code to appropriate place once kill_container is\n      // implemented.\n      nodeStatusUpdater.sendOutofBandHeartBeat();\n    }\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.NodeStatusUpdaterImpl": {
                "code_before_change": [],
                "code_after_change": []
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.serviceInit": {
                "code_before_change": "  protected void serviceInit(Configuration conf) throws Exception {\n    this.rmAddress = conf.getSocketAddr(\n        YarnConfiguration.RM_RESOURCE_TRACKER_ADDRESS,\n        YarnConfiguration.DEFAULT_RM_RESOURCE_TRACKER_ADDRESS,\n        YarnConfiguration.DEFAULT_RM_RESOURCE_TRACKER_PORT);\n\n    int memoryMb = \n        conf.getInt(\n            YarnConfiguration.NM_PMEM_MB, YarnConfiguration.DEFAULT_NM_PMEM_MB);\n    float vMemToPMem =             \n        conf.getFloat(\n            YarnConfiguration.NM_VMEM_PMEM_RATIO, \n            YarnConfiguration.DEFAULT_NM_VMEM_PMEM_RATIO); \n    int virtualMemoryMb = (int)Math.ceil(memoryMb * vMemToPMem);\n    \n    int virtualCores =\n        conf.getInt(\n            YarnConfiguration.NM_VCORES, YarnConfiguration.DEFAULT_NM_VCORES);\n\n    this.totalResource = recordFactory.newRecordInstance(Resource.class);\n    this.totalResource.setMemory(memoryMb);\n    this.totalResource.setVirtualCores(virtualCores);\n    metrics.addResource(totalResource);\n    this.tokenKeepAliveEnabled = isTokenKeepAliveEnabled(conf);\n    this.tokenRemovalDelayMs =\n        conf.getInt(YarnConfiguration.RM_NM_EXPIRY_INTERVAL_MS,\n            YarnConfiguration.DEFAULT_RM_NM_EXPIRY_INTERVAL_MS);\n    \n    LOG.info(\"Initialized nodemanager for \" + nodeId + \":\" +\n        \" physical-memory=\" + memoryMb + \" virtual-memory=\" + virtualMemoryMb +\n        \" virtual-cores=\" + virtualCores);\n    \n    super.serviceInit(conf);\n  }",
                "code_after_change": "  protected void serviceInit(Configuration conf) throws Exception {\n    int memoryMb = \n        conf.getInt(\n            YarnConfiguration.NM_PMEM_MB, YarnConfiguration.DEFAULT_NM_PMEM_MB);\n    float vMemToPMem =             \n        conf.getFloat(\n            YarnConfiguration.NM_VMEM_PMEM_RATIO, \n            YarnConfiguration.DEFAULT_NM_VMEM_PMEM_RATIO); \n    int virtualMemoryMb = (int)Math.ceil(memoryMb * vMemToPMem);\n    \n    int virtualCores =\n        conf.getInt(\n            YarnConfiguration.NM_VCORES, YarnConfiguration.DEFAULT_NM_VCORES);\n\n    this.totalResource = recordFactory.newRecordInstance(Resource.class);\n    this.totalResource.setMemory(memoryMb);\n    this.totalResource.setVirtualCores(virtualCores);\n    metrics.addResource(totalResource);\n    this.tokenKeepAliveEnabled = isTokenKeepAliveEnabled(conf);\n    this.tokenRemovalDelayMs =\n        conf.getInt(YarnConfiguration.RM_NM_EXPIRY_INTERVAL_MS,\n            YarnConfiguration.DEFAULT_RM_NM_EXPIRY_INTERVAL_MS);\n    \n    // Default duration to track stopped containers on nodemanager is 10Min.\n    // This should not be assigned very large value as it will remember all the\n    // containers stopped during that time.\n    durationToTrackStoppedContainers =\n        conf.getLong(YARN_NODEMANAGER_DURATION_TO_TRACK_STOPPED_CONTAINERS,\n          600000);\n    if (durationToTrackStoppedContainers < 0) {\n      String message = \"Invalid configuration for \"\n        + YARN_NODEMANAGER_DURATION_TO_TRACK_STOPPED_CONTAINERS + \" default \"\n          + \"value is 10Min(600000).\";\n      LOG.error(message);\n      throw new YarnException(message);\n    }\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(YARN_NODEMANAGER_DURATION_TO_TRACK_STOPPED_CONTAINERS + \" :\"\n        + durationToTrackStoppedContainers);\n    }\n    super.serviceInit(conf);\n    LOG.info(\"Initialized nodemanager for \" + nodeId + \":\" +\n        \" physical-memory=\" + memoryMb + \" virtual-memory=\" + virtualMemoryMb +\n        \" virtual-cores=\" + virtualCores);\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.getContainerStatusInternal": {
                "code_before_change": [],
                "code_after_change": "  private ContainerStatus getContainerStatusInternal(ContainerId containerID,\n      NMTokenIdentifier nmTokenIdentifier) throws YarnException {\n    String containerIDStr = containerID.toString();\n    Container container = this.context.getContainers().get(containerID);\n\n    LOG.info(\"Getting container-status for \" + containerIDStr);\n    authorizeGetAndStopContainerRequest(containerID, container, false,\n      nmTokenIdentifier);\n\n    if (container == null) {\n      if (nodeStatusUpdater.isContainerRecentlyStopped(containerID)) {\n        throw RPCUtil.getRemoteException(\"Container \" + containerIDStr\n          + \" was recently stopped on node manager.\");\n      } else {\n        throw RPCUtil.getRemoteException(\"Container \" + containerIDStr\n          + \" is not handled by this NodeManager\");\n      }\n    }\n    ContainerStatus containerStatus = container.cloneAndGetContainerStatus();\n    LOG.info(\"Returning \" + containerStatus);\n    return containerStatus;\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.NodeManager.main": {
                "code_before_change": "  public static void main(String[] args) {\n    Thread.setDefaultUncaughtExceptionHandler(new YarnUncaughtExceptionHandler());\n    StringUtils.startupShutdownMessage(NodeManager.class, args, LOG);\n    NodeManager nodeManager = new NodeManager();\n    Configuration conf = new YarnConfiguration();\n    nodeManager.initAndStartNodeManager(conf, false);\n  }",
                "code_after_change": "  public static void main(String[] args) {\n    Thread.setDefaultUncaughtExceptionHandler(new YarnUncaughtExceptionHandler());\n    StringUtils.startupShutdownMessage(NodeManager.class, args, LOG);\n    NodeManager nodeManager = new NodeManager();\n    Configuration conf = new YarnConfiguration();\n    nodeManager.initAndStartNodeManager(conf, false);\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Buggy Method"
            },
            "fix_suggestion": "Preventive",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the root cause as a misconfiguration or state issue in the NodeManager, which is related to the ground truth method 'ContainerManagerImpl.authorizeGetAndStopContainerRequest'. However, it does not precisely identify the root cause at the ground truth method level, hence it is classified as 'Partial' with 'Buggy Method' sub-category. The fix suggestion involves checking the configuration and ensuring proper container management, which is preventive in nature as it aims to mitigate the issue. The problem location identification is precise as it mentions 'ContainerManagerImpl.authorizeGetAndStopContainerRequest', which is one of the ground truth methods. There is no wrong information in the bug report as all details are relevant to the context of the bug."
        }
    },
    {
        "filename": "YARN-8236.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-applications.hadoop-yarn-services.hadoop-yarn-services-core.src.main.java.org.apache.hadoop.yarn.service.client.ServiceClient.addKeytabResourceIfSecure": {
                "code_before_change": "  private void addKeytabResourceIfSecure(SliderFileSystem fileSystem,\n      Map<String, LocalResource> localResource, Service service)\n      throws IOException, YarnException {\n    if (!UserGroupInformation.isSecurityEnabled()) {\n      return;\n    }\n    String principalName = service.getKerberosPrincipal().getPrincipalName();\n    if (StringUtils.isEmpty(principalName)) {\n      LOG.warn(\"No Kerberos principal name specified for \" + service.getName());\n      return;\n    }\n    if(StringUtils.isEmpty(service.getKerberosPrincipal().getKeytab())) {\n      LOG.warn(\"No Kerberos keytab specified for \" + service.getName());\n      return;\n    }\n\n    URI keytabURI;\n    try {\n      keytabURI = new URI(service.getKerberosPrincipal().getKeytab());\n    } catch (URISyntaxException e) {\n      throw new YarnException(e);\n    }\n\n    switch (keytabURI.getScheme()) {\n    case \"hdfs\":\n      Path keytabOnhdfs = new Path(keytabURI);\n      if (!fileSystem.getFileSystem().exists(keytabOnhdfs)) {\n        LOG.warn(service.getName() + \"'s keytab (principalName = \" +\n            principalName + \") doesn't exist at: \" + keytabOnhdfs);\n        return;\n      }\n      LocalResource keytabRes =\n          fileSystem.createAmResource(keytabOnhdfs, LocalResourceType.FILE);\n      localResource.put(String.format(YarnServiceConstants.KEYTAB_LOCATION,\n          service.getName()), keytabRes);\n      LOG.debug(\"Adding \" + service.getName() + \"'s keytab for \" +\n          \"localization, uri = \" + keytabOnhdfs);\n      break;\n    case \"file\":\n      LOG.debug(\"Using a keytab from localhost: \" + keytabURI);\n      break;\n    default:\n      LOG.warn(\"Unsupported URI scheme \" + keytabURI);\n      break;\n    }\n  }",
                "code_after_change": "  private void addKeytabResourceIfSecure(SliderFileSystem fileSystem,\n      Map<String, LocalResource> localResource, Service service)\n      throws IOException, YarnException {\n    if (!UserGroupInformation.isSecurityEnabled()) {\n      return;\n    }\n    String principalName = service.getKerberosPrincipal().getPrincipalName();\n    if (StringUtils.isEmpty(principalName)) {\n      LOG.warn(\"No Kerberos principal name specified for \" + service.getName());\n      return;\n    }\n    if (StringUtils.isEmpty(service.getKerberosPrincipal().getKeytab())) {\n      LOG.warn(\"No Kerberos keytab specified for \" + service.getName());\n      return;\n    }\n\n    URI keytabURI;\n    try {\n      keytabURI = new URI(service.getKerberosPrincipal().getKeytab());\n    } catch (URISyntaxException e) {\n      throw new YarnException(e);\n    }\n\n    if (keytabURI.getScheme() != null) {\n      switch (keytabURI.getScheme()) {\n      case \"hdfs\":\n        Path keytabOnhdfs = new Path(keytabURI);\n        if (!fileSystem.getFileSystem().exists(keytabOnhdfs)) {\n          LOG.warn(service.getName() + \"'s keytab (principalName = \"\n              + principalName + \") doesn't exist at: \" + keytabOnhdfs);\n          return;\n        }\n        LocalResource keytabRes = fileSystem.createAmResource(keytabOnhdfs,\n            LocalResourceType.FILE);\n        localResource.put(String.format(YarnServiceConstants.KEYTAB_LOCATION,\n            service.getName()), keytabRes);\n        LOG.info(\"Adding \" + service.getName() + \"'s keytab for \"\n            + \"localization, uri = \" + keytabOnhdfs);\n        break;\n      case \"file\":\n        LOG.info(\"Using a keytab from localhost: \" + keytabURI);\n        break;\n      default:\n        LOG.warn(\"Unsupported keytab URI scheme \" + keytabURI);\n        break;\n      }\n    } else {\n      LOG.warn(\"Unsupported keytab URI scheme \" + keytabURI);\n    }\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-applications.hadoop-yarn-services.hadoop-yarn-services-core.src.main.java.org.apache.hadoop.yarn.service.utils.ServiceApiUtil.validateAndResolveService": {
                "code_before_change": "  public static void validateAndResolveService(Service service,\n      SliderFileSystem fs, org.apache.hadoop.conf.Configuration conf) throws\n      IOException {\n    boolean dnsEnabled = conf.getBoolean(RegistryConstants.KEY_DNS_ENABLED,\n        RegistryConstants.DEFAULT_DNS_ENABLED);\n    if (dnsEnabled) {\n      if (RegistryUtils.currentUser().length()\n          > RegistryConstants.MAX_FQDN_LABEL_LENGTH) {\n        throw new IllegalArgumentException(\n            RestApiErrorMessages.ERROR_USER_NAME_INVALID);\n      }\n      userNamePattern.validate(RegistryUtils.currentUser());\n    }\n\n    if (StringUtils.isEmpty(service.getName())) {\n      throw new IllegalArgumentException(\n          RestApiErrorMessages.ERROR_APPLICATION_NAME_INVALID);\n    }\n\n    if (StringUtils.isEmpty(service.getVersion())) {\n      throw new IllegalArgumentException(String.format(\n          RestApiErrorMessages.ERROR_APPLICATION_VERSION_INVALID,\n          service.getName()));\n    }\n\n    validateNameFormat(service.getName(), conf);\n\n    // If the service has no components, throw error\n    if (!hasComponent(service)) {\n      throw new IllegalArgumentException(\n          \"No component specified for \" + service.getName());\n    }\n\n    if (UserGroupInformation.isSecurityEnabled()) {\n      if (!StringUtils.isEmpty(service.getKerberosPrincipal().getKeytab())) {\n        try {\n          // validate URI format\n          new URI(service.getKerberosPrincipal().getKeytab());\n        } catch (URISyntaxException e) {\n          throw new IllegalArgumentException(e);\n        }\n      }\n    }\n\n    // Validate the Docker client config.\n    try {\n      validateDockerClientConfiguration(service, conf);\n    } catch (IOException e) {\n      throw new IllegalArgumentException(e);\n    }\n\n    // Validate there are no component name collisions (collisions are not\n    // currently supported) and add any components from external services\n    Configuration globalConf = service.getConfiguration();\n    Set<String> componentNames = new HashSet<>();\n    List<Component> componentsToRemove = new ArrayList<>();\n    List<Component> componentsToAdd = new ArrayList<>();\n    for (Component comp : service.getComponents()) {\n      int maxCompLength = RegistryConstants.MAX_FQDN_LABEL_LENGTH;\n      maxCompLength = maxCompLength - Long.toString(Long.MAX_VALUE).length();\n      if (dnsEnabled && comp.getName().length() > maxCompLength) {\n        throw new IllegalArgumentException(String.format(RestApiErrorMessages\n            .ERROR_COMPONENT_NAME_INVALID, maxCompLength, comp.getName()));\n      }\n      if (componentNames.contains(comp.getName())) {\n        throw new IllegalArgumentException(\"Component name collision: \" +\n            comp.getName());\n      }\n      // If artifact is of type SERVICE (which cannot be filled from\n      // global), read external service and add its components to this\n      // service\n      if (comp.getArtifact() != null && comp.getArtifact().getType() ==\n          Artifact.TypeEnum.SERVICE) {\n        if (StringUtils.isEmpty(comp.getArtifact().getId())) {\n          throw new IllegalArgumentException(\n              RestApiErrorMessages.ERROR_ARTIFACT_ID_INVALID);\n        }\n        LOG.info(\"Marking {} for removal\", comp.getName());\n        componentsToRemove.add(comp);\n        List<Component> externalComponents = getComponents(fs,\n            comp.getArtifact().getId());\n        for (Component c : externalComponents) {\n          Component override = service.getComponent(c.getName());\n          if (override != null && override.getArtifact() == null) {\n            // allow properties from external components to be overridden /\n            // augmented by properties in this component, except for artifact\n            // which must be read from external component\n            override.mergeFrom(c);\n            LOG.info(\"Merging external component {} from external {}\", c\n                .getName(), comp.getName());\n          } else {\n            if (componentNames.contains(c.getName())) {\n              throw new IllegalArgumentException(\"Component name collision: \" +\n                  c.getName());\n            }\n            componentNames.add(c.getName());\n            componentsToAdd.add(c);\n            LOG.info(\"Adding component {} from external {}\", c.getName(),\n                comp.getName());\n          }\n        }\n      } else {\n        // otherwise handle as a normal component\n        componentNames.add(comp.getName());\n        // configuration\n        comp.getConfiguration().mergeFrom(globalConf);\n      }\n    }\n    service.getComponents().removeAll(componentsToRemove);\n    service.getComponents().addAll(componentsToAdd);\n\n    // Validate components and let global values take effect if component level\n    // values are not provided\n    Artifact globalArtifact = service.getArtifact();\n    Resource globalResource = service.getResource();\n    for (Component comp : service.getComponents()) {\n      // fill in global artifact unless it is type SERVICE\n      if (comp.getArtifact() == null && service.getArtifact() != null\n          && service.getArtifact().getType() != Artifact.TypeEnum\n          .SERVICE) {\n        comp.setArtifact(globalArtifact);\n      }\n      // fill in global resource\n      if (comp.getResource() == null) {\n        comp.setResource(globalResource);\n      }\n      // validate dependency existence\n      if (comp.getDependencies() != null) {\n        for (String dependency : comp.getDependencies()) {\n          if (!componentNames.contains(dependency)) {\n            throw new IllegalArgumentException(String.format(\n                RestApiErrorMessages.ERROR_DEPENDENCY_INVALID, dependency,\n                comp.getName()));\n          }\n        }\n      }\n      validateComponent(comp, fs.getFileSystem(), conf);\n    }\n    validatePlacementPolicy(service.getComponents(), componentNames);\n\n    // validate dependency tree\n    sortByDependencies(service.getComponents());\n\n    // Service lifetime if not specified, is set to unlimited lifetime\n    if (service.getLifetime() == null) {\n      service.setLifetime(RestApiConstants.DEFAULT_UNLIMITED_LIFETIME);\n    }\n  }",
                "code_after_change": "  public static void validateAndResolveService(Service service,\n      SliderFileSystem fs, org.apache.hadoop.conf.Configuration conf) throws\n      IOException {\n    boolean dnsEnabled = conf.getBoolean(RegistryConstants.KEY_DNS_ENABLED,\n        RegistryConstants.DEFAULT_DNS_ENABLED);\n    if (dnsEnabled) {\n      if (RegistryUtils.currentUser().length()\n          > RegistryConstants.MAX_FQDN_LABEL_LENGTH) {\n        throw new IllegalArgumentException(\n            RestApiErrorMessages.ERROR_USER_NAME_INVALID);\n      }\n      userNamePattern.validate(RegistryUtils.currentUser());\n    }\n\n    if (StringUtils.isEmpty(service.getName())) {\n      throw new IllegalArgumentException(\n          RestApiErrorMessages.ERROR_APPLICATION_NAME_INVALID);\n    }\n\n    if (StringUtils.isEmpty(service.getVersion())) {\n      throw new IllegalArgumentException(String.format(\n          RestApiErrorMessages.ERROR_APPLICATION_VERSION_INVALID,\n          service.getName()));\n    }\n\n    validateNameFormat(service.getName(), conf);\n\n    // If the service has no components, throw error\n    if (!hasComponent(service)) {\n      throw new IllegalArgumentException(\n          \"No component specified for \" + service.getName());\n    }\n\n    if (UserGroupInformation.isSecurityEnabled()) {\n      validateKerberosPrincipal(service.getKerberosPrincipal());\n    }\n\n    // Validate the Docker client config.\n    try {\n      validateDockerClientConfiguration(service, conf);\n    } catch (IOException e) {\n      throw new IllegalArgumentException(e);\n    }\n\n    // Validate there are no component name collisions (collisions are not\n    // currently supported) and add any components from external services\n    Configuration globalConf = service.getConfiguration();\n    Set<String> componentNames = new HashSet<>();\n    List<Component> componentsToRemove = new ArrayList<>();\n    List<Component> componentsToAdd = new ArrayList<>();\n    for (Component comp : service.getComponents()) {\n      int maxCompLength = RegistryConstants.MAX_FQDN_LABEL_LENGTH;\n      maxCompLength = maxCompLength - Long.toString(Long.MAX_VALUE).length();\n      if (dnsEnabled && comp.getName().length() > maxCompLength) {\n        throw new IllegalArgumentException(String.format(RestApiErrorMessages\n            .ERROR_COMPONENT_NAME_INVALID, maxCompLength, comp.getName()));\n      }\n      if (componentNames.contains(comp.getName())) {\n        throw new IllegalArgumentException(\"Component name collision: \" +\n            comp.getName());\n      }\n      // If artifact is of type SERVICE (which cannot be filled from global),\n      // read external service and add its components to this service\n      if (comp.getArtifact() != null && comp.getArtifact().getType() ==\n          Artifact.TypeEnum.SERVICE) {\n        if (StringUtils.isEmpty(comp.getArtifact().getId())) {\n          throw new IllegalArgumentException(\n              RestApiErrorMessages.ERROR_ARTIFACT_ID_INVALID);\n        }\n        LOG.info(\"Marking {} for removal\", comp.getName());\n        componentsToRemove.add(comp);\n        List<Component> externalComponents = getComponents(fs,\n            comp.getArtifact().getId());\n        for (Component c : externalComponents) {\n          Component override = service.getComponent(c.getName());\n          if (override != null && override.getArtifact() == null) {\n            // allow properties from external components to be overridden /\n            // augmented by properties in this component, except for artifact\n            // which must be read from external component\n            override.mergeFrom(c);\n            LOG.info(\"Merging external component {} from external {}\", c\n                .getName(), comp.getName());\n          } else {\n            if (componentNames.contains(c.getName())) {\n              throw new IllegalArgumentException(\"Component name collision: \" +\n                  c.getName());\n            }\n            componentNames.add(c.getName());\n            componentsToAdd.add(c);\n            LOG.info(\"Adding component {} from external {}\", c.getName(),\n                comp.getName());\n          }\n        }\n      } else {\n        // otherwise handle as a normal component\n        componentNames.add(comp.getName());\n        // configuration\n        comp.getConfiguration().mergeFrom(globalConf);\n      }\n    }\n    service.getComponents().removeAll(componentsToRemove);\n    service.getComponents().addAll(componentsToAdd);\n\n    // Validate components and let global values take effect if component level\n    // values are not provided\n    Artifact globalArtifact = service.getArtifact();\n    Resource globalResource = service.getResource();\n    for (Component comp : service.getComponents()) {\n      // fill in global artifact unless it is type SERVICE\n      if (comp.getArtifact() == null && service.getArtifact() != null\n          && service.getArtifact().getType() != Artifact.TypeEnum\n          .SERVICE) {\n        comp.setArtifact(globalArtifact);\n      }\n      // fill in global resource\n      if (comp.getResource() == null) {\n        comp.setResource(globalResource);\n      }\n      // validate dependency existence\n      if (comp.getDependencies() != null) {\n        for (String dependency : comp.getDependencies()) {\n          if (!componentNames.contains(dependency)) {\n            throw new IllegalArgumentException(String.format(\n                RestApiErrorMessages.ERROR_DEPENDENCY_INVALID, dependency,\n                comp.getName()));\n          }\n        }\n      }\n      validateComponent(comp, fs.getFileSystem(), conf);\n    }\n    validatePlacementPolicy(service.getComponents(), componentNames);\n\n    // validate dependency tree\n    sortByDependencies(service.getComponents());\n\n    // Service lifetime if not specified, is set to unlimited lifetime\n    if (service.getLifetime() == null) {\n      service.setLifetime(RestApiConstants.DEFAULT_UNLIMITED_LIFETIME);\n    }\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Correct",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report precisely identifies the root cause as a NullPointerException due to a potentially null 'Service' object in the 'addKeytabResourceIfSecure' method, which matches the ground truth method. The fix suggestion to add null checks in the 'validateAndResolveService' method aligns with the developer's fix, which includes validation of the Kerberos principal. The problem location is precisely identified as it mentions both 'addKeytabResourceIfSecure' and 'validateAndResolveService', which are the ground truth methods. There is no wrong information in the bug report as all details are consistent with the context of the bug."
        }
    },
    {
        "filename": "YARN-2857.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.ContainerLogAppender.activateOptions": {
                "code_before_change": "  public void activateOptions() {\n    synchronized (this) {\n      if (maxEvents > 0) {\n        tail = new LinkedList<LoggingEvent>();\n      }\n      setFile(new File(this.containerLogDir, \"syslog\").toString());\n      setAppend(true);\n      super.activateOptions();\n    }\n  }",
                "code_after_change": "  public void activateOptions() {\n    synchronized (this) {\n      if (maxEvents > 0) {\n        tail = new LinkedList<LoggingEvent>();\n      }\n      setFile(new File(this.containerLogDir, \"syslog\").toString());\n      setAppend(true);\n      super.activateOptions();\n    }\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.ContainerLogAppender.flush": {
                "code_before_change": "  public void flush() {\n    if (qw != null) {\n      qw.flush();\n    }\n  }",
                "code_after_change": "  public void flush() {\n    if (qw != null) {\n      qw.flush();\n    }\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Buggy Method"
            },
            "fix_suggestion": "Alternative Fix",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Buggy Method"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the root cause as a `ConcurrentModificationException` in the `ContainerLogAppender.close` method, which is where the error occurs, but not where the actual fix was made. Thus, it is classified as 'Partial' under 'Buggy Method'. The fix suggestion involves using concurrent collections or synchronized blocks, which is an alternative approach to the developer's fix, which did not change the method. The problem location is identified as `ContainerLogAppender.close`, which is the method where the error occurs, but not where the fix was made, hence 'Partial' under 'Buggy Method'. There is no wrong information in the bug report as all details are relevant to the context of the bug."
        }
    },
    {
        "filename": "YARN-2416.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.transition": {
                "code_before_change": "    public void transition(RMAppAttemptImpl appAttempt,\n        RMAppAttemptEvent event) {\n    }",
                "code_after_change": "    public void transition(RMAppAttemptImpl appAttempt,\n        RMAppAttemptEvent event) {\n    }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.StatusUpdateTransition": {
                "code_before_change": [],
                "code_after_change": []
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.AttemptStoredTransition": {
                "code_before_change": [],
                "code_after_change": []
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Buggy Method"
            },
            "fix_suggestion": "Preventive",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Buggy Method"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the root cause as invalid state transitions in the YARN framework, specifically mentioning the RMAppAttemptImpl.handle method, which is related to the issue but not the exact ground truth method where the fix was applied. This places it in the 'Buggy Method' sub-category for root cause identification. The fix suggestion is preventive as it suggests reviewing state transition logic and implementing error handling, which would mitigate the issue. The problem location identification is also partial and falls under 'Buggy Method' because it mentions RMAppAttemptImpl.handle, which is related but not the exact method where the fix was made. There is no wrong information in the bug report as all details are relevant to the context of the bug."
        }
    },
    {
        "filename": "YARN-345.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.transition": {
                "code_before_change": "    public void transition(ApplicationImpl app, ApplicationEvent event) {\n      ApplicationInitEvent initEvent = (ApplicationInitEvent)event;\n      app.applicationACLs = initEvent.getApplicationACLs();\n      app.aclsManager.addApplication(app.getAppId(), app.applicationACLs);\n      // Inform the logAggregator\n      app.dispatcher.getEventHandler().handle(\n          new LogHandlerAppStartedEvent(app.appId, app.user,\n              app.credentials, ContainerLogsRetentionPolicy.ALL_CONTAINERS,\n              app.applicationACLs)); \n    }",
                "code_after_change": "    public void transition(ApplicationImpl app, ApplicationEvent event) {\n      ApplicationInitEvent initEvent = (ApplicationInitEvent)event;\n      app.applicationACLs = initEvent.getApplicationACLs();\n      app.aclsManager.addApplication(app.getAppId(), app.applicationACLs);\n      // Inform the logAggregator\n      app.dispatcher.getEventHandler().handle(\n          new LogHandlerAppStartedEvent(app.appId, app.user,\n              app.credentials, ContainerLogsRetentionPolicy.ALL_CONTAINERS,\n              app.applicationACLs)); \n    }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService.initApp": {
                "code_before_change": "  private void initApp(final ApplicationId appId, String user,\n      Credentials credentials, ContainerLogsRetentionPolicy logRetentionPolicy,\n      Map<ApplicationAccessType, String> appAcls) {\n    ApplicationEvent eventResponse;\n    try {\n      initAppAggregator(appId, user, credentials, logRetentionPolicy, appAcls);\n      eventResponse = new ApplicationEvent(appId,\n          ApplicationEventType.APPLICATION_LOG_HANDLING_INITED);\n    } catch (YarnException e) {\n      eventResponse = new ApplicationFinishEvent(appId,\n          \"Application failed to init aggregation: \" + e.getMessage());\n    }\n    this.dispatcher.getEventHandler().handle(eventResponse);\n  }",
                "code_after_change": "  private void initApp(final ApplicationId appId, String user,\n      Credentials credentials, ContainerLogsRetentionPolicy logRetentionPolicy,\n      Map<ApplicationAccessType, String> appAcls) {\n    ApplicationEvent eventResponse;\n    try {\n      initAppAggregator(appId, user, credentials, logRetentionPolicy, appAcls);\n      eventResponse = new ApplicationEvent(appId,\n          ApplicationEventType.APPLICATION_LOG_HANDLING_INITED);\n    } catch (YarnException e) {\n      LOG.warn(\"Application failed to init aggregation: \" + e.getMessage());\n      eventResponse = new ApplicationEvent(appId,\n          ApplicationEventType.APPLICATION_LOG_HANDLING_FAILED);\n    }\n    this.dispatcher.getEventHandler().handle(eventResponse);\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.AppInitTransition": {
                "code_before_change": [],
                "code_after_change": []
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Same Class or Module"
            },
            "fix_suggestion": "Alternative Fix",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Same Class or Module"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the root cause as a misconfiguration in the state machine's transitions, particularly in the ApplicationImpl class, which is in the same class or module as the ground truth methods. However, it does not precisely identify the specific methods where the bug occurred. The fix suggestion involves reviewing state transitions and modifying the addTransition method, which is an alternative approach to resolving the issue. The problem location mentions ApplicationImpl.handle, which is in the same class as the ground truth methods but not the exact method. There is no wrong information in the bug report as all details are relevant to the context of the bug."
        }
    },
    {
        "filename": "YARN-3894.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.setChildQueues": {
                "code_before_change": "  synchronized void setChildQueues(Collection<CSQueue> childQueues) {\n    // Validate\n    float childCapacities = 0;\n    for (CSQueue queue : childQueues) {\n      childCapacities += queue.getCapacity();\n    }\n    float delta = Math.abs(1.0f - childCapacities);  // crude way to check\n    // allow capacities being set to 0, and enforce child 0 if parent is 0\n    if (((queueCapacities.getCapacity() > 0) && (delta > PRECISION)) || \n        ((queueCapacities.getCapacity() == 0) && (childCapacities > 0))) {\n      throw new IllegalArgumentException(\"Illegal\" +\n      \t\t\" capacity of \" + childCapacities + \n      \t\t\" for children of queue \" + queueName);\n    }\n    // check label capacities\n    for (String nodeLabel : labelManager.getClusterNodeLabelNames()) {\n      float capacityByLabel = queueCapacities.getCapacity(nodeLabel);\n      // check children's labels\n      float sum = 0;\n      for (CSQueue queue : childQueues) {\n        sum += queue.getQueueCapacities().getCapacity(nodeLabel);\n      }\n      if ((capacityByLabel > 0 && Math.abs(1.0f - sum) > PRECISION)\n          || (capacityByLabel == 0) && (sum > 0)) {\n        throw new IllegalArgumentException(\"Illegal\" + \" capacity of \"\n            + sum + \" for children of queue \" + queueName\n            + \" for label=\" + nodeLabel);\n      }\n    }\n    \n    this.childQueues.clear();\n    this.childQueues.addAll(childQueues);\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"setChildQueues: \" + getChildQueuesToPrint());\n    }\n  }",
                "code_after_change": "  synchronized void setChildQueues(Collection<CSQueue> childQueues) {\n    // Validate\n    float childCapacities = 0;\n    for (CSQueue queue : childQueues) {\n      childCapacities += queue.getCapacity();\n    }\n    float delta = Math.abs(1.0f - childCapacities);  // crude way to check\n    // allow capacities being set to 0, and enforce child 0 if parent is 0\n    if (((queueCapacities.getCapacity() > 0) && (delta > PRECISION)) || \n        ((queueCapacities.getCapacity() == 0) && (childCapacities > 0))) {\n      throw new IllegalArgumentException(\"Illegal\" +\n      \t\t\" capacity of \" + childCapacities + \n      \t\t\" for children of queue \" + queueName);\n    }\n    // check label capacities\n    for (String nodeLabel : queueCapacities.getExistingNodeLabels()) {\n      float capacityByLabel = queueCapacities.getCapacity(nodeLabel);\n      // check children's labels\n      float sum = 0;\n      for (CSQueue queue : childQueues) {\n        sum += queue.getQueueCapacities().getCapacity(nodeLabel);\n      }\n      if ((capacityByLabel > 0 && Math.abs(1.0f - sum) > PRECISION)\n          || (capacityByLabel == 0) && (sum > 0)) {\n        throw new IllegalArgumentException(\"Illegal\" + \" capacity of \"\n            + sum + \" for children of queue \" + queueName\n            + \" for label=\" + nodeLabel);\n      }\n    }\n    \n    this.childQueues.clear();\n    this.childQueues.addAll(childQueues);\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"setChildQueues: \" + getChildQueuesToPrint());\n    }\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Correct",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report precisely identifies the root cause as a misconfiguration in the YARN queue settings, specifically in the `setChildQueues` method of the `ParentQueue` class, which matches the ground truth method. The fix suggestion in the report advises reviewing and adjusting the queue configurations to ensure valid capacities, which aligns with the developer's fix that involves checking and validating the capacities. The problem location is also precisely identified, as the `problem_location` field mentions the `ParentQueue.setChildQueues` method, which is the ground truth method. There is no wrong information in the bug report; all details are consistent with the context of the bug."
        }
    },
    {
        "filename": "YARN-4347.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl.transition": {
                "code_before_change": "    public void transition(RMAppAttemptImpl appAttempt,\n        RMAppAttemptEvent event) {\n    }",
                "code_after_change": "    public void transition(RMAppAttemptImpl appAttempt,\n        RMAppAttemptEvent event) {\n    }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl.isAppInFinalState": {
                "code_before_change": "  public static boolean isAppInFinalState(RMApp rmApp) {\n    RMAppState appState = ((RMAppImpl) rmApp).getRecoveredFinalState();\n    if (appState == null) {\n      appState = rmApp.getState();\n    }\n    return appState == RMAppState.FAILED || appState == RMAppState.FINISHED\n        || appState == RMAppState.KILLED;\n  }",
                "code_after_change": "  public static boolean isAppInFinalState(RMApp rmApp) {\n    RMAppState appState = ((RMAppImpl) rmApp).getRecoveredFinalState();\n    if (appState == null) {\n      appState = rmApp.getState();\n    }\n    return appState == RMAppState.FAILED || appState == RMAppState.FINISHED\n        || appState == RMAppState.KILLED;\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Preventive",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the issue in the `addApplicationAttempt` method of the `CapacityScheduler` class, which is not the exact root cause but is part of the shared stack trace context with the ground truth methods. The fix suggestion involves adding checks to handle cases where applications are not found, which is preventive as it aims to mitigate the issue. The problem location is identified in the `addApplicationAttempt` and `addApplication` methods, which are part of the shared stack trace context but not the precise ground truth methods. There is no wrong information in the bug report as all details are relevant to the context of the bug."
        }
    },
    {
        "filename": "YARN-1692.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AppSchedulable.updateDemand": {
                "code_before_change": "  public void updateDemand() {\n    demand = Resources.createResource(0);\n    // Demand is current consumption plus outstanding requests\n    Resources.addTo(demand, app.getCurrentConsumption());\n\n    // Add up outstanding resource requests\n    for (Priority p : app.getPriorities()) {\n      for (ResourceRequest r : app.getResourceRequests(p).values()) {\n        Resource total = Resources.multiply(r.getCapability(), r.getNumContainers());\n        Resources.addTo(demand, total);\n      }\n    }\n  }",
                "code_after_change": "  public void updateDemand() {\n    demand = Resources.createResource(0);\n    // Demand is current consumption plus outstanding requests\n    Resources.addTo(demand, app.getCurrentConsumption());\n\n    // Add up outstanding resource requests\n    synchronized (app) {\n      for (Priority p : app.getPriorities()) {\n        for (ResourceRequest r : app.getResourceRequests(p).values()) {\n          Resource total = Resources.multiply(r.getCapability(), r.getNumContainers());\n          Resources.addTo(demand, total);\n        }\n      }\n    }\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp.getLocalityWaitFactor": {
                "code_before_change": "  public synchronized float getLocalityWaitFactor(\n      Priority priority, int clusterNodes) {\n    // Estimate: Required unique resources (i.e. hosts + racks)\n    int requiredResources = \n        Math.max(this.getResourceRequests(priority).size() - 1, 0);\n    \n    // waitFactor can't be more than '1' \n    // i.e. no point skipping more than clustersize opportunities\n    return Math.min(((float)requiredResources / clusterNodes), 1.0f);\n  }",
                "code_after_change": "  public synchronized float getLocalityWaitFactor(\n      Priority priority, int clusterNodes) {\n    // Estimate: Required unique resources (i.e. hosts + racks)\n    int requiredResources = \n        Math.max(this.getResourceRequests(priority).size() - 1, 0);\n    \n    // waitFactor can't be more than '1' \n    // i.e. no point skipping more than clustersize opportunities\n    return Math.min(((float)requiredResources / clusterNodes), 1.0f);\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt.getPriorities": {
                "code_before_change": "  public Collection<Priority> getPriorities() {\n    return appSchedulingInfo.getPriorities();\n  }",
                "code_after_change": "  public Collection<Priority> getPriorities() {\n    return appSchedulingInfo.getPriorities();\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt.getTotalRequiredResources": {
                "code_before_change": "  public synchronized int getTotalRequiredResources(Priority priority) {\n    return getResourceRequest(priority, ResourceRequest.ANY).getNumContainers();\n  }",
                "code_after_change": "  public synchronized int getTotalRequiredResources(Priority priority) {\n    return getResourceRequest(priority, ResourceRequest.ANY).getNumContainers();\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Correct",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report precisely identifies the root cause as the lack of synchronization in the `updateDemand` method of the `AppSchedulable` class, which matches the ground truth method. The fix suggestion to use a synchronized block or a `ConcurrentHashMap` is correct and aligns with the developer's fix, which involved adding a synchronized block. The problem location is precisely identified as it mentions the `updateDemand` method in `AppSchedulable`, which is part of the ground truth methods. There is no wrong information in the bug report as all details are consistent with the context of the bug."
        }
    },
    {
        "filename": "YARN-7697.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController.renderAggregatedLogsBlock": {
                "code_before_change": "  public void renderAggregatedLogsBlock(Block html, ViewContext context) {\n    IndexedFileAggregatedLogsBlock block = new IndexedFileAggregatedLogsBlock(\n        context, this.conf, this);\n    block.render(html);\n  }",
                "code_after_change": "  public void renderAggregatedLogsBlock(Block html, ViewContext context) {\n    IndexedFileAggregatedLogsBlock block = new IndexedFileAggregatedLogsBlock(\n        context, this.conf, this);\n    block.render(html);\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController.getRemoteAppLogDir": {
                "code_before_change": "  public Path getRemoteAppLogDir(ApplicationId appId, String user)\n      throws IOException {\n    return LogAggregationUtils.getRemoteAppLogDir(conf, appId, user,\n        this.remoteRootLogDir, this.remoteRootLogDirSuffix);\n  }",
                "code_after_change": "  public Path getRemoteAppLogDir(ApplicationId appId, String user)\n      throws IOException {\n    return LogAggregationUtils.getRemoteAppLogDir(conf, appId, user,\n        this.remoteRootLogDir, this.remoteRootLogDirSuffix);\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController.initializeWriterInRolling": {
                "code_before_change": "  private Path initializeWriterInRolling(final Path remoteLogFile,\n      final ApplicationId appId, final String nodeId) throws Exception {\n    Path aggregatedLogFile = null;\n    // check uuid\n    // if we can not find uuid, we would load the uuid\n    // from previous aggregated log files, and at the same\n    // time, we would delete any aggregated log files which\n    // has invalid uuid.\n    if (uuid == null) {\n      uuid = loadUUIDFromLogFile(fc, remoteLogFile.getParent(),\n            appId, nodeId);\n    }\n    Path currentRemoteLogFile = getCurrentRemoteLogFile(\n        fc, remoteLogFile.getParent(), nodeId);\n    // check checksum file\n    boolean overwriteCheckSum = true;\n    remoteLogCheckSumFile = new Path(remoteLogFile.getParent(),\n        (remoteLogFile.getName() + CHECK_SUM_FILE_SUFFIX));\n    if(fc.util().exists(remoteLogCheckSumFile)) {\n      // if the checksum file exists, we should reset cached\n      // indexedLogsMeta.\n      indexedLogsMeta.getLogMetas().clear();\n      if (currentRemoteLogFile != null) {\n        FSDataInputStream checksumFileInputStream = null;\n        try {\n          checksumFileInputStream = fc.open(remoteLogCheckSumFile);\n          int nameLength = checksumFileInputStream.readInt();\n          byte[] b = new byte[nameLength];\n          int actualLength = checksumFileInputStream.read(b);\n          if (actualLength == nameLength) {\n            String recoveredLogFile = new String(\n                b, Charset.forName(\"UTF-8\"));\n            if (recoveredLogFile.equals(\n                currentRemoteLogFile.getName())) {\n              overwriteCheckSum = false;\n              long endIndex = checksumFileInputStream.readLong();\n              IndexedLogsMeta recoveredLogsMeta = null;\n              try {\n                truncateFileWithRetries(fc, currentRemoteLogFile,\n                    endIndex);\n                recoveredLogsMeta = loadIndexedLogsMeta(\n                    currentRemoteLogFile);\n              } catch (Exception ex) {\n                recoveredLogsMeta = loadIndexedLogsMeta(\n                    currentRemoteLogFile, endIndex);\n              }\n              if (recoveredLogsMeta != null) {\n                indexedLogsMeta = recoveredLogsMeta;\n              }\n            }\n          }\n        } finally {\n          IOUtils.cleanupWithLogger(LOG, checksumFileInputStream);\n        }\n      }\n    }\n    // check whether we need roll over old logs\n    if (currentRemoteLogFile == null || isRollover(\n        fc, currentRemoteLogFile)) {\n      indexedLogsMeta.getLogMetas().clear();\n      overwriteCheckSum = true;\n      aggregatedLogFile = new Path(remoteLogFile.getParent(),\n          remoteLogFile.getName() + \"_\" + sysClock.getTime());\n      fsDataOStream = fc.create(aggregatedLogFile,\n          EnumSet.of(CreateFlag.CREATE, CreateFlag.OVERWRITE),\n          new Options.CreateOpts[] {});\n      // writes the uuid\n      fsDataOStream.write(uuid);\n      fsDataOStream.flush();\n    } else {\n      aggregatedLogFile = currentRemoteLogFile;\n      fsDataOStream = fc.create(currentRemoteLogFile,\n          EnumSet.of(CreateFlag.CREATE, CreateFlag.APPEND),\n          new Options.CreateOpts[] {});\n    }\n    // recreate checksum file if needed before aggregate the logs\n    if (overwriteCheckSum) {\n      final long currentAggregatedLogFileLength = fc\n          .getFileStatus(aggregatedLogFile).getLen();\n      FSDataOutputStream checksumFileOutputStream = null;\n      try {\n        checksumFileOutputStream = fc.create(remoteLogCheckSumFile,\n            EnumSet.of(CreateFlag.CREATE, CreateFlag.OVERWRITE),\n            new Options.CreateOpts[] {});\n        String fileName = aggregatedLogFile.getName();\n        checksumFileOutputStream.writeInt(fileName.length());\n        checksumFileOutputStream.write(fileName.getBytes(\n            Charset.forName(\"UTF-8\")));\n        checksumFileOutputStream.writeLong(\n            currentAggregatedLogFileLength);\n        checksumFileOutputStream.flush();\n      } finally {\n        IOUtils.cleanupWithLogger(LOG, checksumFileOutputStream);\n      }\n    }\n    return aggregatedLogFile;\n  }",
                "code_after_change": "  private Path initializeWriterInRolling(final Path remoteLogFile,\n      final ApplicationId appId, final String nodeId) throws Exception {\n    Path aggregatedLogFile = null;\n    // check uuid\n    // if we can not find uuid, we would load the uuid\n    // from previous aggregated log files, and at the same\n    // time, we would delete any aggregated log files which\n    // has invalid uuid.\n    if (uuid == null) {\n      uuid = loadUUIDFromLogFile(fc, remoteLogFile.getParent(),\n            appId, nodeId);\n    }\n    Path currentRemoteLogFile = getCurrentRemoteLogFile(\n        fc, remoteLogFile.getParent(), nodeId);\n    // check checksum file\n    boolean overwriteCheckSum = true;\n    remoteLogCheckSumFile = new Path(remoteLogFile.getParent(),\n        (remoteLogFile.getName() + CHECK_SUM_FILE_SUFFIX));\n    if(fc.util().exists(remoteLogCheckSumFile)) {\n      // if the checksum file exists, we should reset cached\n      // indexedLogsMeta.\n      indexedLogsMeta.getLogMetas().clear();\n      if (currentRemoteLogFile != null) {\n        FSDataInputStream checksumFileInputStream = null;\n        try {\n          checksumFileInputStream = fc.open(remoteLogCheckSumFile);\n          int nameLength = checksumFileInputStream.readInt();\n          byte[] b = new byte[nameLength];\n          int actualLength = checksumFileInputStream.read(b);\n          if (actualLength == nameLength) {\n            String recoveredLogFile = new String(\n                b, Charset.forName(\"UTF-8\"));\n            if (recoveredLogFile.equals(\n                currentRemoteLogFile.getName())) {\n              overwriteCheckSum = false;\n              long endIndex = checksumFileInputStream.readLong();\n              IndexedLogsMeta recoveredLogsMeta = loadIndexedLogsMeta(\n                  currentRemoteLogFile, endIndex, appId);\n              if (recoveredLogsMeta != null) {\n                indexedLogsMeta = recoveredLogsMeta;\n              }\n            }\n          }\n        } finally {\n          IOUtils.cleanupWithLogger(LOG, checksumFileInputStream);\n        }\n      }\n    }\n    // check whether we need roll over old logs\n    if (currentRemoteLogFile == null || isRollover(\n        fc, currentRemoteLogFile)) {\n      indexedLogsMeta.getLogMetas().clear();\n      overwriteCheckSum = true;\n      aggregatedLogFile = new Path(remoteLogFile.getParent(),\n          remoteLogFile.getName() + \"_\" + sysClock.getTime());\n      fsDataOStream = fc.create(aggregatedLogFile,\n          EnumSet.of(CreateFlag.CREATE, CreateFlag.OVERWRITE),\n          new Options.CreateOpts[] {});\n      // writes the uuid\n      fsDataOStream.write(uuid);\n      fsDataOStream.flush();\n    } else {\n      aggregatedLogFile = currentRemoteLogFile;\n      fsDataOStream = fc.create(currentRemoteLogFile,\n          EnumSet.of(CreateFlag.CREATE, CreateFlag.APPEND),\n          new Options.CreateOpts[] {});\n    }\n    // recreate checksum file if needed before aggregate the logs\n    if (overwriteCheckSum) {\n      final long currentAggregatedLogFileLength = fc\n          .getFileStatus(aggregatedLogFile).getLen();\n      FSDataOutputStream checksumFileOutputStream = null;\n      try {\n        checksumFileOutputStream = fc.create(remoteLogCheckSumFile,\n            EnumSet.of(CreateFlag.CREATE, CreateFlag.OVERWRITE),\n            new Options.CreateOpts[] {});\n        String fileName = aggregatedLogFile.getName();\n        checksumFileOutputStream.writeInt(fileName.length());\n        checksumFileOutputStream.write(fileName.getBytes(\n            Charset.forName(\"UTF-8\")));\n        checksumFileOutputStream.writeLong(\n            currentAggregatedLogFileLength);\n        checksumFileOutputStream.flush();\n      } finally {\n        IOUtils.cleanupWithLogger(LOG, checksumFileOutputStream);\n      }\n    }\n    return aggregatedLogFile;\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController.loadIndexedLogsMeta": {
                "code_before_change": "  public IndexedLogsMeta loadIndexedLogsMeta(Path remoteLogPath, long end)\n      throws IOException {\n    FileContext fileContext =\n        FileContext.getFileContext(remoteLogPath.toUri(), conf);\n    FSDataInputStream fsDataIStream = null;\n    try {\n      fsDataIStream = fileContext.open(remoteLogPath);\n      if (end == 0) {\n        return null;\n      }\n      long fileLength = end < 0 ? fileContext.getFileStatus(\n          remoteLogPath).getLen() : end;\n      fsDataIStream.seek(fileLength - Integer.SIZE/ Byte.SIZE - UUID_LENGTH);\n      int offset = fsDataIStream.readInt();\n      byte[] array = new byte[offset];\n      fsDataIStream.seek(\n          fileLength - offset - Integer.SIZE/ Byte.SIZE - UUID_LENGTH);\n      int actual = fsDataIStream.read(array);\n      if (actual != offset) {\n        throw new IOException(\"Error on loading log meta from \"\n            + remoteLogPath);\n      }\n      return (IndexedLogsMeta)SerializationUtils\n          .deserialize(array);\n    } finally {\n      IOUtils.cleanupWithLogger(LOG, fsDataIStream);\n    }\n  }",
                "code_after_change": "  public IndexedLogsMeta loadIndexedLogsMeta(Path remoteLogPath, long end,\n      ApplicationId appId) throws IOException {\n    FileContext fileContext =\n        FileContext.getFileContext(remoteLogPath.toUri(), conf);\n    FSDataInputStream fsDataIStream = null;\n    try {\n      fsDataIStream = fileContext.open(remoteLogPath);\n      if (end == 0) {\n        return null;\n      }\n      long fileLength = end < 0 ? fileContext.getFileStatus(\n          remoteLogPath).getLen() : end;\n\n      fsDataIStream.seek(fileLength - Integer.SIZE/ Byte.SIZE - UUID_LENGTH);\n      int offset = fsDataIStream.readInt();\n      // If the offset/log meta size is larger than 64M,\n      // output a warn message for better debug.\n      if (offset > 64 * 1024 * 1024) {\n        LOG.warn(\"The log meta size read from \" + remoteLogPath\n            + \" is \" + offset);\n      }\n\n      // Load UUID and make sure the UUID is correct.\n      byte[] uuidRead = new byte[UUID_LENGTH];\n      int uuidReadLen = fsDataIStream.read(uuidRead);\n      if (this.uuid == null) {\n        this.uuid = createUUID(appId);\n      }\n      if (uuidReadLen != UUID_LENGTH || !Arrays.equals(this.uuid, uuidRead)) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"the length of loaded UUID:\" + uuidReadLen);\n          LOG.debug(\"the loaded UUID:\" + new String(uuidRead,\n              Charset.forName(\"UTF-8\")));\n          LOG.debug(\"the expected UUID:\" + new String(this.uuid,\n              Charset.forName(\"UTF-8\")));\n        }\n        throw new IOException(\"The UUID from \"\n            + remoteLogPath + \" is not correct. The offset of loaded UUID is \"\n            + (fileLength - UUID_LENGTH));\n      }\n\n      // Load Log Meta\n      byte[] array = new byte[offset];\n      fsDataIStream.seek(\n          fileLength - offset - Integer.SIZE/ Byte.SIZE - UUID_LENGTH);\n      int actual = fsDataIStream.read(array);\n      if (actual != offset) {\n        throw new IOException(\"Error on loading log meta from \"\n            + remoteLogPath);\n      }\n      return (IndexedLogsMeta)SerializationUtils\n          .deserialize(array);\n    } finally {\n      IOUtils.cleanupWithLogger(LOG, fsDataIStream);\n    }\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController.readAggregatedLogs": {
                "code_before_change": "  public boolean readAggregatedLogs(ContainerLogsRequest logRequest,\n      OutputStream os) throws IOException {\n    boolean findLogs = false;\n    boolean createPrintStream = (os == null);\n    ApplicationId appId = logRequest.getAppId();\n    String nodeId = logRequest.getNodeId();\n    String nodeIdStr = (nodeId == null || nodeId.isEmpty()) ? null\n        : LogAggregationUtils.getNodeString(nodeId);\n    List<String> logTypes = new ArrayList<>();\n    if (logRequest.getLogTypes() != null && !logRequest\n        .getLogTypes().isEmpty()) {\n      logTypes.addAll(logRequest.getLogTypes());\n    }\n    String containerIdStr = logRequest.getContainerId();\n    boolean getAllContainers = (containerIdStr == null\n        || containerIdStr.isEmpty());\n    long size = logRequest.getBytes();\n    List<FileStatus> nodeFiles = LogAggregationUtils\n        .getRemoteNodeFileList(conf, appId, logRequest.getAppOwner(),\n        this.remoteRootLogDir, this.remoteRootLogDirSuffix);\n    if (nodeFiles.isEmpty()) {\n      throw new IOException(\"There is no available log fils for \"\n          + \"application:\" + appId);\n    }\n    Map<String, Long> checkSumFiles = parseCheckSumFiles(nodeFiles);\n    List<FileStatus> fileToRead = getNodeLogFileToRead(\n        nodeFiles, nodeIdStr, appId);\n    byte[] buf = new byte[65535];\n    for (FileStatus thisNodeFile : fileToRead) {\n      String nodeName = thisNodeFile.getPath().getName();\n      Long checkSumIndex = checkSumFiles.get(nodeName);\n      long endIndex = -1;\n      if (checkSumIndex != null) {\n        endIndex = checkSumIndex.longValue();\n      }\n      IndexedLogsMeta indexedLogsMeta = null;\n      try {\n        indexedLogsMeta = loadIndexedLogsMeta(thisNodeFile.getPath(),\n            endIndex);\n      } catch (Exception ex) {\n        // DO NOTHING\n        LOG.warn(\"Can not load log meta from the log file:\"\n            + thisNodeFile.getPath());\n        continue;\n      }\n      if (indexedLogsMeta == null) {\n        continue;\n      }\n      String compressAlgo = indexedLogsMeta.getCompressName();\n      List<IndexedFileLogMeta> candidates = new ArrayList<>();\n      for (IndexedPerAggregationLogMeta logMeta\n          : indexedLogsMeta.getLogMetas()) {\n        for (Entry<String, List<IndexedFileLogMeta>> meta\n            : logMeta.getLogMetas().entrySet()) {\n          for (IndexedFileLogMeta log : meta.getValue()) {\n            if (!getAllContainers && !log.getContainerId()\n                .equals(containerIdStr)) {\n              continue;\n            }\n            if (logTypes != null && !logTypes.isEmpty() &&\n                !logTypes.contains(log.getFileName())) {\n              continue;\n            }\n            candidates.add(log);\n          }\n        }\n      }\n      if (candidates.isEmpty()) {\n        continue;\n      }\n\n      Algorithm compressName = Compression.getCompressionAlgorithmByName(\n          compressAlgo);\n      Decompressor decompressor = compressName.getDecompressor();\n      FileContext fileContext = FileContext.getFileContext(\n          thisNodeFile.getPath().toUri(), conf);\n      FSDataInputStream fsin = fileContext.open(thisNodeFile.getPath());\n      String currentContainer = \"\";\n      for (IndexedFileLogMeta candidate : candidates) {\n        if (!candidate.getContainerId().equals(currentContainer)) {\n          if (createPrintStream) {\n            closePrintStream(os);\n            os = LogToolUtils.createPrintStream(\n                logRequest.getOutputLocalDir(),\n                thisNodeFile.getPath().getName(),\n                candidate.getContainerId());\n            currentContainer = candidate.getContainerId();\n          }\n        }\n        InputStream in = null;\n        try {\n          in = compressName.createDecompressionStream(\n              new BoundedRangeFileInputStream(fsin,\n                  candidate.getStartIndex(),\n                  candidate.getFileCompressedSize()),\n              decompressor, getFSInputBufferSize(conf));\n          LogToolUtils.outputContainerLog(candidate.getContainerId(),\n              nodeName, candidate.getFileName(), candidate.getFileSize(), size,\n              Times.format(candidate.getLastModificatedTime()),\n              in, os, buf, ContainerLogAggregationType.AGGREGATED);\n          byte[] b = aggregatedLogSuffix(candidate.getFileName())\n              .getBytes(Charset.forName(\"UTF-8\"));\n          os.write(b, 0, b.length);\n          findLogs = true;\n        } catch (IOException e) {\n          System.err.println(e.getMessage());\n          compressName.returnDecompressor(decompressor);\n          continue;\n        } finally {\n          os.flush();\n          IOUtils.cleanupWithLogger(LOG, in);\n        }\n      }\n    }\n    return findLogs;\n  }",
                "code_after_change": "  public boolean readAggregatedLogs(ContainerLogsRequest logRequest,\n      OutputStream os) throws IOException {\n    boolean findLogs = false;\n    boolean createPrintStream = (os == null);\n    ApplicationId appId = logRequest.getAppId();\n    String nodeId = logRequest.getNodeId();\n    String nodeIdStr = (nodeId == null || nodeId.isEmpty()) ? null\n        : LogAggregationUtils.getNodeString(nodeId);\n    List<String> logTypes = new ArrayList<>();\n    if (logRequest.getLogTypes() != null && !logRequest\n        .getLogTypes().isEmpty()) {\n      logTypes.addAll(logRequest.getLogTypes());\n    }\n    String containerIdStr = logRequest.getContainerId();\n    boolean getAllContainers = (containerIdStr == null\n        || containerIdStr.isEmpty());\n    long size = logRequest.getBytes();\n    List<FileStatus> nodeFiles = LogAggregationUtils\n        .getRemoteNodeFileList(conf, appId, logRequest.getAppOwner(),\n        this.remoteRootLogDir, this.remoteRootLogDirSuffix);\n    if (nodeFiles.isEmpty()) {\n      throw new IOException(\"There is no available log fils for \"\n          + \"application:\" + appId);\n    }\n    Map<String, Long> checkSumFiles = parseCheckSumFiles(nodeFiles);\n    List<FileStatus> fileToRead = getNodeLogFileToRead(\n        nodeFiles, nodeIdStr, appId);\n    byte[] buf = new byte[65535];\n    for (FileStatus thisNodeFile : fileToRead) {\n      String nodeName = thisNodeFile.getPath().getName();\n      Long checkSumIndex = checkSumFiles.get(nodeName);\n      long endIndex = -1;\n      if (checkSumIndex != null) {\n        endIndex = checkSumIndex.longValue();\n      }\n      IndexedLogsMeta indexedLogsMeta = null;\n      try {\n        indexedLogsMeta = loadIndexedLogsMeta(thisNodeFile.getPath(),\n            endIndex, appId);\n      } catch (Exception ex) {\n        // DO NOTHING\n        LOG.warn(\"Can not load log meta from the log file:\"\n            + thisNodeFile.getPath() + \"\\n\" + ex.getMessage());\n        continue;\n      }\n      if (indexedLogsMeta == null) {\n        continue;\n      }\n      String compressAlgo = indexedLogsMeta.getCompressName();\n      List<IndexedFileLogMeta> candidates = new ArrayList<>();\n      for (IndexedPerAggregationLogMeta logMeta\n          : indexedLogsMeta.getLogMetas()) {\n        for (Entry<String, List<IndexedFileLogMeta>> meta\n            : logMeta.getLogMetas().entrySet()) {\n          for (IndexedFileLogMeta log : meta.getValue()) {\n            if (!getAllContainers && !log.getContainerId()\n                .equals(containerIdStr)) {\n              continue;\n            }\n            if (logTypes != null && !logTypes.isEmpty() &&\n                !logTypes.contains(log.getFileName())) {\n              continue;\n            }\n            candidates.add(log);\n          }\n        }\n      }\n      if (candidates.isEmpty()) {\n        continue;\n      }\n\n      Algorithm compressName = Compression.getCompressionAlgorithmByName(\n          compressAlgo);\n      Decompressor decompressor = compressName.getDecompressor();\n      FileContext fileContext = FileContext.getFileContext(\n          thisNodeFile.getPath().toUri(), conf);\n      FSDataInputStream fsin = fileContext.open(thisNodeFile.getPath());\n      String currentContainer = \"\";\n      for (IndexedFileLogMeta candidate : candidates) {\n        if (!candidate.getContainerId().equals(currentContainer)) {\n          if (createPrintStream) {\n            closePrintStream(os);\n            os = LogToolUtils.createPrintStream(\n                logRequest.getOutputLocalDir(),\n                thisNodeFile.getPath().getName(),\n                candidate.getContainerId());\n            currentContainer = candidate.getContainerId();\n          }\n        }\n        InputStream in = null;\n        try {\n          in = compressName.createDecompressionStream(\n              new BoundedRangeFileInputStream(fsin,\n                  candidate.getStartIndex(),\n                  candidate.getFileCompressedSize()),\n              decompressor, getFSInputBufferSize(conf));\n          LogToolUtils.outputContainerLog(candidate.getContainerId(),\n              nodeName, candidate.getFileName(), candidate.getFileSize(), size,\n              Times.format(candidate.getLastModificatedTime()),\n              in, os, buf, ContainerLogAggregationType.AGGREGATED);\n          byte[] b = aggregatedLogSuffix(candidate.getFileName())\n              .getBytes(Charset.forName(\"UTF-8\"));\n          os.write(b, 0, b.length);\n          findLogs = true;\n        } catch (IOException e) {\n          System.err.println(e.getMessage());\n          compressName.returnDecompressor(decompressor);\n          continue;\n        } finally {\n          os.flush();\n          IOUtils.cleanupWithLogger(LOG, in);\n        }\n      }\n    }\n    return findLogs;\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController.apply": {
                "code_before_change": "          public boolean apply(FileStatus next) {\n            return next.getPath().getName().endsWith(\n                CHECK_SUM_FILE_SUFFIX);\n          }",
                "code_after_change": "          public boolean apply(FileStatus next) {\n            return next.getPath().getName().endsWith(\n                CHECK_SUM_FILE_SUFFIX);\n          }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.IndexedFileAggregatedLogsBlock.render": {
                "code_before_change": "  protected void render(Block html) {\n    BlockParameters params = verifyAndParseParameters(html);\n    if (params == null) {\n      return;\n    }\n\n    ApplicationId appId = params.getAppId();\n    ContainerId containerId = params.getContainerId();\n    NodeId nodeId = params.getNodeId();\n    String appOwner = params.getAppOwner();\n    String logEntity = params.getLogEntity();\n    long start = params.getStartIndex();\n    long end = params.getEndIndex();\n    long startTime = params.getStartTime();\n    long endTime = params.getEndTime();\n\n    List<FileStatus> nodeFiles = null;\n    try {\n      nodeFiles = LogAggregationUtils\n          .getRemoteNodeFileList(conf, appId, appOwner,\n              this.fileController.getRemoteRootLogDir(),\n              this.fileController.getRemoteRootLogDirSuffix());\n    } catch(Exception ex) {\n      html.h1(\"Unable to locate any logs for container \"\n          + containerId.toString());\n      LOG.error(ex.getMessage());\n      return;\n    }\n\n    Map<String, Long> checkSumFiles;\n    try {\n      checkSumFiles = fileController.parseCheckSumFiles(nodeFiles);\n    } catch (IOException ex) {\n      LOG.error(\"Error getting logs for \" + logEntity, ex);\n      html.h1(\"Error getting logs for \" + logEntity);\n      return;\n    }\n\n    List<FileStatus> fileToRead;\n    try {\n      fileToRead = fileController.getNodeLogFileToRead(nodeFiles,\n          nodeId.toString(), appId);\n    } catch (IOException ex) {\n      LOG.error(\"Error getting logs for \" + logEntity, ex);\n      html.h1(\"Error getting logs for \" + logEntity);\n      return;\n    }\n\n    boolean foundLog = false;\n    String desiredLogType = $(CONTAINER_LOG_TYPE);\n    try {\n      for (FileStatus thisNodeFile : fileToRead) {\n        Long checkSumIndex = checkSumFiles.get(\n            thisNodeFile.getPath().getName());\n        long endIndex = -1;\n        if (checkSumIndex != null) {\n          endIndex = checkSumIndex.longValue();\n        }\n        IndexedLogsMeta indexedLogsMeta = null;\n        try {\n          indexedLogsMeta = fileController.loadIndexedLogsMeta(\n              thisNodeFile.getPath(), endIndex);\n        } catch (Exception ex) {\n          // DO NOTHING\n          LOG.warn(\"Can not load log meta from the log file:\"\n              + thisNodeFile.getPath());\n          continue;\n        }\n        if (indexedLogsMeta == null) {\n          continue;\n        }\n        Map<ApplicationAccessType, String> appAcls = indexedLogsMeta.getAcls();\n        String user = indexedLogsMeta.getUser();\n        String remoteUser = request().getRemoteUser();\n        if (!checkAcls(conf, appId, user, appAcls, remoteUser)) {\n          html.h1().__(\"User [\" + remoteUser\n              + \"] is not authorized to view the logs for \" + logEntity\n              + \" in log file [\" + thisNodeFile.getPath().getName() + \"]\")\n              .__();\n          LOG.error(\"User [\" + remoteUser\n              + \"] is not authorized to view the logs for \" + logEntity);\n          continue;\n        }\n        String compressAlgo = indexedLogsMeta.getCompressName();\n        List<IndexedFileLogMeta> candidates = new ArrayList<>();\n        for (IndexedPerAggregationLogMeta logMeta\n            : indexedLogsMeta.getLogMetas()) {\n          for (Entry<String, List<IndexedFileLogMeta>> meta\n              : logMeta.getLogMetas().entrySet()) {\n            for (IndexedFileLogMeta log : meta.getValue()) {\n              if (!log.getContainerId().equals(containerId.toString())) {\n                continue;\n              }\n              if (desiredLogType != null && !desiredLogType.isEmpty()\n                  && !desiredLogType.equals(log.getFileName())) {\n                continue;\n              }\n              candidates.add(log);\n            }\n          }\n        }\n        if (candidates.isEmpty()) {\n          continue;\n        }\n\n        Algorithm compressName = Compression.getCompressionAlgorithmByName(\n            compressAlgo);\n        Decompressor decompressor = compressName.getDecompressor();\n        FileContext fileContext = FileContext.getFileContext(\n            thisNodeFile.getPath().toUri(), conf);\n        FSDataInputStream fsin = fileContext.open(thisNodeFile.getPath());\n        int bufferSize = 65536;\n        for (IndexedFileLogMeta candidate : candidates) {\n          if (candidate.getLastModificatedTime() < startTime\n              || candidate.getLastModificatedTime() > endTime) {\n            continue;\n          }\n          byte[] cbuf = new byte[bufferSize];\n          InputStream in = null;\n          try {\n            in = compressName.createDecompressionStream(\n                new BoundedRangeFileInputStream(fsin,\n                    candidate.getStartIndex(),\n                    candidate.getFileCompressedSize()),\n                    decompressor,\n                    LogAggregationIndexedFileController.getFSInputBufferSize(\n                        conf));\n            long logLength = candidate.getFileSize();\n            html.pre().__(\"\\n\\n\").__();\n            html.p().__(\"Log Type: \" + candidate.getFileName()).__();\n            html.p().__(\"Log Upload Time: \" + Times.format(\n                candidate.getLastModificatedTime())).__();\n            html.p().__(\"Log Length: \" + Long.toString(\n                logLength)).__();\n            long startIndex = start < 0\n                ? logLength + start : start;\n            startIndex = startIndex < 0 ? 0 : startIndex;\n            startIndex = startIndex > logLength ? logLength : startIndex;\n            long endLogIndex = end < 0\n                ? logLength + end : end;\n            endLogIndex = endLogIndex < 0 ? 0 : endLogIndex;\n            endLogIndex = endLogIndex > logLength ? logLength : endLogIndex;\n            endLogIndex = endLogIndex < startIndex ?\n                startIndex : endLogIndex;\n            long toRead = endLogIndex - startIndex;\n            if (toRead < logLength) {\n              html.p().__(\"Showing \" + toRead + \" bytes of \" + logLength\n                  + \" total. Click \").a(url(\"logs\", $(NM_NODENAME),\n                      $(CONTAINER_ID), $(ENTITY_STRING), $(APP_OWNER),\n                      candidate.getFileName(), \"?start=0&start.time=\"\n                      + startTime + \"&end.time=\" + endTime), \"here\").\n                      __(\" for the full log.\").__();\n            }\n            long totalSkipped = 0;\n            while (totalSkipped < startIndex) {\n              long ret = in.skip(startIndex - totalSkipped);\n              if (ret == 0) {\n                //Read one byte\n                int nextByte = in.read();\n                // Check if we have reached EOF\n                if (nextByte == -1) {\n                  throw new IOException(\"Premature EOF from container log\");\n                }\n                ret = 1;\n              }\n              totalSkipped += ret;\n            }\n            int len = 0;\n            int currentToRead = toRead > bufferSize ? bufferSize : (int) toRead;\n            PRE<Hamlet> pre = html.pre();\n\n            while (toRead > 0\n                && (len = in.read(cbuf, 0, currentToRead)) > 0) {\n              pre.__(new String(cbuf, 0, len, Charset.forName(\"UTF-8\")));\n              toRead = toRead - len;\n              currentToRead = toRead > bufferSize ? bufferSize : (int) toRead;\n            }\n\n            pre.__();\n            foundLog = true;\n          } catch (Exception ex) {\n            LOG.error(\"Error getting logs for \" + logEntity, ex);\n            continue;\n          } finally {\n            IOUtils.closeQuietly(in);\n          }\n        }\n      }\n      if (!foundLog) {\n        if (desiredLogType.isEmpty()) {\n          html.h1(\"No logs available for container \" + containerId.toString());\n        } else {\n          html.h1(\"Unable to locate '\" + desiredLogType\n              + \"' log for container \" + containerId.toString());\n        }\n      }\n    } catch (RuntimeException e) {\n      throw e;\n    } catch (Exception ex) {\n      html.h1().__(\"Error getting logs for \" + logEntity).__();\n      LOG.error(\"Error getting logs for \" + logEntity, ex);\n    }\n  }",
                "code_after_change": "  protected void render(Block html) {\n    BlockParameters params = verifyAndParseParameters(html);\n    if (params == null) {\n      return;\n    }\n\n    ApplicationId appId = params.getAppId();\n    ContainerId containerId = params.getContainerId();\n    NodeId nodeId = params.getNodeId();\n    String appOwner = params.getAppOwner();\n    String logEntity = params.getLogEntity();\n    long start = params.getStartIndex();\n    long end = params.getEndIndex();\n    long startTime = params.getStartTime();\n    long endTime = params.getEndTime();\n\n    List<FileStatus> nodeFiles = null;\n    try {\n      nodeFiles = LogAggregationUtils\n          .getRemoteNodeFileList(conf, appId, appOwner,\n              this.fileController.getRemoteRootLogDir(),\n              this.fileController.getRemoteRootLogDirSuffix());\n    } catch(Exception ex) {\n      html.h1(\"Unable to locate any logs for container \"\n          + containerId.toString());\n      LOG.error(ex.getMessage());\n      return;\n    }\n\n    Map<String, Long> checkSumFiles;\n    try {\n      checkSumFiles = fileController.parseCheckSumFiles(nodeFiles);\n    } catch (IOException ex) {\n      LOG.error(\"Error getting logs for \" + logEntity, ex);\n      html.h1(\"Error getting logs for \" + logEntity);\n      return;\n    }\n\n    List<FileStatus> fileToRead;\n    try {\n      fileToRead = fileController.getNodeLogFileToRead(nodeFiles,\n          nodeId.toString(), appId);\n    } catch (IOException ex) {\n      LOG.error(\"Error getting logs for \" + logEntity, ex);\n      html.h1(\"Error getting logs for \" + logEntity);\n      return;\n    }\n\n    boolean foundLog = false;\n    String desiredLogType = $(CONTAINER_LOG_TYPE);\n    try {\n      for (FileStatus thisNodeFile : fileToRead) {\n        Long checkSumIndex = checkSumFiles.get(\n            thisNodeFile.getPath().getName());\n        long endIndex = -1;\n        if (checkSumIndex != null) {\n          endIndex = checkSumIndex.longValue();\n        }\n        IndexedLogsMeta indexedLogsMeta = null;\n        try {\n          indexedLogsMeta = fileController.loadIndexedLogsMeta(\n              thisNodeFile.getPath(), endIndex, appId);\n        } catch (Exception ex) {\n          // DO NOTHING\n          LOG.warn(\"Can not load log meta from the log file:\"\n              + thisNodeFile.getPath());\n          continue;\n        }\n        if (indexedLogsMeta == null) {\n          continue;\n        }\n        Map<ApplicationAccessType, String> appAcls = indexedLogsMeta.getAcls();\n        String user = indexedLogsMeta.getUser();\n        String remoteUser = request().getRemoteUser();\n        if (!checkAcls(conf, appId, user, appAcls, remoteUser)) {\n          html.h1().__(\"User [\" + remoteUser\n              + \"] is not authorized to view the logs for \" + logEntity\n              + \" in log file [\" + thisNodeFile.getPath().getName() + \"]\")\n              .__();\n          LOG.error(\"User [\" + remoteUser\n              + \"] is not authorized to view the logs for \" + logEntity);\n          continue;\n        }\n        String compressAlgo = indexedLogsMeta.getCompressName();\n        List<IndexedFileLogMeta> candidates = new ArrayList<>();\n        for (IndexedPerAggregationLogMeta logMeta\n            : indexedLogsMeta.getLogMetas()) {\n          for (Entry<String, List<IndexedFileLogMeta>> meta\n              : logMeta.getLogMetas().entrySet()) {\n            for (IndexedFileLogMeta log : meta.getValue()) {\n              if (!log.getContainerId().equals(containerId.toString())) {\n                continue;\n              }\n              if (desiredLogType != null && !desiredLogType.isEmpty()\n                  && !desiredLogType.equals(log.getFileName())) {\n                continue;\n              }\n              candidates.add(log);\n            }\n          }\n        }\n        if (candidates.isEmpty()) {\n          continue;\n        }\n\n        Algorithm compressName = Compression.getCompressionAlgorithmByName(\n            compressAlgo);\n        Decompressor decompressor = compressName.getDecompressor();\n        FileContext fileContext = FileContext.getFileContext(\n            thisNodeFile.getPath().toUri(), conf);\n        FSDataInputStream fsin = fileContext.open(thisNodeFile.getPath());\n        int bufferSize = 65536;\n        for (IndexedFileLogMeta candidate : candidates) {\n          if (candidate.getLastModificatedTime() < startTime\n              || candidate.getLastModificatedTime() > endTime) {\n            continue;\n          }\n          byte[] cbuf = new byte[bufferSize];\n          InputStream in = null;\n          try {\n            in = compressName.createDecompressionStream(\n                new BoundedRangeFileInputStream(fsin,\n                    candidate.getStartIndex(),\n                    candidate.getFileCompressedSize()),\n                    decompressor,\n                    LogAggregationIndexedFileController.getFSInputBufferSize(\n                        conf));\n            long logLength = candidate.getFileSize();\n            html.pre().__(\"\\n\\n\").__();\n            html.p().__(\"Log Type: \" + candidate.getFileName()).__();\n            html.p().__(\"Log Upload Time: \" + Times.format(\n                candidate.getLastModificatedTime())).__();\n            html.p().__(\"Log Length: \" + Long.toString(\n                logLength)).__();\n            long startIndex = start < 0\n                ? logLength + start : start;\n            startIndex = startIndex < 0 ? 0 : startIndex;\n            startIndex = startIndex > logLength ? logLength : startIndex;\n            long endLogIndex = end < 0\n                ? logLength + end : end;\n            endLogIndex = endLogIndex < 0 ? 0 : endLogIndex;\n            endLogIndex = endLogIndex > logLength ? logLength : endLogIndex;\n            endLogIndex = endLogIndex < startIndex ?\n                startIndex : endLogIndex;\n            long toRead = endLogIndex - startIndex;\n            if (toRead < logLength) {\n              html.p().__(\"Showing \" + toRead + \" bytes of \" + logLength\n                  + \" total. Click \").a(url(\"logs\", $(NM_NODENAME),\n                      $(CONTAINER_ID), $(ENTITY_STRING), $(APP_OWNER),\n                      candidate.getFileName(), \"?start=0&start.time=\"\n                      + startTime + \"&end.time=\" + endTime), \"here\").\n                      __(\" for the full log.\").__();\n            }\n            long totalSkipped = 0;\n            while (totalSkipped < startIndex) {\n              long ret = in.skip(startIndex - totalSkipped);\n              if (ret == 0) {\n                //Read one byte\n                int nextByte = in.read();\n                // Check if we have reached EOF\n                if (nextByte == -1) {\n                  throw new IOException(\"Premature EOF from container log\");\n                }\n                ret = 1;\n              }\n              totalSkipped += ret;\n            }\n            int len = 0;\n            int currentToRead = toRead > bufferSize ? bufferSize : (int) toRead;\n            PRE<Hamlet> pre = html.pre();\n\n            while (toRead > 0\n                && (len = in.read(cbuf, 0, currentToRead)) > 0) {\n              pre.__(new String(cbuf, 0, len, Charset.forName(\"UTF-8\")));\n              toRead = toRead - len;\n              currentToRead = toRead > bufferSize ? bufferSize : (int) toRead;\n            }\n\n            pre.__();\n            foundLog = true;\n          } catch (Exception ex) {\n            LOG.error(\"Error getting logs for \" + logEntity, ex);\n            continue;\n          } finally {\n            IOUtils.closeQuietly(in);\n          }\n        }\n      }\n      if (!foundLog) {\n        if (desiredLogType.isEmpty()) {\n          html.h1(\"No logs available for container \" + containerId.toString());\n        } else {\n          html.h1(\"Unable to locate '\" + desiredLogType\n              + \"' log for container \" + containerId.toString());\n        }\n      }\n    } catch (RuntimeException e) {\n      throw e;\n    } catch (Exception ex) {\n      html.h1().__(\"Error getting logs for \" + logEntity).__();\n      LOG.error(\"Error getting logs for \" + logEntity, ex);\n    }\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.logaggregation.filecontroller.tfile.LogAggregationTFileController.renderAggregatedLogsBlock": {
                "code_before_change": "  public void renderAggregatedLogsBlock(Block html, ViewContext context) {\n    TFileAggregatedLogsBlock block = new TFileAggregatedLogsBlock(\n        context, conf);\n    block.render(html);\n  }",
                "code_after_change": "  public void renderAggregatedLogsBlock(Block html, ViewContext context) {\n    TFileAggregatedLogsBlock block = new TFileAggregatedLogsBlock(\n        context, conf);\n    block.render(html);\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.logaggregation.filecontroller.ifile.LogAggregationIndexedFileController.readAggregatedLogsMeta": {
                "code_before_change": "  public List<ContainerLogMeta> readAggregatedLogsMeta(\n      ContainerLogsRequest logRequest) throws IOException {\n    List<IndexedLogsMeta> listOfLogsMeta = new ArrayList<>();\n    List<ContainerLogMeta> containersLogMeta = new ArrayList<>();\n    String containerIdStr = logRequest.getContainerId();\n    String nodeId = logRequest.getNodeId();\n    ApplicationId appId = logRequest.getAppId();\n    String appOwner = logRequest.getAppOwner();\n    boolean getAllContainers = (containerIdStr == null ||\n        containerIdStr.isEmpty());\n    String nodeIdStr = (nodeId == null || nodeId.isEmpty()) ? null\n        : LogAggregationUtils.getNodeString(nodeId);\n    List<FileStatus> nodeFiles = LogAggregationUtils\n        .getRemoteNodeFileList(conf, appId, appOwner, this.remoteRootLogDir,\n        this.remoteRootLogDirSuffix);\n    if (nodeFiles.isEmpty()) {\n      throw new IOException(\"There is no available log fils for \"\n          + \"application:\" + appId);\n    }\n    Map<String, Long> checkSumFiles = parseCheckSumFiles(nodeFiles);\n    List<FileStatus> fileToRead = getNodeLogFileToRead(\n        nodeFiles, nodeIdStr, appId);\n    for(FileStatus thisNodeFile : fileToRead) {\n      try {\n        Long checkSumIndex = checkSumFiles.get(\n            thisNodeFile.getPath().getName());\n        long endIndex = -1;\n        if (checkSumIndex != null) {\n          endIndex = checkSumIndex.longValue();\n        }\n        IndexedLogsMeta current = loadIndexedLogsMeta(\n            thisNodeFile.getPath(), endIndex);\n        if (current != null) {\n          listOfLogsMeta.add(current);\n        }\n      } catch (IOException ex) {\n        // DO NOTHING\n        LOG.warn(\"Can not get log meta from the log file:\"\n            + thisNodeFile.getPath());\n      }\n    }\n    for (IndexedLogsMeta indexedLogMeta : listOfLogsMeta) {\n      String curNodeId = indexedLogMeta.getNodeId();\n      for (IndexedPerAggregationLogMeta logMeta :\n          indexedLogMeta.getLogMetas()) {\n        if (getAllContainers) {\n          for (Entry<String, List<IndexedFileLogMeta>> log : logMeta\n              .getLogMetas().entrySet()) {\n            ContainerLogMeta meta = new ContainerLogMeta(\n                log.getKey().toString(), curNodeId);\n            for (IndexedFileLogMeta aMeta : log.getValue()) {\n              meta.addLogMeta(aMeta.getFileName(), Long.toString(\n                  aMeta.getFileSize()),\n                  Times.format(aMeta.getLastModificatedTime()));\n            }\n            containersLogMeta.add(meta);\n          }\n        } else if (logMeta.getContainerLogMeta(containerIdStr) != null) {\n          ContainerLogMeta meta = new ContainerLogMeta(containerIdStr,\n              curNodeId);\n          for (IndexedFileLogMeta log :\n              logMeta.getContainerLogMeta(containerIdStr)) {\n            meta.addLogMeta(log.getFileName(), Long.toString(\n                log.getFileSize()),\n                Times.format(log.getLastModificatedTime()));\n          }\n          containersLogMeta.add(meta);\n        }\n      }\n    }\n    Collections.sort(containersLogMeta, new Comparator<ContainerLogMeta>() {\n      @Override\n      public int compare(ContainerLogMeta o1, ContainerLogMeta o2) {\n        return o1.getContainerId().compareTo(o2.getContainerId());\n      }\n    });\n    return containersLogMeta;\n  }",
                "code_after_change": "  public List<ContainerLogMeta> readAggregatedLogsMeta(\n      ContainerLogsRequest logRequest) throws IOException {\n    List<IndexedLogsMeta> listOfLogsMeta = new ArrayList<>();\n    List<ContainerLogMeta> containersLogMeta = new ArrayList<>();\n    String containerIdStr = logRequest.getContainerId();\n    String nodeId = logRequest.getNodeId();\n    ApplicationId appId = logRequest.getAppId();\n    String appOwner = logRequest.getAppOwner();\n    boolean getAllContainers = (containerIdStr == null ||\n        containerIdStr.isEmpty());\n    String nodeIdStr = (nodeId == null || nodeId.isEmpty()) ? null\n        : LogAggregationUtils.getNodeString(nodeId);\n    List<FileStatus> nodeFiles = LogAggregationUtils\n        .getRemoteNodeFileList(conf, appId, appOwner, this.remoteRootLogDir,\n        this.remoteRootLogDirSuffix);\n    if (nodeFiles.isEmpty()) {\n      throw new IOException(\"There is no available log fils for \"\n          + \"application:\" + appId);\n    }\n    Map<String, Long> checkSumFiles = parseCheckSumFiles(nodeFiles);\n    List<FileStatus> fileToRead = getNodeLogFileToRead(\n        nodeFiles, nodeIdStr, appId);\n    for(FileStatus thisNodeFile : fileToRead) {\n      try {\n        Long checkSumIndex = checkSumFiles.get(\n            thisNodeFile.getPath().getName());\n        long endIndex = -1;\n        if (checkSumIndex != null) {\n          endIndex = checkSumIndex.longValue();\n        }\n        IndexedLogsMeta current = loadIndexedLogsMeta(\n            thisNodeFile.getPath(), endIndex, appId);\n        if (current != null) {\n          listOfLogsMeta.add(current);\n        }\n      } catch (IOException ex) {\n        // DO NOTHING\n        LOG.warn(\"Can not get log meta from the log file:\"\n            + thisNodeFile.getPath() + \"\\n\" + ex.getMessage());\n      }\n    }\n    for (IndexedLogsMeta indexedLogMeta : listOfLogsMeta) {\n      String curNodeId = indexedLogMeta.getNodeId();\n      for (IndexedPerAggregationLogMeta logMeta :\n          indexedLogMeta.getLogMetas()) {\n        if (getAllContainers) {\n          for (Entry<String, List<IndexedFileLogMeta>> log : logMeta\n              .getLogMetas().entrySet()) {\n            ContainerLogMeta meta = new ContainerLogMeta(\n                log.getKey().toString(), curNodeId);\n            for (IndexedFileLogMeta aMeta : log.getValue()) {\n              meta.addLogMeta(aMeta.getFileName(), Long.toString(\n                  aMeta.getFileSize()),\n                  Times.format(aMeta.getLastModificatedTime()));\n            }\n            containersLogMeta.add(meta);\n          }\n        } else if (logMeta.getContainerLogMeta(containerIdStr) != null) {\n          ContainerLogMeta meta = new ContainerLogMeta(containerIdStr,\n              curNodeId);\n          for (IndexedFileLogMeta log :\n              logMeta.getContainerLogMeta(containerIdStr)) {\n            meta.addLogMeta(log.getFileName(), Long.toString(\n                log.getFileSize()),\n                Times.format(log.getLastModificatedTime()));\n          }\n          containersLogMeta.add(meta);\n        }\n      }\n    }\n    Collections.sort(containersLogMeta, new Comparator<ContainerLogMeta>() {\n      @Override\n      public int compare(ContainerLogMeta o1, ContainerLogMeta o2) {\n        return o1.getContainerId().compareTo(o2.getContainerId());\n      }\n    });\n    return containersLogMeta;\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.logaggregation.filecontroller.LogAggregationFileController.getApplicationOwner": {
                "code_before_change": "  public abstract String getApplicationOwner(Path aggregatedLogPath)\n      throws IOException;\n\n  /**\n   * Returns ACLs for the application. An empty map is returned if no ACLs are\n   * found.\n   *\n   * @param aggregatedLogPath the aggregatedLog path.\n   * @return a map of the Application ACLs.\n   * @throws IOException if we can not get the application acls\n   */\n  public abstract Map<ApplicationAccessType, String> getApplicationAcls(\n      Path aggregatedLogPath) throws IOException;\n\n  /**\n   * Verify and create the remote log directory.\n   */\n  public void verifyAndCreateRemoteLogDir() {\n    boolean logPermError = true;\n    // Checking the existence of the TLD\n    FileSystem remoteFS = null;\n    try {\n      remoteFS = getFileSystem(conf);\n    } catch (IOException e) {\n      throw new YarnRuntimeException(\n          \"Unable to get Remote FileSystem instance\", e);\n    }\n    boolean remoteExists = true;\n    Path remoteRootLogDir = getRemoteRootLogDir();\n    try {\n      FsPermission perms =\n          remoteFS.getFileStatus(remoteRootLogDir).getPermission();\n      if (!perms.equals(TLDIR_PERMISSIONS) && logPermError) {\n        LOG.warn(\"Remote Root Log Dir [\" + remoteRootLogDir\n            + \"] already exist, but with incorrect permissions. \"\n            + \"Expected: [\" + TLDIR_PERMISSIONS + \"], Found: [\" + perms\n            + \"].\" + \" The cluster may have problems with multiple users.\");\n        logPermError = false;\n      } else {\n        logPermError = true;\n      }\n    } catch (FileNotFoundException e) {\n      remoteExists = false;\n    } catch (IOException e) {\n      throw new YarnRuntimeException(\n          \"Failed to check permissions for dir [\"\n              + remoteRootLogDir + \"]\", e);\n    }\n    if (!remoteExists) {\n      LOG.warn(\"Remote Root Log Dir [\" + remoteRootLogDir\n          + \"] does not exist. Attempting to create it.\");\n      try {\n        Path qualified =\n            remoteRootLogDir.makeQualified(remoteFS.getUri(),\n                remoteFS.getWorkingDirectory());\n        remoteFS.mkdirs(qualified, new FsPermission(TLDIR_PERMISSIONS));\n        remoteFS.setPermission(qualified, new FsPermission(TLDIR_PERMISSIONS));\n\n        UserGroupInformation loginUser = UserGroupInformation.getLoginUser();\n        String primaryGroupName = null;\n        try {\n          primaryGroupName = loginUser.getPrimaryGroupName();\n        } catch (IOException e) {\n          LOG.warn(\"No primary group found. The remote root log directory\" +\n              \" will be created with the HDFS superuser being its group \" +\n              \"owner. JobHistoryServer may be unable to read the directory.\");\n        }\n        // set owner on the remote directory only if the primary group exists\n        if (primaryGroupName != null) {\n          remoteFS.setOwner(qualified,\n              loginUser.getShortUserName(), primaryGroupName);\n        }\n      } catch (IOException e) {\n        throw new YarnRuntimeException(\"Failed to create remoteLogDir [\"\n            + remoteRootLogDir + \"]\", e);\n      }\n    }\n  }",
                "code_after_change": "  public abstract String getApplicationOwner(Path aggregatedLogPath,\n      ApplicationId appId)\n      throws IOException;\n\n  /**\n   * Returns ACLs for the application. An empty map is returned if no ACLs are\n   * found.\n   *\n   * @param aggregatedLogPath the aggregatedLog path.\n   * @param appId the ApplicationId\n   * @return a map of the Application ACLs.\n   * @throws IOException if we can not get the application acls\n   */\n  public abstract Map<ApplicationAccessType, String> getApplicationAcls(\n      Path aggregatedLogPath, ApplicationId appId) throws IOException;\n\n  /**\n   * Verify and create the remote log directory.\n   */\n  public void verifyAndCreateRemoteLogDir() {\n    boolean logPermError = true;\n    // Checking the existence of the TLD\n    FileSystem remoteFS = null;\n    try {\n      remoteFS = getFileSystem(conf);\n    } catch (IOException e) {\n      throw new YarnRuntimeException(\n          \"Unable to get Remote FileSystem instance\", e);\n    }\n    boolean remoteExists = true;\n    Path remoteRootLogDir = getRemoteRootLogDir();\n    try {\n      FsPermission perms =\n          remoteFS.getFileStatus(remoteRootLogDir).getPermission();\n      if (!perms.equals(TLDIR_PERMISSIONS) && logPermError) {\n        LOG.warn(\"Remote Root Log Dir [\" + remoteRootLogDir\n            + \"] already exist, but with incorrect permissions. \"\n            + \"Expected: [\" + TLDIR_PERMISSIONS + \"], Found: [\" + perms\n            + \"].\" + \" The cluster may have problems with multiple users.\");\n        logPermError = false;\n      } else {\n        logPermError = true;\n      }\n    } catch (FileNotFoundException e) {\n      remoteExists = false;\n    } catch (IOException e) {\n      throw new YarnRuntimeException(\n          \"Failed to check permissions for dir [\"\n              + remoteRootLogDir + \"]\", e);\n    }\n    if (!remoteExists) {\n      LOG.warn(\"Remote Root Log Dir [\" + remoteRootLogDir\n          + \"] does not exist. Attempting to create it.\");\n      try {\n        Path qualified =\n            remoteRootLogDir.makeQualified(remoteFS.getUri(),\n                remoteFS.getWorkingDirectory());\n        remoteFS.mkdirs(qualified, new FsPermission(TLDIR_PERMISSIONS));\n        remoteFS.setPermission(qualified, new FsPermission(TLDIR_PERMISSIONS));\n\n        UserGroupInformation loginUser = UserGroupInformation.getLoginUser();\n        String primaryGroupName = null;\n        try {\n          primaryGroupName = loginUser.getPrimaryGroupName();\n        } catch (IOException e) {\n          LOG.warn(\"No primary group found. The remote root log directory\" +\n              \" will be created with the HDFS superuser being its group \" +\n              \"owner. JobHistoryServer may be unable to read the directory.\");\n        }\n        // set owner on the remote directory only if the primary group exists\n        if (primaryGroupName != null) {\n          remoteFS.setOwner(qualified,\n              loginUser.getShortUserName(), primaryGroupName);\n        }\n      } catch (IOException e) {\n        throw new YarnRuntimeException(\"Failed to create remoteLogDir [\"\n            + remoteRootLogDir + \"]\", e);\n      }\n    }\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-common.src.main.java.org.apache.hadoop.yarn.logaggregation.filecontroller.LogAggregationFileController.renderAggregatedLogsBlock": {
                "code_before_change": "  public abstract void renderAggregatedLogsBlock(Block html,\n      ViewContext context);\n\n  /**\n   * Returns the owner of the application.\n   *\n   * @param aggregatedLogPath the aggregatedLog path\n   * @return the application owner\n   * @throws IOException if we can not get the application owner\n   */\n  public abstract String getApplicationOwner(Path aggregatedLogPath)\n      throws IOException;\n\n  /**\n   * Returns ACLs for the application. An empty map is returned if no ACLs are\n   * found.\n   *\n   * @param aggregatedLogPath the aggregatedLog path.\n   * @return a map of the Application ACLs.\n   * @throws IOException if we can not get the application acls\n   */\n  public abstract Map<ApplicationAccessType, String> getApplicationAcls(\n      Path aggregatedLogPath) throws IOException;\n\n  /**\n   * Verify and create the remote log directory.\n   */\n  public void verifyAndCreateRemoteLogDir() {\n    boolean logPermError = true;\n    // Checking the existence of the TLD\n    FileSystem remoteFS = null;\n    try {\n      remoteFS = getFileSystem(conf);\n    } catch (IOException e) {\n      throw new YarnRuntimeException(\n          \"Unable to get Remote FileSystem instance\", e);\n    }\n    boolean remoteExists = true;\n    Path remoteRootLogDir = getRemoteRootLogDir();\n    try {\n      FsPermission perms =\n          remoteFS.getFileStatus(remoteRootLogDir).getPermission();\n      if (!perms.equals(TLDIR_PERMISSIONS) && logPermError) {\n        LOG.warn(\"Remote Root Log Dir [\" + remoteRootLogDir\n            + \"] already exist, but with incorrect permissions. \"\n            + \"Expected: [\" + TLDIR_PERMISSIONS + \"], Found: [\" + perms\n            + \"].\" + \" The cluster may have problems with multiple users.\");\n        logPermError = false;\n      } else {\n        logPermError = true;\n      }\n    } catch (FileNotFoundException e) {\n      remoteExists = false;\n    } catch (IOException e) {\n      throw new YarnRuntimeException(\n          \"Failed to check permissions for dir [\"\n              + remoteRootLogDir + \"]\", e);\n    }\n    if (!remoteExists) {\n      LOG.warn(\"Remote Root Log Dir [\" + remoteRootLogDir\n          + \"] does not exist. Attempting to create it.\");\n      try {\n        Path qualified =\n            remoteRootLogDir.makeQualified(remoteFS.getUri(),\n                remoteFS.getWorkingDirectory());\n        remoteFS.mkdirs(qualified, new FsPermission(TLDIR_PERMISSIONS));\n        remoteFS.setPermission(qualified, new FsPermission(TLDIR_PERMISSIONS));\n\n        UserGroupInformation loginUser = UserGroupInformation.getLoginUser();\n        String primaryGroupName = null;\n        try {\n          primaryGroupName = loginUser.getPrimaryGroupName();\n        } catch (IOException e) {\n          LOG.warn(\"No primary group found. The remote root log directory\" +\n              \" will be created with the HDFS superuser being its group \" +\n              \"owner. JobHistoryServer may be unable to read the directory.\");\n        }\n        // set owner on the remote directory only if the primary group exists\n        if (primaryGroupName != null) {\n          remoteFS.setOwner(qualified,\n              loginUser.getShortUserName(), primaryGroupName);\n        }\n      } catch (IOException e) {\n        throw new YarnRuntimeException(\"Failed to create remoteLogDir [\"\n            + remoteRootLogDir + \"]\", e);\n      }\n    }\n  }",
                "code_after_change": "  public abstract void renderAggregatedLogsBlock(Block html,\n      ViewContext context);\n\n  /**\n   * Returns the owner of the application.\n   *\n   * @param aggregatedLogPath the aggregatedLog path\n   * @param appId the ApplicationId\n   * @return the application owner\n   * @throws IOException if we can not get the application owner\n   */\n  public abstract String getApplicationOwner(Path aggregatedLogPath,\n      ApplicationId appId)\n      throws IOException;\n\n  /**\n   * Returns ACLs for the application. An empty map is returned if no ACLs are\n   * found.\n   *\n   * @param aggregatedLogPath the aggregatedLog path.\n   * @param appId the ApplicationId\n   * @return a map of the Application ACLs.\n   * @throws IOException if we can not get the application acls\n   */\n  public abstract Map<ApplicationAccessType, String> getApplicationAcls(\n      Path aggregatedLogPath, ApplicationId appId) throws IOException;\n\n  /**\n   * Verify and create the remote log directory.\n   */\n  public void verifyAndCreateRemoteLogDir() {\n    boolean logPermError = true;\n    // Checking the existence of the TLD\n    FileSystem remoteFS = null;\n    try {\n      remoteFS = getFileSystem(conf);\n    } catch (IOException e) {\n      throw new YarnRuntimeException(\n          \"Unable to get Remote FileSystem instance\", e);\n    }\n    boolean remoteExists = true;\n    Path remoteRootLogDir = getRemoteRootLogDir();\n    try {\n      FsPermission perms =\n          remoteFS.getFileStatus(remoteRootLogDir).getPermission();\n      if (!perms.equals(TLDIR_PERMISSIONS) && logPermError) {\n        LOG.warn(\"Remote Root Log Dir [\" + remoteRootLogDir\n            + \"] already exist, but with incorrect permissions. \"\n            + \"Expected: [\" + TLDIR_PERMISSIONS + \"], Found: [\" + perms\n            + \"].\" + \" The cluster may have problems with multiple users.\");\n        logPermError = false;\n      } else {\n        logPermError = true;\n      }\n    } catch (FileNotFoundException e) {\n      remoteExists = false;\n    } catch (IOException e) {\n      throw new YarnRuntimeException(\n          \"Failed to check permissions for dir [\"\n              + remoteRootLogDir + \"]\", e);\n    }\n    if (!remoteExists) {\n      LOG.warn(\"Remote Root Log Dir [\" + remoteRootLogDir\n          + \"] does not exist. Attempting to create it.\");\n      try {\n        Path qualified =\n            remoteRootLogDir.makeQualified(remoteFS.getUri(),\n                remoteFS.getWorkingDirectory());\n        remoteFS.mkdirs(qualified, new FsPermission(TLDIR_PERMISSIONS));\n        remoteFS.setPermission(qualified, new FsPermission(TLDIR_PERMISSIONS));\n\n        UserGroupInformation loginUser = UserGroupInformation.getLoginUser();\n        String primaryGroupName = null;\n        try {\n          primaryGroupName = loginUser.getPrimaryGroupName();\n        } catch (IOException e) {\n          LOG.warn(\"No primary group found. The remote root log directory\" +\n              \" will be created with the HDFS superuser being its group \" +\n              \"owner. JobHistoryServer may be unable to read the directory.\");\n        }\n        // set owner on the remote directory only if the primary group exists\n        if (primaryGroupName != null) {\n          remoteFS.setOwner(qualified,\n              loginUser.getShortUserName(), primaryGroupName);\n        }\n      } catch (IOException e) {\n        throw new YarnRuntimeException(\"Failed to create remoteLogDir [\"\n            + remoteRootLogDir + \"]\", e);\n      }\n    }\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Preventive",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report precisely identifies the root cause by pointing to inefficient memory management in the methods responsible for loading and initializing log metadata, specifically mentioning 'loadIndexedLogsMeta' and 'initializeWriterInRolling', which are part of the ground truth methods. The fix suggestion is preventive as it suggests increasing JVM heap size and optimizing memory usage, which would mitigate the issue but does not directly match the developer's fix. The problem location identification is precise as it mentions methods 'loadIndexedLogsMeta' and 'initializeWriterInRolling', which are in the ground truth list. There is no wrong information in the bug report as all details are relevant and accurate."
        }
    },
    {
        "filename": "YARN-7382.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSAppAttempt.recoverContainer": {
                "code_before_change": "  public synchronized void recoverContainer(SchedulerNode node,\n      RMContainer rmContainer) {\n    try {\n      writeLock.lock();\n\n      super.recoverContainer(node, rmContainer);\n\n      if (!rmContainer.getState().equals(RMContainerState.COMPLETED)) {\n        getQueue().incUsedResource(rmContainer.getContainer().getResource());\n      }\n    } finally {\n      writeLock.unlock();\n    }\n  }",
                "code_after_change": "  public synchronized void recoverContainer(SchedulerNode node,\n      RMContainer rmContainer) {\n    try {\n      writeLock.lock();\n\n      super.recoverContainer(node, rmContainer);\n\n      if (!rmContainer.getState().equals(RMContainerState.COMPLETED)) {\n        getQueue().incUsedResource(rmContainer.getContainer().getResource());\n      }\n\n      // If not running unmanaged, the first container we recover is always\n      // the AM. Set the amResource for this app and update the leaf queue's AM\n      // usage\n      if (!isAmRunning() && !getUnmanagedAM()) {\n        Resource resource = rmContainer.getAllocatedResource();\n        setAMResource(resource);\n        getQueue().addAMResourceUsage(resource);\n        setAmRunning(true);\n      }\n    } finally {\n      writeLock.unlock();\n    }\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Preventive",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the root cause as the `getNextPendingAsk` method in the `AppSchedulingInfo` class, which is part of the stack trace but not the ground truth method. Therefore, it is classified as 'Partial' with 'Shared Stack Trace Context'. The fix suggestion to check for an empty `schedulerKeys` collection is preventive, as it would prevent the exception but does not match the actual fix. The problem location is also 'Partial' with 'Shared Stack Trace Context' because it points to methods in the stack trace but not the ground truth method. There is no wrong information in the bug report as all details are relevant to the context of the bug."
        }
    },
    {
        "filename": "YARN-1094.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.RMContextImpl.RMContextImpl": {
                "code_before_change": [],
                "code_after_change": []
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.RMContextImpl.getAMFinishingMonitor": {
                "code_before_change": "  public AMLivelinessMonitor getAMFinishingMonitor() {\n    return this.amFinishingMonitor;\n  }",
                "code_after_change": "  public AMLivelinessMonitor getAMFinishingMonitor() {\n    return this.amFinishingMonitor;\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceStart": {
                "code_before_change": "    protected void serviceStart() throws Exception {\n      this.eventProcessor.start();\n      super.serviceStart();\n    }",
                "code_after_change": "    protected void serviceStart() throws Exception {\n      this.eventProcessor.start();\n      super.serviceStart();\n    }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl.transition": {
                "code_before_change": "    public void transition(RMNodeImpl rmNode, RMNodeEvent event) {\n      // Inform the scheduler\n\n      rmNode.context.getDispatcher().getEventHandler().handle(\n          new NodeAddedSchedulerEvent(rmNode));\n      \n      String host = rmNode.nodeId.getHost();\n      if (rmNode.context.getInactiveRMNodes().containsKey(host)) {\n        // Old node rejoining\n        RMNode previouRMNode = rmNode.context.getInactiveRMNodes().get(host);\n        rmNode.context.getInactiveRMNodes().remove(host);\n        rmNode.updateMetricsForRejoinedNode(previouRMNode.getState());\n      } else {\n        // Increment activeNodes explicitly because this is a new node.\n        ClusterMetrics.getMetrics().incrNumActiveNodes();\n      }\n    }",
                "code_after_change": "    public void transition(RMNodeImpl rmNode, RMNodeEvent event) {\n      // Inform the scheduler\n\n      rmNode.context.getDispatcher().getEventHandler().handle(\n          new NodeAddedSchedulerEvent(rmNode));\n      \n      String host = rmNode.nodeId.getHost();\n      if (rmNode.context.getInactiveRMNodes().containsKey(host)) {\n        // Old node rejoining\n        RMNode previouRMNode = rmNode.context.getInactiveRMNodes().get(host);\n        rmNode.context.getInactiveRMNodes().remove(host);\n        rmNode.updateMetricsForRejoinedNode(previouRMNode.getState());\n      } else {\n        // Increment activeNodes explicitly because this is a new node.\n        ClusterMetrics.getMetrics().incrNumActiveNodes();\n      }\n    }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.serviceInit": {
                "code_before_change": "  protected void serviceInit(Configuration conf) throws Exception {\n\n    validateConfigs(conf);\n\n    this.conf = conf;\n\n    this.conf.setBoolean(Dispatcher.DISPATCHER_EXIT_ON_ERROR_KEY, true);\n\n    this.rmDispatcher = createDispatcher();\n    addIfService(this.rmDispatcher);\n\n    this.amRmTokenSecretManager = createAMRMTokenSecretManager(conf);\n\n    this.containerAllocationExpirer = new ContainerAllocationExpirer(\n        this.rmDispatcher);\n    addService(this.containerAllocationExpirer);\n\n    AMLivelinessMonitor amLivelinessMonitor = createAMLivelinessMonitor();\n    addService(amLivelinessMonitor);\n\n    AMLivelinessMonitor amFinishingMonitor = createAMLivelinessMonitor();\n    addService(amFinishingMonitor);\n\n    DelegationTokenRenewer tokenRenewer = createDelegationTokenRenewer();\n    addService(tokenRenewer);\n\n    this.containerTokenSecretManager = createContainerTokenSecretManager(conf);\n    this.nmTokenSecretManager = createNMTokenSecretManager(conf);\n    \n    boolean isRecoveryEnabled = conf.getBoolean(\n        YarnConfiguration.RECOVERY_ENABLED,\n        YarnConfiguration.DEFAULT_RM_RECOVERY_ENABLED);\n    \n    RMStateStore rmStore = null;\n    if(isRecoveryEnabled) {\n      recoveryEnabled = true;\n      rmStore =  RMStateStoreFactory.getStore(conf);\n    } else {\n      recoveryEnabled = false;\n      rmStore = new NullRMStateStore();\n    }\n\n    try {\n      rmStore.init(conf);\n      rmStore.setRMDispatcher(rmDispatcher);\n    } catch (Exception e) {\n      // the Exception from stateStore.init() needs to be handled for \n      // HA and we need to give up master status if we got fenced\n      LOG.error(\"Failed to init state store\", e);\n      ExitUtil.terminate(1, e);\n    }\n\n    this.rmContext =\n        new RMContextImpl(this.rmDispatcher, rmStore,\n          this.containerAllocationExpirer, amLivelinessMonitor,\n          amFinishingMonitor, tokenRenewer, this.amRmTokenSecretManager,\n          this.containerTokenSecretManager, this.nmTokenSecretManager,\n          this.clientToAMSecretManager);\n    \n    // Register event handler for NodesListManager\n    this.nodesListManager = new NodesListManager(this.rmContext);\n    this.rmDispatcher.register(NodesListManagerEventType.class, \n        this.nodesListManager);\n    addService(nodesListManager);\n\n    // Initialize the scheduler\n    this.scheduler = createScheduler();\n    this.schedulerDispatcher = createSchedulerEventDispatcher();\n    addIfService(this.schedulerDispatcher);\n    this.rmDispatcher.register(SchedulerEventType.class,\n        this.schedulerDispatcher);\n\n    // Register event handler for RmAppEvents\n    this.rmDispatcher.register(RMAppEventType.class,\n        new ApplicationEventDispatcher(this.rmContext));\n\n    // Register event handler for RmAppAttemptEvents\n    this.rmDispatcher.register(RMAppAttemptEventType.class,\n        new ApplicationAttemptEventDispatcher(this.rmContext));\n\n    // Register event handler for RmNodes\n    this.rmDispatcher.register(RMNodeEventType.class,\n        new NodeEventDispatcher(this.rmContext));    \n\n    this.nmLivelinessMonitor = createNMLivelinessMonitor();\n    addService(this.nmLivelinessMonitor);\n\n    this.resourceTracker = createResourceTrackerService();\n    addService(resourceTracker);\n\n    DefaultMetricsSystem.initialize(\"ResourceManager\");\n    JvmMetrics.initSingleton(\"ResourceManager\", null);\n\n    try {\n      this.scheduler.reinitialize(conf, this.rmContext);\n    } catch (IOException ioe) {\n      throw new RuntimeException(\"Failed to initialize scheduler\", ioe);\n    }\n\n    // creating monitors that handle preemption\n    createPolicyMonitors();\n\n    masterService = createApplicationMasterService();\n    addService(masterService) ;\n\n    this.applicationACLsManager = new ApplicationACLsManager(conf);\n\n    this.rmAppManager = createRMAppManager();\n    // Register event handler for RMAppManagerEvents\n    this.rmDispatcher.register(RMAppManagerEventType.class,\n        this.rmAppManager);\n    this.rmDTSecretManager = createRMDelegationTokenSecretManager(this.rmContext);\n    clientRM = createClientRMService();\n    addService(clientRM);\n    \n    adminService = createAdminService(clientRM, masterService, resourceTracker);\n    addService(adminService);\n\n    this.applicationMasterLauncher = createAMLauncher();\n    this.rmDispatcher.register(AMLauncherEventType.class, \n        this.applicationMasterLauncher);\n\n    addService(applicationMasterLauncher);\n\n    new RMNMInfo(this.rmContext, this.scheduler);\n    \n    super.serviceInit(conf);\n  }",
                "code_after_change": "  protected void serviceInit(Configuration conf) throws Exception {\n\n    validateConfigs(conf);\n\n    this.conf = conf;\n\n    this.conf.setBoolean(Dispatcher.DISPATCHER_EXIT_ON_ERROR_KEY, true);\n\n    this.rmDispatcher = createDispatcher();\n    addIfService(this.rmDispatcher);\n\n    this.amRmTokenSecretManager = createAMRMTokenSecretManager(conf);\n\n    this.containerAllocationExpirer = new ContainerAllocationExpirer(\n        this.rmDispatcher);\n    addService(this.containerAllocationExpirer);\n\n    AMLivelinessMonitor amLivelinessMonitor = createAMLivelinessMonitor();\n    addService(amLivelinessMonitor);\n\n    AMLivelinessMonitor amFinishingMonitor = createAMLivelinessMonitor();\n    addService(amFinishingMonitor);\n\n    if (UserGroupInformation.isSecurityEnabled()) {\n      this.delegationTokenRenewer = createDelegationTokenRenewer();\n      addService(delegationTokenRenewer);\n    }\n\n    this.containerTokenSecretManager = createContainerTokenSecretManager(conf);\n    this.nmTokenSecretManager = createNMTokenSecretManager(conf);\n    \n    boolean isRecoveryEnabled = conf.getBoolean(\n        YarnConfiguration.RECOVERY_ENABLED,\n        YarnConfiguration.DEFAULT_RM_RECOVERY_ENABLED);\n    \n    RMStateStore rmStore = null;\n    if(isRecoveryEnabled) {\n      recoveryEnabled = true;\n      rmStore =  RMStateStoreFactory.getStore(conf);\n    } else {\n      recoveryEnabled = false;\n      rmStore = new NullRMStateStore();\n    }\n\n    try {\n      rmStore.init(conf);\n      rmStore.setRMDispatcher(rmDispatcher);\n    } catch (Exception e) {\n      // the Exception from stateStore.init() needs to be handled for \n      // HA and we need to give up master status if we got fenced\n      LOG.error(\"Failed to init state store\", e);\n      ExitUtil.terminate(1, e);\n    }\n\n    this.rmContext =\n        new RMContextImpl(this.rmDispatcher, rmStore,\n          this.containerAllocationExpirer, amLivelinessMonitor,\n          amFinishingMonitor, delegationTokenRenewer, this.amRmTokenSecretManager,\n          this.containerTokenSecretManager, this.nmTokenSecretManager,\n          this.clientToAMSecretManager);\n    \n    // Register event handler for NodesListManager\n    this.nodesListManager = new NodesListManager(this.rmContext);\n    this.rmDispatcher.register(NodesListManagerEventType.class, \n        this.nodesListManager);\n    addService(nodesListManager);\n\n    // Initialize the scheduler\n    this.scheduler = createScheduler();\n    this.schedulerDispatcher = createSchedulerEventDispatcher();\n    addIfService(this.schedulerDispatcher);\n    this.rmDispatcher.register(SchedulerEventType.class,\n        this.schedulerDispatcher);\n\n    // Register event handler for RmAppEvents\n    this.rmDispatcher.register(RMAppEventType.class,\n        new ApplicationEventDispatcher(this.rmContext));\n\n    // Register event handler for RmAppAttemptEvents\n    this.rmDispatcher.register(RMAppAttemptEventType.class,\n        new ApplicationAttemptEventDispatcher(this.rmContext));\n\n    // Register event handler for RmNodes\n    this.rmDispatcher.register(RMNodeEventType.class,\n        new NodeEventDispatcher(this.rmContext));    \n\n    this.nmLivelinessMonitor = createNMLivelinessMonitor();\n    addService(this.nmLivelinessMonitor);\n\n    this.resourceTracker = createResourceTrackerService();\n    addService(resourceTracker);\n\n    DefaultMetricsSystem.initialize(\"ResourceManager\");\n    JvmMetrics.initSingleton(\"ResourceManager\", null);\n\n    try {\n      this.scheduler.reinitialize(conf, this.rmContext);\n    } catch (IOException ioe) {\n      throw new RuntimeException(\"Failed to initialize scheduler\", ioe);\n    }\n\n    // creating monitors that handle preemption\n    createPolicyMonitors();\n\n    masterService = createApplicationMasterService();\n    addService(masterService) ;\n\n    this.applicationACLsManager = new ApplicationACLsManager(conf);\n\n    this.rmAppManager = createRMAppManager();\n    // Register event handler for RMAppManagerEvents\n    this.rmDispatcher.register(RMAppManagerEventType.class,\n        this.rmAppManager);\n    this.rmDTSecretManager = createRMDelegationTokenSecretManager(this.rmContext);\n    clientRM = createClientRMService();\n    addService(clientRM);\n    \n    adminService = createAdminService(clientRM, masterService, resourceTracker);\n    addService(adminService);\n\n    this.applicationMasterLauncher = createAMLauncher();\n    this.rmDispatcher.register(AMLauncherEventType.class, \n        this.applicationMasterLauncher);\n\n    addService(applicationMasterLauncher);\n\n    new RMNMInfo(this.rmContext, this.scheduler);\n    \n    super.serviceInit(conf);\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Direct Caller/Callee"
            },
            "fix_suggestion": "Correct",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Direct Caller/Callee"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the root cause as a NullPointerException occurring when the Resource Manager attempts to call `addApplication` on the `DelegationTokenRenewer`. This is a direct caller of the `serviceInit` method in the `ResourceManager` class, which is part of the ground truth methods. Therefore, the root cause identification is 'Partial' with a 'Direct Caller/Callee' sub-category. The fix suggestion in the bug report matches the developer's fix, which involves ensuring the `DelegationTokenRenewer` is properly initialized in the `serviceInit` method, so it is 'Correct'. The problem location identification mentions `ResourceManager.recover` and `RMAppManager.submitApplication`, which are direct callers of the ground truth method `ResourceManager.serviceInit`, hence it is 'Partial' with a 'Direct Caller/Callee' sub-category. There is no wrong information in the bug report as all statements are relevant to the context of the bug."
        }
    },
    {
        "filename": "YARN-7269.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-web-proxy.src.main.java.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.findRedirectUrl": {
                "code_before_change": "  public String findRedirectUrl() throws ServletException {\n    String addr = null;\n    if (proxyUriBases.size() == 1) {\n      // external proxy or not RM HA\n      addr = proxyUriBases.values().iterator().next();\n    } else {\n      // RM HA\n      YarnConfiguration conf = new YarnConfiguration();\n      for (String rmId : getRmIds(conf)) {\n        String url = getUrlByRmId(conf, rmId);\n        if (isValidUrl(url)) {\n          addr = url;\n          break;\n        }\n      }\n    }\n\n    if (addr == null) {\n      throw new ServletException(\n          \"Could not determine the proxy server for redirection\");\n    }\n    return addr;\n  }",
                "code_after_change": "  public String findRedirectUrl() throws ServletException {\n    String addr = null;\n    if (proxyUriBases.size() == 1) {\n      // external proxy or not RM HA\n      addr = proxyUriBases.values().iterator().next();\n    } else if (rmUrls != null) {\n      for (String url : rmUrls) {\n        String host = proxyUriBases.get(url);\n        if (isValidUrl(host)) {\n          addr = host;\n          break;\n        }\n      }\n    }\n\n    if (addr == null) {\n      throw new ServletException(\n          \"Could not determine the proxy server for redirection\");\n    }\n    return addr;\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-web-proxy.src.main.java.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.init": {
                "code_before_change": "  public void init(FilterConfig conf) throws ServletException {\n    // Maintain for backwards compatibility\n    if (conf.getInitParameter(PROXY_HOST) != null\n        && conf.getInitParameter(PROXY_URI_BASE) != null) {\n      proxyHosts = new String[]{conf.getInitParameter(PROXY_HOST)};\n      proxyUriBases = new HashMap<>(1);\n      proxyUriBases.put(\"dummy\", conf.getInitParameter(PROXY_URI_BASE));\n    } else {\n      proxyHosts = conf.getInitParameter(PROXY_HOSTS)\n          .split(PROXY_HOSTS_DELIMITER);\n\n      String[] proxyUriBasesArr = conf.getInitParameter(PROXY_URI_BASES)\n          .split(PROXY_URI_BASES_DELIMITER);\n      proxyUriBases = new HashMap<>(proxyUriBasesArr.length);\n      for (String proxyUriBase : proxyUriBasesArr) {\n        try {\n          URL url = new URL(proxyUriBase);\n          proxyUriBases.put(url.getHost() + \":\" + url.getPort(), proxyUriBase);\n        } catch(MalformedURLException e) {\n          LOG.warn(\"{} does not appear to be a valid URL\", proxyUriBase, e);\n        }\n      }\n    }\n  }",
                "code_after_change": "  public void init(FilterConfig conf) throws ServletException {\n    // Maintain for backwards compatibility\n    if (conf.getInitParameter(PROXY_HOST) != null\n        && conf.getInitParameter(PROXY_URI_BASE) != null) {\n      proxyHosts = new String[]{conf.getInitParameter(PROXY_HOST)};\n      proxyUriBases = new HashMap<>(1);\n      proxyUriBases.put(\"dummy\", conf.getInitParameter(PROXY_URI_BASE));\n    } else {\n      proxyHosts = conf.getInitParameter(PROXY_HOSTS)\n          .split(PROXY_HOSTS_DELIMITER);\n\n      String[] proxyUriBasesArr = conf.getInitParameter(PROXY_URI_BASES)\n          .split(PROXY_URI_BASES_DELIMITER);\n      proxyUriBases = new HashMap<>(proxyUriBasesArr.length);\n      for (String proxyUriBase : proxyUriBasesArr) {\n        try {\n          URL url = new URL(proxyUriBase);\n          proxyUriBases.put(url.getHost() + \":\" + url.getPort(), proxyUriBase);\n        } catch(MalformedURLException e) {\n          LOG.warn(\"{} does not appear to be a valid URL\", proxyUriBase, e);\n        }\n      }\n    }\n\n    if (conf.getInitParameter(AmFilterInitializer.RM_HA_URLS) != null) {\n      rmUrls = conf.getInitParameter(AmFilterInitializer.RM_HA_URLS).split(\",\");\n    }\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-web-proxy.src.main.java.org.apache.hadoop.yarn.server.webproxy.amfilter.AmFilterInitializer.initFilter": {
                "code_before_change": "  public void initFilter(FilterContainer container, Configuration conf) {\n    Map<String, String> params = new HashMap<>();\n    List<String> proxies = WebAppUtils.getProxyHostsAndPortsForAmFilter(conf);\n    StringBuilder sb = new StringBuilder();\n    for (String proxy : proxies) {\n      sb.append(proxy.split(\":\")[0]).append(AmIpFilter.PROXY_HOSTS_DELIMITER);\n    }\n    sb.setLength(sb.length() - 1);\n    params.put(AmIpFilter.PROXY_HOSTS, sb.toString());\n\n    String prefix = WebAppUtils.getHttpSchemePrefix(conf);\n    String proxyBase = getApplicationWebProxyBase();\n    sb = new StringBuilder();\n    for (String proxy : proxies) {\n      sb.append(prefix).append(proxy).append(proxyBase)\n          .append(AmIpFilter.PROXY_HOSTS_DELIMITER);\n    }\n    sb.setLength(sb.length() - 1);\n    params.put(AmIpFilter.PROXY_URI_BASES, sb.toString());\n    container.addFilter(FILTER_NAME, FILTER_CLASS, params);\n  }",
                "code_after_change": "  public void initFilter(FilterContainer container, Configuration conf) {\n    Map<String, String> params = new HashMap<>();\n    List<String> proxies = WebAppUtils.getProxyHostsAndPortsForAmFilter(conf);\n    StringBuilder sb = new StringBuilder();\n    for (String proxy : proxies) {\n      sb.append(proxy.split(\":\")[0]).append(AmIpFilter.PROXY_HOSTS_DELIMITER);\n    }\n    sb.setLength(sb.length() - 1);\n    params.put(AmIpFilter.PROXY_HOSTS, sb.toString());\n\n    String prefix = WebAppUtils.getHttpSchemePrefix(conf);\n    String proxyBase = getApplicationWebProxyBase();\n    sb = new StringBuilder();\n    for (String proxy : proxies) {\n      sb.append(prefix).append(proxy).append(proxyBase)\n          .append(AmIpFilter.PROXY_HOSTS_DELIMITER);\n    }\n    sb.setLength(sb.length() - 1);\n    params.put(AmIpFilter.PROXY_URI_BASES, sb.toString());\n    container.addFilter(FILTER_NAME, FILTER_CLASS, params);\n\n    // Handle RM HA urls\n    List<String> urls = new ArrayList<>();\n\n    // Include yarn-site.xml in the classpath\n    YarnConfiguration yarnConf = new YarnConfiguration(conf);\n    for (String rmId : getRmIds(yarnConf)) {\n      String url = getUrlByRmId(yarnConf, rmId);\n      urls.add(url);\n    }\n    params.put(RM_HA_URLS, StringUtils.join(\",\", urls));\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Preventive",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report precisely identifies the root cause as the inability of the `AmIpFilter` class to determine the proxy server for redirection, which aligns with the ground truth method `findRedirectUrl`. The fix suggestion is preventive as it suggests checking proxy configuration settings and ensuring necessary classes are present, which would mitigate the issue. The problem location identification is precise as it directly mentions the `findRedirectUrl` and `doFilter` methods in the `AmIpFilter` class, which are part of the ground truth. There is no wrong information in the bug report as all details are relevant and grounded in the context of the bug."
        }
    },
    {
        "filename": "YARN-7249.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-resourcemanager.src.main.java.org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.completedContainerInternal": {
                "code_before_change": "  protected void completedContainerInternal(\n      RMContainer rmContainer, ContainerStatus containerStatus,\n      RMContainerEventType event) {\n    Container container = rmContainer.getContainer();\n    ContainerId containerId = container.getId();\n\n    // Get the application for the finished container\n    FiCaSchedulerApp application = getCurrentAttemptForContainer(\n        container.getId());\n    ApplicationId appId =\n        containerId.getApplicationAttemptId().getApplicationId();\n    if (application == null) {\n      LOG.info(\n          \"Container \" + container + \" of\" + \" finished application \" + appId\n              + \" completed with event \" + event);\n      return;\n    }\n\n    // Get the node on which the container was allocated\n    FiCaSchedulerNode node = getNode(container.getNodeId());\n    if (null == node) {\n      LOG.info(\"Container \" + container + \" of\" + \" removed node \" + container\n          .getNodeId() + \" completed with event \" + event);\n      return;\n    }\n\n    // Inform the queue\n    LeafQueue queue = (LeafQueue) application.getQueue();\n    queue.completedContainer(getClusterResource(), application, node,\n        rmContainer, containerStatus, event, null, true);\n  }",
                "code_after_change": "  protected void completedContainerInternal(\n      RMContainer rmContainer, ContainerStatus containerStatus,\n      RMContainerEventType event) {\n    Container container = rmContainer.getContainer();\n    ContainerId containerId = container.getId();\n    \n    // Get the application for the finished container\n    FiCaSchedulerApp application =\n        getCurrentAttemptForContainer(container.getId());\n    ApplicationId appId =\n        containerId.getApplicationAttemptId().getApplicationId();\n    if (application == null) {\n      LOG.info(\"Container \" + container + \" of\" + \" finished application \"\n          + appId + \" completed with event \" + event);\n      return;\n    }\n    \n    // Get the node on which the container was allocated\n    FiCaSchedulerNode node = getNode(container.getNodeId());\n    if (node == null) {\n      LOG.info(\"Container=\" + container + \" of application=\" + appId\n          + \" completed with event=\" + event + \" on a node=\" + container\n          .getNodeId() + \". However the node might be already removed by RM.\");\n      return;\n    }\n    \n    // Inform the queue\n    LeafQueue queue = (LeafQueue)application.getQueue();\n    queue.completedContainer(clusterResource, application, node, \n        rmContainer, containerStatus, event, null, true);\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Correct",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report precisely identifies the root cause as a race condition leading to a null node being passed to the `completedContainer` method in the `LeafQueue` class, which is directly related to the ground truth method `CapacityScheduler.completedContainerInternal`. The fix suggestion to add a null check for the node in the `CapacityScheduler` matches the developer's fix, which involves checking for a null node in the `completedContainerInternal` method. The problem location is precisely identified as it mentions the `CapacityScheduler.completedContainerInternal` method, which is the ground truth method. There is no wrong information in the bug report as all details are consistent with the context of the bug."
        }
    },
    {
        "filename": "YARN-4598.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerImpl.KillTransition": {
                "code_before_change": [],
                "code_after_change": []
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Preventive",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The root cause identification is partial because the bug report identifies the issue with the state transition logic in the YARN framework, specifically mentioning 'ContainerImpl.handle' and 'StateMachineFactory.doTransition', which are in the same stack trace as the ground truth method 'ContainerImpl.KillTransition'. The fix suggestion is preventive as it suggests reviewing the state transition logic and adding checks to prevent invalid events, which would mitigate the issue. The problem location identification is partial because it mentions 'ContainerImpl.handle' and 'StateMachineFactory.doTransition', which are in the shared stack trace context with the ground truth method. There is no wrong information in the bug report as all details are relevant to the context of the bug."
        }
    },
    {
        "filename": "YARN-1149.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.serviceStop": {
                "code_before_change": "  public void serviceStop() throws Exception {\n    if (auxiliaryServices.getServiceState() == STARTED) {\n      auxiliaryServices.unregisterServiceListener(this);\n    }\n    if (server != null) {\n      server.stop();\n    }\n    super.serviceStop();\n  }",
                "code_after_change": "  public void serviceStop() throws Exception {\n    setBlockNewContainerRequests(true);\n    this.writeLock.lock();\n    try {\n      serviceStopped = true;\n      if (context != null) {\n        cleanUpApplications(NodeManagerEventType.SHUTDOWN);\n      }\n    } finally {\n      this.writeLock.unlock();\n    }\n    if (auxiliaryServices.getServiceState() == STARTED) {\n      auxiliaryServices.unregisterServiceListener(this);\n    }\n    if (server != null) {\n      server.stop();\n    }\n    super.serviceStop();\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.AppInitTransition": {
                "code_before_change": [],
                "code_after_change": []
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.run": {
                "code_before_change": "      public void run() {\n        int lastHeartBeatID = 0;\n        while (!isStopped) {\n          // Send heartbeat\n          try {\n            NodeHeartbeatResponse response = null;\n            NodeStatus nodeStatus = getNodeStatusAndUpdateContainersInContext();\n            nodeStatus.setResponseId(lastHeartBeatID);\n            \n            NodeHeartbeatRequest request = recordFactory\n                .newRecordInstance(NodeHeartbeatRequest.class);\n            request.setNodeStatus(nodeStatus);\n            request\n              .setLastKnownContainerTokenMasterKey(NodeStatusUpdaterImpl.this.context\n                .getContainerTokenSecretManager().getCurrentKey());\n            request\n              .setLastKnownNMTokenMasterKey(NodeStatusUpdaterImpl.this.context\n                .getNMTokenSecretManager().getCurrentKey());\n            response = resourceTracker.nodeHeartbeat(request);\n            //get next heartbeat interval from response\n            nextHeartBeatInterval = response.getNextHeartBeatInterval();\n            updateMasterKeys(response);\n\n            if (response.getNodeAction() == NodeAction.SHUTDOWN) {\n              LOG\n                .warn(\"Recieved SHUTDOWN signal from Resourcemanager as part of heartbeat,\"\n                    + \" hence shutting down.\");\n              LOG.warn(\"Message from ResourceManager: \"\n                  + response.getDiagnosticsMessage());\n              dispatcher.getEventHandler().handle(\n                  new NodeManagerEvent(NodeManagerEventType.SHUTDOWN));\n              break;\n            }\n            if (response.getNodeAction() == NodeAction.RESYNC) {\n              LOG.warn(\"Node is out of sync with ResourceManager,\"\n                  + \" hence resyncing.\");\n              LOG.warn(\"Message from ResourceManager: \"\n                  + response.getDiagnosticsMessage());\n              // Invalidate the RMIdentifier while resync\n              NodeStatusUpdaterImpl.this.rmIdentifier =\n                  ResourceManagerConstants.RM_INVALID_IDENTIFIER;\n              dispatcher.getEventHandler().handle(\n                  new NodeManagerEvent(NodeManagerEventType.RESYNC));\n              break;\n            }\n\n            lastHeartBeatID = response.getResponseId();\n            List<ContainerId> containersToCleanup = response\n                .getContainersToCleanup();\n            if (containersToCleanup.size() != 0) {\n              dispatcher.getEventHandler().handle(\n                  new CMgrCompletedContainersEvent(containersToCleanup, \n                      CMgrCompletedContainersEvent.Reason.BY_RESOURCEMANAGER));\n            }\n            List<ApplicationId> appsToCleanup =\n                response.getApplicationsToCleanup();\n            //Only start tracking for keepAlive on FINISH_APP\n            trackAppsForKeepAlive(appsToCleanup);\n            if (appsToCleanup.size() != 0) {\n              dispatcher.getEventHandler().handle(\n                  new CMgrCompletedAppsEvent(appsToCleanup));\n            }\n          } catch (ConnectException e) {\n            //catch and throw the exception if tried MAX wait time to connect RM\n            dispatcher.getEventHandler().handle(\n                new NodeManagerEvent(NodeManagerEventType.SHUTDOWN));\n            throw new YarnRuntimeException(e);\n          } catch (Throwable e) {\n\n            // TODO Better error handling. Thread can die with the rest of the\n            // NM still running.\n            LOG.error(\"Caught exception in status-updater\", e);\n          } finally {\n            synchronized (heartbeatMonitor) {\n              nextHeartBeatInterval = nextHeartBeatInterval <= 0 ?\n                  YarnConfiguration.DEFAULT_RM_NM_HEARTBEAT_INTERVAL_MS :\n                    nextHeartBeatInterval;\n              try {\n                heartbeatMonitor.wait(nextHeartBeatInterval);\n              } catch (InterruptedException e) {\n                // Do Nothing\n              }\n            }\n          }\n        }\n      }",
                "code_after_change": "      public void run() {\n        int lastHeartBeatID = 0;\n        while (!isStopped) {\n          // Send heartbeat\n          try {\n            NodeHeartbeatResponse response = null;\n            NodeStatus nodeStatus = getNodeStatusAndUpdateContainersInContext();\n            nodeStatus.setResponseId(lastHeartBeatID);\n            \n            NodeHeartbeatRequest request = recordFactory\n                .newRecordInstance(NodeHeartbeatRequest.class);\n            request.setNodeStatus(nodeStatus);\n            request\n              .setLastKnownContainerTokenMasterKey(NodeStatusUpdaterImpl.this.context\n                .getContainerTokenSecretManager().getCurrentKey());\n            request\n              .setLastKnownNMTokenMasterKey(NodeStatusUpdaterImpl.this.context\n                .getNMTokenSecretManager().getCurrentKey());\n            response = resourceTracker.nodeHeartbeat(request);\n            //get next heartbeat interval from response\n            nextHeartBeatInterval = response.getNextHeartBeatInterval();\n            updateMasterKeys(response);\n\n            if (response.getNodeAction() == NodeAction.SHUTDOWN) {\n              LOG\n                .warn(\"Recieved SHUTDOWN signal from Resourcemanager as part of heartbeat,\"\n                    + \" hence shutting down.\");\n              LOG.warn(\"Message from ResourceManager: \"\n                  + response.getDiagnosticsMessage());\n              dispatcher.getEventHandler().handle(\n                  new NodeManagerEvent(NodeManagerEventType.SHUTDOWN));\n              break;\n            }\n            if (response.getNodeAction() == NodeAction.RESYNC) {\n              LOG.warn(\"Node is out of sync with ResourceManager,\"\n                  + \" hence resyncing.\");\n              LOG.warn(\"Message from ResourceManager: \"\n                  + response.getDiagnosticsMessage());\n              // Invalidate the RMIdentifier while resync\n              NodeStatusUpdaterImpl.this.rmIdentifier =\n                  ResourceManagerConstants.RM_INVALID_IDENTIFIER;\n              dispatcher.getEventHandler().handle(\n                  new NodeManagerEvent(NodeManagerEventType.RESYNC));\n              break;\n            }\n\n            lastHeartBeatID = response.getResponseId();\n            List<ContainerId> containersToCleanup = response\n                .getContainersToCleanup();\n            if (!containersToCleanup.isEmpty()) {\n              dispatcher.getEventHandler().handle(\n                  new CMgrCompletedContainersEvent(containersToCleanup));\n            }\n            List<ApplicationId> appsToCleanup =\n                response.getApplicationsToCleanup();\n            //Only start tracking for keepAlive on FINISH_APP\n            trackAppsForKeepAlive(appsToCleanup);\n            if (!appsToCleanup.isEmpty()) {\n              dispatcher.getEventHandler().handle(\n                  new CMgrCompletedAppsEvent(appsToCleanup,\n                      CMgrCompletedAppsEvent.Reason.BY_RESOURCEMANAGER));\n            }\n          } catch (ConnectException e) {\n            //catch and throw the exception if tried MAX wait time to connect RM\n            dispatcher.getEventHandler().handle(\n                new NodeManagerEvent(NodeManagerEventType.SHUTDOWN));\n            throw new YarnRuntimeException(e);\n          } catch (Throwable e) {\n\n            // TODO Better error handling. Thread can die with the rest of the\n            // NM still running.\n            LOG.error(\"Caught exception in status-updater\", e);\n          } finally {\n            synchronized (heartbeatMonitor) {\n              nextHeartBeatInterval = nextHeartBeatInterval <= 0 ?\n                  YarnConfiguration.DEFAULT_RM_NM_HEARTBEAT_INTERVAL_MS :\n                    nextHeartBeatInterval;\n              try {\n                heartbeatMonitor.wait(nextHeartBeatInterval);\n              } catch (InterruptedException e) {\n                // Do Nothing\n              }\n            }\n          }\n        }\n      }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.ContainerManagerImpl": {
                "code_before_change": [],
                "code_after_change": []
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.handleAppFinishWithContainersCleanedup": {
                "code_before_change": "  void handleAppFinishWithContainersCleanedup() {\n    // Delete Application level resources\n    this.dispatcher.getEventHandler().handle(\n        new ApplicationLocalizationEvent(\n            LocalizationEventType.DESTROY_APPLICATION_RESOURCES, this));\n\n    // tell any auxiliary services that the app is done \n    this.dispatcher.getEventHandler().handle(\n        new AuxServicesEvent(AuxServicesEventType.APPLICATION_STOP, appId));\n\n    // TODO: Trigger the LogsManager\n  }",
                "code_after_change": "  void handleAppFinishWithContainersCleanedup() {\n    // Delete Application level resources\n    this.dispatcher.getEventHandler().handle(\n        new ApplicationLocalizationEvent(\n            LocalizationEventType.DESTROY_APPLICATION_RESOURCES, this));\n\n    // tell any auxiliary services that the app is done \n    this.dispatcher.getEventHandler().handle(\n        new AuxServicesEvent(AuxServicesEventType.APPLICATION_STOP, appId));\n\n    // TODO: Trigger the LogsManager\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.handle": {
                "code_before_change": "    public void handle(ContainerEvent event) {\n      Map<ContainerId,Container> containers =\n        ContainerManagerImpl.this.context.getContainers();\n      Container c = containers.get(event.getContainerID());\n      if (c != null) {\n        c.handle(event);\n      } else {\n        LOG.warn(\"Event \" + event + \" sent to absent container \" +\n            event.getContainerID());\n      }\n    }",
                "code_after_change": "    public void handle(ContainerEvent event) {\n      Map<ContainerId,Container> containers =\n        ContainerManagerImpl.this.context.getContainers();\n      Container c = containers.get(event.getContainerID());\n      if (c != null) {\n        c.handle(event);\n      } else {\n        LOG.warn(\"Event \" + event + \" sent to absent container \" +\n            event.getContainerID());\n      }\n    }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.startContainerInternal": {
                "code_before_change": "  private void startContainerInternal(NMTokenIdentifier nmTokenIdentifier,\n      ContainerTokenIdentifier containerTokenIdentifier,\n      StartContainerRequest request) throws YarnException, IOException {\n\n    /*\n     * 1) It should save the NMToken into NMTokenSecretManager. This is done\n     * here instead of RPC layer because at the time of opening/authenticating\n     * the connection it doesn't know what all RPC calls user will make on it.\n     * Also new NMToken is issued only at startContainer (once it gets renewed).\n     * \n     * 2) It should validate containerToken. Need to check below things. a) It\n     * is signed by correct master key (part of retrieve password). b) It\n     * belongs to correct Node Manager (part of retrieve password). c) It has\n     * correct RMIdentifier. d) It is not expired.\n     */\n    authorizeStartRequest(nmTokenIdentifier, containerTokenIdentifier);\n \n    if (containerTokenIdentifier.getRMIdentifer() != nodeStatusUpdater\n        .getRMIdentifier()) {\n        // Is the container coming from unknown RM\n        StringBuilder sb = new StringBuilder(\"\\nContainer \");\n        sb.append(containerTokenIdentifier.getContainerID().toString())\n          .append(\" rejected as it is allocated by a previous RM\");\n        throw new InvalidContainerException(sb.toString());\n    }\n    // update NMToken\n    updateNMTokenIdentifier(nmTokenIdentifier);\n\n    ContainerId containerId = containerTokenIdentifier.getContainerID();\n    String containerIdStr = containerId.toString();\n    String user = containerTokenIdentifier.getApplicationSubmitter();\n\n    LOG.info(\"Start request for \" + containerIdStr + \" by user \" + user);\n\n    ContainerLaunchContext launchContext = request.getContainerLaunchContext();\n\n    Credentials credentials = parseCredentials(launchContext);\n\n    Container container =\n        new ContainerImpl(getConfig(), this.dispatcher, launchContext,\n          credentials, metrics, containerTokenIdentifier);\n    ApplicationId applicationID =\n        containerId.getApplicationAttemptId().getApplicationId();\n    if (context.getContainers().putIfAbsent(containerId, container) != null) {\n      NMAuditLogger.logFailure(user, AuditConstants.START_CONTAINER,\n        \"ContainerManagerImpl\", \"Container already running on this node!\",\n        applicationID, containerId);\n      throw RPCUtil.getRemoteException(\"Container \" + containerIdStr\n          + \" already is running on this node!!\");\n    }\n\n    // Create the application\n    Application application =\n        new ApplicationImpl(dispatcher, user, applicationID, credentials, context);\n    if (null == context.getApplications().putIfAbsent(applicationID,\n      application)) {\n      LOG.info(\"Creating a new application reference for app \" + applicationID);\n\n      dispatcher.getEventHandler().handle(\n        new ApplicationInitEvent(applicationID, container.getLaunchContext()\n          .getApplicationACLs()));\n    }\n\n    dispatcher.getEventHandler().handle(\n      new ApplicationContainerInitEvent(container));\n\n    this.context.getContainerTokenSecretManager().startContainerSuccessful(\n      containerTokenIdentifier);\n    NMAuditLogger.logSuccess(user, AuditConstants.START_CONTAINER,\n      \"ContainerManageImpl\", applicationID, containerId);\n    // TODO launchedContainer misplaced -> doesn't necessarily mean a container\n    // launch. A finished Application will not launch containers.\n    metrics.launchedContainer();\n    metrics.allocateContainer(containerTokenIdentifier.getResource()); \n  }",
                "code_after_change": "  private void startContainerInternal(NMTokenIdentifier nmTokenIdentifier,\n      ContainerTokenIdentifier containerTokenIdentifier,\n      StartContainerRequest request) throws YarnException, IOException {\n\n    /*\n     * 1) It should save the NMToken into NMTokenSecretManager. This is done\n     * here instead of RPC layer because at the time of opening/authenticating\n     * the connection it doesn't know what all RPC calls user will make on it.\n     * Also new NMToken is issued only at startContainer (once it gets renewed).\n     * \n     * 2) It should validate containerToken. Need to check below things. a) It\n     * is signed by correct master key (part of retrieve password). b) It\n     * belongs to correct Node Manager (part of retrieve password). c) It has\n     * correct RMIdentifier. d) It is not expired.\n     */\n    authorizeStartRequest(nmTokenIdentifier, containerTokenIdentifier);\n\n    if (containerTokenIdentifier.getRMIdentifer() != nodeStatusUpdater\n      .getRMIdentifier()) {\n      // Is the container coming from unknown RM\n      StringBuilder sb = new StringBuilder(\"\\nContainer \");\n      sb.append(containerTokenIdentifier.getContainerID().toString()).append(\n        \" rejected as it is allocated by a previous RM\");\n      throw new InvalidContainerException(sb.toString());\n    }\n    // update NMToken\n    updateNMTokenIdentifier(nmTokenIdentifier);\n\n    ContainerId containerId = containerTokenIdentifier.getContainerID();\n    String containerIdStr = containerId.toString();\n    String user = containerTokenIdentifier.getApplicationSubmitter();\n\n    LOG.info(\"Start request for \" + containerIdStr + \" by user \" + user);\n\n    ContainerLaunchContext launchContext = request.getContainerLaunchContext();\n\n    Map<String, ByteBuffer> serviceData = getAuxServiceMetaData();\n    if (launchContext.getServiceData() != null\n        && !launchContext.getServiceData().isEmpty()) {\n      for (Map.Entry<String, ByteBuffer> meta : launchContext.getServiceData()\n        .entrySet()) {\n        if (null == serviceData.get(meta.getKey())) {\n          throw new InvalidAuxServiceException(\"The auxService:\"\n              + meta.getKey() + \" does not exist\");\n        }\n      }\n    }\n\n    Credentials credentials = parseCredentials(launchContext);\n\n    Container container =\n        new ContainerImpl(getConfig(), this.dispatcher, launchContext,\n          credentials, metrics, containerTokenIdentifier);\n    ApplicationId applicationID =\n        containerId.getApplicationAttemptId().getApplicationId();\n    if (context.getContainers().putIfAbsent(containerId, container) != null) {\n      NMAuditLogger.logFailure(user, AuditConstants.START_CONTAINER,\n        \"ContainerManagerImpl\", \"Container already running on this node!\",\n        applicationID, containerId);\n      throw RPCUtil.getRemoteException(\"Container \" + containerIdStr\n          + \" already is running on this node!!\");\n    }\n\n    this.readLock.lock();\n    try {\n      if (!serviceStopped) {\n        // Create the application\n        Application application =\n            new ApplicationImpl(dispatcher, this.aclsManager, user,\n              applicationID, credentials, context);\n        if (null == context.getApplications().putIfAbsent(applicationID,\n          application)) {\n          LOG.info(\"Creating a new application reference for app \"\n              + applicationID);\n\n          dispatcher.getEventHandler().handle(\n            new ApplicationInitEvent(applicationID, container\n              .getLaunchContext().getApplicationACLs()));\n        }\n\n        dispatcher.getEventHandler().handle(\n          new ApplicationContainerInitEvent(container));\n\n        this.context.getContainerTokenSecretManager().startContainerSuccessful(\n          containerTokenIdentifier);\n        NMAuditLogger.logSuccess(user, AuditConstants.START_CONTAINER,\n          \"ContainerManageImpl\", applicationID, containerId);\n        // TODO launchedContainer misplaced -> doesn't necessarily mean a\n        // container\n        // launch. A finished Application will not launch containers.\n        metrics.launchedContainer();\n        metrics.allocateContainer(containerTokenIdentifier.getResource());\n      } else {\n        throw new YarnException(\"Container start failed as the NodeManager is \"\n            + \"in the process of shutting down\");\n      }\n    } finally {\n      this.readLock.unlock();\n    }\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.NodeManager.serviceStop": {
                "code_before_change": "  protected void serviceStop() throws Exception {\n    if (isStopping.getAndSet(true)) {\n      return;\n    }\n    if (context != null) {\n      cleanupContainers(NodeManagerEventType.SHUTDOWN);\n    }\n    super.serviceStop();\n    DefaultMetricsSystem.shutdown();\n  }",
                "code_after_change": "  protected void serviceStop() throws Exception {\n    if (isStopping.getAndSet(true)) {\n      return;\n    }\n    super.serviceStop();\n    DefaultMetricsSystem.shutdown();\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.NodeManager.serviceInit": {
                "code_before_change": "  protected void serviceInit(Configuration conf) throws Exception {\n\n    conf.setBoolean(Dispatcher.DISPATCHER_EXIT_ON_ERROR_KEY, true);\n\n    NMContainerTokenSecretManager containerTokenSecretManager =\n        new NMContainerTokenSecretManager(conf);\n\n    NMTokenSecretManagerInNM nmTokenSecretManager =\n        new NMTokenSecretManagerInNM();\n    \n    this.aclsManager = new ApplicationACLsManager(conf);\n\n    ContainerExecutor exec = ReflectionUtils.newInstance(\n        conf.getClass(YarnConfiguration.NM_CONTAINER_EXECUTOR,\n          DefaultContainerExecutor.class, ContainerExecutor.class), conf);\n    try {\n      exec.init();\n    } catch (IOException e) {\n      throw new YarnRuntimeException(\"Failed to initialize container executor\", e);\n    }    \n    DeletionService del = createDeletionService(exec);\n    addService(del);\n\n    // NodeManager level dispatcher\n    this.dispatcher = new AsyncDispatcher();\n\n    nodeHealthChecker = new NodeHealthCheckerService();\n    addService(nodeHealthChecker);\n    dirsHandler = nodeHealthChecker.getDiskHandler();\n\n    this.context = createNMContext(containerTokenSecretManager,\n        nmTokenSecretManager);\n    \n    nodeStatusUpdater =\n        createNodeStatusUpdater(context, dispatcher, nodeHealthChecker);\n\n    NodeResourceMonitor nodeResourceMonitor = createNodeResourceMonitor();\n    addService(nodeResourceMonitor);\n\n    containerManager =\n        createContainerManager(context, exec, del, nodeStatusUpdater,\n        this.aclsManager, dirsHandler);\n    addService(containerManager);\n    ((NMContext) context).setContainerManager(containerManager);\n\n    WebServer webServer = createWebServer(context, containerManager\n        .getContainersMonitor(), this.aclsManager, dirsHandler);\n    addService(webServer);\n    ((NMContext) context).setWebServer(webServer);\n\n    dispatcher.register(ContainerManagerEventType.class, containerManager);\n    dispatcher.register(NodeManagerEventType.class, this);\n    addService(dispatcher);\n    \n    DefaultMetricsSystem.initialize(\"NodeManager\");\n\n    // StatusUpdater should be added last so that it get started last \n    // so that we make sure everything is up before registering with RM. \n    addService(nodeStatusUpdater);\n    \n    waitForContainersOnShutdownMillis =\n        conf.getLong(YarnConfiguration.NM_SLEEP_DELAY_BEFORE_SIGKILL_MS,\n            YarnConfiguration.DEFAULT_NM_SLEEP_DELAY_BEFORE_SIGKILL_MS) + \n        conf.getLong(YarnConfiguration.NM_PROCESS_KILL_WAIT_MS,\n            YarnConfiguration.DEFAULT_NM_PROCESS_KILL_WAIT_MS) +\n        SHUTDOWN_CLEANUP_SLOP_MS;\n    \n    super.serviceInit(conf);\n    // TODO add local dirs to del\n  }",
                "code_after_change": "  protected void serviceInit(Configuration conf) throws Exception {\n\n    conf.setBoolean(Dispatcher.DISPATCHER_EXIT_ON_ERROR_KEY, true);\n\n    NMContainerTokenSecretManager containerTokenSecretManager =\n        new NMContainerTokenSecretManager(conf);\n\n    NMTokenSecretManagerInNM nmTokenSecretManager =\n        new NMTokenSecretManagerInNM();\n    \n    this.context =\n        createNMContext(containerTokenSecretManager, nmTokenSecretManager);\n\n    this.aclsManager = new ApplicationACLsManager(conf);\n\n    ContainerExecutor exec = ReflectionUtils.newInstance(\n        conf.getClass(YarnConfiguration.NM_CONTAINER_EXECUTOR,\n          DefaultContainerExecutor.class, ContainerExecutor.class), conf);\n    try {\n      exec.init();\n    } catch (IOException e) {\n      throw new YarnRuntimeException(\"Failed to initialize container executor\", e);\n    }    \n    DeletionService del = createDeletionService(exec);\n    addService(del);\n\n    // NodeManager level dispatcher\n    this.dispatcher = new AsyncDispatcher();\n\n    nodeHealthChecker = new NodeHealthCheckerService();\n    addService(nodeHealthChecker);\n    dirsHandler = nodeHealthChecker.getDiskHandler();\n\n\n    nodeStatusUpdater =\n        createNodeStatusUpdater(context, dispatcher, nodeHealthChecker);\n\n    NodeResourceMonitor nodeResourceMonitor = createNodeResourceMonitor();\n    addService(nodeResourceMonitor);\n\n    containerManager =\n        createContainerManager(context, exec, del, nodeStatusUpdater,\n        this.aclsManager, dirsHandler);\n    addService(containerManager);\n    ((NMContext) context).setContainerManager(containerManager);\n\n    WebServer webServer = createWebServer(context, containerManager\n        .getContainersMonitor(), this.aclsManager, dirsHandler);\n    addService(webServer);\n    ((NMContext) context).setWebServer(webServer);\n\n    dispatcher.register(ContainerManagerEventType.class, containerManager);\n    dispatcher.register(NodeManagerEventType.class, this);\n    addService(dispatcher);\n    \n    DefaultMetricsSystem.initialize(\"NodeManager\");\n\n    // StatusUpdater should be added last so that it get started last \n    // so that we make sure everything is up before registering with RM. \n    addService(nodeStatusUpdater);\n    \n    super.serviceInit(conf);\n    // TODO add local dirs to del\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.transition": {
                "code_before_change": "    public void transition(ApplicationImpl app, ApplicationEvent event) {\n      ApplicationInitEvent initEvent = (ApplicationInitEvent)event;\n      app.applicationACLs = initEvent.getApplicationACLs();\n      app.aclsManager.addApplication(app.getAppId(), app.applicationACLs);\n      // Inform the logAggregator\n      app.dispatcher.getEventHandler().handle(\n          new LogHandlerAppStartedEvent(app.appId, app.user,\n              app.credentials, ContainerLogsRetentionPolicy.ALL_CONTAINERS,\n              app.applicationACLs)); \n    }",
                "code_after_change": "    public void transition(ApplicationImpl app, ApplicationEvent event) {\n      ApplicationInitEvent initEvent = (ApplicationInitEvent)event;\n      app.applicationACLs = initEvent.getApplicationACLs();\n      app.aclsManager.addApplication(app.getAppId(), app.applicationACLs);\n      // Inform the logAggregator\n      app.dispatcher.getEventHandler().handle(\n          new LogHandlerAppStartedEvent(app.appId, app.user,\n              app.credentials, ContainerLogsRetentionPolicy.ALL_CONTAINERS,\n              app.applicationACLs)); \n    }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.refreshServiceAcls": {
                "code_before_change": "  void refreshServiceAcls(Configuration configuration, \n      PolicyProvider policyProvider) {\n    this.server.refreshServiceAcl(configuration, policyProvider);\n  }",
                "code_after_change": "  void refreshServiceAcls(Configuration configuration, \n      PolicyProvider policyProvider) {\n    this.server.refreshServiceAcl(configuration, policyProvider);\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl.serviceInit": {
                "code_before_change": "  public void serviceInit(Configuration conf) throws Exception {\n    LogHandler logHandler =\n      createLogHandler(conf, this.context, this.deletionService);\n    addIfService(logHandler);\n    dispatcher.register(LogHandlerEventType.class, logHandler);\n    \n    super.serviceInit(conf);\n  }",
                "code_after_change": "  public void serviceInit(Configuration conf) throws Exception {\n    LogHandler logHandler =\n      createLogHandler(conf, this.context, this.deletionService);\n    addIfService(logHandler);\n    dispatcher.register(LogHandlerEventType.class, logHandler);\n    \n    waitForContainersOnShutdownMillis =\n        conf.getLong(YarnConfiguration.NM_SLEEP_DELAY_BEFORE_SIGKILL_MS,\n            YarnConfiguration.DEFAULT_NM_SLEEP_DELAY_BEFORE_SIGKILL_MS) +\n        conf.getLong(YarnConfiguration.NM_PROCESS_KILL_WAIT_MS,\n            YarnConfiguration.DEFAULT_NM_PROCESS_KILL_WAIT_MS) +\n        SHUTDOWN_CLEANUP_SLOP_MS;\n\n    super.serviceInit(conf);\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.NodeManager.resyncWithRM": {
                "code_before_change": "  protected void resyncWithRM() {\n    //we do not want to block dispatcher thread here\n    new Thread() {\n      @Override\n      public void run() {\n        LOG.info(\"Notifying ContainerManager to block new container-requests\");\n        containerManager.setBlockNewContainerRequests(true);\n        cleanupContainers(NodeManagerEventType.RESYNC);\n        ((NodeStatusUpdaterImpl) nodeStatusUpdater ).rebootNodeStatusUpdater();\n      }\n    }.start();\n  }",
                "code_after_change": "  protected void resyncWithRM() {\n    //we do not want to block dispatcher thread here\n    new Thread() {\n      @Override\n      public void run() {\n        LOG.info(\"Notifying ContainerManager to block new container-requests\");\n        containerManager.setBlockNewContainerRequests(true);\n        containerManager.cleanUpApplications(NodeManagerEventType.RESYNC);\n        ((NodeStatusUpdaterImpl) nodeStatusUpdater ).rebootNodeStatusUpdater();\n      }\n    }.start();\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Preventive",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the root cause as an invalid state transition in the YARN framework, which is related to the state management logic. This is partially correct as it is mentioned in the same stack trace context as the ground truth methods, but it does not pinpoint the exact ground truth methods where the fix was applied. The fix suggestion is preventive as it suggests reviewing the state management logic to ensure proper event handling, which aligns with the nature of the fix but does not match the exact code changes. The problem location identification is partial because it mentions methods like 'ApplicationImpl.handle' and 'StateMachineFactory.doTransition' that are in the shared stack trace context but not the exact ground truth methods. There is no wrong information in the bug report as all the details are relevant to the context of the bug."
        }
    },
    {
        "filename": "YARN-7818.json",
        "code_diff": {
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DockerLinuxContainerRuntime.relaunchContainer": {
                "code_before_change": [],
                "code_after_change": "  public void relaunchContainer(ContainerRuntimeContext ctx)\n      throws ContainerExecutionException {\n    ContainerId containerId = ctx.getContainer().getContainerId();\n    String containerIdStr = containerId.toString();\n    // Check to see if the container already exists for relaunch\n    DockerCommandExecutor.DockerContainerStatus containerStatus =\n        DockerCommandExecutor.getContainerStatus(containerIdStr, conf,\n            privilegedOperationExecutor, nmContext);\n    if (containerStatus != null &&\n        DockerCommandExecutor.isStartable(containerStatus)) {\n      DockerStartCommand startCommand = new DockerStartCommand(containerIdStr);\n      String commandFile = dockerClient.writeCommandToTempFile(startCommand,\n          containerId, nmContext);\n      PrivilegedOperation launchOp = buildLaunchOp(ctx, commandFile,\n          startCommand);\n\n      // Some failures here are acceptable. Let the calling executor decide.\n      launchOp.disableFailureLogging();\n\n      try {\n        privilegedOperationExecutor.executePrivilegedOperation(null,\n            launchOp, null, null, false, false);\n      } catch (PrivilegedOperationException e) {\n        throw new ContainerExecutionException(\"Relaunch container failed\", e\n            .getExitCode(), e.getOutput(), e.getErrorOutput());\n      }\n    } else {\n      throw new ContainerExecutionException(\"Container is not in a startable \"\n          + \"state, unable to relaunch: \" + containerIdStr);\n    }\n\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DockerLinuxContainerRuntime.launchContainer": {
                "code_before_change": "  public void launchContainer(ContainerRuntimeContext ctx)\n      throws ContainerExecutionException {\n    Container container = ctx.getContainer();\n    Map<String, String> environment = container.getLaunchContext()\n        .getEnvironment();\n    String imageName = environment.get(ENV_DOCKER_CONTAINER_IMAGE);\n    String network = environment.get(ENV_DOCKER_CONTAINER_NETWORK);\n    String hostname = environment.get(ENV_DOCKER_CONTAINER_HOSTNAME);\n\n    if(network == null || network.isEmpty()) {\n      network = defaultNetwork;\n    }\n\n    validateContainerNetworkType(network);\n\n    validateHostname(hostname);\n\n    validateImageName(imageName);\n\n    String containerIdStr = container.getContainerId().toString();\n    String runAsUser = ctx.getExecutionAttribute(RUN_AS_USER);\n    String dockerRunAsUser = runAsUser;\n    Path containerWorkDir = ctx.getExecutionAttribute(CONTAINER_WORK_DIR);\n    String[] groups = null;\n\n    if (enableUserReMapping) {\n      String uid = getUserIdInfo(runAsUser);\n      groups = getGroupIdInfo(runAsUser);\n      String gid = groups[0];\n      if(Integer.parseInt(uid) < userRemappingUidThreshold) {\n        String message = \"uid: \" + uid + \" below threshold: \"\n            + userRemappingUidThreshold;\n        throw new ContainerExecutionException(message);\n      }\n      for(int i = 0; i < groups.length; i++) {\n        String group = groups[i];\n        if (Integer.parseInt(group) < userRemappingGidThreshold) {\n          String message = \"gid: \" + group\n              + \" below threshold: \" + userRemappingGidThreshold;\n          throw new ContainerExecutionException(message);\n        }\n      }\n      dockerRunAsUser = uid + \":\" + gid;\n    }\n\n    //List<String> -> stored as List -> fetched/converted to List<String>\n    //we can't do better here thanks to type-erasure\n    @SuppressWarnings(\"unchecked\")\n    List<String> filecacheDirs = ctx.getExecutionAttribute(FILECACHE_DIRS);\n    @SuppressWarnings(\"unchecked\")\n    List<String> containerLocalDirs = ctx.getExecutionAttribute(\n        CONTAINER_LOCAL_DIRS);\n    @SuppressWarnings(\"unchecked\")\n    List<String> containerLogDirs = ctx.getExecutionAttribute(\n        CONTAINER_LOG_DIRS);\n    @SuppressWarnings(\"unchecked\")\n    Map<Path, List<String>> localizedResources = ctx.getExecutionAttribute(\n        LOCALIZED_RESOURCES);\n    @SuppressWarnings(\"unchecked\")\n    List<String> userLocalDirs = ctx.getExecutionAttribute(USER_LOCAL_DIRS);\n\n    @SuppressWarnings(\"unchecked\")\n    DockerRunCommand runCommand = new DockerRunCommand(containerIdStr,\n        dockerRunAsUser, imageName)\n        .detachOnRun()\n        .setContainerWorkDir(containerWorkDir.toString())\n        .setNetworkType(network);\n    setHostname(runCommand, containerIdStr, hostname);\n    runCommand.setCapabilities(capabilities);\n\n    if(cgroupsRootDirectory != null) {\n      runCommand.addReadOnlyMountLocation(cgroupsRootDirectory,\n          cgroupsRootDirectory, false);\n    }\n\n    List<String> allDirs = new ArrayList<>(containerLocalDirs);\n    allDirs.addAll(filecacheDirs);\n    allDirs.add(containerWorkDir.toString());\n    allDirs.addAll(containerLogDirs);\n    allDirs.addAll(userLocalDirs);\n    for (String dir: allDirs) {\n      runCommand.addMountLocation(dir, dir, true);\n    }\n\n    if (environment.containsKey(ENV_DOCKER_CONTAINER_LOCAL_RESOURCE_MOUNTS)) {\n      String mounts = environment.get(\n          ENV_DOCKER_CONTAINER_LOCAL_RESOURCE_MOUNTS);\n      if (!mounts.isEmpty()) {\n        for (String mount : StringUtils.split(mounts)) {\n          String[] dir = StringUtils.split(mount, ':');\n          if (dir.length != 2) {\n            throw new ContainerExecutionException(\"Invalid mount : \" +\n                mount);\n          }\n          String src = validateMount(dir[0], localizedResources);\n          String dst = dir[1];\n          runCommand.addReadOnlyMountLocation(src, dst, true);\n        }\n      }\n    }\n\n    if (environment.containsKey(ENV_DOCKER_CONTAINER_MOUNTS)) {\n      Matcher parsedMounts = USER_MOUNT_PATTERN.matcher(\n          environment.get(ENV_DOCKER_CONTAINER_MOUNTS));\n      if (!parsedMounts.find()) {\n        throw new ContainerExecutionException(\n            \"Unable to parse user supplied mount list: \"\n                + environment.get(ENV_DOCKER_CONTAINER_MOUNTS));\n      }\n      parsedMounts.reset();\n      while (parsedMounts.find()) {\n        String src = parsedMounts.group(1);\n        String dst = parsedMounts.group(2);\n        String mode = parsedMounts.group(3);\n        if (!mode.equals(\"ro\") && !mode.equals(\"rw\")) {\n          throw new ContainerExecutionException(\n              \"Invalid mount mode requested for mount: \"\n                  + parsedMounts.group());\n        }\n        if (mode.equals(\"ro\")) {\n          runCommand.addReadOnlyMountLocation(src, dst);\n        } else {\n          runCommand.addReadWriteMountLocation(src, dst);\n        }\n      }\n    }\n\n    if (allowHostPidNamespace(container)) {\n      runCommand.setPidNamespace(\"host\");\n    }\n\n    if (allowPrivilegedContainerExecution(container)) {\n      runCommand.setPrivileged();\n    }\n\n    String resourcesOpts = ctx.getExecutionAttribute(RESOURCES_OPTIONS);\n\n    addCGroupParentIfRequired(resourcesOpts, containerIdStr, runCommand);\n\n    String disableOverride = environment.get(\n        ENV_DOCKER_CONTAINER_RUN_OVERRIDE_DISABLE);\n\n    if (disableOverride != null && disableOverride.equals(\"true\")) {\n      LOG.info(\"command override disabled\");\n    } else {\n      List<String> overrideCommands = new ArrayList<>();\n      Path launchDst =\n          new Path(containerWorkDir, ContainerLaunch.CONTAINER_SCRIPT);\n\n      overrideCommands.add(\"bash\");\n      overrideCommands.add(launchDst.toUri().getPath());\n      runCommand.setOverrideCommandWithArgs(overrideCommands);\n    }\n\n    if(enableUserReMapping) {\n      runCommand.groupAdd(groups);\n    }\n\n    // use plugins to update docker run command.\n    if (nmContext != null\n        && nmContext.getResourcePluginManager().getNameToPlugins() != null) {\n      for (ResourcePlugin plugin : nmContext.getResourcePluginManager()\n          .getNameToPlugins().values()) {\n        DockerCommandPlugin dockerCommandPlugin =\n            plugin.getDockerCommandPluginInstance();\n        if (dockerCommandPlugin != null) {\n          dockerCommandPlugin.updateDockerRunCommand(runCommand, container);\n        }\n      }\n    }\n\n    String commandFile = dockerClient.writeCommandToTempFile(runCommand,\n        containerIdStr);\n    PrivilegedOperation launchOp = buildLaunchOp(ctx,\n        commandFile, runCommand);\n\n    try {\n      privilegedOperationExecutor.executePrivilegedOperation(null,\n          launchOp, null, null, false, false);\n    } catch (PrivilegedOperationException e) {\n      LOG.warn(\"Launch container failed. Exception: \", e);\n      LOG.info(\"Docker command used: \" + runCommand);\n\n      throw new ContainerExecutionException(\"Launch container failed\", e\n          .getExitCode(), e.getOutput(), e.getErrorOutput());\n    }\n  }",
                "code_after_change": "  public void launchContainer(ContainerRuntimeContext ctx)\n      throws ContainerExecutionException {\n    Container container = ctx.getContainer();\n    ContainerId containerId = container.getContainerId();\n    Map<String, String> environment = container.getLaunchContext()\n        .getEnvironment();\n    String imageName = environment.get(ENV_DOCKER_CONTAINER_IMAGE);\n    String network = environment.get(ENV_DOCKER_CONTAINER_NETWORK);\n    String hostname = environment.get(ENV_DOCKER_CONTAINER_HOSTNAME);\n\n    if(network == null || network.isEmpty()) {\n      network = defaultNetwork;\n    }\n\n    validateContainerNetworkType(network);\n\n    validateHostname(hostname);\n\n    validateImageName(imageName);\n\n    String containerIdStr = containerId.toString();\n    String runAsUser = ctx.getExecutionAttribute(RUN_AS_USER);\n    String dockerRunAsUser = runAsUser;\n    Path containerWorkDir = ctx.getExecutionAttribute(CONTAINER_WORK_DIR);\n    String[] groups = null;\n\n    if (enableUserReMapping) {\n      String uid = getUserIdInfo(runAsUser);\n      groups = getGroupIdInfo(runAsUser);\n      String gid = groups[0];\n      if(Integer.parseInt(uid) < userRemappingUidThreshold) {\n        String message = \"uid: \" + uid + \" below threshold: \"\n            + userRemappingUidThreshold;\n        throw new ContainerExecutionException(message);\n      }\n      for(int i = 0; i < groups.length; i++) {\n        String group = groups[i];\n        if (Integer.parseInt(group) < userRemappingGidThreshold) {\n          String message = \"gid: \" + group\n              + \" below threshold: \" + userRemappingGidThreshold;\n          throw new ContainerExecutionException(message);\n        }\n      }\n      if (!allowPrivilegedContainerExecution(container)) {\n        dockerRunAsUser = uid + \":\" + gid;\n      } else {\n        dockerRunAsUser = ctx.getExecutionAttribute(USER);\n      }\n    }\n\n    //List<String> -> stored as List -> fetched/converted to List<String>\n    //we can't do better here thanks to type-erasure\n    @SuppressWarnings(\"unchecked\")\n    List<String> filecacheDirs = ctx.getExecutionAttribute(FILECACHE_DIRS);\n    @SuppressWarnings(\"unchecked\")\n    List<String> containerLogDirs = ctx.getExecutionAttribute(\n        CONTAINER_LOG_DIRS);\n    @SuppressWarnings(\"unchecked\")\n    List<String> userFilecacheDirs =\n        ctx.getExecutionAttribute(USER_FILECACHE_DIRS);\n    @SuppressWarnings(\"unchecked\")\n    List<String> applicationLocalDirs =\n        ctx.getExecutionAttribute(APPLICATION_LOCAL_DIRS);\n    @SuppressWarnings(\"unchecked\")\n    Map<Path, List<String>> localizedResources = ctx.getExecutionAttribute(\n        LOCALIZED_RESOURCES);\n\n    @SuppressWarnings(\"unchecked\")\n    DockerRunCommand runCommand = new DockerRunCommand(containerIdStr,\n        dockerRunAsUser, imageName)\n        .detachOnRun()\n        .setContainerWorkDir(containerWorkDir.toString())\n        .setNetworkType(network);\n    // Only add hostname if network is not host or if Registry DNS is enabled.\n    if (!network.equalsIgnoreCase(\"host\") ||\n        conf.getBoolean(RegistryConstants.KEY_DNS_ENABLED,\n            RegistryConstants.DEFAULT_DNS_ENABLED)) {\n      setHostname(runCommand, containerIdStr, hostname);\n    }\n    runCommand.setCapabilities(capabilities);\n\n    runCommand.addAllReadWriteMountLocations(containerLogDirs);\n    runCommand.addAllReadWriteMountLocations(applicationLocalDirs);\n    runCommand.addAllReadOnlyMountLocations(filecacheDirs);\n    runCommand.addAllReadOnlyMountLocations(userFilecacheDirs);\n\n    if (environment.containsKey(ENV_DOCKER_CONTAINER_LOCAL_RESOURCE_MOUNTS)) {\n      String mounts = environment.get(\n          ENV_DOCKER_CONTAINER_LOCAL_RESOURCE_MOUNTS);\n      if (!mounts.isEmpty()) {\n        for (String mount : StringUtils.split(mounts)) {\n          String[] dir = StringUtils.split(mount, ':');\n          if (dir.length != 2) {\n            throw new ContainerExecutionException(\"Invalid mount : \" +\n                mount);\n          }\n          String src = validateMount(dir[0], localizedResources);\n          String dst = dir[1];\n          runCommand.addReadOnlyMountLocation(src, dst, true);\n        }\n      }\n    }\n\n    if (environment.containsKey(ENV_DOCKER_CONTAINER_MOUNTS)) {\n      Matcher parsedMounts = USER_MOUNT_PATTERN.matcher(\n          environment.get(ENV_DOCKER_CONTAINER_MOUNTS));\n      if (!parsedMounts.find()) {\n        throw new ContainerExecutionException(\n            \"Unable to parse user supplied mount list: \"\n                + environment.get(ENV_DOCKER_CONTAINER_MOUNTS));\n      }\n      parsedMounts.reset();\n      while (parsedMounts.find()) {\n        String src = parsedMounts.group(1);\n        String dst = parsedMounts.group(2);\n        String mode = parsedMounts.group(3);\n        if (!mode.equals(\"ro\") && !mode.equals(\"rw\")) {\n          throw new ContainerExecutionException(\n              \"Invalid mount mode requested for mount: \"\n                  + parsedMounts.group());\n        }\n        if (mode.equals(\"ro\")) {\n          runCommand.addReadOnlyMountLocation(src, dst);\n        } else {\n          runCommand.addReadWriteMountLocation(src, dst);\n        }\n      }\n    }\n\n    if (allowHostPidNamespace(container)) {\n      runCommand.setPidNamespace(\"host\");\n    }\n\n    if (allowPrivilegedContainerExecution(container)) {\n      runCommand.setPrivileged();\n    }\n\n    addDockerClientConfigToRunCommand(ctx, runCommand);\n\n    String resourcesOpts = ctx.getExecutionAttribute(RESOURCES_OPTIONS);\n\n    addCGroupParentIfRequired(resourcesOpts, containerIdStr, runCommand);\n\n    String disableOverride = environment.get(\n        ENV_DOCKER_CONTAINER_RUN_OVERRIDE_DISABLE);\n\n    if (disableOverride != null && disableOverride.equals(\"true\")) {\n      LOG.info(\"command override disabled\");\n    } else {\n      List<String> overrideCommands = new ArrayList<>();\n      Path launchDst =\n          new Path(containerWorkDir, ContainerLaunch.CONTAINER_SCRIPT);\n\n      overrideCommands.add(\"bash\");\n      overrideCommands.add(launchDst.toUri().getPath());\n      runCommand.setOverrideCommandWithArgs(overrideCommands);\n    }\n\n    if(enableUserReMapping) {\n      if (!allowPrivilegedContainerExecution(container)) {\n        runCommand.groupAdd(groups);\n      }\n    }\n\n    // use plugins to update docker run command.\n    if (nmContext != null\n        && nmContext.getResourcePluginManager().getNameToPlugins() != null) {\n      for (ResourcePlugin plugin : nmContext.getResourcePluginManager()\n          .getNameToPlugins().values()) {\n        DockerCommandPlugin dockerCommandPlugin =\n            plugin.getDockerCommandPluginInstance();\n        if (dockerCommandPlugin != null) {\n          dockerCommandPlugin.updateDockerRunCommand(runCommand, container);\n        }\n      }\n    }\n\n    String commandFile = dockerClient.writeCommandToTempFile(runCommand,\n        containerId, nmContext);\n    PrivilegedOperation launchOp = buildLaunchOp(ctx,\n        commandFile, runCommand);\n\n    // Some failures here are acceptable. Let the calling executor decide.\n    launchOp.disableFailureLogging();\n\n    try {\n      privilegedOperationExecutor.executePrivilegedOperation(null,\n          launchOp, null, null, false, false);\n    } catch (PrivilegedOperationException e) {\n      throw new ContainerExecutionException(\"Launch container failed\", e\n          .getExitCode(), e.getOutput(), e.getErrorOutput());\n    }\n  }"
            },
            "hadoop-yarn-project.hadoop-yarn.hadoop-yarn-server.hadoop-yarn-server-nodemanager.src.main.java.org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DefaultLinuxContainerRuntime.launchContainer": {
                "code_before_change": "  public void launchContainer(ContainerRuntimeContext ctx)\n      throws ContainerExecutionException {\n    PrivilegedOperation launchOp = new PrivilegedOperation(\n        PrivilegedOperation.OperationType.LAUNCH_CONTAINER);\n\n    //All of these arguments are expected to be available in the runtime context\n    launchOp.appendArgs(ctx.getExecutionAttribute(RUN_AS_USER),\n        ctx.getExecutionAttribute(USER),\n        Integer.toString(PrivilegedOperation.\n            RunAsUserCommand.LAUNCH_CONTAINER.getValue()),\n        ctx.getExecutionAttribute(APPID),\n        ctx.getExecutionAttribute(CONTAINER_ID_STR),\n        ctx.getExecutionAttribute(CONTAINER_WORK_DIR).toString(),\n        ctx.getExecutionAttribute(NM_PRIVATE_CONTAINER_SCRIPT_PATH).toUri()\n            .getPath(),\n        ctx.getExecutionAttribute(NM_PRIVATE_TOKENS_PATH).toUri().getPath(),\n        ctx.getExecutionAttribute(PID_FILE_PATH).toString(),\n        StringUtils.join(PrivilegedOperation.LINUX_FILE_PATH_SEPARATOR,\n            ctx.getExecutionAttribute(LOCAL_DIRS)),\n        StringUtils.join(PrivilegedOperation.LINUX_FILE_PATH_SEPARATOR,\n            ctx.getExecutionAttribute(LOG_DIRS)),\n        ctx.getExecutionAttribute(RESOURCES_OPTIONS));\n\n    String tcCommandFile = ctx.getExecutionAttribute(TC_COMMAND_FILE);\n\n    if (tcCommandFile != null) {\n      launchOp.appendArgs(tcCommandFile);\n    }\n\n    //List<String> -> stored as List -> fetched/converted to List<String>\n    //we can't do better here thanks to type-erasure\n    @SuppressWarnings(\"unchecked\")\n    List<String> prefixCommands = (List<String>) ctx.getExecutionAttribute(\n        CONTAINER_LAUNCH_PREFIX_COMMANDS);\n\n    try {\n      privilegedOperationExecutor.executePrivilegedOperation(prefixCommands,\n            launchOp, null, null, false, false);\n    } catch (PrivilegedOperationException e) {\n      LOG.warn(\"Launch container failed. Exception: \", e);\n\n      throw new ContainerExecutionException(\"Launch container failed\", e\n          .getExitCode(), e.getOutput(), e.getErrorOutput());\n    }\n  }",
                "code_after_change": "  public void launchContainer(ContainerRuntimeContext ctx)\n      throws ContainerExecutionException {\n    PrivilegedOperation launchOp = new PrivilegedOperation(\n        PrivilegedOperation.OperationType.LAUNCH_CONTAINER);\n\n    //All of these arguments are expected to be available in the runtime context\n    launchOp.appendArgs(ctx.getExecutionAttribute(RUN_AS_USER),\n        ctx.getExecutionAttribute(USER),\n        Integer.toString(PrivilegedOperation.\n            RunAsUserCommand.LAUNCH_CONTAINER.getValue()),\n        ctx.getExecutionAttribute(APPID),\n        ctx.getExecutionAttribute(CONTAINER_ID_STR),\n        ctx.getExecutionAttribute(CONTAINER_WORK_DIR).toString(),\n        ctx.getExecutionAttribute(NM_PRIVATE_CONTAINER_SCRIPT_PATH).toUri()\n            .getPath(),\n        ctx.getExecutionAttribute(NM_PRIVATE_TOKENS_PATH).toUri().getPath(),\n        ctx.getExecutionAttribute(PID_FILE_PATH).toString(),\n        StringUtils.join(PrivilegedOperation.LINUX_FILE_PATH_SEPARATOR,\n            ctx.getExecutionAttribute(LOCAL_DIRS)),\n        StringUtils.join(PrivilegedOperation.LINUX_FILE_PATH_SEPARATOR,\n            ctx.getExecutionAttribute(LOG_DIRS)),\n        ctx.getExecutionAttribute(RESOURCES_OPTIONS));\n\n    String tcCommandFile = ctx.getExecutionAttribute(TC_COMMAND_FILE);\n\n    if (tcCommandFile != null) {\n      launchOp.appendArgs(tcCommandFile);\n    }\n\n    // Some failures here are acceptable. Let the calling executor decide.\n    launchOp.disableFailureLogging();\n\n    //List<String> -> stored as List -> fetched/converted to List<String>\n    //we can't do better here thanks to type-erasure\n    @SuppressWarnings(\"unchecked\")\n    List<String> prefixCommands = (List<String>) ctx.getExecutionAttribute(\n        CONTAINER_LAUNCH_PREFIX_COMMANDS);\n\n    try {\n      privilegedOperationExecutor.executePrivilegedOperation(prefixCommands,\n            launchOp, null, null, false, false);\n    } catch (PrivilegedOperationException e) {\n      throw new ContainerExecutionException(\"Launch container failed\", e\n          .getExitCode(), e.getOutput(), e.getErrorOutput());\n    }\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Preventive",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the root cause as a failure in the container launch process due to a SIGTERM signal, which is related to resource constraints or improper handling of container states. This is partially correct as it shares the stack trace context with the ground truth method 'DefaultLinuxContainerRuntime.launchContainer'. The fix suggestion is preventive as it suggests ensuring adequate resources and modifying container state management, which would mitigate the issue. The problem location identification is partial because it mentions methods like 'DefaultLinuxContainerRuntime.launchContainer' that are in the shared stack trace context with the ground truth methods. There is no wrong information in the bug report as all statements are relevant to the context of the bug."
        }
    }
]