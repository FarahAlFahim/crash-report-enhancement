[
    {
        "filename": "STORM-2443.json",
        "code_diff": {
            "storm-server.src.main.java.org.apache.storm.daemon.nimbus.Nimbus.setLogConfig": {
                "code_before_change": [],
                "code_after_change": "    public void setLogConfig(String topoId, LogConfig config) throws TException {\n        try {\n            setLogConfigCalls.mark();\n            Map<String, Object> topoConf = tryReadTopoConf(topoId, blobStore);\n            topoConf = merge(conf, topoConf);\n            String topoName = (String) topoConf.get(Config.TOPOLOGY_NAME);\n            checkAuthorization(topoName, topoConf, \"setLogConfig\");\n            IStormClusterState state = stormClusterState;\n            LogConfig mergedLogConfig = state.topologyLogConfig(topoId, null);\n            if (mergedLogConfig == null) {\n                mergedLogConfig = new LogConfig();\n            }\n\n            if (mergedLogConfig.is_set_named_logger_level()) {\n                Map<String, LogLevel> namedLoggers = mergedLogConfig.get_named_logger_level();\n                for (LogLevel level: namedLoggers.values()) {\n                    level.set_action(LogLevelAction.UNCHANGED);\n                }\n            }\n\n            if (config.is_set_named_logger_level()) {\n                for (Entry<String, LogLevel> entry: config.get_named_logger_level().entrySet()) {\n                    LogLevel logConfig = entry.getValue();\n                    String loggerName = entry.getKey();\n                    LogLevelAction action = logConfig.get_action();\n                    if (loggerName.isEmpty()) {\n                        throw new RuntimeException(\"Named loggers need a valid name. Use ROOT for the root logger\");\n                    }\n                    switch (action) {\n                        case UPDATE:\n                            setLoggerTimeouts(logConfig);\n                            mergedLogConfig.put_to_named_logger_level(loggerName, logConfig);\n                            break;\n                        case REMOVE:\n                            Map<String, LogLevel> nl = mergedLogConfig.get_named_logger_level();\n                            if (nl != null) {\n                                nl.remove(loggerName);\n                            }\n                            break;\n                        default:\n                            //NOOP\n                            break;\n                    }\n                }\n            }\n            LOG.info(\"Setting log config for {}:{}\", topoName, mergedLogConfig);\n            state.setTopologyLogConfig(topoId, mergedLogConfig);\n        } catch (Exception e) {\n            LOG.warn(\"set log config topology exception. (topology id='{}')\", topoId, e);\n            if (e instanceof TException) {\n                throw (TException)e;\n            }\n            throw new RuntimeException(e);\n        }\n    }"
            },
            "storm-client.src.jvm.org.apache.storm.daemon.worker.LogConfigManager.processLogConfigChange": {
                "code_before_change": [],
                "code_after_change": "    public void processLogConfigChange(LogConfig logConfig) {\n        if (null != logConfig) {\n            LOG.debug(\"Processing received log config: {}\", logConfig);\n            TreeMap<String, LogLevel> loggers = new TreeMap<>(logConfig.get_named_logger_level());\n            LoggerContext logContext = (LoggerContext) LogManager.getContext(false);\n            Map<String, LogLevel> newLogConfigs = new HashMap<>();\n            for (Map.Entry<String, LogLevel> entry : loggers.entrySet()) {\n                String msgLoggerName = entry.getKey();\n                msgLoggerName = (\"ROOT\".equalsIgnoreCase(msgLoggerName)) ? LogManager.ROOT_LOGGER_NAME : msgLoggerName;\n                LogLevel loggerLevel = entry.getValue();\n                // the new-timeouts map now contains logger => timeout\n                if (loggerLevel.is_set_reset_log_level_timeout_epoch()) {\n                    LogLevel copy = new LogLevel(loggerLevel);\n                    if (originalLogLevels.containsKey(msgLoggerName)) {\n                        copy.set_reset_log_level(originalLogLevels.get(msgLoggerName).name());\n                    } else {\n                        copy.set_reset_log_level(Level.INFO.name());\n                    }\n\n                    newLogConfigs.put(msgLoggerName, copy);\n                }\n\n            }\n\n            // Look for deleted log timeouts\n            TreeMap<String,LogLevel> latestConf = latestLogConfig.get();\n            if (latestConf != null) {\n                for (String loggerName : latestConf.descendingKeySet()) {\n                    if (! newLogConfigs.containsKey(loggerName)) {\n                        // if we had a timeout, but the timeout is no longer active\n                        setLoggerLevel(logContext, loggerName, latestConf.get(loggerName).get_reset_log_level());\n\n                    }\n                }\n            }\n\n            // apply new log settings we just received\n            // the merged configs are only for the reset logic\n            for (String loggerName : new TreeSet<>(logConfig.get_named_logger_level().keySet())) {\n                LogLevel logLevel = logConfig.get_named_logger_level().get(loggerName);\n                loggerName = (\"ROOT\".equalsIgnoreCase(loggerName)) ? LogManager.ROOT_LOGGER_NAME : loggerName;\n                LogLevelAction action = logLevel.get_action();\n                if (action == LogLevelAction.UPDATE) {\n                    setLoggerLevel(logContext, loggerName, logLevel.get_target_log_level());\n                }\n\n            }\n\n            logContext.updateLoggers();\n            latestLogConfig.set(new TreeMap<>(newLogConfigs));\n            LOG.debug(\"New merged log config is {}\", latestLogConfig.get());\n        }\n    }"
            },
            "storm-client.src.jvm.org.apache.storm.daemon.worker.LogConfigManager.resetLogLevels": {
                "code_before_change": [],
                "code_after_change": "    public void resetLogLevels() {\n        TreeMap<String, LogLevel> latestLogLevelMap = latestLogConfig.get();\n\n        LOG.debug(\"Resetting log levels: Latest log config is {}\", latestLogLevelMap);\n\n        LoggerContext loggerContext = (LoggerContext) LogManager.getContext(false);\n\n        for (String loggerName : latestLogLevelMap.descendingKeySet()) {\n            LogLevel loggerSetting = latestLogLevelMap.get(loggerName);\n            long timeout = loggerSetting.get_reset_log_level_timeout_epoch();\n            String resetLogLevel = loggerSetting.get_reset_log_level();\n            if (timeout < Time.currentTimeMillis()) {\n                LOG.info(\"{}: Resetting level to {}\", loggerName, resetLogLevel);\n                setLoggerLevel(loggerContext, loggerName, resetLogLevel);\n                latestLogConfig.getAndUpdate(input -> {\n                    TreeMap<String, LogLevel> result = new TreeMap<>(input);\n                    result.remove(loggerName);\n                    return result;\n                });\n            }\n        }\n        loggerContext.updateLoggers();\n    }"
            },
            "storm-server.src.main.java.org.apache.storm.daemon.nimbus.Nimbus.validateTopologySize": {
                "code_before_change": [],
                "code_after_change": "    private static void validateTopologySize(Map<String, Object> topoConf, Map<String, Object> nimbusConf, StormTopology topology) throws InvalidTopologyException {\n        int workerCount = ObjectReader.getInt(topoConf.get(Config.TOPOLOGY_WORKERS), 1);\n        Integer allowedWorkers = ObjectReader.getInt(nimbusConf.get(DaemonConfig.NIMBUS_SLOTS_PER_TOPOLOGY), null);\n        int executorsCount = 0;\n        for (Object comp : StormCommon.allComponents(topology).values()) {\n            executorsCount += StormCommon.numStartExecutors(comp);\n        }\n        Integer allowedExecutors = ObjectReader.getInt(nimbusConf.get(DaemonConfig.NIMBUS_EXECUTORS_PER_TOPOLOGY), null);\n        if (allowedExecutors != null && executorsCount > allowedExecutors) {\n            throw new InvalidTopologyException(\"Failed to submit topology. Topology requests more than \" +\n                    allowedExecutors + \" executors.\");\n        }\n        \n        if (allowedWorkers != null && workerCount > allowedWorkers) {\n            throw new InvalidTopologyException(\"Failed to submit topology. Topology requests more than \" +\n                    allowedWorkers + \" workers.\");\n        }\n    }"
            },
            "storm-server.src.main.java.org.apache.storm.daemon.nimbus.Nimbus.setLoggerTimeouts": {
                "code_before_change": [],
                "code_after_change": "    private static void setLoggerTimeouts(LogLevel level) {\n        int timeoutSecs = level.get_reset_log_level_timeout_secs();\n        if (timeoutSecs > 0) {\n            level.set_reset_log_level_timeout_epoch(Time.currentTimeMillis() + Time.secsToMillis(timeoutSecs));\n        } else {\n            level.unset_reset_log_level_timeout_epoch();\n        }\n    }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Preventive",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report precisely identifies the root cause as a NullPointerException in the 'setLogConfig' method of the Nimbus class, which matches the ground truth method. The fix suggestion is preventive as it suggests adding null checks to prevent the NullPointerException, which aligns with the developer's fix of ensuring objects are not null. The problem location is precise as it directly mentions the 'setLogConfig' method, which is part of the ground truth. There is no wrong information in the bug report as all details are consistent with the context of the bug."
        }
    },
    {
        "filename": "STORM-3213.json",
        "code_diff": {
            "storm-server.src.main.java.org.apache.storm.daemon.nimbus.Nimbus.getCommonTopoInfo": {
                "code_before_change": "    private CommonTopoInfo getCommonTopoInfo(String topoId, String operation) throws NotAliveException,\n        AuthorizationException, IOException, InvalidTopologyException {\n        IStormClusterState state = stormClusterState;\n        CommonTopoInfo ret = new CommonTopoInfo();\n        ret.topoConf = tryReadTopoConf(topoId, topoCache);\n        ret.topoName = (String) ret.topoConf.get(Config.TOPOLOGY_NAME);\n        checkAuthorization(ret.topoName, ret.topoConf, operation);\n        ret.topology = tryReadTopology(topoId, topoCache);\n        ret.taskToComponent = StormCommon.stormTaskInfo(ret.topology, ret.topoConf);\n        ret.base = state.stormBase(topoId, null);\n        if (ret.base != null && ret.base.is_set_launch_time_secs()) {\n            ret.launchTimeSecs = ret.base.get_launch_time_secs();\n        } else {\n            ret.launchTimeSecs = 0;\n        }\n        ret.assignment = state.assignmentInfo(topoId, null);\n        //get it from cluster state/zookeeper every time to collect the UI stats, may replace it with other StateStore later\n        ret.beats = ret.assignment != null ? StatsUtil.convertExecutorBeats(state.executorBeats(topoId,\n                                                                                                ret.assignment\n                                                                                                    .get_executor_node_port())) :\n            Collections\n            .emptyMap();\n        ret.allComponents = new HashSet<>(ret.taskToComponent.values());\n        return ret;\n    }",
                "code_after_change": "    private CommonTopoInfo getCommonTopoInfo(String topoId, String operation) throws NotAliveException,\n        AuthorizationException, IOException, InvalidTopologyException {\n        IStormClusterState state = stormClusterState;\n        CommonTopoInfo ret = new CommonTopoInfo();\n        ret.topoConf = tryReadTopoConf(topoId, topoCache);\n        ret.topoName = (String) ret.topoConf.get(Config.TOPOLOGY_NAME);\n        checkAuthorization(ret.topoName, ret.topoConf, operation);\n        StormTopology topology = tryReadTopology(topoId, topoCache);\n        ret.topology = StormCommon.systemTopology(ret.topoConf, topology);\n        ret.taskToComponent = StormCommon.stormTaskInfo(topology, ret.topoConf);\n        ret.base = state.stormBase(topoId, null);\n        if (ret.base != null && ret.base.is_set_launch_time_secs()) {\n            ret.launchTimeSecs = ret.base.get_launch_time_secs();\n        } else {\n            ret.launchTimeSecs = 0;\n        }\n        ret.assignment = state.assignmentInfo(topoId, null);\n        //get it from cluster state/zookeeper every time to collect the UI stats, may replace it with other StateStore later\n        ret.beats = ret.assignment != null ? StatsUtil.convertExecutorBeats(state.executorBeats(topoId,\n                                                                                                ret.assignment\n                                                                                                    .get_executor_node_port())) :\n            Collections\n            .emptyMap();\n        ret.allComponents = new HashSet<>(ret.taskToComponent.values());\n        return ret;\n    }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Buggy Method"
            },
            "fix_suggestion": "Preventive",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Buggy Method"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the root cause as a NullPointerException in the ResourceUtils.getBoltResources method, which is where the error occurs, but not where the actual fix was made. The ground truth method where the fix was applied is Nimbus.getCommonTopoInfo. The fix suggestion is preventive as it suggests adding null checks in ResourceUtils.getBoltResources, which would prevent the error from occurring. The problem location identification is partial, pointing to the method where the error occurs (ResourceUtils.getBoltResources), but not where the fix was applied. There is no wrong information in the bug report as all details are relevant to the context of the bug."
        }
    },
    {
        "filename": "STORM-2496.json",
        "code_diff": {
            "storm-core.src.jvm.org.apache.storm.dependency.DependencyUploader.uploadDependencyToBlobStore": {
                "code_before_change": [],
                "code_after_change": "    private boolean uploadDependencyToBlobStore(String key, File dependency)\n            throws KeyAlreadyExistsException, AuthorizationException, IOException {\n\n        boolean uploadNew = false;\n        try {\n            // FIXME: we can filter by listKeys() with local blobstore when STORM-1986 is going to be resolved\n            // as a workaround, we call getBlobMeta() for all keys\n            getBlobStore().getBlobMeta(key);\n        } catch (KeyNotFoundException e) {\n            // set acl to below so that it can be shared by other users as well, but allows only read\n            List<AccessControl> acls = new ArrayList<>();\n            acls.add(new AccessControl(AccessControlType.USER,\n                    BlobStoreAclHandler.READ | BlobStoreAclHandler.WRITE | BlobStoreAclHandler.ADMIN));\n            acls.add(new AccessControl(AccessControlType.OTHER,\n                    BlobStoreAclHandler.READ));\n\n            AtomicOutputStream blob = getBlobStore().createBlob(key, new SettableBlobMeta(acls));\n            Files.copy(dependency.toPath(), blob);\n            blob.close();\n\n            uploadNew = true;\n        }\n\n        return uploadNew;\n    }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Buggy Method"
            },
            "fix_suggestion": "Alternative Fix",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Buggy Method"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the root cause as the lack of READ permissions for all users on uploaded artifacts, which is related to the method 'Localizer.downloadBlob'. However, the actual root cause is in 'DependencyUploader.uploadDependencyToBlobStore', where the permissions are set. Thus, it is classified as 'Partial' under 'Buggy Method'. The fix suggestion in the report suggests modifying 'downloadBlob' to set permissions, which is an alternative approach to fixing the issue, as the actual fix was made in 'uploadDependencyToBlobStore'. The problem location identification is 'Partial' under 'Buggy Method' because it points to 'Localizer.downloadBlob', which is where the error manifests but not where the fix was applied. There is no wrong information in the report; all details are relevant to the context of the bug."
        }
    },
    {
        "filename": "STORM-2879.json",
        "code_diff": {
            "storm-core.src.jvm.org.apache.storm.daemon.supervisor.Slot.Slot": {
                "code_before_change": [],
                "code_after_change": []
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Buggy Method"
            },
            "fix_suggestion": "Alternative Fix",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Buggy Method"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the root cause as the lack of a transactional mechanism during file deletion, which is related to the method 'Slot.cleanupCurrentContainer' mentioned in the problem location. However, the actual ground truth method is 'storm-core.src.jvm.org.apache.storm.daemon.supervisor.Slot.Slot', making it a 'Buggy Method' identification. The fix suggestion to implement a transactional mechanism is an alternative fix that would likely resolve the issue, as it addresses the problem of incomplete file deletion. The problem location is identified as 'Slot.cleanupCurrentContainer', which is where the error occurs, but not where the actual fix was made, hence classified as 'Buggy Method'. There is no wrong information in the bug report as all details are relevant to the context of the bug."
        }
    },
    {
        "filename": "STORM-3012.json",
        "code_diff": {
            "storm-client.src.jvm.org.apache.storm.pacemaker.PacemakerClient.send": {
                "code_before_change": "    public HBMessage send(HBMessage m) throws InterruptedException {\n        LOG.debug(\"Sending message: {}\", m.toString());\n\n        int next = availableMessageSlots.take();\n        synchronized (m) {\n            m.set_message_id(next);\n            messages[next] = m;\n            LOG.debug(\"Put message in slot: {}\", Integer.toString(next));\n            do {\n                try {\n                    waitUntilReady();\n                    Channel channel = channelRef.get();\n                    if (channel != null) {\n                        channel.write(m);\n                        m.wait(1000);\n                    }\n                } catch (PacemakerConnectionException exp) {\n                    LOG.error(\"error attempting to write to a channel {}\", exp);\n                }\n            } while (messages[next] == m);\n        }\n\n        HBMessage ret = messages[next];\n        if(ret == null) {\n            // This can happen if we lost the connection and subsequently reconnected or timed out.\n            send(m);\n        }\n        messages[next] = null;\n        LOG.debug(\"Got Response: {}\", ret);\n        return ret;\n\n    }",
                "code_after_change": "    public HBMessage send(HBMessage m) throws InterruptedException {\n        LOG.debug(\"Sending message: {}\", m.toString());\n\n        int next = availableMessageSlots.take();\n        synchronized (m) {\n            m.set_message_id(next);\n            messages[next] = m;\n            LOG.debug(\"Put message in slot: {}\", Integer.toString(next));\n            do {\n                try {\n                    waitUntilReady();\n                    Channel channel = channelRef.get();\n                    if (channel != null) {\n                        channel.write(m);\n                        m.wait(1000);\n                    }\n                } catch (PacemakerConnectionException exp) {\n                    LOG.error(\"error attempting to write to a channel {}\", exp);\n                }\n            } while (messages[next] == m);\n        }\n\n        HBMessage ret = messages[next];\n        if (ret == null) {\n            // This can happen if we lost the connection and subsequently reconnected or timed out.\n            LOG.warn(\"Got null response. This can happen if we lost the connection and subsequently reconnected or timed out. \"\n                    + \"Resending message...\");\n            ret = send(m);\n        }\n        messages[next] = null;\n        LOG.debug(\"Got Response: {}\", ret);\n        return ret;\n\n    }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Correct",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report precisely identifies the root cause as the null response handling issue in the 'PacemakerClient.send' method, which matches the ground truth method. The fix suggestion in the report is correct as it aligns with the developer's fix, which involves handling null responses in the 'send' method. The problem location is also precise, as it directly mentions the 'PacemakerClient.send' method, which is the ground truth method. There is no wrong information in the bug report; all details are consistent with the context of the bug."
        }
    },
    {
        "filename": "STORM-3073.json",
        "code_diff": {
            "storm-client.src.jvm.org.apache.storm.executor.spout.SpoutExecutor.getSpoutRecvqCheckSkipCount": {
                "code_before_change": "    public int getSpoutRecvqCheckSkipCount() {\n        if (ackingEnabled) {\n            return 0; // always check recQ if ACKing enabled\n        }\n        return ObjectReader.getInt(conf.get(Config.TOPOLOGY_SPOUT_RECVQ_SKIPS), 0);\n    }",
                "code_after_change": "    public int getSpoutRecvqCheckSkipCount() {\n        if (ackingEnabled) {\n            return 0; // always check recQ if ACKing enabled\n        }\n        return ObjectReader.getInt(conf.get(Config.TOPOLOGY_SPOUT_RECVQ_SKIPS), 0);\n    }"
            },
            "examples.storm-loadgen.src.main.java.org.apache.storm.loadgen.LoadSpout.done": {
                "code_before_change": "        public void done() {\n            histogram.recordValue(Math.max(0, System.nanoTime() - time));\n        }",
                "code_after_change": "        public void done() {\n            histogram.recordValue(Math.max(0, System.nanoTime() - time));\n        }"
            },
            "examples.storm-loadgen.src.main.java.org.apache.storm.loadgen.LoadSpout.ack": {
                "code_before_change": "    public void ack(Object id) {\n        ((SentWithTime)id).done();\n    }",
                "code_after_change": "    public void ack(Object id) {\n        ((SentWithTime)id).done();\n    }"
            },
            "examples.storm-loadgen.src.main.java.org.apache.storm.loadgen.LoadSpout.open": {
                "code_before_change": "    public void open(Map<String, Object> conf, TopologyContext context, SpoutOutputCollector collector) {\n        streams = Collections.unmodifiableList(streamStats.stream()\n            .map((ss) -> new OutputStreamEngineWithHisto(ss, context)).collect(Collectors.toList()));\n        this.collector = collector;\n    }",
                "code_after_change": "    public void open(Map<String, Object> conf, TopologyContext context, SpoutOutputCollector collector) {\n        streams = Collections.unmodifiableList(streamStats.stream()\n            .map((ss) -> new OutputStreamEngineWithHisto(ss, context)).collect(Collectors.toList()));\n        this.collector = collector;\n    }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Buggy Method"
            },
            "fix_suggestion": "Preventive",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Buggy Method"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the issue with the 'pendingEmits' queue becoming full, which is related to the methods mentioned in the stack trace, such as 'LoadSpout.fail'. However, it does not precisely identify the root cause in the ground truth methods. The fix suggestion is preventive as it suggests implementing backpressure mechanisms and reviewing the logic in 'LoadSpout.fail', which could mitigate the issue. The problem location identification is partial because it points to methods like 'LoadSpout.fail', which are related to the problem but not the exact ground truth methods. There is no wrong information in the bug report as all the details are relevant to the context of the bug."
        }
    },
    {
        "filename": "STORM-1672.json",
        "code_diff": {
            "storm-core.src.jvm.org.apache.storm.stats.StatsUtil.filterSysStreams2Stat": {
                "code_before_change": [],
                "code_after_change": "    private static <K, V> Map<K, V> filterSysStreams2Stat(Map<K, V> stream2stat, boolean includeSys) {\n        LOG.trace(\"Filter Sys Streams2Stat {}\", stream2stat);\n        if (!includeSys) {\n            for (Iterator itr = stream2stat.keySet().iterator(); itr.hasNext(); ) {\n                Object key = itr.next();\n                if (key instanceof String && Utils.isSystemId((String) key)) {\n                    itr.remove();\n                }\n            }\n        }\n        return stream2stat;\n    }"
            },
            "storm-core.src.jvm.org.apache.storm.stats.StatsUtil.aggPreMergeCompPageBolt": {
                "code_before_change": "    public static Map<String, Object> aggPreMergeCompPageBolt(Map<String, Object> beat, String window, boolean includeSys) {\n        Map<String, Object> ret = new HashMap<>();\n\n        putKV(ret, EXECUTOR_ID, getByKey(beat, \"exec-id\"));\n        putKV(ret, HOST, getByKey(beat, HOST));\n        putKV(ret, PORT, getByKey(beat, PORT));\n        putKV(ret, UPTIME, getByKey(beat, UPTIME));\n        putKV(ret, NUM_EXECUTORS, 1);\n        putKV(ret, NUM_TASKS, getByKey(beat, NUM_TASKS));\n\n        Map stat2win2sid2num = getMapByKey(beat, STATS);\n        putKV(ret, CAPACITY, computeAggCapacity(stat2win2sid2num, getByKeyOr0(beat, UPTIME).intValue()));\n\n        // calc cid+sid->input_stats\n        Map inputStats = new HashMap();\n        Map sid2acked = (Map) windowSetConverter(getMapByKey(stat2win2sid2num, ACKED), TO_STRING).get(window);\n        Map sid2failed = (Map) windowSetConverter(getMapByKey(stat2win2sid2num, FAILED), TO_STRING).get(window);\n        putKV(inputStats, ACKED, sid2acked != null ? sid2acked : new HashMap());\n        putKV(inputStats, FAILED, sid2failed != null ? sid2failed : new HashMap());\n\n        inputStats = swapMapOrder(inputStats);\n\n        Map sid2execLat = (Map) windowSetConverter(getMapByKey(stat2win2sid2num, EXEC_LATENCIES), TO_STRING).get(window);\n        Map sid2procLat = (Map) windowSetConverter(getMapByKey(stat2win2sid2num, PROC_LATENCIES), TO_STRING).get(window);\n        Map sid2exec = (Map) windowSetConverter(getMapByKey(stat2win2sid2num, EXECUTED), TO_STRING).get(window);\n        mergeMaps(inputStats, aggBoltStreamsLatAndCount(sid2execLat, sid2procLat, sid2exec));\n        putKV(ret, CID_SID_TO_IN_STATS, inputStats);\n\n        // calc sid->output_stats\n        Map outputStats = new HashMap();\n        Map sid2emitted = (Map) windowSetConverter(getMapByKey(stat2win2sid2num, EMITTED), TO_STRING).get(window);\n        Map sid2transferred = (Map) windowSetConverter(getMapByKey(stat2win2sid2num, TRANSFERRED), TO_STRING).get(window);\n        if (sid2emitted != null) {\n            putKV(outputStats, EMITTED, filterSysStreams(sid2emitted, includeSys));\n        } else {\n            putKV(outputStats, EMITTED, new HashMap());\n        }\n        if (sid2transferred != null) {\n            putKV(outputStats, TRANSFERRED, filterSysStreams(sid2transferred, includeSys));\n        } else {\n            putKV(outputStats, TRANSFERRED, new HashMap());\n        }\n        outputStats = swapMapOrder(outputStats);\n        putKV(ret, SID_TO_OUT_STATS, outputStats);\n\n        return ret;\n    }",
                "code_after_change": "    public static Map<String, Object> aggPreMergeCompPageBolt(Map<String, Object> beat, String window, boolean includeSys) {\n        Map<String, Object> ret = new HashMap<>();\n\n        putKV(ret, EXECUTOR_ID, getByKey(beat, \"exec-id\"));\n        putKV(ret, HOST, getByKey(beat, HOST));\n        putKV(ret, PORT, getByKey(beat, PORT));\n        putKV(ret, UPTIME, getByKey(beat, UPTIME));\n        putKV(ret, NUM_EXECUTORS, 1);\n        putKV(ret, NUM_TASKS, getByKey(beat, NUM_TASKS));\n\n        Map stat2win2sid2num = getMapByKey(beat, STATS);\n        putKV(ret, CAPACITY, computeAggCapacity(stat2win2sid2num, getByKeyOr0(beat, UPTIME).intValue()));\n\n        // calc cid+sid->input_stats\n        Map inputStats = new HashMap();\n        Map sid2acked = (Map) windowSetConverter(getMapByKey(stat2win2sid2num, ACKED), TO_STRING).get(window);\n        Map sid2failed = (Map) windowSetConverter(getMapByKey(stat2win2sid2num, FAILED), TO_STRING).get(window);\n        putKV(inputStats, ACKED, sid2acked != null ? sid2acked : new HashMap());\n        putKV(inputStats, FAILED, sid2failed != null ? sid2failed : new HashMap());\n\n        inputStats = swapMapOrder(inputStats);\n\n        Map sid2execLat = (Map) windowSetConverter(getMapByKey(stat2win2sid2num, EXEC_LATENCIES), TO_STRING).get(window);\n        Map sid2procLat = (Map) windowSetConverter(getMapByKey(stat2win2sid2num, PROC_LATENCIES), TO_STRING).get(window);\n        Map sid2exec = (Map) windowSetConverter(getMapByKey(stat2win2sid2num, EXECUTED), TO_STRING).get(window);\n        mergeMaps(inputStats, aggBoltStreamsLatAndCount(sid2execLat, sid2procLat, sid2exec));\n        putKV(ret, CID_SID_TO_IN_STATS, inputStats);\n\n        // calc sid->output_stats\n        Map outputStats = new HashMap();\n        Map sid2emitted = (Map) windowSetConverter(getMapByKey(stat2win2sid2num, EMITTED), TO_STRING).get(window);\n        Map sid2transferred = (Map) windowSetConverter(getMapByKey(stat2win2sid2num, TRANSFERRED), TO_STRING).get(window);\n        if (sid2emitted != null) {\n            putKV(outputStats, EMITTED, filterSysStreams2Stat(sid2emitted, includeSys));\n        } else {\n            putKV(outputStats, EMITTED, new HashMap());\n        }\n        if (sid2transferred != null) {\n            putKV(outputStats, TRANSFERRED, filterSysStreams2Stat(sid2transferred, includeSys));\n        } else {\n            putKV(outputStats, TRANSFERRED, new HashMap());\n        }\n        outputStats = swapMapOrder(outputStats);\n        putKV(ret, SID_TO_OUT_STATS, outputStats);\n\n        return ret;\n    }"
            },
            "storm-core.src.jvm.org.apache.storm.stats.StatsUtil.aggPreMergeCompPageSpout": {
                "code_before_change": "    public static Map<String, Object> aggPreMergeCompPageSpout(Map<String, Object> beat, String window, boolean includeSys) {\n        Map<String, Object> ret = new HashMap<>();\n        putKV(ret, EXECUTOR_ID, getByKey(beat, \"exec-id\"));\n        putKV(ret, HOST, getByKey(beat, HOST));\n        putKV(ret, PORT, getByKey(beat, PORT));\n        putKV(ret, UPTIME, getByKey(beat, UPTIME));\n        putKV(ret, NUM_EXECUTORS, 1);\n        putKV(ret, NUM_TASKS, getByKey(beat, NUM_TASKS));\n\n        Map stat2win2sid2num = getMapByKey(beat, STATS);\n\n        // calc sid->output-stats\n        Map outputStats = new HashMap();\n        Map win2sid2acked = windowSetConverter(getMapByKey(stat2win2sid2num, ACKED), TO_STRING);\n        Map win2sid2failed = windowSetConverter(getMapByKey(stat2win2sid2num, FAILED), TO_STRING);\n        Map win2sid2emitted = windowSetConverter(getMapByKey(stat2win2sid2num, EMITTED), TO_STRING);\n        Map win2sid2transferred = windowSetConverter(getMapByKey(stat2win2sid2num, TRANSFERRED), TO_STRING);\n        Map win2sid2compLat = windowSetConverter(getMapByKey(stat2win2sid2num, COMP_LATENCIES), TO_STRING);\n\n        putKV(outputStats, ACKED, win2sid2acked.get(window));\n        putKV(outputStats, FAILED, win2sid2failed.get(window));\n        putKV(outputStats, EMITTED, filterSysStreams((Map) win2sid2emitted.get(window), includeSys));\n        putKV(outputStats, TRANSFERRED, filterSysStreams((Map) win2sid2transferred.get(window), includeSys));\n        outputStats = swapMapOrder(outputStats);\n\n        Map sid2compLat = (Map) win2sid2compLat.get(window);\n        Map sid2acked = (Map) win2sid2acked.get(window);\n        mergeMaps(outputStats, aggSpoutStreamsLatAndCount(sid2compLat, sid2acked));\n        putKV(ret, SID_TO_OUT_STATS, outputStats);\n\n        return ret;\n    }",
                "code_after_change": "    public static Map<String, Object> aggPreMergeCompPageSpout(Map<String, Object> beat, String window, boolean includeSys) {\n        Map<String, Object> ret = new HashMap<>();\n        putKV(ret, EXECUTOR_ID, getByKey(beat, \"exec-id\"));\n        putKV(ret, HOST, getByKey(beat, HOST));\n        putKV(ret, PORT, getByKey(beat, PORT));\n        putKV(ret, UPTIME, getByKey(beat, UPTIME));\n        putKV(ret, NUM_EXECUTORS, 1);\n        putKV(ret, NUM_TASKS, getByKey(beat, NUM_TASKS));\n\n        Map stat2win2sid2num = getMapByKey(beat, STATS);\n\n        // calc sid->output-stats\n        Map outputStats = new HashMap();\n        Map win2sid2acked = windowSetConverter(getMapByKey(stat2win2sid2num, ACKED), TO_STRING);\n        Map win2sid2failed = windowSetConverter(getMapByKey(stat2win2sid2num, FAILED), TO_STRING);\n        Map win2sid2emitted = windowSetConverter(getMapByKey(stat2win2sid2num, EMITTED), TO_STRING);\n        Map win2sid2transferred = windowSetConverter(getMapByKey(stat2win2sid2num, TRANSFERRED), TO_STRING);\n        Map win2sid2compLat = windowSetConverter(getMapByKey(stat2win2sid2num, COMP_LATENCIES), TO_STRING);\n\n        putKV(outputStats, ACKED, win2sid2acked.get(window));\n        putKV(outputStats, FAILED, win2sid2failed.get(window));\n        Map<String, Long> sid2emitted = (Map) win2sid2emitted.get(window);\n        if (sid2emitted == null) {\n            sid2emitted = new HashMap<>();\n        }\n        putKV(outputStats, EMITTED, filterSysStreams2Stat(sid2emitted, includeSys));\n\n        Map<String, Long> sid2transferred = (Map) win2sid2transferred.get(window);\n        if (sid2transferred == null) {\n            sid2transferred = new HashMap<>();\n        }\n        putKV(outputStats, TRANSFERRED, filterSysStreams2Stat(sid2transferred, includeSys));\n        outputStats = swapMapOrder(outputStats);\n\n        Map sid2compLat = (Map) win2sid2compLat.get(window);\n        Map sid2acked = (Map) win2sid2acked.get(window);\n        mergeMaps(outputStats, aggSpoutStreamsLatAndCount(sid2compLat, sid2acked));\n        putKV(ret, SID_TO_OUT_STATS, outputStats);\n\n        return ret;\n    }"
            },
            "storm-core.src.jvm.org.apache.storm.stats.StatsUtil.mergeMaps": {
                "code_before_change": "    private static Map mergeMaps(Map m1, Map m2) {\n        if (m2 == null) {\n            return m1;\n        }\n        for (Object o : m2.entrySet()) {\n            Map.Entry entry = (Map.Entry) o;\n            Object k = entry.getKey();\n\n            Map existing = (Map) m1.get(k);\n            if (existing == null) {\n                m1.put(k, entry.getValue());\n            } else {\n                existing.putAll((Map) m2.get(k));\n            }\n        }\n        return m1;\n    }",
                "code_after_change": "    private static Map mergeMaps(Map m1, Map m2) {\n        if (m2 == null) {\n            return m1;\n        }\n        for (Object o : m2.entrySet()) {\n            Map.Entry entry = (Map.Entry) o;\n            Object k = entry.getKey();\n\n            Map existing = (Map) m1.get(k);\n            if (existing == null) {\n                m1.put(k, entry.getValue());\n            } else {\n                existing.putAll((Map) m2.get(k));\n            }\n        }\n        return m1;\n    }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Buggy Method"
            },
            "fix_suggestion": "Alternative Fix",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Buggy Method"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the method 'filterSysStreams' as the location of the ClassCastException, which is indeed where the error occurs, but not where the fix was applied. The actual fix involved changing the method calls in 'aggPreMergeCompPageBolt' and 'aggPreMergeCompPageSpout' to use 'filterSysStreams2Stat'. Therefore, the root cause identification is partial, pointing to the buggy method. The fix suggestion is an alternative fix because it suggests reviewing data structures, which could resolve the issue but does not match the exact developer's fix. The problem location identification is partial, as it points to the method where the error occurs but not where the fix was made. There is no wrong information in the bug report as all details are relevant to the context of the bug."
        }
    },
    {
        "filename": "STORM-1977.json",
        "code_diff": {
            "storm-core.src.jvm.org.apache.storm.zookeeper.Zookeeper.zkLeaderElectorImpl": {
                "code_before_change": "    protected ILeaderElector zkLeaderElectorImpl(Map conf) throws UnknownHostException {\n        List<String> servers = (List<String>) conf.get(Config.STORM_ZOOKEEPER_SERVERS);\n        Object port = conf.get(Config.STORM_ZOOKEEPER_PORT);\n        CuratorFramework zk = mkClientImpl(conf, servers, port, \"\", conf);\n        String leaderLockPath = conf.get(Config.STORM_ZOOKEEPER_ROOT) + \"/leader-lock\";\n        String id = NimbusInfo.fromConf(conf).toHostPortString();\n        AtomicReference<LeaderLatch> leaderLatchAtomicReference = new AtomicReference<>(new LeaderLatch(zk, leaderLockPath, id));\n        AtomicReference<LeaderLatchListener> leaderLatchListenerAtomicReference =\n                new AtomicReference<>(leaderLatchListenerImpl(conf, zk, leaderLatchAtomicReference.get()));\n        return new LeaderElectorImp(conf, servers, zk, leaderLockPath, id, leaderLatchAtomicReference, leaderLatchListenerAtomicReference);\n    }",
                "code_after_change": "    protected ILeaderElector zkLeaderElectorImpl(Map conf, BlobStore blobStore) throws UnknownHostException {\n        List<String> servers = (List<String>) conf.get(Config.STORM_ZOOKEEPER_SERVERS);\n        Object port = conf.get(Config.STORM_ZOOKEEPER_PORT);\n        CuratorFramework zk = mkClientImpl(conf, servers, port, \"\", conf);\n        String leaderLockPath = conf.get(Config.STORM_ZOOKEEPER_ROOT) + \"/leader-lock\";\n        String id = NimbusInfo.fromConf(conf).toHostPortString();\n        AtomicReference<LeaderLatch> leaderLatchAtomicReference = new AtomicReference<>(new LeaderLatch(zk, leaderLockPath, id));\n        AtomicReference<LeaderLatchListener> leaderLatchListenerAtomicReference =\n                new AtomicReference<>(leaderLatchListenerImpl(conf, zk, blobStore, leaderLatchAtomicReference.get()));\n        return new LeaderElectorImp(conf, servers, zk, leaderLockPath, id, leaderLatchAtomicReference,\n            leaderLatchListenerAtomicReference, blobStore);\n    }"
            },
            "storm-core.src.jvm.org.apache.storm.zookeeper.LeaderElectorImp.addToLeaderLockQueue": {
                "code_before_change": "    public void addToLeaderLockQueue() throws Exception {\n        // if this latch is already closed, we need to create new instance.\n        if (LeaderLatch.State.CLOSED.equals(leaderLatch.get().getState())) {\n            leaderLatch.set(new LeaderLatch(zk, leaderlockPath));\n            leaderLatchListener.set(Zookeeper.leaderLatchListenerImpl(conf, zk, leaderLatch.get()));\n            LOG.info(\"LeaderLatch was in closed state. Resetted the leaderLatch and listeners.\");\n        }\n        // Only if the latch is not already started we invoke start\n        if (LeaderLatch.State.LATENT.equals(leaderLatch.get().getState())) {\n            leaderLatch.get().addListener(leaderLatchListener.get());\n            leaderLatch.get().start();\n            LOG.info(\"Queued up for leader lock.\");\n        } else {\n            LOG.info(\"Node already in queue for leader lock.\");\n        }\n    }",
                "code_after_change": "    public void addToLeaderLockQueue() throws Exception {\n        // if this latch is already closed, we need to create new instance.\n        if (LeaderLatch.State.CLOSED.equals(leaderLatch.get().getState())) {\n            leaderLatch.set(new LeaderLatch(zk, leaderlockPath));\n            leaderLatchListener.set(Zookeeper.leaderLatchListenerImpl(conf, zk, blobStore, leaderLatch.get()));\n            LOG.info(\"LeaderLatch was in closed state. Resetted the leaderLatch and listeners.\");\n        }\n        // Only if the latch is not already started we invoke start\n        if (LeaderLatch.State.LATENT.equals(leaderLatch.get().getState())) {\n            leaderLatch.get().addListener(leaderLatchListener.get());\n            leaderLatch.get().start();\n            LOG.info(\"Queued up for leader lock.\");\n        } else {\n            LOG.info(\"Node already in queue for leader lock.\");\n        }\n    }"
            },
            "storm-core.src.jvm.org.apache.storm.zookeeper.Zookeeper.toNimbusInfo": {
                "code_before_change": "    public static NimbusInfo toNimbusInfo(Participant participant) {\n        String id = participant.getId();\n        if (StringUtils.isBlank(id)) {\n            throw new RuntimeException(\"No nimbus leader participant host found, have you started your nimbus hosts?\");\n        }\n        NimbusInfo nimbusInfo = NimbusInfo.parse(id);\n        nimbusInfo.setLeader(participant.isLeader());\n        return nimbusInfo;\n    }",
                "code_after_change": "    public static NimbusInfo toNimbusInfo(Participant participant) {\n        String id = participant.getId();\n        if (StringUtils.isBlank(id)) {\n            throw new RuntimeException(\"No nimbus leader participant host found, have you started your nimbus hosts?\");\n        }\n        NimbusInfo nimbusInfo = NimbusInfo.parse(id);\n        nimbusInfo.setLeader(participant.isLeader());\n        return nimbusInfo;\n    }"
            },
            "storm-core.src.jvm.org.apache.storm.zookeeper.LeaderElectorImp.LeaderElectorImp": {
                "code_before_change": [],
                "code_after_change": []
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Alternative Fix",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the root cause as a KeyNotFoundException due to missing topology codes, which is related to the stack trace context but not the exact ground truth methods. The fix suggestion to reintroduce leadership checks or handle exceptions is an alternative fix that could resolve the issue, though it differs from the actual code changes made. The problem location mentions methods in the stack trace context but not the exact ground truth methods. There is no wrong information as the report accurately describes the issue and its context."
        }
    },
    {
        "filename": "STORM-2321.json",
        "code_diff": {
            "storm-core.src.jvm.org.apache.storm.blobstore.KeySequenceNumber.KeySequenceNumber": {
                "code_before_change": [],
                "code_after_change": []
            },
            "storm-core.src.jvm.org.apache.storm.daemon.nimbus.Nimbus.createStateInZookeeper": {
                "code_before_change": "    public void createStateInZookeeper(String key) throws TException {\n        try {\n            IStormClusterState state = stormClusterState;\n            BlobStore store = blobStore;\n            NimbusInfo ni = nimbusHostPortInfo;\n            if (store instanceof LocalFsBlobStore) {\n                state.setupBlobstore(key, ni, getVersionForKey(key, ni, conf));\n            }\n            LOG.debug(\"Created state in zookeeper {} {} {}\", state, store, ni);\n        } catch (Exception e) {\n            LOG.warn(\"Begin file upload exception\", e);\n            if (e instanceof TException) {\n                throw (TException)e;\n            }\n            throw new RuntimeException(e);\n        }\n    }",
                "code_after_change": "    public void createStateInZookeeper(String key) throws TException {\n        try {\n            IStormClusterState state = stormClusterState;\n            BlobStore store = blobStore;\n            NimbusInfo ni = nimbusHostPortInfo;\n            if (store instanceof LocalFsBlobStore) {\n                state.setupBlobstore(key, ni, getVersionForKey(key, ni, conf));\n            }\n            LOG.debug(\"Created state in zookeeper {} {} {}\", state, store, ni);\n        } catch (Exception e) {\n            LOG.warn(\"Exception while creating state in zookeeper - key: \" + key, e);\n            if (e instanceof TException) {\n                throw (TException)e;\n            }\n            throw new RuntimeException(e);\n        }\n    }"
            },
            "storm-core.src.jvm.org.apache.storm.blobstore.LocalFsBlobStore.checkForBlobOrDownload": {
                "code_before_change": "    public synchronized boolean checkForBlobOrDownload(String key) {\n        boolean checkBlobDownload = false;\n        try {\n            List<String> keyList = BlobStoreUtils.getKeyListFromBlobStore(this);\n            if (!keyList.contains(key)) {\n                if (zkClient.checkExists().forPath(BLOBSTORE_SUBTREE + key) != null) {\n                    Set<NimbusInfo> nimbusSet = BlobStoreUtils.getNimbodesWithLatestSequenceNumberOfBlob(zkClient, key);\n                    if (BlobStoreUtils.downloadMissingBlob(conf, this, key, nimbusSet)) {\n                        LOG.debug(\"Updating blobs state\");\n                        BlobStoreUtils.createStateInZookeeper(conf, key, nimbusInfo);\n                        checkBlobDownload = true;\n                    }\n                }\n            }\n        } catch (Exception e) {\n            throw new RuntimeException(e);\n        }\n        return checkBlobDownload;\n    }",
                "code_after_change": "    public synchronized boolean checkForBlobOrDownload(String key) throws KeyNotFoundException {\n        boolean checkBlobDownload = false;\n        try {\n            List<String> keyList = BlobStoreUtils.getKeyListFromBlobStore(this);\n            if (!keyList.contains(key)) {\n                if (zkClient.checkExists().forPath(BLOBSTORE_SUBTREE + key) != null) {\n                    Set<NimbusInfo> nimbusSet = BlobStoreUtils.getNimbodesWithLatestSequenceNumberOfBlob(zkClient, key);\n                    if (BlobStoreUtils.downloadMissingBlob(conf, this, key, nimbusSet)) {\n                        LOG.debug(\"Updating blobs state\");\n                        BlobStoreUtils.createStateInZookeeper(conf, key, nimbusInfo);\n                        checkBlobDownload = true;\n                    }\n                }\n            }\n        } catch (KeyNotFoundException e) {\n            throw e;\n        } catch (Exception e) {\n            throw new RuntimeException(e);\n        }\n        return checkBlobDownload;\n    }"
            },
            "storm-core.src.jvm.org.apache.storm.daemon.nimbus.Nimbus.setupBlobstore": {
                "code_before_change": "    private void setupBlobstore() throws AuthorizationException, KeyNotFoundException {\n        IStormClusterState state = stormClusterState;\n        BlobStore store = blobStore;\n        Set<String> localKeys = new HashSet<>();\n        for (Iterator<String> it = store.listKeys(); it.hasNext();) {\n            localKeys.add(it.next());\n        }\n        Set<String> activeKeys = new HashSet<>(state.activeKeys());\n        Set<String> activeLocalKeys = new HashSet<>(localKeys);\n        activeLocalKeys.retainAll(activeKeys);\n        Set<String> keysToDelete = new HashSet<>(localKeys);\n        keysToDelete.removeAll(activeKeys);\n        NimbusInfo nimbusInfo = nimbusHostPortInfo;\n        LOG.debug(\"Deleting keys not on the zookeeper {}\", keysToDelete);\n        for (String toDelete: keysToDelete) {\n            store.deleteBlob(toDelete, NIMBUS_SUBJECT);\n        }\n        LOG.debug(\"Creating list of key entries for blobstore inside zookeeper {} local {}\", activeKeys, activeLocalKeys);\n        for (String key: activeLocalKeys) {\n            state.setupBlobstore(key, nimbusInfo, getVersionForKey(key, nimbusInfo, conf));\n        }\n    }",
                "code_after_change": "    private void setupBlobstore() throws AuthorizationException, KeyNotFoundException {\n        IStormClusterState state = stormClusterState;\n        BlobStore store = blobStore;\n        Set<String> localKeys = new HashSet<>();\n        for (Iterator<String> it = store.listKeys(); it.hasNext();) {\n            localKeys.add(it.next());\n        }\n        Set<String> activeKeys = new HashSet<>(state.activeKeys());\n        Set<String> activeLocalKeys = new HashSet<>(localKeys);\n        activeLocalKeys.retainAll(activeKeys);\n        Set<String> keysToDelete = new HashSet<>(localKeys);\n        keysToDelete.removeAll(activeKeys);\n        NimbusInfo nimbusInfo = nimbusHostPortInfo;\n        LOG.debug(\"Deleting keys not on the zookeeper {}\", keysToDelete);\n        for (String toDelete: keysToDelete) {\n            store.deleteBlob(toDelete, NIMBUS_SUBJECT);\n        }\n        LOG.debug(\"Creating list of key entries for blobstore inside zookeeper {} local {}\", activeKeys, activeLocalKeys);\n        for (String key: activeLocalKeys) {\n            try {\n                state.setupBlobstore(key, nimbusInfo, getVersionForKey(key, nimbusInfo, conf));\n            } catch (KeyNotFoundException e) {\n                // invalid key, remove it from blobstore\n                store.deleteBlob(key, NIMBUS_SUBJECT);\n            }\n        }\n    }"
            },
            "storm-core.src.jvm.org.apache.storm.daemon.nimbus.Nimbus.getKeyListFromId": {
                "code_before_change": "    static List<String> getKeyListFromId(Map<String, Object> conf, String id) {\n        List<String> ret = new ArrayList<>(3);\n        ret.add(ConfigUtils.masterStormCodeKey(id));\n        ret.add(ConfigUtils.masterStormConfKey(id));\n        if (!ConfigUtils.isLocalMode(conf)) {\n            ret.add(ConfigUtils.masterStormJarKey(id));\n        }\n        return ret;\n    }",
                "code_after_change": "    static List<String> getKeyListFromId(Map<String, Object> conf, String id) {\n        List<String> ret = new ArrayList<>(3);\n        ret.add(ConfigUtils.masterStormCodeKey(id));\n        ret.add(ConfigUtils.masterStormConfKey(id));\n        if (!ConfigUtils.isLocalMode(conf)) {\n            ret.add(ConfigUtils.masterStormJarKey(id));\n        }\n        return ret;\n    }"
            },
            "storm-core.src.jvm.org.apache.storm.blobstore.LocalFsBlobStore.updateBlobReplication": {
                "code_before_change": "    public int updateBlobReplication(String key, int replication, Subject who) throws AuthorizationException, KeyNotFoundException {\n        throw new UnsupportedOperationException(\"For local file system blob store the update blobs function does not work. \" +\n                \"Please use HDFS blob store to make this feature available.\");\n    }",
                "code_after_change": "    public int updateBlobReplication(String key, int replication, Subject who) throws AuthorizationException, KeyNotFoundException {\n        throw new UnsupportedOperationException(\"For local file system blob store the update blobs function does not work. \" +\n                \"Please use HDFS blob store to make this feature available.\");\n    }"
            },
            "storm-core.src.jvm.org.apache.storm.blobstore.BlobSynchronizer.syncBlobs": {
                "code_before_change": "    public synchronized void syncBlobs() {\n        try {\n            LOG.debug(\"Sync blobs - blobstore keys {}, zookeeper keys {}\",getBlobStoreKeySet(), getZookeeperKeySet());\n            zkClient = BlobStoreUtils.createZKClient(conf);\n            deleteKeySetFromBlobStoreNotOnZookeeper(getBlobStoreKeySet(), getZookeeperKeySet());\n            updateKeySetForBlobStore(getBlobStoreKeySet());\n            Set<String> keySetToDownload = getKeySetToDownload(getBlobStoreKeySet(), getZookeeperKeySet());\n            LOG.debug(\"Key set Blobstore-> Zookeeper-> DownloadSet {}-> {}-> {}\", getBlobStoreKeySet(), getZookeeperKeySet(), keySetToDownload);\n\n            for (String key : keySetToDownload) {\n                Set<NimbusInfo> nimbusInfoSet = BlobStoreUtils.getNimbodesWithLatestSequenceNumberOfBlob(zkClient, key);\n                if(BlobStoreUtils.downloadMissingBlob(conf, blobStore, key, nimbusInfoSet)) {\n                    BlobStoreUtils.createStateInZookeeper(conf, key, nimbusInfo);\n                }\n            }\n            if (zkClient !=null) {\n                zkClient.close();\n            }\n        } catch(InterruptedException | ClosedByInterruptException exp) {\n            LOG.error(\"Interrupt Exception {}\", exp);\n        } catch(Exception exp) {\n            throw new RuntimeException(exp);\n        }\n    }",
                "code_after_change": "    public synchronized void syncBlobs() {\n        try {\n            LOG.debug(\"Sync blobs - blobstore keys {}, zookeeper keys {}\",getBlobStoreKeySet(), getZookeeperKeySet());\n            zkClient = BlobStoreUtils.createZKClient(conf);\n            deleteKeySetFromBlobStoreNotOnZookeeper(getBlobStoreKeySet(), getZookeeperKeySet());\n            updateKeySetForBlobStore(getBlobStoreKeySet());\n            Set<String> keySetToDownload = getKeySetToDownload(getBlobStoreKeySet(), getZookeeperKeySet());\n            LOG.debug(\"Key set Blobstore-> Zookeeper-> DownloadSet {}-> {}-> {}\", getBlobStoreKeySet(), getZookeeperKeySet(), keySetToDownload);\n\n            for (String key : keySetToDownload) {\n                try {\n                    Set<NimbusInfo> nimbusInfoSet = BlobStoreUtils.getNimbodesWithLatestSequenceNumberOfBlob(zkClient, key);\n                    if (BlobStoreUtils.downloadMissingBlob(conf, blobStore, key, nimbusInfoSet)) {\n                        BlobStoreUtils.createStateInZookeeper(conf, key, nimbusInfo);\n                    }\n                } catch (KeyNotFoundException e) {\n                    LOG.debug(\"Detected deletion for the key {} - deleting the blob instead\", key);\n                    // race condition with a delete, delete the blob in key instead\n                    blobStore.deleteBlob(key, BlobStoreUtils.getNimbusSubject());\n                }\n            }\n            if (zkClient !=null) {\n                zkClient.close();\n            }\n        } catch(InterruptedException | ClosedByInterruptException exp) {\n            LOG.error(\"Interrupt Exception {}\", exp);\n        } catch(Exception exp) {\n            throw new RuntimeException(exp);\n        }\n    }"
            },
            "storm-core.src.jvm.org.apache.storm.blobstore.KeySequenceNumber.getKeySequenceNumber": {
                "code_before_change": "    public synchronized int getKeySequenceNumber(Map conf) {\n        TreeSet<Integer> sequenceNumbers = new TreeSet<Integer>();\n        CuratorFramework zkClient = BlobStoreUtils.createZKClient(conf);\n        try {\n            // Key has not been created yet and it is the first time it is being created\n            if(zkClient.checkExists().forPath(BLOBSTORE_SUBTREE + \"/\" + key) == null) {\n                zkClient.create().creatingParentsIfNeeded().withMode(CreateMode.PERSISTENT)\n                        .withACL(ZooDefs.Ids.OPEN_ACL_UNSAFE).forPath(BLOBSTORE_MAX_KEY_SEQUENCE_SUBTREE + \"/\" + key);\n                zkClient.setData().forPath(BLOBSTORE_MAX_KEY_SEQUENCE_SUBTREE + \"/\" + key,\n                        ByteBuffer.allocate(INT_CAPACITY).putInt(INITIAL_SEQUENCE_NUMBER).array());\n                return INITIAL_SEQUENCE_NUMBER;\n            }\n\n            // When all nimbodes go down and one or few of them come up\n            // Unfortunately there might not be an exact way to know which one contains the most updated blob,\n            // if all go down which is unlikely. Hence there might be a need to update the blob if all go down.\n            List<String> stateInfoList = zkClient.getChildren().forPath(BLOBSTORE_SUBTREE + \"/\" + key);\n            LOG.debug(\"stateInfoList-size {} stateInfoList-data {}\", stateInfoList.size(), stateInfoList);\n            if(stateInfoList.isEmpty()) {\n                return getMaxSequenceNumber(zkClient);\n            }\n\n            LOG.debug(\"stateInfoSize {}\", stateInfoList.size());\n            // In all other cases check for the latest update sequence of the blob on the nimbus\n            // and assign the appropriate number. Check if all are have same sequence number,\n            // if not assign the highest sequence number.\n            for (String stateInfo:stateInfoList) {\n                sequenceNumbers.add(Integer.parseInt(BlobStoreUtils.normalizeNimbusHostPortSequenceNumberInfo(stateInfo)\n                        .getSequenceNumber()));\n            }\n\n            // Update scenario 2 and 3 explain the code logic written here\n            // especially when nimbus crashes and comes up after and before update\n            // respectively.\n            int currentSeqNumber = getMaxSequenceNumber(zkClient);\n            if (!checkIfStateContainsCurrentNimbusHost(stateInfoList, nimbusInfo) && !nimbusInfo.isLeader()) {\n                if (sequenceNumbers.last() < currentSeqNumber) {\n                    return currentSeqNumber;\n                } else {\n                    return INITIAL_SEQUENCE_NUMBER - 1;\n                }\n            }\n\n            // It covers scenarios expalined in scenario 3 when nimbus-1 holding the latest\n            // update goes down before it is downloaded by nimbus-2. Nimbus-2 gets elected as a leader\n            // after which nimbus-1 comes back up and a read or update is performed.\n            if (!checkIfStateContainsCurrentNimbusHost(stateInfoList, nimbusInfo) && nimbusInfo.isLeader()) {\n                incrementMaxSequenceNumber(zkClient, currentSeqNumber);\n                return currentSeqNumber + 1;\n            }\n\n            // This code logic covers the update scenarios in 2 when the nimbus-1 goes down\n            // before syncing the blob to nimbus-2 and an update happens.\n            // If seq-num for nimbus-2 is 2 and max-seq-number is 3 then next sequence number is 4\n            // (max-seq-number + 1).\n            // Other scenario it covers is when max-seq-number and nimbus seq number are equal.\n            if (sequenceNumbers.size() == 1) {\n                if (sequenceNumbers.first() < currentSeqNumber) {\n                    incrementMaxSequenceNumber(zkClient, currentSeqNumber);\n                    return currentSeqNumber + 1;\n                } else {\n                    incrementMaxSequenceNumber(zkClient, currentSeqNumber);\n                    return sequenceNumbers.first() + 1;\n                }\n            }\n        } catch(Exception e) {\n            LOG.error(\"Exception {}\", e);\n        } finally {\n            if (zkClient != null) {\n                zkClient.close();\n            }\n        }\n        // Normal create update sync scenario returns the greatest sequence number in the set\n        return sequenceNumbers.last();\n    }",
                "code_after_change": "    public synchronized int getKeySequenceNumber(Map conf) throws KeyNotFoundException {\n        TreeSet<Integer> sequenceNumbers = new TreeSet<Integer>();\n        CuratorFramework zkClient = BlobStoreUtils.createZKClient(conf);\n        try {\n            // Key has not been created yet and it is the first time it is being created\n            if (zkClient.checkExists().forPath(BLOBSTORE_SUBTREE + \"/\" + key) == null) {\n                zkClient.create().creatingParentsIfNeeded().withMode(CreateMode.PERSISTENT)\n                        .withACL(ZooDefs.Ids.OPEN_ACL_UNSAFE).forPath(BLOBSTORE_MAX_KEY_SEQUENCE_SUBTREE + \"/\" + key);\n                zkClient.setData().forPath(BLOBSTORE_MAX_KEY_SEQUENCE_SUBTREE + \"/\" + key,\n                        ByteBuffer.allocate(INT_CAPACITY).putInt(INITIAL_SEQUENCE_NUMBER).array());\n                return INITIAL_SEQUENCE_NUMBER;\n            }\n\n            // When all nimbodes go down and one or few of them come up\n            // Unfortunately there might not be an exact way to know which one contains the most updated blob,\n            // if all go down which is unlikely. Hence there might be a need to update the blob if all go down.\n            List<String> stateInfoList = zkClient.getChildren().forPath(BLOBSTORE_SUBTREE + \"/\" + key);\n            LOG.debug(\"stateInfoList-size {} stateInfoList-data {}\", stateInfoList.size(), stateInfoList);\n            if (stateInfoList.isEmpty()) {\n                return getMaxSequenceNumber(zkClient);\n            }\n\n            LOG.debug(\"stateInfoSize {}\", stateInfoList.size());\n            // In all other cases check for the latest update sequence of the blob on the nimbus\n            // and assign the appropriate number. Check if all are have same sequence number,\n            // if not assign the highest sequence number.\n            for (String stateInfo : stateInfoList) {\n                sequenceNumbers.add(Integer.parseInt(BlobStoreUtils.normalizeNimbusHostPortSequenceNumberInfo(stateInfo)\n                        .getSequenceNumber()));\n            }\n\n            // Update scenario 2 and 3 explain the code logic written here\n            // especially when nimbus crashes and comes up after and before update\n            // respectively.\n            int currentSeqNumber = getMaxSequenceNumber(zkClient);\n            if (!checkIfStateContainsCurrentNimbusHost(stateInfoList, nimbusInfo) && !nimbusInfo.isLeader()) {\n                if (sequenceNumbers.last() < currentSeqNumber) {\n                    return currentSeqNumber;\n                } else {\n                    return INITIAL_SEQUENCE_NUMBER - 1;\n                }\n            }\n\n            // It covers scenarios expalined in scenario 3 when nimbus-1 holding the latest\n            // update goes down before it is downloaded by nimbus-2. Nimbus-2 gets elected as a leader\n            // after which nimbus-1 comes back up and a read or update is performed.\n            if (!checkIfStateContainsCurrentNimbusHost(stateInfoList, nimbusInfo) && nimbusInfo.isLeader()) {\n                incrementMaxSequenceNumber(zkClient, currentSeqNumber);\n                return currentSeqNumber + 1;\n            }\n\n            // This code logic covers the update scenarios in 2 when the nimbus-1 goes down\n            // before syncing the blob to nimbus-2 and an update happens.\n            // If seq-num for nimbus-2 is 2 and max-seq-number is 3 then next sequence number is 4\n            // (max-seq-number + 1).\n            // Other scenario it covers is when max-seq-number and nimbus seq number are equal.\n            if (sequenceNumbers.size() == 1) {\n                if (sequenceNumbers.first() < currentSeqNumber) {\n                    incrementMaxSequenceNumber(zkClient, currentSeqNumber);\n                    return currentSeqNumber + 1;\n                } else {\n                    incrementMaxSequenceNumber(zkClient, currentSeqNumber);\n                    return sequenceNumbers.first() + 1;\n                }\n            }\n\n            // Normal create update sync scenario returns the greatest sequence number in the set\n            return sequenceNumbers.last();\n        } catch (KeeperException.NoNodeException e) {\n            // there's a race condition with a delete: either blobstore or blobstoremaxsequence\n            // this should be thrown to the caller to indicate that the key is invalid now\n            throw new KeyNotFoundException(key);\n        } catch(Exception e) {\n            // in other case, just set this to 0 to trigger re-sync later\n            LOG.error(\"Exception {}\", e);\n            return INITIAL_SEQUENCE_NUMBER - 1;\n        } finally {\n            if (zkClient != null) {\n                zkClient.close();\n            }\n        }\n    }"
            },
            "storm-core.src.jvm.org.apache.storm.blobstore.BlobStoreUtils.updateKeyForBlobStore": {
                "code_before_change": "    public static void updateKeyForBlobStore (Map<String, Object> conf, BlobStore blobStore, CuratorFramework zkClient, String key, NimbusInfo nimbusDetails) {\n        try {\n            // Most of clojure tests currently try to access the blobs using getBlob. Since, updateKeyForBlobStore\n            // checks for updating the correct version of the blob as a part of nimbus ha before performing any\n            // operation on it, there is a necessity to stub several test cases to ignore this method. It is a valid\n            // trade off to return if nimbusDetails which include the details of the current nimbus host port data are\n            // not initialized as a part of the test. Moreover, this applies to only local blobstore when used along with\n            // nimbus ha.\n            if (nimbusDetails == null) {\n                return;\n            }\n            boolean isListContainsCurrentNimbusInfo = false;\n            List<String> stateInfo;\n            if (zkClient.checkExists().forPath(BLOBSTORE_SUBTREE + \"/\" + key) == null) {\n                return;\n            }\n            stateInfo = zkClient.getChildren().forPath(BLOBSTORE_SUBTREE + \"/\" + key);\n\n            LOG.debug(\"StateInfo for update {}\", stateInfo);\n            Set<NimbusInfo> nimbusInfoList = getNimbodesWithLatestSequenceNumberOfBlob(zkClient, key);\n\n            for (NimbusInfo nimbusInfo:nimbusInfoList) {\n                if (nimbusInfo.getHost().equals(nimbusDetails.getHost())) {\n                    isListContainsCurrentNimbusInfo = true;\n                    break;\n                }\n            }\n\n            if (!isListContainsCurrentNimbusInfo && downloadUpdatedBlob(conf, blobStore, key, nimbusInfoList)) {\n                LOG.debug(\"Updating state inside zookeeper for an update\");\n                createStateInZookeeper(conf, key, nimbusDetails);\n            }\n        } catch (NoNodeException e) {\n            //race condition with a delete\n            return;\n        } catch (Exception exp) {\n            throw new RuntimeException(exp);\n        }\n    }",
                "code_after_change": "    public static void updateKeyForBlobStore (Map<String, Object> conf, BlobStore blobStore, CuratorFramework zkClient, String key, NimbusInfo nimbusDetails) {\n        try {\n            // Most of clojure tests currently try to access the blobs using getBlob. Since, updateKeyForBlobStore\n            // checks for updating the correct version of the blob as a part of nimbus ha before performing any\n            // operation on it, there is a necessity to stub several test cases to ignore this method. It is a valid\n            // trade off to return if nimbusDetails which include the details of the current nimbus host port data are\n            // not initialized as a part of the test. Moreover, this applies to only local blobstore when used along with\n            // nimbus ha.\n            if (nimbusDetails == null) {\n                return;\n            }\n            boolean isListContainsCurrentNimbusInfo = false;\n            List<String> stateInfo;\n            if (zkClient.checkExists().forPath(BLOBSTORE_SUBTREE + \"/\" + key) == null) {\n                return;\n            }\n            stateInfo = zkClient.getChildren().forPath(BLOBSTORE_SUBTREE + \"/\" + key);\n\n            LOG.debug(\"StateInfo for update {}\", stateInfo);\n            Set<NimbusInfo> nimbusInfoList = getNimbodesWithLatestSequenceNumberOfBlob(zkClient, key);\n\n            for (NimbusInfo nimbusInfo : nimbusInfoList) {\n                if (nimbusInfo.getHost().equals(nimbusDetails.getHost())) {\n                    isListContainsCurrentNimbusInfo = true;\n                    break;\n                }\n            }\n\n            if (!isListContainsCurrentNimbusInfo && downloadUpdatedBlob(conf, blobStore, key, nimbusInfoList)) {\n                LOG.debug(\"Updating state inside zookeeper for an update\");\n                createStateInZookeeper(conf, key, nimbusDetails);\n            }\n        } catch (NoNodeException | KeyNotFoundException e) {\n            //race condition with a delete\n            return;\n        } catch (Exception exp) {\n            throw new RuntimeException(exp);\n        }\n    }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Preventive",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the root cause as a missing node in ZooKeeper, which is related to the ground truth methods as it is mentioned in the stack trace. However, it does not precisely identify the root cause at the exact ground truth method level, hence it is classified as 'Partial' with 'Shared Stack Trace Context'. The fix suggestion is preventive as it suggests ensuring the necessary blobs are present in ZooKeeper before restarting Nimbus, which would prevent the issue but does not match the exact developer's fix. The problem location identification is also 'Partial' with 'Shared Stack Trace Context' because the methods mentioned in the problem location field are part of the stack trace but do not directly match the ground truth methods. There is no wrong information in the bug report as all the details are relevant to the context of the bug."
        }
    },
    {
        "filename": "STORM-3013.json",
        "code_diff": {
            "external.storm-kafka-client.src.main.java.org.apache.storm.kafka.spout.KafkaSpout.fail": {
                "code_before_change": "    public void fail(Object messageId) {\n        if (!isAtLeastOnceProcessing()) {\n            return;\n        }\n        // Only need to keep track of failed tuples if commits to Kafka are controlled by\n        // tuple acks, which happens only for at-least-once processing semantics\n        final KafkaSpoutMessageId msgId = (KafkaSpoutMessageId) messageId;\n        if (!emitted.contains(msgId)) {\n            LOG.debug(\"Received fail for tuple this spout is no longer tracking.\"\n                + \" Partitions may have been reassigned. Ignoring message [{}]\", msgId);\n            return;\n        }\n        Validate.isTrue(!retryService.isScheduled(msgId), \"The message id \" + msgId + \" is queued for retry while being failed.\"\n            + \" This should never occur barring errors in the RetryService implementation or the spout code.\");\n\n        msgId.incrementNumFails();\n\n        if (!retryService.schedule(msgId)) {\n            LOG.debug(\"Reached maximum number of retries. Message [{}] being marked as acked.\", msgId);\n            // this tuple should be removed from emitted only inside the ack() method. This is to ensure\n            // that the OffsetManager for that TopicPartition is updated and allows commit progression\n            tupleListener.onMaxRetryReached(msgId);\n            ack(msgId);\n        } else {\n            tupleListener.onRetry(msgId);\n            emitted.remove(msgId);\n        }\n    }",
                "code_after_change": "    public void fail(Object messageId) {\n        if (!isAtLeastOnceProcessing()) {\n            return;\n        }\n        // Only need to keep track of failed tuples if commits to Kafka are controlled by\n        // tuple acks, which happens only for at-least-once processing semantics\n        final KafkaSpoutMessageId msgId = (KafkaSpoutMessageId) messageId;\n        if (!emitted.contains(msgId)) {\n            LOG.debug(\"Received fail for tuple this spout is no longer tracking.\"\n                + \" Partitions may have been reassigned. Ignoring message [{}]\", msgId);\n            return;\n        }\n        Validate.isTrue(!retryService.isScheduled(msgId), \"The message id \" + msgId + \" is queued for retry while being failed.\"\n            + \" This should never occur barring errors in the RetryService implementation or the spout code.\");\n\n        msgId.incrementNumFails();\n\n        if (!retryService.schedule(msgId)) {\n            LOG.debug(\"Reached maximum number of retries. Message [{}] being marked as acked.\", msgId);\n            // this tuple should be removed from emitted only inside the ack() method. This is to ensure\n            // that the OffsetManager for that TopicPartition is updated and allows commit progression\n            tupleListener.onMaxRetryReached(msgId);\n            ack(msgId);\n        } else {\n            tupleListener.onRetry(msgId);\n            emitted.remove(msgId);\n        }\n    }"
            },
            "external.storm-kafka-client.src.main.java.org.apache.storm.kafka.spout.metrics.KafkaOffsetMetric.KafkaOffsetMetric": {
                "code_before_change": [],
                "code_after_change": []
            },
            "external.storm-kafka-client.src.main.java.org.apache.storm.kafka.spout.KafkaSpout.close": {
                "code_before_change": "    public void close() {\n        try {\n            shutdown();\n        } catch (InterruptException e) {\n            throwKafkaConsumerInterruptedException();\n        }\n    }",
                "code_after_change": "    public void close() {\n        try {\n            shutdown();\n        } catch (InterruptException e) {\n            throwKafkaConsumerInterruptedException();\n        }\n    }"
            },
            "external.storm-kafka-client.src.main.java.org.apache.storm.kafka.spout.KafkaSpout.shouldPoll": {
                "code_before_change": "        public boolean shouldPoll() {\n            return !this.pollablePartitions.isEmpty();\n        }",
                "code_after_change": "        public boolean shouldPoll() {\n            return !this.pollablePartitions.isEmpty();\n        }"
            },
            "external.storm-kafka-client.src.main.java.org.apache.storm.kafka.spout.KafkaSpout.createFetchedOffsetsMetadata": {
                "code_before_change": "    private Map<TopicPartition, OffsetAndMetadata> createFetchedOffsetsMetadata(Set<TopicPartition> assignedPartitions) {\n        Map<TopicPartition, OffsetAndMetadata> offsetsToCommit = new HashMap<>();\n        for (TopicPartition tp : assignedPartitions) {\n            offsetsToCommit.put(tp, new OffsetAndMetadata(kafkaConsumer.position(tp), commitMetadataManager.getCommitMetadata()));\n        }\n        return offsetsToCommit;\n    }",
                "code_after_change": "    private Map<TopicPartition, OffsetAndMetadata> createFetchedOffsetsMetadata(Set<TopicPartition> assignedPartitions) {\n        Map<TopicPartition, OffsetAndMetadata> offsetsToCommit = new HashMap<>();\n        for (TopicPartition tp : assignedPartitions) {\n            offsetsToCommit.put(tp, new OffsetAndMetadata(kafkaConsumer.position(tp), commitMetadataManager.getCommitMetadata()));\n        }\n        return offsetsToCommit;\n    }"
            },
            "external.storm-kafka-client.src.main.java.org.apache.storm.kafka.spout.KafkaSpout.onPartitionsRevoked": {
                "code_before_change": "        public void onPartitionsRevoked(Collection<TopicPartition> partitions) {\n            previousAssignment = partitions;\n\n            LOG.info(\"Partitions revoked. [consumer-group={}, consumer={}, topic-partitions={}]\",\n                kafkaSpoutConfig.getConsumerGroupId(), kafkaConsumer, partitions);\n\n            if (isAtLeastOnceProcessing()) {\n                commitOffsetsForAckedTuples(new HashSet<>(partitions));\n            }\n        }",
                "code_after_change": "        public void onPartitionsRevoked(Collection<TopicPartition> partitions) {\n            previousAssignment = partitions;\n\n            LOG.info(\"Partitions revoked. [consumer-group={}, consumer={}, topic-partitions={}]\",\n                kafkaSpoutConfig.getConsumerGroupId(), kafkaConsumer, partitions);\n\n            if (isAtLeastOnceProcessing()) {\n                commitOffsetsForAckedTuples();\n            }\n        }"
            },
            "external.storm-kafka-client.src.main.java.org.apache.storm.kafka.spout.KafkaSpout.commitOffsetsForAckedTuples": {
                "code_before_change": "    private void commitOffsetsForAckedTuples(Set<TopicPartition> assignedPartitions) {\n        // Find offsets that are ready to be committed for every assigned topic partition\n        final Map<TopicPartition, OffsetManager> assignedOffsetManagers = offsetManagers.entrySet().stream()\n            .filter(entry -> assignedPartitions.contains(entry.getKey()))\n            .collect(Collectors.toMap(Entry::getKey, Entry::getValue));\n\n        final Map<TopicPartition, OffsetAndMetadata> nextCommitOffsets = new HashMap<>();\n        for (Map.Entry<TopicPartition, OffsetManager> tpOffset : assignedOffsetManagers.entrySet()) {\n            final OffsetAndMetadata nextCommitOffset = tpOffset.getValue().findNextCommitOffset(commitMetadataManager.getCommitMetadata());\n            if (nextCommitOffset != null) {\n                nextCommitOffsets.put(tpOffset.getKey(), nextCommitOffset);\n            }\n        }\n\n        // Commit offsets that are ready to be committed for every topic partition\n        if (!nextCommitOffsets.isEmpty()) {\n            kafkaConsumer.commitSync(nextCommitOffsets);\n            LOG.debug(\"Offsets successfully committed to Kafka [{}]\", nextCommitOffsets);\n            // Instead of iterating again, it would be possible to commit and update the state for each TopicPartition\n            // in the prior loop, but the multiple network calls should be more expensive than iterating twice over a small loop\n            for (Map.Entry<TopicPartition, OffsetAndMetadata> tpOffset : nextCommitOffsets.entrySet()) {\n                //Update the OffsetManager for each committed partition, and update numUncommittedOffsets\n                final TopicPartition tp = tpOffset.getKey();\n                long position = kafkaConsumer.position(tp);\n                long committedOffset = tpOffset.getValue().offset();\n                if (position < committedOffset) {\n                    /*\n                     * The position is behind the committed offset. This can happen in some cases, e.g. if a message failed, lots of (more\n                     * than max.poll.records) later messages were acked, and the failed message then gets acked. The consumer may only be\n                     * part way through \"catching up\" to where it was when it went back to retry the failed tuple. Skip the consumer forward\n                     * to the committed offset and drop the current waiting to emit list, since it'll likely contain committed offsets.\n                     */\n                    LOG.debug(\"Consumer fell behind committed offset. Catching up. Position was [{}], skipping to [{}]\",\n                        position, committedOffset);\n                    kafkaConsumer.seek(tp, committedOffset);\n                    List<ConsumerRecord<K, V>> waitingToEmitForTp = waitingToEmit.get(tp);\n                    if (waitingToEmitForTp != null) {\n                        //Discard the pending records that are already committed\n                        waitingToEmit.put(tp, waitingToEmitForTp.stream()\n                            .filter(record -> record.offset() >= committedOffset)\n                            .collect(Collectors.toList()));\n                    }\n                }\n\n                final OffsetManager offsetManager = assignedOffsetManagers.get(tp);\n                offsetManager.commit(tpOffset.getValue());\n                LOG.debug(\"[{}] uncommitted offsets for partition [{}] after commit\", offsetManager.getNumUncommittedOffsets(), tp);\n            }\n        } else {\n            LOG.trace(\"No offsets to commit. {}\", this);\n        }\n    }",
                "code_after_change": "    private void commitOffsetsForAckedTuples() {\n        final Map<TopicPartition, OffsetAndMetadata> nextCommitOffsets = new HashMap<>();\n        for (Map.Entry<TopicPartition, OffsetManager> tpOffset : offsetManagers.entrySet()) {\n            final OffsetAndMetadata nextCommitOffset = tpOffset.getValue().findNextCommitOffset(commitMetadataManager.getCommitMetadata());\n            if (nextCommitOffset != null) {\n                nextCommitOffsets.put(tpOffset.getKey(), nextCommitOffset);\n            }\n        }\n\n        // Commit offsets that are ready to be committed for every topic partition\n        if (!nextCommitOffsets.isEmpty()) {\n            kafkaConsumer.commitSync(nextCommitOffsets);\n            LOG.debug(\"Offsets successfully committed to Kafka [{}]\", nextCommitOffsets);\n            // Instead of iterating again, it would be possible to commit and update the state for each TopicPartition\n            // in the prior loop, but the multiple network calls should be more expensive than iterating twice over a small loop\n            for (Map.Entry<TopicPartition, OffsetAndMetadata> tpOffset : nextCommitOffsets.entrySet()) {\n                //Update the OffsetManager for each committed partition, and update numUncommittedOffsets\n                final TopicPartition tp = tpOffset.getKey();\n                long position = kafkaConsumer.position(tp);\n                long committedOffset = tpOffset.getValue().offset();\n                if (position < committedOffset) {\n                    /*\n                     * The position is behind the committed offset. This can happen in some cases, e.g. if a message failed, lots of (more\n                     * than max.poll.records) later messages were acked, and the failed message then gets acked. The consumer may only be\n                     * part way through \"catching up\" to where it was when it went back to retry the failed tuple. Skip the consumer forward\n                     * to the committed offset and drop the current waiting to emit list, since it'll likely contain committed offsets.\n                     */\n                    LOG.debug(\"Consumer fell behind committed offset. Catching up. Position was [{}], skipping to [{}]\",\n                        position, committedOffset);\n                    kafkaConsumer.seek(tp, committedOffset);\n                    List<ConsumerRecord<K, V>> waitingToEmitForTp = waitingToEmit.get(tp);\n                    if (waitingToEmitForTp != null) {\n                        //Discard the pending records that are already committed\n                        List<ConsumerRecord<K, V>> filteredRecords = new ArrayList<>();\n                        for (ConsumerRecord<K, V> record : waitingToEmitForTp) {\n                            if (record.offset() >= committedOffset) {\n                                filteredRecords.add(record);\n                            }\n                        }\n                        waitingToEmit.put(tp, filteredRecords);\n                    }\n                }\n\n                final OffsetManager offsetManager = offsetManagers.get(tp);\n                offsetManager.commit(tpOffset.getValue());\n                LOG.debug(\"[{}] uncommitted offsets for partition [{}] after commit\", offsetManager.getNumUncommittedOffsets(), tp);\n            }\n        } else {\n            LOG.trace(\"No offsets to commit. {}\", this);\n        }\n    }"
            },
            "external.storm-kafka-client.src.main.java.org.apache.storm.kafka.spout.KafkaSpout.open": {
                "code_before_change": "    public void open(Map<String, Object> conf, TopologyContext context, SpoutOutputCollector collector) {\n        this.context = context;\n\n        // Spout internals\n        this.collector = collector;\n\n        // Offset management\n        firstPollOffsetStrategy = kafkaSpoutConfig.getFirstPollOffsetStrategy();\n\n        // Retries management\n        retryService = kafkaSpoutConfig.getRetryService();\n\n        tupleListener = kafkaSpoutConfig.getTupleListener();\n\n        if (kafkaSpoutConfig.getProcessingGuarantee() != KafkaSpoutConfig.ProcessingGuarantee.AT_MOST_ONCE) {\n            // In at-most-once mode the offsets are committed after every poll, and not periodically as controlled by the timer\n            commitTimer = new Timer(TIMER_DELAY_MS, kafkaSpoutConfig.getOffsetsCommitPeriodMs(), TimeUnit.MILLISECONDS);\n        }\n        refreshAssignmentTimer = new Timer(TIMER_DELAY_MS, kafkaSpoutConfig.getPartitionRefreshPeriodMs(), TimeUnit.MILLISECONDS);\n\n        offsetManagers = new HashMap<>();\n        emitted = new HashSet<>();\n        waitingToEmit = new HashMap<>();\n        commitMetadataManager = new CommitMetadataManager(context, kafkaSpoutConfig.getProcessingGuarantee());\n\n        rebalanceListener = new KafkaSpoutConsumerRebalanceListener();\n\n        tupleListener.open(conf, context);\n        if (canRegisterMetrics()) {\n            registerMetric();\n        }\n\n        LOG.info(\"Kafka Spout opened with the following configuration: {}\", kafkaSpoutConfig);\n    }",
                "code_after_change": "    public void open(Map conf, TopologyContext context, SpoutOutputCollector collector) {\n        this.context = context;\n\n        // Spout internals\n        this.collector = collector;\n\n        // Offset management\n        firstPollOffsetStrategy = kafkaSpoutConfig.getFirstPollOffsetStrategy();\n\n        // Retries management\n        retryService = kafkaSpoutConfig.getRetryService();\n\n        tupleListener = kafkaSpoutConfig.getTupleListener();\n\n        if (kafkaSpoutConfig.getProcessingGuarantee() != KafkaSpoutConfig.ProcessingGuarantee.AT_MOST_ONCE) {\n            // In at-most-once mode the offsets are committed after every poll, and not periodically as controlled by the timer\n            commitTimer = new Timer(TIMER_DELAY_MS, kafkaSpoutConfig.getOffsetsCommitPeriodMs(), TimeUnit.MILLISECONDS);\n        }\n        refreshSubscriptionTimer = new Timer(TIMER_DELAY_MS, kafkaSpoutConfig.getPartitionRefreshPeriodMs(), TimeUnit.MILLISECONDS);\n\n        offsetManagers = new HashMap<>();\n        emitted = new HashSet<>();\n        waitingToEmit = new HashMap<>();\n        commitMetadataManager = new CommitMetadataManager(context, kafkaSpoutConfig.getProcessingGuarantee());\n\n        kafkaConsumer = kafkaConsumerFactory.createConsumer(kafkaSpoutConfig);\n\n        tupleListener.open(conf, context);\n        if (canRegisterMetrics()) {\n            registerMetric();\n        }\n\n        LOG.info(\"Kafka Spout opened with the following configuration: {}\", kafkaSpoutConfig);\n    }"
            },
            "external.storm-kafka-client.src.main.java.org.apache.storm.kafka.spout.KafkaSpout.nextTuple": {
                "code_before_change": "    public void nextTuple() {\n        try {\n            if (refreshAssignmentTimer.isExpiredResetOnTrue()) {\n                refreshAssignment();\n            }\n\n            if (commitTimer != null && commitTimer.isExpiredResetOnTrue()) {\n                if (isAtLeastOnceProcessing()) {\n                    commitOffsetsForAckedTuples(kafkaConsumer.assignment());\n                } else if (kafkaSpoutConfig.getProcessingGuarantee() == ProcessingGuarantee.NO_GUARANTEE) {\n                    Map<TopicPartition, OffsetAndMetadata> offsetsToCommit = \n                        createFetchedOffsetsMetadata(kafkaConsumer.assignment());\n                    kafkaConsumer.commitAsync(offsetsToCommit, null);\n                    LOG.debug(\"Committed offsets {} to Kafka\", offsetsToCommit);\n                }\n            }\n\n            PollablePartitionsInfo pollablePartitionsInfo = getPollablePartitionsInfo();\n            if (pollablePartitionsInfo.shouldPoll()) {\n                try {\n                    setWaitingToEmit(pollKafkaBroker(pollablePartitionsInfo));\n                } catch (RetriableException e) {\n                    LOG.error(\"Failed to poll from kafka.\", e);\n                }\n            }\n\n            emitIfWaitingNotEmitted();\n        } catch (InterruptException e) {\n            throwKafkaConsumerInterruptedException();\n        }\n    }",
                "code_after_change": "    public void nextTuple() {\n        try {\n            if (refreshSubscriptionTimer.isExpiredResetOnTrue()) {\n                kafkaSpoutConfig.getSubscription().refreshAssignment();\n            }\n\n            if (commitTimer != null && commitTimer.isExpiredResetOnTrue()) {\n                if (isAtLeastOnceProcessing()) {\n                    commitOffsetsForAckedTuples();\n                } else if (kafkaSpoutConfig.getProcessingGuarantee() == ProcessingGuarantee.NO_GUARANTEE) {\n                    Map<TopicPartition, OffsetAndMetadata> offsetsToCommit =\n                        createFetchedOffsetsMetadata(kafkaConsumer.assignment());\n                    kafkaConsumer.commitAsync(offsetsToCommit, null);\n                    LOG.debug(\"Committed offsets {} to Kafka\", offsetsToCommit);\n                }\n            }\n\n            PollablePartitionsInfo pollablePartitionsInfo = getPollablePartitionsInfo();\n            if (pollablePartitionsInfo.shouldPoll()) {\n                try {\n                    setWaitingToEmit(pollKafkaBroker(pollablePartitionsInfo));\n                } catch (RetriableException e) {\n                    LOG.error(\"Failed to poll from kafka.\", e);\n                }\n            }\n\n            emitIfWaitingNotEmitted();\n        } catch (InterruptException e) {\n            throwKafkaConsumerInterruptedException();\n        }\n    }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Buggy Method"
            },
            "fix_suggestion": "Preventive",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Buggy Method"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the root cause as the Kafka consumer being accessed after it has been closed, which is related to the method `KafkaOffsetMetric.getValueAndReset`. However, the actual fix was made in other methods of the KafkaSpout class, making it a 'Buggy Method' identification. The fix suggestion to check if the Kafka consumer is open before accessing it is preventive, as it would mitigate the issue but does not match the actual fix. The problem location is identified as `KafkaOffsetMetric.getValueAndReset`, which is where the error occurs, but not where the fix was applied, hence 'Buggy Method'. There is no wrong information in the bug report as all details are relevant to the context of the bug."
        }
    },
    {
        "filename": "STORM-3117.json",
        "code_diff": {
            "storm-client.src.jvm.org.apache.storm.generated.Nimbus.deleteBlob_result": {
                "code_before_change": [],
                "code_after_change": []
            },
            "storm-client.src.jvm.org.apache.storm.generated.Nimbus.hashCode": {
                "code_before_change": "    public int hashCode() {\n      int hashCode = 1;\n\n      hashCode = hashCode * 8191 + ((is_set_name()) ? 131071 : 524287);\n      if (is_set_name())\n        hashCode = hashCode * 8191 + name.hashCode();\n\n      hashCode = hashCode * 8191 + ((is_set_uploadedJarLocation()) ? 131071 : 524287);\n      if (is_set_uploadedJarLocation())\n        hashCode = hashCode * 8191 + uploadedJarLocation.hashCode();\n\n      hashCode = hashCode * 8191 + ((is_set_jsonConf()) ? 131071 : 524287);\n      if (is_set_jsonConf())\n        hashCode = hashCode * 8191 + jsonConf.hashCode();\n\n      hashCode = hashCode * 8191 + ((is_set_topology()) ? 131071 : 524287);\n      if (is_set_topology())\n        hashCode = hashCode * 8191 + topology.hashCode();\n\n      return hashCode;\n    }",
                "code_after_change": "    public int hashCode() {\n      int hashCode = 1;\n\n      hashCode = hashCode * 8191 + ((is_set_name()) ? 131071 : 524287);\n      if (is_set_name())\n        hashCode = hashCode * 8191 + name.hashCode();\n\n      hashCode = hashCode * 8191 + ((is_set_uploadedJarLocation()) ? 131071 : 524287);\n      if (is_set_uploadedJarLocation())\n        hashCode = hashCode * 8191 + uploadedJarLocation.hashCode();\n\n      hashCode = hashCode * 8191 + ((is_set_jsonConf()) ? 131071 : 524287);\n      if (is_set_jsonConf())\n        hashCode = hashCode * 8191 + jsonConf.hashCode();\n\n      hashCode = hashCode * 8191 + ((is_set_topology()) ? 131071 : 524287);\n      if (is_set_topology())\n        hashCode = hashCode * 8191 + topology.hashCode();\n\n      return hashCode;\n    }"
            },
            "storm-client.src.jvm.org.apache.storm.generated.Nimbus.recv_deleteBlob": {
                "code_before_change": "    public void recv_deleteBlob() throws AuthorizationException, KeyNotFoundException, org.apache.storm.thrift.TException\n    {\n      deleteBlob_result result = new deleteBlob_result();\n      receiveBase(result, \"deleteBlob\");\n      if (result.aze != null) {\n        throw result.aze;\n      }\n      if (result.knf != null) {\n        throw result.knf;\n      }\n      return;\n    }",
                "code_after_change": "    public void recv_deleteBlob() throws AuthorizationException, KeyNotFoundException, IllegalStateException, org.apache.storm.thrift.TException\n    {\n      deleteBlob_result result = new deleteBlob_result();\n      receiveBase(result, \"deleteBlob\");\n      if (result.aze != null) {\n        throw result.aze;\n      }\n      if (result.knf != null) {\n        throw result.knf;\n      }\n      if (result.ise != null) {\n        throw result.ise;\n      }\n      return;\n    }"
            },
            "storm-client.src.jvm.org.apache.storm.generated.Nimbus.getFieldValue": {
                "code_before_change": "    public java.lang.Object getFieldValue(_Fields field) {\n      switch (field) {\n      case NAME:\n        return get_name();\n\n      case UPLOADED_JAR_LOCATION:\n        return get_uploadedJarLocation();\n\n      case JSON_CONF:\n        return get_jsonConf();\n\n      case TOPOLOGY:\n        return get_topology();\n\n      }\n      throw new java.lang.IllegalStateException();\n    }",
                "code_after_change": "    public java.lang.Object getFieldValue(_Fields field) {\n      switch (field) {\n      case NAME:\n        return get_name();\n\n      case UPLOADED_JAR_LOCATION:\n        return get_uploadedJarLocation();\n\n      case JSON_CONF:\n        return get_jsonConf();\n\n      case TOPOLOGY:\n        return get_topology();\n\n      }\n      throw new java.lang.IllegalStateException();\n    }"
            },
            "storm-client.src.jvm.org.apache.storm.generated.Nimbus.getResult": {
                "code_before_change": "      public Void getResult() throws AlreadyAliveException, InvalidTopologyException, AuthorizationException, org.apache.storm.thrift.TException {\n        if (getState() != org.apache.storm.thrift.async.TAsyncMethodCall.State.RESPONSE_READ) {\n          throw new java.lang.IllegalStateException(\"Method call not finished!\");\n        }\n        org.apache.storm.thrift.transport.TMemoryInputTransport memoryTransport = new org.apache.storm.thrift.transport.TMemoryInputTransport(getFrameBuffer().array());\n        org.apache.storm.thrift.protocol.TProtocol prot = client.getProtocolFactory().getProtocol(memoryTransport);\n        return null;\n      }",
                "code_after_change": "      public Void getResult() throws AlreadyAliveException, InvalidTopologyException, AuthorizationException, org.apache.storm.thrift.TException {\n        if (getState() != org.apache.storm.thrift.async.TAsyncMethodCall.State.RESPONSE_READ) {\n          throw new java.lang.IllegalStateException(\"Method call not finished!\");\n        }\n        org.apache.storm.thrift.transport.TMemoryInputTransport memoryTransport = new org.apache.storm.thrift.transport.TMemoryInputTransport(getFrameBuffer().array());\n        org.apache.storm.thrift.protocol.TProtocol prot = client.getProtocolFactory().getProtocol(memoryTransport);\n        return null;\n      }"
            },
            "storm-client.src.jvm.org.apache.storm.generated.Nimbus.equals": {
                "code_before_change": "    public boolean equals(java.lang.Object that) {\n      if (that == null)\n        return false;\n      if (that instanceof submitTopology_args)\n        return this.equals((submitTopology_args)that);\n      return false;\n    }",
                "code_after_change": "    public boolean equals(java.lang.Object that) {\n      if (that == null)\n        return false;\n      if (that instanceof submitTopology_args)\n        return this.equals((submitTopology_args)that);\n      return false;\n    }"
            },
            "storm-client.src.jvm.org.apache.storm.generated.Nimbus.send_deleteBlob": {
                "code_before_change": "    public void send_deleteBlob(java.lang.String key) throws org.apache.storm.thrift.TException\n    {\n      deleteBlob_args args = new deleteBlob_args();\n      args.set_key(key);\n      sendBase(\"deleteBlob\", args);\n    }",
                "code_after_change": "    public void send_deleteBlob(java.lang.String key) throws org.apache.storm.thrift.TException\n    {\n      deleteBlob_args args = new deleteBlob_args();\n      args.set_key(key);\n      sendBase(\"deleteBlob\", args);\n    }"
            },
            "storm-client.src.jvm.org.apache.storm.generated.Nimbus.set_knf_isSet": {
                "code_before_change": "    public void set_knf_isSet(boolean value) {\n      if (!value) {\n        this.knf = null;\n      }\n    }",
                "code_after_change": "    public void set_knf_isSet(boolean value) {\n      if (!value) {\n        this.knf = null;\n      }\n    }"
            },
            "storm-client.src.jvm.org.apache.storm.generated.Nimbus.setFieldValue": {
                "code_before_change": "    public void setFieldValue(_Fields field, java.lang.Object value) {\n      switch (field) {\n      case NAME:\n        if (value == null) {\n          unset_name();\n        } else {\n          set_name((java.lang.String)value);\n        }\n        break;\n\n      case UPLOADED_JAR_LOCATION:\n        if (value == null) {\n          unset_uploadedJarLocation();\n        } else {\n          set_uploadedJarLocation((java.lang.String)value);\n        }\n        break;\n\n      case JSON_CONF:\n        if (value == null) {\n          unset_jsonConf();\n        } else {\n          set_jsonConf((java.lang.String)value);\n        }\n        break;\n\n      case TOPOLOGY:\n        if (value == null) {\n          unset_topology();\n        } else {\n          set_topology((StormTopology)value);\n        }\n        break;\n\n      }\n    }",
                "code_after_change": "    public void setFieldValue(_Fields field, java.lang.Object value) {\n      switch (field) {\n      case NAME:\n        if (value == null) {\n          unset_name();\n        } else {\n          set_name((java.lang.String)value);\n        }\n        break;\n\n      case UPLOADED_JAR_LOCATION:\n        if (value == null) {\n          unset_uploadedJarLocation();\n        } else {\n          set_uploadedJarLocation((java.lang.String)value);\n        }\n        break;\n\n      case JSON_CONF:\n        if (value == null) {\n          unset_jsonConf();\n        } else {\n          set_jsonConf((java.lang.String)value);\n        }\n        break;\n\n      case TOPOLOGY:\n        if (value == null) {\n          unset_topology();\n        } else {\n          set_topology((StormTopology)value);\n        }\n        break;\n\n      }\n    }"
            },
            "storm-client.src.jvm.org.apache.storm.generated.Nimbus.findByThriftId": {
                "code_before_change": "      public static _Fields findByThriftId(int fieldId) {\n        switch(fieldId) {\n          case 1: // NAME\n            return NAME;\n          case 2: // UPLOADED_JAR_LOCATION\n            return UPLOADED_JAR_LOCATION;\n          case 3: // JSON_CONF\n            return JSON_CONF;\n          case 4: // TOPOLOGY\n            return TOPOLOGY;\n          default:\n            return null;\n        }\n      }",
                "code_after_change": "      public static _Fields findByThriftId(int fieldId) {\n        switch(fieldId) {\n          case 1: // NAME\n            return NAME;\n          case 2: // UPLOADED_JAR_LOCATION\n            return UPLOADED_JAR_LOCATION;\n          case 3: // JSON_CONF\n            return JSON_CONF;\n          case 4: // TOPOLOGY\n            return TOPOLOGY;\n          default:\n            return null;\n        }\n      }"
            },
            "storm-client.src.jvm.org.apache.storm.generated.Nimbus.recv_downloadBlobChunk": {
                "code_before_change": "    public java.nio.ByteBuffer recv_downloadBlobChunk() throws AuthorizationException, org.apache.storm.thrift.TException\n    {\n      downloadBlobChunk_result result = new downloadBlobChunk_result();\n      receiveBase(result, \"downloadBlobChunk\");\n      if (result.is_set_success()) {\n        return result.success;\n      }\n      if (result.aze != null) {\n        throw result.aze;\n      }\n      throw new org.apache.storm.thrift.TApplicationException(org.apache.storm.thrift.TApplicationException.MISSING_RESULT, \"downloadBlobChunk failed: unknown result\");\n    }",
                "code_after_change": "    public java.nio.ByteBuffer recv_downloadBlobChunk() throws AuthorizationException, org.apache.storm.thrift.TException\n    {\n      downloadBlobChunk_result result = new downloadBlobChunk_result();\n      receiveBase(result, \"downloadBlobChunk\");\n      if (result.is_set_success()) {\n        return result.success;\n      }\n      if (result.aze != null) {\n        throw result.aze;\n      }\n      throw new org.apache.storm.thrift.TApplicationException(org.apache.storm.thrift.TApplicationException.MISSING_RESULT, \"downloadBlobChunk failed: unknown result\");\n    }"
            },
            "storm-client.src.jvm.org.apache.storm.generated.Nimbus.write_args": {
                "code_before_change": "      public void write_args(org.apache.storm.thrift.protocol.TProtocol prot) throws org.apache.storm.thrift.TException {\n        prot.writeMessageBegin(new org.apache.storm.thrift.protocol.TMessage(\"submitTopology\", org.apache.storm.thrift.protocol.TMessageType.CALL, 0));\n        submitTopology_args args = new submitTopology_args();\n        args.set_name(name);\n        args.set_uploadedJarLocation(uploadedJarLocation);\n        args.set_jsonConf(jsonConf);\n        args.set_topology(topology);\n        args.write(prot);\n        prot.writeMessageEnd();\n      }",
                "code_after_change": "      public void write_args(org.apache.storm.thrift.protocol.TProtocol prot) throws org.apache.storm.thrift.TException {\n        prot.writeMessageBegin(new org.apache.storm.thrift.protocol.TMessage(\"submitTopology\", org.apache.storm.thrift.protocol.TMessageType.CALL, 0));\n        submitTopology_args args = new submitTopology_args();\n        args.set_name(name);\n        args.set_uploadedJarLocation(uploadedJarLocation);\n        args.set_jsonConf(jsonConf);\n        args.set_topology(topology);\n        args.write(prot);\n        prot.writeMessageEnd();\n      }"
            },
            "storm-client.src.jvm.org.apache.storm.generated.Nimbus.read": {
                "code_before_change": "    public void read(org.apache.storm.thrift.protocol.TProtocol iprot) throws org.apache.storm.thrift.TException {\n      scheme(iprot).read(iprot, this);\n    }",
                "code_after_change": "    public void read(org.apache.storm.thrift.protocol.TProtocol iprot) throws org.apache.storm.thrift.TException {\n      scheme(iprot).read(iprot, this);\n    }"
            },
            "storm-client.src.jvm.org.apache.storm.generated.Nimbus.isSet": {
                "code_before_change": "    public boolean isSet(_Fields field) {\n      if (field == null) {\n        throw new java.lang.IllegalArgumentException();\n      }\n\n      switch (field) {\n      case NAME:\n        return is_set_name();\n      case UPLOADED_JAR_LOCATION:\n        return is_set_uploadedJarLocation();\n      case JSON_CONF:\n        return is_set_jsonConf();\n      case TOPOLOGY:\n        return is_set_topology();\n      }\n      throw new java.lang.IllegalStateException();\n    }",
                "code_after_change": "    public boolean isSet(_Fields field) {\n      if (field == null) {\n        throw new java.lang.IllegalArgumentException();\n      }\n\n      switch (field) {\n      case NAME:\n        return is_set_name();\n      case UPLOADED_JAR_LOCATION:\n        return is_set_uploadedJarLocation();\n      case JSON_CONF:\n        return is_set_jsonConf();\n      case TOPOLOGY:\n        return is_set_topology();\n      }\n      throw new java.lang.IllegalStateException();\n    }"
            },
            "storm-client.src.jvm.org.apache.storm.generated.Nimbus.toString": {
                "code_before_change": "    public java.lang.String toString() {\n      java.lang.StringBuilder sb = new java.lang.StringBuilder(\"submitTopology_args(\");\n      boolean first = true;\n\n      sb.append(\"name:\");\n      if (this.name == null) {\n        sb.append(\"null\");\n      } else {\n        sb.append(this.name);\n      }\n      first = false;\n      if (!first) sb.append(\", \");\n      sb.append(\"uploadedJarLocation:\");\n      if (this.uploadedJarLocation == null) {\n        sb.append(\"null\");\n      } else {\n        sb.append(this.uploadedJarLocation);\n      }\n      first = false;\n      if (!first) sb.append(\", \");\n      sb.append(\"jsonConf:\");\n      if (this.jsonConf == null) {\n        sb.append(\"null\");\n      } else {\n        sb.append(this.jsonConf);\n      }\n      first = false;\n      if (!first) sb.append(\", \");\n      sb.append(\"topology:\");\n      if (this.topology == null) {\n        sb.append(\"null\");\n      } else {\n        sb.append(this.topology);\n      }\n      first = false;\n      sb.append(\")\");\n      return sb.toString();\n    }",
                "code_after_change": "    public java.lang.String toString() {\n      java.lang.StringBuilder sb = new java.lang.StringBuilder(\"submitTopology_args(\");\n      boolean first = true;\n\n      sb.append(\"name:\");\n      if (this.name == null) {\n        sb.append(\"null\");\n      } else {\n        sb.append(this.name);\n      }\n      first = false;\n      if (!first) sb.append(\", \");\n      sb.append(\"uploadedJarLocation:\");\n      if (this.uploadedJarLocation == null) {\n        sb.append(\"null\");\n      } else {\n        sb.append(this.uploadedJarLocation);\n      }\n      first = false;\n      if (!first) sb.append(\", \");\n      sb.append(\"jsonConf:\");\n      if (this.jsonConf == null) {\n        sb.append(\"null\");\n      } else {\n        sb.append(this.jsonConf);\n      }\n      first = false;\n      if (!first) sb.append(\", \");\n      sb.append(\"topology:\");\n      if (this.topology == null) {\n        sb.append(\"null\");\n      } else {\n        sb.append(this.topology);\n      }\n      first = false;\n      sb.append(\")\");\n      return sb.toString();\n    }"
            },
            "storm-client.src.jvm.org.apache.storm.generated.Nimbus.getFieldName": {
                "code_before_change": "      public java.lang.String getFieldName() {\n        return _fieldName;\n      }",
                "code_after_change": "      public java.lang.String getFieldName() {\n        return _fieldName;\n      }"
            },
            "storm-client.src.jvm.org.apache.storm.generated.Nimbus.onError": {
                "code_before_change": "          public void onError(java.lang.Exception e) {\n            byte msgType = org.apache.storm.thrift.protocol.TMessageType.REPLY;\n            org.apache.storm.thrift.TSerializable msg;\n            submitTopology_result result = new submitTopology_result();\n            if (e instanceof AlreadyAliveException) {\n              result.e = (AlreadyAliveException) e;\n              result.set_e_isSet(true);\n              msg = result;\n            } else if (e instanceof InvalidTopologyException) {\n              result.ite = (InvalidTopologyException) e;\n              result.set_ite_isSet(true);\n              msg = result;\n            } else if (e instanceof AuthorizationException) {\n              result.aze = (AuthorizationException) e;\n              result.set_aze_isSet(true);\n              msg = result;\n            } else if (e instanceof org.apache.storm.thrift.transport.TTransportException) {\n              _LOGGER.error(\"TTransportException inside handler\", e);\n              fb.close();\n              return;\n            } else if (e instanceof org.apache.storm.thrift.TApplicationException) {\n              _LOGGER.error(\"TApplicationException inside handler\", e);\n              msgType = org.apache.storm.thrift.protocol.TMessageType.EXCEPTION;\n              msg = (org.apache.storm.thrift.TApplicationException)e;\n            } else {\n              _LOGGER.error(\"Exception inside handler\", e);\n              msgType = org.apache.storm.thrift.protocol.TMessageType.EXCEPTION;\n              msg = new org.apache.storm.thrift.TApplicationException(org.apache.storm.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());\n            }\n            try {\n              fcall.sendResponse(fb,msg,msgType,seqid);\n            } catch (java.lang.Exception ex) {\n              _LOGGER.error(\"Exception writing to internal frame buffer\", ex);\n              fb.close();\n            }\n          }",
                "code_after_change": "          public void onError(java.lang.Exception e) {\n            byte msgType = org.apache.storm.thrift.protocol.TMessageType.REPLY;\n            org.apache.storm.thrift.TSerializable msg;\n            submitTopology_result result = new submitTopology_result();\n            if (e instanceof AlreadyAliveException) {\n              result.e = (AlreadyAliveException) e;\n              result.set_e_isSet(true);\n              msg = result;\n            } else if (e instanceof InvalidTopologyException) {\n              result.ite = (InvalidTopologyException) e;\n              result.set_ite_isSet(true);\n              msg = result;\n            } else if (e instanceof AuthorizationException) {\n              result.aze = (AuthorizationException) e;\n              result.set_aze_isSet(true);\n              msg = result;\n            } else if (e instanceof org.apache.storm.thrift.transport.TTransportException) {\n              _LOGGER.error(\"TTransportException inside handler\", e);\n              fb.close();\n              return;\n            } else if (e instanceof org.apache.storm.thrift.TApplicationException) {\n              _LOGGER.error(\"TApplicationException inside handler\", e);\n              msgType = org.apache.storm.thrift.protocol.TMessageType.EXCEPTION;\n              msg = (org.apache.storm.thrift.TApplicationException)e;\n            } else {\n              _LOGGER.error(\"Exception inside handler\", e);\n              msgType = org.apache.storm.thrift.protocol.TMessageType.EXCEPTION;\n              msg = new org.apache.storm.thrift.TApplicationException(org.apache.storm.thrift.TApplicationException.INTERNAL_ERROR, e.getMessage());\n            }\n            try {\n              fcall.sendResponse(fb,msg,msgType,seqid);\n            } catch (java.lang.Exception ex) {\n              _LOGGER.error(\"Exception writing to internal frame buffer\", ex);\n              fb.close();\n            }\n          }"
            },
            "storm-client.src.jvm.org.apache.storm.generated.Nimbus.compareTo": {
                "code_before_change": "    public int compareTo(submitTopology_args other) {\n      if (!getClass().equals(other.getClass())) {\n        return getClass().getName().compareTo(other.getClass().getName());\n      }\n\n      int lastComparison = 0;\n\n      lastComparison = java.lang.Boolean.valueOf(is_set_name()).compareTo(other.is_set_name());\n      if (lastComparison != 0) {\n        return lastComparison;\n      }\n      if (is_set_name()) {\n        lastComparison = org.apache.storm.thrift.TBaseHelper.compareTo(this.name, other.name);\n        if (lastComparison != 0) {\n          return lastComparison;\n        }\n      }\n      lastComparison = java.lang.Boolean.valueOf(is_set_uploadedJarLocation()).compareTo(other.is_set_uploadedJarLocation());\n      if (lastComparison != 0) {\n        return lastComparison;\n      }\n      if (is_set_uploadedJarLocation()) {\n        lastComparison = org.apache.storm.thrift.TBaseHelper.compareTo(this.uploadedJarLocation, other.uploadedJarLocation);\n        if (lastComparison != 0) {\n          return lastComparison;\n        }\n      }\n      lastComparison = java.lang.Boolean.valueOf(is_set_jsonConf()).compareTo(other.is_set_jsonConf());\n      if (lastComparison != 0) {\n        return lastComparison;\n      }\n      if (is_set_jsonConf()) {\n        lastComparison = org.apache.storm.thrift.TBaseHelper.compareTo(this.jsonConf, other.jsonConf);\n        if (lastComparison != 0) {\n          return lastComparison;\n        }\n      }\n      lastComparison = java.lang.Boolean.valueOf(is_set_topology()).compareTo(other.is_set_topology());\n      if (lastComparison != 0) {\n        return lastComparison;\n      }\n      if (is_set_topology()) {\n        lastComparison = org.apache.storm.thrift.TBaseHelper.compareTo(this.topology, other.topology);\n        if (lastComparison != 0) {\n          return lastComparison;\n        }\n      }\n      return 0;\n    }",
                "code_after_change": "    public int compareTo(submitTopology_args other) {\n      if (!getClass().equals(other.getClass())) {\n        return getClass().getName().compareTo(other.getClass().getName());\n      }\n\n      int lastComparison = 0;\n\n      lastComparison = java.lang.Boolean.valueOf(is_set_name()).compareTo(other.is_set_name());\n      if (lastComparison != 0) {\n        return lastComparison;\n      }\n      if (is_set_name()) {\n        lastComparison = org.apache.storm.thrift.TBaseHelper.compareTo(this.name, other.name);\n        if (lastComparison != 0) {\n          return lastComparison;\n        }\n      }\n      lastComparison = java.lang.Boolean.valueOf(is_set_uploadedJarLocation()).compareTo(other.is_set_uploadedJarLocation());\n      if (lastComparison != 0) {\n        return lastComparison;\n      }\n      if (is_set_uploadedJarLocation()) {\n        lastComparison = org.apache.storm.thrift.TBaseHelper.compareTo(this.uploadedJarLocation, other.uploadedJarLocation);\n        if (lastComparison != 0) {\n          return lastComparison;\n        }\n      }\n      lastComparison = java.lang.Boolean.valueOf(is_set_jsonConf()).compareTo(other.is_set_jsonConf());\n      if (lastComparison != 0) {\n        return lastComparison;\n      }\n      if (is_set_jsonConf()) {\n        lastComparison = org.apache.storm.thrift.TBaseHelper.compareTo(this.jsonConf, other.jsonConf);\n        if (lastComparison != 0) {\n          return lastComparison;\n        }\n      }\n      lastComparison = java.lang.Boolean.valueOf(is_set_topology()).compareTo(other.is_set_topology());\n      if (lastComparison != 0) {\n        return lastComparison;\n      }\n      if (is_set_topology()) {\n        lastComparison = org.apache.storm.thrift.TBaseHelper.compareTo(this.topology, other.topology);\n        if (lastComparison != 0) {\n          return lastComparison;\n        }\n      }\n      return 0;\n    }"
            },
            "storm-client.src.jvm.org.apache.storm.generated.Nimbus.write": {
                "code_before_change": "    public void write(org.apache.storm.thrift.protocol.TProtocol oprot) throws org.apache.storm.thrift.TException {\n      scheme(oprot).write(oprot, this);\n    }",
                "code_after_change": "    public void write(org.apache.storm.thrift.protocol.TProtocol oprot) throws org.apache.storm.thrift.TException {\n      scheme(oprot).write(oprot, this);\n    }"
            },
            "storm-client.src.jvm.org.apache.storm.generated.Nimbus.deepCopy": {
                "code_before_change": "    public submitTopology_args deepCopy() {\n      return new submitTopology_args(this);\n    }",
                "code_after_change": "    public submitTopology_args deepCopy() {\n      return new submitTopology_args(this);\n    }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Preventive",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the root cause as Nimbus attempting to access blob metadata for a topology that has already had its blobs deleted, which is related to the stack trace context but not the exact ground truth methods. The fix suggestion is preventive, as it suggests implementing checks to prevent blob deletion while in use, which would mitigate the issue. The problem location mentions methods in the stack trace context but not the exact ground truth methods. There is no wrong information in the bug report; all details are relevant to the described issue."
        }
    },
    {
        "filename": "STORM-2993.json",
        "code_diff": {
            "external.storm-hdfs.src.main.java.org.apache.storm.hdfs.bolt.AbstractHdfsBolt.cleanup": {
                "code_before_change": "    public void cleanup() {\n        doRotationAndRemoveAllWriters();\n        this.rotationTimer.cancel();\n    }",
                "code_after_change": "    public void cleanup() {\n        doRotationAndRemoveAllWriters();\n        this.rotationTimer.cancel();\n    }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Buggy Method"
            },
            "fix_suggestion": "Preventive",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Buggy Method"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the root cause as improper synchronization in the timed rotation policy, which is related to the issue but not the exact root cause in the ground truth method 'AbstractHdfsBolt.cleanup'. The report points to methods like 'AbstractHdfsBolt.execute', 'HDFSWriter.doWrite', and 'AbstractHDFSWriter.write', which are involved in the bug but not where the actual fix was made, hence categorized as 'Buggy Method'. The fix suggestion involves adding synchronization and checks, which is preventive as it addresses the issue but does not match the developer's fix. The problem location is identified in methods that are involved in the bug but not the exact ground truth method, hence 'Buggy Method'. There is no wrong information as the report accurately describes the context of the bug."
        }
    },
    {
        "filename": "STORM-1540.json",
        "code_diff": {
            "storm-core.src.jvm.org.apache.storm.trident.tuple.ConsList.get": {
                "code_before_change": "    public Object get(int i) {\n        if(i==0) return _first;\n        else {\n            return _elems.get(i - 1);\n        }\n    }",
                "code_after_change": "    public Object get(int i) {\n        if(i==0) return _first;\n        else {\n            return _elems.get(i - 1);\n        }\n    }"
            },
            "storm-core.src.jvm.org.apache.storm.serialization.SerializationFactory.getKryo": {
                "code_before_change": "    public static Kryo getKryo(Map conf) {\n        IKryoFactory kryoFactory = (IKryoFactory) Utils.newInstance((String) conf.get(Config.TOPOLOGY_KRYO_FACTORY));\n        Kryo k = kryoFactory.getKryo(conf);\n        k.register(byte[].class);\n\n        /* tuple payload serializer is specified via configuration */\n        String payloadSerializerName = (String)conf.get(Config.TOPOLOGY_TUPLE_SERIALIZER);\n        try {\n            Class serializerClass  = Class.forName(payloadSerializerName);\n            Serializer serializer = resolveSerializerInstance(k, ListDelegate.class, serializerClass, conf);\n            k.register(ListDelegate.class, serializer);\n        } catch (ClassNotFoundException ex) {\n            throw new RuntimeException(ex);\n        }\n\n        k.register(ArrayList.class, new ArrayListSerializer());\n        k.register(HashMap.class, new HashMapSerializer());\n        k.register(HashSet.class, new HashSetSerializer());\n        k.register(BigInteger.class, new BigIntegerSerializer());\n        k.register(TransactionAttempt.class);\n        k.register(Values.class);\n        k.register(org.apache.storm.metric.api.IMetricsConsumer.DataPoint.class);\n        k.register(org.apache.storm.metric.api.IMetricsConsumer.TaskInfo.class);\n        try {\n            JavaBridge.registerPrimitives(k);\n            JavaBridge.registerCollections(k);\n        } catch(Exception e) {\n            throw new RuntimeException(e);\n        }\n\n        Map<String, String> registrations = normalizeKryoRegister(conf);\n\n        kryoFactory.preRegister(k, conf);\n\n        boolean skipMissing = (Boolean) conf.get(Config.TOPOLOGY_SKIP_MISSING_KRYO_REGISTRATIONS);\n        for(Map.Entry<String, String> entry: registrations.entrySet()) {\n            String serializerClassName = entry.getValue();\n            try {\n                Class klass = Class.forName(entry.getKey());\n                Class serializerClass = null;\n                if(serializerClassName!=null)\n                    serializerClass = Class.forName(serializerClassName);\n                if(serializerClass == null) {\n                    k.register(klass);\n                } else {\n                    k.register(klass, resolveSerializerInstance(k, klass, serializerClass, conf));\n                }\n            } catch (ClassNotFoundException e) {\n                if(skipMissing) {\n                    LOG.info(\"Could not find serialization or class for \" + serializerClassName + \". Skipping registration...\");\n                } else {\n                    throw new RuntimeException(e);\n                }\n            }\n        }\n\n        kryoFactory.postRegister(k, conf);\n\n        if (conf.get(Config.TOPOLOGY_KRYO_DECORATORS) != null) {\n            for(String klassName : (List<String>)conf.get(Config.TOPOLOGY_KRYO_DECORATORS)) {\n                try {\n                    Class klass = Class.forName(klassName);\n                    IKryoDecorator decorator = (IKryoDecorator)klass.newInstance();\n                    decorator.decorate(k);\n                } catch(ClassNotFoundException e) {\n                    if(skipMissing) {\n                        LOG.info(\"Could not find kryo decorator named \" + klassName + \". Skipping registration...\");\n                    } else {\n                        throw new RuntimeException(e);\n                    }\n                } catch(InstantiationException e) {\n                    throw new RuntimeException(e);\n                } catch(IllegalAccessException e) {\n                    throw new RuntimeException(e);\n                }\n            }\n        }\n\n        kryoFactory.postDecorate(k, conf);\n\n        return k;\n    }",
                "code_after_change": "    public static Kryo getKryo(Map conf) {\n        IKryoFactory kryoFactory = (IKryoFactory) Utils.newInstance((String) conf.get(Config.TOPOLOGY_KRYO_FACTORY));\n        Kryo k = kryoFactory.getKryo(conf);\n        k.register(byte[].class);\n\n        /* tuple payload serializer is specified via configuration */\n        String payloadSerializerName = (String)conf.get(Config.TOPOLOGY_TUPLE_SERIALIZER);\n        try {\n            Class serializerClass  = Class.forName(payloadSerializerName);\n            Serializer serializer = resolveSerializerInstance(k, ListDelegate.class, serializerClass, conf);\n            k.register(ListDelegate.class, serializer);\n        } catch (ClassNotFoundException ex) {\n            throw new RuntimeException(ex);\n        }\n\n        k.register(ArrayList.class, new ArrayListSerializer());\n        k.register(HashMap.class, new HashMapSerializer());\n        k.register(HashSet.class, new HashSetSerializer());\n        k.register(BigInteger.class, new BigIntegerSerializer());\n        k.register(TransactionAttempt.class);\n        k.register(Values.class);\n        k.register(org.apache.storm.metric.api.IMetricsConsumer.DataPoint.class);\n        k.register(org.apache.storm.metric.api.IMetricsConsumer.TaskInfo.class);\n        k.register(ConsList.class);\n        try {\n            JavaBridge.registerPrimitives(k);\n            JavaBridge.registerCollections(k);\n        } catch(Exception e) {\n            throw new RuntimeException(e);\n        }\n\n        Map<String, String> registrations = normalizeKryoRegister(conf);\n\n        kryoFactory.preRegister(k, conf);\n\n        boolean skipMissing = (Boolean) conf.get(Config.TOPOLOGY_SKIP_MISSING_KRYO_REGISTRATIONS);\n        for(Map.Entry<String, String> entry: registrations.entrySet()) {\n            String serializerClassName = entry.getValue();\n            try {\n                Class klass = Class.forName(entry.getKey());\n                Class serializerClass = null;\n                if(serializerClassName!=null)\n                    serializerClass = Class.forName(serializerClassName);\n                if(serializerClass == null) {\n                    k.register(klass);\n                } else {\n                    k.register(klass, resolveSerializerInstance(k, klass, serializerClass, conf));\n                }\n            } catch (ClassNotFoundException e) {\n                if(skipMissing) {\n                    LOG.info(\"Could not find serialization or class for \" + serializerClassName + \". Skipping registration...\");\n                } else {\n                    throw new RuntimeException(e);\n                }\n            }\n        }\n\n        kryoFactory.postRegister(k, conf);\n\n        if (conf.get(Config.TOPOLOGY_KRYO_DECORATORS) != null) {\n            for(String klassName : (List<String>)conf.get(Config.TOPOLOGY_KRYO_DECORATORS)) {\n                try {\n                    Class klass = Class.forName(klassName);\n                    IKryoDecorator decorator = (IKryoDecorator)klass.newInstance();\n                    decorator.decorate(k);\n                } catch(ClassNotFoundException e) {\n                    if(skipMissing) {\n                        LOG.info(\"Could not find kryo decorator named \" + klassName + \". Skipping registration...\");\n                    } else {\n                        throw new RuntimeException(e);\n                    }\n                } catch(InstantiationException e) {\n                    throw new RuntimeException(e);\n                } catch(IllegalAccessException e) {\n                    throw new RuntimeException(e);\n                }\n            }\n        }\n\n        kryoFactory.postDecorate(k, conf);\n\n        return k;\n    }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Buggy Method"
            },
            "fix_suggestion": "Alternative Fix",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Buggy Method"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the root cause as the 'ConsList' class not being serializable, which is related to the issue but not the exact root cause in the ground truth methods. It points to the method where the error occurred, but not where the actual fix was made, hence 'Partial' with 'Buggy Method'. The fix suggestion to make 'ConsList' serializable is an alternative fix, as the actual fix involved registering 'ConsList' in the Kryo serialization. The problem location mentions methods involved in the serialization process, which are related to the bug but not the exact ground truth methods, hence 'Partial' with 'Buggy Method'. There is no wrong information in the bug report as all statements are relevant to the context of the bug."
        }
    },
    {
        "filename": "STORM-2275.json",
        "code_diff": {
            "storm-core.src.jvm.org.apache.storm.daemon.nimbus.Nimbus.transition": {
                "code_before_change": "    private void transition(String topoId, TopologyActions event, Object eventArg) throws Exception {\n        transition(topoId, event, eventArg, false);\n    }",
                "code_after_change": "    private void transition(String topoId, TopologyActions event, Object eventArg) throws Exception {\n        transition(topoId, event, eventArg, false);\n    }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Preventive",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report precisely identifies the root cause as a NullPointerException due to an assumption that a variable is non-null in the 'Nimbus.transition' method, which matches the ground truth method. The fix suggestion is preventive as it suggests adding null checks, which would prevent the NullPointerException, although the actual fix did not involve code changes. The problem location identification is precise as it mentions 'Nimbus.transition', which is the ground truth method. There is no wrong information in the bug report as all details are relevant and accurate."
        }
    },
    {
        "filename": "STORM-2279.json",
        "code_diff": {
            "storm-core.src.jvm.org.apache.storm.utils.TupleUtils.putTickFrequencyIntoComponentConfig": {
                "code_before_change": "    public static Map<String, Object> putTickFrequencyIntoComponentConfig(Map<String, Object> conf, int tickFreqSecs) {\n      if (conf == null) {\n          conf = new Config();\n      }\n\n      if (tickFreqSecs > 0) {\n          LOG.info(\"Enabling tick tuple with interval [{}]\", tickFreqSecs);\n          conf.put(Config.TOPOLOGY_TICK_TUPLE_FREQ_SECS, tickFreqSecs);\n      }\n\n      return conf;\n    }",
                "code_after_change": "    public static Map<String, Object> putTickFrequencyIntoComponentConfig(Map<String, Object> conf, int tickFreqSecs) {\n      if (conf == null) {\n          conf = new Config();\n      }\n\n      if (tickFreqSecs > 0) {\n          LOG.info(\"Enabling tick tuple with interval [{}]\", tickFreqSecs);\n          conf.put(Config.TOPOLOGY_TICK_TUPLE_FREQ_SECS, tickFreqSecs);\n      }\n\n      return conf;\n    }"
            },
            "storm-core.src.jvm.org.apache.storm.daemon.nimbus.Nimbus.getComponentPageInfo": {
                "code_before_change": "    public ComponentPageInfo getComponentPageInfo(String topoId, String componentId, String window,\n            boolean includeSys) throws NotAliveException, AuthorizationException, TException {\n        try {\n            getComponentPageInfoCalls.mark();\n            CommonTopoInfo info = getCommonTopoInfo(topoId, \"getComponentPageInfo\");\n            if (info.base == null) {\n                throw new NotAliveException(topoId);\n            }\n            StormTopology topology = info.topology;\n            Map<String, Object> topoConf = info.topoConf;\n            Assignment assignment = info.assignment;\n            Map<List<Long>, List<Object>> exec2NodePort = new HashMap<>();\n            Map<String, String> nodeToHost;\n            Map<List<Long>, List<Object>> exec2HostPort = new HashMap<>();\n            if (assignment != null) {\n                Map<List<Long>, NodeInfo> execToNodeInfo = assignment.get_executor_node_port();\n                nodeToHost = assignment.get_node_host();\n                for (Entry<List<Long>, NodeInfo> entry: execToNodeInfo.entrySet()) {\n                    NodeInfo ni = entry.getValue();\n                    List<Object> nodePort = Arrays.asList(ni.get_node(), ni.get_port_iterator().next());\n                    List<Object> hostPort = Arrays.asList(nodeToHost.get(ni.get_node()), ni.get_port_iterator().next());\n                    exec2NodePort.put(entry.getKey(), nodePort);\n                    exec2HostPort.put(entry.getKey(), hostPort);\n                }\n            } else {\n                nodeToHost = Collections.emptyMap();\n            }\n             \n            ComponentPageInfo compPageInfo = StatsUtil.aggCompExecsStats(exec2HostPort, info.taskToComponent, info.beats, window, \n                    includeSys, topoId, topology, componentId);\n            if (compPageInfo.get_component_type() == ComponentType.SPOUT) {\n                compPageInfo.set_resources_map(setResourcesDefaultIfNotSet(\n                        ResourceUtils.getSpoutsResources(topology, topoConf), componentId, topoConf));\n            } else { //bolt\n                compPageInfo.set_resources_map(setResourcesDefaultIfNotSet(\n                        ResourceUtils.getBoltsResources(topology, topoConf), componentId, topoConf));\n            }\n            compPageInfo.set_topology_name(info.topoName);\n            compPageInfo.set_errors(stormClusterState.errors(topoId, componentId));\n            compPageInfo.set_topology_status(extractStatusStr(info.base));\n            if (info.base.is_set_component_debug()) {\n                DebugOptions debug = info.base.get_component_debug().get(componentId);\n                if (debug != null) {\n                    compPageInfo.set_debug_options(debug);\n                }\n            }\n            // Add the event logger details.\n            Map<String, List<Integer>> compToTasks = Utils.reverseMap(info.taskToComponent);\n            if (compToTasks.containsKey(StormCommon.EVENTLOGGER_COMPONENT_ID)) {\n                List<Integer> tasks = compToTasks.get(StormCommon.EVENTLOGGER_COMPONENT_ID);\n                tasks.sort(null);\n                // Find the task the events from this component route to.\n                int taskIndex = TupleUtils.listHashCode(Arrays.asList(componentId)) %\n                        tasks.size();\n                int taskId = tasks.get(taskIndex);\n                String host = null;\n                Integer port = null;\n                for (Entry<List<Long>, List<Object>> entry: exec2HostPort.entrySet()) {\n                    int start = entry.getKey().get(0).intValue();\n                    int end = entry.getKey().get(1).intValue();\n                    if (taskId >= start && taskId <= end) {\n                        host = (String) entry.getValue().get(0);\n                        port = ((Number) entry.getValue().get(1)).intValue();\n                        break;\n                    }\n                }\n                \n                if (host != null && port != null) {\n                    compPageInfo.set_eventlog_host(host);\n                    compPageInfo.set_eventlog_port(port);\n                }\n            }\n            return compPageInfo;\n        } catch (Exception e) {\n            LOG.warn(\"getComponentPageInfo exception. (topo id='{}')\", topoId, e);\n            if (e instanceof TException) {\n                throw (TException)e;\n            }\n            throw new RuntimeException(e);\n        }\n    }",
                "code_after_change": "    public ComponentPageInfo getComponentPageInfo(String topoId, String componentId, String window,\n            boolean includeSys) throws NotAliveException, AuthorizationException, TException {\n        try {\n            getComponentPageInfoCalls.mark();\n            CommonTopoInfo info = getCommonTopoInfo(topoId, \"getComponentPageInfo\");\n            if (info.base == null) {\n                throw new NotAliveException(topoId);\n            }\n            StormTopology topology = info.topology;\n            Map<String, Object> topoConf = info.topoConf;\n            Assignment assignment = info.assignment;\n            Map<List<Long>, List<Object>> exec2NodePort = new HashMap<>();\n            Map<String, String> nodeToHost;\n            Map<List<Long>, List<Object>> exec2HostPort = new HashMap<>();\n            if (assignment != null) {\n                Map<List<Long>, NodeInfo> execToNodeInfo = assignment.get_executor_node_port();\n                nodeToHost = assignment.get_node_host();\n                for (Entry<List<Long>, NodeInfo> entry: execToNodeInfo.entrySet()) {\n                    NodeInfo ni = entry.getValue();\n                    List<Object> nodePort = Arrays.asList(ni.get_node(), ni.get_port_iterator().next());\n                    List<Object> hostPort = Arrays.asList(nodeToHost.get(ni.get_node()), ni.get_port_iterator().next());\n                    exec2NodePort.put(entry.getKey(), nodePort);\n                    exec2HostPort.put(entry.getKey(), hostPort);\n                }\n            } else {\n                nodeToHost = Collections.emptyMap();\n            }\n             \n            ComponentPageInfo compPageInfo = StatsUtil.aggCompExecsStats(exec2HostPort, info.taskToComponent, info.beats, window, \n                    includeSys, topoId, topology, componentId);\n            if (compPageInfo.get_component_type() == ComponentType.SPOUT) {\n                compPageInfo.set_resources_map(setResourcesDefaultIfNotSet(\n                        ResourceUtils.getSpoutsResources(topology, topoConf), componentId, topoConf));\n            } else { //bolt\n                compPageInfo.set_resources_map(setResourcesDefaultIfNotSet(\n                        ResourceUtils.getBoltsResources(topology, topoConf), componentId, topoConf));\n            }\n            compPageInfo.set_topology_name(info.topoName);\n            compPageInfo.set_errors(stormClusterState.errors(topoId, componentId));\n            compPageInfo.set_topology_status(extractStatusStr(info.base));\n            if (info.base.is_set_component_debug()) {\n                DebugOptions debug = info.base.get_component_debug().get(componentId);\n                if (debug != null) {\n                    compPageInfo.set_debug_options(debug);\n                }\n            }\n            // Add the event logger details.\n            Map<String, List<Integer>> compToTasks = Utils.reverseMap(info.taskToComponent);\n            if (compToTasks.containsKey(StormCommon.EVENTLOGGER_COMPONENT_ID)) {\n                List<Integer> tasks = compToTasks.get(StormCommon.EVENTLOGGER_COMPONENT_ID);\n                tasks.sort(null);\n                // Find the task the events from this component route to.\n                int taskIndex = TupleUtils.chooseTaskIndex(Collections.singletonList(componentId), tasks.size());\n                int taskId = tasks.get(taskIndex);\n                String host = null;\n                Integer port = null;\n                for (Entry<List<Long>, List<Object>> entry: exec2HostPort.entrySet()) {\n                    int start = entry.getKey().get(0).intValue();\n                    int end = entry.getKey().get(1).intValue();\n                    if (taskId >= start && taskId <= end) {\n                        host = (String) entry.getValue().get(0);\n                        port = ((Number) entry.getValue().get(1)).intValue();\n                        break;\n                    }\n                }\n                \n                if (host != null && port != null) {\n                    compPageInfo.set_eventlog_host(host);\n                    compPageInfo.set_eventlog_port(port);\n                }\n            }\n            return compPageInfo;\n        } catch (Exception e) {\n            LOG.warn(\"getComponentPageInfo exception. (topo id='{}')\", topoId, e);\n            if (e instanceof TException) {\n                throw (TException)e;\n            }\n            throw new RuntimeException(e);\n        }\n    }"
            },
            "storm-core.src.jvm.org.apache.storm.utils.TupleUtils.isTick": {
                "code_before_change": "    public static boolean isTick(Tuple tuple) {\n      return tuple != null\n             && Constants.SYSTEM_COMPONENT_ID.equals(tuple.getSourceComponent())\n             && Constants.SYSTEM_TICK_STREAM_ID.equals(tuple.getSourceStreamId());\n    }",
                "code_after_change": "    public static boolean isTick(Tuple tuple) {\n      return tuple != null\n             && Constants.SYSTEM_COMPONENT_ID.equals(tuple.getSourceComponent())\n             && Constants.SYSTEM_TICK_STREAM_ID.equals(tuple.getSourceStreamId());\n    }"
            },
            "storm-core.src.jvm.org.apache.storm.daemon.GrouperFactory.prepare": {
                "code_before_change": "        public void prepare(WorkerTopologyContext context, GlobalStreamId stream, List<Integer> targetTasks) {\n            customStreamGrouping.prepare(context, stream, targetTasks);\n        }",
                "code_after_change": "        public void prepare(WorkerTopologyContext context, GlobalStreamId stream, List<Integer> targetTasks) {\n            customStreamGrouping.prepare(context, stream, targetTasks);\n        }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Correct",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report precisely identifies the root cause as an ArrayIndexOutOfBoundsException in the Nimbus class's getComponentPageInfo method, which matches the ground truth. The fix suggestion to ensure the index is non-negative before accessing the list is correct and aligns with the developer's fix. The problem location is precisely identified as the Nimbus.getComponentPageInfo method, which is part of the ground truth methods. There is no wrong information in the bug report as all details are consistent with the context of the bug."
        }
    },
    {
        "filename": "STORM-3079.json",
        "code_diff": {
            "storm-server.src.main.java.org.apache.storm.daemon.nimbus.Nimbus.rebalance": {
                "code_before_change": "    public void rebalance(String topoName, RebalanceOptions options)\n        throws NotAliveException, InvalidTopologyException, AuthorizationException, TException {\n        rebalanceCalls.mark();\n        assertTopoActive(topoName, true);\n        try {\n            Map<String, Object> topoConf = tryReadTopoConfFromName(topoName);\n            topoConf = Utils.merge(conf, topoConf);\n            final String operation = \"rebalance\";\n            checkAuthorization(topoName, topoConf, operation);\n            // Set principal in RebalanceOptions to nil because users are not suppose to set this\n            options.set_principal(null);\n            Map<String, Integer> execOverrides = options.is_set_num_executors() ? options.get_num_executors() : Collections.emptyMap();\n            for (Integer value : execOverrides.values()) {\n                if (value == null || value <= 0) {\n                    throw new InvalidTopologyException(\"Number of executors must be greater than 0\");\n                }\n            }\n            if (options.is_set_topology_conf_overrides()) {\n                Map<String, Object> topoConfigOverrides = Utils.parseJson(options.get_topology_conf_overrides());\n                //Clean up some things the user should not set.  (Not a security issue, just might confuse the topology)\n                topoConfigOverrides.remove(Config.TOPOLOGY_SUBMITTER_PRINCIPAL);\n                topoConfigOverrides.remove(Config.TOPOLOGY_SUBMITTER_USER);\n                topoConfigOverrides.remove(Config.STORM_ZOOKEEPER_SUPERACL);\n                topoConfigOverrides.remove(Config.STORM_ZOOKEEPER_TOPOLOGY_AUTH_SCHEME);\n                topoConfigOverrides.remove(Config.STORM_ZOOKEEPER_TOPOLOGY_AUTH_PAYLOAD);\n                if ((boolean) conf.getOrDefault(DaemonConfig.STORM_TOPOLOGY_CLASSPATH_BEGINNING_ENABLED, false)) {\n                    topoConfigOverrides.remove(Config.TOPOLOGY_CLASSPATH_BEGINNING);\n                }\n                options.set_topology_conf_overrides(JSONValue.toJSONString(topoConfigOverrides));\n            }\n            Subject subject = getSubject();\n            if (subject != null) {\n                options.set_principal(subject.getPrincipals().iterator().next().getName());\n            }\n\n            transitionName(topoName, TopologyActions.REBALANCE, options, true);\n            notifyTopologyActionListener(topoName, operation);\n        } catch (Exception e) {\n            LOG.warn(\"rebalance topology exception. (topology name='{}')\", topoName, e);\n            if (e instanceof TException) {\n                throw (TException) e;\n            }\n            throw new RuntimeException(e);\n        }\n    }",
                "code_after_change": "    public void rebalance(String topoName, RebalanceOptions options)\n        throws NotAliveException, InvalidTopologyException, AuthorizationException, TException {\n        rebalanceCalls.mark();\n        assertTopoActive(topoName, true);\n        try {\n            Map<String, Object> topoConf = tryReadTopoConfFromName(topoName);\n            topoConf = Utils.merge(conf, topoConf);\n            final String operation = \"rebalance\";\n            checkAuthorization(topoName, topoConf, operation);\n            // Set principal in RebalanceOptions to nil because users are not suppose to set this\n            options.set_principal(null);\n            Map<String, Integer> execOverrides = options.is_set_num_executors() ? options.get_num_executors() : Collections.emptyMap();\n            for (Integer value : execOverrides.values()) {\n                if (value == null || value <= 0) {\n                    throw new WrappedInvalidTopologyException(\"Number of executors must be greater than 0\");\n                }\n            }\n            if (options.is_set_topology_conf_overrides()) {\n                Map<String, Object> topoConfigOverrides = Utils.parseJson(options.get_topology_conf_overrides());\n                //Clean up some things the user should not set.  (Not a security issue, just might confuse the topology)\n                topoConfigOverrides.remove(Config.TOPOLOGY_SUBMITTER_PRINCIPAL);\n                topoConfigOverrides.remove(Config.TOPOLOGY_SUBMITTER_USER);\n                topoConfigOverrides.remove(Config.STORM_ZOOKEEPER_SUPERACL);\n                topoConfigOverrides.remove(Config.STORM_ZOOKEEPER_TOPOLOGY_AUTH_SCHEME);\n                topoConfigOverrides.remove(Config.STORM_ZOOKEEPER_TOPOLOGY_AUTH_PAYLOAD);\n                if ((boolean) conf.getOrDefault(DaemonConfig.STORM_TOPOLOGY_CLASSPATH_BEGINNING_ENABLED, false)) {\n                    topoConfigOverrides.remove(Config.TOPOLOGY_CLASSPATH_BEGINNING);\n                }\n                options.set_topology_conf_overrides(JSONValue.toJSONString(topoConfigOverrides));\n            }\n            Subject subject = getSubject();\n            if (subject != null) {\n                options.set_principal(subject.getPrincipals().iterator().next().getName());\n            }\n\n            transitionName(topoName, TopologyActions.REBALANCE, options, true);\n            notifyTopologyActionListener(topoName, operation);\n        } catch (Exception e) {\n            LOG.warn(\"rebalance topology exception. (topology name='{}')\", topoName, e);\n            if (e instanceof TException) {\n                throw (TException) e;\n            }\n            throw new RuntimeException(e);\n        }\n    }"
            },
            "storm-client.src.jvm.org.apache.storm.blobstore.BlobStoreAclHandler.hasPermissions": {
                "code_before_change": "    public void hasPermissions(List<AccessControl> acl, int mask, Subject who, String key) throws AuthorizationException {\n        if (!doAclValidation) {\n            return;\n        }\n        Set<String> user = constructUserFromPrincipals(who);\n        LOG.debug(\"user {}\", user);\n        if (checkForValidUsers(who, mask)) {\n            return;\n        }\n        for (AccessControl ac : acl) {\n            int allowed = getAllowed(ac, user);\n            mask = ~allowed & mask;\n            LOG.debug(\" user: {} allowed: {} disallowed: {} key: {}\", user, allowed, mask, key);\n        }\n        if (mask == 0) {\n            return;\n        }\n        throw new AuthorizationException(\n            user + \" does not have \" + namedPerms(mask) + \" access to \" + key);\n    }",
                "code_after_change": "    public void hasPermissions(List<AccessControl> acl, int mask, Subject who, String key) throws AuthorizationException {\n        if (!doAclValidation) {\n            return;\n        }\n        Set<String> user = constructUserFromPrincipals(who);\n        LOG.debug(\"user {}\", user);\n        if (checkForValidUsers(who, mask)) {\n            return;\n        }\n        for (AccessControl ac : acl) {\n            int allowed = getAllowed(ac, user);\n            mask = ~allowed & mask;\n            LOG.debug(\" user: {} allowed: {} disallowed: {} key: {}\", user, allowed, mask, key);\n        }\n        if (mask == 0) {\n            return;\n        }\n        throw new WrappedAuthorizationException(\n            user + \" does not have \" + namedPerms(mask) + \" access to \" + key);\n    }"
            },
            "storm-client.src.jvm.org.apache.storm.utils.Utils.memoizedLocalHostname": {
                "code_before_change": "    public static String memoizedLocalHostname() throws UnknownHostException {\n        if (memoizedLocalHostnameString == null) {\n            memoizedLocalHostnameString = localHostname();\n        }\n        return memoizedLocalHostnameString;\n    }",
                "code_after_change": "    public static String memoizedLocalHostname() throws UnknownHostException {\n        if (memoizedLocalHostnameString == null) {\n            memoizedLocalHostnameString = localHostname();\n        }\n        return memoizedLocalHostnameString;\n    }"
            },
            "storm-server.src.main.java.org.apache.storm.daemon.nimbus.Nimbus.getTopologyPageInfo": {
                "code_before_change": "    public TopologyPageInfo getTopologyPageInfo(String topoId, String window, boolean includeSys)\n        throws NotAliveException, AuthorizationException, TException {\n        try {\n            getTopologyPageInfoCalls.mark();\n            CommonTopoInfo common = getCommonTopoInfo(topoId, \"getTopologyPageInfo\");\n            String topoName = common.topoName;\n            IStormClusterState state = stormClusterState;\n            int launchTimeSecs = common.launchTimeSecs;\n            Assignment assignment = common.assignment;\n            Map<List<Integer>, Map<String, Object>> beats = common.beats;\n            Map<Integer, String> taskToComp = common.taskToComponent;\n            StormTopology topology = common.topology;\n            Map<String, Object> topoConf = Utils.merge(conf, common.topoConf);\n            StormBase base = common.base;\n            if (base == null) {\n                throw new NotAliveException(topoId);\n            }\n            Map<WorkerSlot, WorkerResources> workerToResources = getWorkerResourcesForTopology(topoId);\n            List<WorkerSummary> workerSummaries = null;\n            Map<List<Long>, List<Object>> exec2NodePort = new HashMap<>();\n            if (assignment != null) {\n                Map<List<Long>, NodeInfo> execToNodeInfo = assignment.get_executor_node_port();\n                Map<String, String> nodeToHost = assignment.get_node_host();\n                for (Entry<List<Long>, NodeInfo> entry : execToNodeInfo.entrySet()) {\n                    NodeInfo ni = entry.getValue();\n                    List<Object> nodePort = Arrays.asList(ni.get_node(), ni.get_port_iterator().next());\n                    exec2NodePort.put(entry.getKey(), nodePort);\n                }\n\n                workerSummaries = StatsUtil.aggWorkerStats(topoId,\n                                                           topoName,\n                                                           taskToComp,\n                                                           beats,\n                                                           exec2NodePort,\n                                                           nodeToHost,\n                                                           workerToResources,\n                                                           includeSys,\n                                                           true); //this is the topology page, so we know the user is authorized\n            }\n\n            TopologyPageInfo topoPageInfo = StatsUtil.aggTopoExecsStats(topoId,\n                                                                        exec2NodePort,\n                                                                        taskToComp,\n                                                                        beats,\n                                                                        topology,\n                                                                        window,\n                                                                        includeSys,\n                                                                        state);\n\n            if (topology.is_set_storm_version()) {\n                topoPageInfo.set_storm_version(topology.get_storm_version());\n            }\n\n            Map<String, NormalizedResourceRequest> spoutResources = ResourceUtils.getSpoutsResources(topology, topoConf);\n            for (Entry<String, ComponentAggregateStats> entry : topoPageInfo.get_id_to_spout_agg_stats().entrySet()) {\n                CommonAggregateStats commonStats = entry.getValue().get_common_stats();\n                setResourcesDefaultIfNotSet(spoutResources, entry.getKey(), topoConf);\n                commonStats.set_resources_map(spoutResources.get(entry.getKey()).toNormalizedMap());\n            }\n\n            Map<String, NormalizedResourceRequest> boltResources = ResourceUtils.getBoltsResources(topology, topoConf);\n            for (Entry<String, ComponentAggregateStats> entry : topoPageInfo.get_id_to_bolt_agg_stats().entrySet()) {\n                CommonAggregateStats commonStats = entry.getValue().get_common_stats();\n                setResourcesDefaultIfNotSet(boltResources, entry.getKey(), topoConf);\n                commonStats.set_resources_map(boltResources.get(entry.getKey()).toNormalizedMap());\n            }\n\n            if (workerSummaries != null) {\n                topoPageInfo.set_workers(workerSummaries);\n            }\n            if (base.is_set_owner()) {\n                topoPageInfo.set_owner(base.get_owner());\n            }\n\n            if (base.is_set_topology_version()) {\n                topoPageInfo.set_topology_version(base.get_topology_version());\n            }\n\n            String schedStatus = idToSchedStatus.get().get(topoId);\n            if (schedStatus != null) {\n                topoPageInfo.set_sched_status(schedStatus);\n            }\n            TopologyResources resources = getResourcesForTopology(topoId, base);\n            if (resources != null) {\n                topoPageInfo.set_requested_memonheap(resources.getRequestedMemOnHeap());\n                topoPageInfo.set_requested_memoffheap(resources.getRequestedMemOffHeap());\n                topoPageInfo.set_requested_cpu(resources.getRequestedCpu());\n                topoPageInfo.set_assigned_memonheap(resources.getAssignedMemOnHeap());\n                topoPageInfo.set_assigned_memoffheap(resources.getAssignedMemOffHeap());\n                topoPageInfo.set_assigned_cpu(resources.getAssignedCpu());\n                topoPageInfo.set_requested_shared_off_heap_memory(resources.getRequestedSharedMemOffHeap());\n                topoPageInfo.set_requested_regular_off_heap_memory(resources.getRequestedNonSharedMemOffHeap());\n                topoPageInfo.set_requested_shared_on_heap_memory(resources.getRequestedSharedMemOnHeap());\n                topoPageInfo.set_requested_regular_on_heap_memory(resources.getRequestedNonSharedMemOnHeap());\n                topoPageInfo.set_assigned_shared_off_heap_memory(resources.getAssignedSharedMemOffHeap());\n                topoPageInfo.set_assigned_regular_off_heap_memory(resources.getAssignedNonSharedMemOffHeap());\n                topoPageInfo.set_assigned_shared_on_heap_memory(resources.getAssignedSharedMemOnHeap());\n                topoPageInfo.set_assigned_regular_on_heap_memory(resources.getAssignedNonSharedMemOnHeap());\n            }\n            topoPageInfo.set_name(topoName);\n            topoPageInfo.set_status(extractStatusStr(base));\n            topoPageInfo.set_uptime_secs(Time.deltaSecs(launchTimeSecs));\n            topoPageInfo.set_topology_conf(JSONValue.toJSONString(topoConf));\n            topoPageInfo.set_replication_count(getBlobReplicationCount(ConfigUtils.masterStormCodeKey(topoId)));\n            if (base.is_set_component_debug()) {\n                DebugOptions debug = base.get_component_debug().get(topoId);\n                if (debug != null) {\n                    topoPageInfo.set_debug_options(debug);\n                }\n            }\n            return topoPageInfo;\n        } catch (Exception e) {\n            LOG.warn(\"Get topo page info exception. (topology id='{}')\", topoId, e);\n            if (e instanceof TException) {\n                throw (TException) e;\n            }\n            throw new RuntimeException(e);\n        }\n    }",
                "code_after_change": "    public TopologyPageInfo getTopologyPageInfo(String topoId, String window, boolean includeSys)\n        throws NotAliveException, AuthorizationException, TException {\n        try {\n            getTopologyPageInfoCalls.mark();\n            CommonTopoInfo common = getCommonTopoInfo(topoId, \"getTopologyPageInfo\");\n            String topoName = common.topoName;\n            IStormClusterState state = stormClusterState;\n            int launchTimeSecs = common.launchTimeSecs;\n            Assignment assignment = common.assignment;\n            Map<List<Integer>, Map<String, Object>> beats = common.beats;\n            Map<Integer, String> taskToComp = common.taskToComponent;\n            StormTopology topology = common.topology;\n            Map<String, Object> topoConf = Utils.merge(conf, common.topoConf);\n            StormBase base = common.base;\n            if (base == null) {\n                throw new WrappedNotAliveException(topoId);\n            }\n            Map<WorkerSlot, WorkerResources> workerToResources = getWorkerResourcesForTopology(topoId);\n            List<WorkerSummary> workerSummaries = null;\n            Map<List<Long>, List<Object>> exec2NodePort = new HashMap<>();\n            if (assignment != null) {\n                Map<List<Long>, NodeInfo> execToNodeInfo = assignment.get_executor_node_port();\n                Map<String, String> nodeToHost = assignment.get_node_host();\n                for (Entry<List<Long>, NodeInfo> entry : execToNodeInfo.entrySet()) {\n                    NodeInfo ni = entry.getValue();\n                    List<Object> nodePort = Arrays.asList(ni.get_node(), ni.get_port_iterator().next());\n                    exec2NodePort.put(entry.getKey(), nodePort);\n                }\n\n                workerSummaries = StatsUtil.aggWorkerStats(topoId,\n                                                           topoName,\n                                                           taskToComp,\n                                                           beats,\n                                                           exec2NodePort,\n                                                           nodeToHost,\n                                                           workerToResources,\n                                                           includeSys,\n                                                           true); //this is the topology page, so we know the user is authorized\n            }\n\n            TopologyPageInfo topoPageInfo = StatsUtil.aggTopoExecsStats(topoId,\n                                                                        exec2NodePort,\n                                                                        taskToComp,\n                                                                        beats,\n                                                                        topology,\n                                                                        window,\n                                                                        includeSys,\n                                                                        state);\n\n            if (topology.is_set_storm_version()) {\n                topoPageInfo.set_storm_version(topology.get_storm_version());\n            }\n\n            Map<String, NormalizedResourceRequest> spoutResources = ResourceUtils.getSpoutsResources(topology, topoConf);\n            for (Entry<String, ComponentAggregateStats> entry : topoPageInfo.get_id_to_spout_agg_stats().entrySet()) {\n                CommonAggregateStats commonStats = entry.getValue().get_common_stats();\n                setResourcesDefaultIfNotSet(spoutResources, entry.getKey(), topoConf);\n                commonStats.set_resources_map(spoutResources.get(entry.getKey()).toNormalizedMap());\n            }\n\n            Map<String, NormalizedResourceRequest> boltResources = ResourceUtils.getBoltsResources(topology, topoConf);\n            for (Entry<String, ComponentAggregateStats> entry : topoPageInfo.get_id_to_bolt_agg_stats().entrySet()) {\n                CommonAggregateStats commonStats = entry.getValue().get_common_stats();\n                setResourcesDefaultIfNotSet(boltResources, entry.getKey(), topoConf);\n                commonStats.set_resources_map(boltResources.get(entry.getKey()).toNormalizedMap());\n            }\n\n            if (workerSummaries != null) {\n                topoPageInfo.set_workers(workerSummaries);\n            }\n            if (base.is_set_owner()) {\n                topoPageInfo.set_owner(base.get_owner());\n            }\n\n            if (base.is_set_topology_version()) {\n                topoPageInfo.set_topology_version(base.get_topology_version());\n            }\n\n            String schedStatus = idToSchedStatus.get().get(topoId);\n            if (schedStatus != null) {\n                topoPageInfo.set_sched_status(schedStatus);\n            }\n            TopologyResources resources = getResourcesForTopology(topoId, base);\n            if (resources != null) {\n                topoPageInfo.set_requested_memonheap(resources.getRequestedMemOnHeap());\n                topoPageInfo.set_requested_memoffheap(resources.getRequestedMemOffHeap());\n                topoPageInfo.set_requested_cpu(resources.getRequestedCpu());\n                topoPageInfo.set_assigned_memonheap(resources.getAssignedMemOnHeap());\n                topoPageInfo.set_assigned_memoffheap(resources.getAssignedMemOffHeap());\n                topoPageInfo.set_assigned_cpu(resources.getAssignedCpu());\n                topoPageInfo.set_requested_shared_off_heap_memory(resources.getRequestedSharedMemOffHeap());\n                topoPageInfo.set_requested_regular_off_heap_memory(resources.getRequestedNonSharedMemOffHeap());\n                topoPageInfo.set_requested_shared_on_heap_memory(resources.getRequestedSharedMemOnHeap());\n                topoPageInfo.set_requested_regular_on_heap_memory(resources.getRequestedNonSharedMemOnHeap());\n                topoPageInfo.set_assigned_shared_off_heap_memory(resources.getAssignedSharedMemOffHeap());\n                topoPageInfo.set_assigned_regular_off_heap_memory(resources.getAssignedNonSharedMemOffHeap());\n                topoPageInfo.set_assigned_shared_on_heap_memory(resources.getAssignedSharedMemOnHeap());\n                topoPageInfo.set_assigned_regular_on_heap_memory(resources.getAssignedNonSharedMemOnHeap());\n            }\n            topoPageInfo.set_name(topoName);\n            topoPageInfo.set_status(extractStatusStr(base));\n            topoPageInfo.set_uptime_secs(Time.deltaSecs(launchTimeSecs));\n            topoPageInfo.set_topology_conf(JSONValue.toJSONString(topoConf));\n            topoPageInfo.set_replication_count(getBlobReplicationCount(ConfigUtils.masterStormCodeKey(topoId)));\n            if (base.is_set_component_debug()) {\n                DebugOptions debug = base.get_component_debug().get(topoId);\n                if (debug != null) {\n                    topoPageInfo.set_debug_options(debug);\n                }\n            }\n            return topoPageInfo;\n        } catch (Exception e) {\n            LOG.warn(\"Get topo page info exception. (topology id='{}')\", topoId, e);\n            if (e instanceof TException) {\n                throw (TException) e;\n            }\n            throw new RuntimeException(e);\n        }\n    }"
            },
            "storm-server.src.main.java.org.apache.storm.localizer.LocalizedResource.downloadToTempLocation": {
                "code_before_change": "    public long downloadToTempLocation(ClientBlobStore store) throws IOException, KeyNotFoundException, AuthorizationException {\n        String key = getKey();\n        ReadableBlobMeta meta = store.getBlobMeta(key);\n        if (!ServerUtils.canUserReadBlob(meta, user, conf)) {\n            throw new AuthorizationException(user + \" does not have READ access to \" + key);\n        }\n        long version;\n        Path downloadFile;\n        Path finalLocation;\n        try (InputStreamWithMeta in = store.getBlob(key)) {\n            version = in.getVersion();\n            finalLocation = constructBlobWithVersionFileName(baseDir, getKey(), version);\n            if (uncompressed) {\n                // we need to download to temp file and then unpack into the one requested\n                downloadFile = tmpOutputLocation();\n            } else {\n                downloadFile = finalLocation;\n            }\n            byte[] buffer = new byte[1024];\n            int len;\n            LOG.debug(\"Downloading {} to {}\", key, downloadFile);\n            Path parent = downloadFile.getParent();\n            if (!Files.exists(parent)) {\n                //There is a race here that we can still lose\n                try {\n                    Files.createDirectory(parent);\n                } catch (FileAlreadyExistsException e) {\n                    //Ignored\n                }\n            }\n            try (FileOutputStream out = new FileOutputStream(downloadFile.toFile())) {\n                while ((len = in.read(buffer)) >= 0) {\n                    out.write(buffer, 0, len);\n                }\n            }\n        }\n        if (uncompressed) {\n            ServerUtils.unpack(downloadFile.toFile(), finalLocation.toFile(), symLinksDisabled);\n            LOG.debug(\"Uncompressed {} to: {}\", downloadFile, finalLocation);\n        }\n        setBlobPermissions(conf, user, finalLocation);\n        return version;\n    }",
                "code_after_change": "    public long downloadToTempLocation(ClientBlobStore store) throws IOException, KeyNotFoundException, AuthorizationException {\n        String key = getKey();\n        ReadableBlobMeta meta = store.getBlobMeta(key);\n        if (!ServerUtils.canUserReadBlob(meta, user, conf)) {\n            throw new WrappedAuthorizationException(user + \" does not have READ access to \" + key);\n        }\n        long version;\n        Path downloadFile;\n        Path finalLocation;\n        try (InputStreamWithMeta in = store.getBlob(key)) {\n            version = in.getVersion();\n            finalLocation = constructBlobWithVersionFileName(baseDir, getKey(), version);\n            if (uncompressed) {\n                // we need to download to temp file and then unpack into the one requested\n                downloadFile = tmpOutputLocation();\n            } else {\n                downloadFile = finalLocation;\n            }\n            byte[] buffer = new byte[1024];\n            int len;\n            LOG.debug(\"Downloading {} to {}\", key, downloadFile);\n            Path parent = downloadFile.getParent();\n            if (!Files.exists(parent)) {\n                //There is a race here that we can still lose\n                try {\n                    Files.createDirectory(parent);\n                } catch (FileAlreadyExistsException e) {\n                    //Ignored\n                }\n            }\n            try (FileOutputStream out = new FileOutputStream(downloadFile.toFile())) {\n                while ((len = in.read(buffer)) >= 0) {\n                    out.write(buffer, 0, len);\n                }\n            }\n        }\n        if (uncompressed) {\n            ServerUtils.unpack(downloadFile.toFile(), finalLocation.toFile(), symLinksDisabled);\n            LOG.debug(\"Uncompressed {} to: {}\", downloadFile, finalLocation);\n        }\n        setBlobPermissions(conf, user, finalLocation);\n        return version;\n    }"
            },
            "external.storm-hdfs-blobstore.src.main.java.org.apache.storm.hdfs.blobstore.HdfsBlobStore.getStoredBlobMeta": {
                "code_before_change": "    private SettableBlobMeta getStoredBlobMeta(String key) throws KeyNotFoundException {\n        InputStream in = null;\n        try {\n            BlobStoreFile pf = hbs.read(META_PREFIX + key);\n            try {\n                in = pf.getInputStream();\n            } catch (FileNotFoundException fnf) {\n                throw new KeyNotFoundException(key);\n            }\n            ByteArrayOutputStream out = new ByteArrayOutputStream();\n            byte[] buffer = new byte[2048];\n            int len;\n            while ((len = in.read(buffer)) > 0) {\n                out.write(buffer, 0, len);\n            }\n            in.close();\n            in = null;\n            return Utils.thriftDeserialize(SettableBlobMeta.class, out.toByteArray());\n        } catch (IOException e) {\n            throw new RuntimeException(e);\n        } finally {\n            if (in != null) {\n                try {\n                    in.close();\n                } catch (IOException e) {\n                    //Ignored\n                }\n            }\n        }\n    }",
                "code_after_change": "    private SettableBlobMeta getStoredBlobMeta(String key) throws KeyNotFoundException {\n        InputStream in = null;\n        try {\n            BlobStoreFile pf = hbs.read(META_PREFIX + key);\n            try {\n                in = pf.getInputStream();\n            } catch (FileNotFoundException fnf) {\n                throw new WrappedKeyNotFoundException(key);\n            }\n            ByteArrayOutputStream out = new ByteArrayOutputStream();\n            byte[] buffer = new byte[2048];\n            int len;\n            while ((len = in.read(buffer)) > 0) {\n                out.write(buffer, 0, len);\n            }\n            in.close();\n            in = null;\n            return Utils.thriftDeserialize(SettableBlobMeta.class, out.toByteArray());\n        } catch (IOException e) {\n            throw new RuntimeException(e);\n        } finally {\n            if (in != null) {\n                try {\n                    in.close();\n                } catch (IOException e) {\n                    //Ignored\n                }\n            }\n        }\n    }"
            },
            "storm-client.src.jvm.org.apache.storm.blobstore.BlobStoreAclHandler.validateSettableACLs": {
                "code_before_change": "    public static void validateSettableACLs(String key, List<AccessControl> acls) throws AuthorizationException {\n        Set<String> aclUsers = new HashSet<>();\n        List<String> duplicateUsers = new ArrayList<>();\n        for (AccessControl acl : acls) {\n            String aclUser = acl.get_name();\n            if (!StringUtils.isEmpty(aclUser) && !aclUsers.add(aclUser)) {\n                LOG.error(\"'{}' user can't appear more than once in the ACLs\", aclUser);\n                duplicateUsers.add(aclUser);\n            }\n        }\n        if (duplicateUsers.size() > 0) {\n            String errorMessage = \"user \" + Arrays.toString(duplicateUsers.toArray())\n                                  + \" can't appear more than once in the ACLs for key [\" + key + \"].\";\n            throw new AuthorizationException(errorMessage);\n        }\n    }",
                "code_after_change": "    public static void validateSettableACLs(String key, List<AccessControl> acls) throws AuthorizationException {\n        Set<String> aclUsers = new HashSet<>();\n        List<String> duplicateUsers = new ArrayList<>();\n        for (AccessControl acl : acls) {\n            String aclUser = acl.get_name();\n            if (!StringUtils.isEmpty(aclUser) && !aclUsers.add(aclUser)) {\n                LOG.error(\"'{}' user can't appear more than once in the ACLs\", aclUser);\n                duplicateUsers.add(aclUser);\n            }\n        }\n        if (duplicateUsers.size() > 0) {\n            String errorMessage = \"user \" + Arrays.toString(duplicateUsers.toArray())\n                                  + \" can't appear more than once in the ACLs for key [\" + key + \"].\";\n            throw new WrappedAuthorizationException(errorMessage);\n        }\n    }"
            },
            "storm-client.src.jvm.org.apache.storm.utils.Utils.run": {
                "code_before_change": "            public void run() {\n                try {\n                    LOG.info(\"Halting after {} seconds\", numSecs);\n                    Time.sleepSecs(numSecs);\n                    LOG.warn(\"Forcing Halt...\");\n                    Runtime.getRuntime().halt(20);\n                } catch (Exception e) {\n                    LOG.warn(\"Exception in the ShutDownHook\", e);\n                }\n            }",
                "code_after_change": "            public void run() {\n                try {\n                    LOG.info(\"Halting after {} seconds\", numSecs);\n                    Time.sleepSecs(numSecs);\n                    LOG.warn(\"Forcing Halt...\");\n                    Runtime.getRuntime().halt(20);\n                } catch (Exception e) {\n                    LOG.warn(\"Exception in the ShutDownHook\", e);\n                }\n            }"
            },
            "storm-server.src.main.java.org.apache.storm.daemon.nimbus.Nimbus.tryReadTopoConfFromName": {
                "code_before_change": "    private Map<String, Object> tryReadTopoConfFromName(final String topoName) throws NotAliveException,\n        AuthorizationException, IOException {\n        IStormClusterState state = stormClusterState;\n        String topoId = state.getTopoId(topoName)\n                             .orElseThrow(() -> new NotAliveException(topoName + \" is not alive\"));\n        return tryReadTopoConf(topoId, topoCache);\n    }",
                "code_after_change": "    private Map<String, Object> tryReadTopoConfFromName(final String topoName) throws NotAliveException,\n        AuthorizationException, IOException {\n        IStormClusterState state = stormClusterState;\n        String topoId = state.getTopoId(topoName)\n                             .orElseThrow(() -> new WrappedNotAliveException(topoName + \" is not alive\"));\n        return tryReadTopoConf(topoId, topoCache);\n    }"
            },
            "storm-server.src.main.java.org.apache.storm.daemon.nimbus.Nimbus.submitTopologyWithOpts": {
                "code_before_change": "    public void submitTopologyWithOpts(String topoName, String uploadedJarLocation, String jsonConf,\n                                       StormTopology topology, SubmitOptions options)\n        throws AlreadyAliveException, InvalidTopologyException, AuthorizationException, TException {\n        try {\n            submitTopologyWithOptsCalls.mark();\n            assertIsLeader();\n            assert (options != null);\n            validateTopologyName(topoName);\n            checkAuthorization(topoName, null, \"submitTopology\");\n            assertTopoActive(topoName, false);\n            @SuppressWarnings(\"unchecked\")\n            Map<String, Object> topoConf = (Map<String, Object>) JSONValue.parse(jsonConf);\n            try {\n                ConfigValidation.validateFields(topoConf);\n            } catch (IllegalArgumentException ex) {\n                throw new InvalidTopologyException(ex.getMessage());\n            }\n            validator.validate(topoName, topoConf, topology);\n            if ((boolean) conf.getOrDefault(Config.DISABLE_SYMLINKS, false)) {\n                @SuppressWarnings(\"unchecked\")\n                Map<String, Object> blobMap = (Map<String, Object>) topoConf.get(Config.TOPOLOGY_BLOBSTORE_MAP);\n                if (blobMap != null && !blobMap.isEmpty()) {\n                    throw new InvalidTopologyException(\"symlinks are disabled so blobs are not supported but \" +\n                                                       Config.TOPOLOGY_BLOBSTORE_MAP + \" = \" + blobMap);\n                }\n            }\n            validateTopologyWorkerMaxHeapSizeConfigs(topoConf, topology,\n                                                     ObjectReader.getDouble(conf.get(Config.TOPOLOGY_WORKER_MAX_HEAP_SIZE_MB)));\n            Utils.validateTopologyBlobStoreMap(topoConf, blobStore);\n            long uniqueNum = submittedCount.incrementAndGet();\n            String topoId = topoName + \"-\" + uniqueNum + \"-\" + Time.currentTimeSecs();\n            Map<String, String> creds = null;\n            if (options.is_set_creds()) {\n                creds = options.get_creds().get_creds();\n            }\n            topoConf.put(Config.STORM_ID, topoId);\n            topoConf.put(Config.TOPOLOGY_NAME, topoName);\n            topoConf = normalizeConf(conf, topoConf, topology);\n\n            ReqContext req = ReqContext.context();\n            Principal principal = req.principal();\n            String submitterPrincipal = principal == null ? null : principal.toString();\n            String submitterUser = principalToLocal.toLocal(principal);\n            String systemUser = System.getProperty(\"user.name\");\n            @SuppressWarnings(\"unchecked\")\n            Set<String> topoAcl = new HashSet<>((List<String>) topoConf.getOrDefault(Config.TOPOLOGY_USERS, Collections.emptyList()));\n            topoAcl.add(submitterPrincipal);\n            topoAcl.add(submitterUser);\n\n            String topologyPrincipal = Utils.OR(submitterPrincipal, \"\");\n            topoConf.put(Config.TOPOLOGY_SUBMITTER_PRINCIPAL, topologyPrincipal);\n            String topologyOwner = Utils.OR(submitterUser, systemUser);\n            topoConf.put(Config.TOPOLOGY_SUBMITTER_USER, topologyOwner); //Don't let the user set who we launch as\n            topoConf.put(Config.TOPOLOGY_USERS, new ArrayList<>(topoAcl));\n            topoConf.put(Config.STORM_ZOOKEEPER_SUPERACL, conf.get(Config.STORM_ZOOKEEPER_SUPERACL));\n            if (!Utils.isZkAuthenticationConfiguredStormServer(conf)) {\n                topoConf.remove(Config.STORM_ZOOKEEPER_TOPOLOGY_AUTH_SCHEME);\n                topoConf.remove(Config.STORM_ZOOKEEPER_TOPOLOGY_AUTH_PAYLOAD);\n            }\n            if (!(Boolean) conf.getOrDefault(DaemonConfig.STORM_TOPOLOGY_CLASSPATH_BEGINNING_ENABLED, false)) {\n                topoConf.remove(Config.TOPOLOGY_CLASSPATH_BEGINNING);\n            }\n            String topoVersionString = topology.get_storm_version();\n            if (topoVersionString == null) {\n                topoVersionString = (String) conf.getOrDefault(Config.SUPERVISOR_WORKER_DEFAULT_VERSION, VersionInfo.getVersion());\n            }\n            //Check if we can run a topology with that version of storm.\n            SimpleVersion topoVersion = new SimpleVersion(topoVersionString);\n            List<String> cp = Utils.getCompatibleVersion(supervisorClasspaths, topoVersion, \"classpath\", null);\n            if (cp == null) {\n                throw new InvalidTopologyException(\"Topology submitted with storm version \" + topoVersionString\n                                                   + \" but could not find a configured compatible version to use \" +\n                                                   supervisorClasspaths.keySet());\n            }\n            Map<String, Object> otherConf = Utils.getConfigFromClasspath(cp, conf);\n            Map<String, Object> totalConfToSave = Utils.merge(otherConf, topoConf);\n            Map<String, Object> totalConf = Utils.merge(conf, totalConfToSave);\n            //When reading the conf in nimbus we want to fall back to our own settings\n            // if the other config does not have it set.\n            topology = normalizeTopology(totalConf, topology);\n\n            // if the Resource Aware Scheduler is used,\n            // we might need to set the number of acker executors and eventlogger executors to be the estimated number of workers.\n            if (ServerUtils.isRAS(conf)) {\n                int estimatedNumWorker = ServerUtils.getEstimatedWorkerCountForRASTopo(totalConf, topology);\n                int numAckerExecs = ObjectReader.getInt(totalConf.get(Config.TOPOLOGY_ACKER_EXECUTORS), estimatedNumWorker);\n                int numEventLoggerExecs = ObjectReader.getInt(totalConf.get(Config.TOPOLOGY_EVENTLOGGER_EXECUTORS), estimatedNumWorker);\n\n                totalConfToSave.put(Config.TOPOLOGY_ACKER_EXECUTORS, numAckerExecs);\n                totalConfToSave.put(Config.TOPOLOGY_EVENTLOGGER_EXECUTORS, numEventLoggerExecs);\n\n                LOG.debug(\"{} set to: {}\", Config.TOPOLOGY_ACKER_EXECUTORS, numAckerExecs);\n                LOG.debug(\"{} set to: {}\", Config.TOPOLOGY_EVENTLOGGER_EXECUTORS, numEventLoggerExecs);\n            }\n\n            IStormClusterState state = stormClusterState;\n\n            if (creds == null && workerTokenManager != null) {\n                //Make sure we can store the worker tokens even if no creds are provided.\n                creds = new HashMap<>();\n            }\n            if (creds != null) {\n                Map<String, Object> finalConf = Collections.unmodifiableMap(topoConf);\n                for (INimbusCredentialPlugin autocred : nimbusAutocredPlugins) {\n                    autocred.populateCredentials(creds, finalConf);\n                }\n                upsertWorkerTokensInCreds(creds, topologyPrincipal, topoId);\n            }\n\n            if (ObjectReader.getBoolean(conf.get(Config.SUPERVISOR_RUN_WORKER_AS_USER), false) &&\n                (submitterUser == null || submitterUser.isEmpty())) {\n                throw new AuthorizationException(\"Could not determine the user to run this topology as.\");\n            }\n            StormCommon.systemTopology(totalConf, topology); //this validates the structure of the topology\n            validateTopologySize(topoConf, conf, topology);\n            if (Utils.isZkAuthenticationConfiguredStormServer(conf) &&\n                !Utils.isZkAuthenticationConfiguredTopology(topoConf)) {\n                throw new IllegalArgumentException(\"The cluster is configured for zookeeper authentication, but no payload was provided.\");\n            }\n            LOG.info(\"Received topology submission for {} (storm-{} JDK-{}) with conf {}\", topoName,\n                     topoVersionString, topology.get_jdk_version(),\n                     Utils.redactValue(topoConf, Config.STORM_ZOOKEEPER_TOPOLOGY_AUTH_PAYLOAD));\n\n            // lock protects against multiple topologies being submitted at once and\n            // cleanup thread killing topology in b/w assignment and starting the topology\n            synchronized (submitLock) {\n                assertTopoActive(topoName, false);\n                //cred-update-lock is not needed here because creds are being added for the first time.\n                if (creds != null) {\n                    state.setCredentials(topoId, new Credentials(creds), topoConf);\n                }\n                LOG.info(\"uploadedJar {}\", uploadedJarLocation);\n                setupStormCode(conf, topoId, uploadedJarLocation, totalConfToSave, topology);\n                waitForDesiredCodeReplication(totalConf, topoId);\n                state.setupHeatbeats(topoId, topoConf);\n                state.setupErrors(topoId, topoConf);\n                if (ObjectReader.getBoolean(totalConf.get(Config.TOPOLOGY_BACKPRESSURE_ENABLE), false)) {\n                    state.setupBackpressure(topoId, topoConf);\n                }\n                notifyTopologyActionListener(topoName, \"submitTopology\");\n                TopologyStatus status = null;\n                switch (options.get_initial_status()) {\n                    case INACTIVE:\n                        status = TopologyStatus.INACTIVE;\n                        break;\n                    case ACTIVE:\n                        status = TopologyStatus.ACTIVE;\n                        break;\n                    default:\n                        throw new IllegalArgumentException(\"Inital Status of \" + options.get_initial_status() + \" is not allowed.\");\n\n                }\n                startTopology(topoName, topoId, status, topologyOwner, topologyPrincipal);\n            }\n        } catch (Exception e) {\n            LOG.warn(\"Topology submission exception. (topology name='{}')\", topoName, e);\n            if (e instanceof TException) {\n                throw (TException) e;\n            }\n            throw new RuntimeException(e);\n        }\n    }",
                "code_after_change": "    public void submitTopologyWithOpts(String topoName, String uploadedJarLocation, String jsonConf,\n                                       StormTopology topology, SubmitOptions options)\n        throws AlreadyAliveException, InvalidTopologyException, AuthorizationException, TException {\n        try {\n            submitTopologyWithOptsCalls.mark();\n            assertIsLeader();\n            assert (options != null);\n            validateTopologyName(topoName);\n            checkAuthorization(topoName, null, \"submitTopology\");\n            assertTopoActive(topoName, false);\n            @SuppressWarnings(\"unchecked\")\n            Map<String, Object> topoConf = (Map<String, Object>) JSONValue.parse(jsonConf);\n            try {\n                ConfigValidation.validateFields(topoConf);\n            } catch (IllegalArgumentException ex) {\n                throw new WrappedInvalidTopologyException(ex.getMessage());\n            }\n            validator.validate(topoName, topoConf, topology);\n            if ((boolean) conf.getOrDefault(Config.DISABLE_SYMLINKS, false)) {\n                @SuppressWarnings(\"unchecked\")\n                Map<String, Object> blobMap = (Map<String, Object>) topoConf.get(Config.TOPOLOGY_BLOBSTORE_MAP);\n                if (blobMap != null && !blobMap.isEmpty()) {\n                    throw new WrappedInvalidTopologyException(\"symlinks are disabled so blobs are not supported but \"\n                                                       + Config.TOPOLOGY_BLOBSTORE_MAP + \" = \" + blobMap);\n                }\n            }\n            validateTopologyWorkerMaxHeapSizeConfigs(topoConf, topology,\n                                                     ObjectReader.getDouble(conf.get(Config.TOPOLOGY_WORKER_MAX_HEAP_SIZE_MB)));\n            Utils.validateTopologyBlobStoreMap(topoConf, blobStore);\n            long uniqueNum = submittedCount.incrementAndGet();\n            String topoId = topoName + \"-\" + uniqueNum + \"-\" + Time.currentTimeSecs();\n            Map<String, String> creds = null;\n            if (options.is_set_creds()) {\n                creds = options.get_creds().get_creds();\n            }\n            topoConf.put(Config.STORM_ID, topoId);\n            topoConf.put(Config.TOPOLOGY_NAME, topoName);\n            topoConf = normalizeConf(conf, topoConf, topology);\n\n            ReqContext req = ReqContext.context();\n            Principal principal = req.principal();\n            String submitterPrincipal = principal == null ? null : principal.toString();\n            String submitterUser = principalToLocal.toLocal(principal);\n            String systemUser = System.getProperty(\"user.name\");\n            @SuppressWarnings(\"unchecked\")\n            Set<String> topoAcl = new HashSet<>((List<String>) topoConf.getOrDefault(Config.TOPOLOGY_USERS, Collections.emptyList()));\n            topoAcl.add(submitterPrincipal);\n            topoAcl.add(submitterUser);\n\n            String topologyPrincipal = Utils.OR(submitterPrincipal, \"\");\n            topoConf.put(Config.TOPOLOGY_SUBMITTER_PRINCIPAL, topologyPrincipal);\n            String topologyOwner = Utils.OR(submitterUser, systemUser);\n            topoConf.put(Config.TOPOLOGY_SUBMITTER_USER, topologyOwner); //Don't let the user set who we launch as\n            topoConf.put(Config.TOPOLOGY_USERS, new ArrayList<>(topoAcl));\n            topoConf.put(Config.STORM_ZOOKEEPER_SUPERACL, conf.get(Config.STORM_ZOOKEEPER_SUPERACL));\n            if (!Utils.isZkAuthenticationConfiguredStormServer(conf)) {\n                topoConf.remove(Config.STORM_ZOOKEEPER_TOPOLOGY_AUTH_SCHEME);\n                topoConf.remove(Config.STORM_ZOOKEEPER_TOPOLOGY_AUTH_PAYLOAD);\n            }\n            if (!(Boolean) conf.getOrDefault(DaemonConfig.STORM_TOPOLOGY_CLASSPATH_BEGINNING_ENABLED, false)) {\n                topoConf.remove(Config.TOPOLOGY_CLASSPATH_BEGINNING);\n            }\n            String topoVersionString = topology.get_storm_version();\n            if (topoVersionString == null) {\n                topoVersionString = (String) conf.getOrDefault(Config.SUPERVISOR_WORKER_DEFAULT_VERSION, VersionInfo.getVersion());\n            }\n            //Check if we can run a topology with that version of storm.\n            SimpleVersion topoVersion = new SimpleVersion(topoVersionString);\n            List<String> cp = Utils.getCompatibleVersion(supervisorClasspaths, topoVersion, \"classpath\", null);\n            if (cp == null) {\n                throw new WrappedInvalidTopologyException(\"Topology submitted with storm version \" + topoVersionString\n                                                   + \" but could not find a configured compatible version to use \"\n                                                   + supervisorClasspaths.keySet());\n            }\n            Map<String, Object> otherConf = Utils.getConfigFromClasspath(cp, conf);\n            Map<String, Object> totalConfToSave = Utils.merge(otherConf, topoConf);\n            Map<String, Object> totalConf = Utils.merge(conf, totalConfToSave);\n            //When reading the conf in nimbus we want to fall back to our own settings\n            // if the other config does not have it set.\n            topology = normalizeTopology(totalConf, topology);\n\n            // if the Resource Aware Scheduler is used,\n            // we might need to set the number of acker executors and eventlogger executors to be the estimated number of workers.\n            if (ServerUtils.isRAS(conf)) {\n                int estimatedNumWorker = ServerUtils.getEstimatedWorkerCountForRASTopo(totalConf, topology);\n                int numAckerExecs = ObjectReader.getInt(totalConf.get(Config.TOPOLOGY_ACKER_EXECUTORS), estimatedNumWorker);\n                int numEventLoggerExecs = ObjectReader.getInt(totalConf.get(Config.TOPOLOGY_EVENTLOGGER_EXECUTORS), estimatedNumWorker);\n\n                totalConfToSave.put(Config.TOPOLOGY_ACKER_EXECUTORS, numAckerExecs);\n                totalConfToSave.put(Config.TOPOLOGY_EVENTLOGGER_EXECUTORS, numEventLoggerExecs);\n\n                LOG.debug(\"{} set to: {}\", Config.TOPOLOGY_ACKER_EXECUTORS, numAckerExecs);\n                LOG.debug(\"{} set to: {}\", Config.TOPOLOGY_EVENTLOGGER_EXECUTORS, numEventLoggerExecs);\n            }\n\n            IStormClusterState state = stormClusterState;\n\n            if (creds == null && workerTokenManager != null) {\n                //Make sure we can store the worker tokens even if no creds are provided.\n                creds = new HashMap<>();\n            }\n            if (creds != null) {\n                Map<String, Object> finalConf = Collections.unmodifiableMap(topoConf);\n                for (INimbusCredentialPlugin autocred : nimbusAutocredPlugins) {\n                    autocred.populateCredentials(creds, finalConf);\n                }\n                upsertWorkerTokensInCreds(creds, topologyPrincipal, topoId);\n            }\n\n            if (ObjectReader.getBoolean(conf.get(Config.SUPERVISOR_RUN_WORKER_AS_USER), false)\n                && (submitterUser == null || submitterUser.isEmpty())) {\n                throw new WrappedAuthorizationException(\"Could not determine the user to run this topology as.\");\n            }\n            StormCommon.systemTopology(totalConf, topology); //this validates the structure of the topology\n            validateTopologySize(topoConf, conf, topology);\n            if (Utils.isZkAuthenticationConfiguredStormServer(conf)\n                && !Utils.isZkAuthenticationConfiguredTopology(topoConf)) {\n                throw new IllegalArgumentException(\"The cluster is configured for zookeeper authentication, but no payload was provided.\");\n            }\n            LOG.info(\"Received topology submission for {} (storm-{} JDK-{}) with conf {}\", topoName,\n                     topoVersionString, topology.get_jdk_version(),\n                     Utils.redactValue(topoConf, Config.STORM_ZOOKEEPER_TOPOLOGY_AUTH_PAYLOAD));\n\n            // lock protects against multiple topologies being submitted at once and\n            // cleanup thread killing topology in b/w assignment and starting the topology\n            synchronized (submitLock) {\n                assertTopoActive(topoName, false);\n                //cred-update-lock is not needed here because creds are being added for the first time.\n                if (creds != null) {\n                    state.setCredentials(topoId, new Credentials(creds), topoConf);\n                }\n                LOG.info(\"uploadedJar {}\", uploadedJarLocation);\n                setupStormCode(conf, topoId, uploadedJarLocation, totalConfToSave, topology);\n                waitForDesiredCodeReplication(totalConf, topoId);\n                state.setupHeatbeats(topoId, topoConf);\n                state.setupErrors(topoId, topoConf);\n                if (ObjectReader.getBoolean(totalConf.get(Config.TOPOLOGY_BACKPRESSURE_ENABLE), false)) {\n                    state.setupBackpressure(topoId, topoConf);\n                }\n                notifyTopologyActionListener(topoName, \"submitTopology\");\n                TopologyStatus status = null;\n                switch (options.get_initial_status()) {\n                    case INACTIVE:\n                        status = TopologyStatus.INACTIVE;\n                        break;\n                    case ACTIVE:\n                        status = TopologyStatus.ACTIVE;\n                        break;\n                    default:\n                        throw new IllegalArgumentException(\"Inital Status of \" + options.get_initial_status() + \" is not allowed.\");\n\n                }\n                startTopology(topoName, topoId, status, topologyOwner, topologyPrincipal);\n            }\n        } catch (Exception e) {\n            LOG.warn(\"Topology submission exception. (topology name='{}')\", topoName, e);\n            if (e instanceof TException) {\n                throw (TException) e;\n            }\n            throw new RuntimeException(e);\n        }\n    }"
            },
            "storm-client.src.jvm.org.apache.storm.utils.Utils.validateTopologyBlobStoreMap": {
                "code_before_change": "    public static void validateTopologyBlobStoreMap(Map<String, Object> topoConf) throws InvalidTopologyException, AuthorizationException {\n        try (NimbusBlobStore client = new NimbusBlobStore()) {\n            client.prepare(topoConf);\n            validateTopologyBlobStoreMap(topoConf, client);\n        }\n    }",
                "code_after_change": "    public static void validateTopologyBlobStoreMap(Map<String, Object> topoConf) throws InvalidTopologyException, AuthorizationException {\n        try (NimbusBlobStore client = new NimbusBlobStore()) {\n            client.prepare(topoConf);\n            validateTopologyBlobStoreMap(topoConf, client);\n        }\n    }"
            },
            "external.storm-hdfs-blobstore.src.main.java.org.apache.storm.hdfs.blobstore.HdfsBlobStore.createBlob": {
                "code_before_change": "    public AtomicOutputStream createBlob(String key, SettableBlobMeta meta, Subject who)\n            throws AuthorizationException, KeyAlreadyExistsException {\n        if (meta.get_replication_factor() <= 0) {\n            meta.set_replication_factor((int)conf.get(Config.STORM_BLOBSTORE_REPLICATION_FACTOR));\n        }\n        who = checkAndGetSubject(who);\n        validateKey(key);\n        aclHandler.normalizeSettableBlobMeta(key, meta, who, READ | WRITE | ADMIN);\n        BlobStoreAclHandler.validateSettableACLs(key, meta.get_acl());\n        aclHandler.hasPermissions(meta.get_acl(), READ | WRITE | ADMIN, who, key);\n        if (hbs.exists(DATA_PREFIX + key)) {\n            throw new KeyAlreadyExistsException(key);\n        }\n        BlobStoreFileOutputStream mOut = null;\n        try {\n            BlobStoreFile metaFile = hbs.write(META_PREFIX + key, true);\n            metaFile.setMetadata(meta);\n            mOut = new BlobStoreFileOutputStream(metaFile);\n            mOut.write(Utils.thriftSerialize(meta));\n            mOut.close();\n            mOut = null;\n            BlobStoreFile dataFile = hbs.write(DATA_PREFIX + key, true);\n            dataFile.setMetadata(meta);\n            return new BlobStoreFileOutputStream(dataFile);\n        } catch (IOException e) {\n            throw new RuntimeException(e);\n        } finally {\n            if (mOut != null) {\n                try {\n                    mOut.cancel();\n                } catch (IOException e) {\n                    //Ignored\n                }\n            }\n        }\n    }",
                "code_after_change": "    public AtomicOutputStream createBlob(String key, SettableBlobMeta meta, Subject who)\n            throws AuthorizationException, KeyAlreadyExistsException {\n        if (meta.get_replication_factor() <= 0) {\n            meta.set_replication_factor((int)conf.get(Config.STORM_BLOBSTORE_REPLICATION_FACTOR));\n        }\n        who = checkAndGetSubject(who);\n        validateKey(key);\n        aclHandler.normalizeSettableBlobMeta(key, meta, who, READ | WRITE | ADMIN);\n        BlobStoreAclHandler.validateSettableACLs(key, meta.get_acl());\n        aclHandler.hasPermissions(meta.get_acl(), READ | WRITE | ADMIN, who, key);\n        if (hbs.exists(DATA_PREFIX + key)) {\n            throw new WrappedKeyAlreadyExistsException(key);\n        }\n        BlobStoreFileOutputStream mOut = null;\n        try {\n            BlobStoreFile metaFile = hbs.write(META_PREFIX + key, true);\n            metaFile.setMetadata(meta);\n            mOut = new BlobStoreFileOutputStream(metaFile);\n            mOut.write(Utils.thriftSerialize(meta));\n            mOut.close();\n            mOut = null;\n            BlobStoreFile dataFile = hbs.write(DATA_PREFIX + key, true);\n            dataFile.setMetadata(meta);\n            return new BlobStoreFileOutputStream(dataFile);\n        } catch (IOException e) {\n            throw new RuntimeException(e);\n        } finally {\n            if (mOut != null) {\n                try {\n                    mOut.cancel();\n                } catch (IOException e) {\n                    //Ignored\n                }\n            }\n        }\n    }"
            },
            "storm-server.src.main.java.org.apache.storm.daemon.nimbus.Nimbus.getTopologyInfoWithOpts": {
                "code_before_change": "    public TopologyInfo getTopologyInfoWithOpts(String topoId, GetInfoOptions options)\n        throws NotAliveException, AuthorizationException, TException {\n        try {\n            getTopologyInfoWithOptsCalls.mark();\n            CommonTopoInfo common = getCommonTopoInfo(topoId, \"getTopologyInfo\");\n            if (common.base == null) {\n                throw new NotAliveException(topoId);\n            }\n            IStormClusterState state = stormClusterState;\n            NumErrorsChoice numErrChoice = Utils.OR(options.get_num_err_choice(), NumErrorsChoice.ALL);\n            Map<String, List<ErrorInfo>> errors = new HashMap<>();\n            for (String component : common.allComponents) {\n                switch (numErrChoice) {\n                    case NONE:\n                        errors.put(component, Collections.emptyList());\n                        break;\n                    case ONE:\n                        List<ErrorInfo> errList = new ArrayList<>();\n                        ErrorInfo info = state.lastError(topoId, component);\n                        if (info != null) {\n                            errList.add(info);\n                        }\n                        errors.put(component, errList);\n                        break;\n                    case ALL:\n                        errors.put(component, state.errors(topoId, component));\n                        break;\n                    default:\n                        LOG.warn(\"Got invalid NumErrorsChoice '{}'\", numErrChoice);\n                        errors.put(component, state.errors(topoId, component));\n                        break;\n                }\n            }\n\n            List<ExecutorSummary> summaries = new ArrayList<>();\n            if (common.assignment != null) {\n                for (Entry<List<Long>, NodeInfo> entry : common.assignment.get_executor_node_port().entrySet()) {\n                    NodeInfo ni = entry.getValue();\n                    ExecutorInfo execInfo = toExecInfo(entry.getKey());\n                    Map<String, String> nodeToHost = common.assignment.get_node_host();\n                    Map<String, Object> heartbeat = common.beats.get(StatsUtil.convertExecutor(entry.getKey()));\n                    if (heartbeat == null) {\n                        heartbeat = Collections.emptyMap();\n                    }\n                    ExecutorSummary summ = new ExecutorSummary(execInfo,\n                                                               common.taskToComponent.get(execInfo.get_task_start()),\n                                                               nodeToHost.get(ni.get_node()), ni.get_port_iterator().next().intValue(),\n                                                               (Integer) heartbeat.getOrDefault(\"uptime\", 0));\n\n                    //heartbeats \"stats\"\n                    Map ex = (Map) heartbeat.get(\"stats\");\n                    if (ex != null) {\n                        ExecutorStats stats = StatsUtil.thriftifyExecutorStats(ex);\n                        summ.set_stats(stats);\n                    }\n                    summaries.add(summ);\n                }\n            }\n            TopologyInfo topoInfo = new TopologyInfo(topoId, common.topoName, Time.deltaSecs(common.launchTimeSecs),\n                                                     summaries, extractStatusStr(common.base), errors);\n            if (common.topology.is_set_storm_version()) {\n                topoInfo.set_storm_version(common.topology.get_storm_version());\n            }\n            if (common.base.is_set_owner()) {\n                topoInfo.set_owner(common.base.get_owner());\n            }\n            String schedStatus = idToSchedStatus.get().get(topoId);\n            if (schedStatus != null) {\n                topoInfo.set_sched_status(schedStatus);\n            }\n            TopologyResources resources = getResourcesForTopology(topoId, common.base);\n            if (resources != null) {\n                topoInfo.set_requested_memonheap(resources.getRequestedMemOnHeap());\n                topoInfo.set_requested_memoffheap(resources.getRequestedMemOffHeap());\n                topoInfo.set_requested_cpu(resources.getRequestedCpu());\n                topoInfo.set_assigned_memonheap(resources.getAssignedMemOnHeap());\n                topoInfo.set_assigned_memoffheap(resources.getAssignedMemOffHeap());\n                topoInfo.set_assigned_cpu(resources.getAssignedCpu());\n\n            }\n            if (common.base.is_set_component_debug()) {\n                topoInfo.set_component_debug(common.base.get_component_debug());\n            }\n            topoInfo.set_replication_count(getBlobReplicationCount(ConfigUtils.masterStormCodeKey(topoId)));\n            return topoInfo;\n        } catch (Exception e) {\n            LOG.warn(\"Get topo info exception. (topology id='{}')\", topoId, e);\n            if (e instanceof TException) {\n                throw (TException) e;\n            }\n            throw new RuntimeException(e);\n        }\n    }",
                "code_after_change": "    public TopologyInfo getTopologyInfoWithOpts(String topoId, GetInfoOptions options)\n        throws NotAliveException, AuthorizationException, TException {\n        try {\n            getTopologyInfoWithOptsCalls.mark();\n            CommonTopoInfo common = getCommonTopoInfo(topoId, \"getTopologyInfo\");\n            if (common.base == null) {\n                throw new WrappedNotAliveException(topoId);\n            }\n            IStormClusterState state = stormClusterState;\n            NumErrorsChoice numErrChoice = Utils.OR(options.get_num_err_choice(), NumErrorsChoice.ALL);\n            Map<String, List<ErrorInfo>> errors = new HashMap<>();\n            for (String component : common.allComponents) {\n                switch (numErrChoice) {\n                    case NONE:\n                        errors.put(component, Collections.emptyList());\n                        break;\n                    case ONE:\n                        List<ErrorInfo> errList = new ArrayList<>();\n                        ErrorInfo info = state.lastError(topoId, component);\n                        if (info != null) {\n                            errList.add(info);\n                        }\n                        errors.put(component, errList);\n                        break;\n                    case ALL:\n                        errors.put(component, state.errors(topoId, component));\n                        break;\n                    default:\n                        LOG.warn(\"Got invalid NumErrorsChoice '{}'\", numErrChoice);\n                        errors.put(component, state.errors(topoId, component));\n                        break;\n                }\n            }\n\n            List<ExecutorSummary> summaries = new ArrayList<>();\n            if (common.assignment != null) {\n                for (Entry<List<Long>, NodeInfo> entry : common.assignment.get_executor_node_port().entrySet()) {\n                    NodeInfo ni = entry.getValue();\n                    ExecutorInfo execInfo = toExecInfo(entry.getKey());\n                    Map<String, String> nodeToHost = common.assignment.get_node_host();\n                    Map<String, Object> heartbeat = common.beats.get(StatsUtil.convertExecutor(entry.getKey()));\n                    if (heartbeat == null) {\n                        heartbeat = Collections.emptyMap();\n                    }\n                    ExecutorSummary summ = new ExecutorSummary(execInfo,\n                                                               common.taskToComponent.get(execInfo.get_task_start()),\n                                                               nodeToHost.get(ni.get_node()), ni.get_port_iterator().next().intValue(),\n                                                               (Integer) heartbeat.getOrDefault(\"uptime\", 0));\n\n                    //heartbeats \"stats\"\n                    Map ex = (Map) heartbeat.get(\"stats\");\n                    if (ex != null) {\n                        ExecutorStats stats = StatsUtil.thriftifyExecutorStats(ex);\n                        summ.set_stats(stats);\n                    }\n                    summaries.add(summ);\n                }\n            }\n            TopologyInfo topoInfo = new TopologyInfo(topoId, common.topoName, Time.deltaSecs(common.launchTimeSecs),\n                                                     summaries, extractStatusStr(common.base), errors);\n            if (common.topology.is_set_storm_version()) {\n                topoInfo.set_storm_version(common.topology.get_storm_version());\n            }\n            if (common.base.is_set_owner()) {\n                topoInfo.set_owner(common.base.get_owner());\n            }\n            String schedStatus = idToSchedStatus.get().get(topoId);\n            if (schedStatus != null) {\n                topoInfo.set_sched_status(schedStatus);\n            }\n            TopologyResources resources = getResourcesForTopology(topoId, common.base);\n            if (resources != null) {\n                topoInfo.set_requested_memonheap(resources.getRequestedMemOnHeap());\n                topoInfo.set_requested_memoffheap(resources.getRequestedMemOffHeap());\n                topoInfo.set_requested_cpu(resources.getRequestedCpu());\n                topoInfo.set_assigned_memonheap(resources.getAssignedMemOnHeap());\n                topoInfo.set_assigned_memoffheap(resources.getAssignedMemOffHeap());\n                topoInfo.set_assigned_cpu(resources.getAssignedCpu());\n\n            }\n            if (common.base.is_set_component_debug()) {\n                topoInfo.set_component_debug(common.base.get_component_debug());\n            }\n            topoInfo.set_replication_count(getBlobReplicationCount(ConfigUtils.masterStormCodeKey(topoId)));\n            return topoInfo;\n        } catch (Exception e) {\n            LOG.warn(\"Get topo info exception. (topology id='{}')\", topoId, e);\n            if (e instanceof TException) {\n                throw (TException) e;\n            }\n            throw new RuntimeException(e);\n        }\n    }"
            },
            "storm-client.src.jvm.org.apache.storm.utils.Utils.asyncLoop": {
                "code_before_change": "    public static SmartThread asyncLoop(final Callable afn, boolean isDaemon, final Thread.UncaughtExceptionHandler eh,\n                                        int priority, final boolean isFactory, boolean startImmediately,\n                                        String threadName) {\n        SmartThread thread = new SmartThread(new Runnable() {\n            public void run() {\n                try {\n                    final Callable<Long> fn = isFactory ? (Callable<Long>) afn.call() : afn;\n                    while (true) {\n                        final Long s = fn.call();\n                        if (s == null) // then stop running it\n                        {\n                            break;\n                        }\n                        if (s > 0) {\n                            Time.sleep(s);\n                        }\n                    }\n                } catch (Throwable t) {\n                    if (Utils.exceptionCauseIsInstanceOf(\n                        InterruptedException.class, t)) {\n                        LOG.info(\"Async loop interrupted!\");\n                        return;\n                    }\n                    LOG.error(\"Async loop died!\", t);\n                    throw new RuntimeException(t);\n                }\n            }\n        });\n        if (eh != null) {\n            thread.setUncaughtExceptionHandler(eh);\n        } else {\n            thread.setUncaughtExceptionHandler(new Thread.UncaughtExceptionHandler() {\n                public void uncaughtException(Thread t, Throwable e) {\n                    LOG.error(\"Async loop died!\", e);\n                    Utils.exitProcess(1, \"Async loop died!\");\n                }\n            });\n        }\n        thread.setDaemon(isDaemon);\n        thread.setPriority(priority);\n        if (threadName != null && !threadName.isEmpty()) {\n            thread.setName(thread.getName() + \"-\" + threadName);\n        }\n        if (startImmediately) {\n            thread.start();\n        }\n        return thread;\n    }",
                "code_after_change": "    public static SmartThread asyncLoop(final Callable afn, boolean isDaemon, final Thread.UncaughtExceptionHandler eh,\n                                        int priority, final boolean isFactory, boolean startImmediately,\n                                        String threadName) {\n        SmartThread thread = new SmartThread(new Runnable() {\n            public void run() {\n                try {\n                    final Callable<Long> fn = isFactory ? (Callable<Long>) afn.call() : afn;\n                    while (true) {\n                        final Long s = fn.call();\n                        if (s == null) { // then stop running it\n                            break;\n                        }\n                        if (s > 0) {\n                            Time.sleep(s);\n                        }\n                    }\n                } catch (Throwable t) {\n                    if (Utils.exceptionCauseIsInstanceOf(\n                        InterruptedException.class, t)) {\n                        LOG.info(\"Async loop interrupted!\");\n                        return;\n                    }\n                    LOG.error(\"Async loop died!\", t);\n                    throw new RuntimeException(t);\n                }\n            }\n        });\n        if (eh != null) {\n            thread.setUncaughtExceptionHandler(eh);\n        } else {\n            thread.setUncaughtExceptionHandler(new Thread.UncaughtExceptionHandler() {\n                public void uncaughtException(Thread t, Throwable e) {\n                    LOG.error(\"Async loop died!\", e);\n                    Utils.exitProcess(1, \"Async loop died!\");\n                }\n            });\n        }\n        thread.setDaemon(isDaemon);\n        thread.setPriority(priority);\n        if (threadName != null && !threadName.isEmpty()) {\n            thread.setName(thread.getName() + \"-\" + threadName);\n        }\n        if (startImmediately) {\n            thread.start();\n        }\n        return thread;\n    }"
            },
            "storm-server.src.main.java.org.apache.storm.daemon.nimbus.Nimbus.tryReadTopoConf": {
                "code_before_change": "    private static Map<String, Object> tryReadTopoConf(String topoId, TopoCache tc)\n        throws NotAliveException, AuthorizationException, IOException {\n        try {\n            return readTopoConfAsNimbus(topoId, tc);\n            //Was a try-cause but I looked at the code around this and key not found is not wrapped in runtime,\n            // so it is not needed\n        } catch (KeyNotFoundException e) {\n            if (topoId == null) {\n                throw new NullPointerException();\n            }\n            throw new NotAliveException(topoId);\n        }\n    }",
                "code_after_change": "    private static Map<String, Object> tryReadTopoConf(String topoId, TopoCache tc)\n        throws NotAliveException, AuthorizationException, IOException {\n        try {\n            return readTopoConfAsNimbus(topoId, tc);\n            //Was a try-cause but I looked at the code around this and key not found is not wrapped in runtime,\n            // so it is not needed\n        } catch (KeyNotFoundException e) {\n            if (topoId == null) {\n                throw new NullPointerException();\n            }\n            throw new WrappedNotAliveException(topoId);\n        }\n    }"
            },
            "storm-server.src.main.java.org.apache.storm.LocalCluster.finishFileUpload": {
                "code_before_change": "    public void finishFileUpload(String location) throws AuthorizationException, TException {\n        //Just throw it away in local mode\n    }",
                "code_after_change": "    public void finishFileUpload(String location) throws AuthorizationException, TException {\n        //Just throw it away in local mode\n    }"
            },
            "storm-server.src.main.java.org.apache.storm.daemon.nimbus.Nimbus.startTopology": {
                "code_before_change": "    private void startTopology(String topoName, String topoId, TopologyStatus initStatus, String owner, String principal)\n        throws KeyNotFoundException, AuthorizationException, IOException, InvalidTopologyException {\n        assert (TopologyStatus.ACTIVE == initStatus || TopologyStatus.INACTIVE == initStatus);\n        IStormClusterState state = stormClusterState;\n        Map<String, Object> topoConf = readTopoConf(topoId, topoCache);\n        StormTopology topology = StormCommon.systemTopology(topoConf, readStormTopology(topoId, topoCache));\n        Map<String, Integer> numExecutors = new HashMap<>();\n        for (Entry<String, Object> entry : StormCommon.allComponents(topology).entrySet()) {\n            numExecutors.put(entry.getKey(), StormCommon.numStartExecutors(entry.getValue()));\n        }\n        LOG.info(\"Activating {}: {}\", topoName, topoId);\n        StormBase base = new StormBase();\n        base.set_name(topoName);\n        if (topoConf.containsKey(Config.TOPOLOGY_VERSION)) {\n            base.set_topology_version(ObjectReader.getString(topoConf.get(Config.TOPOLOGY_VERSION)));\n        }\n        base.set_launch_time_secs(Time.currentTimeSecs());\n        base.set_status(initStatus);\n        base.set_num_workers(ObjectReader.getInt(topoConf.get(Config.TOPOLOGY_WORKERS), 0));\n        base.set_component_executors(numExecutors);\n        base.set_owner(owner);\n        base.set_principal(principal);\n        base.set_component_debug(new HashMap<>());\n        state.activateStorm(topoId, base, topoConf);\n        notifyTopologyActionListener(topoName, \"activate\");\n    }",
                "code_after_change": "    private void startTopology(String topoName, String topoId, TopologyStatus initStatus, String owner, String principal)\n        throws KeyNotFoundException, AuthorizationException, IOException, InvalidTopologyException {\n        assert (TopologyStatus.ACTIVE == initStatus || TopologyStatus.INACTIVE == initStatus);\n        IStormClusterState state = stormClusterState;\n        Map<String, Object> topoConf = readTopoConf(topoId, topoCache);\n        StormTopology topology = StormCommon.systemTopology(topoConf, readStormTopology(topoId, topoCache));\n        Map<String, Integer> numExecutors = new HashMap<>();\n        for (Entry<String, Object> entry : StormCommon.allComponents(topology).entrySet()) {\n            numExecutors.put(entry.getKey(), StormCommon.numStartExecutors(entry.getValue()));\n        }\n        LOG.info(\"Activating {}: {}\", topoName, topoId);\n        StormBase base = new StormBase();\n        base.set_name(topoName);\n        if (topoConf.containsKey(Config.TOPOLOGY_VERSION)) {\n            base.set_topology_version(ObjectReader.getString(topoConf.get(Config.TOPOLOGY_VERSION)));\n        }\n        base.set_launch_time_secs(Time.currentTimeSecs());\n        base.set_status(initStatus);\n        base.set_num_workers(ObjectReader.getInt(topoConf.get(Config.TOPOLOGY_WORKERS), 0));\n        base.set_component_executors(numExecutors);\n        base.set_owner(owner);\n        base.set_principal(principal);\n        base.set_component_debug(new HashMap<>());\n        state.activateStorm(topoId, base, topoConf);\n        notifyTopologyActionListener(topoName, \"activate\");\n    }"
            },
            "storm-server.src.main.java.org.apache.storm.daemon.drpc.DRPC.DRPC": {
                "code_before_change": [],
                "code_after_change": []
            },
            "storm-server.src.main.java.org.apache.storm.daemon.nimbus.Nimbus.getComponentPageInfo": {
                "code_before_change": "    public ComponentPageInfo getComponentPageInfo(String topoId, String componentId, String window, boolean includeSys)\n        throws NotAliveException, AuthorizationException, TException {\n        try {\n            getComponentPageInfoCalls.mark();\n            CommonTopoInfo info = getCommonTopoInfo(topoId, \"getComponentPageInfo\");\n            if (info.base == null) {\n                throw new NotAliveException(topoId);\n            }\n            StormTopology topology = info.topology;\n            Map<String, Object> topoConf = info.topoConf;\n            topoConf = Utils.merge(conf, topoConf);\n            Assignment assignment = info.assignment;\n            Map<List<Long>, List<Object>> exec2NodePort = new HashMap<>();\n            Map<String, String> nodeToHost;\n            Map<List<Long>, List<Object>> exec2HostPort = new HashMap<>();\n            if (assignment != null) {\n                Map<List<Long>, NodeInfo> execToNodeInfo = assignment.get_executor_node_port();\n                nodeToHost = assignment.get_node_host();\n                for (Entry<List<Long>, NodeInfo> entry : execToNodeInfo.entrySet()) {\n                    NodeInfo ni = entry.getValue();\n                    List<Object> nodePort = Arrays.asList(ni.get_node(), ni.get_port_iterator().next());\n                    List<Object> hostPort = Arrays.asList(nodeToHost.get(ni.get_node()), ni.get_port_iterator().next());\n                    exec2NodePort.put(entry.getKey(), nodePort);\n                    exec2HostPort.put(entry.getKey(), hostPort);\n                }\n            } else {\n                nodeToHost = Collections.emptyMap();\n            }\n\n            ComponentPageInfo compPageInfo = StatsUtil.aggCompExecsStats(exec2HostPort, info.taskToComponent, info.beats, window,\n                                                                         includeSys, topoId, topology, componentId);\n            if (compPageInfo.get_component_type() == ComponentType.SPOUT) {\n                NormalizedResourceRequest spoutResources = ResourceUtils.getSpoutResources(topology, topoConf, componentId);\n                if (spoutResources == null) {\n                    spoutResources = new NormalizedResourceRequest(topoConf);\n                }\n                compPageInfo.set_resources_map(spoutResources.toNormalizedMap());\n            } else { //bolt\n                NormalizedResourceRequest boltResources = ResourceUtils.getBoltResources(topology, topoConf, componentId);\n                if (boltResources == null) {\n                    boltResources = new NormalizedResourceRequest(topoConf);\n                }\n                compPageInfo.set_resources_map(boltResources.toNormalizedMap());\n            }\n            compPageInfo.set_topology_name(info.topoName);\n            compPageInfo.set_errors(stormClusterState.errors(topoId, componentId));\n            compPageInfo.set_topology_status(extractStatusStr(info.base));\n            if (info.base.is_set_component_debug()) {\n                DebugOptions debug = info.base.get_component_debug().get(componentId);\n                if (debug != null) {\n                    compPageInfo.set_debug_options(debug);\n                }\n            }\n            // Add the event logger details.\n            Map<String, List<Integer>> compToTasks = Utils.reverseMap(info.taskToComponent);\n            if (compToTasks.containsKey(StormCommon.EVENTLOGGER_COMPONENT_ID)) {\n                List<Integer> tasks = compToTasks.get(StormCommon.EVENTLOGGER_COMPONENT_ID);\n                tasks.sort(null);\n                // Find the task the events from this component route to.\n                int taskIndex = TupleUtils.chooseTaskIndex(Collections.singletonList(componentId), tasks.size());\n                int taskId = tasks.get(taskIndex);\n                String host = null;\n                Integer port = null;\n                for (Entry<List<Long>, List<Object>> entry : exec2HostPort.entrySet()) {\n                    int start = entry.getKey().get(0).intValue();\n                    int end = entry.getKey().get(1).intValue();\n                    if (taskId >= start && taskId <= end) {\n                        host = (String) entry.getValue().get(0);\n                        port = ((Number) entry.getValue().get(1)).intValue();\n                        break;\n                    }\n                }\n\n                if (host != null && port != null) {\n                    compPageInfo.set_eventlog_host(host);\n                    compPageInfo.set_eventlog_port(port);\n                }\n            }\n            return compPageInfo;\n        } catch (Exception e) {\n            LOG.warn(\"getComponentPageInfo exception. (topo id='{}')\", topoId, e);\n            if (e instanceof TException) {\n                throw (TException) e;\n            }\n            throw new RuntimeException(e);\n        }\n    }",
                "code_after_change": "    public ComponentPageInfo getComponentPageInfo(String topoId, String componentId, String window, boolean includeSys)\n        throws NotAliveException, AuthorizationException, TException {\n        try {\n            getComponentPageInfoCalls.mark();\n            CommonTopoInfo info = getCommonTopoInfo(topoId, \"getComponentPageInfo\");\n            if (info.base == null) {\n                throw new WrappedNotAliveException(topoId);\n            }\n            StormTopology topology = info.topology;\n            Map<String, Object> topoConf = info.topoConf;\n            topoConf = Utils.merge(conf, topoConf);\n            Assignment assignment = info.assignment;\n            Map<List<Long>, List<Object>> exec2NodePort = new HashMap<>();\n            Map<String, String> nodeToHost;\n            Map<List<Long>, List<Object>> exec2HostPort = new HashMap<>();\n            if (assignment != null) {\n                Map<List<Long>, NodeInfo> execToNodeInfo = assignment.get_executor_node_port();\n                nodeToHost = assignment.get_node_host();\n                for (Entry<List<Long>, NodeInfo> entry : execToNodeInfo.entrySet()) {\n                    NodeInfo ni = entry.getValue();\n                    List<Object> nodePort = Arrays.asList(ni.get_node(), ni.get_port_iterator().next());\n                    List<Object> hostPort = Arrays.asList(nodeToHost.get(ni.get_node()), ni.get_port_iterator().next());\n                    exec2NodePort.put(entry.getKey(), nodePort);\n                    exec2HostPort.put(entry.getKey(), hostPort);\n                }\n            } else {\n                nodeToHost = Collections.emptyMap();\n            }\n\n            ComponentPageInfo compPageInfo = StatsUtil.aggCompExecsStats(exec2HostPort, info.taskToComponent, info.beats, window,\n                                                                         includeSys, topoId, topology, componentId);\n            if (compPageInfo.get_component_type() == ComponentType.SPOUT) {\n                NormalizedResourceRequest spoutResources = ResourceUtils.getSpoutResources(topology, topoConf, componentId);\n                if (spoutResources == null) {\n                    spoutResources = new NormalizedResourceRequest(topoConf);\n                }\n                compPageInfo.set_resources_map(spoutResources.toNormalizedMap());\n            } else { //bolt\n                NormalizedResourceRequest boltResources = ResourceUtils.getBoltResources(topology, topoConf, componentId);\n                if (boltResources == null) {\n                    boltResources = new NormalizedResourceRequest(topoConf);\n                }\n                compPageInfo.set_resources_map(boltResources.toNormalizedMap());\n            }\n            compPageInfo.set_topology_name(info.topoName);\n            compPageInfo.set_errors(stormClusterState.errors(topoId, componentId));\n            compPageInfo.set_topology_status(extractStatusStr(info.base));\n            if (info.base.is_set_component_debug()) {\n                DebugOptions debug = info.base.get_component_debug().get(componentId);\n                if (debug != null) {\n                    compPageInfo.set_debug_options(debug);\n                }\n            }\n            // Add the event logger details.\n            Map<String, List<Integer>> compToTasks = Utils.reverseMap(info.taskToComponent);\n            if (compToTasks.containsKey(StormCommon.EVENTLOGGER_COMPONENT_ID)) {\n                List<Integer> tasks = compToTasks.get(StormCommon.EVENTLOGGER_COMPONENT_ID);\n                tasks.sort(null);\n                // Find the task the events from this component route to.\n                int taskIndex = TupleUtils.chooseTaskIndex(Collections.singletonList(componentId), tasks.size());\n                int taskId = tasks.get(taskIndex);\n                String host = null;\n                Integer port = null;\n                for (Entry<List<Long>, List<Object>> entry : exec2HostPort.entrySet()) {\n                    int start = entry.getKey().get(0).intValue();\n                    int end = entry.getKey().get(1).intValue();\n                    if (taskId >= start && taskId <= end) {\n                        host = (String) entry.getValue().get(0);\n                        port = ((Number) entry.getValue().get(1)).intValue();\n                        break;\n                    }\n                }\n\n                if (host != null && port != null) {\n                    compPageInfo.set_eventlog_host(host);\n                    compPageInfo.set_eventlog_port(port);\n                }\n            }\n            return compPageInfo;\n        } catch (Exception e) {\n            LOG.warn(\"getComponentPageInfo exception. (topo id='{}')\", topoId, e);\n            if (e instanceof TException) {\n                throw (TException) e;\n            }\n            throw new RuntimeException(e);\n        }\n    }"
            },
            "storm-server.src.main.java.org.apache.storm.daemon.nimbus.Nimbus.validateTopologySize": {
                "code_before_change": "    private static void validateTopologySize(Map<String, Object> topoConf, Map<String, Object> nimbusConf,\n        StormTopology topology) throws InvalidTopologyException {\n        // check allowedWorkers only if the scheduler is not the Resource Aware Scheduler\n        if (!ServerUtils.isRAS(nimbusConf)) {\n            int workerCount = ObjectReader.getInt(topoConf.get(Config.TOPOLOGY_WORKERS), 1);\n            Integer allowedWorkers = ObjectReader.getInt(nimbusConf.get(DaemonConfig.NIMBUS_SLOTS_PER_TOPOLOGY), null);\n            if (allowedWorkers != null && workerCount > allowedWorkers) {\n                throw new InvalidTopologyException(\"Failed to submit topology. Topology requests more than \"\n                        + allowedWorkers + \" workers.\");\n            }\n        }\n        int executorsCount = 0;\n        for (Object comp : StormCommon.allComponents(topology).values()) {\n            executorsCount += StormCommon.numStartExecutors(comp);\n        }\n        Integer allowedExecutors = ObjectReader.getInt(nimbusConf.get(DaemonConfig.NIMBUS_EXECUTORS_PER_TOPOLOGY), null);\n        if (allowedExecutors != null && executorsCount > allowedExecutors) {\n            throw new InvalidTopologyException(\"Failed to submit topology. Topology requests more than \"\n                    + allowedExecutors + \" executors.\");\n        }\n    }",
                "code_after_change": "    private static void validateTopologySize(Map<String, Object> topoConf, Map<String, Object> nimbusConf,\n        StormTopology topology) throws InvalidTopologyException {\n        // check allowedWorkers only if the scheduler is not the Resource Aware Scheduler\n        if (!ServerUtils.isRAS(nimbusConf)) {\n            int workerCount = ObjectReader.getInt(topoConf.get(Config.TOPOLOGY_WORKERS), 1);\n            Integer allowedWorkers = ObjectReader.getInt(nimbusConf.get(DaemonConfig.NIMBUS_SLOTS_PER_TOPOLOGY), null);\n            if (allowedWorkers != null && workerCount > allowedWorkers) {\n                throw new WrappedInvalidTopologyException(\"Failed to submit topology. Topology requests more than \"\n                        + allowedWorkers + \" workers.\");\n            }\n        }\n        int executorsCount = 0;\n        for (Object comp : StormCommon.allComponents(topology).values()) {\n            executorsCount += StormCommon.numStartExecutors(comp);\n        }\n        Integer allowedExecutors = ObjectReader.getInt(nimbusConf.get(DaemonConfig.NIMBUS_EXECUTORS_PER_TOPOLOGY), null);\n        if (allowedExecutors != null && executorsCount > allowedExecutors) {\n            throw new WrappedInvalidTopologyException(\"Failed to submit topology. Topology requests more than \"\n                    + allowedExecutors + \" executors.\");\n        }\n    }"
            },
            "storm-server.src.main.java.org.apache.storm.daemon.nimbus.Nimbus.doRebalance": {
                "code_before_change": "    void doRebalance(String topoId, StormBase stormBase) throws Exception {\n        RebalanceOptions rbo = stormBase.get_topology_action_options().get_rebalance_options();\n        StormBase updated = new StormBase();\n        updated.set_topology_action_options(null);\n        updated.set_component_debug(Collections.emptyMap());\n\n        if (rbo.is_set_num_executors()) {\n            updated.set_component_executors(rbo.get_num_executors());\n        }\n\n        if (rbo.is_set_num_workers()) {\n            updated.set_num_workers(rbo.get_num_workers());\n        }\n        stormClusterState.updateStorm(topoId, updated);\n        updateBlobStore(topoId, rbo, ServerUtils.principalNameToSubject(rbo.get_principal()));\n        mkAssignments(topoId);\n    }",
                "code_after_change": "    void doRebalance(String topoId, StormBase stormBase) throws Exception {\n        RebalanceOptions rbo = stormBase.get_topology_action_options().get_rebalance_options();\n        StormBase updated = new StormBase();\n        updated.set_topology_action_options(null);\n        updated.set_component_debug(Collections.emptyMap());\n\n        if (rbo.is_set_num_executors()) {\n            updated.set_component_executors(rbo.get_num_executors());\n        }\n\n        if (rbo.is_set_num_workers()) {\n            updated.set_num_workers(rbo.get_num_workers());\n        }\n        stormClusterState.updateStorm(topoId, updated);\n        updateBlobStore(topoId, rbo, ServerUtils.principalNameToSubject(rbo.get_principal()));\n        mkAssignments(topoId);\n    }"
            },
            "storm-server.src.main.java.org.apache.storm.daemon.nimbus.Nimbus.checkAuthorization": {
                "code_before_change": "    public void checkAuthorization(String topoName, Map<String, Object> topoConf, String operation)\n        throws AuthorizationException {\n        checkAuthorization(topoName, topoConf, operation, null);\n    }",
                "code_after_change": "    public void checkAuthorization(String topoName, Map<String, Object> topoConf, String operation)\n        throws AuthorizationException {\n        checkAuthorization(topoName, topoConf, operation, null);\n    }"
            },
            "storm-server.src.main.java.org.apache.storm.daemon.nimbus.Nimbus.toExecInfo": {
                "code_before_change": "    private static ExecutorInfo toExecInfo(List<Long> exec) {\n        return new ExecutorInfo(exec.get(0).intValue(), exec.get(1).intValue());\n    }",
                "code_after_change": "    private static ExecutorInfo toExecInfo(List<Long> exec) {\n        return new ExecutorInfo(exec.get(0).intValue(), exec.get(1).intValue());\n    }"
            },
            "storm-client.src.jvm.org.apache.storm.blobstore.BlobStoreAclHandler.hasAnyPermissions": {
                "code_before_change": "    public void hasAnyPermissions(List<AccessControl> acl, int mask, Subject who, String key) throws AuthorizationException {\n        if (!doAclValidation) {\n            return;\n        }\n        Set<String> user = constructUserFromPrincipals(who);\n        LOG.debug(\"user {}\", user);\n        if (checkForValidUsers(who, mask)) {\n            return;\n        }\n        for (AccessControl ac : acl) {\n            int allowed = getAllowed(ac, user);\n            LOG.debug(\" user: {} allowed: {} key: {}\", user, allowed, key);\n            if ((allowed & mask) > 0) {\n                return;\n            }\n        }\n        throw new AuthorizationException(\n            user + \" does not have access to \" + key);\n    }",
                "code_after_change": "    public void hasAnyPermissions(List<AccessControl> acl, int mask, Subject who, String key) throws AuthorizationException {\n        if (!doAclValidation) {\n            return;\n        }\n        Set<String> user = constructUserFromPrincipals(who);\n        LOG.debug(\"user {}\", user);\n        if (checkForValidUsers(who, mask)) {\n            return;\n        }\n        for (AccessControl ac : acl) {\n            int allowed = getAllowed(ac, user);\n            LOG.debug(\" user: {} allowed: {} key: {}\", user, allowed, key);\n            if ((allowed & mask) > 0) {\n                return;\n            }\n        }\n        throw new WrappedAuthorizationException(\n            user + \" does not have access to \" + key);\n    }"
            },
            "storm-server.src.main.java.org.apache.storm.daemon.nimbus.Nimbus.getClusterInfoImpl": {
                "code_before_change": "    private ClusterSummary getClusterInfoImpl() throws Exception {\n        IStormClusterState state = stormClusterState;\n        Map<String, SupervisorInfo> infos = state.allSupervisorInfo();\n        List<SupervisorSummary> summaries = new ArrayList<>(infos.size());\n        for (Entry<String, SupervisorInfo> entry : infos.entrySet()) {\n            summaries.add(makeSupervisorSummary(entry.getKey(), entry.getValue()));\n        }\n        int uptime = this.uptime.upTime();\n        Map<String, StormBase> bases = state.topologyBases();\n\n        List<NimbusSummary> nimbuses = state.nimbuses();\n        //update the isLeader field for each nimbus summary\n        NimbusInfo leader = leaderElector.getLeader();\n        for (NimbusSummary nimbusSummary : nimbuses) {\n            nimbusSummary.set_uptime_secs(Time.deltaSecs(nimbusSummary.get_uptime_secs()));\n            nimbusSummary.set_isLeader(leader.getHost().equals(nimbusSummary.get_host()) &&\n                                       leader.getPort() == nimbusSummary.get_port());\n        }\n\n        List<TopologySummary> topologySummaries = new ArrayList<>();\n        for (Entry<String, StormBase> entry : bases.entrySet()) {\n            StormBase base = entry.getValue();\n            if (base == null) {\n                continue;\n            }\n            String topoId = entry.getKey();\n            Assignment assignment = state.assignmentInfo(topoId, null);\n\n            int numTasks = 0;\n            int numExecutors = 0;\n            int numWorkers = 0;\n            if (assignment != null && assignment.is_set_executor_node_port()) {\n                for (List<Long> ids : assignment.get_executor_node_port().keySet()) {\n                    numTasks += StormCommon.executorIdToTasks(ids).size();\n                }\n\n                numExecutors = assignment.get_executor_node_port_size();\n                numWorkers = new HashSet<>(assignment.get_executor_node_port().values()).size();\n            }\n\n            TopologySummary summary = new TopologySummary(topoId, base.get_name(), numTasks, numExecutors, numWorkers,\n                                                          Time.deltaSecs(base.get_launch_time_secs()), extractStatusStr(base));\n            try {\n                StormTopology topo = tryReadTopology(topoId, topoCache);\n                if (topo != null && topo.is_set_storm_version()) {\n                    summary.set_storm_version(topo.get_storm_version());\n                }\n            } catch (NotAliveException e) {\n                //Ignored it is not set\n            }\n\n            if (base.is_set_owner()) {\n                summary.set_owner(base.get_owner());\n            }\n\n            if (base.is_set_topology_version()) {\n                summary.set_topology_version(base.get_topology_version());\n            }\n\n            String status = idToSchedStatus.get().get(topoId);\n            if (status != null) {\n                summary.set_sched_status(status);\n            }\n            TopologyResources resources = getResourcesForTopology(topoId, base);\n            if (resources != null) {\n                summary.set_requested_memonheap(resources.getRequestedMemOnHeap());\n                summary.set_requested_memoffheap(resources.getRequestedMemOffHeap());\n                summary.set_requested_cpu(resources.getRequestedCpu());\n                summary.set_assigned_memonheap(resources.getAssignedMemOnHeap());\n                summary.set_assigned_memoffheap(resources.getAssignedMemOffHeap());\n                summary.set_assigned_cpu(resources.getAssignedCpu());\n            }\n            summary.set_replication_count(getBlobReplicationCount(ConfigUtils.masterStormCodeKey(topoId)));\n            topologySummaries.add(summary);\n        }\n\n        ClusterSummary ret = new ClusterSummary(summaries, topologySummaries, nimbuses);\n        ret.set_nimbus_uptime_secs(uptime);\n        return ret;\n    }",
                "code_after_change": "    private ClusterSummary getClusterInfoImpl() throws Exception {\n        IStormClusterState state = stormClusterState;\n        Map<String, SupervisorInfo> infos = state.allSupervisorInfo();\n        List<SupervisorSummary> summaries = new ArrayList<>(infos.size());\n        for (Entry<String, SupervisorInfo> entry : infos.entrySet()) {\n            summaries.add(makeSupervisorSummary(entry.getKey(), entry.getValue()));\n        }\n        int uptime = this.uptime.upTime();\n        Map<String, StormBase> bases = state.topologyBases();\n\n        List<NimbusSummary> nimbuses = state.nimbuses();\n        //update the isLeader field for each nimbus summary\n        NimbusInfo leader = leaderElector.getLeader();\n        for (NimbusSummary nimbusSummary : nimbuses) {\n            nimbusSummary.set_uptime_secs(Time.deltaSecs(nimbusSummary.get_uptime_secs()));\n            nimbusSummary.set_isLeader(leader.getHost().equals(nimbusSummary.get_host())\n                                       && leader.getPort() == nimbusSummary.get_port());\n        }\n\n        List<TopologySummary> topologySummaries = new ArrayList<>();\n        for (Entry<String, StormBase> entry : bases.entrySet()) {\n            StormBase base = entry.getValue();\n            if (base == null) {\n                continue;\n            }\n            String topoId = entry.getKey();\n            Assignment assignment = state.assignmentInfo(topoId, null);\n\n            int numTasks = 0;\n            int numExecutors = 0;\n            int numWorkers = 0;\n            if (assignment != null && assignment.is_set_executor_node_port()) {\n                for (List<Long> ids : assignment.get_executor_node_port().keySet()) {\n                    numTasks += StormCommon.executorIdToTasks(ids).size();\n                }\n\n                numExecutors = assignment.get_executor_node_port_size();\n                numWorkers = new HashSet<>(assignment.get_executor_node_port().values()).size();\n            }\n\n            TopologySummary summary = new TopologySummary(topoId, base.get_name(), numTasks, numExecutors, numWorkers,\n                                                          Time.deltaSecs(base.get_launch_time_secs()), extractStatusStr(base));\n            try {\n                StormTopology topo = tryReadTopology(topoId, topoCache);\n                if (topo != null && topo.is_set_storm_version()) {\n                    summary.set_storm_version(topo.get_storm_version());\n                }\n            } catch (NotAliveException e) {\n                //Ignored it is not set\n            }\n\n            if (base.is_set_owner()) {\n                summary.set_owner(base.get_owner());\n            }\n\n            if (base.is_set_topology_version()) {\n                summary.set_topology_version(base.get_topology_version());\n            }\n\n            String status = idToSchedStatus.get().get(topoId);\n            if (status != null) {\n                summary.set_sched_status(status);\n            }\n            TopologyResources resources = getResourcesForTopology(topoId, base);\n            if (resources != null) {\n                summary.set_requested_memonheap(resources.getRequestedMemOnHeap());\n                summary.set_requested_memoffheap(resources.getRequestedMemOffHeap());\n                summary.set_requested_cpu(resources.getRequestedCpu());\n                summary.set_assigned_memonheap(resources.getAssignedMemOnHeap());\n                summary.set_assigned_memoffheap(resources.getAssignedMemOffHeap());\n                summary.set_assigned_cpu(resources.getAssignedCpu());\n            }\n            summary.set_replication_count(getBlobReplicationCount(ConfigUtils.masterStormCodeKey(topoId)));\n            topologySummaries.add(summary);\n        }\n\n        ClusterSummary ret = new ClusterSummary(summaries, topologySummaries, nimbuses);\n        ret.set_nimbus_uptime_secs(uptime);\n        return ret;\n    }"
            },
            "storm-server.src.main.java.org.apache.storm.daemon.supervisor.Supervisor.sendSupervisorWorkerHeartbeat": {
                "code_before_change": "                public void sendSupervisorWorkerHeartbeat(SupervisorWorkerHeartbeat heartbeat)\n                    throws AuthorizationException, NotAliveException, TException {\n                    // do nothing except validate heartbeat for now.\n                    String id = heartbeat.get_storm_id();\n                    Map<String, Object> topoConf = null;\n                    try {\n                        topoConf = ConfigUtils.readSupervisorStormConf(conf, id);\n                    } catch (IOException e) {\n                        LOG.warn(\"Topology config is not localized yet...\");\n                        throw new NotAliveException(id + \" does not appear to be alive, you should probably exit\");\n                    }\n                    checkAuthorization(id, topoConf, \"sendSupervisorWorkerHeartbeat\");\n                }",
                "code_after_change": "                public void sendSupervisorWorkerHeartbeat(SupervisorWorkerHeartbeat heartbeat)\n                    throws AuthorizationException, NotAliveException, TException {\n                    // do nothing except validate heartbeat for now.\n                    String id = heartbeat.get_storm_id();\n                    Map<String, Object> topoConf = null;\n                    try {\n                        topoConf = ConfigUtils.readSupervisorStormConf(conf, id);\n                    } catch (IOException e) {\n                        LOG.warn(\"Topology config is not localized yet...\");\n                        throw new WrappedNotAliveException(id + \" does not appear to be alive, you should probably exit\");\n                    }\n                    checkAuthorization(id, topoConf, \"sendSupervisorWorkerHeartbeat\");\n                }"
            },
            "storm-server.src.main.java.org.apache.storm.LocalCluster.waitForIdle": {
                "code_before_change": "    public void waitForIdle() throws InterruptedException {\n        waitForIdle(Testing.TEST_TIMEOUT_MS);\n    }",
                "code_after_change": "    public void waitForIdle() throws InterruptedException {\n        waitForIdle(Testing.TEST_TIMEOUT_MS);\n    }"
            },
            "storm-server.src.main.java.org.apache.storm.daemon.nimbus.Nimbus.getTopologyHistory": {
                "code_before_change": "    public TopologyHistoryInfo getTopologyHistory(String user) throws AuthorizationException, TException {\n        try {\n            List<String> adminUsers = (List<String>) conf.getOrDefault(Config.NIMBUS_ADMINS, Collections.emptyList());\n            List<String> adminGroups = (List<String>) conf.getOrDefault(Config.NIMBUS_ADMINS_GROUPS, Collections.emptyList());\n            IStormClusterState state = stormClusterState;\n            List<String> assignedIds = state.assignments(null);\n            Set<String> ret = new HashSet<>();\n            boolean isAdmin = adminUsers.contains(user);\n            for (String topoId : assignedIds) {\n                Map<String, Object> topoConf = tryReadTopoConf(topoId, topoCache);\n                topoConf = Utils.merge(conf, topoConf);\n                List<String> groups = ServerConfigUtils.getTopoLogsGroups(topoConf);\n                List<String> topoLogUsers = ServerConfigUtils.getTopoLogsUsers(topoConf);\n                if (user == null || isAdmin ||\n                    isUserPartOf(user, groups) ||\n                    isUserPartOf(user, adminGroups) ||\n                    topoLogUsers.contains(user)) {\n                    ret.add(topoId);\n                }\n            }\n            ret.addAll(readTopologyHistory(user, adminUsers));\n            return new TopologyHistoryInfo(new ArrayList<>(ret));\n        } catch (Exception e) {\n            LOG.warn(\"Get topology history. (user='{}')\", user, e);\n            if (e instanceof TException) {\n                throw (TException) e;\n            }\n            throw new RuntimeException(e);\n        }\n    }",
                "code_after_change": "    public TopologyHistoryInfo getTopologyHistory(String user) throws AuthorizationException, TException {\n        try {\n            List<String> adminUsers = (List<String>) conf.getOrDefault(Config.NIMBUS_ADMINS, Collections.emptyList());\n            List<String> adminGroups = (List<String>) conf.getOrDefault(Config.NIMBUS_ADMINS_GROUPS, Collections.emptyList());\n            IStormClusterState state = stormClusterState;\n            List<String> assignedIds = state.assignments(null);\n            Set<String> ret = new HashSet<>();\n            boolean isAdmin = adminUsers.contains(user);\n            for (String topoId : assignedIds) {\n                Map<String, Object> topoConf = tryReadTopoConf(topoId, topoCache);\n                topoConf = Utils.merge(conf, topoConf);\n                List<String> groups = ServerConfigUtils.getTopoLogsGroups(topoConf);\n                List<String> topoLogUsers = ServerConfigUtils.getTopoLogsUsers(topoConf);\n                if (user == null || isAdmin\n                    || isUserPartOf(user, groups)\n                    || isUserPartOf(user, adminGroups)\n                    || topoLogUsers.contains(user)) {\n                    ret.add(topoId);\n                }\n            }\n            ret.addAll(readTopologyHistory(user, adminUsers));\n            return new TopologyHistoryInfo(new ArrayList<>(ret));\n        } catch (Exception e) {\n            LOG.warn(\"Get topology history. (user='{}')\", user, e);\n            if (e instanceof TException) {\n                throw (TException) e;\n            }\n            throw new RuntimeException(e);\n        }\n    }"
            },
            "storm-server.src.main.java.org.apache.storm.daemon.supervisor.Supervisor.checkAuthorization": {
                "code_before_change": "    public void checkAuthorization(String operation) throws AuthorizationException {\n        checkAuthorization(null, null, operation, null);\n    }",
                "code_after_change": "    public void checkAuthorization(String operation) throws AuthorizationException {\n        checkAuthorization(null, null, operation, null);\n    }"
            },
            "storm-server.src.main.java.org.apache.storm.LocalCluster.cancelBlobUpload": {
                "code_before_change": "    public void cancelBlobUpload(String session) throws AuthorizationException, TException {\n        throw new RuntimeException(\"BLOBS NOT SUPPORTED IN LOCAL MODE\");\n    }",
                "code_after_change": "    public void cancelBlobUpload(String session) throws AuthorizationException, TException {\n        throw new RuntimeException(\"BLOBS NOT SUPPORTED IN LOCAL MODE\");\n    }"
            },
            "storm-client.src.jvm.org.apache.storm.blobstore.BlobStoreAclHandler.checkForValidUsers": {
                "code_before_change": "    public boolean checkForValidUsers(Subject who, int mask) {\n        return isNimbus(who) || isAdmin(who) || isSupervisor(who, mask);\n    }",
                "code_after_change": "    public boolean checkForValidUsers(Subject who, int mask) {\n        return isNimbus(who) || isAdmin(who) || isSupervisor(who, mask);\n    }"
            },
            "storm-client.src.jvm.org.apache.storm.daemon.StormCommon.validateIds": {
                "code_before_change": "    private static Set<String> validateIds(Map<String, ? extends Object> componentMap) throws InvalidTopologyException {\n        Set<String> keys = componentMap.keySet();\n        for (String id : keys) {\n            if (Utils.isSystemId(id)) {\n                throw new InvalidTopologyException(id + \" is not a valid component id.\");\n            }\n        }\n        for (Object componentObj : componentMap.values()) {\n            ComponentCommon common = getComponentCommon(componentObj);\n            Set<String> streamIds = common.get_streams().keySet();\n            for (String id : streamIds) {\n                if (Utils.isSystemId(id)) {\n                    throw new InvalidTopologyException(id + \" is not a valid stream id.\");\n                }\n            }\n        }\n        return keys;\n    }",
                "code_after_change": "    private static Set<String> validateIds(Map<String, ? extends Object> componentMap) throws InvalidTopologyException {\n        Set<String> keys = componentMap.keySet();\n        for (String id : keys) {\n            if (Utils.isSystemId(id)) {\n                throw new WrappedInvalidTopologyException(id + \" is not a valid component id.\");\n            }\n        }\n        for (Object componentObj : componentMap.values()) {\n            ComponentCommon common = getComponentCommon(componentObj);\n            Set<String> streamIds = common.get_streams().keySet();\n            for (String id : streamIds) {\n                if (Utils.isSystemId(id)) {\n                    throw new WrappedInvalidTopologyException(id + \" is not a valid stream id.\");\n                }\n            }\n        }\n        return keys;\n    }"
            },
            "storm-server.src.main.java.org.apache.storm.daemon.nimbus.Nimbus.userGroups": {
                "code_before_change": "    private Set<String> userGroups(String user) throws IOException {\n        if (user == null || user.isEmpty()) {\n            return Collections.emptySet();\n        }\n        return groupMapper.getGroups(user);\n    }",
                "code_after_change": "    private Set<String> userGroups(String user) throws IOException {\n        if (user == null || user.isEmpty()) {\n            return Collections.emptySet();\n        }\n        return groupMapper.getGroups(user);\n    }"
            },
            "storm-client.src.jvm.org.apache.storm.blobstore.BlobStoreAclHandler.removeBadACLs": {
                "code_before_change": "    private List<AccessControl> removeBadACLs(List<AccessControl> accessControls) {\n        List<AccessControl> resultAcl = new ArrayList<AccessControl>();\n        for (AccessControl control : accessControls) {\n            if (control.get_type().equals(AccessControlType.OTHER) && (control.get_access() == 0)) {\n                LOG.debug(\"Removing invalid blobstore world ACL \" +\n                          BlobStoreAclHandler.accessControlToString(control));\n                continue;\n            }\n            resultAcl.add(control);\n        }\n        return resultAcl;\n    }",
                "code_after_change": "    private List<AccessControl> removeBadACLs(List<AccessControl> accessControls) {\n        List<AccessControl> resultAcl = new ArrayList<AccessControl>();\n        for (AccessControl control : accessControls) {\n            if (control.get_type().equals(AccessControlType.OTHER) && (control.get_access() == 0)) {\n                LOG.debug(\"Removing invalid blobstore world ACL \"\n                          + BlobStoreAclHandler.accessControlToString(control));\n                continue;\n            }\n            resultAcl.add(control);\n        }\n        return resultAcl;\n    }"
            },
            "storm-server.src.main.java.org.apache.storm.LocalCluster.listBlobs": {
                "code_before_change": "    public ListBlobsResult listBlobs(String session) throws TException {\n        //Blobs are not supported in local mode.  Return nothing\n        ListBlobsResult ret = new ListBlobsResult();\n        ret.set_keys(new ArrayList<>());\n        return ret;\n    }",
                "code_after_change": "    public ListBlobsResult listBlobs(String session) throws TException {\n        //Blobs are not supported in local mode.  Return nothing\n        ListBlobsResult ret = new ListBlobsResult();\n        ret.set_keys(new ArrayList<>());\n        return ret;\n    }"
            },
            "storm-server.src.main.java.org.apache.storm.daemon.nimbus.Nimbus.mkAssignments": {
                "code_before_change": "    private void mkAssignments() throws Exception {\n        mkAssignments(null);\n    }",
                "code_after_change": "    private void mkAssignments() throws Exception {\n        mkAssignments(null);\n    }"
            },
            "storm-server.src.main.java.org.apache.storm.daemon.nimbus.Nimbus.getKeyListFromId": {
                "code_before_change": "    static List<String> getKeyListFromId(Map<String, Object> conf, String id) {\n        List<String> ret = new ArrayList<>(3);\n        ret.add(ConfigUtils.masterStormCodeKey(id));\n        ret.add(ConfigUtils.masterStormConfKey(id));\n        if (!ConfigUtils.isLocalMode(conf)) {\n            ret.add(ConfigUtils.masterStormJarKey(id));\n        }\n        return ret;\n    }",
                "code_after_change": "    static List<String> getKeyListFromId(Map<String, Object> conf, String id) {\n        List<String> ret = new ArrayList<>(3);\n        ret.add(ConfigUtils.masterStormCodeKey(id));\n        ret.add(ConfigUtils.masterStormConfKey(id));\n        if (!ConfigUtils.isLocalMode(conf)) {\n            ret.add(ConfigUtils.masterStormJarKey(id));\n        }\n        return ret;\n    }"
            },
            "storm-server.src.main.java.org.apache.storm.daemon.nimbus.Nimbus.tryReadTopology": {
                "code_before_change": "    private static StormTopology tryReadTopology(String topoId, TopoCache tc)\n        throws NotAliveException, AuthorizationException, IOException {\n        try {\n            return readStormTopologyAsNimbus(topoId, tc);\n        } catch (KeyNotFoundException e) {\n            throw new NotAliveException(topoId);\n        }\n    }",
                "code_after_change": "    private static StormTopology tryReadTopology(String topoId, TopoCache tc)\n        throws NotAliveException, AuthorizationException, IOException {\n        try {\n            return readStormTopologyAsNimbus(topoId, tc);\n        } catch (KeyNotFoundException e) {\n            throw new WrappedNotAliveException(topoId);\n        }\n    }"
            },
            "storm-server.src.main.java.org.apache.storm.daemon.supervisor.Supervisor.getLocalAssignmentForStorm": {
                "code_before_change": "                public Assignment getLocalAssignmentForStorm(String id)\n                    throws NotAliveException, AuthorizationException, TException {\n                    Map<String, Object> topoConf = null;\n                    try {\n                        topoConf = ConfigUtils.readSupervisorStormConf(conf, id);\n                    } catch (IOException e) {\n                        LOG.warn(\"Topology config is not localized yet...\");\n                    }\n                    checkAuthorization(id, topoConf, \"getLocalAssignmentForStorm\");\n                    Assignment assignment = getStormClusterState().assignmentInfo(id, null);\n                    if (null == assignment) {\n                        throw new NotAliveException(\"No local assignment assigned for storm: \"\n                                                    + id\n                                                    + \" for node: \"\n                                                    + getHostName());\n                    }\n                    return assignment;\n                }",
                "code_after_change": "                public Assignment getLocalAssignmentForStorm(String id)\n                    throws NotAliveException, AuthorizationException, TException {\n                    Map<String, Object> topoConf = null;\n                    try {\n                        topoConf = ConfigUtils.readSupervisorStormConf(conf, id);\n                    } catch (IOException e) {\n                        LOG.warn(\"Topology config is not localized yet...\");\n                    }\n                    checkAuthorization(id, topoConf, \"getLocalAssignmentForStorm\");\n                    Assignment assignment = getStormClusterState().assignmentInfo(id, null);\n                    if (null == assignment) {\n                        throw new WrappedNotAliveException(\"No local assignment assigned for storm: \"\n                                                    + id\n                                                    + \" for node: \"\n                                                    + getHostName());\n                    }\n                    return assignment;\n                }"
            },
            "storm-server.src.main.java.org.apache.storm.LocalCluster.beginCreateBlob": {
                "code_before_change": "    public String beginCreateBlob(String key, SettableBlobMeta meta)\n        throws AuthorizationException, KeyAlreadyExistsException, TException {\n        throw new RuntimeException(\"BLOBS NOT SUPPORTED IN LOCAL MODE\");\n    }",
                "code_after_change": "    public String beginCreateBlob(String key, SettableBlobMeta meta)\n        throws AuthorizationException, KeyAlreadyExistsException, TException {\n        throw new RuntimeException(\"BLOBS NOT SUPPORTED IN LOCAL MODE\");\n    }"
            },
            "storm-server.src.main.java.org.apache.storm.daemon.drpc.BlockingOutstandingRequest.getResult": {
                "code_before_change": "    public String getResult() throws DRPCExecutionException {\n        try {\n            _sem.acquire();\n        } catch (InterruptedException e) {\n            //Ignored\n        }\n\n        if (_result != null) {\n            return _result;\n        }\n\n        if (_e == null) {\n            _e = new DRPCExecutionException(\"Internal Error: No Result and No Exception\");\n            _e.set_type(DRPCExceptionType.INTERNAL_ERROR);\n        }\n        throw _e;\n    }",
                "code_after_change": "    public String getResult() throws DRPCExecutionException {\n        try {\n            _sem.acquire();\n        } catch (InterruptedException e) {\n            //Ignored\n        }\n\n        if (_result != null) {\n            return _result;\n        }\n\n        if (_e == null) {\n            _e = new WrappedDRPCExecutionException(\"Internal Error: No Result and No Exception\");\n            _e.set_type(DRPCExceptionType.INTERNAL_ERROR);\n        }\n        throw _e;\n    }"
            },
            "storm-client.src.jvm.org.apache.storm.utils.Utils.nullToZero": {
                "code_before_change": "    public static double nullToZero(Double v) {\n        return (v != null ? v : 0);\n    }",
                "code_after_change": "    public static double nullToZero(Double v) {\n        return (v != null ? v : 0);\n    }"
            },
            "storm-server.src.main.java.org.apache.storm.daemon.drpc.DRPC.checkAuthorization": {
                "code_before_change": "    static void checkAuthorization(ReqContext reqContext, IAuthorizer auth, String operation, String function)\n        throws AuthorizationException {\n        checkAuthorization(reqContext, auth, operation, function, true);\n    }",
                "code_after_change": "    static void checkAuthorization(ReqContext reqContext, IAuthorizer auth, String operation, String function)\n        throws AuthorizationException {\n        checkAuthorization(reqContext, auth, operation, function, true);\n    }"
            },
            "storm-server.src.main.java.org.apache.storm.blobstore.LocalFsBlobStore.getBlobReplication": {
                "code_before_change": "    public int getBlobReplication(String key, Subject who) throws Exception {\n        int replicationCount = 0;\n        validateKey(key);\n        SettableBlobMeta meta = getStoredBlobMeta(key);\n        _aclHandler.hasPermissions(meta.get_acl(), READ, who, key);\n        if (zkClient.checkExists().forPath(BLOBSTORE_SUBTREE + key) == null) {\n            return 0;\n        }\n        try {\n            replicationCount = zkClient.getChildren().forPath(BLOBSTORE_SUBTREE + key).size();\n        } catch (NoNodeException e) {\n            //Race with delete\n            //If it is not here the replication is 0 \n        }\n        return replicationCount;\n    }",
                "code_after_change": "    public int getBlobReplication(String key, Subject who) throws Exception {\n        int replicationCount = 0;\n        validateKey(key);\n        SettableBlobMeta meta = getStoredBlobMeta(key);\n        _aclHandler.hasPermissions(meta.get_acl(), READ, who, key);\n        if (zkClient.checkExists().forPath(BLOBSTORE_SUBTREE + key) == null) {\n            return 0;\n        }\n        try {\n            replicationCount = zkClient.getChildren().forPath(BLOBSTORE_SUBTREE + key).size();\n        } catch (NoNodeException e) {\n            //Race with delete\n            //If it is not here the replication is 0 \n        }\n        return replicationCount;\n    }"
            },
            "storm-server.src.main.java.org.apache.storm.daemon.nimbus.Nimbus.debug": {
                "code_before_change": "    public void debug(String topoName, String componentId, boolean enable, double samplingPercentage)\n        throws NotAliveException, AuthorizationException, TException {\n        debugCalls.mark();\n        try {\n            IStormClusterState state = stormClusterState;\n            String topoId = toTopoId(topoName);\n            Map<String, Object> topoConf = tryReadTopoConf(topoId, topoCache);\n            topoConf = Utils.merge(conf, topoConf);\n            // make sure samplingPct is within bounds.\n            double spct = Math.max(Math.min(samplingPercentage, 100.0), 0.0);\n            // while disabling we retain the sampling pct.\n            checkAuthorization(topoName, topoConf, \"debug\");\n            if (topoId == null) {\n                throw new NotAliveException(topoName);\n            }\n            boolean hasCompId = componentId != null && !componentId.isEmpty();\n\n            DebugOptions options = new DebugOptions();\n            options.set_enable(enable);\n            if (enable) {\n                options.set_samplingpct(spct);\n            }\n            StormBase updates = new StormBase();\n            //For backwards compatability\n            updates.set_component_executors(Collections.emptyMap());\n            String key = hasCompId ? componentId : topoId;\n            updates.put_to_component_debug(key, options);\n\n            LOG.info(\"Nimbus setting debug to {} for storm-name '{}' storm-id '{}' sanpling pct '{}'\" +\n                     (hasCompId ? \" component-id '\" + componentId + \"'\" : \"\"),\n                     enable, topoName, topoId, spct);\n            synchronized (submitLock) {\n                state.updateStorm(topoId, updates);\n            }\n        } catch (Exception e) {\n            LOG.warn(\"debug topology exception. (topology name='{}')\", topoName, e);\n            if (e instanceof TException) {\n                throw (TException) e;\n            }\n            throw new RuntimeException(e);\n        }\n    }",
                "code_after_change": "    public void debug(String topoName, String componentId, boolean enable, double samplingPercentage)\n        throws NotAliveException, AuthorizationException, TException {\n        debugCalls.mark();\n        try {\n            IStormClusterState state = stormClusterState;\n            String topoId = toTopoId(topoName);\n            Map<String, Object> topoConf = tryReadTopoConf(topoId, topoCache);\n            topoConf = Utils.merge(conf, topoConf);\n            // make sure samplingPct is within bounds.\n            double spct = Math.max(Math.min(samplingPercentage, 100.0), 0.0);\n            // while disabling we retain the sampling pct.\n            checkAuthorization(topoName, topoConf, \"debug\");\n            if (topoId == null) {\n                throw new WrappedNotAliveException(topoName);\n            }\n            boolean hasCompId = componentId != null && !componentId.isEmpty();\n\n            DebugOptions options = new DebugOptions();\n            options.set_enable(enable);\n            if (enable) {\n                options.set_samplingpct(spct);\n            }\n            StormBase updates = new StormBase();\n            //For backwards compatability\n            updates.set_component_executors(Collections.emptyMap());\n            String key = hasCompId ? componentId : topoId;\n            updates.put_to_component_debug(key, options);\n\n            LOG.info(\"Nimbus setting debug to {} for storm-name '{}' storm-id '{}' sanpling pct '{}'\"\n                     + (hasCompId ? \" component-id '\" + componentId + \"'\" : \"\"),\n                     enable, topoName, topoId, spct);\n            synchronized (submitLock) {\n                state.updateStorm(topoId, updates);\n            }\n        } catch (Exception e) {\n            LOG.warn(\"debug topology exception. (topology name='{}')\", topoName, e);\n            if (e instanceof TException) {\n                throw (TException) e;\n            }\n            throw new RuntimeException(e);\n        }\n    }"
            },
            "storm-server.src.main.java.org.apache.storm.nimbus.StrictTopologyValidator.validate": {
                "code_before_change": "    public void validate(String topologyName, Map topologyConf, StormTopology topology) throws InvalidTopologyException {\n        if (topologyName.contains(\".\")) {\n            throw new InvalidTopologyException(String.format(\"Topology name '%s' contains illegal character '.'\", topologyName));\n        }\n        Map<String, SpoutSpec> spouts = topology.get_spouts();\n        for (String spoutName : spouts.keySet()) {\n            if (spoutName.contains(\".\")) {\n                throw new InvalidTopologyException(String.format(\"Spout name '%s' contains illegal character '.'\", spoutName));\n            }\n            SpoutSpec spoutSpec = spouts.get(spoutName);\n            for (String streamName : spoutSpec.get_common().get_streams().keySet()) {\n                if (streamName.contains(\".\")) {\n                    throw new InvalidTopologyException(String.format(\"Stream name '%s' contains illegal character '.'\", streamName));\n                }\n            }\n        }\n\n        Map<String, Bolt> bolts = topology.get_bolts();\n        for (String boltName : bolts.keySet()) {\n            if (boltName.contains(\".\")) {\n                throw new InvalidTopologyException(String.format(\"Bolt name '%s' contains illegal character '.'\", boltName));\n            }\n            Bolt bolt = bolts.get(boltName);\n            for (String streamName : bolt.get_common().get_streams().keySet()) {\n                if (streamName.contains(\".\")) {\n                    throw new InvalidTopologyException(String.format(\"Stream name '%s' contains illegal character '.'\", streamName));\n                }\n            }\n        }\n    }",
                "code_after_change": "    public void validate(String topologyName, Map topologyConf, StormTopology topology) throws InvalidTopologyException {\n        if (topologyName.contains(\".\")) {\n            throw new WrappedInvalidTopologyException(String.format(\"Topology name '%s' contains illegal character '.'\", topologyName));\n        }\n        Map<String, SpoutSpec> spouts = topology.get_spouts();\n        for (String spoutName : spouts.keySet()) {\n            if (spoutName.contains(\".\")) {\n                throw new WrappedInvalidTopologyException(String.format(\"Spout name '%s' contains illegal character '.'\", spoutName));\n            }\n            SpoutSpec spoutSpec = spouts.get(spoutName);\n            for (String streamName : spoutSpec.get_common().get_streams().keySet()) {\n                if (streamName.contains(\".\")) {\n                    throw new WrappedInvalidTopologyException(String.format(\"Stream name '%s' contains illegal character '.'\", streamName));\n                }\n            }\n        }\n\n        Map<String, Bolt> bolts = topology.get_bolts();\n        for (String boltName : bolts.keySet()) {\n            if (boltName.contains(\".\")) {\n                throw new WrappedInvalidTopologyException(String.format(\"Bolt name '%s' contains illegal character '.'\", boltName));\n            }\n            Bolt bolt = bolts.get(boltName);\n            for (String streamName : bolt.get_common().get_streams().keySet()) {\n                if (streamName.contains(\".\")) {\n                    throw new WrappedInvalidTopologyException(String.format(\"Stream name '%s' contains illegal character '.'\", streamName));\n                }\n            }\n        }\n    }"
            },
            "storm-server.src.main.java.org.apache.storm.blobstore.LocalFsBlobStore.getStoredBlobMeta": {
                "code_before_change": "    private SettableBlobMeta getStoredBlobMeta(String key) throws KeyNotFoundException {\n        InputStream in = null;\n        try {\n            LocalFsBlobStoreFile pf = fbs.read(META_PREFIX + key);\n            try {\n                in = pf.getInputStream();\n            } catch (FileNotFoundException fnf) {\n                throw new KeyNotFoundException(key);\n            }\n            ByteArrayOutputStream out = new ByteArrayOutputStream();\n            byte[] buffer = new byte[2048];\n            int len;\n            while ((len = in.read(buffer)) > 0) {\n                out.write(buffer, 0, len);\n            }\n            in.close();\n            in = null;\n            return Utils.thriftDeserialize(SettableBlobMeta.class, out.toByteArray());\n        } catch (IOException e) {\n            throw new RuntimeException(e);\n        } finally {\n            if (in != null) {\n                try {\n                    in.close();\n                } catch (IOException e) {\n                    //Ignored\n                }\n            }\n        }\n    }",
                "code_after_change": "    private SettableBlobMeta getStoredBlobMeta(String key) throws KeyNotFoundException {\n        InputStream in = null;\n        try {\n            LocalFsBlobStoreFile pf = fbs.read(META_PREFIX + key);\n            try {\n                in = pf.getInputStream();\n            } catch (FileNotFoundException fnf) {\n                throw new WrappedKeyNotFoundException(key);\n            }\n            ByteArrayOutputStream out = new ByteArrayOutputStream();\n            byte[] buffer = new byte[2048];\n            int len;\n            while ((len = in.read(buffer)) > 0) {\n                out.write(buffer, 0, len);\n            }\n            in.close();\n            in = null;\n            return Utils.thriftDeserialize(SettableBlobMeta.class, out.toByteArray());\n        } catch (IOException e) {\n            throw new RuntimeException(e);\n        } finally {\n            if (in != null) {\n                try {\n                    in.close();\n                } catch (IOException e) {\n                    //Ignored\n                }\n            }\n        }\n    }"
            },
            "storm-server.src.main.java.org.apache.storm.daemon.nimbus.Nimbus.waitForDesiredCodeReplication": {
                "code_before_change": "    private void waitForDesiredCodeReplication(Map<String, Object> topoConf, String topoId) throws Exception {\n        int minReplicationCount = ObjectReader.getInt(topoConf.get(Config.TOPOLOGY_MIN_REPLICATION_COUNT));\n        int maxWaitTime = ObjectReader.getInt(topoConf.get(Config.TOPOLOGY_MAX_REPLICATION_WAIT_TIME_SEC));\n        int jarCount = minReplicationCount;\n        if (!ConfigUtils.isLocalMode(topoConf)) {\n            jarCount = getBlobReplicationCount(ConfigUtils.masterStormJarKey(topoId));\n        }\n        int codeCount = getBlobReplicationCount(ConfigUtils.masterStormCodeKey(topoId));\n        int confCount = getBlobReplicationCount(ConfigUtils.masterStormConfKey(topoId));\n        long totalWaitTime = 0;\n        //When is this ever null?\n        if (blobStore != null) {\n            while (jarCount < minReplicationCount &&\n                   codeCount < minReplicationCount &&\n                   confCount < minReplicationCount) {\n                if (maxWaitTime > 0 && totalWaitTime > maxWaitTime) {\n                    LOG.info(\"desired replication count of {} not achieved but we have hit the max wait time {}\"\n                             + \" so moving on with replication count for conf key = {} for code key = {} for jar key = \",\n                             minReplicationCount, maxWaitTime, confCount, codeCount, jarCount);\n                    return;\n                }\n                LOG.debug(\"Checking if I am still the leader\");\n                assertIsLeader();\n                LOG.info(\"WAITING... storm-id {}, {} <? {} {} {}\", topoId, minReplicationCount, jarCount, codeCount, confCount);\n                LOG.info(\"WAITING... {} <? {}\", totalWaitTime, maxWaitTime);\n                Time.sleepSecs(1);\n                totalWaitTime++;\n                if (!ConfigUtils.isLocalMode(topoConf)) {\n                    jarCount = getBlobReplicationCount(ConfigUtils.masterStormJarKey(topoId));\n                }\n                codeCount = getBlobReplicationCount(ConfigUtils.masterStormCodeKey(topoId));\n                confCount = getBlobReplicationCount(ConfigUtils.masterStormConfKey(topoId));\n            }\n        }\n        LOG.info(\"desired replication count {} achieved, current-replication-count for conf key = {},\"\n                 + \" current-replication-count for code key = {}, current-replication-count for jar key = {}\",\n                 minReplicationCount, confCount, codeCount, jarCount);\n    }",
                "code_after_change": "    private void waitForDesiredCodeReplication(Map<String, Object> topoConf, String topoId) throws Exception {\n        int minReplicationCount = ObjectReader.getInt(topoConf.get(Config.TOPOLOGY_MIN_REPLICATION_COUNT));\n        int maxWaitTime = ObjectReader.getInt(topoConf.get(Config.TOPOLOGY_MAX_REPLICATION_WAIT_TIME_SEC));\n        int jarCount = minReplicationCount;\n        if (!ConfigUtils.isLocalMode(topoConf)) {\n            jarCount = getBlobReplicationCount(ConfigUtils.masterStormJarKey(topoId));\n        }\n        int codeCount = getBlobReplicationCount(ConfigUtils.masterStormCodeKey(topoId));\n        int confCount = getBlobReplicationCount(ConfigUtils.masterStormConfKey(topoId));\n        long totalWaitTime = 0;\n        //When is this ever null?\n        if (blobStore != null) {\n            while (jarCount < minReplicationCount\n                   && codeCount < minReplicationCount\n                   && confCount < minReplicationCount) {\n                if (maxWaitTime > 0 && totalWaitTime > maxWaitTime) {\n                    LOG.info(\"desired replication count of {} not achieved but we have hit the max wait time {}\"\n                             + \" so moving on with replication count for conf key = {} for code key = {} for jar key = \",\n                             minReplicationCount, maxWaitTime, confCount, codeCount, jarCount);\n                    return;\n                }\n                LOG.debug(\"Checking if I am still the leader\");\n                assertIsLeader();\n                LOG.info(\"WAITING... storm-id {}, {} <? {} {} {}\", topoId, minReplicationCount, jarCount, codeCount, confCount);\n                LOG.info(\"WAITING... {} <? {}\", totalWaitTime, maxWaitTime);\n                Time.sleepSecs(1);\n                totalWaitTime++;\n                if (!ConfigUtils.isLocalMode(topoConf)) {\n                    jarCount = getBlobReplicationCount(ConfigUtils.masterStormJarKey(topoId));\n                }\n                codeCount = getBlobReplicationCount(ConfigUtils.masterStormCodeKey(topoId));\n                confCount = getBlobReplicationCount(ConfigUtils.masterStormConfKey(topoId));\n            }\n        }\n        LOG.info(\"desired replication count {} achieved, current-replication-count for conf key = {},\"\n                 + \" current-replication-count for code key = {}, current-replication-count for jar key = {}\",\n                 minReplicationCount, confCount, codeCount, jarCount);\n    }"
            },
            "storm-server.src.main.java.org.apache.storm.nimbus.StrictTopologyValidator.prepare": {
                "code_before_change": "    public void prepare(Map stormConf) {\n    }",
                "code_after_change": "    public void prepare(Map stormConf) {\n    }"
            },
            "storm-server.src.main.java.org.apache.storm.LocalCluster.downloadBlobChunk": {
                "code_before_change": "    public ByteBuffer downloadBlobChunk(String session) throws AuthorizationException, TException {\n        throw new RuntimeException(\"BLOBS NOT SUPPORTED IN LOCAL MODE\");\n    }",
                "code_after_change": "    public ByteBuffer downloadBlobChunk(String session) throws AuthorizationException, TException {\n        throw new RuntimeException(\"BLOBS NOT SUPPORTED IN LOCAL MODE\");\n    }"
            },
            "storm-client.src.jvm.org.apache.storm.daemon.StormCommon.validateBasic": {
                "code_before_change": "    public static void validateBasic(StormTopology topology) throws InvalidTopologyException {\n        validateIds(topology);\n\n        for (StormTopology._Fields field : Thrift.getSpoutFields()) {\n            Map<String, Object> spoutComponents = (Map<String, Object>) topology.getFieldValue(field);\n            if (spoutComponents != null) {\n                for (Object obj : spoutComponents.values()) {\n                    ComponentCommon common = getComponentCommon(obj);\n                    if (!isEmptyInputs(common)) {\n                        throw new InvalidTopologyException(\"May not declare inputs for a spout\");\n                    }\n                }\n            }\n        }\n\n        Map<String, Object> componentMap = allComponents(topology);\n        for (Object componentObj : componentMap.values()) {\n            Map<String, Object> conf = componentConf(componentObj);\n            ComponentCommon common = getComponentCommon(componentObj);\n            int parallelismHintNum = Thrift.getParallelismHint(common);\n            Integer taskNum = ObjectReader.getInt(conf.get(Config.TOPOLOGY_TASKS), 0);\n            if (taskNum > 0 && parallelismHintNum <= 0) {\n                throw new InvalidTopologyException(\"Number of executors must be greater than 0 when number of tasks is greater than 0\");\n            }\n        }\n    }",
                "code_after_change": "    public static void validateBasic(StormTopology topology) throws InvalidTopologyException {\n        validateIds(topology);\n\n        for (StormTopology._Fields field : Thrift.getSpoutFields()) {\n            Map<String, Object> spoutComponents = (Map<String, Object>) topology.getFieldValue(field);\n            if (spoutComponents != null) {\n                for (Object obj : spoutComponents.values()) {\n                    ComponentCommon common = getComponentCommon(obj);\n                    if (!isEmptyInputs(common)) {\n                        throw new WrappedInvalidTopologyException(\"May not declare inputs for a spout\");\n                    }\n                }\n            }\n        }\n\n        Map<String, Object> componentMap = allComponents(topology);\n        for (Object componentObj : componentMap.values()) {\n            Map<String, Object> conf = componentConf(componentObj);\n            ComponentCommon common = getComponentCommon(componentObj);\n            int parallelismHintNum = Thrift.getParallelismHint(common);\n            Integer taskNum = ObjectReader.getInt(conf.get(Config.TOPOLOGY_TASKS), 0);\n            if (taskNum > 0 && parallelismHintNum <= 0) {\n                throw new WrappedInvalidTopologyException(\n                        \"Number of executors must be greater than 0 when number of tasks is greater than 0\");\n            }\n        }\n    }"
            },
            "storm-server.src.main.java.org.apache.storm.blobstore.LocalFsBlobStore.createBlob": {
                "code_before_change": "    public AtomicOutputStream createBlob(String key, SettableBlobMeta meta, Subject who) throws AuthorizationException,\n        KeyAlreadyExistsException {\n        LOG.debug(\"Creating Blob for key {}\", key);\n        validateKey(key);\n        _aclHandler.normalizeSettableBlobMeta(key, meta, who, allPermissions);\n        BlobStoreAclHandler.validateSettableACLs(key, meta.get_acl());\n        _aclHandler.hasPermissions(meta.get_acl(), allPermissions, who, key);\n        if (fbs.exists(DATA_PREFIX + key)) {\n            throw new KeyAlreadyExistsException(key);\n        }\n        BlobStoreFileOutputStream mOut = null;\n        try {\n            mOut = new BlobStoreFileOutputStream(fbs.write(META_PREFIX + key, true));\n            mOut.write(Utils.thriftSerialize(meta));\n            mOut.close();\n            mOut = null;\n            this.stormClusterState.setupBlob(key, this.nimbusInfo, getVersionForKey(key, this.nimbusInfo, zkClient));\n            return new BlobStoreFileOutputStream(fbs.write(DATA_PREFIX+key, true));\n        } catch (IOException e) {\n            throw new RuntimeException(e);\n        } catch (KeyNotFoundException e) {\n            throw new RuntimeException(e);\n        } finally {\n            if (mOut != null) {\n                try {\n                    mOut.cancel();\n                } catch (IOException e) {\n                    //Ignored\n                }\n            }\n        }\n    }",
                "code_after_change": "    public AtomicOutputStream createBlob(String key, SettableBlobMeta meta, Subject who) throws AuthorizationException,\n        KeyAlreadyExistsException {\n        LOG.debug(\"Creating Blob for key {}\", key);\n        validateKey(key);\n        _aclHandler.normalizeSettableBlobMeta(key, meta, who, allPermissions);\n        BlobStoreAclHandler.validateSettableACLs(key, meta.get_acl());\n        _aclHandler.hasPermissions(meta.get_acl(), allPermissions, who, key);\n        if (fbs.exists(DATA_PREFIX + key)) {\n            throw new WrappedKeyAlreadyExistsException(key);\n        }\n        BlobStoreFileOutputStream mOut = null;\n        try {\n            mOut = new BlobStoreFileOutputStream(fbs.write(META_PREFIX + key, true));\n            mOut.write(Utils.thriftSerialize(meta));\n            mOut.close();\n            mOut = null;\n            this.stormClusterState.setupBlob(key, this.nimbusInfo, getVersionForKey(key, this.nimbusInfo, zkClient));\n            return new BlobStoreFileOutputStream(fbs.write(DATA_PREFIX + key, true));\n        } catch (IOException e) {\n            throw new RuntimeException(e);\n        } catch (KeyNotFoundException e) {\n            throw new RuntimeException(e);\n        } finally {\n            if (mOut != null) {\n                try {\n                    mOut.cancel();\n                } catch (IOException e) {\n                    //Ignored\n                }\n            }\n        }\n    }"
            },
            "storm-server.src.main.java.org.apache.storm.daemon.nimbus.Nimbus.getOwnerResourceSummaries": {
                "code_before_change": "    public List<OwnerResourceSummary> getOwnerResourceSummaries(String owner) throws AuthorizationException, TException {\n        try {\n            getOwnerResourceSummariesCalls.mark();\n            checkAuthorization(null, null, \"getOwnerResourceSummaries\");\n            IStormClusterState state = stormClusterState;\n            Map<String, Assignment> topoIdToAssignments = state.assignmentsInfo();\n            Map<String, StormBase> topoIdToBases = state.topologyBases();\n            Map<String, Number> clusterSchedulerConfig = scheduler.config();\n\n            //put [owner-> StormBase-list] mapping to ownerToBasesMap\n            //if this owner (the input parameter) is null, add all the owners with stormbase and guarantees\n            //else, add only this owner (the input paramter) to the map\n            Map<String, List<StormBase>> ownerToBasesMap = new HashMap<>();\n\n            if (owner == null) {\n                // add all the owners to the map\n                for (StormBase base : topoIdToBases.values()) {\n                    String baseOwner = base.get_owner();\n                    if (!ownerToBasesMap.containsKey(baseOwner)) {\n                        List<StormBase> stormbases = new ArrayList<>();\n                        stormbases.add(base);\n                        ownerToBasesMap.put(baseOwner, stormbases);\n                    } else {\n                        ownerToBasesMap.get(baseOwner).add(base);\n                    }\n                }\n                //in addition, add all the owners with guarantees\n                List<String> ownersWithGuarantees = new ArrayList<>(clusterSchedulerConfig.keySet());\n                for (String ownerWithGuarantees : ownersWithGuarantees) {\n                    if (!ownerToBasesMap.containsKey(ownerWithGuarantees)) {\n                        ownerToBasesMap.put(ownerWithGuarantees, new ArrayList<>());\n                    }\n                }\n            } else {\n                //only put this owner to the map\n                List<StormBase> stormbases = new ArrayList<>();\n                for (StormBase base : topoIdToBases.values()) {\n                    if (owner.equals(base.get_owner())) {\n                        stormbases.add(base);\n                    }\n                }\n                ownerToBasesMap.put(owner, stormbases);\n            }\n\n            List<OwnerResourceSummary> ret = new ArrayList<>();\n\n            //for each owner, get resources, configs, and aggregate\n            for (Entry<String, List<StormBase>> ownerToBasesEntry : ownerToBasesMap.entrySet()) {\n                String theOwner = ownerToBasesEntry.getKey();\n                TopologyResources totalResourcesAggregate = new TopologyResources();\n\n                int totalExecutors = 0;\n                int totalWorkers = 0;\n                int totalTasks = 0;\n\n                for (StormBase base : ownerToBasesEntry.getValue()) {\n                    try {\n                        String topoId = state.getTopoId(base.get_name())\n                                             .orElseThrow(() -> new NotAliveException(base.get_name() + \" is not alive\"));\n                        TopologyResources resources = getResourcesForTopology(topoId, base);\n                        totalResourcesAggregate = totalResourcesAggregate.add(resources);\n                        Assignment ownerAssignment = topoIdToAssignments.get(topoId);\n                        if (ownerAssignment != null && ownerAssignment.get_executor_node_port() != null) {\n                            totalExecutors += ownerAssignment.get_executor_node_port().keySet().size();\n                            totalWorkers += new HashSet(ownerAssignment.get_executor_node_port().values()).size();\n                            for (List<Long> executorId : ownerAssignment.get_executor_node_port().keySet()) {\n                                totalTasks += StormCommon.executorIdToTasks(executorId).size();\n                            }\n                        }\n                    } catch (NotAliveException e) {\n                        LOG.warn(\"{} is not alive.\", base.get_name());\n                    }\n                }\n\n                double requestedTotalMemory = totalResourcesAggregate.getRequestedMemOnHeap()\n                                              + totalResourcesAggregate.getRequestedMemOffHeap();\n                double assignedTotalMemory = totalResourcesAggregate.getAssignedMemOnHeap()\n                                             + totalResourcesAggregate.getAssignedMemOffHeap();\n\n                OwnerResourceSummary ownerResourceSummary = new OwnerResourceSummary(theOwner);\n                ownerResourceSummary.set_total_topologies(ownerToBasesEntry.getValue().size());\n                ownerResourceSummary.set_total_executors(totalExecutors);\n                ownerResourceSummary.set_total_workers(totalWorkers);\n                ownerResourceSummary.set_total_tasks(totalTasks);\n                ownerResourceSummary.set_memory_usage(assignedTotalMemory);\n                ownerResourceSummary.set_cpu_usage(totalResourcesAggregate.getAssignedCpu());\n                ownerResourceSummary.set_requested_on_heap_memory(totalResourcesAggregate.getRequestedMemOnHeap());\n                ownerResourceSummary.set_requested_off_heap_memory(totalResourcesAggregate.getRequestedMemOffHeap());\n                ownerResourceSummary.set_requested_total_memory(requestedTotalMemory);\n                ownerResourceSummary.set_requested_cpu(totalResourcesAggregate.getRequestedCpu());\n                ownerResourceSummary.set_assigned_on_heap_memory(totalResourcesAggregate.getAssignedMemOnHeap());\n                ownerResourceSummary.set_assigned_off_heap_memory(totalResourcesAggregate.getAssignedMemOffHeap());\n\n                if (clusterSchedulerConfig.containsKey(theOwner)) {\n                    if (underlyingScheduler instanceof ResourceAwareScheduler) {\n                        Map<String, Object> schedulerConfig = (Map) clusterSchedulerConfig.get(theOwner);\n                        if (schedulerConfig != null) {\n                            ownerResourceSummary.set_memory_guarantee((double) schedulerConfig.getOrDefault(\"memory\", 0));\n                            ownerResourceSummary.set_cpu_guarantee((double) schedulerConfig.getOrDefault(\"cpu\", 0));\n                            ownerResourceSummary.set_memory_guarantee_remaining(ownerResourceSummary.get_memory_guarantee()\n                                                                                - ownerResourceSummary.get_memory_usage());\n                            ownerResourceSummary.set_cpu_guarantee_remaining(ownerResourceSummary.get_cpu_guarantee()\n                                                                             - ownerResourceSummary.get_cpu_usage());\n                        }\n                    } else if (underlyingScheduler instanceof MultitenantScheduler) {\n                        ownerResourceSummary.set_isolated_node_guarantee((int) clusterSchedulerConfig.getOrDefault(theOwner, 0));\n                    }\n                }\n\n                LOG.debug(\"{}\", ownerResourceSummary.toString());\n                ret.add(ownerResourceSummary);\n            }\n\n            return ret;\n        } catch (Exception e) {\n            LOG.warn(\"Get owner resource summaries exception. (owner = '{}')\", owner);\n            if (e instanceof TException) {\n                throw (TException) e;\n            }\n            throw new RuntimeException(e);\n        }\n    }",
                "code_after_change": "    public List<OwnerResourceSummary> getOwnerResourceSummaries(String owner) throws AuthorizationException, TException {\n        try {\n            getOwnerResourceSummariesCalls.mark();\n            checkAuthorization(null, null, \"getOwnerResourceSummaries\");\n            IStormClusterState state = stormClusterState;\n            Map<String, Assignment> topoIdToAssignments = state.assignmentsInfo();\n            Map<String, StormBase> topoIdToBases = state.topologyBases();\n            Map<String, Number> clusterSchedulerConfig = scheduler.config();\n\n            //put [owner-> StormBase-list] mapping to ownerToBasesMap\n            //if this owner (the input parameter) is null, add all the owners with stormbase and guarantees\n            //else, add only this owner (the input paramter) to the map\n            Map<String, List<StormBase>> ownerToBasesMap = new HashMap<>();\n\n            if (owner == null) {\n                // add all the owners to the map\n                for (StormBase base : topoIdToBases.values()) {\n                    String baseOwner = base.get_owner();\n                    if (!ownerToBasesMap.containsKey(baseOwner)) {\n                        List<StormBase> stormbases = new ArrayList<>();\n                        stormbases.add(base);\n                        ownerToBasesMap.put(baseOwner, stormbases);\n                    } else {\n                        ownerToBasesMap.get(baseOwner).add(base);\n                    }\n                }\n                //in addition, add all the owners with guarantees\n                List<String> ownersWithGuarantees = new ArrayList<>(clusterSchedulerConfig.keySet());\n                for (String ownerWithGuarantees : ownersWithGuarantees) {\n                    if (!ownerToBasesMap.containsKey(ownerWithGuarantees)) {\n                        ownerToBasesMap.put(ownerWithGuarantees, new ArrayList<>());\n                    }\n                }\n            } else {\n                //only put this owner to the map\n                List<StormBase> stormbases = new ArrayList<>();\n                for (StormBase base : topoIdToBases.values()) {\n                    if (owner.equals(base.get_owner())) {\n                        stormbases.add(base);\n                    }\n                }\n                ownerToBasesMap.put(owner, stormbases);\n            }\n\n            List<OwnerResourceSummary> ret = new ArrayList<>();\n\n            //for each owner, get resources, configs, and aggregate\n            for (Entry<String, List<StormBase>> ownerToBasesEntry : ownerToBasesMap.entrySet()) {\n                String theOwner = ownerToBasesEntry.getKey();\n                TopologyResources totalResourcesAggregate = new TopologyResources();\n\n                int totalExecutors = 0;\n                int totalWorkers = 0;\n                int totalTasks = 0;\n\n                for (StormBase base : ownerToBasesEntry.getValue()) {\n                    try {\n                        String topoId = state.getTopoId(base.get_name())\n                                             .orElseThrow(() -> new WrappedNotAliveException(base.get_name() + \" is not alive\"));\n                        TopologyResources resources = getResourcesForTopology(topoId, base);\n                        totalResourcesAggregate = totalResourcesAggregate.add(resources);\n                        Assignment ownerAssignment = topoIdToAssignments.get(topoId);\n                        if (ownerAssignment != null && ownerAssignment.get_executor_node_port() != null) {\n                            totalExecutors += ownerAssignment.get_executor_node_port().keySet().size();\n                            totalWorkers += new HashSet(ownerAssignment.get_executor_node_port().values()).size();\n                            for (List<Long> executorId : ownerAssignment.get_executor_node_port().keySet()) {\n                                totalTasks += StormCommon.executorIdToTasks(executorId).size();\n                            }\n                        }\n                    } catch (NotAliveException e) {\n                        LOG.warn(\"{} is not alive.\", base.get_name());\n                    }\n                }\n\n                double requestedTotalMemory = totalResourcesAggregate.getRequestedMemOnHeap()\n                                              + totalResourcesAggregate.getRequestedMemOffHeap();\n                double assignedTotalMemory = totalResourcesAggregate.getAssignedMemOnHeap()\n                                             + totalResourcesAggregate.getAssignedMemOffHeap();\n\n                OwnerResourceSummary ownerResourceSummary = new OwnerResourceSummary(theOwner);\n                ownerResourceSummary.set_total_topologies(ownerToBasesEntry.getValue().size());\n                ownerResourceSummary.set_total_executors(totalExecutors);\n                ownerResourceSummary.set_total_workers(totalWorkers);\n                ownerResourceSummary.set_total_tasks(totalTasks);\n                ownerResourceSummary.set_memory_usage(assignedTotalMemory);\n                ownerResourceSummary.set_cpu_usage(totalResourcesAggregate.getAssignedCpu());\n                ownerResourceSummary.set_requested_on_heap_memory(totalResourcesAggregate.getRequestedMemOnHeap());\n                ownerResourceSummary.set_requested_off_heap_memory(totalResourcesAggregate.getRequestedMemOffHeap());\n                ownerResourceSummary.set_requested_total_memory(requestedTotalMemory);\n                ownerResourceSummary.set_requested_cpu(totalResourcesAggregate.getRequestedCpu());\n                ownerResourceSummary.set_assigned_on_heap_memory(totalResourcesAggregate.getAssignedMemOnHeap());\n                ownerResourceSummary.set_assigned_off_heap_memory(totalResourcesAggregate.getAssignedMemOffHeap());\n\n                if (clusterSchedulerConfig.containsKey(theOwner)) {\n                    if (underlyingScheduler instanceof ResourceAwareScheduler) {\n                        Map<String, Object> schedulerConfig = (Map) clusterSchedulerConfig.get(theOwner);\n                        if (schedulerConfig != null) {\n                            ownerResourceSummary.set_memory_guarantee((double) schedulerConfig.getOrDefault(\"memory\", 0));\n                            ownerResourceSummary.set_cpu_guarantee((double) schedulerConfig.getOrDefault(\"cpu\", 0));\n                            ownerResourceSummary.set_memory_guarantee_remaining(ownerResourceSummary.get_memory_guarantee()\n                                                                                - ownerResourceSummary.get_memory_usage());\n                            ownerResourceSummary.set_cpu_guarantee_remaining(ownerResourceSummary.get_cpu_guarantee()\n                                                                             - ownerResourceSummary.get_cpu_usage());\n                        }\n                    } else if (underlyingScheduler instanceof MultitenantScheduler) {\n                        ownerResourceSummary.set_isolated_node_guarantee((int) clusterSchedulerConfig.getOrDefault(theOwner, 0));\n                    }\n                }\n\n                LOG.debug(\"{}\", ownerResourceSummary.toString());\n                ret.add(ownerResourceSummary);\n            }\n\n            return ret;\n        } catch (Exception e) {\n            LOG.warn(\"Get owner resource summaries exception. (owner = '{}')\", owner);\n            if (e instanceof TException) {\n                throw (TException) e;\n            }\n            throw new RuntimeException(e);\n        }\n    }"
            },
            "storm-server.src.main.java.org.apache.storm.blobstore.BlobStoreUtils.getNimbodesWithLatestSequenceNumberOfBlob": {
                "code_before_change": "    public static Set<NimbusInfo> getNimbodesWithLatestSequenceNumberOfBlob(CuratorFramework zkClient, String key) throws Exception {\n        List<String> stateInfoList;\n        try {\n            stateInfoList = zkClient.getChildren().forPath(\"/blobstore/\" + key);\n        } catch (KeeperException.NoNodeException e) {\n            // there's a race condition with a delete: blobstore\n            // this should be thrown to the caller to indicate that the key is invalid now\n            throw new KeyNotFoundException(key);\n        }\n\n        Set<NimbusInfo> nimbusInfoSet = new HashSet<NimbusInfo>();\n        int latestSeqNumber = getLatestSequenceNumber(stateInfoList);\n        LOG.debug(\"getNimbodesWithLatestSequenceNumberOfBlob stateInfo {} version {}\", stateInfoList, latestSeqNumber);\n        // Get the nimbodes with the latest version\n        for (String state : stateInfoList) {\n            BlobKeySequenceInfo sequenceInfo = normalizeNimbusHostPortSequenceNumberInfo(state);\n            if (latestSeqNumber == Integer.parseInt(sequenceInfo.getSequenceNumber())) {\n                nimbusInfoSet.add(NimbusInfo.parse(sequenceInfo.getNimbusHostPort()));\n            }\n        }\n        LOG.debug(\"nimbusInfoList {}\", nimbusInfoSet);\n        return nimbusInfoSet;\n    }",
                "code_after_change": "    public static Set<NimbusInfo> getNimbodesWithLatestSequenceNumberOfBlob(CuratorFramework zkClient, String key) throws Exception {\n        List<String> stateInfoList;\n        try {\n            stateInfoList = zkClient.getChildren().forPath(\"/blobstore/\" + key);\n        } catch (KeeperException.NoNodeException e) {\n            // there's a race condition with a delete: blobstore\n            // this should be thrown to the caller to indicate that the key is invalid now\n            throw new WrappedKeyNotFoundException(key);\n        }\n\n        Set<NimbusInfo> nimbusInfoSet = new HashSet<NimbusInfo>();\n        int latestSeqNumber = getLatestSequenceNumber(stateInfoList);\n        LOG.debug(\"getNimbodesWithLatestSequenceNumberOfBlob stateInfo {} version {}\", stateInfoList, latestSeqNumber);\n        // Get the nimbodes with the latest version\n        for (String state : stateInfoList) {\n            BlobKeySequenceInfo sequenceInfo = normalizeNimbusHostPortSequenceNumberInfo(state);\n            if (latestSeqNumber == Integer.parseInt(sequenceInfo.getSequenceNumber())) {\n                nimbusInfoSet.add(NimbusInfo.parse(sequenceInfo.getNimbusHostPort()));\n            }\n        }\n        LOG.debug(\"nimbusInfoList {}\", nimbusInfoSet);\n        return nimbusInfoSet;\n    }"
            },
            "storm-server.src.main.java.org.apache.storm.daemon.nimbus.Nimbus.uploadNewCredentials": {
                "code_before_change": "    public void uploadNewCredentials(String topoName, Credentials credentials)\n        throws NotAliveException, InvalidTopologyException, AuthorizationException, TException {\n        try {\n            uploadNewCredentialsCalls.mark();\n            IStormClusterState state = stormClusterState;\n            String topoId = toTopoId(topoName);\n            if (topoId == null) {\n                throw new NotAliveException(topoName + \" is not alive\");\n            }\n            Map<String, Object> topoConf = tryReadTopoConf(topoId, topoCache);\n            topoConf = Utils.merge(conf, topoConf);\n            if (credentials == null) {\n                credentials = new Credentials(Collections.emptyMap());\n            }\n            checkAuthorization(topoName, topoConf, \"uploadNewCredentials\");\n            synchronized (credUpdateLock) {\n                //Merge the old credentials so creds nimbus created are not lost.\n                // And in case the user forgot to upload something important this time.\n                Credentials origCreds = state.credentials(topoId, null);\n                if (origCreds != null) {\n                    Map<String, String> mergedCreds = origCreds.get_creds();\n                    mergedCreds.putAll(credentials.get_creds());\n                    credentials.set_creds(mergedCreds);\n                }\n                state.setCredentials(topoId, credentials, topoConf);\n            }\n        } catch (Exception e) {\n            LOG.warn(\"Upload Creds topology exception. (topology name='{}')\", topoName, e);\n            if (e instanceof TException) {\n                throw (TException) e;\n            }\n            throw new RuntimeException(e);\n        }\n    }",
                "code_after_change": "    public void uploadNewCredentials(String topoName, Credentials credentials)\n        throws NotAliveException, InvalidTopologyException, AuthorizationException, TException {\n        try {\n            uploadNewCredentialsCalls.mark();\n            IStormClusterState state = stormClusterState;\n            String topoId = toTopoId(topoName);\n            if (topoId == null) {\n                throw new WrappedNotAliveException(topoName + \" is not alive\");\n            }\n            Map<String, Object> topoConf = tryReadTopoConf(topoId, topoCache);\n            topoConf = Utils.merge(conf, topoConf);\n            if (credentials == null) {\n                credentials = new Credentials(Collections.emptyMap());\n            }\n            checkAuthorization(topoName, topoConf, \"uploadNewCredentials\");\n            synchronized (credUpdateLock) {\n                //Merge the old credentials so creds nimbus created are not lost.\n                // And in case the user forgot to upload something important this time.\n                Credentials origCreds = state.credentials(topoId, null);\n                if (origCreds != null) {\n                    Map<String, String> mergedCreds = origCreds.get_creds();\n                    mergedCreds.putAll(credentials.get_creds());\n                    credentials.set_creds(mergedCreds);\n                }\n                state.setCredentials(topoId, credentials, topoConf);\n            }\n        } catch (Exception e) {\n            LOG.warn(\"Upload Creds topology exception. (topology name='{}')\", topoName, e);\n            if (e instanceof TException) {\n                throw (TException) e;\n            }\n            throw new RuntimeException(e);\n        }\n    }"
            },
            "storm-server.src.main.java.org.apache.storm.localizer.AsyncLocalizer.getClientBlobStore": {
                "code_before_change": "    protected ClientBlobStore getClientBlobStore() {\n        return ServerUtils.getClientBlobStoreForSupervisor(conf);\n    }",
                "code_after_change": "    protected ClientBlobStore getClientBlobStore() {\n        return ServerUtils.getClientBlobStoreForSupervisor(conf);\n    }"
            },
            "storm-server.src.main.java.org.apache.storm.blobstore.KeySequenceNumber.getKeySequenceNumber": {
                "code_before_change": "    public synchronized int getKeySequenceNumber(CuratorFramework zkClient) throws KeyNotFoundException {\n        TreeSet<Integer> sequenceNumbers = new TreeSet<Integer>();\n        try {\n            // Key has not been created yet and it is the first time it is being created\n            if (zkClient.checkExists().forPath(BlobStoreUtils.getBlobStoreSubtree() + \"/\" + key) == null) {\n                zkClient.create().creatingParentsIfNeeded().withMode(CreateMode.PERSISTENT)\n                        .withACL(ZooDefs.Ids.OPEN_ACL_UNSAFE).forPath(BLOBSTORE_MAX_KEY_SEQUENCE_SUBTREE + \"/\" + key);\n                zkClient.setData().forPath(BLOBSTORE_MAX_KEY_SEQUENCE_SUBTREE + \"/\" + key,\n                                           ByteBuffer.allocate(INT_CAPACITY).putInt(INITIAL_SEQUENCE_NUMBER).array());\n                return INITIAL_SEQUENCE_NUMBER;\n            }\n\n            // When all nimbodes go down and one or few of them come up\n            // Unfortunately there might not be an exact way to know which one contains the most updated blob,\n            // if all go down which is unlikely. Hence there might be a need to update the blob if all go down.\n            List<String> stateInfoList = zkClient.getChildren().forPath(BlobStoreUtils.getBlobStoreSubtree() + \"/\" + key);\n            LOG.debug(\"stateInfoList-size {} stateInfoList-data {}\", stateInfoList.size(), stateInfoList);\n            if (stateInfoList.isEmpty()) {\n                return getMaxSequenceNumber(zkClient);\n            }\n\n            LOG.debug(\"stateInfoSize {}\", stateInfoList.size());\n            // In all other cases check for the latest update sequence of the blob on the nimbus\n            // and assign the appropriate number. Check if all are have same sequence number,\n            // if not assign the highest sequence number.\n            for (String stateInfo : stateInfoList) {\n                sequenceNumbers.add(Integer.parseInt(BlobStoreUtils.normalizeNimbusHostPortSequenceNumberInfo(stateInfo)\n                                                                   .getSequenceNumber()));\n            }\n\n            // Update scenario 2 and 3 explain the code logic written here\n            // especially when nimbus crashes and comes up after and before update\n            // respectively.\n            int currentSeqNumber = getMaxSequenceNumber(zkClient);\n            if (!checkIfStateContainsCurrentNimbusHost(stateInfoList, nimbusInfo) && !nimbusInfo.isLeader()) {\n                if (sequenceNumbers.last() < currentSeqNumber) {\n                    return currentSeqNumber;\n                } else {\n                    return INITIAL_SEQUENCE_NUMBER - 1;\n                }\n            }\n\n            // It covers scenarios expalined in scenario 3 when nimbus-1 holding the latest\n            // update goes down before it is downloaded by nimbus-2. Nimbus-2 gets elected as a leader\n            // after which nimbus-1 comes back up and a read or update is performed.\n            if (!checkIfStateContainsCurrentNimbusHost(stateInfoList, nimbusInfo) && nimbusInfo.isLeader()) {\n                incrementMaxSequenceNumber(zkClient, currentSeqNumber);\n                return currentSeqNumber + 1;\n            }\n\n            // This code logic covers the update scenarios in 2 when the nimbus-1 goes down\n            // before syncing the blob to nimbus-2 and an update happens.\n            // If seq-num for nimbus-2 is 2 and max-seq-number is 3 then next sequence number is 4\n            // (max-seq-number + 1).\n            // Other scenario it covers is when max-seq-number and nimbus seq number are equal.\n            if (sequenceNumbers.size() == 1) {\n                if (sequenceNumbers.first() < currentSeqNumber) {\n                    incrementMaxSequenceNumber(zkClient, currentSeqNumber);\n                    return currentSeqNumber + 1;\n                } else {\n                    incrementMaxSequenceNumber(zkClient, currentSeqNumber);\n                    return sequenceNumbers.first() + 1;\n                }\n            }\n\n            // Normal create update sync scenario returns the greatest sequence number in the set\n            return sequenceNumbers.last();\n        } catch (KeeperException.NoNodeException e) {\n            // there's a race condition with a delete: either blobstore or blobstoremaxsequence\n            // this should be thrown to the caller to indicate that the key is invalid now\n            throw new KeyNotFoundException(key);\n        } catch (Exception e) {\n            // in other case, just set this to 0 to trigger re-sync later\n            LOG.error(\"Exception {}\", e);\n            return INITIAL_SEQUENCE_NUMBER - 1;\n        }\n    }",
                "code_after_change": "    public synchronized int getKeySequenceNumber(CuratorFramework zkClient) throws KeyNotFoundException {\n        TreeSet<Integer> sequenceNumbers = new TreeSet<Integer>();\n        try {\n            // Key has not been created yet and it is the first time it is being created\n            if (zkClient.checkExists().forPath(BlobStoreUtils.getBlobStoreSubtree() + \"/\" + key) == null) {\n                zkClient.create().creatingParentsIfNeeded().withMode(CreateMode.PERSISTENT)\n                        .withACL(ZooDefs.Ids.OPEN_ACL_UNSAFE).forPath(BLOBSTORE_MAX_KEY_SEQUENCE_SUBTREE + \"/\" + key);\n                zkClient.setData().forPath(BLOBSTORE_MAX_KEY_SEQUENCE_SUBTREE + \"/\" + key,\n                                           ByteBuffer.allocate(INT_CAPACITY).putInt(INITIAL_SEQUENCE_NUMBER).array());\n                return INITIAL_SEQUENCE_NUMBER;\n            }\n\n            // When all nimbodes go down and one or few of them come up\n            // Unfortunately there might not be an exact way to know which one contains the most updated blob,\n            // if all go down which is unlikely. Hence there might be a need to update the blob if all go down.\n            List<String> stateInfoList = zkClient.getChildren().forPath(BlobStoreUtils.getBlobStoreSubtree() + \"/\" + key);\n            LOG.debug(\"stateInfoList-size {} stateInfoList-data {}\", stateInfoList.size(), stateInfoList);\n            if (stateInfoList.isEmpty()) {\n                return getMaxSequenceNumber(zkClient);\n            }\n\n            LOG.debug(\"stateInfoSize {}\", stateInfoList.size());\n            // In all other cases check for the latest update sequence of the blob on the nimbus\n            // and assign the appropriate number. Check if all are have same sequence number,\n            // if not assign the highest sequence number.\n            for (String stateInfo : stateInfoList) {\n                sequenceNumbers.add(Integer.parseInt(BlobStoreUtils.normalizeNimbusHostPortSequenceNumberInfo(stateInfo)\n                                                                   .getSequenceNumber()));\n            }\n\n            // Update scenario 2 and 3 explain the code logic written here\n            // especially when nimbus crashes and comes up after and before update\n            // respectively.\n            int currentSeqNumber = getMaxSequenceNumber(zkClient);\n            if (!checkIfStateContainsCurrentNimbusHost(stateInfoList, nimbusInfo) && !nimbusInfo.isLeader()) {\n                if (sequenceNumbers.last() < currentSeqNumber) {\n                    return currentSeqNumber;\n                } else {\n                    return INITIAL_SEQUENCE_NUMBER - 1;\n                }\n            }\n\n            // It covers scenarios expalined in scenario 3 when nimbus-1 holding the latest\n            // update goes down before it is downloaded by nimbus-2. Nimbus-2 gets elected as a leader\n            // after which nimbus-1 comes back up and a read or update is performed.\n            if (!checkIfStateContainsCurrentNimbusHost(stateInfoList, nimbusInfo) && nimbusInfo.isLeader()) {\n                incrementMaxSequenceNumber(zkClient, currentSeqNumber);\n                return currentSeqNumber + 1;\n            }\n\n            // This code logic covers the update scenarios in 2 when the nimbus-1 goes down\n            // before syncing the blob to nimbus-2 and an update happens.\n            // If seq-num for nimbus-2 is 2 and max-seq-number is 3 then next sequence number is 4\n            // (max-seq-number + 1).\n            // Other scenario it covers is when max-seq-number and nimbus seq number are equal.\n            if (sequenceNumbers.size() == 1) {\n                if (sequenceNumbers.first() < currentSeqNumber) {\n                    incrementMaxSequenceNumber(zkClient, currentSeqNumber);\n                    return currentSeqNumber + 1;\n                } else {\n                    incrementMaxSequenceNumber(zkClient, currentSeqNumber);\n                    return sequenceNumbers.first() + 1;\n                }\n            }\n\n            // Normal create update sync scenario returns the greatest sequence number in the set\n            return sequenceNumbers.last();\n        } catch (KeeperException.NoNodeException e) {\n            // there's a race condition with a delete: either blobstore or blobstoremaxsequence\n            // this should be thrown to the caller to indicate that the key is invalid now\n            throw new WrappedKeyNotFoundException(key);\n        } catch (Exception e) {\n            // in other case, just set this to 0 to trigger re-sync later\n            LOG.error(\"Exception {}\", e);\n            return INITIAL_SEQUENCE_NUMBER - 1;\n        }\n    }"
            },
            "storm-server.src.main.java.org.apache.storm.daemon.nimbus.Nimbus.assignmentsForNode": {
                "code_before_change": "    private static Map<String, Assignment> assignmentsForNode(Map<String, Assignment> assignmentMap, String nodeId) {\n        Map<String, Assignment> ret = new HashMap<>();\n        assignmentMap.entrySet().stream().filter(assignmentEntry -> assignmentEntry.getValue().get_node_host().keySet()\n                                                                                   .contains(nodeId))\n                     .forEach(assignmentEntry -> { ret.put(assignmentEntry.getKey(), assignmentEntry.getValue()); });\n\n        return ret;\n    }",
                "code_after_change": "    private static Map<String, Assignment> assignmentsForNode(Map<String, Assignment> assignmentMap, String nodeId) {\n        Map<String, Assignment> ret = new HashMap<>();\n        assignmentMap.entrySet().stream().filter(assignmentEntry -> assignmentEntry.getValue().get_node_host().keySet()\n                                                                                   .contains(nodeId))\n                     .forEach(assignmentEntry -> {\n                         ret.put(assignmentEntry.getKey(), assignmentEntry.getValue());\n                     });\n\n        return ret;\n    }"
            },
            "storm-server.src.main.java.org.apache.storm.blobstore.LocalFsBlobStore.blobSync": {
                "code_before_change": "    private void blobSync() throws Exception {\n        if (\"distributed\".equals(conf.get(Config.STORM_CLUSTER_MODE))) {\n            if (!this.leaderElector.isLeader()) {\n                IStormClusterState state = stormClusterState;\n                NimbusInfo nimbusInfo = this.nimbusInfo;\n                BlobStore store = this;\n                Set<String> allKeys = new HashSet<>();\n                for (Iterator<String> it = store.listKeys(); it.hasNext();) {\n                    allKeys.add(it.next());\n                }\n                Set<String> zkKeys = new HashSet<>(state.blobstore(() -> {\n                    try {\n                        this.blobSync();\n                    } catch(Exception e) {\n                        throw new RuntimeException(e);\n                    }\n                }));\n                LOG.debug(\"blob-sync blob-store-keys {} zookeeper-keys {}\", allKeys, zkKeys);\n                LocalFsBlobStoreSynchronizer sync = new LocalFsBlobStoreSynchronizer(store, conf);\n                sync.setNimbusInfo(nimbusInfo);\n                sync.setBlobStoreKeySet(allKeys);\n                sync.setZookeeperKeySet(zkKeys);\n                sync.setZkClient(zkClient);\n                sync.syncBlobs();\n            } //else not leader (NOOP)\n        } //else local (NOOP)\n    }",
                "code_after_change": "    private void blobSync() throws Exception {\n        if (\"distributed\".equals(conf.get(Config.STORM_CLUSTER_MODE))) {\n            if (!this.leaderElector.isLeader()) {\n                IStormClusterState state = stormClusterState;\n                NimbusInfo nimbusInfo = this.nimbusInfo;\n                BlobStore store = this;\n                Set<String> allKeys = new HashSet<>();\n                for (Iterator<String> it = store.listKeys(); it.hasNext();) {\n                    allKeys.add(it.next());\n                }\n                Set<String> zkKeys = new HashSet<>(state.blobstore(() -> {\n                    try {\n                        this.blobSync();\n                    } catch (Exception e) {\n                        throw new RuntimeException(e);\n                    }\n                }));\n                LOG.debug(\"blob-sync blob-store-keys {} zookeeper-keys {}\", allKeys, zkKeys);\n                LocalFsBlobStoreSynchronizer sync = new LocalFsBlobStoreSynchronizer(store, conf);\n                sync.setNimbusInfo(nimbusInfo);\n                sync.setBlobStoreKeySet(allKeys);\n                sync.setZookeeperKeySet(zkKeys);\n                sync.setZkClient(zkClient);\n                sync.syncBlobs();\n            } //else not leader (NOOP)\n        } //else local (NOOP)\n    }"
            },
            "storm-client.src.jvm.org.apache.storm.daemon.StormCommon.validateStructure": {
                "code_before_change": "    public static void validateStructure(StormTopology topology) throws InvalidTopologyException {\n        Map<String, Object> componentMap = allComponents(topology);\n        for (Map.Entry<String, Object> entry : componentMap.entrySet()) {\n            String componentId = entry.getKey();\n            ComponentCommon common = getComponentCommon(entry.getValue());\n            Map<GlobalStreamId, Grouping> inputs = common.get_inputs();\n            for (Map.Entry<GlobalStreamId, Grouping> input : inputs.entrySet()) {\n                String sourceStreamId = input.getKey().get_streamId();\n                String sourceComponentId = input.getKey().get_componentId();\n                if (!componentMap.keySet().contains(sourceComponentId)) {\n                    throw new InvalidTopologyException(\"Component: [\" + componentId +\n                                                       \"] subscribes from non-existent component [\" + sourceComponentId + \"]\");\n                }\n\n                ComponentCommon sourceComponent = getComponentCommon(componentMap.get(sourceComponentId));\n                if (!sourceComponent.get_streams().containsKey(sourceStreamId)) {\n                    throw new InvalidTopologyException(\"Component: [\" + componentId +\n                                                       \"] subscribes from non-existent stream: \" +\n                                                       \"[\" + sourceStreamId + \"] of component [\" + sourceComponentId + \"]\");\n                }\n\n                Grouping grouping = input.getValue();\n                if (Thrift.groupingType(grouping) == Grouping._Fields.FIELDS) {\n                    List<String> fields = new ArrayList<>(grouping.get_fields());\n                    Map<String, StreamInfo> streams = sourceComponent.get_streams();\n                    Set<String> sourceOutputFields = getStreamOutputFields(streams);\n                    fields.removeAll(sourceOutputFields);\n                    if (fields.size() != 0) {\n                        throw new InvalidTopologyException(\"Component: [\" + componentId +\n                                                           \"] subscribes from stream: [\" + sourceStreamId + \"] of component \" +\n                                                           \"[\" + sourceComponentId + \"] + with non-existent fields: \" + fields);\n                    }\n                }\n            }\n        }\n    }",
                "code_after_change": "    public static void validateStructure(StormTopology topology) throws InvalidTopologyException {\n        Map<String, Object> componentMap = allComponents(topology);\n        for (Map.Entry<String, Object> entry : componentMap.entrySet()) {\n            String componentId = entry.getKey();\n            ComponentCommon common = getComponentCommon(entry.getValue());\n            Map<GlobalStreamId, Grouping> inputs = common.get_inputs();\n            for (Map.Entry<GlobalStreamId, Grouping> input : inputs.entrySet()) {\n                String sourceStreamId = input.getKey().get_streamId();\n                String sourceComponentId = input.getKey().get_componentId();\n                if (!componentMap.keySet().contains(sourceComponentId)) {\n                    throw new WrappedInvalidTopologyException(\"Component: [\" + componentId\n                                                       + \"] subscribes from non-existent component [\" + sourceComponentId + \"]\");\n                }\n\n                ComponentCommon sourceComponent = getComponentCommon(componentMap.get(sourceComponentId));\n                if (!sourceComponent.get_streams().containsKey(sourceStreamId)) {\n                    throw new WrappedInvalidTopologyException(\"Component: [\" + componentId\n                                                       + \"] subscribes from non-existent stream: \"\n                                                       + \"[\" + sourceStreamId + \"] of component [\" + sourceComponentId + \"]\");\n                }\n\n                Grouping grouping = input.getValue();\n                if (Thrift.groupingType(grouping) == Grouping._Fields.FIELDS) {\n                    List<String> fields = new ArrayList<>(grouping.get_fields());\n                    Map<String, StreamInfo> streams = sourceComponent.get_streams();\n                    Set<String> sourceOutputFields = getStreamOutputFields(streams);\n                    fields.removeAll(sourceOutputFields);\n                    if (fields.size() != 0) {\n                        throw new WrappedInvalidTopologyException(\"Component: [\" + componentId\n                                                           + \"] subscribes from stream: [\" + sourceStreamId + \"] of component \"\n                                                           + \"[\" + sourceComponentId + \"] + with non-existent fields: \" + fields);\n                    }\n                }\n            }\n        }\n    }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Buggy Method"
            },
            "fix_suggestion": "Alternative Fix",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Buggy Method"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the method 'LocalFsBlobStore.getStoredBlobMeta' as the location where the KeyNotFoundException is thrown, which is indeed a method where the error occurs, but not where the actual fix was made. The actual fix was made in 'HdfsBlobStore.getStoredBlobMeta'. Therefore, the root cause identification and problem location identification are classified as 'Partial' with the sub-category 'Buggy Method'. The fix suggestion in the bug report suggests enhancing the exception message, which is an alternative approach to the developer's fix of wrapping the exception. There is no wrong information in the bug report as it accurately describes the issue with the exception message."
        }
    },
    {
        "filename": "STORM-3096.json",
        "code_diff": {
            "storm-server.src.main.java.org.apache.storm.daemon.nimbus.Nimbus.topoIdsToClean": {
                "code_before_change": "    public static Set<String> topoIdsToClean(IStormClusterState state, BlobStore store, Map<String, Object> conf) {\n        Set<String> ret = new HashSet<>();\n        ret.addAll(Utils.OR(state.heartbeatStorms(), EMPTY_STRING_LIST));\n        ret.addAll(Utils.OR(state.errorTopologies(), EMPTY_STRING_LIST));\n        ret.addAll(Utils.OR(getExpiredTopologyIds(store, conf), EMPTY_STRING_SET));\n        ret.addAll(Utils.OR(state.backpressureTopologies(), EMPTY_STRING_LIST));\n        ret.addAll(Utils.OR(state.idsOfTopologiesWithPrivateWorkerKeys(), EMPTY_STRING_SET));\n        ret.removeAll(Utils.OR(state.activeStorms(), EMPTY_STRING_LIST));\n        return ret;\n    }",
                "code_after_change": "    public static Set<String> topoIdsToClean(IStormClusterState state, BlobStore store, Map<String, Object> conf) {\n        Set<String> ret = new HashSet<>();\n        ret.addAll(Utils.OR(state.heartbeatStorms(), EMPTY_STRING_LIST));\n        ret.addAll(Utils.OR(state.errorTopologies(), EMPTY_STRING_LIST));\n        ret.addAll(Utils.OR(store.storedTopoIds(), EMPTY_STRING_SET));\n        ret.addAll(Utils.OR(state.backpressureTopologies(), EMPTY_STRING_LIST));\n        ret.addAll(Utils.OR(state.idsOfTopologiesWithPrivateWorkerKeys(), EMPTY_STRING_SET));\n        ret = getExpiredTopologyIds(ret, conf);\n        ret.removeAll(Utils.OR(state.activeStorms(), EMPTY_STRING_LIST));\n        return ret;\n    }"
            },
            "storm-server.src.main.java.org.apache.storm.daemon.nimbus.Nimbus.getTopologyCleanupDetectedTime": {
                "code_before_change": "    private static long getTopologyCleanupDetectedTime(String topologyId) {\n        Long firstDetectedForDeletion = topologyCleanupDetected.get(topologyId);\n        if (firstDetectedForDeletion == null) {\n            firstDetectedForDeletion = Time.currentTimeMillis();\n            topologyCleanupDetected.put(topologyId, firstDetectedForDeletion);\n        }\n        return firstDetectedForDeletion;\n    }",
                "code_after_change": "    private static long getTopologyCleanupDetectedTime(String topologyId) {\n        Long firstDetectedForDeletion = topologyCleanupDetected.get(topologyId);\n        if (firstDetectedForDeletion == null) {\n            firstDetectedForDeletion = Time.currentTimeMillis();\n            topologyCleanupDetected.put(topologyId, firstDetectedForDeletion);\n        }\n        return firstDetectedForDeletion;\n    }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Direct Caller/Callee"
            },
            "fix_suggestion": "Alternative Fix",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Direct Caller/Callee"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the root cause as the cleanup process (doCleanup) being executed prematurely, which is a direct caller of the ground truth method 'Nimbus.topoIdsToClean'. This makes it a 'Partial' identification with a 'Direct Caller/Callee' sub-category. The fix suggestion involves modifying the timing of the cleanup process, which is an alternative approach to the developer's fix but addresses the same issue. The problem location mentions 'Nimbus.doCleanup', which is a direct caller of the ground truth method, thus 'Partial' with 'Direct Caller/Callee'. There is no wrong information in the report as all details are relevant to the bug context."
        }
    },
    {
        "filename": "STORM-1642.json",
        "code_diff": {
            "storm-client.src.jvm.org.apache.storm.utils.Utils.isZkAuthenticationConfiguredTopology": {
                "code_before_change": [],
                "code_after_change": "    public static boolean isZkAuthenticationConfiguredTopology(Map<String, Object> conf) {\n        return (conf != null\n                && conf.get(Config.STORM_ZOOKEEPER_TOPOLOGY_AUTH_SCHEME) != null\n                && !((String)conf.get(Config.STORM_ZOOKEEPER_TOPOLOGY_AUTH_SCHEME)).isEmpty());\n    }"
            },
            "storm-client.src.jvm.org.apache.storm.messaging.netty.StormServerHandler.messageReceived": {
                "code_before_change": [],
                "code_after_change": "    public void messageReceived(ChannelHandlerContext ctx, MessageEvent e) {\n      Object msgs = e.getMessage();\n      if (msgs == null) {\n        return;\n      }\n      \n      try {\n        server.received(msgs, e.getRemoteAddress().toString(), channel);\n      } catch (InterruptedException e1) {\n        LOG.info(\"failed to enqueue a request message\", e);\n        failure_count.incrementAndGet();\n      }\n    }"
            },
            "storm-client.src.jvm.org.apache.storm.utils.Utils.handleUncaughtException": {
                "code_before_change": [],
                "code_after_change": "    public static void handleUncaughtException(Throwable t) {\n        handleUncaughtException(t, defaultAllowedExceptions);\n    }"
            },
            "storm-client.src.jvm.org.apache.storm.messaging.netty.StormServerHandler.exceptionCaught": {
                "code_before_change": [],
                "code_after_change": "    public void exceptionCaught(ChannelHandlerContext ctx, ExceptionEvent e) {\n        try {\n            LOG.error(\"server errors in handling the request\", e.getCause());\n        } catch (Throwable err) {\n            // Doing nothing (probably due to an oom issue) and hoping Utils.handleUncaughtException will handle it\n        }\n        try {\n            Utils.handleUncaughtException(e.getCause(), allowedExceptions);\n        } catch (Error error) {\n            LOG.info(\"Received error in netty thread.. terminating server...\");\n            Runtime.getRuntime().exit(1);\n        }\n\n    }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Preventive",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the root cause as a null buffer being passed to the Input.setBuffer method, which is mentioned in the stack trace but not in the ground truth methods. Therefore, it is classified as 'Partial' with 'Shared Stack Trace Context'. The fix suggestion is 'Preventive' as it suggests checking for null and initializing the buffer, which would prevent the issue but does not match the developer's fix. The problem location is also 'Partial' with 'Shared Stack Trace Context' because it mentions methods in the stack trace but not the ground truth methods. There is no wrong information in the bug report as all details are relevant to the context of the bug."
        }
    },
    {
        "filename": "STORM-2700.json",
        "code_diff": {
            "storm-server.src.main.java.org.apache.storm.utils.ServerUtils.nimbusVersionOfBlob": {
                "code_before_change": "    public static long nimbusVersionOfBlob(String key, ClientBlobStore cb) throws AuthorizationException, KeyNotFoundException {\n        long nimbusBlobVersion = 0;\n        ReadableBlobMeta metadata = cb.getBlobMeta(key);\n        nimbusBlobVersion = metadata.get_version();\n        return nimbusBlobVersion;\n    }",
                "code_after_change": "    public static long nimbusVersionOfBlob(String key, ClientBlobStore cb) throws AuthorizationException, KeyNotFoundException {\n        long nimbusBlobVersion = 0;\n        ReadableBlobMeta metadata = cb.getBlobMeta(key);\n        nimbusBlobVersion = metadata.get_version();\n        return nimbusBlobVersion;\n    }"
            },
            "storm-server.src.main.java.org.apache.storm.localizer.Localizer.downloadBlob": {
                "code_before_change": "  private LocalizedResource downloadBlob(Map<String, Object> conf, String key, File localFile,\n      String user, boolean uncompress, boolean isUpdate)\n      throws AuthorizationException, KeyNotFoundException, IOException {\n    ClientBlobStore blobstore = null;\n    try {\n      blobstore = getClientBlobStore();\n      long nimbusBlobVersion = ServerUtils.nimbusVersionOfBlob(key, blobstore);\n      long oldVersion = ServerUtils.localVersionOfBlob(localFile.toString());\n      FileOutputStream out = null;\n      PrintWriter writer = null;\n      int numTries = 0;\n      String localizedPath = localFile.toString();\n      String localFileWithVersion = ServerUtils.constructBlobWithVersionFileName(localFile.toString(),\n              nimbusBlobVersion);\n      String localVersionFile = ServerUtils.constructVersionFileName(localFile.toString());\n      String downloadFile = localFileWithVersion;\n      if (uncompress) {\n        // we need to download to temp file and then unpack into the one requested\n        downloadFile = new File(localFile.getParent(), TO_UNCOMPRESS + localFile.getName()).toString();\n      }\n      while (numTries < _blobDownloadRetries) {\n        out = new FileOutputStream(downloadFile);\n        numTries++;\n        try {\n          if (!ServerUtils.canUserReadBlob(blobstore.getBlobMeta(key), user)) {\n            throw new AuthorizationException(user + \" does not have READ access to \" + key);\n          }\n          InputStreamWithMeta in = blobstore.getBlob(key);\n          byte[] buffer = new byte[1024];\n          int len;\n          while ((len = in.read(buffer)) >= 0) {\n            out.write(buffer, 0, len);\n          }\n          out.close();\n          in.close();\n          if (uncompress) {\n            ServerUtils.unpack(new File(downloadFile), new File(localFileWithVersion));\n            LOG.debug(\"uncompressed \" + downloadFile + \" to: \" + localFileWithVersion);\n          }\n\n          // Next write the version.\n          LOG.info(\"Blob: \" + key + \" updated with new Nimbus-provided version: \" +\n              nimbusBlobVersion + \" local version was: \" + oldVersion);\n          // The false parameter ensures overwriting the version file, not appending\n          writer = new PrintWriter(\n              new BufferedWriter(new FileWriter(localVersionFile, false)));\n          writer.println(nimbusBlobVersion);\n          writer.close();\n\n          try {\n            setBlobPermissions(conf, user, localFileWithVersion);\n            setBlobPermissions(conf, user, localVersionFile);\n\n            // Update the key.current symlink. First create tmp symlink and do\n            // move of tmp to current so that the operation is atomic.\n            String tmp_uuid_local = java.util.UUID.randomUUID().toString();\n            LOG.debug(\"Creating a symlink @\" + localFile + \".\" + tmp_uuid_local + \" , \" +\n                \"linking to: \" + localFile + \".\" + nimbusBlobVersion);\n            File uuid_symlink = new File(localFile + \".\" + tmp_uuid_local);\n\n            Files.createSymbolicLink(uuid_symlink.toPath(),\n                Paths.get(ServerUtils.constructBlobWithVersionFileName(localFile.toString(),\n                        nimbusBlobVersion)));\n            File current_symlink = new File(ServerUtils.constructBlobCurrentSymlinkName(\n                    localFile.toString()));\n            Files.move(uuid_symlink.toPath(), current_symlink.toPath(), ATOMIC_MOVE);\n          } catch (IOException e) {\n            // if we fail after writing the version file but before we move current link we need to\n            // restore the old version to the file\n            try {\n              PrintWriter restoreWriter = new PrintWriter(\n                  new BufferedWriter(new FileWriter(localVersionFile, false)));\n              restoreWriter.println(oldVersion);\n              restoreWriter.close();\n            } catch (IOException ignore) {}\n            throw e;\n          }\n\n          String oldBlobFile = localFile + \".\" + oldVersion;\n          try {\n            // Remove the old version. Note that if a number of processes have that file open,\n            // the OS will keep the old blob file around until they all close the handle and only\n            // then deletes it. No new process will open the old blob, since the users will open the\n            // blob through the \"blob.current\" symlink, which always points to the latest version of\n            // a blob. Remove the old version after the current symlink is updated as to not affect\n            // anyone trying to read it.\n            if ((oldVersion != -1) && (oldVersion != nimbusBlobVersion)) {\n              LOG.info(\"Removing an old blob file:\" + oldBlobFile);\n              Files.delete(Paths.get(oldBlobFile));\n            }\n          } catch (IOException e) {\n            // At this point we have downloaded everything and moved symlinks.  If the remove of\n            // old fails just log an error\n            LOG.error(\"Exception removing old blob version: \" + oldBlobFile);\n          }\n\n          break;\n        } catch (AuthorizationException ae) {\n          // we consider this non-retriable exceptions\n          if (out != null) {\n            out.close();\n          }\n          new File(downloadFile).delete();\n          throw ae;\n        } catch (IOException | KeyNotFoundException e) {\n          if (out != null) {\n            out.close();\n          }\n          if (writer != null) {\n            writer.close();\n          }\n          new File(downloadFile).delete();\n          if (uncompress) {\n            try {\n              FileUtils.deleteDirectory(new File(localFileWithVersion));\n            } catch (IOException ignore) {}\n          }\n          if (!isUpdate) {\n            // don't want to remove existing version file if its an update\n            new File(localVersionFile).delete();\n          }\n\n          if (numTries < _blobDownloadRetries) {\n            LOG.error(\"Failed to download blob, retrying\", e);\n          } else {\n            throw e;\n          }\n        }\n      }\n      return new LocalizedResource(key, localizedPath, uncompress);\n    } finally {\n      if(blobstore != null) {\n        blobstore.shutdown();\n      }\n    }\n  }",
                "code_after_change": "  private LocalizedResource downloadBlob(Map<String, Object> conf, String key, File localFile,\n      String user, boolean uncompress, boolean isUpdate)\n      throws AuthorizationException, KeyNotFoundException, IOException {\n    ClientBlobStore blobstore = null;\n    try {\n      blobstore = getClientBlobStore();\n      long nimbusBlobVersion = ServerUtils.nimbusVersionOfBlob(key, blobstore);\n      long oldVersion = ServerUtils.localVersionOfBlob(localFile.toString());\n      FileOutputStream out = null;\n      PrintWriter writer = null;\n      int numTries = 0;\n      String localizedPath = localFile.toString();\n      String localFileWithVersion = ServerUtils.constructBlobWithVersionFileName(localFile.toString(),\n              nimbusBlobVersion);\n      String localVersionFile = ServerUtils.constructVersionFileName(localFile.toString());\n      String downloadFile = localFileWithVersion;\n      if (uncompress) {\n        // we need to download to temp file and then unpack into the one requested\n        downloadFile = new File(localFile.getParent(), TO_UNCOMPRESS + localFile.getName()).toString();\n      }\n      while (numTries < _blobDownloadRetries) {\n        out = new FileOutputStream(downloadFile);\n        numTries++;\n        try {\n          if (!ServerUtils.canUserReadBlob(blobstore.getBlobMeta(key), user, conf)) {\n            throw new AuthorizationException(user + \" does not have READ access to \" + key);\n          }\n          InputStreamWithMeta in = blobstore.getBlob(key);\n          byte[] buffer = new byte[1024];\n          int len;\n          while ((len = in.read(buffer)) >= 0) {\n            out.write(buffer, 0, len);\n          }\n          out.close();\n          in.close();\n          if (uncompress) {\n            ServerUtils.unpack(new File(downloadFile), new File(localFileWithVersion));\n            LOG.debug(\"uncompressed \" + downloadFile + \" to: \" + localFileWithVersion);\n          }\n\n          // Next write the version.\n          LOG.info(\"Blob: \" + key + \" updated with new Nimbus-provided version: \" +\n              nimbusBlobVersion + \" local version was: \" + oldVersion);\n          // The false parameter ensures overwriting the version file, not appending\n          writer = new PrintWriter(\n              new BufferedWriter(new FileWriter(localVersionFile, false)));\n          writer.println(nimbusBlobVersion);\n          writer.close();\n\n          try {\n            setBlobPermissions(conf, user, localFileWithVersion);\n            setBlobPermissions(conf, user, localVersionFile);\n\n            // Update the key.current symlink. First create tmp symlink and do\n            // move of tmp to current so that the operation is atomic.\n            String tmp_uuid_local = java.util.UUID.randomUUID().toString();\n            LOG.debug(\"Creating a symlink @\" + localFile + \".\" + tmp_uuid_local + \" , \" +\n                \"linking to: \" + localFile + \".\" + nimbusBlobVersion);\n            File uuid_symlink = new File(localFile + \".\" + tmp_uuid_local);\n\n            Files.createSymbolicLink(uuid_symlink.toPath(),\n                Paths.get(ServerUtils.constructBlobWithVersionFileName(localFile.toString(),\n                        nimbusBlobVersion)));\n            File current_symlink = new File(ServerUtils.constructBlobCurrentSymlinkName(\n                    localFile.toString()));\n            Files.move(uuid_symlink.toPath(), current_symlink.toPath(), ATOMIC_MOVE);\n          } catch (IOException e) {\n            // if we fail after writing the version file but before we move current link we need to\n            // restore the old version to the file\n            try {\n              PrintWriter restoreWriter = new PrintWriter(\n                  new BufferedWriter(new FileWriter(localVersionFile, false)));\n              restoreWriter.println(oldVersion);\n              restoreWriter.close();\n            } catch (IOException ignore) {}\n            throw e;\n          }\n\n          String oldBlobFile = localFile + \".\" + oldVersion;\n          try {\n            // Remove the old version. Note that if a number of processes have that file open,\n            // the OS will keep the old blob file around until they all close the handle and only\n            // then deletes it. No new process will open the old blob, since the users will open the\n            // blob through the \"blob.current\" symlink, which always points to the latest version of\n            // a blob. Remove the old version after the current symlink is updated as to not affect\n            // anyone trying to read it.\n            if ((oldVersion != -1) && (oldVersion != nimbusBlobVersion)) {\n              LOG.info(\"Removing an old blob file:\" + oldBlobFile);\n              Files.delete(Paths.get(oldBlobFile));\n            }\n          } catch (IOException e) {\n            // At this point we have downloaded everything and moved symlinks.  If the remove of\n            // old fails just log an error\n            LOG.error(\"Exception removing old blob version: \" + oldBlobFile);\n          }\n\n          break;\n        } catch (AuthorizationException ae) {\n          // we consider this non-retriable exceptions\n          if (out != null) {\n            out.close();\n          }\n          new File(downloadFile).delete();\n          throw ae;\n        } catch (IOException | KeyNotFoundException e) {\n          if (out != null) {\n            out.close();\n          }\n          if (writer != null) {\n            writer.close();\n          }\n          new File(downloadFile).delete();\n          if (uncompress) {\n            try {\n              FileUtils.deleteDirectory(new File(localFileWithVersion));\n            } catch (IOException ignore) {}\n          }\n          if (!isUpdate) {\n            // don't want to remove existing version file if its an update\n            new File(localVersionFile).delete();\n          }\n\n          if (numTries < _blobDownloadRetries) {\n            LOG.error(\"Failed to download blob, retrying\", e);\n          } else {\n            throw e;\n          }\n        }\n      }\n      return new LocalizedResource(key, localizedPath, uncompress);\n    } finally {\n      if(blobstore != null) {\n        blobstore.shutdown();\n      }\n    }\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Correct",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report precisely identifies the root cause as the 'downloadBlob' method in the Localizer class, which is part of the ground truth methods. The fix suggestion in the bug report matches the developer's fix, which involves checking the 'storm.blobstore.acl.validation.enabled' configuration before performing ACL checks in the 'downloadBlob' method. The problem location is also precisely identified as it mentions the 'Localizer.downloadBlob' method, which is part of the ground truth. There is no wrong information in the bug report as all details are consistent with the context of the bug."
        }
    },
    {
        "filename": "STORM-1663.json",
        "code_diff": {
            "storm-core.src.jvm.org.apache.storm.stats.StatsUtil.extractDataFromHb": {
                "code_before_change": "    public static List<Map<String, Object>> extractDataFromHb(Map executor2hostPort, Map task2component,\n                                                              Map<List<Integer>, Map<String, Object>> beats,\n                                                              boolean includeSys, StormTopology topology) {\n        return extractDataFromHb(executor2hostPort, task2component, beats, includeSys, topology, null);\n    }",
                "code_after_change": "    public static List<Map<String, Object>> extractDataFromHb(Map executor2hostPort, Map task2component,\n                                                              Map<List<Integer>, Map<String, Object>> beats,\n                                                              boolean includeSys, StormTopology topology) {\n        return extractDataFromHb(executor2hostPort, task2component, beats, includeSys, topology, null);\n    }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Preventive",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the root cause as a transport layer failure when reading data from the Nimbus server, which is related to the stack trace context but not the exact ground truth method. The fix suggestion involves implementing error handling and retry mechanisms, which are preventive measures that could mitigate the issue. The problem location is identified in the AccessLoggingFilter methods, which are part of the shared stack trace context but not the ground truth method. There is no wrong information in the bug report as all details are relevant to the context of the bug."
        }
    },
    {
        "filename": "STORM-2518.json",
        "code_diff": {
            "storm-client.src.jvm.org.apache.storm.blobstore.BlobStoreAclHandler.normalizeSettableACLs": {
                "code_before_change": "    private final List<AccessControl> normalizeSettableACLs(String key, List<AccessControl> acls, Subject who,\n                                                            int opMask) {\n        List<AccessControl> cleanAcls = removeBadACLs(acls);\n        Set<String> userNames = getUserNamesFromSubject(who);\n        for (String user : userNames) {\n            fixACLsForUser(cleanAcls, user, opMask);\n        }\n        if ((who == null || userNames.isEmpty()) && !worldEverything(acls)) {\n            cleanAcls.addAll(BlobStoreAclHandler.WORLD_EVERYTHING);\n            LOG.debug(\"Access Control for key {} is normalized to world everything {}\", key, cleanAcls);\n            if (!acls.isEmpty())\n                LOG.warn(\"Access control for blob with key {} is normalized to WORLD_EVERYTHING\", key);\n        }\n        return cleanAcls;\n    }",
                "code_after_change": "    private final List<AccessControl> normalizeSettableACLs(String key, List<AccessControl> acls, Subject who,\n                                                            int opMask) {\n        List<AccessControl> cleanAcls = removeBadACLs(acls);\n        Set<String> userNames = getUserNamesFromSubject(who);\n        for (String user : userNames) {\n            fixACLsForUser(cleanAcls, user, opMask);\n        }\n        fixEmptyNameACLForUsers(cleanAcls, userNames, opMask);\n        if ((who == null || userNames.isEmpty()) && !worldEverything(acls)) {\n            cleanAcls.addAll(BlobStoreAclHandler.WORLD_EVERYTHING);\n            LOG.debug(\"Access Control for key {} is normalized to world everything {}\", key, cleanAcls);\n            if (!acls.isEmpty())\n                LOG.warn(\"Access control for blob with key {} is normalized to WORLD_EVERYTHING\", key);\n        }\n        return cleanAcls;\n    }"
            },
            "storm-client.src.jvm.org.apache.storm.blobstore.BlobStoreAclHandler.fixACLsForUser": {
                "code_before_change": "    private void fixACLsForUser(List<AccessControl> acls, String user, int mask) {\n        boolean foundUserACL = false;\n        for (AccessControl control : acls) {\n            if (control.get_type() == AccessControlType.USER && control.get_name().equals(user)) {\n                int currentAccess = control.get_access();\n                if ((currentAccess & mask) != mask) {\n                    control.set_access(currentAccess | mask);\n                }\n                foundUserACL = true;\n                break;\n            }\n        }\n        if (!foundUserACL) {\n            AccessControl userACL = new AccessControl();\n            userACL.set_type(AccessControlType.USER);\n            userACL.set_name(user);\n            userACL.set_access(mask);\n            acls.add(userACL);\n        }\n    }",
                "code_after_change": "    private void fixACLsForUser(List<AccessControl> acls, String user, int mask) {\n        boolean foundUserACL = false;\n        List<AccessControl> emptyUserACLs = new ArrayList<>();\n\n        for (AccessControl control : acls) {\n            if (control.get_type() == AccessControlType.USER) {\n                if (!control.is_set_name()) {\n                    emptyUserACLs.add(control);\n                } else if (control.get_name().equals(user)) {\n                    int currentAccess = control.get_access();\n                    if ((currentAccess & mask) != mask) {\n                        control.set_access(currentAccess | mask);\n                    }\n                    foundUserACL = true;\n                }\n            }\n        }\n\n        // if ACLs have two user ACLs for empty user and principal, discard empty user ACL\n        if (!emptyUserACLs.isEmpty() && foundUserACL) {\n            acls.removeAll(emptyUserACLs);\n        }\n\n        // add default user ACL when only empty user ACL is not present\n        if (emptyUserACLs.isEmpty() && !foundUserACL) {\n            AccessControl userACL = new AccessControl();\n            userACL.set_type(AccessControlType.USER);\n            userACL.set_name(user);\n            userACL.set_access(mask);\n            acls.add(userACL);\n        }\n    }"
            },
            "storm-client.src.jvm.org.apache.storm.blobstore.BlobStoreAclHandler.worldEverything": {
                "code_before_change": "    private boolean worldEverything(List<AccessControl> acls) {\n        boolean isWorldEverything = false;\n        for (AccessControl acl : acls) {\n            if (acl.get_type() == AccessControlType.OTHER && acl.get_access() == (READ|WRITE|ADMIN)) {\n                isWorldEverything = true;\n                break;\n            }\n        }\n        return isWorldEverything;\n    }",
                "code_after_change": "    private boolean worldEverything(List<AccessControl> acls) {\n        boolean isWorldEverything = false;\n        for (AccessControl acl : acls) {\n            if (acl.get_type() == AccessControlType.OTHER && acl.get_access() == (READ|WRITE|ADMIN)) {\n                isWorldEverything = true;\n                break;\n            }\n        }\n        return isWorldEverything;\n    }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Correct",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report precisely identifies the root cause of the NullPointerException in the 'fixACLsForUser' method, which is one of the ground truth methods. The report suggests implementing a null check for the 'name' field, which matches the developer's fix. The problem location is also precisely identified as it mentions the 'fixACLsForUser' method, which is part of the ground truth methods. There is no wrong information in the bug report as all details are consistent with the provided context and ground truth."
        }
    },
    {
        "filename": "STORM-3124.json",
        "code_diff": {
            "storm-client.src.jvm.org.apache.storm.pacemaker.PacemakerClient.secretKey": {
                "code_before_change": "    public String secretKey() {\n        return secret;\n    }",
                "code_after_change": "    public String secretKey() {\n        return secret;\n    }"
            },
            "storm-client.src.jvm.org.apache.storm.pacemaker.PacemakerClientPool.sendAll": {
                "code_before_change": "    public List<HBMessage> sendAll(HBMessage m) throws PacemakerConnectionException, InterruptedException {\n        List<HBMessage> responses = new ArrayList<HBMessage>();\n        LOG.debug(\"Using servers: {}\", servers);\n        for (String s : servers) {\n            try {\n                HBMessage response = getClientForServer(s).send(m);\n                responses.add(response);\n            } catch (PacemakerConnectionException e) {\n                LOG.warn(\"Failed to connect to the pacemaker server {}\", s);\n            }\n        }\n        if (responses.size() == 0) {\n            throw new PacemakerConnectionException(\"Failed to connect to any Pacemaker.\");\n        }\n        return responses;\n    }",
                "code_after_change": "    public List<HBMessage> sendAll(HBMessage m) throws PacemakerConnectionException, InterruptedException {\n        List<HBMessage> responses = new ArrayList<HBMessage>();\n        LOG.debug(\"Using servers: {}\", servers);\n        for (String s : servers) {\n            try {\n                HBMessage response = getClientForServer(s).send(m);\n                responses.add(response);\n            } catch (PacemakerConnectionException e) {\n                LOG.warn(\"Failed to connect to the pacemaker server {}, attempting to reconnect\", s);\n                getClientForServer(s).reconnect();\n            }\n        }\n        if (responses.size() == 0) {\n            throw new PacemakerConnectionException(\"Failed to connect to any Pacemaker.\");\n        }\n        return responses;\n    }"
            },
            "storm-client.src.jvm.org.apache.storm.messaging.netty.KerberosSaslClientHandler.handleControlMessage": {
                "code_before_change": "    private void handleControlMessage(ChannelHandlerContext ctx, ControlMessage controlMessage) throws Exception {\n        Channel channel = ctx.channel();\n        KerberosSaslNettyClient saslNettyClient = getChannelSaslClient(channel);\n        if (controlMessage == ControlMessage.SASL_COMPLETE_REQUEST) {\n                LOG.debug(\"Server has sent us the SaslComplete message. Allowing normal work to proceed.\");\n\n                if (!saslNettyClient.isComplete()) {\n                String errorMessage =\n                    \"Server returned a Sasl-complete message, but as far as we can tell, we are not authenticated yet.\";\n                LOG.error(errorMessage);\n                throw new Exception(errorMessage);\n                }\n            ctx.pipeline().remove(this);\n            this.client.channelReady(channel);\n\n            // We call fireChannelRead since the client is allowed to\n                // perform this request. The client's request will now proceed\n                // to the next pipeline component namely StormClientHandler.\n            ctx.fireChannelRead(controlMessage);\n            } else {\n            LOG.warn(\"Unexpected control message: {}\", controlMessage);\n            }\n    }",
                "code_after_change": "    private void handleControlMessage(ChannelHandlerContext ctx, ControlMessage controlMessage) throws Exception {\n        Channel channel = ctx.channel();\n        KerberosSaslNettyClient saslNettyClient = getChannelSaslClient(channel);\n        if (controlMessage == ControlMessage.SASL_COMPLETE_REQUEST) {\n            LOG.debug(\"Server has sent us the SaslComplete message. Allowing normal work to proceed.\");\n\n            if (!saslNettyClient.isComplete()) {\n                String errorMessage =\n                    \"Server returned a Sasl-complete message, but as far as we can tell, we are not authenticated yet.\";\n                LOG.error(errorMessage);\n                throw new Exception(errorMessage);\n            }\n            ctx.pipeline().remove(this);\n            this.client.channelReady(channel);\n\n            // We call fireChannelRead since the client is allowed to\n            // perform this request. The client's request will now proceed\n            // to the next pipeline component namely StormClientHandler.\n            ctx.fireChannelRead(controlMessage);\n        } else {\n            LOG.warn(\"Unexpected control message: {}\", controlMessage);\n        }\n    }"
            },
            "storm-client.src.jvm.org.apache.storm.pacemaker.PacemakerClient.send": {
                "code_before_change": "    public HBMessage send(HBMessage m) throws PacemakerConnectionException, InterruptedException {\n        LOG.debug(\"Sending message: {}\", m.toString());\n\n        int next = availableMessageSlots.take();\n        synchronized (m) {\n            m.set_message_id(next);\n            messages[next] = m;\n            LOG.debug(\"Put message in slot: {}\", Integer.toString(next));\n            int retry = maxRetries;\n            while (true) {\n                try {\n                    waitUntilReady();\n                    Channel channel = channelRef.get();\n                    if (channel != null) {\n                        channel.writeAndFlush(m, channel.voidPromise());\n                        m.wait(1000);\n                    }\n                    if (messages[next] != m && messages[next] != null) {\n                        // messages[next] == null can happen if we lost the connection and subsequently reconnected or timed out.\n                        HBMessage ret = messages[next];\n                        messages[next] = null;\n                        LOG.debug(\"Got Response: {}\", ret);\n                        return ret;\n                    }\n                } catch (PacemakerConnectionException e) {\n                    if (retry <= 0) {\n                        throw e;\n                    }\n                    LOG.error(\"error attempting to write to a channel {}.\", e.getMessage());\n                }\n                if (retry <= 0) {\n                    throw new PacemakerConnectionException(\"couldn't get response after \" + maxRetries + \" attempts.\");\n                }\n                retry--;\n                LOG.error(\"Not getting response or getting null response. Making {} more attempts.\", retry);\n            }\n        }\n    }",
                "code_after_change": "    public HBMessage send(HBMessage m) throws PacemakerConnectionException, InterruptedException {\n        LOG.debug(\"Sending pacemaker message to {}: {}\", host, m);\n\n        int next = availableMessageSlots.take();\n        synchronized (m) {\n            m.set_message_id(next);\n            messages[next] = m;\n            LOG.debug(\"Put message in slot: {} for {}\", Integer.toString(next), host);\n            int retry = maxRetries;\n            while (true) {\n                try {\n                    waitUntilReady();\n                    Channel channel = channelRef.get();\n                    if (channel != null) {\n                        channel.writeAndFlush(m, channel.voidPromise());\n                        m.wait(1000);\n                    }\n                    if (messages[next] != m && messages[next] != null) {\n                        // messages[next] == null can happen if we lost the connection and subsequently reconnected or timed out.\n                        HBMessage ret = messages[next];\n                        messages[next] = null;\n                        LOG.debug(\"Got Response: {}\", ret);\n                        return ret;\n                    }\n                } catch (PacemakerConnectionException e) {\n                    if (retry <= 0) {\n                        throw e;\n                    }\n                    LOG.error(\"error attempting to write to a channel {}.\", e.getMessage());\n                }\n                if (retry <= 0) {\n                    throw new PacemakerConnectionException(\"couldn't get response after \" + maxRetries + \" attempts.\");\n                }\n                retry--;\n                LOG.error(\"Not getting response or getting null response. Making {} more attempts for {}.\", retry, host);\n            }\n        }\n    }"
            },
            "storm-client.src.jvm.org.apache.storm.pacemaker.PacemakerClient.run": {
                "code_before_change": "            public void run() {\n                client.doReconnect();\n            }",
                "code_after_change": "            public void run() {\n                client.doReconnect();\n            }"
            },
            "storm-client.src.jvm.org.apache.storm.pacemaker.PacemakerClient.PacemakerClient": {
                "code_before_change": [],
                "code_after_change": []
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Buggy Method"
            },
            "fix_suggestion": "Alternative Fix",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Buggy Method"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the root cause as a failure to connect to the Pacemaker server, which is related to the ground truth method 'PacemakerClientPool.sendAll' where the error occurs, but not where the actual fix was made. The fix suggestion includes checking network connectivity and configuration, which is an alternative approach to the developer's fix of adding a reconnect attempt. The problem location mentions 'PacemakerClientPool.sendAll', which is the method where the error occurs, but not where the actual fix was made. There is no wrong information in the bug report as all details are relevant to the context of the bug."
        }
    },
    {
        "filename": "STORM-2095.json",
        "code_diff": {
            "storm-core.src.jvm.org.apache.storm.blobstore.FileBlobStoreImpl.listKeys": {
                "code_before_change": "    public Iterator<String> listKeys() throws IOException {\n        return new KeyInHashDirIterator();\n    }",
                "code_after_change": "    public Iterator<String> listKeys() throws IOException {\n        return new KeyInHashDirIterator();\n    }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Buggy Method"
            },
            "fix_suggestion": "Preventive",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Buggy Method"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the issue occurring in the 'LocalFsBlobStore.deleteBlob' and 'FileBlobStoreImpl.delete' methods, which are involved in the stack trace and are related to the problem, but not the exact ground truth method 'FileBlobStoreImpl.listKeys'. Therefore, the root cause identification is 'Partial' with 'Buggy Method' as the sub-category. The fix suggestion is 'Preventive' as it suggests adding a check to ensure the directory is empty before deletion, which would prevent the error. The problem location identification is also 'Partial' with 'Buggy Method' as the sub-category because it points to methods involved in the error but not the exact ground truth method. There is no wrong information in the bug report as all details are relevant to the context of the bug."
        }
    },
    {
        "filename": "STORM-2847.json",
        "code_diff": {
            "external.storm-kafka-client.src.main.java.org.apache.storm.kafka.spout.KafkaSpout.getTopicsString": {
                "code_before_change": "    private String getTopicsString() {\n        return kafkaSpoutConfig.getSubscription().getTopicsString();\n    }",
                "code_after_change": "    private String getTopicsString() {\n        return kafkaSpoutConfig.getSubscription().getTopicsString();\n    }"
            },
            "external.storm-kafka-client.src.main.java.org.apache.storm.kafka.spout.KafkaSpout.initialize": {
                "code_before_change": "        private void initialize(Collection<TopicPartition> partitions) {\n            if (isAtLeastOnceProcessing()) {\n                // remove from acked all partitions that are no longer assigned to this spout\n                offsetManagers.keySet().retainAll(partitions);\n                retryService.retainAll(partitions);\n\n                /*\n                 * Emitted messages for partitions that are no longer assigned to this spout can't\n                 * be acked and should not be retried, hence remove them from emitted collection.\n                 */\n                emitted.removeIf(msgId -> !partitions.contains(msgId.getTopicPartition()));\n            }\n\n            Set<TopicPartition> newPartitions = new HashSet<>(partitions);\n            newPartitions.removeAll(previousAssignment);\n            for (TopicPartition tp : newPartitions) {\n                final OffsetAndMetadata committedOffset = kafkaConsumer.committed(tp);\n                final long fetchOffset = doSeek(tp, committedOffset);\n                // If this partition was previously assigned to this spout, leave the acked offsets as they were to resume where it left off\n                if (isAtLeastOnceProcessing() && !offsetManagers.containsKey(tp)) {\n                    offsetManagers.put(tp, new OffsetManager(tp, fetchOffset));\n                }\n            }\n            initialized = true;\n            LOG.info(\"Initialization complete\");\n        }",
                "code_after_change": "        private void initialize(Collection<TopicPartition> partitions) {\n            if (isAtLeastOnceProcessing()) {\n                // remove from acked all partitions that are no longer assigned to this spout\n                offsetManagers.keySet().retainAll(partitions);\n                retryService.retainAll(partitions);\n\n                /*\n                 * Emitted messages for partitions that are no longer assigned to this spout can't\n                 * be acked and should not be retried, hence remove them from emitted collection.\n                 */\n                Iterator<KafkaSpoutMessageId> msgIdIterator = emitted.iterator();\n                while (msgIdIterator.hasNext()) {\n                    KafkaSpoutMessageId msgId = msgIdIterator.next();\n                    if (!partitions.contains(msgId.getTopicPartition())) {\n                        msgIdIterator.remove();\n                    }\n                }\n            }\n\n            Set<TopicPartition> newPartitions = new HashSet<>(partitions);\n            newPartitions.removeAll(previousAssignment);\n            for (TopicPartition tp : newPartitions) {\n                final OffsetAndMetadata committedOffset = kafkaConsumer.committed(tp);\n                final long fetchOffset = doSeek(tp, committedOffset);\n                LOG.debug(\"Set consumer position to [{}] for topic-partition [{}], based on strategy [{}] and committed offset [{}]\",\n                    fetchOffset, tp, firstPollOffsetStrategy, committedOffset);\n                // If this partition was previously assigned to this spout, leave the acked offsets as they were to resume where it left off\n                if (isAtLeastOnceProcessing() && !offsetManagers.containsKey(tp)) {\n                    offsetManagers.put(tp, new OffsetManager(tp, fetchOffset));\n                }\n            }\n            LOG.info(\"Initialization complete\");\n        }"
            },
            "external.storm-kafka-client.src.main.java.org.apache.storm.kafka.spout.KafkaSpout.close": {
                "code_before_change": "    public void close() {\n        try {\n            shutdown();\n        } catch (InterruptException e) {\n            throwKafkaConsumerInterruptedException();\n        }\n    }",
                "code_after_change": "    public void close() {\n        try {\n            shutdown();\n        } catch (InterruptException e) {\n            throwKafkaConsumerInterruptedException();\n        }\n    }"
            },
            "external.storm-kafka-client.src.main.java.org.apache.storm.kafka.spout.KafkaSpout.ackRetriableOffsetsIfCompactedAway": {
                "code_before_change": "    private void ackRetriableOffsetsIfCompactedAway(Map<TopicPartition, Long> earliestRetriableOffsets,\n        ConsumerRecords<K, V> consumerRecords) {\n        for (Entry<TopicPartition, Long> entry : earliestRetriableOffsets.entrySet()) {\n            TopicPartition tp = entry.getKey();\n            List<ConsumerRecord<K, V>> records = consumerRecords.records(tp);\n            if (!records.isEmpty()) {\n                ConsumerRecord<K, V> record = records.get(0);\n                long seekOffset = entry.getValue();\n                long earliestReceivedOffset = record.offset();\n                if (seekOffset < earliestReceivedOffset) {\n                    //Since we asked for tuples starting at seekOffset, some retriable records must have been compacted away.\n                    //Ack up to the first offset received if the record is not already acked or currently in the topology\n                    for (long i = seekOffset; i < earliestReceivedOffset; i++) {\n                        KafkaSpoutMessageId msgId = retryService.getMessageId(tp, i);\n                        if (!offsetManagers.get(tp).contains(msgId) && !emitted.contains(msgId)) {\n                            LOG.debug(\"Record at offset [{}] appears to have been compacted away from topic [{}], marking as acked\", i, tp);\n                            retryService.remove(msgId);\n                            emitted.add(msgId);\n                            ack(msgId);\n                        }\n                    }\n                }\n            }\n        }\n    }",
                "code_after_change": "    private void ackRetriableOffsetsIfCompactedAway(Map<TopicPartition, Long> earliestRetriableOffsets,\n        ConsumerRecords<K, V> consumerRecords) {\n        for (Entry<TopicPartition, Long> entry : earliestRetriableOffsets.entrySet()) {\n            TopicPartition tp = entry.getKey();\n            List<ConsumerRecord<K, V>> records = consumerRecords.records(tp);\n            if (!records.isEmpty()) {\n                ConsumerRecord<K, V> record = records.get(0);\n                long seekOffset = entry.getValue();\n                long earliestReceivedOffset = record.offset();\n                if (seekOffset < earliestReceivedOffset) {\n                    //Since we asked for tuples starting at seekOffset, some retriable records must have been compacted away.\n                    //Ack up to the first offset received if the record is not already acked or currently in the topology\n                    for (long i = seekOffset; i < earliestReceivedOffset; i++) {\n                        KafkaSpoutMessageId msgId = retryService.getMessageId(new ConsumerRecord<>(tp.topic(), tp.partition(), i, null, null));\n                        if (!offsetManagers.get(tp).contains(msgId) && !emitted.contains(msgId)) {\n                            LOG.debug(\"Record at offset [{}] appears to have been compacted away from topic [{}], marking as acked\", i, tp);\n                            retryService.remove(msgId);\n                            emitted.add(msgId);\n                            ack(msgId);\n                        }\n                    }\n                }\n            }\n        }\n    }"
            },
            "external.storm-kafka-client.src.main.java.org.apache.storm.kafka.spout.KafkaSpout.throwKafkaConsumerInterruptedException": {
                "code_before_change": "    private void throwKafkaConsumerInterruptedException() {\n        //Kafka throws their own type of exception when interrupted.\n        //Throw a new Java InterruptedException to ensure Storm can recognize the exception as a reaction to an interrupt.\n        throw new RuntimeException(new InterruptedException(\"Kafka consumer was interrupted\"));\n    }",
                "code_after_change": "    private void throwKafkaConsumerInterruptedException() {\n        //Kafka throws their own type of exception when interrupted.\n        //Throw a new Java InterruptedException to ensure Storm can recognize the exception as a reaction to an interrupt.\n        throw new RuntimeException(new InterruptedException(\"Kafka consumer was interrupted\"));\n    }"
            },
            "external.storm-kafka-client.src.main.java.org.apache.storm.kafka.spout.KafkaSpout.nextTuple": {
                "code_before_change": "    public void nextTuple() {\n        try {\n            if (initialized) {             \n             \n                if (refreshSubscriptionTimer.isExpiredResetOnTrue()) {\n                    kafkaSpoutConfig.getSubscription().refreshAssignment();\n                }\n\n                if (commit()) {\n                    commitOffsetsForAckedTuples();\n                }\n\n                Set<TopicPartition> pollablePartitions = poll();\n                if (!pollablePartitions.isEmpty()) {\n                    try {\n                        setWaitingToEmit(pollKafkaBroker(pollablePartitions));\n                    } catch (RetriableException e) {\n                        LOG.error(\"Failed to poll from kafka.\", e);\n                    }\n                }\n\n                if (waitingToEmit()) {\n                    emit();\n                }\n            } else {\n                LOG.debug(\"Spout not initialized. Not sending tuples until initialization completes\");\n            }\n        } catch (InterruptException e) {\n            throwKafkaConsumerInterruptedException();\n        }\n    }",
                "code_after_change": "    public void nextTuple() {\n        try {\n                if (refreshSubscriptionTimer.isExpiredResetOnTrue()) {\n                    kafkaSpoutConfig.getSubscription().refreshAssignment();\n                }\n\n                if (shouldCommit()) {\n                commitOffsetsForAckedTuples(kafkaConsumer.assignment());\n                }\n\n                PollablePartitionsInfo pollablePartitionsInfo = getPollablePartitionsInfo();\n                if (pollablePartitionsInfo.shouldPoll()) {\n                    try {\n                        setWaitingToEmit(pollKafkaBroker(pollablePartitionsInfo));\n                    } catch (RetriableException e) {\n                        LOG.error(\"Failed to poll from kafka.\", e);\n                    }\n                }\n\n                emitIfWaitingNotEmitted();\n        } catch (InterruptException e) {\n            throwKafkaConsumerInterruptedException();\n        }\n    }"
            },
            "external.storm-kafka-client.src.main.java.org.apache.storm.kafka.spout.KafkaSpout.commitOffsetsForAckedTuples": {
                "code_before_change": "    private void commitOffsetsForAckedTuples() {\n        // Find offsets that are ready to be committed for every topic partition\n        final Map<TopicPartition, OffsetAndMetadata> nextCommitOffsets = new HashMap<>();\n        for (Map.Entry<TopicPartition, OffsetManager> tpOffset : offsetManagers.entrySet()) {\n            final OffsetAndMetadata nextCommitOffset = tpOffset.getValue().findNextCommitOffset();\n            if (nextCommitOffset != null) {\n                nextCommitOffsets.put(tpOffset.getKey(), nextCommitOffset);\n            }\n        }\n\n        // Commit offsets that are ready to be committed for every topic partition\n        if (!nextCommitOffsets.isEmpty()) {\n            kafkaConsumer.commitSync(nextCommitOffsets);\n            LOG.debug(\"Offsets successfully committed to Kafka [{}]\", nextCommitOffsets);\n            // Instead of iterating again, it would be possible to commit and update the state for each TopicPartition\n            // in the prior loop, but the multiple network calls should be more expensive than iterating twice over a small loop\n            for (Map.Entry<TopicPartition, OffsetAndMetadata> tpOffset : nextCommitOffsets.entrySet()) {\n                //Update the OffsetManager for each committed partition, and update numUncommittedOffsets\n                final TopicPartition tp = tpOffset.getKey();\n                long position = kafkaConsumer.position(tp);\n                long committedOffset = tpOffset.getValue().offset();\n                if (position < committedOffset) {\n                    /*\n                     * The position is behind the committed offset. This can happen in some cases, e.g. if a message failed,\n                     * lots of (more than max.poll.records) later messages were acked, and the failed message then gets acked. \n                     * The consumer may only be part way through \"catching up\" to where it was when it went back to retry the failed tuple. \n                     * Skip the consumer forward to the committed offset drop the current waiting to emit list,\n                     * since it'll likely contain committed offsets.\n                     */\n                    LOG.debug(\"Consumer fell behind committed offset. Catching up. Position was [{}], skipping to [{}]\",\n                        position, committedOffset);\n                    kafkaConsumer.seek(tp, committedOffset);\n                    waitingToEmit = null;\n                }\n                \n                \n                final OffsetManager offsetManager = offsetManagers.get(tp);\n                offsetManager.commit(tpOffset.getValue());\n                LOG.debug(\"[{}] uncommitted offsets for partition [{}] after commit\", offsetManager.getNumUncommittedOffsets(), tp);\n            }\n        } else {\n            LOG.trace(\"No offsets to commit. {}\", this);\n        }\n    }",
                "code_after_change": "    private void commitOffsetsForAckedTuples(Set<TopicPartition> assignedPartitions) {\n        // Find offsets that are ready to be committed for every assigned topic partition\n        final Map<TopicPartition, OffsetManager> assignedOffsetManagers = new HashMap<>();\n        for (Entry<TopicPartition, OffsetManager> entry : offsetManagers.entrySet()) {\n            if (assignedPartitions.contains(entry.getKey())) {\n                assignedOffsetManagers.put(entry.getKey(), entry.getValue());\n            }\n        }\n\n        final Map<TopicPartition, OffsetAndMetadata> nextCommitOffsets = new HashMap<>();\n        for (Map.Entry<TopicPartition, OffsetManager> tpOffset : assignedOffsetManagers.entrySet()) {\n            final OffsetAndMetadata nextCommitOffset = tpOffset.getValue().findNextCommitOffset();\n            if (nextCommitOffset != null) {\n                nextCommitOffsets.put(tpOffset.getKey(), nextCommitOffset);\n            }\n        }\n\n        // Commit offsets that are ready to be committed for every topic partition\n        if (!nextCommitOffsets.isEmpty()) {\n            kafkaConsumer.commitSync(nextCommitOffsets);\n            LOG.debug(\"Offsets successfully committed to Kafka [{}]\", nextCommitOffsets);\n            // Instead of iterating again, it would be possible to commit and update the state for each TopicPartition\n            // in the prior loop, but the multiple network calls should be more expensive than iterating twice over a small loop\n            for (Map.Entry<TopicPartition, OffsetAndMetadata> tpOffset : nextCommitOffsets.entrySet()) {\n                //Update the OffsetManager for each committed partition, and update numUncommittedOffsets\n                final TopicPartition tp = tpOffset.getKey();\n                long position = kafkaConsumer.position(tp);\n                long committedOffset = tpOffset.getValue().offset();\n                if (position < committedOffset) {\n                    /*\n                     * The position is behind the committed offset. This can happen in some cases, e.g. if a message failed,\n                     * lots of (more than max.poll.records) later messages were acked, and the failed message then gets acked. \n                     * The consumer may only be part way through \"catching up\" to where it was when it went back to retry the failed tuple. \n                     * Skip the consumer forward to the committed offset drop the current waiting to emit list,\n                     * since it'll likely contain committed offsets.\n                     */\n                    LOG.debug(\"Consumer fell behind committed offset. Catching up. Position was [{}], skipping to [{}]\",\n                        position, committedOffset);\n                    kafkaConsumer.seek(tp, committedOffset);\n                    waitingToEmit = null;\n                }\n                \n                final OffsetManager offsetManager = assignedOffsetManagers.get(tp);\n                offsetManager.commit(tpOffset.getValue());\n                LOG.debug(\"[{}] uncommitted offsets for partition [{}] after commit\", offsetManager.getNumUncommittedOffsets(), tp);\n            }\n        } else {\n            LOG.trace(\"No offsets to commit. {}\", this);\n        }\n    }"
            },
            "external.storm-kafka-client.src.main.java.org.apache.storm.kafka.spout.ManualPartitionSubscription.refreshAssignment": {
                "code_before_change": [],
                "code_after_change": "    public void refreshAssignment() {\n        List<TopicPartition> allPartitions = partitionFilter.getFilteredTopicPartitions(consumer);\n        Collections.sort(allPartitions, TopicPartitionComparator.INSTANCE);\n        Set<TopicPartition> newAssignment = new HashSet<>(partitioner.partition(allPartitions, context));\n        Set<TopicPartition> currentAssignment = consumer.assignment();\n        if (!newAssignment.equals(currentAssignment)) {\n            listener.onPartitionsRevoked(currentAssignment);\n            consumer.assign(newAssignment);\n            listener.onPartitionsAssigned(newAssignment);\n        }\n    }"
            },
            "external.storm-kafka-client.src.main.java.org.apache.storm.kafka.spout.KafkaSpout.isEmitTuple": {
                "code_before_change": "    private boolean isEmitTuple(List<Object> tuple) {\n        return tuple != null || kafkaSpoutConfig.isEmitNullTuples();\n    }",
                "code_after_change": "    private boolean isEmitTuple(List<Object> tuple) {\n        return tuple != null || kafkaSpoutConfig.isEmitNullTuples();\n    }"
            },
            "external.storm-kafka-client.src.main.java.org.apache.storm.kafka.spout.KafkaSpout.KafkaSpout": {
                "code_before_change": [],
                "code_after_change": []
            },
            "external.storm-kafka-client.src.main.java.org.apache.storm.kafka.spout.KafkaSpout.doSeek": {
                "code_before_change": "        private long doSeek(TopicPartition tp, OffsetAndMetadata committedOffset) {\n            if (committedOffset != null) {             // offset was committed for this TopicPartition\n                if (firstPollOffsetStrategy.equals(EARLIEST)) {\n                    kafkaConsumer.seekToBeginning(Collections.singleton(tp));\n                } else if (firstPollOffsetStrategy.equals(LATEST)) {\n                    kafkaConsumer.seekToEnd(Collections.singleton(tp));\n                } else {\n                    // By default polling starts at the last committed offset, i.e. the first offset that was not marked as processed.\n                    kafkaConsumer.seek(tp, committedOffset.offset());\n                }\n            } else {    // no commits have ever been done, so start at the beginning or end depending on the strategy\n                if (firstPollOffsetStrategy.equals(EARLIEST) || firstPollOffsetStrategy.equals(UNCOMMITTED_EARLIEST)) {\n                    kafkaConsumer.seekToBeginning(Collections.singleton(tp));\n                } else if (firstPollOffsetStrategy.equals(LATEST) || firstPollOffsetStrategy.equals(UNCOMMITTED_LATEST)) {\n                    kafkaConsumer.seekToEnd(Collections.singleton(tp));\n                }\n            }\n            return kafkaConsumer.position(tp);\n        }",
                "code_after_change": "        private long doSeek(TopicPartition tp, OffsetAndMetadata committedOffset) {\n            if (committedOffset != null) {             // offset was committed for this TopicPartition\n                if (firstPollOffsetStrategy.equals(EARLIEST)) {\n                    kafkaConsumer.seekToBeginning(Collections.singleton(tp));\n                } else if (firstPollOffsetStrategy.equals(LATEST)) {\n                    kafkaConsumer.seekToEnd(Collections.singleton(tp));\n                } else {\n                    // By default polling starts at the last committed offset, i.e. the first offset that was not marked as processed.\n                    kafkaConsumer.seek(tp, committedOffset.offset());\n                }\n            } else {    // no commits have ever been done, so start at the beginning or end depending on the strategy\n                if (firstPollOffsetStrategy.equals(EARLIEST) || firstPollOffsetStrategy.equals(UNCOMMITTED_EARLIEST)) {\n                    kafkaConsumer.seekToBeginning(Collections.singleton(tp));\n                } else if (firstPollOffsetStrategy.equals(LATEST) || firstPollOffsetStrategy.equals(UNCOMMITTED_LATEST)) {\n                    kafkaConsumer.seekToEnd(Collections.singleton(tp));\n                }\n            }\n            return kafkaConsumer.position(tp);\n        }"
            },
            "external.storm-kafka-client.src.main.java.org.apache.storm.kafka.spout.KafkaSpout.emitOrRetryTuple": {
                "code_before_change": [],
                "code_after_change": "    private boolean emitOrRetryTuple(ConsumerRecord<K, V> record) {\n        final TopicPartition tp = new TopicPartition(record.topic(), record.partition());\n        final KafkaSpoutMessageId msgId = retryService.getMessageId(record);\n\n        if (offsetManagers.containsKey(tp) && offsetManagers.get(tp).contains(msgId)) {   // has been acked\n            LOG.trace(\"Tuple for record [{}] has already been acked. Skipping\", record);\n        } else if (emitted.contains(msgId)) {   // has been emitted and it is pending ack or fail\n            LOG.trace(\"Tuple for record [{}] has already been emitted. Skipping\", record);\n        } else {\n            if (kafkaConsumer.committed(tp) != null && (kafkaConsumer.committed(tp).offset() > kafkaConsumer.position(tp))) {\n                throw new IllegalStateException(\"Attempting to emit a message that has already been committed.\");\n            }\n\n            final List<Object> tuple = kafkaSpoutConfig.getTranslator().apply(record);\n            if (isEmitTuple(tuple)) {\n                final boolean isScheduled = retryService.isScheduled(msgId);\n                // not scheduled <=> never failed (i.e. never emitted), or scheduled and ready to be retried\n                if (!isScheduled || retryService.isReady(msgId)) {\n                    final String stream = tuple instanceof KafkaTuple ? ((KafkaTuple) tuple).getStream() : Utils.DEFAULT_STREAM_ID;\n\n                    if (!isAtLeastOnceProcessing()) {\n                        if (kafkaSpoutConfig.isTupleTrackingEnforced()) {\n                            collector.emit(stream, tuple, msgId);\n                            LOG.trace(\"Emitted tuple [{}] for record [{}] with msgId [{}]\", tuple, record, msgId);\n                        } else {\n                            collector.emit(stream, tuple);\n                            LOG.trace(\"Emitted tuple [{}] for record [{}]\", tuple, record);\n                        }\n                    } else {\n                        emitted.add(msgId);\n                        offsetManagers.get(tp).addToEmitMsgs(msgId.offset());\n                        if (isScheduled) {  // Was scheduled for retry and re-emitted, so remove from schedule.\n                            retryService.remove(msgId);\n                        }\n                        collector.emit(stream, tuple, msgId);\n                        tupleListener.onEmit(tuple, msgId);\n                        LOG.trace(\"Emitted tuple [{}] for record [{}] with msgId [{}]\", tuple, record, msgId);\n                    }\n                    return true;\n                }\n            } else {\n                LOG.debug(\"Not emitting null tuple for record [{}] as defined in configuration.\", record);\n                msgId.setEmitted(false);\n                ack(msgId);\n            }\n        }\n        return false;\n    }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Correct",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report precisely identifies the root cause as the Kafka consumer attempting to call the 'position' method on partitions not assigned to it, which matches the issue in the ground truth method 'commitOffsetsForAckedTuples'. The fix suggestion in the bug report is correct as it aligns with the developer's fix, which involves checking assigned partitions before calling 'position'. The problem location is also precise, as the report directly mentions the 'commitOffsetsForAckedTuples' method, which is in the ground truth list. There is no wrong information in the bug report; all details are consistent with the context of the bug."
        }
    },
    {
        "filename": "STORM-1114.json",
        "code_diff": {
            "storm-client.src.jvm.org.apache.storm.transactional.state.TransactionalState.TransactionalState": {
                "code_before_change": [],
                "code_after_change": []
            },
            "storm-client.src.jvm.org.apache.storm.transactional.state.TransactionalState.setData": {
                "code_before_change": [],
                "code_after_change": "    public void setData(String path, Object obj) {\n        path = \"/\" + path;\n        byte[] ser = _ser.serializeObject(obj);\n        try {\n            if(_curator.checkExists().forPath(path)!=null) {\n                _curator.setData().forPath(path, ser);\n            } else {\n                TransactionalState.createNode(_curator, path, ser, _zkAcls,\n                        CreateMode.PERSISTENT);\n            }\n        } catch (KeeperException.NodeExistsException nee) {\n            LOG.warn(\"Path {} already exists.\", path);\n        } catch (Exception e) {\n            throw new RuntimeException(e);\n        }\n    }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Same Class or Module"
            },
            "fix_suggestion": "Preventive",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Same Class or Module"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies a race condition in the handling of Zookeeper node creation and deletion within the Trident framework, which is related to the ground truth methods in the same class/module, but not the exact root cause. The fix suggestion involves implementing synchronization mechanisms, which is preventive as it aims to mitigate the race condition issue. The problem location mentions methods in the same class as the ground truth methods, but not the exact methods. There is no wrong information in the bug report as all details are relevant to the context of the bug."
        }
    },
    {
        "filename": "STORM-2811.json",
        "code_diff": {
            "integration-test.src.main.java.org.apache.storm.st.topology.window.SlidingTimeCorrectness.open": {
                "code_before_change": "        public void open(Map<String, Object> conf, TopologyContext context, SpoutOutputCollector collector) {\n            componentId = context.getThisComponentId();\n            this.collector = collector;\n        }",
                "code_after_change": "        public void open(Map conf, TopologyContext context, SpoutOutputCollector collector) {\n            componentId = context.getThisComponentId();\n            this.collector = collector;\n        }"
            },
            "storm-core.src.jvm.org.apache.storm.windowing.TimeEvictionPolicy.TimeEvictionPolicy": {
                "code_before_change": [],
                "code_after_change": []
            },
            "integration-test.src.main.java.org.apache.storm.st.topology.window.TumblingTimeCorrectness.open": {
                "code_before_change": "        public void open(Map<String, Object> conf, TopologyContext context, SpoutOutputCollector collector) {\n            componentId = context.getThisComponentId();\n            this.collector = collector;\n        }",
                "code_after_change": "        public void open(Map conf, TopologyContext context, SpoutOutputCollector collector) {\n            componentId = context.getThisComponentId();\n            this.collector = collector;\n        }"
            },
            "storm-core.src.jvm.org.apache.storm.windowing.WatermarkCountEvictionPolicy.WatermarkCountEvictionPolicy": {
                "code_before_change": [],
                "code_after_change": []
            },
            "integration-test.src.main.java.org.apache.storm.st.topology.window.SlidingWindowCorrectness.newTopology": {
                "code_before_change": "    public StormTopology newTopology() {\n        TopologyBuilder builder = new TopologyBuilder();\n        builder.setSpout(getSpoutName(), new IncrementingSpout(), 1);\n        builder.setBolt(getBoltName(),\n                new VerificationBolt()\n                        .withWindow(new BaseWindowedBolt.Count(windowSize), new BaseWindowedBolt.Count(slideSize)),\n                1)\n                .shuffleGrouping(getSpoutName());\n        return builder.createTopology();\n    }",
                "code_after_change": "    public StormTopology newTopology() {\n        TopologyBuilder builder = new TopologyBuilder();\n        builder.setSpout(getSpoutName(), new IncrementingSpout(), 1);\n        builder.setBolt(getBoltName(),\n                new VerificationBolt()\n                        .withWindow(new BaseWindowedBolt.Count(windowSize), new BaseWindowedBolt.Count(slideSize)),\n                1)\n                .shuffleGrouping(getSpoutName());\n        return builder.createTopology();\n    }"
            },
            "storm-core.src.jvm.org.apache.storm.windowing.WatermarkTimeEvictionPolicy.WatermarkTimeEvictionPolicy": {
                "code_before_change": [],
                "code_after_change": []
            },
            "storm-core.src.jvm.org.apache.storm.windowing.WatermarkCountEvictionPolicy.track": {
                "code_before_change": [],
                "code_after_change": "    public void track(Event<T> event) {\n        // NOOP\n    }"
            },
            "integration-test.src.main.java.org.apache.storm.st.topology.window.TumblingWindowCorrectness.newTopology": {
                "code_before_change": "    public StormTopology newTopology() {\n        TopologyBuilder builder = new TopologyBuilder();\n        builder.setSpout(getSpoutName(), new IncrementingSpout(), 1);\n        builder.setBolt(getBoltName(),\n                new VerificationBolt()\n                        .withTumblingWindow(new BaseWindowedBolt.Count(tumbleSize)), 1)\n                .shuffleGrouping(getSpoutName());\n        return builder.createTopology();\n    }",
                "code_after_change": "    public StormTopology newTopology() {\n        TopologyBuilder builder = new TopologyBuilder();\n        builder.setSpout(getSpoutName(), new IncrementingSpout(), 1);\n        builder.setBolt(getBoltName(),\n                new VerificationBolt()\n                        .withTumblingWindow(new BaseWindowedBolt.Count(tumbleSize)), 1)\n                .shuffleGrouping(getSpoutName());\n        return builder.createTopology();\n    }"
            },
            "storm-core.src.jvm.org.apache.storm.topology.WindowedBoltExecutor.initWindowManager": {
                "code_before_change": [],
                "code_after_change": "    private WindowManager<Tuple> initWindowManager(WindowLifecycleListener<Tuple> lifecycleListener, Map stormConf,\n                                                   TopologyContext context) {\n        WindowManager<Tuple> manager = new WindowManager<>(lifecycleListener);\n        Duration windowLengthDuration = null;\n        Count windowLengthCount = null;\n        Duration slidingIntervalDuration = null;\n        Count slidingIntervalCount = null;\n        // window length\n        if (stormConf.containsKey(Config.TOPOLOGY_BOLTS_WINDOW_LENGTH_COUNT)) {\n            windowLengthCount = new Count(((Number) stormConf.get(Config.TOPOLOGY_BOLTS_WINDOW_LENGTH_COUNT)).intValue());\n        } else if (stormConf.containsKey(Config.TOPOLOGY_BOLTS_WINDOW_LENGTH_DURATION_MS)) {\n            windowLengthDuration = new Duration(\n                    ((Number) stormConf.get(Config.TOPOLOGY_BOLTS_WINDOW_LENGTH_DURATION_MS)).intValue(),\n                    TimeUnit.MILLISECONDS);\n        }\n        // sliding interval\n        if (stormConf.containsKey(Config.TOPOLOGY_BOLTS_SLIDING_INTERVAL_COUNT)) {\n            slidingIntervalCount = new Count(((Number) stormConf.get(Config.TOPOLOGY_BOLTS_SLIDING_INTERVAL_COUNT)).intValue());\n        } else if (stormConf.containsKey(Config.TOPOLOGY_BOLTS_SLIDING_INTERVAL_DURATION_MS)) {\n            slidingIntervalDuration = new Duration(((Number) stormConf.get(Config.TOPOLOGY_BOLTS_SLIDING_INTERVAL_DURATION_MS)).intValue(), TimeUnit.MILLISECONDS);\n        } else {\n            // default is a sliding window of count 1\n            slidingIntervalCount = new Count(1);\n        }\n        // tuple ts\n        if (timestampExtractor != null) {\n            // late tuple stream\n            lateTupleStream = (String) stormConf.get(Config.TOPOLOGY_BOLTS_LATE_TUPLE_STREAM);\n            if (lateTupleStream != null) {\n                if (!context.getThisStreams().contains(lateTupleStream)) {\n                    throw new IllegalArgumentException(\"Stream for late tuples must be defined with the builder method withLateTupleStream\");\n                }\n            }\n            // max lag\n            if (stormConf.containsKey(Config.TOPOLOGY_BOLTS_TUPLE_TIMESTAMP_MAX_LAG_MS)) {\n                maxLagMs = ((Number) stormConf.get(Config.TOPOLOGY_BOLTS_TUPLE_TIMESTAMP_MAX_LAG_MS)).intValue();\n            } else {\n                maxLagMs = DEFAULT_MAX_LAG_MS;\n            }\n            // watermark interval\n            int watermarkInterval;\n            if (stormConf.containsKey(Config.TOPOLOGY_BOLTS_WATERMARK_EVENT_INTERVAL_MS)) {\n                watermarkInterval = ((Number) stormConf.get(Config.TOPOLOGY_BOLTS_WATERMARK_EVENT_INTERVAL_MS)).intValue();\n            } else {\n                watermarkInterval = DEFAULT_WATERMARK_EVENT_INTERVAL_MS;\n            }\n            waterMarkEventGenerator = new WaterMarkEventGenerator<>(manager, watermarkInterval,\n                                                                    maxLagMs, getComponentStreams(context));\n        } else {\n            if (stormConf.containsKey(Config.TOPOLOGY_BOLTS_LATE_TUPLE_STREAM)) {\n                throw new IllegalArgumentException(\"Late tuple stream can be defined only when specifying a timestamp field\");\n            }\n        }\n        // validate\n        validate(stormConf, windowLengthCount, windowLengthDuration,\n                 slidingIntervalCount, slidingIntervalDuration);\n        evictionPolicy = getEvictionPolicy(windowLengthCount, windowLengthDuration);\n        triggerPolicy = getTriggerPolicy(slidingIntervalCount, slidingIntervalDuration,\n                                                              manager, evictionPolicy);\n        manager.setEvictionPolicy(evictionPolicy);\n        manager.setTriggerPolicy(triggerPolicy);\n        return manager;\n    }"
            },
            "storm-core.src.jvm.org.apache.storm.topology.WindowedBoltExecutor.getTriggerPolicy": {
                "code_before_change": [],
                "code_after_change": "    private TriggerPolicy<Tuple> getTriggerPolicy(Count slidingIntervalCount, Duration slidingIntervalDuration,\n                                                  WindowManager<Tuple> manager, EvictionPolicy<Tuple> evictionPolicy) {\n        if (slidingIntervalCount != null) {\n            if (isTupleTs()) {\n                return new WatermarkCountTriggerPolicy<>(slidingIntervalCount.value, manager, evictionPolicy, manager);\n            } else {\n                return new CountTriggerPolicy<>(slidingIntervalCount.value, manager, evictionPolicy);\n            }\n        } else {\n            if (isTupleTs()) {\n                return new WatermarkTimeTriggerPolicy<>(slidingIntervalDuration.value, manager, evictionPolicy, manager);\n            } else {\n                return new TimeTriggerPolicy<>(slidingIntervalDuration.value, manager, evictionPolicy);\n            }\n        }\n    }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Preventive",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the root cause as an NPE due to an unchecked empty Optional in the method `getTopoId`, which is not in the ground truth list but is part of the stack trace, hence 'Partial' with 'Shared Stack Trace Context'. The fix suggestion to check for the presence of a topology ID is preventive, as it would mitigate the NPE issue. The problem location mentions methods in the stack trace but not the ground truth methods, so it is 'Partial' with 'Shared Stack Trace Context'. There is no wrong information in the bug report."
        }
    },
    {
        "filename": "STORM-2903.json",
        "code_diff": {
            "external.storm-kafka-client.src.main.java.org.apache.storm.kafka.spout.KafkaSpout.emitOrRetryTuple": {
                "code_before_change": "    private boolean emitOrRetryTuple(ConsumerRecord<K, V> record) {\n        final TopicPartition tp = new TopicPartition(record.topic(), record.partition());\n        final KafkaSpoutMessageId msgId = retryService.getMessageId(tp, record.offset());\n\n        if (offsetManagers.containsKey(tp) && offsetManagers.get(tp).contains(msgId)) {   // has been acked\n            LOG.trace(\"Tuple for record [{}] has already been acked. Skipping\", record);\n        } else if (emitted.contains(msgId)) {   // has been emitted and it is pending ack or fail\n            LOG.trace(\"Tuple for record [{}] has already been emitted. Skipping\", record);\n        } else {\n            final OffsetAndMetadata committedOffset = kafkaConsumer.committed(tp);\n            if (committedOffset != null && isOffsetCommittedByThisTopology(tp, committedOffset)\n                && committedOffset.offset() > record.offset()) {\n                // Ensures that after a topology with this id is started, the consumer fetch\n                // position never falls behind the committed offset (STORM-2844)\n                throw new IllegalStateException(\"Attempting to emit a message that has already been committed.\");\n            }\n\n            final List<Object> tuple = kafkaSpoutConfig.getTranslator().apply(record);\n            if (isEmitTuple(tuple)) {\n                final boolean isScheduled = retryService.isScheduled(msgId);\n                // not scheduled <=> never failed (i.e. never emitted), or scheduled and ready to be retried\n                if (!isScheduled || retryService.isReady(msgId)) {\n                    final String stream = tuple instanceof KafkaTuple ? ((KafkaTuple) tuple).getStream() : Utils.DEFAULT_STREAM_ID;\n\n                    if (!isAtLeastOnceProcessing()) {\n                        if (kafkaSpoutConfig.isTupleTrackingEnforced()) {\n                            collector.emit(stream, tuple, msgId);\n                            LOG.trace(\"Emitted tuple [{}] for record [{}] with msgId [{}]\", tuple, record, msgId);\n                        } else {\n                            collector.emit(stream, tuple);\n                            LOG.trace(\"Emitted tuple [{}] for record [{}]\", tuple, record);\n                        }\n                    } else {\n                        emitted.add(msgId);\n                        offsetManagers.get(tp).addToEmitMsgs(msgId.offset());\n                        if (isScheduled) {  // Was scheduled for retry and re-emitted, so remove from schedule.\n                            retryService.remove(msgId);\n                        }\n                        collector.emit(stream, tuple, msgId);\n                        tupleListener.onEmit(tuple, msgId);\n                        LOG.trace(\"Emitted tuple [{}] for record [{}] with msgId [{}]\", tuple, record, msgId);\n                    }\n                    return true;\n                }\n            } else {\n                LOG.debug(\"Not emitting null tuple for record [{}] as defined in configuration.\", record);\n                msgId.setEmitted(false);\n                ack(msgId);\n            }\n        }\n        return false;\n    }",
                "code_after_change": "    private boolean emitOrRetryTuple(ConsumerRecord<K, V> record) {\n        final TopicPartition tp = new TopicPartition(record.topic(), record.partition());\n        final KafkaSpoutMessageId msgId = retryService.getMessageId(record);\n\n        if (offsetManagers.containsKey(tp) && offsetManagers.get(tp).contains(msgId)) {   // has been acked\n            LOG.trace(\"Tuple for record [{}] has already been acked. Skipping\", record);\n        } else if (emitted.contains(msgId)) {   // has been emitted and it is pending ack or fail\n            LOG.trace(\"Tuple for record [{}] has already been emitted. Skipping\", record);\n        } else {\n            final OffsetAndMetadata committedOffset = kafkaConsumer.committed(tp);\n            if (isAtLeastOnceProcessing()\n                && committedOffset != null \n                && committedOffset.offset() > record.offset()\n                && commitMetadataManager.isOffsetCommittedByThisTopology(tp, committedOffset, Collections.unmodifiableMap(offsetManagers))) {\n                // Ensures that after a topology with this id is started, the consumer fetch\n                // position never falls behind the committed offset (STORM-2844)\n                throw new IllegalStateException(\"Attempting to emit a message that has already been committed.\"\n                    + \" This should never occur when using the at-least-once processing guarantee.\");\n            }\n\n            final List<Object> tuple = kafkaSpoutConfig.getTranslator().apply(record);\n            if (isEmitTuple(tuple)) {\n                final boolean isScheduled = retryService.isScheduled(msgId);\n                // not scheduled <=> never failed (i.e. never emitted), or scheduled and ready to be retried\n                if (!isScheduled || retryService.isReady(msgId)) {\n                    final String stream = tuple instanceof KafkaTuple ? ((KafkaTuple) tuple).getStream() : Utils.DEFAULT_STREAM_ID;\n\n                    if (!isAtLeastOnceProcessing()) {\n                        if (kafkaSpoutConfig.isTupleTrackingEnforced()) {\n                            collector.emit(stream, tuple, msgId);\n                            LOG.trace(\"Emitted tuple [{}] for record [{}] with msgId [{}]\", tuple, record, msgId);\n                        } else {\n                            collector.emit(stream, tuple);\n                            LOG.trace(\"Emitted tuple [{}] for record [{}]\", tuple, record);\n                        }\n                    } else {\n                        emitted.add(msgId);\n                        offsetManagers.get(tp).addToEmitMsgs(msgId.offset());\n                        if (isScheduled) {  // Was scheduled for retry and re-emitted, so remove from schedule.\n                            retryService.remove(msgId);\n                        }\n                        collector.emit(stream, tuple, msgId);\n                        tupleListener.onEmit(tuple, msgId);\n                        LOG.trace(\"Emitted tuple [{}] for record [{}] with msgId [{}]\", tuple, record, msgId);\n                    }\n                    return true;\n                }\n            } else {\n                /*if a null tuple is not configured to be emitted, it should be marked as emitted and acked immediately\n                * to allow its offset to be commited to Kafka*/\n                LOG.debug(\"Not emitting null tuple for record [{}] as defined in configuration.\", record);\n                if (isAtLeastOnceProcessing()) {\n                    msgId.setNullTuple(true);\n                    offsetManagers.get(tp).addToEmitMsgs(msgId.offset());\n                    ack(msgId);\n                }\n            }\n        }\n        return false;\n    }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Preventive",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies a NullPointerException in the 'populateSubject' method, which is not the ground truth method but is mentioned in the stack trace, making it a 'Shared Stack Trace Context'. The fix suggestion to add a null check is preventive as it would mitigate the issue but does not match the actual fix in the ground truth method. The problem location is also identified as 'Shared Stack Trace Context' because it mentions methods in the stack trace but not the ground truth method. There is no wrong information in the bug report as all details are relevant to the context of the bug."
        }
    },
    {
        "filename": "STORM-3168.json",
        "code_diff": {
            "storm-server.src.main.java.org.apache.storm.localizer.AsyncLocalizer.forEachTopologyDistDir": {
                "code_before_change": "    private void forEachTopologyDistDir(ConsumePathAndId consumer) throws IOException {\n        Path stormCodeRoot = Paths.get(ConfigUtils.supervisorStormDistRoot(conf));\n        if (Files.exists(stormCodeRoot) && Files.isDirectory(stormCodeRoot)) {\n            try (DirectoryStream<Path> children = Files.newDirectoryStream(stormCodeRoot)) {\n                for (Path child : children) {\n                    if (Files.isDirectory(child)) {\n                        String topologyId = child.getFileName().toString();\n                        consumer.accept(child, topologyId);\n                    }\n                }\n            }\n        }\n    }",
                "code_after_change": "    private void forEachTopologyDistDir(ConsumePathAndId consumer) throws IOException {\n        Path stormCodeRoot = Paths.get(ConfigUtils.supervisorStormDistRoot(conf));\n        if (Files.exists(stormCodeRoot) && Files.isDirectory(stormCodeRoot)) {\n            try (DirectoryStream<Path> children = Files.newDirectoryStream(stormCodeRoot)) {\n                for (Path child : children) {\n                    if (Files.isDirectory(child)) {\n                        String topologyId = child.getFileName().toString();\n                        consumer.accept(child, topologyId);\n                    }\n                }\n            }\n        }\n    }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Buggy Method"
            },
            "fix_suggestion": "Preventive",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Buggy Method"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the root cause as a KeyNotFoundException in the AsyncLocalizer, which is not the exact root cause in the ground truth method but is related to the method where the error occurred. The fix suggestion is preventive as it suggests adding exception handling to prevent the crash, which is a valid approach but not the exact fix applied by the developer. The problem location is identified as AsyncLocalizer.updateBlobs, which is where the error occurs but not where the fix was made, hence classified as 'Buggy Method'. There is no wrong information in the bug report as all details are relevant to the context of the bug."
        }
    },
    {
        "filename": "STORM-2986.json",
        "code_diff": {
            "storm-webapp.src.main.java.org.apache.storm.daemon.logviewer.LogviewerServer.main": {
                "code_before_change": "    public static void main(String [] args) throws Exception {\n        Utils.setupDefaultUncaughtExceptionHandler();\n        Map<String, Object> conf = ConfigUtils.readStormConfig();\n\n        String logRoot = ConfigUtils.workerArtifactsRoot(conf);\n        File logRootFile = new File(logRoot);\n        WorkerLogs workerLogs = new WorkerLogs(conf, logRootFile);\n        DirectoryCleaner directoryCleaner = new DirectoryCleaner();\n\n        try (LogviewerServer server = new LogviewerServer(conf);\n             LogCleaner logCleaner = new LogCleaner(conf, workerLogs, directoryCleaner, logRootFile)) {\n            Utils.addShutdownHookWithForceKillIn1Sec(() -> server.close());\n            logCleaner.start();\n            StormMetricsRegistry.startMetricsReporters(conf);\n            server.start();\n            server.awaitTermination();\n        }\n    }",
                "code_after_change": "    public static void main(String [] args) throws Exception {\n        Utils.setupDefaultUncaughtExceptionHandler();\n        Map<String, Object> conf = ConfigUtils.readStormConfig();\n\n        String logRoot = ConfigUtils.workerArtifactsRoot(conf);\n        File logRootDir = new File(logRoot);\n        logRootDir.mkdirs();\n        WorkerLogs workerLogs = new WorkerLogs(conf, logRootDir);\n        DirectoryCleaner directoryCleaner = new DirectoryCleaner();\n\n        try (LogviewerServer server = new LogviewerServer(conf);\n             LogCleaner logCleaner = new LogCleaner(conf, workerLogs, directoryCleaner, logRootDir)) {\n            Utils.addShutdownHookWithForceKillIn1Sec(() -> server.close());\n            logCleaner.start();\n            StormMetricsRegistry.startMetricsReporters(conf);\n            server.start();\n            server.awaitTermination();\n        }\n    }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Buggy Method"
            },
            "fix_suggestion": "Preventive",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Buggy Method"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the root cause as the absence of the 'workers-artifacts' directory leading to a NullPointerException in the 'selectDirsForCleanup' method. This is a 'Buggy Method' identification because it points to where the error occurs, but not where the actual fix was made. The fix suggestion is 'Preventive' as it suggests adding a check to prevent the error, which aligns with the developer's fix of ensuring the directory exists. The problem location is also identified as 'Buggy Method' because it mentions 'selectDirsForCleanup', which is where the error occurs, but not the ground truth method where the fix was applied. There is no wrong information in the bug report as all details are relevant to the context of the bug."
        }
    },
    {
        "filename": "STORM-2197.json",
        "code_diff": {
            "storm-core.src.jvm.org.apache.storm.security.auth.ThriftClient.reconnect": {
                "code_before_change": "    public synchronized void reconnect() {\n        close();    \n        try {\n            TSocket socket = new TSocket(_host, _port);\n            if(_timeout!=null) {\n                socket.setTimeout(_timeout);\n            }\n\n            //locate login configuration \n            Configuration login_conf = AuthUtils.GetConfiguration(_conf);\n\n            //construct a transport plugin\n            ITransportPlugin transportPlugin = AuthUtils.GetTransportPlugin(_type, _conf, login_conf);\n\n            //TODO get this from type instead of hardcoding to Nimbus.\n            //establish client-server transport via plugin\n            //do retries if the connect fails\n            TBackoffConnect connectionRetry \n                = new TBackoffConnect(\n                                      Utils.getInt(_conf.get(Config.STORM_NIMBUS_RETRY_TIMES)),\n                                      Utils.getInt(_conf.get(Config.STORM_NIMBUS_RETRY_INTERVAL)),\n                                      Utils.getInt(_conf.get(Config.STORM_NIMBUS_RETRY_INTERVAL_CEILING)),\n                                      _retryForever);\n            _transport = connectionRetry.doConnectWithRetry(transportPlugin, socket, _host, _asUser);\n        } catch (IOException ex) {\n            throw new RuntimeException(ex);\n        }\n        _protocol = null;\n        if (_transport != null) {\n            _protocol = new  TBinaryProtocol(_transport);\n        }\n    }",
                "code_after_change": "    public synchronized void reconnect() {\n        close();\n        TSocket socket = null;\n        try {\n            socket = new TSocket(_host, _port);\n            if(_timeout!=null) {\n                socket.setTimeout(_timeout);\n            }\n\n            //locate login configuration \n            Configuration login_conf = AuthUtils.GetConfiguration(_conf);\n\n            //construct a transport plugin\n            ITransportPlugin transportPlugin = AuthUtils.GetTransportPlugin(_type, _conf, login_conf);\n\n            //TODO get this from type instead of hardcoding to Nimbus.\n            //establish client-server transport via plugin\n            //do retries if the connect fails\n            TBackoffConnect connectionRetry \n                = new TBackoffConnect(\n                                      Utils.getInt(_conf.get(Config.STORM_NIMBUS_RETRY_TIMES)),\n                                      Utils.getInt(_conf.get(Config.STORM_NIMBUS_RETRY_INTERVAL)),\n                                      Utils.getInt(_conf.get(Config.STORM_NIMBUS_RETRY_INTERVAL_CEILING)),\n                                      _retryForever);\n            _transport = connectionRetry.doConnectWithRetry(transportPlugin, socket, _host, _asUser);\n        } catch (Exception ex) {\n            // close the socket, which releases connection if it has created any.\n            if(socket != null) {\n                try {\n                    socket.close();\n                } catch (Exception e) {}\n            }\n            throw new RuntimeException(ex);\n        }\n        _protocol = null;\n        if (_transport != null) {\n            _protocol = new  TBinaryProtocol(_transport);\n        }\n    }"
            },
            "storm-core.src.jvm.org.apache.storm.security.auth.ThriftClient.transport": {
                "code_before_change": "    public synchronized TTransport transport() {\n        return _transport;\n    }",
                "code_after_change": "    public synchronized TTransport transport() {\n        return _transport;\n    }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Correct",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report precisely identifies the root cause as the TSocket instance in the ThriftClient not being closed during connection errors, which matches the ground truth method 'ThriftClient.reconnect'. The fix suggestion in the report is correct as it aligns with the developer's fix, which involves ensuring the TSocket is closed in a finally block. The problem location identification is precise as it mentions 'ThriftClient.reconnect', which is a ground truth method. There is no wrong information in the bug report as all details are relevant and accurate."
        }
    },
    {
        "filename": "STORM-1596.json",
        "code_diff": {
            "storm-core.src.jvm.org.apache.storm.security.auth.kerberos.AutoTGT.populateCredentials": {
                "code_before_change": "    public void populateCredentials(Map<String, String> credentials) {\n        //Log the user in and get the TGT\n        try {\n            Configuration login_conf = AuthUtils.GetConfiguration(conf);\n            ClientCallbackHandler client_callback_handler = new ClientCallbackHandler(login_conf);\n        \n            //login our user\n            Configuration.setConfiguration(login_conf); \n            LoginContext lc = new LoginContext(AuthUtils.LOGIN_CONTEXT_CLIENT, client_callback_handler);\n            try {\n                lc.login();\n                final Subject subject = lc.getSubject();\n                KerberosTicket tgt = getTGT(subject);\n\n                if (tgt == null) { //error\n                    throw new RuntimeException(\"Fail to verify user principal with section \\\"\"\n                            +AuthUtils.LOGIN_CONTEXT_CLIENT+\"\\\" in login configuration file \"+ login_conf);\n                }\n\n                if (!tgt.isForwardable()) {\n                    throw new RuntimeException(\"The TGT found is not forwardable\");\n                }\n\n                if (!tgt.isRenewable()) {\n                    throw new RuntimeException(\"The TGT found is not renewable\");\n                }\n\n                LOG.info(\"Pushing TGT for \"+tgt.getClient()+\" to topology.\");\n                saveTGT(tgt, credentials);\n            } finally {\n                lc.logout();\n            }\n        } catch (Exception e) {\n            throw new RuntimeException(e);\n        }\n    }",
                "code_after_change": "    public void populateCredentials(Map<String, String> credentials) {\n        //Log the user in and get the TGT\n        try {\n            Configuration login_conf = AuthUtils.GetConfiguration(conf);\n            ClientCallbackHandler client_callback_handler = new ClientCallbackHandler(login_conf);\n        \n            //login our user\n            Configuration.setConfiguration(login_conf); \n            LoginContext lc = new LoginContext(AuthUtils.LOGIN_CONTEXT_CLIENT, client_callback_handler);\n            try {\n                lc.login();\n                final Subject subject = lc.getSubject();\n                KerberosTicket tgt = getTGT(subject);\n\n                if (tgt == null) { //error\n                    throw new RuntimeException(\"Fail to verify user principal with section \\\"\"\n                            +AuthUtils.LOGIN_CONTEXT_CLIENT+\"\\\" in login configuration file \"+ login_conf);\n                }\n\n                if (!tgt.isForwardable()) {\n                    throw new RuntimeException(\"The TGT found is not forwardable\");\n                }\n\n                if (!tgt.isRenewable()) {\n                    throw new RuntimeException(\"The TGT found is not renewable\");\n                }\n\n                LOG.info(\"Pushing TGT for \"+tgt.getClient()+\" to topology.\");\n                saveTGT(tgt, credentials);\n            } finally {\n                lc.logout();\n            }\n        } catch (Exception e) {\n            throw new RuntimeException(e);\n        }\n    }"
            },
            "storm-core.src.jvm.org.apache.storm.security.auth.kerberos.AutoTGTKrb5LoginModule.abort": {
                "code_before_change": "    public boolean abort() throws LoginException {\n        if (isSucceeded() == false) {\n            return false;\n        } else {\n            return logout();\n        }\n    }",
                "code_after_change": "    public boolean abort() throws LoginException {\n        if (isSucceeded() == false) {\n            return false;\n        } else {\n            return logout();\n        }\n    }"
            },
            "storm-core.src.jvm.org.apache.storm.security.auth.AuthUtils.makeDigestPayload": {
                "code_before_change": "    public static String makeDigestPayload(Configuration login_config, String config_section) {\n        String username = null;\n        String password = null;\n        try {\n            Map<String, ?> results = AuthUtils.PullConfig(login_config, config_section);\n            username = (String)results.get(USERNAME);\n            password = (String)results.get(PASSWORD);\n        } catch (Exception e) {\n            LOG.error(\"Failed to pull username/password out of jaas conf\", e);\n        }\n\n        if(username == null || password == null) {\n            return null;\n        }\n\n        try {\n            MessageDigest digest = MessageDigest.getInstance(\"SHA-512\");\n            byte[] output = digest.digest((username + \":\" + password).getBytes());\n\n            StringBuilder builder = new StringBuilder();\n            for(byte b : output) {\n                builder.append(String.format(\"%02x\", b));\n            }\n            return builder.toString();\n        } catch (java.security.NoSuchAlgorithmException e) {\n            LOG.error(\"Cant run SHA-512 digest. Algorithm not available.\", e);\n            throw new RuntimeException(e);\n        }\n    }",
                "code_after_change": "    public static String makeDigestPayload(Configuration login_config, String config_section) {\n        String username = null;\n        String password = null;\n        try {\n            Map<String, ?> results = AuthUtils.PullConfig(login_config, config_section);\n            username = (String)results.get(USERNAME);\n            password = (String)results.get(PASSWORD);\n        } catch (Exception e) {\n            LOG.error(\"Failed to pull username/password out of jaas conf\", e);\n        }\n\n        if(username == null || password == null) {\n            return null;\n        }\n\n        try {\n            MessageDigest digest = MessageDigest.getInstance(\"SHA-512\");\n            byte[] output = digest.digest((username + \":\" + password).getBytes());\n\n            StringBuilder builder = new StringBuilder();\n            for(byte b : output) {\n                builder.append(String.format(\"%02x\", b));\n            }\n            return builder.toString();\n        } catch (java.security.NoSuchAlgorithmException e) {\n            LOG.error(\"Cant run SHA-512 digest. Algorithm not available.\", e);\n            throw new RuntimeException(e);\n        }\n    }"
            },
            "storm-core.src.jvm.org.apache.storm.security.auth.kerberos.AutoTGT.populateSubject": {
                "code_before_change": "    public void populateSubject(Subject subject, Map<String, String> credentials) {\n        populateSubjectWithTGT(subject, credentials);\n        loginHadoopUser(subject);\n    }",
                "code_after_change": "    public void populateSubject(Subject subject, Map<String, String> credentials) {\n        populateSubjectWithTGT(subject, credentials);\n        loginHadoopUser(subject);\n    }"
            },
            "storm-core.src.jvm.org.apache.storm.security.auth.kerberos.AutoTGT.saveTGT": {
                "code_before_change": "    public static void saveTGT(KerberosTicket tgt, Map<String, String> credentials) {\n        try {\n\n            byte[] bytes = AuthUtils.serializeKerberosTicket(tgt);\n            credentials.put(\"TGT\", DatatypeConverter.printBase64Binary(bytes));\n        } catch (Exception e) {\n            throw new RuntimeException(e);\n        }\n    }",
                "code_after_change": "    public static void saveTGT(KerberosTicket tgt, Map<String, String> credentials) {\n        try {\n\n            byte[] bytes = AuthUtils.serializeKerberosTicket(tgt);\n            credentials.put(\"TGT\", DatatypeConverter.printBase64Binary(bytes));\n        } catch (Exception e) {\n            throw new RuntimeException(e);\n        }\n    }"
            },
            "storm-core.src.jvm.org.apache.storm.security.auth.kerberos.AutoTGTKrb5LoginModule.commit": {
                "code_before_change": "    public boolean commit() throws LoginException {\n        if (isSucceeded() == false) {\n            return false;\n        }\n        if (subject == null || subject.isReadOnly()) {\n            kerbTicket = null;\n            throw new LoginException(\"Authentication failed because the Subject is invalid.\");\n        }\n        // Let us add the kerbClientPrinc and kerbTicket\n        // We need to clone the ticket because java.security.auth.kerberos assumes TGT is unique for each subject\n        // So, sharing TGT with multiple subjects can cause expired TGT to never refresh.\n        KerberosTicket kerbTicketCopy = AuthUtils.cloneKerberosTicket(kerbTicket);\n        subject.getPrivateCredentials().add(kerbTicketCopy);\n        subject.getPrincipals().add(getKerbTicketClient());\n        LOG.debug(\"Commit Succeeded.\");\n        return true;\n    }",
                "code_after_change": "    public boolean commit() throws LoginException {\n        if (isSucceeded() == false) {\n            return false;\n        }\n        if (subject == null || subject.isReadOnly()) {\n            kerbTicket = null;\n            throw new LoginException(\"Authentication failed because the Subject is invalid.\");\n        }\n        // Let us add the kerbClientPrinc and kerbTicket\n        // We need to clone the ticket because java.security.auth.kerberos assumes TGT is unique for each subject\n        // So, sharing TGT with multiple subjects can cause expired TGT to never refresh.\n        KerberosTicket kerbTicketCopy = AuthUtils.cloneKerberosTicket(kerbTicket);\n        subject.getPrivateCredentials().add(kerbTicketCopy);\n        subject.getPrincipals().add(getKerbTicketClient());\n        LOG.debug(\"Commit Succeeded.\");\n        return true;\n    }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Alternative Fix",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies a race condition with Kerberos Subject, which is related to the ground truth methods dealing with Kerberos tickets, but it does not precisely identify the root cause in the ground truth methods. The suggested fix of using ThreadLocal for Kerberos Subjects is an alternative fix that would likely resolve the issue by preventing shared access, though it differs from the actual fix. The problem location mentions methods in the stack trace context but not the exact ground truth methods. There is no wrong information as the report accurately describes the issue and its context."
        }
    },
    {
        "filename": "STORM-2142.json",
        "code_diff": {
            "storm-core.src.jvm.org.apache.storm.executor.error.ReportErrorAndDie.uncaughtException": {
                "code_before_change": "    public void uncaughtException(Thread t, Throwable e) {\n        try {\n            reportError.report(e);\n        } catch (Exception ex) {\n            LOG.error(\"Error while reporting error to cluster, proceeding with shutdown\", ex);\n        }\n        if (Utils.exceptionCauseIsInstanceOf(InterruptedException.class, e)\n                || Utils.exceptionCauseIsInstanceOf(java.io.InterruptedIOException.class, e)) {\n            LOG.info(\"Got interrupted exception shutting thread down...\");\n            suicideFn.run();\n        }\n    }",
                "code_after_change": "    public void uncaughtException(Thread t, Throwable e) {\n        try {\n            reportError.report(e);\n        } catch (Exception ex) {\n            LOG.error(\"Error while reporting error to cluster, proceeding with shutdown\", ex);\n        }\n        if (Utils.exceptionCauseIsInstanceOf(InterruptedException.class, e)\n                || Utils.exceptionCauseIsInstanceOf(java.io.InterruptedIOException.class, e)) {\n            LOG.info(\"Got interrupted exception shutting thread down...\");\n        } else {\n            suicideFn.run();\n        }\n    }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Correct",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report precisely identifies the root cause as the inconsistent handling of exceptions in the ReportErrorAndDie function, which matches the ground truth method. The fix suggestion is correct as it aligns with the developer's fix, which involves modifying the behavior of the ReportErrorAndDie function to handle exceptions appropriately. The problem location identification is partial because the report mentions the method 'consumeBatchToCursor' in the problem_location field, which is in the shared stack trace context but not the ground truth method. There is no wrong information in the bug report as all details are relevant and accurate."
        }
    },
    {
        "filename": "STORM-3084.json",
        "code_diff": {
            "storm-client.src.jvm.org.apache.storm.cluster.IStormClusterState.allSupervisorInfo": {
                "code_before_change": "    default Map<String, SupervisorInfo> allSupervisorInfo() {\n        return allSupervisorInfo(null);\n    }",
                "code_after_change": "    default Map<String, SupervisorInfo> allSupervisorInfo() {\n        return allSupervisorInfo(null);\n    }"
            },
            "storm-client.src.jvm.org.apache.storm.cluster.IStormClusterState.topologyBases": {
                "code_before_change": "    default Map<String, StormBase> topologyBases() {\n        Map<String, StormBase> stormBases = new HashMap<>();\n        for (String topologyId : activeStorms()) {\n            StormBase base = stormBase(topologyId, null);\n            if (base != null) { //rece condition with delete\n                stormBases.put(topologyId, base);\n            }\n        }\n        return stormBases;\n    }",
                "code_after_change": "    default Map<String, StormBase> topologyBases() {\n        Map<String, StormBase> stormBases = new HashMap<>();\n        for (String topologyId : activeStorms()) {\n            StormBase base = stormBase(topologyId, null);\n            if (base != null) { //race condition with delete\n                stormBases.put(topologyId, base);\n            }\n        }\n        return stormBases;\n    }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Preventive",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the root cause as a NullPointerException in the 'readAllSupervisorDetails' method, which is not the exact ground truth method but is mentioned in the same stack trace context as the ground truth methods. Therefore, it is classified as 'Partial' with 'Shared Stack Trace Context'. The fix suggestion involves adding null checks in the 'readAllSupervisorDetails' method, which is a preventive measure to avoid the NPE, but it does not match the developer's fix, which involved changes in different methods. The problem location mentions methods in the stack trace, but not the ground truth methods, so it is 'Partial' with 'Shared Stack Trace Context'. There is no wrong information in the bug report as all statements are relevant to the context of the bug."
        }
    },
    {
        "filename": "STORM-3118.json",
        "code_diff": {
            "storm-client.src.jvm.org.apache.storm.pacemaker.codec.ThriftEncoder.encodeNettySerializable": {
                "code_before_change": "    private HBMessage encodeNettySerializable(ByteBufAllocator alloc,\n        INettySerializable netty_message, HBServerMessageType mType) {\n\n        HBMessageData message_data = new HBMessageData();\n        HBMessage m = new HBMessage();\n        byte[] messageBuffer = new byte[netty_message.encodeLength()];\n        ByteBuf wrappedBuffer = Unpooled.wrappedBuffer(messageBuffer);\n        try {\n            netty_message.write(wrappedBuffer);\n            \n            message_data.set_message_blob(messageBuffer);\n            m.set_type(mType);\n            m.set_data(message_data);\n            return m;\n        } finally {\n            wrappedBuffer.release();\n        }\n    }",
                "code_after_change": "    private HBMessage encodeNettySerializable(ByteBufAllocator alloc,\n        INettySerializable netty_message, HBServerMessageType mType) {\n\n        HBMessageData message_data = new HBMessageData();\n        HBMessage m = new HBMessage();\n        byte[] messageBuffer = new byte[netty_message.encodeLength()];\n        ByteBuf wrappedBuffer = Unpooled.wrappedBuffer(messageBuffer);\n        try {\n            wrappedBuffer.resetWriterIndex();\n            netty_message.write(wrappedBuffer);\n            \n            message_data.set_message_blob(messageBuffer);\n            m.set_type(mType);\n            m.set_data(message_data);\n            return m;\n        } finally {\n            wrappedBuffer.release();\n        }\n    }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Buggy Method"
            },
            "fix_suggestion": "Preventive",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the root cause as the 'SaslMessageToken.write' method not checking for writable space, which is a related method but not the exact ground truth method. Therefore, it is classified as 'Partial' with 'Buggy Method' as the sub-category. The fix suggestion is 'Preventive' because it suggests adding a check for writable space, which would prevent the error but does not match the developer's fix. The problem location is 'Precise' because the 'problem_location' field mentions 'ThriftEncoder.encodeNettySerializable', which is the ground truth method. There is no wrong information in the bug report as all details are relevant to the context of the bug."
        }
    },
    {
        "filename": "STORM-2682.json",
        "code_diff": {
            "storm-server.src.main.java.org.apache.storm.daemon.supervisor.timer.UpdateBlobs.run": {
                "code_before_change": "    public void run() {\n        try {\n            Map<String, Object> conf = supervisor.getConf();\n            Set<String> downloadedStormIds = SupervisorUtils.readDownloadedTopologyIds(conf);\n            AtomicReference<Map<Long, LocalAssignment>> newAssignment = supervisor.getCurrAssignment();\n            Map<String, LocalAssignment> assignedStormIds = new HashMap<>();\n            for (LocalAssignment localAssignment : newAssignment.get().values()) {\n                assignedStormIds.put(localAssignment.get_topology_id(), localAssignment);\n            }\n            for (String stormId : downloadedStormIds) {\n                LocalAssignment la = assignedStormIds.get(stormId);\n                if (la != null) {\n                    String stormRoot = ConfigUtils.supervisorStormDistRoot(conf, stormId);\n                    LOG.debug(\"Checking Blob updates for storm topology id {} With target_dir: {}\", stormId, stormRoot);\n                    updateBlobsForTopology(conf, stormId, supervisor.getLocalizer(), la.get_owner());\n                }\n            }\n        } catch (Exception e) {\n            if (Utils.exceptionCauseIsInstanceOf(TTransportException.class, e)) {\n                LOG.error(\"Network error while updating blobs, will retry again later\", e);\n            } else if (Utils.exceptionCauseIsInstanceOf(NimbusLeaderNotFoundException.class, e)) {\n                LOG.error(\"Nimbus unavailable to update blobs, will retry again later\", e);\n            } else {\n                throw Utils.wrapInRuntime(e);\n            }\n        }\n    }",
                "code_after_change": "    public void run() {\n        try {\n            Map<String, Object> conf = supervisor.getConf();\n            Set<String> downloadedStormIds = SupervisorUtils.readDownloadedTopologyIds(conf);\n            AtomicReference<Map<Long, LocalAssignment>> newAssignment = supervisor.getCurrAssignment();\n            Map<String, LocalAssignment> assignedStormIds = new HashMap<>();\n            for (LocalAssignment localAssignment : newAssignment.get().values()) {\n                assignedStormIds.put(localAssignment.get_topology_id(), localAssignment);\n            }\n            for (String stormId : downloadedStormIds) {\n                LocalAssignment la = assignedStormIds.get(stormId);\n                if (la != null) {\n                    if (la.get_owner() == null) {\n                        //We got a case where the local assignment is not up to date, no point in going on...\n                        LOG.warn(\"The blobs will not be updated for {} until the local assignment is updated...\", stormId);\n                    } else {\n                        String stormRoot = ConfigUtils.supervisorStormDistRoot(conf, stormId);\n                        LOG.debug(\"Checking Blob updates for storm topology id {} With target_dir: {}\", stormId, stormRoot);\n                        updateBlobsForTopology(conf, stormId, supervisor.getLocalizer(), la.get_owner());\n                    }\n                }\n            }\n        } catch (Exception e) {\n            if (Utils.exceptionCauseIsInstanceOf(TTransportException.class, e)) {\n                LOG.error(\"Network error while updating blobs, will retry again later\", e);\n            } else if (Utils.exceptionCauseIsInstanceOf(NimbusLeaderNotFoundException.class, e)) {\n                LOG.error(\"Nimbus unavailable to update blobs, will retry again later\", e);\n            } else {\n                throw Utils.wrapInRuntime(e);\n            }\n        }\n    }"
            },
            "storm-server.src.main.java.org.apache.storm.daemon.supervisor.Slot.run": {
                "code_before_change": "    public void run() {\n        try {\n            while(!done) {\n                Set<TopoProfileAction> origProfileActions = new HashSet<>(profiling.get());\n                Set<TopoProfileAction> removed = new HashSet<>(origProfileActions);\n                \n                DynamicState nextState = \n                        stateMachineStep(dynamicState.withNewAssignment(newAssignment.get())\n                                .withProfileActions(origProfileActions, dynamicState.pendingStopProfileActions), staticState);\n\n                if (LOG.isDebugEnabled() || dynamicState.state != nextState.state) {\n                    LOG.info(\"STATE {} -> {}\", dynamicState, nextState);\n                }\n                //Save the current state for recovery\n                if ((nextState.currentAssignment != null && !nextState.currentAssignment.equals(dynamicState.currentAssignment)) ||\n                        (dynamicState.currentAssignment != null && !dynamicState.currentAssignment.equals(nextState.currentAssignment))) {\n                    LOG.info(\"SLOT {}: Changing current assignment from {} to {}\", staticState.port, dynamicState.currentAssignment, nextState.currentAssignment);\n                    saveNewAssignment(nextState.currentAssignment);\n                }\n                \n                // clean up the profiler actions that are not being processed\n                removed.removeAll(dynamicState.profileActions);\n                removed.removeAll(dynamicState.pendingStopProfileActions);\n                for (TopoProfileAction action: removed) {\n                    try {\n                        clusterState.deleteTopologyProfileRequests(action.topoId, action.request);\n                    } catch (Exception e) {\n                        LOG.error(\"Error trying to remove profiling request, it will be retried\", e);\n                    }\n                }\n                Set<TopoProfileAction> orig, copy;\n                do {\n                    orig = profiling.get();\n                    copy = new HashSet<>(orig);\n                    copy.removeAll(removed);\n                } while (!profiling.compareAndSet(orig, copy));\n                dynamicState = nextState;\n            }\n        } catch (Throwable e) {\n            if (!Utils.exceptionCauseIsInstanceOf(InterruptedException.class, e)) {\n                LOG.error(\"Error when processing event\", e);\n                Utils.exitProcess(20, \"Error when processing an event\");\n            }\n        }\n    }",
                "code_after_change": "    public void run() {\n        try {\n            while(!done) {\n                Set<TopoProfileAction> origProfileActions = new HashSet<>(profiling.get());\n                Set<TopoProfileAction> removed = new HashSet<>(origProfileActions);\n                \n                DynamicState nextState = \n                        stateMachineStep(dynamicState.withNewAssignment(newAssignment.get())\n                                .withProfileActions(origProfileActions, dynamicState.pendingStopProfileActions), staticState);\n\n                if (LOG.isDebugEnabled() || dynamicState.state != nextState.state) {\n                    LOG.info(\"STATE {} -> {}\", dynamicState, nextState);\n                }\n                //Save the current state for recovery\n                if ((nextState.currentAssignment != null && !nextState.currentAssignment.equals(dynamicState.currentAssignment)) ||\n                        (dynamicState.currentAssignment != null && !dynamicState.currentAssignment.equals(nextState.currentAssignment))) {\n                    LOG.info(\"SLOT {}: Changing current assignment from {} to {}\", staticState.port, dynamicState.currentAssignment, nextState.currentAssignment);\n                    saveNewAssignment(nextState.currentAssignment);\n                }\n\n                if (equivalent(nextState.newAssignment, nextState.currentAssignment)\n                    && nextState.currentAssignment != null && nextState.currentAssignment.get_owner() == null\n                    && nextState.newAssignment != null && nextState.newAssignment.get_owner() != null) {\n                    //This is an odd case for a rolling upgrade where the user on the old assignment may be null,\n                    // but not on the new one.  Although in all other ways they are the same.\n                    // If this happens we want to use the assignment with the owner.\n                    LOG.info(\"Updating assignment to save owner {}\", nextState.newAssignment.get_owner());\n                    saveNewAssignment(nextState.newAssignment);\n                    nextState = nextState.withCurrentAssignment(nextState.container, nextState.newAssignment);\n                }\n                \n                // clean up the profiler actions that are not being processed\n                removed.removeAll(dynamicState.profileActions);\n                removed.removeAll(dynamicState.pendingStopProfileActions);\n                for (TopoProfileAction action: removed) {\n                    try {\n                        clusterState.deleteTopologyProfileRequests(action.topoId, action.request);\n                    } catch (Exception e) {\n                        LOG.error(\"Error trying to remove profiling request, it will be retried\", e);\n                    }\n                }\n                Set<TopoProfileAction> orig, copy;\n                do {\n                    orig = profiling.get();\n                    copy = new HashSet<>(orig);\n                    copy.removeAll(removed);\n                } while (!profiling.compareAndSet(orig, copy));\n                dynamicState = nextState;\n            }\n        } catch (Throwable e) {\n            if (!Utils.exceptionCauseIsInstanceOf(InterruptedException.class, e)) {\n                LOG.error(\"Error when processing event\", e);\n                Utils.exitProcess(20, \"Error when processing an event\");\n            }\n        }\n    }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Direct Caller/Callee"
            },
            "fix_suggestion": "Preventive",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Direct Caller/Callee"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the root cause as a NullPointerException due to a null LocalizedResourceSet in the Localizer.updateBlobs method. This is a direct caller of the ground truth method UpdateBlobs.run, which is where the actual fix was made. The fix suggestion in the bug report is preventive, as it suggests adding a null check to prevent the exception, which aligns with the developer's fix that added a check for a null owner in the UpdateBlobs.run method. The problem location identification is partial because it points to Localizer.updateBlobs, which is a direct caller of the ground truth method. There is no wrong information in the bug report as all details are relevant to the context of the bug."
        }
    },
    {
        "filename": "STORM-3103.json",
        "code_diff": {
            "storm-client.src.jvm.org.apache.storm.StormTimer.run": {
                "code_before_change": "            public void run() {\n                func.run();\n                // This avoids a race condition with cancel-timer.\n                schedule(recurSecs, this, false, 0);\n            }",
                "code_after_change": "            public void run() {\n                func.run();\n                // This avoids a race condition with cancel-timer.\n                schedule(recurSecs, this, false, 0);\n            }"
            },
            "storm-client.src.jvm.org.apache.storm.StormTimer.schedule": {
                "code_before_change": "    public void schedule(int delaySecs, Runnable func, boolean checkActive, int jitterMs) {\n        scheduleMs(Time.secsToMillisLong(delaySecs), func, checkActive, jitterMs);\n    }",
                "code_after_change": "    public void schedule(int delaySecs, Runnable func, boolean checkActive, int jitterMs) {\n        scheduleMs(Time.secsToMillisLong(delaySecs), func, checkActive, jitterMs);\n    }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Preventive",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies a NullPointerException in the Nimbus class methods, which is mentioned in the stack trace but not in the ground truth methods. Therefore, the root cause identification is 'Partial' with 'Shared Stack Trace Context' as the sub-category. The fix suggestion is 'Preventive' because it suggests adding null checks, which would prevent the NullPointerException but does not match the actual fix. The problem location identification is also 'Partial' with 'Shared Stack Trace Context' as the sub-category because the methods mentioned are in the stack trace but not in the ground truth. There is no wrong information in the bug report as all details are relevant to the context of the bug."
        }
    }
]