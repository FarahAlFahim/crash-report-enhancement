[
    {
        "filename": "HIVE-10992.json",
        "code_diff": {
            "hcatalog.webhcat.svr.src.main.java.org.apache.hive.hcatalog.templeton.tool.TempletonControllerJob.run": {
                "code_before_change": "  public int run(String[] args) throws IOException, InterruptedException, ClassNotFoundException,\n          TException {\n    if(LOG.isDebugEnabled()) {\n      LOG.debug(\"Preparing to submit job: \" + Arrays.toString(args));\n    }\n    Configuration conf = getConf();\n\n    conf.set(JAR_ARGS_NAME, TempletonUtils.encodeArray(args));\n    String memoryMb = appConf.mapperMemoryMb();\n    if(memoryMb != null && memoryMb.length() != 0) {\n      conf.set(AppConfig.HADOOP_MAP_MEMORY_MB, memoryMb);\n    }\n    String amMemoryMB = appConf.amMemoryMb();\n    if (amMemoryMB != null && !amMemoryMB.isEmpty()) {\n      conf.set(AppConfig.HADOOP_MR_AM_MEMORY_MB, amMemoryMB);\n    }\n    String amJavaOpts = appConf.controllerAMChildOpts();\n    if (amJavaOpts != null && !amJavaOpts.isEmpty()) {\n      conf.set(AppConfig.HADOOP_MR_AM_JAVA_OPTS, amJavaOpts);\n    }\n\n    String user = UserGroupInformation.getCurrentUser().getShortUserName();\n    conf.set(\"user.name\", user);\n    Job job = new Job(conf);\n    job.setJarByClass(LaunchMapper.class);\n    job.setJobName(TempletonControllerJob.class.getSimpleName());\n    job.setMapperClass(LaunchMapper.class);\n    job.setMapOutputKeyClass(Text.class);\n    job.setMapOutputValueClass(Text.class);\n    job.setInputFormatClass(SingleInputFormat.class);\n\n    NullOutputFormat<NullWritable, NullWritable> of = new NullOutputFormat<NullWritable, NullWritable>();\n    job.setOutputFormatClass(of.getClass());\n    job.setNumReduceTasks(0);\n\n    JobClient jc = new JobClient(new JobConf(job.getConfiguration()));\n\n    Token<DelegationTokenIdentifier> mrdt = jc.getDelegationToken(new Text(\"mr token\"));\n    job.getCredentials().addToken(new Text(\"mr token\"), mrdt);\n\n    String metastoreTokenStrForm = addHMSToken(job, user);\n\n    job.submit();\n\n    submittedJobId = job.getJobID();\n    if(metastoreTokenStrForm != null) {\n      //so that it can be cancelled later from CompleteDelegator\n      DelegationTokenCache.getStringFormTokenCache().storeDelegationToken(\n              submittedJobId.toString(), metastoreTokenStrForm);\n      LOG.debug(\"Added metastore delegation token for jobId=\" + submittedJobId.toString() +\n              \" user=\" + user);\n    }\n    return 0;\n  }",
                "code_after_change": "  public int run(String[] args) throws IOException, InterruptedException, ClassNotFoundException,\n          TException {\n    if(LOG.isDebugEnabled()) {\n      LOG.debug(\"Preparing to submit job: \" + Arrays.toString(args));\n    }\n    Configuration conf = getConf();\n\n    conf.set(JAR_ARGS_NAME, TempletonUtils.encodeArray(args));\n    String memoryMb = appConf.mapperMemoryMb();\n    if(memoryMb != null && memoryMb.length() != 0) {\n      conf.set(AppConfig.HADOOP_MAP_MEMORY_MB, memoryMb);\n    }\n    String amMemoryMB = appConf.amMemoryMb();\n    if (amMemoryMB != null && !amMemoryMB.isEmpty()) {\n      conf.set(AppConfig.HADOOP_MR_AM_MEMORY_MB, amMemoryMB);\n    }\n    String amJavaOpts = appConf.controllerAMChildOpts();\n    if (amJavaOpts != null && !amJavaOpts.isEmpty()) {\n      conf.set(AppConfig.HADOOP_MR_AM_JAVA_OPTS, amJavaOpts);\n    }\n\n    String user = UserGroupInformation.getCurrentUser().getShortUserName();\n    conf.set(\"user.name\", user);\n    Job job = new Job(conf);\n    job.setJarByClass(LaunchMapper.class);\n    job.setJobName(TempletonControllerJob.class.getSimpleName());\n    job.setMapperClass(LaunchMapper.class);\n    job.setMapOutputKeyClass(Text.class);\n    job.setMapOutputValueClass(Text.class);\n    job.setInputFormatClass(SingleInputFormat.class);\n\n    NullOutputFormat<NullWritable, NullWritable> of = new NullOutputFormat<NullWritable, NullWritable>();\n    job.setOutputFormatClass(of.getClass());\n    job.setNumReduceTasks(0);\n\n    JobClient jc = new JobClient(new JobConf(job.getConfiguration()));\n\n    if(UserGroupInformation.isSecurityEnabled()) {\n      Token<DelegationTokenIdentifier> mrdt = jc.getDelegationToken(new Text(\"mr token\"));\n      job.getCredentials().addToken(new Text(\"mr token\"), mrdt);\n    }\n    String metastoreTokenStrForm = addHMSToken(job, user);\n\n    job.submit();\n\n    submittedJobId = job.getJobID();\n    if(metastoreTokenStrForm != null) {\n      //so that it can be cancelled later from CompleteDelegator\n      DelegationTokenCache.getStringFormTokenCache().storeDelegationToken(\n              submittedJobId.toString(), metastoreTokenStrForm);\n      LOG.debug(\"Added metastore delegation token for jobId=\" + submittedJobId.toString() +\n              \" user=\" + user);\n    }\n    return 0;\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Correct",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report precisely identifies the root cause by mentioning the TempletonControllerJob.run() method and the need to check if UserGroupInformation.isSecurityEnabled() before creating delegation tokens. This matches the ground truth method and the developer's fix. The fix suggestion is correct as it aligns with the actual code change, which adds a condition to check if security is enabled before obtaining the delegation token. The problem location is also precise as the report directly mentions the TempletonControllerJob.run() method, which is the ground truth method. There is no wrong information in the bug report as all details are relevant and accurate."
        }
    },
    {
        "filename": "HIVE-16450.json",
        "code_diff": {
            "metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.run": {
                "code_before_change": "    public T run(boolean initTable) throws MetaException, NoSuchObjectException {\n      try {\n        start(initTable);\n        if (doUseDirectSql) {\n          try {\n            directSql.prepareTxn();\n            this.results = getSqlResult(this);\n          } catch (Exception ex) {\n            handleDirectSqlError(ex);\n          }\n        }\n        // Note that this will be invoked in 2 cases:\n        //    1) DirectSQL was disabled to start with;\n        //    2) DirectSQL threw and was disabled in handleDirectSqlError.\n        if (!doUseDirectSql) {\n          this.results = getJdoResult(this);\n        }\n        return commit();\n      } catch (NoSuchObjectException ex) {\n        throw ex;\n      } catch (MetaException ex) {\n        throw ex;\n      } catch (Exception ex) {\n        LOG.error(\"\", ex);\n        throw new MetaException(ex.getMessage());\n      } finally {\n        close();\n      }\n    }",
                "code_after_change": "    public T run(boolean initTable) throws MetaException, NoSuchObjectException {\n      try {\n        start(initTable);\n        if (doUseDirectSql) {\n          try {\n            directSql.prepareTxn();\n            this.results = getSqlResult(this);\n          } catch (Exception ex) {\n            handleDirectSqlError(ex);\n          }\n        }\n        // Note that this will be invoked in 2 cases:\n        //    1) DirectSQL was disabled to start with;\n        //    2) DirectSQL threw and was disabled in handleDirectSqlError.\n        if (!doUseDirectSql) {\n          this.results = getJdoResult(this);\n        }\n        return commit();\n      } catch (NoSuchObjectException ex) {\n        throw ex;\n      } catch (MetaException ex) {\n        throw ex;\n      } catch (Exception ex) {\n        LOG.error(\"\", ex);\n        throw MetaStoreUtils.newMetaException(ex);\n      } finally {\n        close();\n      }\n    }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.Deadline.stopTimer": {
                "code_before_change": "  public static void stopTimer() throws MetaException {\n    Deadline deadline = getCurrentDeadline();\n    if (deadline != null) {\n      deadline.startTime = NO_DEADLINE;\n      deadline.method = null;\n    } else {\n      throw newMetaException(new DeadlineException(\"The threadlocal Deadline is null,\" +\n          \" please register it firstly.\"));\n    }\n  }",
                "code_after_change": "  public static void stopTimer() throws MetaException {\n    Deadline deadline = getCurrentDeadline();\n    if (deadline != null) {\n      deadline.startTime = NO_DEADLINE;\n      deadline.method = null;\n    } else {\n      throw MetaStoreUtils.newMetaException(new DeadlineException(\"The threadlocal Deadline is null,\" +\n          \" please register it first.\"));\n    }\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.Deadline.checkTimeout": {
                "code_before_change": "  public static void checkTimeout() throws MetaException {\n    Deadline deadline = getCurrentDeadline();\n    if (deadline != null) {\n      deadline.check();\n    } else {\n      throw newMetaException(new DeadlineException(\"The threadlocal Deadline is null,\" +\n          \" please register it first.\"));\n    }\n  }",
                "code_after_change": "  public static void checkTimeout() throws MetaException {\n    Deadline deadline = getCurrentDeadline();\n    if (deadline != null) {\n      deadline.check();\n    } else {\n      throw MetaStoreUtils.newMetaException(new DeadlineException(\"The threadlocal Deadline is null,\" +\n          \" please register it first.\"));\n    }\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.Deadline.registerIfNot": {
                "code_before_change": "  public static void registerIfNot(long timeout) {\n    if (getCurrentDeadline() == null) {\n      setCurrentDeadline(new Deadline(timeout));\n    }\n  }",
                "code_after_change": "  public static void registerIfNot(long timeout) {\n    if (getCurrentDeadline() == null) {\n      setCurrentDeadline(new Deadline(timeout));\n    }\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.getMTableColumnStatistics": {
                "code_before_change": "  private List<MTableColumnStatistics> getMTableColumnStatistics(Table table, List<String> colNames, QueryWrapper queryWrapper)\n      throws MetaException {\n    if (colNames == null || colNames.isEmpty()) {\n      return null;\n    }\n\n    boolean committed = false;\n\n    try {\n      openTransaction();\n\n      List<MTableColumnStatistics> result = null;\n      validateTableCols(table, colNames);\n      Query query = queryWrapper.query = pm.newQuery(MTableColumnStatistics.class);\n      String filter = \"tableName == t1 && dbName == t2 && (\";\n      String paramStr = \"java.lang.String t1, java.lang.String t2\";\n      Object[] params = new Object[colNames.size() + 2];\n      params[0] = table.getTableName();\n      params[1] = table.getDbName();\n      for (int i = 0; i < colNames.size(); ++i) {\n        filter += ((i == 0) ? \"\" : \" || \") + \"colName == c\" + i;\n        paramStr += \", java.lang.String c\" + i;\n        params[i + 2] = colNames.get(i);\n      }\n      filter += \")\";\n      query.setFilter(filter);\n      query.declareParameters(paramStr);\n      result = (List<MTableColumnStatistics>) query.executeWithArray(params);\n      pm.retrieveAll(result);\n      if (result.size() > colNames.size()) {\n        throw new MetaException(\"Unexpected \" + result.size() + \" statistics for \"\n            + colNames.size() + \" columns\");\n      }\n      committed = commitTransaction();\n      return result;\n    } catch (Exception ex) {\n      LOG.error(\"Error retrieving statistics via jdo\", ex);\n      if (ex instanceof MetaException) {\n        throw (MetaException) ex;\n      }\n      throw new MetaException(ex.getMessage());\n    } finally {\n      if (!committed) {\n        rollbackTransaction();\n      }\n    }\n  }",
                "code_after_change": "  private List<MTableColumnStatistics> getMTableColumnStatistics(Table table, List<String> colNames, QueryWrapper queryWrapper)\n      throws MetaException {\n    if (colNames == null || colNames.isEmpty()) {\n      return null;\n    }\n\n    boolean committed = false;\n    try {\n      openTransaction();\n\n      List<MTableColumnStatistics> result = null;\n      validateTableCols(table, colNames);\n      Query query = queryWrapper.query = pm.newQuery(MTableColumnStatistics.class);\n      String filter = \"tableName == t1 && dbName == t2 && (\";\n      String paramStr = \"java.lang.String t1, java.lang.String t2\";\n      Object[] params = new Object[colNames.size() + 2];\n      params[0] = table.getTableName();\n      params[1] = table.getDbName();\n      for (int i = 0; i < colNames.size(); ++i) {\n        filter += ((i == 0) ? \"\" : \" || \") + \"colName == c\" + i;\n        paramStr += \", java.lang.String c\" + i;\n        params[i + 2] = colNames.get(i);\n      }\n      filter += \")\";\n      query.setFilter(filter);\n      query.declareParameters(paramStr);\n      result = (List<MTableColumnStatistics>) query.executeWithArray(params);\n      pm.retrieveAll(result);\n      if (result.size() > colNames.size()) {\n        throw new MetaException(\"Unexpected \" + result.size() + \" statistics for \"\n            + colNames.size() + \" columns\");\n      }\n      committed = commitTransaction();\n      return result;\n    } catch (Exception ex) {\n      LOG.error(\"Error retrieving statistics via jdo\", ex);\n      if (ex instanceof MetaException) {\n        throw (MetaException) ex;\n      }\n      throw MetaStoreUtils.newMetaException(ex);\n    } finally {\n      if (!committed) {\n        rollbackTransaction();\n      }\n    }\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.getPartitionColStats": {
                "code_before_change": "  private Map<String, MTableColumnStatistics> getPartitionColStats(Table table,\n     List<String> colNames) throws NoSuchObjectException, MetaException {\n    Map<String, MTableColumnStatistics> statsMap = Maps.newHashMap();\n    QueryWrapper queryWrapper = new QueryWrapper();\n    try {\n      List<MTableColumnStatistics> stats = getMTableColumnStatistics(table,\n          colNames, queryWrapper);\n      for(MTableColumnStatistics cStat : stats) {\n        statsMap.put(cStat.getColName(), cStat);\n      }\n    } finally {\n      queryWrapper.close();\n    }\n    return statsMap;\n  }",
                "code_after_change": "  private Map<String, MTableColumnStatistics> getPartitionColStats(Table table,\n     List<String> colNames) throws NoSuchObjectException, MetaException {\n    Map<String, MTableColumnStatistics> statsMap = Maps.newHashMap();\n    QueryWrapper queryWrapper = new QueryWrapper();\n    try {\n      List<MTableColumnStatistics> stats = getMTableColumnStatistics(table,\n          colNames, queryWrapper);\n      if (stats != null) {\n        for(MTableColumnStatistics cStat : stats) {\n          statsMap.put(cStat.getColName(), cStat);\n        }\n      }\n    } finally {\n      queryWrapper.close();\n    }\n    return statsMap;\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.Deadline.check": {
                "code_before_change": "  private void check() throws MetaException{\n    try {\n      if (startTime == NO_DEADLINE) {\n        throw new DeadlineException(\"Should execute startTimer() method before \" +\n            \"checkTimeout. Error happens in method: \" + method);\n      }\n      long elapsedTime = System.nanoTime() - startTime;\n      if (elapsedTime > timeoutNanos) {\n        throw new DeadlineException(\"Timeout when executing method: \" + method + \"; \"\n            + (elapsedTime / 1000000L) + \"ms exceeds \" + (timeoutNanos / 1000000L)  + \"ms\");\n      }\n    } catch (DeadlineException e) {\n      throw newMetaException(e);\n    }\n  }",
                "code_after_change": "  private void check() throws MetaException{\n    try {\n      if (startTime == NO_DEADLINE) {\n        throw new DeadlineException(\"Should execute startTimer() method before \" +\n            \"checkTimeout. Error happens in method: \" + method);\n      }\n      long elapsedTime = System.nanoTime() - startTime;\n      if (elapsedTime > timeoutNanos) {\n        throw new DeadlineException(\"Timeout when executing method: \" + method + \"; \"\n            + (elapsedTime / 1000000L) + \"ms exceeds \" + (timeoutNanos / 1000000L)  + \"ms\");\n      }\n    } catch (DeadlineException e) {\n      throw MetaStoreUtils.newMetaException(e);\n    }\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.getJdoResult": {
                "code_before_change": "      protected Database getJdoResult(GetHelper<Database> ctx) throws MetaException, NoSuchObjectException {\n        return getJDODatabase(dbName);\n      }",
                "code_after_change": "      protected Database getJdoResult(GetHelper<Database> ctx) throws MetaException, NoSuchObjectException {\n        return getJDODatabase(dbName);\n      }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.Deadline.resetTimeout": {
                "code_before_change": "  public static void resetTimeout(long timeoutMs) throws MetaException {\n    if (timeoutMs <= 0) {\n      throw newMetaException(new DeadlineException(\"The reset timeout value should be \" +\n          \"larger than 0: \" + timeoutMs));\n    }\n    Deadline deadline = getCurrentDeadline();\n    if (deadline != null) {\n      deadline.timeoutNanos = timeoutMs * 1000000L;\n    } else {\n      throw newMetaException(new DeadlineException(\"The threadlocal Deadline is null,\" +\n          \" please register it firstly.\"));\n    }\n  }",
                "code_after_change": "  public static void resetTimeout(long timeoutMs) throws MetaException {\n    if (timeoutMs <= 0) {\n      throw MetaStoreUtils.newMetaException(new DeadlineException(\"The reset timeout value should be \" +\n          \"larger than 0: \" + timeoutMs));\n    }\n    Deadline deadline = getCurrentDeadline();\n    if (deadline != null) {\n      deadline.timeoutNanos = timeoutMs * 1000000L;\n    } else {\n      throw MetaStoreUtils.newMetaException(new DeadlineException(\"The threadlocal Deadline is null,\" +\n          \" please register it first.\"));\n    }\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.alterPartition": {
                "code_before_change": "  public void alterPartition(String dbname, String name, List<String> part_vals, Partition newPart)\n      throws InvalidObjectException, MetaException {\n    boolean success = false;\n    Exception e = null;\n    try {\n      openTransaction();\n      alterPartitionNoTxn(dbname, name, part_vals, newPart);\n      // commit the changes\n      success = commitTransaction();\n    } catch (Exception exception) {\n      e = exception;\n    } finally {\n      if (!success) {\n        rollbackTransaction();\n        MetaException metaException = new MetaException(\n            \"The transaction for alter partition did not commit successfully.\");\n        if (e != null) {\n          metaException.initCause(e);\n        }\n        throw metaException;\n      }\n    }\n  }",
                "code_after_change": "  public void alterPartition(String dbname, String name, List<String> part_vals, Partition newPart)\n      throws InvalidObjectException, MetaException {\n    boolean success = false;\n    Exception e = null;\n    try {\n      openTransaction();\n      alterPartitionNoTxn(dbname, name, part_vals, newPart);\n      // commit the changes\n      success = commitTransaction();\n    } catch (Exception exception) {\n      e = exception;\n    } finally {\n      if (!success) {\n        rollbackTransaction();\n        throw MetaStoreUtils.newMetaException(\n            \"The transaction for alter partition did not commit successfully.\", e);\n      }\n    }\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.getMSchemaVersion": {
                "code_before_change": "  private MVersionTable getMSchemaVersion() throws NoSuchObjectException, MetaException {\n    boolean committed = false;\n    Query query = null;\n    List<MVersionTable> mVerTables = new ArrayList<MVersionTable>();\n    try {\n      openTransaction();\n      query = pm.newQuery(MVersionTable.class);\n      try {\n        mVerTables = (List<MVersionTable>) query.execute();\n        pm.retrieveAll(mVerTables);\n      } catch (JDODataStoreException e) {\n        if (e.getCause() instanceof MissingTableException) {\n          throw new MetaException(\"Version table not found. \" + \"The metastore is not upgraded to \"\n              + MetaStoreSchemaInfo.getHiveSchemaVersion());\n        } else {\n          throw e;\n        }\n      }\n      committed = commitTransaction();\n      if (mVerTables.isEmpty()) {\n        throw new NoSuchObjectException(\"No matching version found\");\n      }\n      if (mVerTables.size() > 1) {\n        String msg = \"Metastore contains multiple versions (\" + mVerTables.size() + \") \";\n        for (MVersionTable version : mVerTables) {\n          msg +=\n              \"[ version = \" + version.getSchemaVersion() + \", comment = \"\n                  + version.getVersionComment() + \" ] \";\n        }\n        throw new MetaException(msg.trim());\n      }\n      return mVerTables.get(0);\n    } finally {\n      if (!committed) {\n        rollbackTransaction();\n      }\n      if (query != null) {\n        query.closeAll();\n      }\n    }\n  }",
                "code_after_change": "  private MVersionTable getMSchemaVersion() throws NoSuchObjectException, MetaException {\n    boolean committed = false;\n    Query query = null;\n    List<MVersionTable> mVerTables = new ArrayList<MVersionTable>();\n    try {\n      openTransaction();\n      query = pm.newQuery(MVersionTable.class);\n      try {\n        mVerTables = (List<MVersionTable>) query.execute();\n        pm.retrieveAll(mVerTables);\n      } catch (JDODataStoreException e) {\n        if (e.getCause() instanceof MissingTableException) {\n          throw new MetaException(\"Version table not found. \" + \"The metastore is not upgraded to \"\n              + MetaStoreSchemaInfo.getHiveSchemaVersion());\n        } else {\n          throw MetaStoreUtils.newMetaException(e);\n        }\n      }\n      committed = commitTransaction();\n      if (mVerTables.isEmpty()) {\n        throw new NoSuchObjectException(\"No matching version found\");\n      }\n      if (mVerTables.size() > 1) {\n        String msg = \"Metastore contains multiple versions (\" + mVerTables.size() + \") \";\n        for (MVersionTable version : mVerTables) {\n          msg +=\n              \"[ version = \" + version.getSchemaVersion() + \", comment = \"\n                  + version.getVersionComment() + \" ] \";\n        }\n        throw new MetaException(msg.trim());\n      }\n      return mVerTables.get(0);\n    } finally {\n      rollbackAndCleanup(committed, query);\n    }\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.getMPartitionColumnStatistics": {
                "code_before_change": "  private List<MPartitionColumnStatistics> getMPartitionColumnStatistics(\n      Table table, List<String> partNames, List<String> colNames, QueryWrapper queryWrapper)\n          throws NoSuchObjectException, MetaException {\n    boolean committed = false;\n\n    try {\n      openTransaction();\n      // We are not going to verify SD for each partition. Just verify for the table.\n      validateTableCols(table, colNames);\n      Query query = queryWrapper.query = pm.newQuery(MPartitionColumnStatistics.class);\n      String paramStr = \"java.lang.String t1, java.lang.String t2\";\n      String filter = \"tableName == t1 && dbName == t2 && (\";\n      Object[] params = new Object[colNames.size() + partNames.size() + 2];\n      int i = 0;\n      params[i++] = table.getTableName();\n      params[i++] = table.getDbName();\n      int firstI = i;\n      for (String s : partNames) {\n        filter += ((i == firstI) ? \"\" : \" || \") + \"partitionName == p\" + i;\n        paramStr += \", java.lang.String p\" + i;\n        params[i++] = s;\n      }\n      filter += \") && (\";\n      firstI = i;\n      for (String s : colNames) {\n        filter += ((i == firstI) ? \"\" : \" || \") + \"colName == c\" + i;\n        paramStr += \", java.lang.String c\" + i;\n        params[i++] = s;\n      }\n      filter += \")\";\n      query.setFilter(filter);\n      query.declareParameters(paramStr);\n      query.setOrdering(\"partitionName ascending\");\n      @SuppressWarnings(\"unchecked\")\n      List<MPartitionColumnStatistics> result =\n          (List<MPartitionColumnStatistics>) query.executeWithArray(params);\n      pm.retrieveAll(result);\n      committed = commitTransaction();\n      return result;\n    } catch (Exception ex) {\n      LOG.error(\"Error retrieving statistics via jdo\", ex);\n      if (ex instanceof MetaException) {\n        throw (MetaException) ex;\n      }\n      throw new MetaException(ex.getMessage());\n    } finally {\n      if (!committed) {\n        rollbackTransaction();\n        return Lists.newArrayList();\n      }\n    }\n  }",
                "code_after_change": "  private List<MPartitionColumnStatistics> getMPartitionColumnStatistics(\n      Table table, List<String> partNames, List<String> colNames, QueryWrapper queryWrapper)\n          throws NoSuchObjectException, MetaException {\n    boolean committed = false;\n\n    try {\n      openTransaction();\n      // We are not going to verify SD for each partition. Just verify for the table.\n      // ToDo: we need verify the partition column instead\n      try {\n        validateTableCols(table, colNames);\n      } catch (MetaException me) {\n        LOG.warn(\"The table does not have the same column definition as its partition.\");\n      }\n      Query query = queryWrapper.query = pm.newQuery(MPartitionColumnStatistics.class);\n      String paramStr = \"java.lang.String t1, java.lang.String t2\";\n      String filter = \"tableName == t1 && dbName == t2 && (\";\n      Object[] params = new Object[colNames.size() + partNames.size() + 2];\n      int i = 0;\n      params[i++] = table.getTableName();\n      params[i++] = table.getDbName();\n      int firstI = i;\n      for (String s : partNames) {\n        filter += ((i == firstI) ? \"\" : \" || \") + \"partitionName == p\" + i;\n        paramStr += \", java.lang.String p\" + i;\n        params[i++] = s;\n      }\n      filter += \") && (\";\n      firstI = i;\n      for (String s : colNames) {\n        filter += ((i == firstI) ? \"\" : \" || \") + \"colName == c\" + i;\n        paramStr += \", java.lang.String c\" + i;\n        params[i++] = s;\n      }\n      filter += \")\";\n      query.setFilter(filter);\n      query.declareParameters(paramStr);\n      query.setOrdering(\"partitionName ascending\");\n      @SuppressWarnings(\"unchecked\")\n      List<MPartitionColumnStatistics> result =\n          (List<MPartitionColumnStatistics>) query.executeWithArray(params);\n      pm.retrieveAll(result);\n      committed = commitTransaction();\n      return result;\n    } catch (Exception ex) {\n      LOG.error(\"Error retrieving statistics via jdo\", ex);\n      if (ex instanceof MetaException) {\n        throw (MetaException) ex;\n      }\n      throw MetaStoreUtils.newMetaException(ex);\n    } finally {\n      if (!committed) {\n        rollbackTransaction();\n        return Lists.newArrayList();\n      }\n    }\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.handleDirectSqlError": {
                "code_before_change": "    private void handleDirectSqlError(Exception ex) throws MetaException, NoSuchObjectException {\n      String message = null;\n      try {\n        message = generateShorterMessage(ex);\n      } catch (Throwable t) {\n        message = ex.toString() + \"; error building a better message: \" + t.getMessage();\n      }\n      LOG.warn(message); // Don't log the exception, people just get confused.\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Full DirectSQL callstack for debugging (note: this is not an error)\", ex);\n      }\n      if (!allowJdo) {\n        if (ex instanceof MetaException) {\n          throw (MetaException)ex;\n        }\n        throw new MetaException(ex.getMessage());\n      }\n      if (!isInTxn) {\n        JDOException rollbackEx = null;\n        try {\n          rollbackTransaction();\n        } catch (JDOException jex) {\n          rollbackEx = jex;\n        }\n        if (rollbackEx != null) {\n          // Datanucleus propagates some pointless exceptions and rolls back in the finally.\n          if (currentTransaction != null && currentTransaction.isActive()) {\n            throw rollbackEx; // Throw if the tx wasn't rolled back.\n          }\n          LOG.info(\"Ignoring exception, rollback succeeded: \" + rollbackEx.getMessage());\n        }\n\n        start = doTrace ? System.nanoTime() : 0;\n        openTransaction();\n        if (table != null) {\n          table = ensureGetTable(dbName, tblName);\n        }\n      } else {\n        start = doTrace ? System.nanoTime() : 0;\n      }\n\n      Metrics metrics = MetricsFactory.getInstance();\n      if (metrics != null) {\n        try {\n          metrics.incrementCounter(MetricsConstant.DIRECTSQL_ERRORS);\n        } catch (Exception e) {\n          LOG.warn(\"Error reporting Direct SQL errors to metrics system\", e);\n        }\n      }\n\n      doUseDirectSql = false;\n    }",
                "code_after_change": "    private void handleDirectSqlError(Exception ex) throws MetaException, NoSuchObjectException {\n      String message = null;\n      try {\n        message = generateShorterMessage(ex);\n      } catch (Throwable t) {\n        message = ex.toString() + \"; error building a better message: \" + t.getMessage();\n      }\n      LOG.warn(message); // Don't log the exception, people just get confused.\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Full DirectSQL callstack for debugging (note: this is not an error)\", ex);\n      }\n      if (!allowJdo) {\n        if (ex instanceof MetaException) {\n          throw (MetaException)ex;\n        }\n        throw MetaStoreUtils.newMetaException(ex);\n      }\n      if (!isInTxn) {\n        JDOException rollbackEx = null;\n        try {\n          rollbackTransaction();\n        } catch (JDOException jex) {\n          rollbackEx = jex;\n        }\n        if (rollbackEx != null) {\n          // Datanucleus propagates some pointless exceptions and rolls back in the finally.\n          if (currentTransaction != null && currentTransaction.isActive()) {\n            throw rollbackEx; // Throw if the tx wasn't rolled back.\n          }\n          LOG.info(\"Ignoring exception, rollback succeeded: \" + rollbackEx.getMessage());\n        }\n\n        start = doTrace ? System.nanoTime() : 0;\n        openTransaction();\n        if (table != null) {\n          table = ensureGetTable(dbName, tblName);\n        }\n      } else {\n        start = doTrace ? System.nanoTime() : 0;\n      }\n\n      Metrics metrics = MetricsFactory.getInstance();\n      if (metrics != null) {\n        try {\n          metrics.incrementCounter(MetricsConstant.DIRECTSQL_ERRORS);\n        } catch (Exception e) {\n          LOG.warn(\"Error reporting Direct SQL errors to metrics system\", e);\n        }\n      }\n\n      doUseDirectSql = false;\n    }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.alterPartitions": {
                "code_before_change": "  public void alterPartitions(String dbname, String name, List<List<String>> part_vals,\n      List<Partition> newParts) throws InvalidObjectException, MetaException {\n    boolean success = false;\n    Exception e = null;\n    try {\n      openTransaction();\n      Iterator<List<String>> part_val_itr = part_vals.iterator();\n      for (Partition tmpPart: newParts) {\n        List<String> tmpPartVals = part_val_itr.next();\n        alterPartitionNoTxn(dbname, name, tmpPartVals, tmpPart);\n      }\n      // commit the changes\n      success = commitTransaction();\n    } catch (Exception exception) {\n      e = exception;\n    } finally {\n      if (!success) {\n        rollbackTransaction();\n        MetaException metaException = new MetaException(\n            \"The transaction for alter partition did not commit successfully.\");\n        if (e != null) {\n          metaException.initCause(e);\n        }\n        throw metaException;\n      }\n    }\n  }",
                "code_after_change": "  public void alterPartitions(String dbname, String name, List<List<String>> part_vals,\n      List<Partition> newParts) throws InvalidObjectException, MetaException {\n    boolean success = false;\n    Exception e = null;\n    try {\n      openTransaction();\n      Iterator<List<String>> part_val_itr = part_vals.iterator();\n      for (Partition tmpPart: newParts) {\n        List<String> tmpPartVals = part_val_itr.next();\n        alterPartitionNoTxn(dbname, name, tmpPartVals, tmpPart);\n      }\n      // commit the changes\n      success = commitTransaction();\n    } catch (Exception exception) {\n      e = exception;\n    } finally {\n      if (!success) {\n        rollbackTransaction();\n        throw MetaStoreUtils.newMetaException(\n            \"The transaction for alter partition did not commit successfully.\", e);\n      }\n    }\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal": {
                "code_before_change": "  public Result invokeInternal(final Object proxy, final Method method, final Object[] args) throws Throwable {\n\n    boolean gotNewConnectUrl = false;\n    boolean reloadConf = HiveConf.getBoolVar(origConf,\n        HiveConf.ConfVars.HMSHANDLERFORCERELOADCONF);\n    long retryInterval = HiveConf.getTimeVar(origConf,\n        HiveConf.ConfVars.HMSHANDLERINTERVAL, TimeUnit.MILLISECONDS);\n    int retryLimit = HiveConf.getIntVar(origConf,\n        HiveConf.ConfVars.HMSHANDLERATTEMPTS);\n    long timeout = HiveConf.getTimeVar(origConf,\n        HiveConf.ConfVars.METASTORE_CLIENT_SOCKET_TIMEOUT, TimeUnit.MILLISECONDS);\n\n    Deadline.registerIfNot(timeout);\n\n    if (reloadConf) {\n      MetaStoreInit.updateConnectionURL(origConf, getActiveConf(),\n        null, metaStoreInitData);\n    }\n\n    int retryCount = 0;\n    Throwable caughtException = null;\n    while (true) {\n      try {\n        if (reloadConf || gotNewConnectUrl) {\n          baseHandler.setConf(getActiveConf());\n        }\n        Object object = null;\n        boolean isStarted = Deadline.startTimer(method.getName());\n        try {\n          object = method.invoke(baseHandler, args);\n        } finally {\n          if (isStarted) {\n            Deadline.stopTimer();\n          }\n        }\n        return new Result(object, retryCount);\n\n      } catch (javax.jdo.JDOException e) {\n        caughtException = e;\n      } catch (UndeclaredThrowableException e) {\n        if (e.getCause() != null) {\n          if (e.getCause() instanceof javax.jdo.JDOException) {\n            // Due to reflection, the jdo exception is wrapped in\n            // invocationTargetException\n            caughtException = e.getCause();\n          } else if (e.getCause() instanceof MetaException && e.getCause().getCause() != null\n              && e.getCause().getCause() instanceof javax.jdo.JDOException) {\n            // The JDOException may be wrapped further in a MetaException\n            caughtException = e.getCause().getCause();\n          } else {\n            LOG.error(ExceptionUtils.getStackTrace(e.getCause()));\n            throw e.getCause();\n          }\n        } else {\n          LOG.error(ExceptionUtils.getStackTrace(e));\n          throw e;\n        }\n      } catch (InvocationTargetException e) {\n        if (e.getCause() instanceof javax.jdo.JDOException) {\n          // Due to reflection, the jdo exception is wrapped in\n          // invocationTargetException\n          caughtException = e.getCause();\n        } else if (e.getCause() instanceof NoSuchObjectException || e.getTargetException().getCause() instanceof NoSuchObjectException) {\n          String methodName = method.getName();\n          if (!methodName.startsWith(\"get_database\") && !methodName.startsWith(\"get_table\")\n              && !methodName.startsWith(\"get_partition\") && !methodName.startsWith(\"get_function\")) {\n            LOG.error(ExceptionUtils.getStackTrace(e.getCause()));\n          }\n          throw e.getCause();\n        } else if (e.getCause() instanceof MetaException && e.getCause().getCause() != null) {\n          if (e.getCause().getCause() instanceof javax.jdo.JDOException ||\n              e.getCause().getCause() instanceof NucleusException) {\n            // The JDOException or the Nucleus Exception may be wrapped further in a MetaException\n            caughtException = e.getCause().getCause();\n          } else if (e.getCause().getCause() instanceof DeadlineException) {\n            // The Deadline Exception needs no retry and be thrown immediately.\n            Deadline.clear();\n            LOG.error(\"Error happens in method \" + method.getName() + \": \" +\n                ExceptionUtils.getStackTrace(e.getCause()));\n            throw e.getCause();\n          } else {\n            LOG.error(ExceptionUtils.getStackTrace(e.getCause()));\n            throw e.getCause();\n          }\n        } else {\n          LOG.error(ExceptionUtils.getStackTrace(e.getCause()));\n          throw e.getCause();\n        }\n      }\n\n      if (retryCount >= retryLimit) {\n        LOG.error(\"HMSHandler Fatal error: \" + ExceptionUtils.getStackTrace(caughtException));\n        MetaException me = new MetaException(caughtException.getMessage());\n        me.initCause(caughtException);\n        throw me;\n      }\n\n      assert (retryInterval >= 0);\n      retryCount++;\n      LOG.error(\n        String.format(\n          \"Retrying HMSHandler after %d ms (attempt %d of %d)\", retryInterval, retryCount, retryLimit) +\n          \" with error: \" + ExceptionUtils.getStackTrace(caughtException));\n\n      Thread.sleep(retryInterval);\n      // If we have a connection error, the JDO connection URL hook might\n      // provide us with a new URL to access the datastore.\n      String lastUrl = MetaStoreInit.getConnectionURL(getActiveConf());\n      gotNewConnectUrl = MetaStoreInit.updateConnectionURL(origConf, getActiveConf(),\n        lastUrl, metaStoreInitData);\n    }\n  }",
                "code_after_change": "  public Result invokeInternal(final Object proxy, final Method method, final Object[] args) throws Throwable {\n\n    boolean gotNewConnectUrl = false;\n    boolean reloadConf = HiveConf.getBoolVar(origConf,\n        HiveConf.ConfVars.HMSHANDLERFORCERELOADCONF);\n    long retryInterval = HiveConf.getTimeVar(origConf,\n        HiveConf.ConfVars.HMSHANDLERINTERVAL, TimeUnit.MILLISECONDS);\n    int retryLimit = HiveConf.getIntVar(origConf,\n        HiveConf.ConfVars.HMSHANDLERATTEMPTS);\n    long timeout = HiveConf.getTimeVar(origConf,\n        HiveConf.ConfVars.METASTORE_CLIENT_SOCKET_TIMEOUT, TimeUnit.MILLISECONDS);\n\n    Deadline.registerIfNot(timeout);\n\n    if (reloadConf) {\n      MetaStoreInit.updateConnectionURL(origConf, getActiveConf(),\n        null, metaStoreInitData);\n    }\n\n    int retryCount = 0;\n    Throwable caughtException = null;\n    while (true) {\n      try {\n        if (reloadConf || gotNewConnectUrl) {\n          baseHandler.setConf(getActiveConf());\n        }\n        Object object = null;\n        boolean isStarted = Deadline.startTimer(method.getName());\n        try {\n          object = method.invoke(baseHandler, args);\n        } finally {\n          if (isStarted) {\n            Deadline.stopTimer();\n          }\n        }\n        return new Result(object, retryCount);\n\n      } catch (UndeclaredThrowableException e) {\n        if (e.getCause() != null) {\n          if (e.getCause() instanceof javax.jdo.JDOException) {\n            // Due to reflection, the jdo exception is wrapped in\n            // invocationTargetException\n            caughtException = e.getCause();\n          } else if (e.getCause() instanceof MetaException && e.getCause().getCause() != null\n              && e.getCause().getCause() instanceof javax.jdo.JDOException) {\n            // The JDOException may be wrapped further in a MetaException\n            caughtException = e.getCause().getCause();\n          } else {\n            LOG.error(ExceptionUtils.getStackTrace(e.getCause()));\n            throw e.getCause();\n          }\n        } else {\n          LOG.error(ExceptionUtils.getStackTrace(e));\n          throw e;\n        }\n      } catch (InvocationTargetException e) {\n        if (e.getCause() instanceof javax.jdo.JDOException) {\n          // Due to reflection, the jdo exception is wrapped in\n          // invocationTargetException\n          caughtException = e.getCause();\n        } else if (e.getCause() instanceof NoSuchObjectException || e.getTargetException().getCause() instanceof NoSuchObjectException) {\n          String methodName = method.getName();\n          if (!methodName.startsWith(\"get_database\") && !methodName.startsWith(\"get_table\")\n              && !methodName.startsWith(\"get_partition\") && !methodName.startsWith(\"get_function\")) {\n            LOG.error(ExceptionUtils.getStackTrace(e.getCause()));\n          }\n          throw e.getCause();\n        } else if (e.getCause() instanceof MetaException && e.getCause().getCause() != null) {\n          if (e.getCause().getCause() instanceof javax.jdo.JDOException ||\n              e.getCause().getCause() instanceof NucleusException) {\n            // The JDOException or the Nucleus Exception may be wrapped further in a MetaException\n            caughtException = e.getCause().getCause();\n          } else if (e.getCause().getCause() instanceof DeadlineException) {\n            // The Deadline Exception needs no retry and be thrown immediately.\n            Deadline.clear();\n            LOG.error(\"Error happens in method \" + method.getName() + \": \" +\n                ExceptionUtils.getStackTrace(e.getCause()));\n            throw e.getCause();\n          } else {\n            LOG.error(ExceptionUtils.getStackTrace(e.getCause()));\n            throw e.getCause();\n          }\n        } else {\n          LOG.error(ExceptionUtils.getStackTrace(e.getCause()));\n          throw e.getCause();\n        }\n      }\n\n      if (retryCount >= retryLimit) {\n        LOG.error(\"HMSHandler Fatal error: \" + ExceptionUtils.getStackTrace(caughtException));\n        MetaException me = new MetaException(caughtException.getMessage());\n        me.initCause(caughtException);\n        throw me;\n      }\n\n      assert (retryInterval >= 0);\n      retryCount++;\n      LOG.error(\n        String.format(\n          \"Retrying HMSHandler after %d ms (attempt %d of %d)\", retryInterval, retryCount, retryLimit) +\n          \" with error: \" + ExceptionUtils.getStackTrace(caughtException));\n\n      Thread.sleep(retryInterval);\n      // If we have a connection error, the JDO connection URL hook might\n      // provide us with a new URL to access the datastore.\n      String lastUrl = MetaStoreInit.getConnectionURL(getActiveConf());\n      gotNewConnectUrl = MetaStoreInit.updateConnectionURL(origConf, getActiveConf(),\n        lastUrl, metaStoreInitData);\n    }\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreUtils.mergeColStats": {
                "code_before_change": "  public static void mergeColStats(ColumnStatistics csNew, ColumnStatistics csOld)\n      throws InvalidObjectException {\n    List<ColumnStatisticsObj> list = new ArrayList<>();\n    if (csNew.getStatsObj().size() != csOld.getStatsObjSize()) {\n      // Some of the columns' stats are missing\n      // This implies partition schema has changed. We will merge columns\n      // present in both, overwrite stats for columns absent in metastore and\n      // leave alone columns stats missing from stats task. This last case may\n      // leave stats in stale state. This will be addressed later.\n      LOG.debug(\"New ColumnStats size is \" + csNew.getStatsObj().size()\n          + \". But old ColumnStats size is \" + csOld.getStatsObjSize());\n    }\n    // In this case, we have to find out which columns can be merged.\n    Map<String, ColumnStatisticsObj> map = new HashMap<>();\n    // We build a hash map from colName to object for old ColumnStats.\n    for (ColumnStatisticsObj obj : csOld.getStatsObj()) {\n      map.put(obj.getColName(), obj);\n    }\n    for (int index = 0; index < csNew.getStatsObj().size(); index++) {\n      ColumnStatisticsObj statsObjNew = csNew.getStatsObj().get(index);\n      ColumnStatisticsObj statsObjOld = map.get(statsObjNew.getColName());\n      if (statsObjOld != null) {\n        // If statsObjOld is found, we can merge.\n        ColumnStatsMerger merger = ColumnStatsMergerFactory.getColumnStatsMerger(statsObjNew,\n            statsObjOld);\n        merger.merge(statsObjNew, statsObjOld);\n      }\n      list.add(statsObjNew);\n    }\n    csNew.setStatsObj(list);\n  }",
                "code_after_change": "  public static void mergeColStats(ColumnStatistics csNew, ColumnStatistics csOld)\n      throws InvalidObjectException {\n    List<ColumnStatisticsObj> list = new ArrayList<>();\n    if (csNew.getStatsObj().size() != csOld.getStatsObjSize()) {\n      // Some of the columns' stats are missing\n      // This implies partition schema has changed. We will merge columns\n      // present in both, overwrite stats for columns absent in metastore and\n      // leave alone columns stats missing from stats task. This last case may\n      // leave stats in stale state. This will be addressed later.\n      LOG.debug(\"New ColumnStats size is \" + csNew.getStatsObj().size()\n          + \". But old ColumnStats size is \" + csOld.getStatsObjSize());\n    }\n    // In this case, we have to find out which columns can be merged.\n    Map<String, ColumnStatisticsObj> map = new HashMap<>();\n    // We build a hash map from colName to object for old ColumnStats.\n    for (ColumnStatisticsObj obj : csOld.getStatsObj()) {\n      map.put(obj.getColName(), obj);\n    }\n    for (int index = 0; index < csNew.getStatsObj().size(); index++) {\n      ColumnStatisticsObj statsObjNew = csNew.getStatsObj().get(index);\n      ColumnStatisticsObj statsObjOld = map.get(statsObjNew.getColName());\n      if (statsObjOld != null) {\n        // If statsObjOld is found, we can merge.\n        ColumnStatsMerger merger = ColumnStatsMergerFactory.getColumnStatsMerger(statsObjNew,\n            statsObjOld);\n        merger.merge(statsObjNew, statsObjOld);\n      }\n      list.add(statsObjNew);\n    }\n    csNew.setStatsObj(list);\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Buggy Method"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Buggy Method"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the issue with retrying operations in the 'RetryingHMSHandler' class, specifically mentioning the handling of exceptions like 'JDOException' and 'NucleusException'. This is related to the ground truth method 'metastore.src.java.org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal', which is a buggy method but not where the actual fix was made. The report does not provide a fix suggestion, hence 'Missing' for fix suggestion. The problem location is identified as 'RetryingHMSHandler', which is a buggy method but not the precise location of the fix. There is no wrong information in the report as it correctly describes the issue context."
        }
    },
    {
        "filename": "HIVE-6389.json",
        "code_diff": {
            "serde.src.java.org.apache.hadoop.hive.serde2.lazybinary.objectinspector.LazyBinaryMapObjectInspector.getMapSize": {
                "code_before_change": "  public int getMapSize(Object data) {\n    if (data == null) {\n      return -1;\n    }\n    return ((LazyBinaryMap) data).getMapSize();\n  }",
                "code_after_change": "  public int getMapSize(Object data) {\n    if (data == null) {\n      return -1;\n    }\n    return ((LazyBinaryMap) data).getMapSize();\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Correct",
            "problem_location_identification": {
                "level": "Missing",
                "sub_category": ""
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the issue with null handling in map lookups, which is related to the ground truth method, but it does not precisely identify the method 'LazyBinaryMapObjectInspector.getMapSize'. Instead, it mentions the need for LazyBinaryMapOI to handle nulls, which is in the shared stack trace context. The fix suggestion in the report matches the developer's fix, as it suggests handling nulls, which aligns with the change in the method. The problem location is not precisely identified in the report, as it does not mention the specific method or class where the fix was made. There is no wrong information in the report; all details are relevant to the context of the bug."
        }
    },
    {
        "filename": "HIVE-2372.json",
        "code_diff": {
            "ql.src.java.org.apache.hadoop.hive.ql.exec.ScriptOperator.safeEnvVarName": {
                "code_before_change": "  static String safeEnvVarName(String var) {\n    StringBuilder safe = new StringBuilder();\n    int len = var.length();\n    for (int i = 0; i < len; i++) {\n      char c = var.charAt(i);\n      char s;\n      if ((c >= '0' && c <= '9') || (c >= 'A' && c <= 'Z')\n          || (c >= 'a' && c <= 'z')) {\n        s = c;\n      } else {\n        s = '_';\n      }\n      safe.append(s);\n    }\n    return safe.toString();\n  }",
                "code_after_change": "  String safeEnvVarName(String name) {\n    StringBuilder safe = new StringBuilder();\n    int len = name.length();\n\n    for (int i = 0; i < len; i++) {\n      char c = name.charAt(i);\n      char s;\n      if ((c >= '0' && c <= '9') || (c >= 'A' && c <= 'Z')\n          || (c >= 'a' && c <= 'z')) {\n        s = c;\n      } else {\n        s = '_';\n      }\n      safe.append(s);\n    }\n    return safe.toString();\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.exec.ScriptOperator.addJobConfToEnvironment": {
                "code_before_change": "  static void addJobConfToEnvironment(Configuration conf,\n      Map<String, String> env) {\n    Iterator<Map.Entry<String, String>> it = conf.iterator();\n    while (it.hasNext()) {\n      Map.Entry<String, String> en = it.next();\n      String name = en.getKey();\n      // String value = (String)en.getValue(); // does not apply variable\n      // expansion\n      String value = conf.get(name); // does variable expansion\n      name = safeEnvVarName(name);\n      env.put(name, value);\n    }\n  }",
                "code_after_change": "  void addJobConfToEnvironment(Configuration conf, Map<String, String> env) {\n    Iterator<Map.Entry<String, String>> it = conf.iterator();\n    while (it.hasNext()) {\n      Map.Entry<String, String> en = it.next();\n      String name = en.getKey();\n      // String value = (String)en.getValue(); // does not apply variable\n      // expansion\n      String value = conf.get(name); // does variable expansion\n      name = safeEnvVarName(name);\n      boolean truncate = conf.getBoolean(HiveConf.ConfVars.HIVESCRIPTTRUNCATEENV.toString(), false);\n      value = safeEnvVarValue(value, name, truncate);\n      env.put(name, value);\n    }\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Buggy Method"
            },
            "fix_suggestion": "Preventive",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Buggy Method"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the issue with the environment variable size in the ScriptOperator, which is related to the ground truth methods but not the exact root cause. It points to the method where the error occurred (ScriptOperator), but not where the actual fix was made (safeEnvVarName and addJobConfToEnvironment). The fix suggestion to 'get rid of this variable in reducers' is preventive as it suggests a way to mitigate the issue by avoiding large environment variables. The problem location is identified as the ScriptOperator, which is where the error manifests, but not the precise ground truth methods. There is no wrong information as the report accurately describes the problem context."
        }
    },
    {
        "filename": "HIVE-11301.json",
        "code_diff": {
            "ql.src.java.org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics": {
                "code_before_change": "  public static Statistics collectStatistics(HiveConf conf, PrunedPartitionList partList,\n      Table table, TableScanOperator tableScanOperator) throws HiveException {\n\n    // column level statistics are required only for the columns that are needed\n    List<ColumnInfo> schema = tableScanOperator.getSchema().getSignature();\n    List<String> neededColumns = tableScanOperator.getNeededColumns();\n    List<String> referencedColumns = tableScanOperator.getReferencedColumns();\n\n    return collectStatistics(conf, partList, table, schema, neededColumns, referencedColumns);\n  }",
                "code_after_change": "  public static Statistics collectStatistics(HiveConf conf, PrunedPartitionList partList,\n      Table table, TableScanOperator tableScanOperator) throws HiveException {\n\n    // column level statistics are required only for the columns that are needed\n    List<ColumnInfo> schema = tableScanOperator.getSchema().getSignature();\n    List<String> neededColumns = tableScanOperator.getNeededColumns();\n    List<String> referencedColumns = tableScanOperator.getReferencedColumns();\n\n    return collectStatistics(conf, partList, table, schema, neededColumns, referencedColumns);\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report mentions methods in the stack trace that are related to the ground truth method 'ql.src.java.org.apache.hadoop.hive.ql.stats.StatsUtils.collectStatistics', but it does not precisely identify the root cause or problem location. The report describes a Thrift error and a connection issue, which are related to the context of the problem but do not pinpoint the exact method where the fix was applied. There is no fix suggestion provided in the report. All information in the report is relevant to the context of the bug, so there is no wrong information."
        }
    },
    {
        "filename": "HIVE-11028.json",
        "code_diff": {
            "ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory.foldExpr": {
                "code_before_change": "  public static ExprNodeDesc foldExpr(ExprNodeGenericFuncDesc funcDesc) {\n\n    GenericUDF udf = funcDesc.getGenericUDF();\n    if (!isDeterministicUdf(udf)) {\n      return funcDesc;\n    }\n    return evaluateFunction(funcDesc.getGenericUDF(),funcDesc.getChildren(), funcDesc.getChildren());\n  }",
                "code_after_change": "  public static ExprNodeDesc foldExpr(ExprNodeGenericFuncDesc funcDesc) {\n\n    GenericUDF udf = funcDesc.getGenericUDF();\n    if (!isDeterministicUdf(udf)) {\n      return funcDesc;\n    }\n    return evaluateFunction(funcDesc.getGenericUDF(),funcDesc.getChildren(), funcDesc.getChildren());\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConstantPropagate.transform": {
                "code_before_change": "  public ParseContext transform(ParseContext pactx) throws SemanticException {\n    pGraphContext = pactx;\n\n    // generate pruned column list for all relevant operators\n    ConstantPropagateProcCtx cppCtx = new ConstantPropagateProcCtx();\n\n    // create a walker which walks the tree in a DFS manner while maintaining\n    // the operator stack. The dispatcher\n    // generates the plan from the operator tree\n    Map<Rule, NodeProcessor> opRules = new LinkedHashMap<Rule, NodeProcessor>();\n\n    opRules.put(new RuleRegExp(\"R1\", FilterOperator.getOperatorName() + \"%\"),\n        ConstantPropagateProcFactory.getFilterProc());\n    opRules.put(new RuleRegExp(\"R2\", GroupByOperator.getOperatorName() + \"%\"),\n        ConstantPropagateProcFactory.getGroupByProc());\n    opRules.put(new RuleRegExp(\"R3\", SelectOperator.getOperatorName() + \"%\"),\n        ConstantPropagateProcFactory.getSelectProc());\n    opRules.put(new RuleRegExp(\"R4\", FileSinkOperator.getOperatorName() + \"%\"),\n        ConstantPropagateProcFactory.getFileSinkProc());\n    opRules.put(new RuleRegExp(\"R5\", ReduceSinkOperator.getOperatorName() + \"%\"),\n        ConstantPropagateProcFactory.getReduceSinkProc());\n    opRules.put(new RuleRegExp(\"R6\", JoinOperator.getOperatorName() + \"%\"),\n        ConstantPropagateProcFactory.getJoinProc());\n    opRules.put(new RuleRegExp(\"R7\", TableScanOperator.getOperatorName() + \"%\"),\n        ConstantPropagateProcFactory.getTableScanProc());\n    opRules.put(new RuleRegExp(\"R8\", ScriptOperator.getOperatorName() + \"%\"),\n        ConstantPropagateProcFactory.getStopProc());\n\n    // The dispatcher fires the processor corresponding to the closest matching\n    // rule and passes the context along\n    Dispatcher disp = new DefaultRuleDispatcher(ConstantPropagateProcFactory\n        .getDefaultProc(), opRules, cppCtx);\n    GraphWalker ogw = new ConstantPropagateWalker(disp);\n\n    // Create a list of operator nodes to start the walking.\n    ArrayList<Node> topNodes = new ArrayList<Node>();\n    topNodes.addAll(pGraphContext.getTopOps().values());\n    ogw.startWalking(topNodes, null);\n    for (Operator<? extends Serializable> opToDelete : cppCtx.getOpToDelete()) {\n      if (opToDelete.getParentOperators() == null || opToDelete.getParentOperators().size() != 1) {\n        throw new RuntimeException(\"Error pruning operator \" + opToDelete\n            + \". It should have only 1 parent.\");\n      }\n      opToDelete.getParentOperators().get(0).removeChildAndAdoptItsChildren(opToDelete);\n    }\n    return pGraphContext;\n  }",
                "code_after_change": "  public ParseContext transform(ParseContext pactx) throws SemanticException {\n    pGraphContext = pactx;\n\n    // generate pruned column list for all relevant operators\n    ConstantPropagateProcCtx cppCtx = new ConstantPropagateProcCtx(constantPropagateOption);\n\n    // create a walker which walks the tree in a DFS manner while maintaining\n    // the operator stack. The dispatcher\n    // generates the plan from the operator tree\n    Map<Rule, NodeProcessor> opRules = new LinkedHashMap<Rule, NodeProcessor>();\n\n    opRules.put(new RuleRegExp(\"R1\", FilterOperator.getOperatorName() + \"%\"),\n        ConstantPropagateProcFactory.getFilterProc());\n    opRules.put(new RuleRegExp(\"R2\", GroupByOperator.getOperatorName() + \"%\"),\n        ConstantPropagateProcFactory.getGroupByProc());\n    opRules.put(new RuleRegExp(\"R3\", SelectOperator.getOperatorName() + \"%\"),\n        ConstantPropagateProcFactory.getSelectProc());\n    opRules.put(new RuleRegExp(\"R4\", FileSinkOperator.getOperatorName() + \"%\"),\n        ConstantPropagateProcFactory.getFileSinkProc());\n    opRules.put(new RuleRegExp(\"R5\", ReduceSinkOperator.getOperatorName() + \"%\"),\n        ConstantPropagateProcFactory.getReduceSinkProc());\n    opRules.put(new RuleRegExp(\"R6\", JoinOperator.getOperatorName() + \"%\"),\n        ConstantPropagateProcFactory.getJoinProc());\n    opRules.put(new RuleRegExp(\"R7\", TableScanOperator.getOperatorName() + \"%\"),\n        ConstantPropagateProcFactory.getTableScanProc());\n    opRules.put(new RuleRegExp(\"R8\", ScriptOperator.getOperatorName() + \"%\"),\n        ConstantPropagateProcFactory.getStopProc());\n\n    // The dispatcher fires the processor corresponding to the closest matching\n    // rule and passes the context along\n    Dispatcher disp = new DefaultRuleDispatcher(ConstantPropagateProcFactory\n        .getDefaultProc(), opRules, cppCtx);\n    GraphWalker ogw = new ConstantPropagateWalker(disp);\n\n    // Create a list of operator nodes to start the walking.\n    ArrayList<Node> topNodes = new ArrayList<Node>();\n    topNodes.addAll(pGraphContext.getTopOps().values());\n    ogw.startWalking(topNodes, null);\n    for (Operator<? extends Serializable> opToDelete : cppCtx.getOpToDelete()) {\n      if (opToDelete.getParentOperators() == null || opToDelete.getParentOperators().size() != 1) {\n        throw new RuntimeException(\"Error pruning operator \" + opToDelete\n            + \". It should have only 1 parent.\");\n      }\n      opToDelete.getParentOperators().get(0).removeChildAndAdoptItsChildren(opToDelete);\n    }\n    return pGraphContext;\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.parse.TezCompiler.runDynamicPartitionPruning": {
                "code_before_change": "  private void runDynamicPartitionPruning(OptimizeTezProcContext procCtx, Set<ReadEntity> inputs,\n      Set<WriteEntity> outputs) throws SemanticException {\n\n    if (!procCtx.conf.getBoolVar(ConfVars.TEZ_DYNAMIC_PARTITION_PRUNING)) {\n      return;\n    }\n\n    // Sequence of TableScan operators to be walked\n    Deque<Operator<?>> deque = new LinkedList<Operator<?>>();\n    deque.addAll(procCtx.parseContext.getTopOps().values());\n\n    Map<Rule, NodeProcessor> opRules = new LinkedHashMap<Rule, NodeProcessor>();\n    opRules.put(\n        new RuleRegExp(new String(\"Dynamic Partition Pruning\"), FilterOperator.getOperatorName()\n            + \"%\"), new DynamicPartitionPruningOptimization());\n\n    // The dispatcher fires the processor corresponding to the closest matching\n    // rule and passes the context along\n    Dispatcher disp = new DefaultRuleDispatcher(null, opRules, procCtx);\n    List<Node> topNodes = new ArrayList<Node>();\n    topNodes.addAll(procCtx.parseContext.getTopOps().values());\n    GraphWalker ogw = new ForwardWalker(disp);\n    ogw.startWalking(topNodes, null);\n\n    // need a new run of the constant folding because we might have created lots\n    // of \"and true and true\" conditions.\n    if(procCtx.conf.getBoolVar(ConfVars.HIVEOPTCONSTANTPROPAGATION)) {\n      new ConstantPropagate().transform(procCtx.parseContext);\n    }\n  }",
                "code_after_change": "  private void runDynamicPartitionPruning(OptimizeTezProcContext procCtx, Set<ReadEntity> inputs,\n      Set<WriteEntity> outputs) throws SemanticException {\n\n    if (!procCtx.conf.getBoolVar(ConfVars.TEZ_DYNAMIC_PARTITION_PRUNING)) {\n      return;\n    }\n\n    // Sequence of TableScan operators to be walked\n    Deque<Operator<?>> deque = new LinkedList<Operator<?>>();\n    deque.addAll(procCtx.parseContext.getTopOps().values());\n\n    Map<Rule, NodeProcessor> opRules = new LinkedHashMap<Rule, NodeProcessor>();\n    opRules.put(\n        new RuleRegExp(new String(\"Dynamic Partition Pruning\"), FilterOperator.getOperatorName()\n            + \"%\"), new DynamicPartitionPruningOptimization());\n\n    // The dispatcher fires the processor corresponding to the closest matching\n    // rule and passes the context along\n    Dispatcher disp = new DefaultRuleDispatcher(null, opRules, procCtx);\n    List<Node> topNodes = new ArrayList<Node>();\n    topNodes.addAll(procCtx.parseContext.getTopOps().values());\n    GraphWalker ogw = new ForwardWalker(disp);\n    ogw.startWalking(topNodes, null);\n\n    // need a new run of the constant folding because we might have created lots\n    // of \"and true and true\" conditions.\n    // Rather than run the full constant folding just need to shortcut AND/OR expressions\n    // involving constant true/false values.\n    if(procCtx.conf.getBoolVar(ConfVars.HIVEOPTCONSTANTPROPAGATION)) {\n      new ConstantPropagate(ConstantPropagateOption.SHORTCUT).transform(procCtx.parseContext);\n    }\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcCtx.addOpToDelete": {
                "code_before_change": "  public void addOpToDelete(Operator<? extends Serializable> op) {\n    opToDelete.add(op);\n  }",
                "code_after_change": "  public void addOpToDelete(Operator<? extends Serializable> op) {\n    opToDelete.add(op);\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConstantPropagateProcFactory.foldExprShortcut": {
                "code_before_change": [],
                "code_after_change": "  private static ExprNodeDesc foldExprShortcut(ExprNodeDesc desc, Map<ColumnInfo, ExprNodeDesc> constants,\n      ConstantPropagateProcCtx cppCtx, Operator<? extends Serializable> op, int tag,\n      boolean propagate) throws UDFArgumentException {\n    if (desc instanceof ExprNodeGenericFuncDesc) {\n      ExprNodeGenericFuncDesc funcDesc = (ExprNodeGenericFuncDesc) desc;\n\n      GenericUDF udf = funcDesc.getGenericUDF();\n\n      boolean propagateNext = propagate && propagatableUdfs.contains(udf.getClass());\n      List<ExprNodeDesc> newExprs = new ArrayList<ExprNodeDesc>();\n      for (ExprNodeDesc childExpr : desc.getChildren()) {\n        newExprs.add(foldExpr(childExpr, constants, cppCtx, op, tag, propagateNext));\n      }\n\n      // Don't evalulate nondeterministic function since the value can only calculate during runtime.\n      if (!isDeterministicUdf(udf)) {\n        LOG.debug(\"Function \" + udf.getClass() + \" is undeterministic. Don't evalulating immediately.\");\n        ((ExprNodeGenericFuncDesc) desc).setChildren(newExprs);\n        return desc;\n      }\n\n      // Check if the function can be short cut.\n      ExprNodeDesc shortcut = shortcutFunction(udf, newExprs, op);\n      if (shortcut != null) {\n        LOG.debug(\"Folding expression:\" + desc + \" -> \" + shortcut);\n        return shortcut;\n      }\n      ((ExprNodeGenericFuncDesc) desc).setChildren(newExprs);\n    }\n    return desc;\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report mentions an IndexOutOfBoundsException occurring in the context of a TezProcessor, which is part of the stack trace. However, it does not precisely identify the root cause related to any of the ground truth methods. The methods mentioned in the stack trace are related to the execution context but not directly to the ground truth methods. There is no fix suggestion provided in the bug report. The problem location is partially identified as it is within the shared stack trace context but does not precisely point to the ground truth methods. There is no wrong information in the bug report as it accurately describes the error encountered."
        }
    },
    {
        "filename": "HIVE-14380.json",
        "code_diff": {
            "shims.0.23.src.main.java.org.apache.hadoop.hive.shims.Hadoop23Shims.isPathEncrypted": {
                "code_before_change": [],
                "code_after_change": []
            },
            "ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.getMetaData": {
                "code_before_change": "  public void getMetaData(QB qb) throws SemanticException {\n    getMetaData(qb, false);\n  }",
                "code_after_change": "  public void getMetaData(QB qb) throws SemanticException {\n    getMetaData(qb, false);\n  }"
            },
            "shims.0.23.src.main.java.org.apache.hadoop.hive.shims.Hadoop23Shims.equivalentEncryptionZones": {
                "code_before_change": [],
                "code_after_change": []
            },
            "shims.common.src.main.java.org.apache.hadoop.hive.shims.HadoopShims.arePathsOnSameEncryptionZone": {
                "code_before_change": "    public boolean arePathsOnSameEncryptionZone(Path path1, Path path2) throws IOException;\n\n    /**\n     * Compares two encrypted path strengths.\n     *\n     * @param path1 HDFS path to compare.\n     * @param path2 HDFS path to compare.\n     * @return 1 if path1 is stronger; 0 if paths are equals; -1 if path1 is weaker.\n     * @throws IOException If an error occurred attempting to get encryption/key metadata\n     */\n    public int comparePathKeyStrength(Path path1, Path path2) throws IOException;\n\n    /**\n     * create encryption zone by path and keyname\n     * @param path HDFS path to create encryption zone\n     * @param keyName keyname\n     * @throws IOException\n     */\n    @VisibleForTesting\n    public void createEncryptionZone(Path path, String keyName) throws IOException;\n\n    /**\n     * Creates an encryption key.\n     *\n     * @param keyName Name of the key\n     * @param bitLength Key encryption length in bits (128 or 256).\n     * @throws IOException If an error occurs while creating the encryption key\n     * @throws NoSuchAlgorithmException If cipher algorithm is invalid.\n     */\n    @VisibleForTesting\n    public void createKey(String keyName, int bitLength)\n      throws IOException, NoSuchAlgorithmException;\n\n    @VisibleForTesting\n    public void deleteKey(String keyName) throws IOException;\n\n    @VisibleForTesting\n    public List<String> getKeys() throws IOException;\n  }\n\n  /**\n   * This is a dummy class used when the hadoop version does not support hdfs encryption.\n   */\n  public static class NoopHdfsEncryptionShim implements HdfsEncryptionShim {",
                "code_after_change": "    public boolean arePathsOnSameEncryptionZone(Path path1, Path path2) throws IOException;\n\n    /**\n     * Checks if two HDFS paths are on the same encrypted or unencrypted zone.\n     *\n     * @param path1 Path to HDFS file system\n     * @param path2 Path to HDFS file system\n     * @param encryptionShim2 The encryption-shim corresponding to path2.\n     * @return True if both paths are in the same zone; False otherwise.\n     * @throws IOException If an error occurred attempting to get encryption information\n     */\n    public boolean arePathsOnSameEncryptionZone(Path path1, Path path2, HdfsEncryptionShim encryptionShim2) throws IOException;\n\n    /**\n     * Compares two encrypted path strengths.\n     *\n     * @param path1 HDFS path to compare.\n     * @param path2 HDFS path to compare.\n     * @return 1 if path1 is stronger; 0 if paths are equals; -1 if path1 is weaker.\n     * @throws IOException If an error occurred attempting to get encryption/key metadata\n     */\n    public int comparePathKeyStrength(Path path1, Path path2) throws IOException;\n\n    /**\n     * create encryption zone by path and keyname\n     * @param path HDFS path to create encryption zone\n     * @param keyName keyname\n     * @throws IOException\n     */\n    @VisibleForTesting\n    public void createEncryptionZone(Path path, String keyName) throws IOException;\n\n    /**\n     * Creates an encryption key.\n     *\n     * @param keyName Name of the key\n     * @param bitLength Key encryption length in bits (128 or 256).\n     * @throws IOException If an error occurs while creating the encryption key\n     * @throws NoSuchAlgorithmException If cipher algorithm is invalid.\n     */\n    @VisibleForTesting\n    public void createKey(String keyName, int bitLength)\n      throws IOException, NoSuchAlgorithmException;\n\n    @VisibleForTesting\n    public void deleteKey(String keyName) throws IOException;\n\n    @VisibleForTesting\n    public List<String> getKeys() throws IOException;\n  }\n\n  /**\n   * This is a dummy class used when the hadoop version does not support hdfs encryption.\n   */\n  public static class NoopHdfsEncryptionShim implements HdfsEncryptionShim {"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.getTxnMgr": {
                "code_before_change": "  public HiveTxnManager getTxnMgr() {\n    return txnMgr;\n  }",
                "code_after_change": "  public HiveTxnManager getTxnMgr() {\n    return txnMgr;\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.needToCopy": {
                "code_before_change": "  static protected boolean needToCopy(Path srcf, Path destf, FileSystem srcFs, FileSystem destFs) throws HiveException {\n    //Check if different FileSystems\n    if (!FileUtils.equalsFileSystem(srcFs, destFs)) {\n      return true;\n    }\n\n    //Check if different encryption zones\n    HadoopShims.HdfsEncryptionShim hdfsEncryptionShim = SessionState.get().getHdfsEncryptionShim();\n    try {\n      return hdfsEncryptionShim != null && (hdfsEncryptionShim.isPathEncrypted(srcf) || hdfsEncryptionShim.isPathEncrypted(destf))\n        && !hdfsEncryptionShim.arePathsOnSameEncryptionZone(srcf, destf);\n    } catch (IOException e) {\n      throw new HiveException(e);\n    }\n  }",
                "code_after_change": "  static protected boolean needToCopy(Path srcf, Path destf, FileSystem srcFs, FileSystem destFs) throws HiveException {\n    //Check if different FileSystems\n    if (!FileUtils.equalsFileSystem(srcFs, destFs)) {\n      return true;\n    }\n\n    //Check if different encryption zones\n    HadoopShims.HdfsEncryptionShim srcHdfsEncryptionShim = SessionState.get().getHdfsEncryptionShim(srcFs);\n    HadoopShims.HdfsEncryptionShim destHdfsEncryptionShim = SessionState.get().getHdfsEncryptionShim(destFs);\n    try {\n      return srcHdfsEncryptionShim != null\n          && destHdfsEncryptionShim != null\n          && (srcHdfsEncryptionShim.isPathEncrypted(srcf) || destHdfsEncryptionShim.isPathEncrypted(destf))\n          && !srcHdfsEncryptionShim.arePathsOnSameEncryptionZone(srcf, destf, destHdfsEncryptionShim);\n    } catch (IOException e) {\n      throw new HiveException(e);\n    }\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Direct Caller/Callee"
            },
            "fix_suggestion": "Alternative Fix",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Direct Caller/Callee"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the issue in the 'SessionState' class, specifically in the 'getHdfsEncryptionShim' method, which is directly related to the ground truth method 'ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.needToCopy'. This method calls 'SessionState.get().getHdfsEncryptionShim()' as part of its logic, making it a direct caller. The fix suggestion in the report suggests fetching the FileSystem instance corresponding to the path being checked, which is an alternative approach to the developer's fix that involves modifying how encryption shims are retrieved. The problem location is partially identified as it points to a method directly related to the ground truth method. There is no wrong information in the report as it accurately describes the issue and provides a reasonable alternative fix."
        }
    },
    {
        "filename": "HIVE-6537.json",
        "code_diff": {
            "ql.src.java.org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask.initializeOperators": {
                "code_before_change": "  private void initializeOperators(Map<FetchOperator, JobConf> fetchOpJobConfMap)\n      throws HiveException {\n    // this mapper operator is used to initialize all the operators\n    for (Map.Entry<String, FetchWork> entry : work.getAliasToFetchWork().entrySet()) {\n      if (entry.getValue() == null) {\n        continue;\n      }\n      JobConf jobClone = new JobConf(job);\n\n      TableScanOperator ts = (TableScanOperator)work.getAliasToWork().get(entry.getKey());\n      // push down projections\n      ColumnProjectionUtils.appendReadColumns(\n          jobClone, ts.getNeededColumnIDs(), ts.getNeededColumns());\n      // push down filters\n      HiveInputFormat.pushFilters(jobClone, ts);\n\n      // create a fetch operator\n      FetchOperator fetchOp = new FetchOperator(entry.getValue(), jobClone);\n      fetchOpJobConfMap.put(fetchOp, jobClone);\n      fetchOperators.put(entry.getKey(), fetchOp);\n      l4j.info(\"fetchoperator for \" + entry.getKey() + \" created\");\n    }\n    // initilize all forward operator\n    for (Map.Entry<String, FetchOperator> entry : fetchOperators.entrySet()) {\n      // get the forward op\n      String alias = entry.getKey();\n      Operator<? extends OperatorDesc> forwardOp = work.getAliasToWork().get(alias);\n\n      // put the exe context into all the operators\n      forwardOp.setExecContext(execContext);\n      // All the operators need to be initialized before process\n      FetchOperator fetchOp = entry.getValue();\n      JobConf jobConf = fetchOpJobConfMap.get(fetchOp);\n\n      if (jobConf == null) {\n        jobConf = job;\n      }\n      // initialize the forward operator\n      ObjectInspector objectInspector = fetchOp.getOutputObjectInspector();\n      forwardOp.initialize(jobConf, new ObjectInspector[] {objectInspector});\n      l4j.info(\"fetchoperator for \" + entry.getKey() + \" initialized\");\n    }\n  }",
                "code_after_change": "  private void initializeOperators(Map<FetchOperator, JobConf> fetchOpJobConfMap)\n      throws HiveException {\n    // this mapper operator is used to initialize all the operators\n    for (Map.Entry<String, FetchWork> entry : work.getAliasToFetchWork().entrySet()) {\n      if (entry.getValue() == null) {\n        continue;\n      }\n      JobConf jobClone = new JobConf(job);\n\n      TableScanOperator ts = (TableScanOperator)work.getAliasToWork().get(entry.getKey());\n      // push down projections\n      ColumnProjectionUtils.appendReadColumns(\n          jobClone, ts.getNeededColumnIDs(), ts.getNeededColumns());\n      // push down filters\n      HiveInputFormat.pushFilters(jobClone, ts);\n\n      // create a fetch operator\n      FetchOperator fetchOp = new FetchOperator(entry.getValue(), jobClone);\n      fetchOpJobConfMap.put(fetchOp, jobClone);\n      fetchOperators.put(entry.getKey(), fetchOp);\n      l4j.info(\"fetchoperator for \" + entry.getKey() + \" created\");\n    }\n    // initialize all forward operator\n    for (Map.Entry<String, FetchOperator> entry : fetchOperators.entrySet()) {\n      // get the forward op\n      String alias = entry.getKey();\n      Operator<? extends OperatorDesc> forwardOp = work.getAliasToWork().get(alias);\n\n      // put the exe context into all the operators\n      forwardOp.setExecContext(execContext);\n      // All the operators need to be initialized before process\n      FetchOperator fetchOp = entry.getValue();\n      JobConf jobConf = fetchOpJobConfMap.get(fetchOp);\n\n      if (jobConf == null) {\n        jobConf = job;\n      }\n      // initialize the forward operator\n      ObjectInspector objectInspector = fetchOp.getOutputObjectInspector();\n      forwardOp.initialize(jobConf, new ObjectInspector[] {objectInspector});\n      l4j.info(\"fetchoperator for \" + entry.getKey() + \" initialized\");\n    }\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.LocalMapJoinProcFactory.process": {
                "code_before_change": "      public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,\n          Object... nodeOutputs) throws SemanticException {\n        return null;\n      }",
                "code_after_change": "      public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,\n          Object... nodeOutputs) throws SemanticException {\n        return null;\n      }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.exec.mr.HashTableLoader.loadDirectly": {
                "code_before_change": "  private void loadDirectly(MapJoinTableContainer[] mapJoinTables, String inputFileName)\n      throws Exception {\n    MapredLocalWork localWork = context.getLocalWork();\n    List<Operator<?>> directWorks = localWork.getDirectFetchOp().get(joinOp);\n    if (directWorks == null || directWorks.isEmpty()) {\n      return;\n    }\n    JobConf job = new JobConf(hconf);\n    MapredLocalTask localTask = new MapredLocalTask(localWork, job, false);\n\n    HashTableSinkOperator sink = new TemporaryHashSinkOperator(desc);\n    sink.setParentOperators(new ArrayList<Operator<? extends OperatorDesc>>(directWorks));\n\n    for (Operator<?> operator : directWorks) {\n      if (operator instanceof TableScanOperator) {\n        operator.setChildOperators(Arrays.<Operator<? extends OperatorDesc>>asList(sink));\n      }\n    }\n    localTask.setExecContext(context);\n    localTask.startForward(inputFileName);\n\n    MapJoinTableContainer[] tables = sink.getMapJoinTables();\n    for (int i = 0; i < sink.getNumParent(); i++) {\n      if (sink.getParentOperators().get(i) != null) {\n        mapJoinTables[i] = tables[i];\n      }\n    }\n\n    Arrays.fill(tables, null);\n  }",
                "code_after_change": "  private void loadDirectly(MapJoinTableContainer[] mapJoinTables, String inputFileName)\n      throws Exception {\n    MapredLocalWork localWork = context.getLocalWork();\n    List<Operator<?>> directWorks = localWork.getDirectFetchOp().get(joinOp);\n    if (directWorks == null || directWorks.isEmpty()) {\n      return;\n    }\n    JobConf job = new JobConf(hconf);\n    MapredLocalTask localTask = new MapredLocalTask(localWork, job, false);\n\n    HashTableSinkOperator sink = new TemporaryHashSinkOperator(desc);\n    sink.setParentOperators(new ArrayList<Operator<? extends OperatorDesc>>(directWorks));\n\n    for (Operator<?> operator : directWorks) {\n      if (operator != null) {\n        operator.setChildOperators(Arrays.<Operator<? extends OperatorDesc>>asList(sink));\n      }\n    }\n    localTask.setExecContext(context);\n    localTask.startForward(inputFileName);\n\n    MapJoinTableContainer[] tables = sink.getMapJoinTables();\n    for (int i = 0; i < sink.getNumParent(); i++) {\n      if (sink.getParentOperators().get(i) != null) {\n        mapJoinTables[i] = tables[i];\n      }\n    }\n\n    Arrays.fill(tables, null);\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Buggy Method"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Buggy Method"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the method 'HashTableLoader.loadDirectly' as the location of the NullPointerException, which is indeed one of the ground truth methods, but it does not precisely identify the root cause. The report mentions that 'tables' in 'Arrays.fill' is null, which is correct, but it does not pinpoint the exact issue in the code logic that leads to this state. Therefore, it is classified as 'Partial' under 'Buggy Method'. There is no fix suggestion provided in the bug report, hence it is marked as 'Missing' for fix suggestion. The problem location is identified as 'Partial' under 'Buggy Method' because it points to the method where the error occurred, but not where the actual fix was made. There is no wrong information in the bug report; all statements are relevant to the context of the bug."
        }
    },
    {
        "filename": "HIVE-13691.json",
        "code_diff": {
            "metastore.src.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.heartbeatTxnRange": {
                "code_before_change": "  public HeartbeatTxnRangeResponse heartbeatTxnRange(HeartbeatTxnRangeRequest rqst)\n    throws MetaException {\n    try {\n      Connection dbConn = null;\n      HeartbeatTxnRangeResponse rsp = new HeartbeatTxnRangeResponse();\n      Set<Long> nosuch = new HashSet<Long>();\n      Set<Long> aborted = new HashSet<Long>();\n      rsp.setNosuch(nosuch);\n      rsp.setAborted(aborted);\n      try {\n        /**\n         * READ_COMMITTED is sufficient since {@link #heartbeatTxn(java.sql.Connection, long)}\n         * only has 1 update statement in it and\n         * we only update existing txns, i.e. nothing can add additional txns that this operation\n         * would care about (which would have required SERIALIZABLE)\n         */\n        dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);\n        for (long txn = rqst.getMin(); txn <= rqst.getMax(); txn++) {\n          try {\n            //todo: do all updates in 1 SQL statement and check update count\n            //if update count is less than was requested, go into more expensive checks\n            //for each txn\n            heartbeatTxn(dbConn, txn);\n          } catch (NoSuchTxnException e) {\n            nosuch.add(txn);\n          } catch (TxnAbortedException e) {\n            aborted.add(txn);\n          }\n        }\n        return rsp;\n      } catch (SQLException e) {\n        LOG.debug(\"Going to rollback\");\n        rollbackDBConn(dbConn);\n        checkRetryable(dbConn, e, \"heartbeatTxnRange(\" + rqst + \")\");\n        throw new MetaException(\"Unable to select from transaction database \" +\n          StringUtils.stringifyException(e));\n      } finally {\n        closeDbConn(dbConn);\n      }\n    } catch (RetryException e) {\n      return heartbeatTxnRange(rqst);\n    }\n  }",
                "code_after_change": "  public HeartbeatTxnRangeResponse heartbeatTxnRange(HeartbeatTxnRangeRequest rqst)\n    throws MetaException {\n    try {\n      Connection dbConn = null;\n      HeartbeatTxnRangeResponse rsp = new HeartbeatTxnRangeResponse();\n      Set<Long> nosuch = new HashSet<Long>();\n      Set<Long> aborted = new HashSet<Long>();\n      rsp.setNosuch(nosuch);\n      rsp.setAborted(aborted);\n      try {\n        /**\n         * READ_COMMITTED is sufficient since {@link #heartbeatTxn(java.sql.Connection, long)}\n         * only has 1 update statement in it and\n         * we only update existing txns, i.e. nothing can add additional txns that this operation\n         * would care about (which would have required SERIALIZABLE)\n         */\n        dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);\n        for (long txn = rqst.getMin(); txn <= rqst.getMax(); txn++) {\n          try {\n            //todo: do all updates in 1 SQL statement and check update count\n            //if update count is less than was requested, go into more expensive checks\n            //for each txn\n            heartbeatTxn(dbConn, txn);\n          } catch (NoSuchTxnException e) {\n            nosuch.add(txn);\n          } catch (TxnAbortedException e) {\n            aborted.add(txn);\n          }\n        }\n        return rsp;\n      } catch (SQLException e) {\n        LOG.debug(\"Going to rollback\");\n        rollbackDBConn(dbConn);\n        checkRetryable(dbConn, e, \"heartbeatTxnRange(\" + rqst + \")\");\n        throw new MetaException(\"Unable to select from transaction database \" +\n          StringUtils.stringifyException(e));\n      } finally {\n        closeDbConn(dbConn);\n      }\n    } catch (RetryException e) {\n      return heartbeatTxnRange(rqst);\n    }\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler.markFailed": {
                "code_before_change": "  public void markFailed(CompactionInfo ci) throws MetaException {//todo: this should not throw\n    //todo: this should take \"comment\" as parameter to set in CC_META_INFO to provide some context for the failure\n    try {\n      Connection dbConn = null;\n      Statement stmt = null;\n      PreparedStatement pStmt = null;\n      ResultSet rs = null;\n      try {\n        dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);\n        stmt = dbConn.createStatement();\n        rs = stmt.executeQuery(\"select CQ_ID, CQ_DATABASE, CQ_TABLE, CQ_PARTITION, CQ_STATE, CQ_TYPE, CQ_WORKER_ID, CQ_START, CQ_RUN_AS, CQ_HIGHEST_TXN_ID, CQ_META_INFO, CQ_HADOOP_JOB_ID from COMPACTION_QUEUE WHERE CQ_ID = \" + ci.id);\n        if(rs.next()) {\n          ci = CompactionInfo.loadFullFromCompactionQueue(rs);\n          String s = \"delete from COMPACTION_QUEUE where cq_id = \" + ci.id;\n          LOG.debug(\"Going to execute update <\" + s + \">\");\n          int updCnt = stmt.executeUpdate(s);\n        }\n        else {\n          throw new IllegalStateException(\"No record with CQ_ID=\" + ci.id + \" found in COMPACTION_QUEUE\");\n        }\n        close(rs, stmt, null);\n\n        pStmt = dbConn.prepareStatement(\"insert into COMPLETED_COMPACTIONS(CC_ID, CC_DATABASE, CC_TABLE, CC_PARTITION, CC_STATE, CC_TYPE, CC_WORKER_ID, CC_START, CC_END, CC_RUN_AS, CC_HIGHEST_TXN_ID, CC_META_INFO, CC_HADOOP_JOB_ID) VALUES(?,?,?,?,?, ?,?,?,?,?, ?,?,?)\");\n        ci.state = FAILED_STATE;\n        CompactionInfo.insertIntoCompletedCompactions(pStmt, ci, getDbTime(dbConn));\n        int updCount = pStmt.executeUpdate();\n        LOG.debug(\"Going to commit\");\n        closeStmt(pStmt);\n        dbConn.commit();\n      } catch (SQLException e) {\n        LOG.error(\"Unable to delete from compaction queue \" + e.getMessage());\n        LOG.debug(\"Going to rollback\");\n        rollbackDBConn(dbConn);\n        try {\n          checkRetryable(dbConn, e, \"markFailed(\" + ci + \")\");\n        }\n        catch(MetaException ex) {\n          LOG.error(\"Unable to connect to transaction database \" + StringUtils.stringifyException(ex));\n        }\n        LOG.error(\"Unable to connect to transaction database \" + StringUtils.stringifyException(e));\n      } finally {\n        close(rs, stmt, null);\n        close(null, pStmt, dbConn);\n      }\n    } catch (RetryException e) {\n      markFailed(ci);\n    }\n  }",
                "code_after_change": "  public void markFailed(CompactionInfo ci) throws MetaException {//todo: this should not throw\n    //todo: this should take \"comment\" as parameter to set in CC_META_INFO to provide some context for the failure\n    try {\n      Connection dbConn = null;\n      Statement stmt = null;\n      PreparedStatement pStmt = null;\n      ResultSet rs = null;\n      try {\n        dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);\n        stmt = dbConn.createStatement();\n        rs = stmt.executeQuery(\"select CQ_ID, CQ_DATABASE, CQ_TABLE, CQ_PARTITION, CQ_STATE, CQ_TYPE, CQ_WORKER_ID, CQ_START, CQ_RUN_AS, CQ_HIGHEST_TXN_ID, CQ_META_INFO, CQ_HADOOP_JOB_ID from COMPACTION_QUEUE WHERE CQ_ID = \" + ci.id);\n        if(rs.next()) {\n          ci = CompactionInfo.loadFullFromCompactionQueue(rs);\n          String s = \"delete from COMPACTION_QUEUE where cq_id = \" + ci.id;\n          LOG.debug(\"Going to execute update <\" + s + \">\");\n          int updCnt = stmt.executeUpdate(s);\n        }\n        else {\n          if(ci.id > 0) {\n            //the record with valid CQ_ID has disappeared - this is a sign of something wrong\n            throw new IllegalStateException(\"No record with CQ_ID=\" + ci.id + \" found in COMPACTION_QUEUE\");\n          }\n        }\n        if(ci.id == 0) {\n          //The failure occurred before we even made an entry in COMPACTION_QUEUE\n          //generate ID so that we can make an entry in COMPLETED_COMPACTIONS\n          ci.id = generateCompactionQueueId(stmt);\n          //mostly this indicates that the Initiator is paying attention to some table even though\n          //compactions are not happening.\n          ci.state = ATTEMPTED_STATE;\n          //this is not strictly accurate, but 'type' cannot be null.\n          ci.type = CompactionType.MINOR;\n        }\n        else {\n          ci.state = FAILED_STATE;\n        }\n        close(rs, stmt, null);\n\n        pStmt = dbConn.prepareStatement(\"insert into COMPLETED_COMPACTIONS(CC_ID, CC_DATABASE, CC_TABLE, CC_PARTITION, CC_STATE, CC_TYPE, CC_WORKER_ID, CC_START, CC_END, CC_RUN_AS, CC_HIGHEST_TXN_ID, CC_META_INFO, CC_HADOOP_JOB_ID) VALUES(?,?,?,?,?, ?,?,?,?,?, ?,?,?)\");\n        CompactionInfo.insertIntoCompletedCompactions(pStmt, ci, getDbTime(dbConn));\n        int updCount = pStmt.executeUpdate();\n        LOG.debug(\"Going to commit\");\n        closeStmt(pStmt);\n        dbConn.commit();\n      } catch (SQLException e) {\n        LOG.error(\"Unable to delete from compaction queue \" + e.getMessage());\n        LOG.debug(\"Going to rollback\");\n        rollbackDBConn(dbConn);\n        try {\n          checkRetryable(dbConn, e, \"markFailed(\" + ci + \")\");\n        }\n        catch(MetaException ex) {\n          LOG.error(\"Unable to connect to transaction database \" + StringUtils.stringifyException(ex));\n        }\n        LOG.error(\"Unable to connect to transaction database \" + StringUtils.stringifyException(e));\n      } finally {\n        close(rs, stmt, null);\n        close(null, pStmt, dbConn);\n      }\n    } catch (RetryException e) {\n      markFailed(ci);\n    }\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler.checkForDeletion": {
                "code_before_change": "  private void checkForDeletion(List<Long> deleteSet, CompactionInfo ci, RetentionCounters rc) {\n    switch (ci.state) {\n      case ATTEMPTED_STATE:\n        if(--rc.attemptedRetention < 0) {\n          deleteSet.add(ci.id);\n        }\n        break;\n      case FAILED_STATE:\n        if(--rc.failedRetention < 0) {\n          deleteSet.add(ci.id);\n        }\n        break;\n      case SUCCEEDED_STATE:\n        if(--rc.succeededRetention < 0) {\n          deleteSet.add(ci.id);\n        }\n        break;\n      default:\n        //do nothing to hanlde future RU/D where we may want to add new state types\n    }\n  }",
                "code_after_change": "  private void checkForDeletion(List<Long> deleteSet, CompactionInfo ci, RetentionCounters rc) {\n    switch (ci.state) {\n      case ATTEMPTED_STATE:\n        if(--rc.attemptedRetention < 0) {\n          deleteSet.add(ci.id);\n        }\n        break;\n      case FAILED_STATE:\n        if(--rc.failedRetention < 0) {\n          deleteSet.add(ci.id);\n        }\n        break;\n      case SUCCEEDED_STATE:\n        if(--rc.succeededRetention < 0) {\n          deleteSet.add(ci.id);\n        }\n        break;\n      default:\n        //do nothing to hanlde future RU/D where we may want to add new state types\n    }\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.showCompact": {
                "code_before_change": "  public ShowCompactResponse showCompact(ShowCompactRequest rqst) throws MetaException {\n    ShowCompactResponse response = new ShowCompactResponse(new ArrayList<ShowCompactResponseElement>());\n    Connection dbConn = null;\n    Statement stmt = null;\n    try {\n      try {\n        dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);\n        stmt = dbConn.createStatement();\n        String s = \"select cq_database, cq_table, cq_partition, cq_state, cq_type, cq_worker_id, \" +\n          \"cq_start, -1 cc_end, cq_run_as, cq_hadoop_job_id, cq_id from COMPACTION_QUEUE union all \" +\n          \"select cc_database, cc_table, cc_partition, cc_state, cc_type, cc_worker_id, \" +\n          \"cc_start, cc_end, cc_run_as, cc_hadoop_job_id, cc_id from COMPLETED_COMPACTIONS\";\n        //what I want is order by cc_end desc, cc_start asc (but derby has a bug https://issues.apache.org/jira/browse/DERBY-6013)\n        //to sort so that currently running jobs are at the end of the list (bottom of screen)\n        //and currently running ones are in sorted by start time\n        //w/o order by likely currently running compactions will be first (LHS of Union)\n        LOG.debug(\"Going to execute query <\" + s + \">\");\n        ResultSet rs = stmt.executeQuery(s);\n        while (rs.next()) {\n          ShowCompactResponseElement e = new ShowCompactResponseElement();\n          e.setDbname(rs.getString(1));\n          e.setTablename(rs.getString(2));\n          e.setPartitionname(rs.getString(3));\n          switch (rs.getString(4).charAt(0)) {\n            case INITIATED_STATE: e.setState(INITIATED_RESPONSE); break;\n            case WORKING_STATE: e.setState(WORKING_RESPONSE); break;\n            case READY_FOR_CLEANING: e.setState(CLEANING_RESPONSE); break;\n            case FAILED_STATE: e.setState(FAILED_RESPONSE); break;\n            case SUCCEEDED_STATE: e.setState(SUCCEEDED_RESPONSE); break;\n            default:\n              //do nothing to handle RU/D if we add another status\n          }\n          switch (rs.getString(5).charAt(0)) {\n            case MAJOR_TYPE: e.setType(CompactionType.MAJOR); break;\n            case MINOR_TYPE: e.setType(CompactionType.MINOR); break;\n            default:\n              //do nothing to handle RU/D if we add another status\n          }\n          e.setWorkerid(rs.getString(6));\n          e.setStart(rs.getLong(7));\n          long endTime = rs.getLong(8);\n          if(endTime != -1) {\n            e.setEndTime(endTime);\n          }\n          e.setRunAs(rs.getString(9));\n          e.setHadoopJobId(rs.getString(10));\n          long id = rs.getLong(11);//for debugging\n          response.addToCompacts(e);\n        }\n        LOG.debug(\"Going to rollback\");\n        dbConn.rollback();\n      } catch (SQLException e) {\n        LOG.debug(\"Going to rollback\");\n        rollbackDBConn(dbConn);\n        checkRetryable(dbConn, e, \"showCompact(\" + rqst + \")\");\n        throw new MetaException(\"Unable to select from transaction database \" +\n          StringUtils.stringifyException(e));\n      } finally {\n        closeStmt(stmt);\n        closeDbConn(dbConn);\n      }\n      return response;\n    } catch (RetryException e) {\n      return showCompact(rqst);\n    }\n  }",
                "code_after_change": "  public ShowCompactResponse showCompact(ShowCompactRequest rqst) throws MetaException {\n    ShowCompactResponse response = new ShowCompactResponse(new ArrayList<ShowCompactResponseElement>());\n    Connection dbConn = null;\n    Statement stmt = null;\n    try {\n      try {\n        dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);\n        stmt = dbConn.createStatement();\n        String s = \"select cq_database, cq_table, cq_partition, cq_state, cq_type, cq_worker_id, \" +\n          \"cq_start, -1 cc_end, cq_run_as, cq_hadoop_job_id, cq_id from COMPACTION_QUEUE union all \" +\n          \"select cc_database, cc_table, cc_partition, cc_state, cc_type, cc_worker_id, \" +\n          \"cc_start, cc_end, cc_run_as, cc_hadoop_job_id, cc_id from COMPLETED_COMPACTIONS\";\n        //what I want is order by cc_end desc, cc_start asc (but derby has a bug https://issues.apache.org/jira/browse/DERBY-6013)\n        //to sort so that currently running jobs are at the end of the list (bottom of screen)\n        //and currently running ones are in sorted by start time\n        //w/o order by likely currently running compactions will be first (LHS of Union)\n        LOG.debug(\"Going to execute query <\" + s + \">\");\n        ResultSet rs = stmt.executeQuery(s);\n        while (rs.next()) {\n          ShowCompactResponseElement e = new ShowCompactResponseElement();\n          e.setDbname(rs.getString(1));\n          e.setTablename(rs.getString(2));\n          e.setPartitionname(rs.getString(3));\n          switch (rs.getString(4).charAt(0)) {\n            case INITIATED_STATE: e.setState(INITIATED_RESPONSE); break;\n            case WORKING_STATE: e.setState(WORKING_RESPONSE); break;\n            case READY_FOR_CLEANING: e.setState(CLEANING_RESPONSE); break;\n            case FAILED_STATE: e.setState(FAILED_RESPONSE); break;\n            case SUCCEEDED_STATE: e.setState(SUCCEEDED_RESPONSE); break;\n            case ATTEMPTED_STATE: e.setState(ATTEMPTED_RESPONSE); break;\n            default:\n              //do nothing to handle RU/D if we add another status\n          }\n          switch (rs.getString(5).charAt(0)) {\n            case MAJOR_TYPE: e.setType(CompactionType.MAJOR); break;\n            case MINOR_TYPE: e.setType(CompactionType.MINOR); break;\n            default:\n              //do nothing to handle RU/D if we add another status\n          }\n          e.setWorkerid(rs.getString(6));\n          e.setStart(rs.getLong(7));\n          long endTime = rs.getLong(8);\n          if(endTime != -1) {\n            e.setEndTime(endTime);\n          }\n          e.setRunAs(rs.getString(9));\n          e.setHadoopJobId(rs.getString(10));\n          long id = rs.getLong(11);//for debugging\n          response.addToCompacts(e);\n        }\n        LOG.debug(\"Going to rollback\");\n        dbConn.rollback();\n      } catch (SQLException e) {\n        LOG.debug(\"Going to rollback\");\n        rollbackDBConn(dbConn);\n        checkRetryable(dbConn, e, \"showCompact(\" + rqst + \")\");\n        throw new MetaException(\"Unable to select from transaction database \" +\n          StringUtils.stringifyException(e));\n      } finally {\n        closeStmt(stmt);\n        closeDbConn(dbConn);\n      }\n      return response;\n    } catch (RetryException e) {\n      return showCompact(rqst);\n    }\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.compact": {
                "code_before_change": "  public long compact(CompactionRequest rqst) throws MetaException {\n    // Put a compaction request in the queue.\n    try {\n      Connection dbConn = null;\n      Statement stmt = null;\n      try {\n        lockInternal();\n        dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);\n        stmt = dbConn.createStatement();\n\n        // Get the id for the next entry in the queue\n        String s = addForUpdateClause(\"select ncq_next from NEXT_COMPACTION_QUEUE_ID\");\n        LOG.debug(\"going to execute query <\" + s + \">\");\n        ResultSet rs = stmt.executeQuery(s);\n        if (!rs.next()) {\n          LOG.debug(\"Going to rollback\");\n          dbConn.rollback();\n          throw new MetaException(\"Transaction tables not properly initiated, \" +\n            \"no record found in next_compaction_queue_id\");\n        }\n        long id = rs.getLong(1);\n        s = \"update NEXT_COMPACTION_QUEUE_ID set ncq_next = \" + (id + 1);\n        LOG.debug(\"Going to execute update <\" + s + \">\");\n        stmt.executeUpdate(s);\n\n        StringBuilder buf = new StringBuilder(\"insert into COMPACTION_QUEUE (cq_id, cq_database, \" +\n          \"cq_table, \");\n        String partName = rqst.getPartitionname();\n        if (partName != null) buf.append(\"cq_partition, \");\n        buf.append(\"cq_state, cq_type\");\n        if (rqst.getRunas() != null) buf.append(\", cq_run_as\");\n        buf.append(\") values (\");\n        buf.append(id);\n        buf.append(\", '\");\n        buf.append(rqst.getDbname());\n        buf.append(\"', '\");\n        buf.append(rqst.getTablename());\n        buf.append(\"', '\");\n        if (partName != null) {\n          buf.append(partName);\n          buf.append(\"', '\");\n        }\n        buf.append(INITIATED_STATE);\n        buf.append(\"', '\");\n        switch (rqst.getType()) {\n          case MAJOR:\n            buf.append(MAJOR_TYPE);\n            break;\n\n          case MINOR:\n            buf.append(MINOR_TYPE);\n            break;\n\n          default:\n            LOG.debug(\"Going to rollback\");\n            dbConn.rollback();\n            throw new MetaException(\"Unexpected compaction type \" + rqst.getType().toString());\n        }\n        if (rqst.getRunas() != null) {\n          buf.append(\"', '\");\n          buf.append(rqst.getRunas());\n        }\n        buf.append(\"')\");\n        s = buf.toString();\n        LOG.debug(\"Going to execute update <\" + s + \">\");\n        stmt.executeUpdate(s);\n        LOG.debug(\"Going to commit\");\n        dbConn.commit();\n        return id;\n      } catch (SQLException e) {\n        LOG.debug(\"Going to rollback\");\n        rollbackDBConn(dbConn);\n        checkRetryable(dbConn, e, \"compact(\" + rqst + \")\");\n        throw new MetaException(\"Unable to select from transaction database \" +\n          StringUtils.stringifyException(e));\n      } finally {\n        closeStmt(stmt);\n        closeDbConn(dbConn);\n        unlockInternal();\n      }\n    } catch (RetryException e) {\n      return compact(rqst);\n    }\n  }",
                "code_after_change": "  public long compact(CompactionRequest rqst) throws MetaException {\n    // Put a compaction request in the queue.\n    try {\n      Connection dbConn = null;\n      Statement stmt = null;\n      try {\n        lockInternal();\n        dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);\n        stmt = dbConn.createStatement();\n        \n        long id = generateCompactionQueueId(stmt);\n\n        StringBuilder buf = new StringBuilder(\"insert into COMPACTION_QUEUE (cq_id, cq_database, \" +\n          \"cq_table, \");\n        String partName = rqst.getPartitionname();\n        if (partName != null) buf.append(\"cq_partition, \");\n        buf.append(\"cq_state, cq_type\");\n        if (rqst.getRunas() != null) buf.append(\", cq_run_as\");\n        buf.append(\") values (\");\n        buf.append(id);\n        buf.append(\", '\");\n        buf.append(rqst.getDbname());\n        buf.append(\"', '\");\n        buf.append(rqst.getTablename());\n        buf.append(\"', '\");\n        if (partName != null) {\n          buf.append(partName);\n          buf.append(\"', '\");\n        }\n        buf.append(INITIATED_STATE);\n        buf.append(\"', '\");\n        switch (rqst.getType()) {\n          case MAJOR:\n            buf.append(MAJOR_TYPE);\n            break;\n\n          case MINOR:\n            buf.append(MINOR_TYPE);\n            break;\n\n          default:\n            LOG.debug(\"Going to rollback\");\n            dbConn.rollback();\n            throw new MetaException(\"Unexpected compaction type \" + rqst.getType().toString());\n        }\n        if (rqst.getRunas() != null) {\n          buf.append(\"', '\");\n          buf.append(rqst.getRunas());\n        }\n        buf.append(\"')\");\n        String s = buf.toString();\n        LOG.debug(\"Going to execute update <\" + s + \">\");\n        stmt.executeUpdate(s);\n        LOG.debug(\"Going to commit\");\n        dbConn.commit();\n        return id;\n      } catch (SQLException e) {\n        LOG.debug(\"Going to rollback\");\n        rollbackDBConn(dbConn);\n        checkRetryable(dbConn, e, \"compact(\" + rqst + \")\");\n        throw new MetaException(\"Unable to select from transaction database \" +\n          StringUtils.stringifyException(e));\n      } finally {\n        closeStmt(stmt);\n        closeDbConn(dbConn);\n        unlockInternal();\n      }\n    } catch (RetryException e) {\n      return compact(rqst);\n    }\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.Initiator.run": {
                "code_before_change": "  public void run() {\n    // Make sure nothing escapes this run method and kills the metastore at large,\n    // so wrap it in a big catch Throwable statement.\n    try {\n      recoverFailedCompactions(false);\n\n      int abortedThreshold = HiveConf.getIntVar(conf,\n          HiveConf.ConfVars.HIVE_COMPACTOR_ABORTEDTXN_THRESHOLD);\n\n      // Make sure we run through the loop once before checking to stop as this makes testing\n      // much easier.  The stop value is only for testing anyway and not used when called from\n      // HiveMetaStore.\n      do {\n        long startedAt = -1;\n        TxnStore.MutexAPI.LockHandle handle = null;\n\n        // Wrap the inner parts of the loop in a catch throwable so that any errors in the loop\n        // don't doom the entire thread.\n        try {\n          handle = txnHandler.getMutexAPI().acquireLock(TxnStore.MUTEX_KEY.Initiator.name());\n          startedAt = System.currentTimeMillis();\n          //todo: add method to only get current i.e. skip history - more efficient\n          ShowCompactResponse currentCompactions = txnHandler.showCompact(new ShowCompactRequest());\n          ValidTxnList txns =\n              TxnUtils.createValidCompactTxnList(txnHandler.getOpenTxnsInfo());\n          Set<CompactionInfo> potentials = txnHandler.findPotentialCompactions(abortedThreshold);\n          LOG.debug(\"Found \" + potentials.size() + \" potential compactions, \" +\n              \"checking to see if we should compact any of them\");\n          for (CompactionInfo ci : potentials) {\n            LOG.info(\"Checking to see if we should compact \" + ci.getFullPartitionName());\n            try {\n              Table t = resolveTable(ci);\n              if (t == null) {\n                // Most likely this means it's a temp table\n                LOG.info(\"Can't find table \" + ci.getFullTableName() + \", assuming it's a temp \" +\n                    \"table or has been dropped and moving on.\");\n                continue;\n              }\n\n              // check if no compaction set for this table\n              if (noAutoCompactSet(t)) {\n                LOG.info(\"Table \" + tableName(t) + \" marked \" + hive_metastoreConstants.TABLE_NO_AUTO_COMPACT + \"=true so we will not compact it.\");\n                continue;\n              }\n\n              // Check to see if this is a table level request on a partitioned table.  If so,\n              // then it's a dynamic partitioning case and we shouldn't check the table itself.\n              if (t.getPartitionKeys() != null && t.getPartitionKeys().size() > 0 &&\n                  ci.partName  == null) {\n                LOG.debug(\"Skipping entry for \" + ci.getFullTableName() + \" as it is from dynamic\" +\n                    \" partitioning\");\n                continue;\n              }\n\n              // Check if we already have initiated or are working on a compaction for this partition\n              // or table.  If so, skip it.  If we are just waiting on cleaning we can still check,\n              // as it may be time to compact again even though we haven't cleaned.\n              //todo: this is not robust.  You can easily run Alter Table to start a compaction between\n              //the time currentCompactions is generated and now\n              if (lookForCurrentCompactions(currentCompactions, ci)) {\n                LOG.debug(\"Found currently initiated or working compaction for \" +\n                    ci.getFullPartitionName() + \" so we will not initiate another compaction\");\n                continue;\n              }\n              if(txnHandler.checkFailedCompactions(ci)) {\n                //todo: make 'a' state entry in completed_compactions\n                LOG.warn(\"Will not initiate compaction for \" + ci.getFullPartitionName() + \" since last 3 attempts to compact it failed.\");\n                continue;\n              }\n\n              // Figure out who we should run the file operations as\n              Partition p = resolvePartition(ci);\n              if (p == null && ci.partName != null) {\n                LOG.info(\"Can't find partition \" + ci.getFullPartitionName() +\n                    \", assuming it has been dropped and moving on.\");\n                continue;\n              }\n              StorageDescriptor sd = resolveStorageDescriptor(t, p);\n              String runAs = findUserToRunAs(sd.getLocation(), t);\n              /*Future thought: checkForCompaction will check a lot of file metadata and may be expensive.\n              * Long term we should consider having a thread pool here and running checkForCompactionS\n              * in parallel*/\n              CompactionType compactionNeeded = checkForCompaction(ci, txns, sd, runAs);\n              if (compactionNeeded != null) requestCompaction(ci, runAs, compactionNeeded);\n            } catch (Throwable t) {\n              LOG.error(\"Caught exception while trying to determine if we should compact \" +\n                  ci + \".  Marking clean to avoid repeated failures, \" +\n                  \"\" + StringUtils.stringifyException(t));\n              txnHandler.markFailed(ci);\n            }\n          }\n\n          // Check for timed out remote workers.\n          recoverFailedCompactions(true);\n\n          // Clean anything from the txns table that has no components left in txn_components.\n          txnHandler.cleanEmptyAbortedTxns();\n        } catch (Throwable t) {\n          LOG.error(\"Initiator loop caught unexpected exception this time through the loop: \" +\n              StringUtils.stringifyException(t));\n        }\n        finally {\n          if(handle != null) {\n            handle.releaseLocks();\n          }\n        }\n\n        long elapsedTime = System.currentTimeMillis() - startedAt;\n        if (elapsedTime >= checkInterval || stop.get())  continue;\n        else Thread.sleep(checkInterval - elapsedTime);\n\n      } while (!stop.get());\n    } catch (Throwable t) {\n      LOG.error(\"Caught an exception in the main loop of compactor initiator, exiting \" +\n          StringUtils.stringifyException(t));\n    }\n  }",
                "code_after_change": "  public void run() {\n    // Make sure nothing escapes this run method and kills the metastore at large,\n    // so wrap it in a big catch Throwable statement.\n    try {\n      recoverFailedCompactions(false);\n\n      int abortedThreshold = HiveConf.getIntVar(conf,\n          HiveConf.ConfVars.HIVE_COMPACTOR_ABORTEDTXN_THRESHOLD);\n\n      // Make sure we run through the loop once before checking to stop as this makes testing\n      // much easier.  The stop value is only for testing anyway and not used when called from\n      // HiveMetaStore.\n      do {\n        long startedAt = -1;\n        TxnStore.MutexAPI.LockHandle handle = null;\n\n        // Wrap the inner parts of the loop in a catch throwable so that any errors in the loop\n        // don't doom the entire thread.\n        try {\n          handle = txnHandler.getMutexAPI().acquireLock(TxnStore.MUTEX_KEY.Initiator.name());\n          startedAt = System.currentTimeMillis();\n          //todo: add method to only get current i.e. skip history - more efficient\n          ShowCompactResponse currentCompactions = txnHandler.showCompact(new ShowCompactRequest());\n          ValidTxnList txns =\n              TxnUtils.createValidCompactTxnList(txnHandler.getOpenTxnsInfo());\n          Set<CompactionInfo> potentials = txnHandler.findPotentialCompactions(abortedThreshold);\n          LOG.debug(\"Found \" + potentials.size() + \" potential compactions, \" +\n              \"checking to see if we should compact any of them\");\n          for (CompactionInfo ci : potentials) {\n            LOG.info(\"Checking to see if we should compact \" + ci.getFullPartitionName());\n            try {\n              Table t = resolveTable(ci);\n              if (t == null) {\n                // Most likely this means it's a temp table\n                LOG.info(\"Can't find table \" + ci.getFullTableName() + \", assuming it's a temp \" +\n                    \"table or has been dropped and moving on.\");\n                continue;\n              }\n\n              // check if no compaction set for this table\n              if (noAutoCompactSet(t)) {\n                LOG.info(\"Table \" + tableName(t) + \" marked \" + hive_metastoreConstants.TABLE_NO_AUTO_COMPACT + \"=true so we will not compact it.\");\n                continue;\n              }\n\n              // Check to see if this is a table level request on a partitioned table.  If so,\n              // then it's a dynamic partitioning case and we shouldn't check the table itself.\n              if (t.getPartitionKeys() != null && t.getPartitionKeys().size() > 0 &&\n                  ci.partName  == null) {\n                LOG.debug(\"Skipping entry for \" + ci.getFullTableName() + \" as it is from dynamic\" +\n                    \" partitioning\");\n                continue;\n              }\n\n              // Check if we already have initiated or are working on a compaction for this partition\n              // or table.  If so, skip it.  If we are just waiting on cleaning we can still check,\n              // as it may be time to compact again even though we haven't cleaned.\n              //todo: this is not robust.  You can easily run Alter Table to start a compaction between\n              //the time currentCompactions is generated and now\n              if (lookForCurrentCompactions(currentCompactions, ci)) {\n                LOG.debug(\"Found currently initiated or working compaction for \" +\n                    ci.getFullPartitionName() + \" so we will not initiate another compaction\");\n                continue;\n              }\n              if(txnHandler.checkFailedCompactions(ci)) {\n                LOG.warn(\"Will not initiate compaction for \" + ci.getFullPartitionName() + \" since last \"\n                  + HiveConf.ConfVars.COMPACTOR_INITIATOR_FAILED_THRESHOLD + \" attempts to compact it failed.\");\n                txnHandler.markFailed(ci);\n                continue;\n              }\n\n              // Figure out who we should run the file operations as\n              Partition p = resolvePartition(ci);\n              if (p == null && ci.partName != null) {\n                LOG.info(\"Can't find partition \" + ci.getFullPartitionName() +\n                    \", assuming it has been dropped and moving on.\");\n                continue;\n              }\n              StorageDescriptor sd = resolveStorageDescriptor(t, p);\n              String runAs = findUserToRunAs(sd.getLocation(), t);\n              /*Future thought: checkForCompaction will check a lot of file metadata and may be expensive.\n              * Long term we should consider having a thread pool here and running checkForCompactionS\n              * in parallel*/\n              CompactionType compactionNeeded = checkForCompaction(ci, txns, sd, runAs);\n              if (compactionNeeded != null) requestCompaction(ci, runAs, compactionNeeded);\n            } catch (Throwable t) {\n              LOG.error(\"Caught exception while trying to determine if we should compact \" +\n                  ci + \".  Marking failed to avoid repeated failures, \" +\n                  \"\" + StringUtils.stringifyException(t));\n              txnHandler.markFailed(ci);\n            }\n          }\n\n          // Check for timed out remote workers.\n          recoverFailedCompactions(true);\n\n          // Clean anything from the txns table that has no components left in txn_components.\n          txnHandler.cleanEmptyAbortedTxns();\n        } catch (Throwable t) {\n          LOG.error(\"Initiator loop caught unexpected exception this time through the loop: \" +\n              StringUtils.stringifyException(t));\n        }\n        finally {\n          if(handle != null) {\n            handle.releaseLocks();\n          }\n        }\n\n        long elapsedTime = System.currentTimeMillis() - startedAt;\n        if (elapsedTime >= checkInterval || stop.get())  continue;\n        else Thread.sleep(checkInterval - elapsedTime);\n\n      } while (!stop.get());\n    } catch (Throwable t) {\n      LOG.error(\"Caught an exception in the main loop of compactor initiator, exiting \" +\n          StringUtils.stringifyException(t));\n    }\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler.checkFailedCompactions": {
                "code_before_change": "  public boolean checkFailedCompactions(CompactionInfo ci) throws MetaException {\n    Connection dbConn = null;\n    Statement stmt = null;\n    ResultSet rs = null;\n    try {\n      try {\n        dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);\n        stmt = dbConn.createStatement();\n        rs = stmt.executeQuery(\"select CC_STATE from COMPLETED_COMPACTIONS where \" +\n          \"CC_DATABASE = \" + quoteString(ci.dbname) + \" and \" +\n          \"CC_TABLE = \" + quoteString(ci.tableName) +\n          (ci.partName != null ? \"and CC_PARTITION = \" + quoteString(ci.partName) : \"\") +\n          \" order by CC_ID desc\");\n        int numFailed = 0;\n        int numTotal = 0;\n        int failedThreshold = conf.getIntVar(HiveConf.ConfVars.COMPACTOR_INITIATOR_FAILED_THRESHOLD);\n        while(rs.next() && ++numTotal <= failedThreshold) {\n          if(rs.getString(1).charAt(0) == FAILED_STATE) {\n            numFailed++;\n          }\n          else {\n            numFailed--;\n          }\n        }\n        return numFailed == failedThreshold;\n      }\n      catch (SQLException e) {\n        LOG.error(\"Unable to delete from compaction queue \" + e.getMessage());\n        LOG.debug(\"Going to rollback\");\n        rollbackDBConn(dbConn);\n        checkRetryable(dbConn, e, \"checkFailedCompactions(\" + ci + \")\");\n        LOG.error(\"Unable to connect to transaction database \" + StringUtils.stringifyException(e));\n        return false;//weren't able to check\n      } finally {\n        close(rs, stmt, dbConn);\n      }\n    } catch (RetryException e) {\n      return checkFailedCompactions(ci);\n    }\n  }",
                "code_after_change": "  public boolean checkFailedCompactions(CompactionInfo ci) throws MetaException {\n    Connection dbConn = null;\n    Statement stmt = null;\n    ResultSet rs = null;\n    try {\n      try {\n        dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);\n        stmt = dbConn.createStatement();\n        rs = stmt.executeQuery(\"select CC_STATE from COMPLETED_COMPACTIONS where \" +\n          \"CC_DATABASE = \" + quoteString(ci.dbname) + \" and \" +\n          \"CC_TABLE = \" + quoteString(ci.tableName) +\n          (ci.partName != null ? \"and CC_PARTITION = \" + quoteString(ci.partName) : \"\") +\n          \" and CC_STATE != \" + quoteChar(ATTEMPTED_STATE) + \" order by CC_ID desc\");\n        int numFailed = 0;\n        int numTotal = 0;\n        int failedThreshold = conf.getIntVar(HiveConf.ConfVars.COMPACTOR_INITIATOR_FAILED_THRESHOLD);\n        while(rs.next() && ++numTotal <= failedThreshold) {\n          if(rs.getString(1).charAt(0) == FAILED_STATE) {\n            numFailed++;\n          }\n          else {\n            numFailed--;\n          }\n        }\n        return numFailed == failedThreshold;\n      }\n      catch (SQLException e) {\n        LOG.error(\"Unable to delete from compaction queue \" + e.getMessage());\n        LOG.debug(\"Going to rollback\");\n        rollbackDBConn(dbConn);\n        checkRetryable(dbConn, e, \"checkFailedCompactions(\" + ci + \")\");\n        LOG.error(\"Unable to connect to transaction database \" + StringUtils.stringifyException(e));\n        return false;//weren't able to check\n      } finally {\n        close(rs, stmt, dbConn);\n      }\n    } catch (RetryException e) {\n      return checkFailedCompactions(ci);\n    }\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Buggy Method"
            },
            "fix_suggestion": "Correct",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the method 'CompactionTxnHandler.markFailed' as the location of the error, which is a ground truth method, but it points to where the error occurred rather than where the fix was made, hence 'Buggy Method'. The fix suggestion in the report aligns with the developer's fix, as it suggests recording an entry in 'completed_compaction_queue' even if an entry in 'compaction_queue' was never made, which matches the changes made in 'markFailed'. The problem location is precise as it directly mentions 'CompactionTxnHandler.markFailed', which is a ground truth method. There is no wrong information in the bug report as all details are relevant to the context of the bug."
        }
    },
    {
        "filename": "HIVE-17758.json",
        "code_diff": {
            "metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.run": {
                "code_before_change": "    public T run(boolean initTable) throws MetaException, NoSuchObjectException {\n      try {\n        start(initTable);\n        if (doUseDirectSql) {\n          try {\n            directSql.prepareTxn();\n            this.results = getSqlResult(this);\n          } catch (Exception ex) {\n            handleDirectSqlError(ex);\n          }\n        }\n        // Note that this will be invoked in 2 cases:\n        //    1) DirectSQL was disabled to start with;\n        //    2) DirectSQL threw and was disabled in handleDirectSqlError.\n        if (!doUseDirectSql) {\n          this.results = getJdoResult(this);\n        }\n        return commit();\n      } catch (NoSuchObjectException ex) {\n        throw ex;\n      } catch (MetaException ex) {\n        throw ex;\n      } catch (Exception ex) {\n        LOG.error(\"\", ex);\n        throw MetaStoreUtils.newMetaException(ex);\n      } finally {\n        close();\n      }\n    }",
                "code_after_change": "    public T run(boolean initTable) throws MetaException, NoSuchObjectException {\n      try {\n        start(initTable);\n        if (doUseDirectSql) {\n          try {\n            directSql.prepareTxn();\n            this.results = getSqlResult(this);\n          } catch (Exception ex) {\n            handleDirectSqlError(ex);\n          }\n        }\n        // Note that this will be invoked in 2 cases:\n        //    1) DirectSQL was disabled to start with;\n        //    2) DirectSQL threw and was disabled in handleDirectSqlError.\n        if (!doUseDirectSql) {\n          this.results = getJdoResult(this);\n        }\n        return commit();\n      } catch (NoSuchObjectException ex) {\n        throw ex;\n      } catch (MetaException ex) {\n        throw ex;\n      } catch (Exception ex) {\n        LOG.error(\"\", ex);\n        throw MetaStoreUtils.newMetaException(ex);\n      } finally {\n        close();\n      }\n    }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the issue with the retry logic and the incorrect use of a default value, which is related to the stack trace context but does not precisely identify the root cause in the ground truth method 'metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.run'. The report does not provide any specific fix suggestion, hence 'Missing' for fix suggestion. The problem location is partially identified as it mentions methods in the same stack trace context but not the exact ground truth method. There is no wrong information in the bug report as it accurately describes the issue context."
        }
    },
    {
        "filename": "HIVE-14898.json",
        "code_diff": {
            "service.src.java.org.apache.hive.service.cli.thrift.ThriftHttpServlet.doPost": {
                "code_before_change": "  protected void doPost(HttpServletRequest request, HttpServletResponse response)\n      throws ServletException, IOException {\n    String clientUserName = null;\n    String clientIpAddress;\n    boolean requireNewCookie = false;\n\n    try {\n      if (hiveConf.getBoolean(ConfVars.HIVE_SERVER2_XSRF_FILTER_ENABLED.varname,false)){\n        boolean continueProcessing = Utils.doXsrfFilter(request,response,null,null);\n        if (!continueProcessing){\n          LOG.warn(\"Request did not have valid XSRF header, rejecting.\");\n          return;\n        }\n      }\n      // If the cookie based authentication is already enabled, parse the\n      // request and validate the request cookies.\n      if (isCookieAuthEnabled) {\n        clientUserName = validateCookie(request);\n        requireNewCookie = (clientUserName == null);\n        if (requireNewCookie) {\n          LOG.info(\"Could not validate cookie sent, will try to generate a new cookie\");\n        }\n      }\n      // If the cookie based authentication is not enabled or the request does\n      // not have a valid cookie, use the kerberos or password based authentication\n      // depending on the server setup.\n      if (clientUserName == null) {\n        // For a kerberos setup\n        if (isKerberosAuthMode(authType)) {\n          String delegationToken = request.getHeader(HIVE_DELEGATION_TOKEN_HEADER);\n          // Each http request must have an Authorization header\n          if ((delegationToken != null) && (!delegationToken.isEmpty())) {\n            clientUserName = doTokenAuth(request, response);\n          } else {\n            clientUserName = doKerberosAuth(request);\n          }\n        }\n        // For password based authentication\n        else {\n          clientUserName = doPasswdAuth(request, authType);\n        }\n      }\n      LOG.debug(\"Client username: \" + clientUserName);\n\n      // Set the thread local username to be used for doAs if true\n      SessionManager.setUserName(clientUserName);\n\n      // find proxy user if any from query param\n      String doAsQueryParam = getDoAsQueryParam(request.getQueryString());\n      if (doAsQueryParam != null) {\n        SessionManager.setProxyUserName(doAsQueryParam);\n      }\n\n      clientIpAddress = request.getRemoteAddr();\n      LOG.debug(\"Client IP Address: \" + clientIpAddress);\n      // Set the thread local ip address\n      SessionManager.setIpAddress(clientIpAddress);\n\n      // get forwarded hosts address\n      String forwarded_for = request.getHeader(X_FORWARDED_FOR);\n      if (forwarded_for != null) {\n        LOG.debug(\"{}:{}\", X_FORWARDED_FOR, forwarded_for);\n        List<String> forwardedAddresses = Arrays.asList(forwarded_for.split(\",\"));\n        SessionManager.setForwardedAddresses(forwardedAddresses);\n      } else {\n        SessionManager.setForwardedAddresses(Collections.<String>emptyList());\n      }\n\n      // Generate new cookie and add it to the response\n      if (requireNewCookie &&\n          !authType.equalsIgnoreCase(HiveAuthFactory.AuthTypes.NOSASL.toString())) {\n        String cookieToken = HttpAuthUtils.createCookieToken(clientUserName);\n        Cookie hs2Cookie = createCookie(signer.signCookie(cookieToken));\n\n        if (isHttpOnlyCookie) {\n          response.setHeader(\"SET-COOKIE\", getHttpOnlyCookieHeader(hs2Cookie));\n        } else {\n          response.addCookie(hs2Cookie);\n        }\n        LOG.info(\"Cookie added for clientUserName \" + clientUserName);\n      }\n      super.doPost(request, response);\n    }\n    catch (HttpAuthenticationException e) {\n      LOG.error(\"Error: \", e);\n      // Send a 401 to the client\n      response.setStatus(HttpServletResponse.SC_UNAUTHORIZED);\n      if(isKerberosAuthMode(authType)) {\n        response.addHeader(HttpAuthUtils.WWW_AUTHENTICATE, HttpAuthUtils.NEGOTIATE);\n      }\n      response.getWriter().println(\"Authentication Error: \" + e.getMessage());\n    }\n    finally {\n      // Clear the thread locals\n      SessionManager.clearUserName();\n      SessionManager.clearIpAddress();\n      SessionManager.clearProxyUserName();\n      SessionManager.clearForwardedAddresses();\n    }\n  }",
                "code_after_change": "  protected void doPost(HttpServletRequest request, HttpServletResponse response)\n      throws ServletException, IOException {\n    String clientUserName = null;\n    String clientIpAddress;\n    boolean requireNewCookie = false;\n\n    try {\n      if (hiveConf.getBoolean(ConfVars.HIVE_SERVER2_XSRF_FILTER_ENABLED.varname,false)){\n        boolean continueProcessing = Utils.doXsrfFilter(request,response,null,null);\n        if (!continueProcessing){\n          LOG.warn(\"Request did not have valid XSRF header, rejecting.\");\n          return;\n        }\n      }\n      // If the cookie based authentication is already enabled, parse the\n      // request and validate the request cookies.\n      if (isCookieAuthEnabled) {\n        clientUserName = validateCookie(request);\n        requireNewCookie = (clientUserName == null);\n        if (requireNewCookie) {\n          LOG.info(\"Could not validate cookie sent, will try to generate a new cookie\");\n        }\n      }\n      // If the cookie based authentication is not enabled or the request does\n      // not have a valid cookie, use the kerberos or password based authentication\n      // depending on the server setup.\n      if (clientUserName == null) {\n        // For a kerberos setup\n        if (isKerberosAuthMode(authType)) {\n          String delegationToken = request.getHeader(HIVE_DELEGATION_TOKEN_HEADER);\n          // Each http request must have an Authorization header\n          if ((delegationToken != null) && (!delegationToken.isEmpty())) {\n            clientUserName = doTokenAuth(request, response);\n          } else {\n            clientUserName = doKerberosAuth(request);\n          }\n        }\n        // For password based authentication\n        else {\n          clientUserName = doPasswdAuth(request, authType);\n        }\n      }\n      LOG.debug(\"Client username: \" + clientUserName);\n\n      // Set the thread local username to be used for doAs if true\n      SessionManager.setUserName(clientUserName);\n\n      // find proxy user if any from query param\n      String doAsQueryParam = getDoAsQueryParam(request.getQueryString());\n      if (doAsQueryParam != null) {\n        SessionManager.setProxyUserName(doAsQueryParam);\n      }\n\n      clientIpAddress = request.getRemoteAddr();\n      LOG.debug(\"Client IP Address: \" + clientIpAddress);\n      // Set the thread local ip address\n      SessionManager.setIpAddress(clientIpAddress);\n\n      // get forwarded hosts address\n      String forwarded_for = request.getHeader(X_FORWARDED_FOR);\n      if (forwarded_for != null) {\n        LOG.debug(\"{}:{}\", X_FORWARDED_FOR, forwarded_for);\n        List<String> forwardedAddresses = Arrays.asList(forwarded_for.split(\",\"));\n        SessionManager.setForwardedAddresses(forwardedAddresses);\n      } else {\n        SessionManager.setForwardedAddresses(Collections.<String>emptyList());\n      }\n\n      // Generate new cookie and add it to the response\n      if (requireNewCookie &&\n          !authType.equalsIgnoreCase(HiveAuthConstants.AuthTypes.NOSASL.toString())) {\n        String cookieToken = HttpAuthUtils.createCookieToken(clientUserName);\n        Cookie hs2Cookie = createCookie(signer.signCookie(cookieToken));\n\n        if (isHttpOnlyCookie) {\n          response.setHeader(\"SET-COOKIE\", getHttpOnlyCookieHeader(hs2Cookie));\n        } else {\n          response.addCookie(hs2Cookie);\n        }\n        LOG.info(\"Cookie added for clientUserName \" + clientUserName);\n      }\n      super.doPost(request, response);\n    }\n    catch (HttpAuthenticationException e) {\n      // Ignore HttpEmptyAuthenticationException, it is normal for knox\n      // to send a request with empty header\n      if (!(e instanceof HttpEmptyAuthenticationException)) {\n        LOG.error(\"Error: \", e);\n      }\n      // Send a 401 to the client\n      response.setStatus(HttpServletResponse.SC_UNAUTHORIZED);\n      if(isKerberosAuthMode(authType)) {\n        response.addHeader(HttpAuthUtils.WWW_AUTHENTICATE, HttpAuthUtils.NEGOTIATE);\n      }\n      response.getWriter().println(\"Authentication Error: \" + e.getMessage());\n    }\n    finally {\n      // Clear the thread locals\n      SessionManager.clearUserName();\n      SessionManager.clearIpAddress();\n      SessionManager.clearProxyUserName();\n      SessionManager.clearForwardedAddresses();\n    }\n  }"
            },
            "service.src.java.org.apache.hive.service.cli.thrift.ThriftHttpServlet.getAuthHeader": {
                "code_before_change": "  private String getAuthHeader(HttpServletRequest request, String authType)\n      throws HttpAuthenticationException {\n    String authHeader = request.getHeader(HttpAuthUtils.AUTHORIZATION);\n    // Each http request must have an Authorization header\n    if (authHeader == null || authHeader.isEmpty()) {\n      throw new HttpAuthenticationException(\"Authorization header received \" +\n          \"from the client is empty.\");\n    }\n\n    String authHeaderBase64String;\n    int beginIndex;\n    if (isKerberosAuthMode(authType)) {\n      beginIndex = (HttpAuthUtils.NEGOTIATE + \" \").length();\n    }\n    else {\n      beginIndex = (HttpAuthUtils.BASIC + \" \").length();\n    }\n    authHeaderBase64String = authHeader.substring(beginIndex);\n    // Authorization header must have a payload\n    if (authHeaderBase64String == null || authHeaderBase64String.isEmpty()) {\n      throw new HttpAuthenticationException(\"Authorization header received \" +\n          \"from the client does not contain any data.\");\n    }\n    return authHeaderBase64String;\n  }",
                "code_after_change": "  private String getAuthHeader(HttpServletRequest request, String authType)\n      throws HttpAuthenticationException {\n    String authHeader = request.getHeader(HttpAuthUtils.AUTHORIZATION);\n    // Each http request must have an Authorization header\n    if (authHeader == null || authHeader.isEmpty()) {\n      throw new HttpEmptyAuthenticationException(\"Authorization header received \" +\n          \"from the client is empty.\");\n    }\n\n    String authHeaderBase64String;\n    int beginIndex;\n    if (isKerberosAuthMode(authType)) {\n      beginIndex = (HttpAuthUtils.NEGOTIATE + \" \").length();\n    }\n    else {\n      beginIndex = (HttpAuthUtils.BASIC + \" \").length();\n    }\n    authHeaderBase64String = authHeader.substring(beginIndex);\n    // Authorization header must have a payload\n    if (authHeaderBase64String == null || authHeaderBase64String.isEmpty()) {\n      throw new HttpAuthenticationException(\"Authorization header received \" +\n          \"from the client does not contain any data.\");\n    }\n    return authHeaderBase64String;\n  }"
            },
            "service.src.java.org.apache.hive.service.cli.thrift.ThriftHttpServlet.doKerberosAuth": {
                "code_before_change": "  private String doKerberosAuth(HttpServletRequest request)\n      throws HttpAuthenticationException {\n    // Try authenticating with the http/_HOST principal\n    if (httpUGI != null) {\n      try {\n        return httpUGI.doAs(new HttpKerberosServerAction(request, httpUGI));\n      } catch (Exception e) {\n        LOG.info(\"Failed to authenticate with http/_HOST kerberos principal, \" +\n            \"trying with hive/_HOST kerberos principal\");\n      }\n    }\n    // Now try with hive/_HOST principal\n    try {\n      return serviceUGI.doAs(new HttpKerberosServerAction(request, serviceUGI));\n    } catch (Exception e) {\n      LOG.error(\"Failed to authenticate with hive/_HOST kerberos principal\");\n      throw new HttpAuthenticationException(e);\n    }\n\n  }",
                "code_after_change": "  private String doKerberosAuth(HttpServletRequest request)\n      throws HttpAuthenticationException {\n    // Try authenticating with the http/_HOST principal\n    if (httpUGI != null) {\n      try {\n        return httpUGI.doAs(new HttpKerberosServerAction(request, httpUGI));\n      } catch (Exception e) {\n        LOG.info(\"Failed to authenticate with http/_HOST kerberos principal, \" +\n            \"trying with hive/_HOST kerberos principal\");\n      }\n    }\n    // Now try with hive/_HOST principal\n    try {\n      return serviceUGI.doAs(new HttpKerberosServerAction(request, serviceUGI));\n    } catch (Exception e) {\n      if (e.getCause() instanceof HttpEmptyAuthenticationException) {\n        throw (HttpEmptyAuthenticationException)e.getCause();\n      }\n      LOG.error(\"Failed to authenticate with hive/_HOST kerberos principal\");\n      throw new HttpAuthenticationException(e);\n    }\n\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report precisely identifies the root cause by mentioning the issue with the empty auth header and the related logging, which is directly related to the ground truth methods 'doPost', 'getAuthHeader', and 'doKerberosAuth'. The report does not provide any fix suggestion, hence it is marked as 'Missing' for fix suggestion. The problem location is precise as the report mentions the methods 'doKerberosAuth' and 'doPost' in the stack trace, which are part of the ground truth methods. There is no wrong information in the bug report as all the details are relevant to the context of the bug."
        }
    },
    {
        "filename": "HIVE-7557.json",
        "code_diff": {
            "ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.validateReduceWorkOperator": {
                "code_before_change": [],
                "code_after_change": "  boolean validateReduceWorkOperator(Operator<? extends OperatorDesc> op) {\n    boolean ret = false;\n    switch (op.getType()) {\n      case EXTRACT:\n        ret = validateExtractOperator((ExtractOperator) op);\n        break;\n      case FILTER:\n        ret = validateFilterOperator((FilterOperator) op);\n        break;\n      case SELECT:\n        ret = validateSelectOperator((SelectOperator) op);\n        break;\n      case REDUCESINK:\n        ret = validateReduceSinkOperator((ReduceSinkOperator) op);\n        break;\n      case FILESINK:\n        ret = validateFileSinkOperator((FileSinkOperator) op);\n        break;\n      case LIMIT:\n        ret = true;\n        break;\n      default:\n        ret = false;\n        break;\n    }\n    return ret;\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.validateExtractOperator": {
                "code_before_change": [],
                "code_after_change": "  private boolean validateExtractOperator(ExtractOperator op) {\n    ExprNodeDesc expr = op.getConf().getCol();\n    boolean ret = validateExprNodeDesc(expr);\n    if (!ret) {\n      return false;\n    }\n    return true;\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the issue as a ClassCastException occurring during vectorized processing, which is related to the stack trace context but does not precisely identify the root cause in the ground truth methods. The stack trace mentions methods like ReduceRecordProcessor.processVectors and VectorizedRowBatch.toString, which are in the same stack trace context as the ground truth methods. However, it does not mention the specific ground truth methods, hence the 'Partial' classification for both root cause and problem location identification. There is no fix suggestion provided in the bug report, leading to a 'Missing' classification for fix suggestion. All information in the bug report is relevant to the context of the bug, so there is no wrong information."
        }
    },
    {
        "filename": "HIVE-1712.json",
        "code_diff": {
            "metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreUtils.getSchema": {
                "code_before_change": "  public static Properties getSchema(\n      org.apache.hadoop.hive.metastore.api.Table table) {\n    return MetaStoreUtils.getSchema(table.getSd(), table.getSd(), table\n        .getParameters(), table.getTableName(), table.getPartitionKeys());\n  }",
                "code_after_change": "  public static Properties getSchema(\n      org.apache.hadoop.hive.metastore.api.Table table) {\n    return MetaStoreUtils.getSchema(table.getSd(), table.getSd(), table\n        .getParameters(), table.getTableName(), table.getPartitionKeys());\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report mentions the method 'org.apache.hadoop.hive.ql.metadata.Hive.getTable' in the stack trace, which is in the same stack trace context as the ground truth method 'metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreUtils.getSchema'. However, it does not precisely identify the root cause at the ground truth method. There is no fix suggestion provided in the bug report. The problem location is partially identified as it is in the shared stack trace context. There is no wrong information in the bug report as all the information is relevant to the context of the bug."
        }
    },
    {
        "filename": "HIVE-12608.json",
        "code_diff": {
            "ql.src.java.org.apache.hadoop.hive.ql.io.parquet.convert.HiveStructConverter.getStructFieldTypeInfo": {
                "code_before_change": "  private TypeInfo getStructFieldTypeInfo(String field, int fieldIndex) {\n    String fieldLowerCase = field.toLowerCase();\n    if (Boolean.valueOf(getMetadata().get(DataWritableReadSupport.PARQUET_COLUMN_INDEX_ACCESS))\n        && fieldIndex < hiveFieldNames.size()) {\n      return hiveFieldTypeInfos.get(fieldIndex);\n    }\n    for (int i = 0; i < hiveFieldNames.size(); i++) {\n      if (fieldLowerCase.equalsIgnoreCase(hiveFieldNames.get(i))) {\n        return hiveFieldTypeInfos.get(i);\n      }\n    }\n    throw new RuntimeException(\"cannot find field \" + field\n        + \" in \" + hiveFieldNames);\n  }",
                "code_after_change": "  private TypeInfo getStructFieldTypeInfo(String field, int fieldIndex) {\n    String fieldLowerCase = field.toLowerCase();\n    if (Boolean.valueOf(getMetadata().get(DataWritableReadSupport.PARQUET_COLUMN_INDEX_ACCESS))\n        && fieldIndex < hiveFieldNames.size()) {\n      return hiveFieldTypeInfos.get(fieldIndex);\n    }\n    for (int i = 0; i < hiveFieldNames.size(); i++) {\n      if (fieldLowerCase.equalsIgnoreCase(hiveFieldNames.get(i))) {\n        return hiveFieldTypeInfos.get(i);\n      }\n    }\n    //This means hive type doesn't refer this field that comes from file schema.\n    //i.e. the field is not required for hive table. It can occur due to schema\n    //evolution where some field is deleted.\n    return null;\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report mentions the exception and the stack trace, which includes the method 'org.apache.hadoop.hive.ql.io.parquet.convert.HiveStructConverter.getStructFieldTypeInfo', indicating a shared stack trace context with the ground truth method. However, it does not precisely identify the root cause or suggest a fix. The problem location is also identified in the shared stack trace context but not precisely. There is no incorrect information in the bug report."
        }
    },
    {
        "filename": "HIVE-17774.json",
        "code_diff": {
            "ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.run": {
                "code_before_change": "  void run(HiveConf conf, String jobName, Table t, StorageDescriptor sd,\n           ValidTxnList txns, CompactionInfo ci, Worker.StatsUpdater su, TxnStore txnHandler) throws IOException {\n\n    if(conf.getBoolVar(HiveConf.ConfVars.HIVE_IN_TEST) && conf.getBoolVar(HiveConf.ConfVars.HIVETESTMODEFAILCOMPACTION)) {\n      throw new RuntimeException(HiveConf.ConfVars.HIVETESTMODEFAILCOMPACTION.name() + \"=true\");\n    }\n    JobConf job = createBaseJobConf(conf, jobName, t, sd, txns, ci);\n\n    // Figure out and encode what files we need to read.  We do this here (rather than in\n    // getSplits below) because as part of this we discover our minimum and maximum transactions,\n    // and discovering that in getSplits is too late as we then have no way to pass it to our\n    // mapper.\n\n    AcidUtils.Directory dir = AcidUtils.getAcidState(new Path(sd.getLocation()), conf, txns, false, true);\n    List<AcidUtils.ParsedDelta> parsedDeltas = dir.getCurrentDirectories();\n    int maxDeltastoHandle = conf.getIntVar(HiveConf.ConfVars.COMPACTOR_MAX_NUM_DELTA);\n    if(parsedDeltas.size() > maxDeltastoHandle) {\n      /**\n       * if here, that means we have very high number of delta files.  This may be sign of a temporary\n       * glitch or a real issue.  For example, if transaction batch size or transaction size is set too\n       * low for the event flow rate in Streaming API, it may generate lots of delta files very\n       * quickly.  Another possibility is that Compaction is repeatedly failing and not actually compacting.\n       * Thus, force N minor compactions first to reduce number of deltas and then follow up with\n       * the compaction actually requested in {@link ci} which now needs to compact a lot fewer deltas\n       */\n      LOG.warn(parsedDeltas.size() + \" delta files found for \" + ci.getFullPartitionName()\n        + \" located at \" + sd.getLocation() + \"! This is likely a sign of misconfiguration, \" +\n        \"especially if this message repeats.  Check that compaction is running properly.  Check for any \" +\n        \"runaway/mis-configured process writing to ACID tables, especially using Streaming Ingest API.\");\n      int numMinorCompactions = parsedDeltas.size() / maxDeltastoHandle;\n      for(int jobSubId = 0; jobSubId < numMinorCompactions; jobSubId++) {\n        JobConf jobMinorCompact = createBaseJobConf(conf, jobName + \"_\" + jobSubId, t, sd, txns, ci);\n        launchCompactionJob(jobMinorCompact,\n          null, CompactionType.MINOR, null,\n          parsedDeltas.subList(jobSubId * maxDeltastoHandle, (jobSubId + 1) * maxDeltastoHandle),\n          maxDeltastoHandle, -1, conf, txnHandler, ci.id, jobName);\n      }\n      //now recompute state since we've done minor compactions and have different 'best' set of deltas\n      dir = AcidUtils.getAcidState(new Path(sd.getLocation()), conf, txns);\n    }\n\n    StringableList dirsToSearch = new StringableList();\n    Path baseDir = null;\n    if (ci.isMajorCompaction()) {\n      // There may not be a base dir if the partition was empty before inserts or if this\n      // partition is just now being converted to ACID.\n      baseDir = dir.getBaseDirectory();\n      if (baseDir == null) {\n        List<HdfsFileStatusWithId> originalFiles = dir.getOriginalFiles();\n        if (!(originalFiles == null) && !(originalFiles.size() == 0)) {\n          // There are original format files\n          for (HdfsFileStatusWithId stat : originalFiles) {\n            Path path = stat.getFileStatus().getPath();\n            //note that originalFiles are all original files recursively not dirs\n            dirsToSearch.add(path);\n            LOG.debug(\"Adding original file \" + path + \" to dirs to search\");\n          }\n          // Set base to the location so that the input format reads the original files.\n          baseDir = new Path(sd.getLocation());\n        }\n      } else {\n        // add our base to the list of directories to search for files in.\n        LOG.debug(\"Adding base directory \" + baseDir + \" to dirs to search\");\n        dirsToSearch.add(baseDir);\n      }\n    }\n    if (parsedDeltas.size() == 0 && dir.getOriginalFiles().size() == 0) {\n      // Skip compaction if there's no delta files AND there's no original files\n      String minOpenInfo = \".\";\n      if(txns.getMinOpenTxn() != null) {\n        minOpenInfo = \" with min Open \" + JavaUtils.txnIdToString(txns.getMinOpenTxn()) +\n          \".  Compaction cannot compact above this txnid\";\n      }\n      LOG.error(\"No delta files or original files found to compact in \" + sd.getLocation() +\n        \" for compactionId=\" + ci.id + minOpenInfo);\n      return;\n    }\n\n    launchCompactionJob(job, baseDir, ci.type, dirsToSearch, dir.getCurrentDirectories(),\n      dir.getCurrentDirectories().size(), dir.getObsolete().size(), conf, txnHandler, ci.id, jobName);\n\n    su.gatherStats();\n  }",
                "code_after_change": "  void run(HiveConf conf, String jobName, Table t, StorageDescriptor sd,\n           ValidTxnList txns, CompactionInfo ci, Worker.StatsUpdater su, TxnStore txnHandler) throws IOException {\n\n    if(conf.getBoolVar(HiveConf.ConfVars.HIVE_IN_TEST) && conf.getBoolVar(HiveConf.ConfVars.HIVETESTMODEFAILCOMPACTION)) {\n      throw new RuntimeException(HiveConf.ConfVars.HIVETESTMODEFAILCOMPACTION.name() + \"=true\");\n    }\n    JobConf job = createBaseJobConf(conf, jobName, t, sd, txns, ci);\n\n    // Figure out and encode what files we need to read.  We do this here (rather than in\n    // getSplits below) because as part of this we discover our minimum and maximum transactions,\n    // and discovering that in getSplits is too late as we then have no way to pass it to our\n    // mapper.\n\n    AcidUtils.Directory dir = AcidUtils.getAcidState(new Path(sd.getLocation()), conf, txns, false, true);\n    List<AcidUtils.ParsedDelta> parsedDeltas = dir.getCurrentDirectories();\n    int maxDeltastoHandle = conf.getIntVar(HiveConf.ConfVars.COMPACTOR_MAX_NUM_DELTA);\n    if(parsedDeltas.size() > maxDeltastoHandle) {\n      /**\n       * if here, that means we have very high number of delta files.  This may be sign of a temporary\n       * glitch or a real issue.  For example, if transaction batch size or transaction size is set too\n       * low for the event flow rate in Streaming API, it may generate lots of delta files very\n       * quickly.  Another possibility is that Compaction is repeatedly failing and not actually compacting.\n       * Thus, force N minor compactions first to reduce number of deltas and then follow up with\n       * the compaction actually requested in {@link ci} which now needs to compact a lot fewer deltas\n       */\n      LOG.warn(parsedDeltas.size() + \" delta files found for \" + ci.getFullPartitionName()\n        + \" located at \" + sd.getLocation() + \"! This is likely a sign of misconfiguration, \" +\n        \"especially if this message repeats.  Check that compaction is running properly.  Check for any \" +\n        \"runaway/mis-configured process writing to ACID tables, especially using Streaming Ingest API.\");\n      int numMinorCompactions = parsedDeltas.size() / maxDeltastoHandle;\n      for(int jobSubId = 0; jobSubId < numMinorCompactions; jobSubId++) {\n        JobConf jobMinorCompact = createBaseJobConf(conf, jobName + \"_\" + jobSubId, t, sd, txns, ci);\n        launchCompactionJob(jobMinorCompact,\n          null, CompactionType.MINOR, null,\n          parsedDeltas.subList(jobSubId * maxDeltastoHandle, (jobSubId + 1) * maxDeltastoHandle),\n          maxDeltastoHandle, -1, conf, txnHandler, ci.id, jobName);\n      }\n      //now recompute state since we've done minor compactions and have different 'best' set of deltas\n      dir = AcidUtils.getAcidState(new Path(sd.getLocation()), conf, txns);\n    }\n\n    StringableList dirsToSearch = new StringableList();\n    Path baseDir = null;\n    if (ci.isMajorCompaction()) {\n      // There may not be a base dir if the partition was empty before inserts or if this\n      // partition is just now being converted to ACID.\n      baseDir = dir.getBaseDirectory();\n      if (baseDir == null) {\n        List<HdfsFileStatusWithId> originalFiles = dir.getOriginalFiles();\n        if (!(originalFiles == null) && !(originalFiles.size() == 0)) {\n          // There are original format files\n          for (HdfsFileStatusWithId stat : originalFiles) {\n            Path path = stat.getFileStatus().getPath();\n            //note that originalFiles are all original files recursively not dirs\n            dirsToSearch.add(path);\n            LOG.debug(\"Adding original file \" + path + \" to dirs to search\");\n          }\n          // Set base to the location so that the input format reads the original files.\n          baseDir = new Path(sd.getLocation());\n        }\n      } else {\n        // add our base to the list of directories to search for files in.\n        LOG.debug(\"Adding base directory \" + baseDir + \" to dirs to search\");\n        dirsToSearch.add(baseDir);\n      }\n    }\n\n    if (parsedDeltas.isEmpty() &&\n        (dir.getOriginalFiles() == null || dir.getOriginalFiles().isEmpty())) {\n      // Skip compaction if there's no delta files AND there's no original files\n      LOG.error(\"No delta files or original files found to compact in \" + sd.getLocation() + \" for compactionId=\" + ci.id);\n      return;\n    }\n\n    launchCompactionJob(job, baseDir, ci.type, dirsToSearch, dir.getCurrentDirectories(),\n      dir.getCurrentDirectories().size(), dir.getObsolete().size(), conf, txnHandler, ci.id, jobName);\n\n    su.gatherStats();\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report mentions the failure of a job due to zero splits and a FileNotFoundException, which is related to the stack trace context of the ground truth method 'CompactorMR.run'. However, it does not precisely identify the root cause in the 'CompactorMR.run' method. The report lacks any fix suggestion, as it only states that the MR job should not have been attempted. The problem location is partially identified as it is related to the stack trace context but does not mention the exact ground truth method. There is no wrong information in the bug report as it accurately describes the failure scenario."
        }
    },
    {
        "filename": "HIVE-14564.json",
        "code_diff": {
            "ql.src.java.org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcFactory.process": {
                "code_before_change": "    public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx ctx,\n        Object... nodeOutputs) throws SemanticException {\n      FilterOperator op = (FilterOperator) nd;\n      ColumnPrunerProcCtx cppCtx = (ColumnPrunerProcCtx) ctx;\n      ExprNodeDesc condn = op.getConf().getPredicate();\n      // get list of columns used in the filter\n      List<String> cl = condn.getCols();\n      // merge it with the downstream col list\n      List<String> filterOpPrunedColLists = Utilities.mergeUniqElems(cppCtx.genColLists(op), cl);\n      List<String> filterOpPrunedColListsOrderPreserved = preserveColumnOrder(op,\n          filterOpPrunedColLists);\n      cppCtx.getPrunedColLists().put(op,\n          filterOpPrunedColListsOrderPreserved);\n\n      pruneOperator(cppCtx, op, cppCtx.getPrunedColLists().get(op));\n      cppCtx.handleFilterUnionChildren(op);\n      return null;\n    }",
                "code_after_change": "    public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx ctx,\n        Object... nodeOutputs) throws SemanticException {\n      FilterOperator op = (FilterOperator) nd;\n      ColumnPrunerProcCtx cppCtx = (ColumnPrunerProcCtx) ctx;\n      ExprNodeDesc condn = op.getConf().getPredicate();\n      List<FieldNode> filterOpPrunedColLists = mergeFieldNodesWithDesc(cppCtx.genColLists(op), condn);\n      List<FieldNode> filterOpPrunedColListsOrderPreserved = preserveColumnOrder(op,\n          filterOpPrunedColLists);\n      cppCtx.getPrunedColLists().put(op,\n          filterOpPrunedColListsOrderPreserved);\n\n      pruneOperator(cppCtx, op, cppCtx.getPrunedColLists().get(op));\n      cppCtx.handleFilterUnionChildren(op);\n      return null;\n    }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Same Class or Module"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Same Class or Module"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the issue with column pruning in the SelectOperator, which is related to the ground truth method 'ColumnPrunerProcFactory.process'. However, it does not precisely identify the root cause at the method level, hence it is classified as 'Partial' with the sub-category 'Same Class or Module'. The report does not provide any specific fix suggestion, so it is marked as 'Missing' for fix suggestion. The problem location is also identified at a class level rather than the specific method, so it is 'Partial' with the same sub-category. There is no incorrect information in the bug report, so 'wrong_information' is 'No'."
        }
    },
    {
        "filename": "HIVE-3651.json",
        "code_diff": {
            "ql.src.java.org.apache.hadoop.hive.ql.plan.BucketMapJoinContext.createFileName": {
                "code_before_change": "  public String createFileName(String inputPath, String fileName) {\n    if (bigTablePartSpecToFileMapping != null) {\n      // partSpecToFileMapping is null if big table is partitioned\n      return prependPartSpec(inputPath, fileName);\n    }\n    return fileName;\n  }",
                "code_after_change": "  public String createFileName(String inputPath, String fileName) {\n    if (bigTablePartSpecToFileMapping != null) {\n      // partSpecToFileMapping is null if big table is partitioned\n      return prependPartSpec(inputPath, fileName);\n    }\n    return fileName;\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.plan.BucketMapJoinContext.revert": {
                "code_before_change": "  private Map<String, String> revert(Map<String, List<String>> mapping) {\n    Map<String, String> converted = new HashMap<String, String>();\n    for (Map.Entry<String, List<String>> entry : mapping.entrySet()) {\n      String partSpec = entry.getKey();\n      for (String file : entry.getValue()) {\n        converted.put(file, partSpec);\n      }\n    }\n    return converted;\n  }",
                "code_after_change": "  private Map<String, String> revert(Map<String, List<String>> mapping) {\n    Map<String, String> converted = new HashMap<String, String>();\n    for (Map.Entry<String, List<String>> entry : mapping.entrySet()) {\n      String partSpec = entry.getKey();\n      for (String file : entry.getValue()) {\n        converted.put(URI.create(file).getPath(), partSpec);\n      }\n    }\n    return converted;\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report mentions an error in the MapReduce job and provides a stack trace that includes 'ExecMapper.map', which is in the shared stack trace context with the ground truth methods. However, it does not precisely identify the root cause related to the ground truth methods 'createFileName' or 'revert'. There is no fix suggestion provided in the bug report. The problem location is partially identified as it mentions methods in the stack trace context but not the exact ground truth methods. There is no wrong information as the details provided are relevant to the context of the bug."
        }
    },
    {
        "filename": "HIVE-5199.json",
        "code_diff": {
            "serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters.getConvertedOI": {
                "code_before_change": "  public static ObjectInspector getConvertedOI(\n      ObjectInspector inputOI,\n      ObjectInspector outputOI) {\n    // If the inputOI is the same as the outputOI, just return it\n    if (inputOI.equals(outputOI)) {\n      return outputOI;\n    }\n    switch (outputOI.getCategory()) {\n    case PRIMITIVE:\n      return outputOI;\n    case STRUCT:\n      StructObjectInspector structOutputOI = (StructObjectInspector) outputOI;\n      if (structOutputOI.isSettable()) {\n        return outputOI;\n      }\n      else {\n        // create a standard settable struct object inspector\n        List<? extends StructField> listFields = structOutputOI.getAllStructFieldRefs();\n        List<String> structFieldNames = new ArrayList<String>(listFields.size());\n        List<ObjectInspector> structFieldObjectInspectors = new ArrayList<ObjectInspector>(\n            listFields.size());\n\n        for (StructField listField : listFields) {\n          structFieldNames.add(listField.getFieldName());\n          structFieldObjectInspectors.add(\n              getSettableConvertedOI(listField.getFieldObjectInspector()));\n        }\n\n        StandardStructObjectInspector structStandardOutputOI = ObjectInspectorFactory\n            .getStandardStructObjectInspector(\n                structFieldNames,\n                structFieldObjectInspectors);\n        return structStandardOutputOI;\n      }\n    case LIST:\n      return outputOI;\n    case MAP:\n      return outputOI;\n    default:\n      throw new RuntimeException(\"Hive internal error: conversion of \"\n          + inputOI.getTypeName() + \" to \" + outputOI.getTypeName()\n          + \" not supported yet.\");\n    }\n  }",
                "code_after_change": "  public static ObjectInspector getConvertedOI(\n      ObjectInspector inputOI,\n      ObjectInspector outputOI,\n      boolean equalsCheck) {\n    // If the inputOI is the same as the outputOI, just return it\n    if (equalsCheck && inputOI.equals(outputOI)) {\n      return outputOI;\n    }\n    // Return the settable equivalent object inspector for primitive categories\n    // For eg: for table T containing partitions p1 and p2 (possibly different\n    // from the table T), return the settable inspector for T. The inspector for\n    // T is settable recursively i.e all the nested fields are also settable.\n    // TODO: Add support for UNION once SettableUnionObjectInspector is implemented.\n    switch (outputOI.getCategory()) {\n    case PRIMITIVE:\n      PrimitiveObjectInspector primInputOI = (PrimitiveObjectInspector) inputOI;\n      return PrimitiveObjectInspectorFactory.\n          getPrimitiveWritableObjectInspector(primInputOI.getPrimitiveCategory());\n    case STRUCT:\n      StructObjectInspector structOutputOI = (StructObjectInspector) outputOI;\n      if (structOutputOI.isSettable()) {\n        return outputOI;\n      }\n      else {\n        // create a standard settable struct object inspector\n        List<? extends StructField> listFields = structOutputOI.getAllStructFieldRefs();\n        List<String> structFieldNames = new ArrayList<String>(listFields.size());\n        List<ObjectInspector> structFieldObjectInspectors = new ArrayList<ObjectInspector>(\n            listFields.size());\n\n        for (StructField listField : listFields) {\n          structFieldNames.add(listField.getFieldName());\n          structFieldObjectInspectors.add(getConvertedOI(listField.getFieldObjectInspector(),\n              listField.getFieldObjectInspector(), false));\n        }\n\n        return ObjectInspectorFactory.getStandardStructObjectInspector(\n                structFieldNames,\n                structFieldObjectInspectors);\n      }\n    case LIST:\n      ListObjectInspector listOutputOI = (ListObjectInspector) outputOI;\n      return ObjectInspectorFactory.getStandardListObjectInspector(\n          listOutputOI.getListElementObjectInspector());\n    case MAP:\n      MapObjectInspector mapOutputOI = (MapObjectInspector) outputOI;\n      return ObjectInspectorFactory.getStandardMapObjectInspector(\n          mapOutputOI.getMapKeyObjectInspector(), mapOutputOI.getMapValueObjectInspector());\n    default:\n      throw new RuntimeException(\"Hive internal error: conversion of \"\n          + inputOI.getTypeName() + \" to \" + outputOI.getTypeName()\n          + \" not supported yet.\");\n    }\n  }"
            },
            "serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters.getConverter": {
                "code_before_change": "  private static Converter getConverter(PrimitiveObjectInspector inputOI,\n      PrimitiveObjectInspector outputOI) {\n    switch (outputOI.getPrimitiveCategory()) {\n    case BOOLEAN:\n      return new PrimitiveObjectInspectorConverter.BooleanConverter(\n          inputOI,\n          (SettableBooleanObjectInspector) outputOI);\n    case BYTE:\n      return new PrimitiveObjectInspectorConverter.ByteConverter(\n          inputOI,\n          (SettableByteObjectInspector) outputOI);\n    case SHORT:\n      return new PrimitiveObjectInspectorConverter.ShortConverter(\n          inputOI,\n          (SettableShortObjectInspector) outputOI);\n    case INT:\n      return new PrimitiveObjectInspectorConverter.IntConverter(\n          inputOI,\n          (SettableIntObjectInspector) outputOI);\n    case LONG:\n      return new PrimitiveObjectInspectorConverter.LongConverter(\n          inputOI,\n          (SettableLongObjectInspector) outputOI);\n    case FLOAT:\n      return new PrimitiveObjectInspectorConverter.FloatConverter(\n          inputOI,\n          (SettableFloatObjectInspector) outputOI);\n    case DOUBLE:\n      return new PrimitiveObjectInspectorConverter.DoubleConverter(\n          inputOI,\n          (SettableDoubleObjectInspector) outputOI);\n    case STRING:\n      if (outputOI instanceof WritableStringObjectInspector) {\n        return new PrimitiveObjectInspectorConverter.TextConverter(\n            inputOI);\n      } else if (outputOI instanceof JavaStringObjectInspector) {\n        return new PrimitiveObjectInspectorConverter.StringConverter(\n            inputOI);\n      }\n    case DATE:\n      return new PrimitiveObjectInspectorConverter.DateConverter(\n          inputOI,\n          (SettableDateObjectInspector) outputOI);\n    case TIMESTAMP:\n      return new PrimitiveObjectInspectorConverter.TimestampConverter(\n          inputOI,\n          (SettableTimestampObjectInspector) outputOI);\n    case BINARY:\n      return new PrimitiveObjectInspectorConverter.BinaryConverter(\n          inputOI,\n          (SettableBinaryObjectInspector)outputOI);\n    case DECIMAL:\n      return new PrimitiveObjectInspectorConverter.HiveDecimalConverter(\n          (PrimitiveObjectInspector) inputOI,\n          (SettableHiveDecimalObjectInspector) outputOI);\n    default:\n      throw new RuntimeException(\"Hive internal error: conversion of \"\n          + inputOI.getTypeName() + \" to \" + outputOI.getTypeName()\n          + \" not supported yet.\");\n    }\n  }",
                "code_after_change": "  private static Converter getConverter(PrimitiveObjectInspector inputOI,\n      PrimitiveObjectInspector outputOI) {\n    switch (outputOI.getPrimitiveCategory()) {\n    case BOOLEAN:\n      return new PrimitiveObjectInspectorConverter.BooleanConverter(\n          inputOI,\n          (SettableBooleanObjectInspector) outputOI);\n    case BYTE:\n      return new PrimitiveObjectInspectorConverter.ByteConverter(\n          inputOI,\n          (SettableByteObjectInspector) outputOI);\n    case SHORT:\n      return new PrimitiveObjectInspectorConverter.ShortConverter(\n          inputOI,\n          (SettableShortObjectInspector) outputOI);\n    case INT:\n      return new PrimitiveObjectInspectorConverter.IntConverter(\n          inputOI,\n          (SettableIntObjectInspector) outputOI);\n    case LONG:\n      return new PrimitiveObjectInspectorConverter.LongConverter(\n          inputOI,\n          (SettableLongObjectInspector) outputOI);\n    case FLOAT:\n      return new PrimitiveObjectInspectorConverter.FloatConverter(\n          inputOI,\n          (SettableFloatObjectInspector) outputOI);\n    case DOUBLE:\n      return new PrimitiveObjectInspectorConverter.DoubleConverter(\n          inputOI,\n          (SettableDoubleObjectInspector) outputOI);\n    case STRING:\n      if (outputOI instanceof WritableStringObjectInspector) {\n        return new PrimitiveObjectInspectorConverter.TextConverter(\n            inputOI);\n      } else if (outputOI instanceof JavaStringObjectInspector) {\n        return new PrimitiveObjectInspectorConverter.StringConverter(\n            inputOI);\n      }\n    case DATE:\n      return new PrimitiveObjectInspectorConverter.DateConverter(\n          inputOI,\n          (SettableDateObjectInspector) outputOI);\n    case TIMESTAMP:\n      return new PrimitiveObjectInspectorConverter.TimestampConverter(\n          inputOI,\n          (SettableTimestampObjectInspector) outputOI);\n    case BINARY:\n      return new PrimitiveObjectInspectorConverter.BinaryConverter(\n          inputOI,\n          (SettableBinaryObjectInspector)outputOI);\n    case DECIMAL:\n      return new PrimitiveObjectInspectorConverter.HiveDecimalConverter(\n          (PrimitiveObjectInspector) inputOI,\n          (SettableHiveDecimalObjectInspector) outputOI);\n    default:\n      throw new RuntimeException(\"Hive internal error: conversion of \"\n          + inputOI.getTypeName() + \" to \" + outputOI.getTypeName()\n          + \" not supported yet.\");\n    }\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.exec.FetchOperator.getRecordReader": {
                "code_before_change": "  private RecordReader<WritableComparable, Writable> getRecordReader() throws Exception {\n    if (currPath == null) {\n      getNextPath();\n      if (currPath == null) {\n        return null;\n      }\n\n      // not using FileInputFormat.setInputPaths() here because it forces a\n      // connection\n      // to the default file system - which may or may not be online during pure\n      // metadata\n      // operations\n      job.set(\"mapred.input.dir\", org.apache.hadoop.util.StringUtils.escapeString(currPath\n          .toString()));\n\n      PartitionDesc partDesc;\n      if (currTbl == null) {\n        partDesc = currPart;\n      } else {\n        partDesc = new PartitionDesc(currTbl, null);\n      }\n\n      Class<? extends InputFormat> formatter = partDesc.getInputFileFormatClass();\n      inputFormat = getInputFormatFromCache(formatter, job);\n      Utilities.copyTableJobPropertiesToConf(partDesc.getTableDesc(), job);\n      InputSplit[] splits = inputFormat.getSplits(job, 1);\n      FetchInputFormatSplit[] inputSplits = new FetchInputFormatSplit[splits.length];\n      for (int i = 0; i < splits.length; i++) {\n        inputSplits[i] = new FetchInputFormatSplit(splits[i], formatter.getName());\n      }\n      if (work.getSplitSample() != null) {\n        inputSplits = splitSampling(work.getSplitSample(), inputSplits);\n      }\n      this.inputSplits = inputSplits;\n\n      splitNum = 0;\n      serde = partDesc.getDeserializerClass().newInstance();\n      serde.initialize(job, partDesc.getOverlayedProperties());\n\n      if (currTbl != null) {\n        tblSerde = serde;\n      }\n      else {\n        tblSerde = currPart.getTableDesc().getDeserializerClass().newInstance();\n        tblSerde.initialize(job, currPart.getTableDesc().getProperties());\n      }\n\n      ObjectInspector outputOI = ObjectInspectorConverters.getConvertedOI(\n          serde.getObjectInspector(),\n          partitionedTableOI == null ? tblSerde.getObjectInspector() : partitionedTableOI);\n\n      partTblObjectInspectorConverter = ObjectInspectorConverters.getConverter(\n          serde.getObjectInspector(), outputOI);\n\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Creating fetchTask with deserializer typeinfo: \"\n            + serde.getObjectInspector().getTypeName());\n        LOG.debug(\"deserializer properties: \" + partDesc.getOverlayedProperties());\n      }\n\n      if (currPart != null) {\n        getRowInspectorFromPartition(currPart, outputOI);\n      }\n    }\n\n    if (splitNum >= inputSplits.length) {\n      if (currRecReader != null) {\n        currRecReader.close();\n        currRecReader = null;\n      }\n      currPath = null;\n      return getRecordReader();\n    }\n\n    final FetchInputFormatSplit target = inputSplits[splitNum];\n\n    @SuppressWarnings(\"unchecked\")\n    final RecordReader<WritableComparable, Writable> reader =\n        inputFormat.getRecordReader(target.getInputSplit(), job, Reporter.NULL);\n    if (hasVC || work.getSplitSample() != null) {\n      currRecReader = new HiveRecordReader<WritableComparable, Writable>(reader, job) {\n        @Override\n        public boolean doNext(WritableComparable key, Writable value) throws IOException {\n          // if current pos is larger than shrinkedLength which is calculated for\n          // each split by table sampling, stop fetching any more (early exit)\n          if (target.shrinkedLength > 0 &&\n              context.getIoCxt().getCurrentBlockStart() > target.shrinkedLength) {\n            return false;\n          }\n          return super.doNext(key, value);\n        }\n      };\n      ((HiveContextAwareRecordReader)currRecReader).\n          initIOContext(target, job, inputFormat.getClass(), reader);\n    } else {\n      currRecReader = reader;\n    }\n    splitNum++;\n    key = currRecReader.createKey();\n    value = currRecReader.createValue();\n    return currRecReader;\n  }",
                "code_after_change": "  private RecordReader<WritableComparable, Writable> getRecordReader() throws Exception {\n    if (currPath == null) {\n      getNextPath();\n      if (currPath == null) {\n        return null;\n      }\n\n      // not using FileInputFormat.setInputPaths() here because it forces a\n      // connection\n      // to the default file system - which may or may not be online during pure\n      // metadata\n      // operations\n      job.set(\"mapred.input.dir\", org.apache.hadoop.util.StringUtils.escapeString(currPath\n          .toString()));\n\n      PartitionDesc partDesc;\n      if (currTbl == null) {\n        partDesc = currPart;\n      } else {\n        partDesc = new PartitionDesc(currTbl, null);\n      }\n\n      Class<? extends InputFormat> formatter = partDesc.getInputFileFormatClass();\n      inputFormat = getInputFormatFromCache(formatter, job);\n      Utilities.copyTableJobPropertiesToConf(partDesc.getTableDesc(), job);\n      InputSplit[] splits = inputFormat.getSplits(job, 1);\n      FetchInputFormatSplit[] inputSplits = new FetchInputFormatSplit[splits.length];\n      for (int i = 0; i < splits.length; i++) {\n        inputSplits[i] = new FetchInputFormatSplit(splits[i], formatter.getName());\n      }\n      if (work.getSplitSample() != null) {\n        inputSplits = splitSampling(work.getSplitSample(), inputSplits);\n      }\n      this.inputSplits = inputSplits;\n\n      splitNum = 0;\n      serde = partDesc.getDeserializerClass().newInstance();\n      serde.initialize(job, partDesc.getOverlayedProperties());\n\n      if (currTbl != null) {\n        tblSerde = serde;\n      }\n      else {\n        tblSerde = currPart.getTableDesc().getDeserializerClass().newInstance();\n        tblSerde.initialize(job, currPart.getTableDesc().getProperties());\n      }\n\n      ObjectInspector outputOI = ObjectInspectorConverters.getConvertedOI(\n          serde.getObjectInspector(),\n          partitionedTableOI == null ? tblSerde.getObjectInspector() : partitionedTableOI, true);\n\n      partTblObjectInspectorConverter = ObjectInspectorConverters.getConverter(\n          serde.getObjectInspector(), outputOI);\n\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Creating fetchTask with deserializer typeinfo: \"\n            + serde.getObjectInspector().getTypeName());\n        LOG.debug(\"deserializer properties: \" + partDesc.getOverlayedProperties());\n      }\n\n      if (currPart != null) {\n        getRowInspectorFromPartition(currPart, outputOI);\n      }\n    }\n\n    if (splitNum >= inputSplits.length) {\n      if (currRecReader != null) {\n        currRecReader.close();\n        currRecReader = null;\n      }\n      currPath = null;\n      return getRecordReader();\n    }\n\n    final FetchInputFormatSplit target = inputSplits[splitNum];\n\n    @SuppressWarnings(\"unchecked\")\n    final RecordReader<WritableComparable, Writable> reader =\n        inputFormat.getRecordReader(target.getInputSplit(), job, Reporter.NULL);\n    if (hasVC || work.getSplitSample() != null) {\n      currRecReader = new HiveRecordReader<WritableComparable, Writable>(reader, job) {\n        @Override\n        public boolean doNext(WritableComparable key, Writable value) throws IOException {\n          // if current pos is larger than shrinkedLength which is calculated for\n          // each split by table sampling, stop fetching any more (early exit)\n          if (target.shrinkedLength > 0 &&\n              context.getIoCxt().getCurrentBlockStart() > target.shrinkedLength) {\n            return false;\n          }\n          return super.doNext(key, value);\n        }\n      };\n      ((HiveContextAwareRecordReader)currRecReader).\n          initIOContext(target, job, inputFormat.getClass(), reader);\n    } else {\n      currRecReader = reader;\n    }\n    splitNum++;\n    key = currRecReader.createKey();\n    value = currRecReader.createValue();\n    return currRecReader;\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.exec.FetchOperator.getOutputObjectInspector": {
                "code_before_change": "  public ObjectInspector getOutputObjectInspector() throws HiveException {\n    try {\n      if (work.isNotPartitioned()) {\n        return getRowInspectorFromTable(work.getTblDesc());\n      }\n      List<PartitionDesc> listParts = work.getPartDesc();\n      // Chose the table descriptor if none of the partitions is present.\n      // For eg: consider the query:\n      // select /*+mapjoin(T1)*/ count(*) from T1 join T2 on T1.key=T2.key\n      // Both T1 and T2 and partitioned tables, but T1 does not have any partitions\n      // FetchOperator is invoked for T1, and listParts is empty. In that case,\n      // use T1's schema to get the ObjectInspector.\n      if (listParts == null || listParts.isEmpty()) {\n        return getRowInspectorFromPartitionedTable(work.getTblDesc());\n      }\n\n      // Choose any partition. It's OI needs to be converted to the table OI\n      // Whenever a new partition is being read, a new converter is being created\n      PartitionDesc partition = listParts.get(0);\n      Deserializer tblSerde = partition.getTableDesc().getDeserializerClass().newInstance();\n      tblSerde.initialize(job, partition.getTableDesc().getProperties());\n\n      partitionedTableOI = null;\n      ObjectInspector tableOI = tblSerde.getObjectInspector();\n\n      // Get the OI corresponding to all the partitions\n      for (PartitionDesc listPart : listParts) {\n        partition = listPart;\n        Deserializer partSerde = listPart.getDeserializerClass().newInstance();\n        partSerde.initialize(job, listPart.getOverlayedProperties());\n\n        partitionedTableOI = ObjectInspectorConverters.getConvertedOI(\n            partSerde.getObjectInspector(), tableOI);\n        if (!partitionedTableOI.equals(tableOI)) {\n          break;\n        }\n      }\n      return getRowInspectorFromPartition(partition, partitionedTableOI);\n    } catch (Exception e) {\n      throw new HiveException(\"Failed with exception \" + e.getMessage()\n          + org.apache.hadoop.util.StringUtils.stringifyException(e));\n    } finally {\n      currPart = null;\n    }\n  }",
                "code_after_change": "  public ObjectInspector getOutputObjectInspector() throws HiveException {\n    try {\n      if (work.isNotPartitioned()) {\n        return getRowInspectorFromTable(work.getTblDesc());\n      }\n      List<PartitionDesc> listParts = work.getPartDesc();\n      // Chose the table descriptor if none of the partitions is present.\n      // For eg: consider the query:\n      // select /*+mapjoin(T1)*/ count(*) from T1 join T2 on T1.key=T2.key\n      // Both T1 and T2 and partitioned tables, but T1 does not have any partitions\n      // FetchOperator is invoked for T1, and listParts is empty. In that case,\n      // use T1's schema to get the ObjectInspector.\n      if (listParts == null || listParts.isEmpty()) {\n        return getRowInspectorFromPartitionedTable(work.getTblDesc());\n      }\n\n      // Choose any partition. It's OI needs to be converted to the table OI\n      // Whenever a new partition is being read, a new converter is being created\n      PartitionDesc partition = listParts.get(0);\n      Deserializer tblSerde = partition.getTableDesc().getDeserializerClass().newInstance();\n      tblSerde.initialize(job, partition.getTableDesc().getProperties());\n\n      partitionedTableOI = null;\n      ObjectInspector tableOI = tblSerde.getObjectInspector();\n\n      // Get the OI corresponding to all the partitions\n      for (PartitionDesc listPart : listParts) {\n        partition = listPart;\n        Deserializer partSerde = listPart.getDeserializerClass().newInstance();\n        partSerde.initialize(job, listPart.getOverlayedProperties());\n\n        partitionedTableOI = ObjectInspectorConverters.getConvertedOI(\n            partSerde.getObjectInspector(), tableOI, true);\n        if (!partitionedTableOI.equals(tableOI)) {\n          break;\n        }\n      }\n      return getRowInspectorFromPartition(partition, partitionedTableOI);\n    } catch (Exception e) {\n      throw new HiveException(\"Failed with exception \" + e.getMessage()\n          + org.apache.hadoop.util.StringUtils.stringifyException(e));\n    } finally {\n      currPart = null;\n    }\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.exec.MapOperator.getConvertedOI": {
                "code_before_change": "  private Map<TableDesc, StructObjectInspector> getConvertedOI(Configuration hconf)\n      throws HiveException {\n    Map<TableDesc, StructObjectInspector> tableDescOI =\n        new HashMap<TableDesc, StructObjectInspector>();\n    Set<TableDesc> identityConverterTableDesc = new HashSet<TableDesc>();\n    try {\n      for (String onefile : conf.getPathToAliases().keySet()) {\n        PartitionDesc pd = conf.getPathToPartitionInfo().get(onefile);\n        TableDesc tableDesc = pd.getTableDesc();\n        Properties tblProps = tableDesc.getProperties();\n        // If the partition does not exist, use table properties\n        Properties partProps = isPartitioned(pd) ? pd.getOverlayedProperties() : tblProps;\n\n        Class sdclass = pd.getDeserializerClass();\n        if (sdclass == null) {\n          String className = checkSerdeClassName(pd.getSerdeClassName(),\n              pd.getProperties().getProperty(\"name\"));\n          sdclass = hconf.getClassByName(className);\n        }\n\n        Deserializer partDeserializer = (Deserializer) sdclass.newInstance();\n        partDeserializer.initialize(hconf, partProps);\n        StructObjectInspector partRawRowObjectInspector = (StructObjectInspector) partDeserializer\n            .getObjectInspector();\n\n        StructObjectInspector tblRawRowObjectInspector = tableDescOI.get(tableDesc);\n        if ((tblRawRowObjectInspector == null) ||\n            (identityConverterTableDesc.contains(tableDesc))) {\n          sdclass = tableDesc.getDeserializerClass();\n          if (sdclass == null) {\n            String className = checkSerdeClassName(tableDesc.getSerdeClassName(),\n                tableDesc.getProperties().getProperty(\"name\"));\n            sdclass = hconf.getClassByName(className);\n          }\n          Deserializer tblDeserializer = (Deserializer) sdclass.newInstance();\n          tblDeserializer.initialize(hconf, tblProps);\n          tblRawRowObjectInspector =\n              (StructObjectInspector) ObjectInspectorConverters.getConvertedOI(\n                  partRawRowObjectInspector,\n                  tblDeserializer.getObjectInspector());\n\n          if (identityConverterTableDesc.contains(tableDesc)) {\n            if (!partRawRowObjectInspector.equals(tblRawRowObjectInspector)) {\n              identityConverterTableDesc.remove(tableDesc);\n            }\n          }\n          else if (partRawRowObjectInspector.equals(tblRawRowObjectInspector)) {\n            identityConverterTableDesc.add(tableDesc);\n          }\n\n          tableDescOI.put(tableDesc, tblRawRowObjectInspector);\n        }\n      }\n    } catch (Exception e) {\n      throw new HiveException(e);\n    }\n    return tableDescOI;\n  }",
                "code_after_change": "  private Map<TableDesc, StructObjectInspector> getConvertedOI(Configuration hconf)\n      throws HiveException {\n    Map<TableDesc, StructObjectInspector> tableDescOI =\n        new HashMap<TableDesc, StructObjectInspector>();\n    Set<TableDesc> identityConverterTableDesc = new HashSet<TableDesc>();\n    try {\n      for (String onefile : conf.getPathToAliases().keySet()) {\n        PartitionDesc pd = conf.getPathToPartitionInfo().get(onefile);\n        TableDesc tableDesc = pd.getTableDesc();\n        Properties tblProps = tableDesc.getProperties();\n        // If the partition does not exist, use table properties\n        Properties partProps = isPartitioned(pd) ? pd.getOverlayedProperties() : tblProps;\n\n        Class sdclass = pd.getDeserializerClass();\n        if (sdclass == null) {\n          String className = checkSerdeClassName(pd.getSerdeClassName(),\n              pd.getProperties().getProperty(\"name\"));\n          sdclass = hconf.getClassByName(className);\n        }\n\n        Deserializer partDeserializer = (Deserializer) sdclass.newInstance();\n        partDeserializer.initialize(hconf, partProps);\n        StructObjectInspector partRawRowObjectInspector = (StructObjectInspector) partDeserializer\n            .getObjectInspector();\n\n        StructObjectInspector tblRawRowObjectInspector = tableDescOI.get(tableDesc);\n        if ((tblRawRowObjectInspector == null) ||\n            (identityConverterTableDesc.contains(tableDesc))) {\n          sdclass = tableDesc.getDeserializerClass();\n          if (sdclass == null) {\n            String className = checkSerdeClassName(tableDesc.getSerdeClassName(),\n                tableDesc.getProperties().getProperty(\"name\"));\n            sdclass = hconf.getClassByName(className);\n          }\n          Deserializer tblDeserializer = (Deserializer) sdclass.newInstance();\n          tblDeserializer.initialize(hconf, tblProps);\n          tblRawRowObjectInspector =\n              (StructObjectInspector) ObjectInspectorConverters.getConvertedOI(\n                  partRawRowObjectInspector,\n                  tblDeserializer.getObjectInspector(), true);\n\n          if (identityConverterTableDesc.contains(tableDesc)) {\n            if (!partRawRowObjectInspector.equals(tblRawRowObjectInspector)) {\n              identityConverterTableDesc.remove(tableDesc);\n            }\n          }\n          else if (partRawRowObjectInspector.equals(tblRawRowObjectInspector)) {\n            identityConverterTableDesc.add(tableDesc);\n          }\n\n          tableDescOI.put(tableDesc, tblRawRowObjectInspector);\n        }\n      }\n    } catch (Exception e) {\n      throw new HiveException(e);\n    }\n    return tableDescOI;\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report precisely identifies the root cause by mentioning the issue with typecasting in 'ObjectInspectorConverters.getConverter', which is a ground truth method. However, there is no explicit fix suggestion provided in the bug report. The problem location is also precise as it mentions 'ObjectInspectorConverters.getConverter', which is a ground truth method. There is no wrong information in the bug report as all the details are relevant to the context of the bug."
        }
    },
    {
        "filename": "HIVE-17368.json",
        "code_diff": {
            "standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.security.DBTokenStore.invokeOnTokenStore": {
                "code_before_change": [],
                "code_after_change": "  private Object invokeOnTokenStore(String methName, Object[] params, Class<?> ... paramTypes)\n      throws TokenStoreException{\n    Object tokenStore;\n    try {\n      switch (serverMode) {\n      case METASTORE:\n        tokenStore = handler.getClass().getMethod(\"getMS\").invoke(handler);\n        break;\n      case HIVESERVER2:\n        Object hiveObject = ((Class<?>) handler).getMethod(\"get\").invoke(handler, null);\n        tokenStore = ((Class<?>) handler).getMethod(\"getMSC\").invoke(hiveObject);\n        break;\n      default:\n        throw new IllegalArgumentException(\"Unexpected value of Server mode \" + serverMode);\n      }\n      return tokenStore.getClass().getMethod(methName, paramTypes).invoke(tokenStore, params);\n    } catch (IllegalArgumentException e) {\n        throw new TokenStoreException(e);\n    } catch (SecurityException e) {\n        throw new TokenStoreException(e);\n    } catch (IllegalAccessException e) {\n        throw new TokenStoreException(e);\n    } catch (InvocationTargetException e) {\n        throw new TokenStoreException(e.getCause());\n    } catch (NoSuchMethodException e) {\n        throw new TokenStoreException(e);\n    }\n  }"
            },
            "itests.util.src.main.java.org.apache.hive.jdbc.miniHS2.MiniHS2.MiniHS2": {
                "code_before_change": [],
                "code_after_change": []
            },
            "ql.src.java.org.apache.hadoop.hive.ql.session.SessionState.close": {
                "code_before_change": "  public void close() throws IOException {\n    registry.clear();\n    if (txnMgr != null) txnMgr.closeTxnManager();\n    JavaUtils.closeClassLoadersTo(sessionConf.getClassLoader(), parentLoader);\n    File resourceDir =\n        new File(getConf().getVar(HiveConf.ConfVars.DOWNLOADED_RESOURCES_DIR));\n    LOG.debug(\"Removing resource dir \" + resourceDir);\n    try {\n      if (resourceDir.exists()) {\n        FileUtils.deleteDirectory(resourceDir);\n      }\n    } catch (IOException e) {\n      LOG.info(\"Error removing session resource dir \" + resourceDir, e);\n    } finally {\n      detachSession();\n    }\n\n    try {\n      if (tezSessionState != null) {\n        TezSessionPoolManager.closeIfNotDefault(tezSessionState, false);\n      }\n    } catch (Exception e) {\n      LOG.info(\"Error closing tez session\", e);\n    } finally {\n      setTezSession(null);\n    }\n\n    try {\n      closeSparkSession();\n      registry.closeCUDFLoaders();\n      dropSessionPaths(sessionConf);\n      unCacheDataNucleusClassLoaders();\n    } finally {\n      // removes the threadlocal variables, closes underlying HMS connection\n      Hive.closeCurrent();\n    }\n    progressMonitor = null;\n  }",
                "code_after_change": "  public void close() throws IOException {\n    for (Closeable cleanupItem : cleanupItems) {\n      try {\n        cleanupItem.close();\n      } catch (Exception err) {\n        LOG.error(\"Error processing SessionState cleanup item \" + cleanupItem.toString(), err);\n      }\n    }\n\n    registry.clear();\n    if (txnMgr != null) txnMgr.closeTxnManager();\n    JavaUtils.closeClassLoadersTo(sessionConf.getClassLoader(), parentLoader);\n    File resourceDir =\n        new File(getConf().getVar(HiveConf.ConfVars.DOWNLOADED_RESOURCES_DIR));\n    LOG.debug(\"Removing resource dir \" + resourceDir);\n    try {\n      if (resourceDir.exists()) {\n        FileUtils.deleteDirectory(resourceDir);\n      }\n    } catch (IOException e) {\n      LOG.info(\"Error removing session resource dir \" + resourceDir, e);\n    } finally {\n      detachSession();\n    }\n\n    try {\n      if (tezSessionState != null) {\n        TezSessionPoolManager.closeIfNotDefault(tezSessionState, false);\n      }\n    } catch (Exception e) {\n      LOG.info(\"Error closing tez session\", e);\n    } finally {\n      setTezSession(null);\n    }\n\n    try {\n      closeSparkSession();\n      registry.closeCUDFLoaders();\n      dropSessionPaths(sessionConf);\n      unCacheDataNucleusClassLoaders();\n    } finally {\n      // removes the threadlocal variables, closes underlying HMS connection\n      Hive.closeCurrent();\n    }\n    progressMonitor = null;\n  }"
            },
            "itests.util.src.main.java.org.apache.hive.jdbc.miniHS2.MiniHS2.withRemoteMetastore": {
                "code_before_change": "    public Builder withRemoteMetastore() {\n      this.isMetastoreRemote = true;\n      return this;\n    }",
                "code_after_change": "    public Builder withRemoteMetastore() {\n      this.isMetastoreRemote = true;\n      return this;\n    }"
            },
            "itests.util.src.main.java.org.apache.hive.jdbc.miniHS2.MiniHS2.isUseMiniKdc": {
                "code_before_change": "  public boolean isUseMiniKdc() {\n    return useMiniKdc;\n  }",
                "code_after_change": "  public boolean isUseMiniKdc() {\n    return useMiniKdc;\n  }"
            },
            "standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.security.DelegationTokenSecretManager.renewDelegationToken": {
                "code_before_change": "  public synchronized long renewDelegationToken(String tokenStrForm) throws IOException {\n    Token<DelegationTokenIdentifier> t= new Token<>();\n    t.decodeFromUrlString(tokenStrForm);\n    String user = UserGroupInformation.getCurrentUser().getUserName();\n    return renewToken(t, user);\n  }",
                "code_after_change": "  public synchronized long renewDelegationToken(String tokenStrForm) throws IOException {\n    Token<DelegationTokenIdentifier> t= new Token<>();\n    t.decodeFromUrlString(tokenStrForm);\n    String user = UserGroupInformation.getCurrentUser().getUserName();\n    return renewToken(t, user);\n  }"
            },
            "itests.util.src.main.java.org.apache.hive.jdbc.miniHS2.MiniHS2.build": {
                "code_before_change": "    public MiniHS2 build() throws Exception {\n      if (miniClusterType == MiniClusterType.MR && useMiniKdc) {\n        throw new IOException(\"Can't create secure miniMr ... yet\");\n      }\n      if (isHTTPTransMode) {\n        hiveConf.setVar(ConfVars.HIVE_SERVER2_TRANSPORT_MODE, HS2_HTTP_MODE);\n      } else {\n        hiveConf.setVar(ConfVars.HIVE_SERVER2_TRANSPORT_MODE, HS2_BINARY_MODE);\n      }\n      return new MiniHS2(hiveConf, miniClusterType, useMiniKdc, serverPrincipal, serverKeytab,\n          isMetastoreRemote, usePortsFromConf, authType, isHA, cleanupLocalDirOnStartup);\n    }",
                "code_after_change": "    public MiniHS2 build() throws Exception {\n      if (miniClusterType == MiniClusterType.MR && useMiniKdc) {\n        throw new IOException(\"Can't create secure miniMr ... yet\");\n      }\n      if (isHTTPTransMode) {\n        hiveConf.setVar(ConfVars.HIVE_SERVER2_TRANSPORT_MODE, HS2_HTTP_MODE);\n      } else {\n        hiveConf.setVar(ConfVars.HIVE_SERVER2_TRANSPORT_MODE, HS2_BINARY_MODE);\n      }\n      return new MiniHS2(hiveConf, miniClusterType, useMiniKdc, serverPrincipal, serverKeytab,\n          isMetastoreRemote, usePortsFromConf, authType, isHA, cleanupLocalDirOnStartup,\n          isMetastoreSecure, metastoreServerPrincipal, metastoreServerKeyTab);\n    }"
            },
            "service.src.java.org.apache.hive.service.cli.session.HiveSessionImplwithUGI.cancelDelegationToken": {
                "code_before_change": "  private void cancelDelegationToken() throws HiveSQLException {\n    if (hmsDelegationTokenStr != null) {\n      try {\n        Hive.get(getHiveConf()).cancelDelegationToken(hmsDelegationTokenStr);\n      } catch (HiveException e) {\n        throw new HiveSQLException(\"Couldn't cancel delegation token\", e);\n      }\n    }\n  }",
                "code_after_change": "  private void cancelDelegationToken() throws HiveSQLException {\n    if (hmsDelegationTokenStr != null) {\n      try {\n        Hive.get(getHiveConf()).cancelDelegationToken(hmsDelegationTokenStr);\n        hmsDelegationTokenStr = null;\n        getHiveConf().setVar(HiveConf.ConfVars.METASTORE_TOKEN_SIGNATURE, \"\");\n      } catch (HiveException e) {\n        throw new HiveSQLException(\"Couldn't cancel delegation token\", e);\n      }\n    }\n  }"
            },
            "standalone-metastore.src.main.java.org.apache.hadoop.hive.metastore.security.MetastoreDelegationTokenManager.getDelegationToken": {
                "code_before_change": "  public String getDelegationToken(final String owner, final String renewer, String remoteAddr)\n      throws IOException,\n      InterruptedException {\n    /*\n     * If the user asking the token is same as the 'owner' then don't do\n     * any proxy authorization checks. For cases like oozie, where it gets\n     * a delegation token for another user, we need to make sure oozie is\n     * authorized to get a delegation token.\n     */\n    // Do all checks on short names\n    UserGroupInformation currUser = UserGroupInformation.getCurrentUser();\n    UserGroupInformation ownerUgi = UserGroupInformation.createRemoteUser(owner);\n    if (!ownerUgi.getShortUserName().equals(currUser.getShortUserName())) {\n      // in the case of proxy users, the getCurrentUser will return the\n      // real user (for e.g. oozie) due to the doAs that happened just before the\n      // server started executing the method getDelegationToken in the MetaStore\n      ownerUgi = UserGroupInformation.createProxyUser(owner, UserGroupInformation.getCurrentUser());\n      ProxyUsers.authorize(ownerUgi, remoteAddr, null);\n    }\n    return ownerUgi.doAs(new PrivilegedExceptionAction<String>() {\n\n      @Override\n      public String run() throws IOException {\n        return secretManager.getDelegationToken(renewer);\n      }\n    });\n  }",
                "code_after_change": "  public String getDelegationToken(final String owner, final String renewer, String remoteAddr)\n      throws IOException,\n      InterruptedException {\n    /*\n     * If the user asking the token is same as the 'owner' then don't do\n     * any proxy authorization checks. For cases like oozie, where it gets\n     * a delegation token for another user, we need to make sure oozie is\n     * authorized to get a delegation token.\n     */\n    // Do all checks on short names\n    UserGroupInformation currUser = UserGroupInformation.getCurrentUser();\n    UserGroupInformation ownerUgi = UserGroupInformation.createRemoteUser(owner);\n    if (!ownerUgi.getShortUserName().equals(currUser.getShortUserName())) {\n      // in the case of proxy users, the getCurrentUser will return the\n      // real user (for e.g. oozie) due to the doAs that happened just before the\n      // server started executing the method getDelegationToken in the MetaStore\n      ownerUgi = UserGroupInformation.createProxyUser(owner, UserGroupInformation.getCurrentUser());\n      ProxyUsers.authorize(ownerUgi, remoteAddr, null);\n    }\n    //if impersonation is turned on this called using the HiveSessionImplWithUGI\n    //using sessionProxy. so the currentUser will be the impersonated user here eg. oozie\n    //we cannot create a proxy user which represents Oozie's client user here since\n    //we cannot authenticate it using Kerberos/Digest. We trust the user which opened\n    //session using Kerberos in this case.\n    //if impersonation is turned off, the current user is Hive which can open\n    //kerberos connections to HMS if required.\n    return secretManager.getDelegationToken(owner, renewer);\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the issue with the DBTokenStore and its failure to connect in a Kerberos-enabled environment, which is related to the ground truth method 'DBTokenStore.invokeOnTokenStore'. However, it does not precisely identify the root cause at the exact method level, hence it is classified as 'Partial' with 'Shared Stack Trace Context' since the method is mentioned in the stack trace. There is no fix suggestion provided in the bug report, so it is marked as 'Missing'. The problem location is also identified as 'Partial' with 'Shared Stack Trace Context' because the report mentions methods in the stack trace that are related to the ground truth methods. There is no wrong information in the bug report as all the details are relevant to the context of the bug."
        }
    },
    {
        "filename": "HIVE-4233.json",
        "code_diff": {
            "metastore.src.java.org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke": {
                "code_before_change": "  public Object invoke(Object proxy, Method method, Object[] args) throws Throwable {\n    Object ret = null;\n    int retriesMade = 0;\n    TException caughtException = null;\n    while (true) {\n      try {\n        ret = method.invoke(base, args);\n        break;\n      } catch (UndeclaredThrowableException e) {\n        throw e.getCause();\n      } catch (InvocationTargetException e) {\n        if ((e.getCause() instanceof TApplicationException) ||\n            (e.getCause() instanceof TProtocolException) ||\n            (e.getCause() instanceof TTransportException)) {\n          caughtException = (TException) e.getCause();\n        } else if ((e.getCause() instanceof MetaException) &&\n            e.getCause().getMessage().matches(\"JDO[a-zA-Z]*Exception\")) {\n          caughtException = (MetaException) e.getCause();\n        } else {\n          throw e.getCause();\n        }\n      }\n\n      if (retriesMade >=  retryLimit) {\n        throw caughtException;\n      }\n      retriesMade++;\n      LOG.warn(\"MetaStoreClient lost connection. Attempting to reconnect.\",\n          caughtException);\n      Thread.sleep(retryDelaySeconds * 1000);\n      base.reconnect();\n    }\n    return ret;\n  }",
                "code_after_change": "  public Object invoke(Object proxy, Method method, Object[] args) throws Throwable {\n    Object ret = null;\n    int retriesMade = 0;\n    TException caughtException = null;\n    while (true) {\n      try {\n        reloginExpiringKeytabUser();\n        if(retriesMade > 0){\n          base.reconnect();\n        }\n        ret = method.invoke(base, args);\n        break;\n      } catch (UndeclaredThrowableException e) {\n        throw e.getCause();\n      } catch (InvocationTargetException e) {\n        if ((e.getCause() instanceof TApplicationException) ||\n            (e.getCause() instanceof TProtocolException) ||\n            (e.getCause() instanceof TTransportException)) {\n          caughtException = (TException) e.getCause();\n        } else if ((e.getCause() instanceof MetaException) &&\n            e.getCause().getMessage().matches(\"JDO[a-zA-Z]*Exception\")) {\n          caughtException = (MetaException) e.getCause();\n        } else {\n          throw e.getCause();\n        }\n      }\n\n      if (retriesMade >=  retryLimit) {\n        throw caughtException;\n      }\n      retriesMade++;\n      LOG.warn(\"MetaStoreClient lost connection. Attempting to reconnect.\",\n          caughtException);\n      Thread.sleep(retryDelaySeconds * 1000);\n    }\n    return ret;\n  }"
            },
            "shims.src.common-secure.java.org.apache.hadoop.hive.shims.HadoopShimsSecure.getTokenFileLocEnvName": {
                "code_before_change": [],
                "code_after_change": "  public String getTokenFileLocEnvName() {\n    return UserGroupInformation.HADOOP_TOKEN_FILE_LOCATION;\n  }"
            },
            "shims.src.0.20.java.org.apache.hadoop.hive.shims.Hadoop20Shims.createRemoteUser": {
                "code_before_change": [],
                "code_after_change": []
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the issue as a Kerberos authentication failure due to the TGT not being renewed, which is related to the stack trace context but not the precise root cause in the ground truth methods. The report does not provide a specific fix suggestion, only a suspicion about the cause. The problem location is partially identified as it mentions the 'CLIService' class and the stack trace context, but not the exact ground truth methods. There is no incorrect information in the report."
        }
    },
    {
        "filename": "HIVE-14303.json",
        "code_diff": {
            "ql.src.java.org.apache.hadoop.hive.ql.exec.CommonJoinOperator.genAllOneUniqueJoinObject": {
                "code_before_change": "  private void genAllOneUniqueJoinObject()\n      throws HiveException {\n    int p = 0;\n    for (int i = 0; i < numAliases; i++) {\n      int sz = joinValues[order[i]].size();\n      List<Object> obj = storage[order[i]].rowIter().first();\n      for (int j = 0; j < sz; j++) {\n        forwardCache[p++] = obj.get(j);\n      }\n    }\n\n    internalForward(forwardCache, outputObjInspector);\n    countAfterReport = 0;\n  }",
                "code_after_change": "  private void genAllOneUniqueJoinObject()\n      throws HiveException {\n    int p = 0;\n    for (int i = 0; i < numAliases; i++) {\n      int sz = joinValues[order[i]].size();\n      List<Object> obj = storage[order[i]].rowIter().first();\n      for (int j = 0; j < sz; j++) {\n        forwardCache[p++] = obj.get(j);\n      }\n    }\n\n    internalForward(forwardCache, outputObjInspector);\n    countAfterReport = 0;\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.exec.CommonJoinOperator.checkAndGenObject": {
                "code_before_change": "  protected void checkAndGenObject() throws HiveException {\n    if (condn[0].getType() == JoinDesc.UNIQUE_JOIN) {\n\n      // Check if results need to be emitted.\n      // Results only need to be emitted if there is a non-null entry in a table\n      // that is preserved or if there are no non-null entries\n      boolean preserve = false; // Will be true if there is a non-null entry\n      // in a preserved table\n      boolean hasNulls = false; // Will be true if there are null entries\n      boolean allOne = true;\n      for (int i = 0; i < numAliases; i++) {\n        Byte alias = order[i];\n        AbstractRowContainer<List<Object>> alw = storage[alias];\n\n        if (!alw.isSingleRow()) {\n          allOne = false;\n        }\n\n        if (!alw.hasRows()) {\n          alw.addRow(dummyObj[i]);\n          hasNulls = true;\n        } else if (condn[i].getPreserved()) {\n          preserve = true;\n        }\n      }\n\n      if (hasNulls && !preserve) {\n        return;\n      }\n\n      if (allOne) {\n        genAllOneUniqueJoinObject();\n      } else {\n        genUniqueJoinObject(0, 0);\n      }\n    } else {\n      // does any result need to be emitted\n      boolean mayHasMoreThanOne = false;\n      boolean hasEmpty = false;\n      for (int i = 0; i < numAliases; i++) {\n        Byte alias = order[i];\n        AbstractRowContainer<List<Object>> alw = storage[alias];\n\n        if (noOuterJoin) {\n          if (!alw.hasRows()) {\n            return;\n          } else if (!alw.isSingleRow()) {\n            mayHasMoreThanOne = true;\n          }\n        } else {\n          if (!alw.hasRows()) {\n            hasEmpty = true;\n            alw.addRow(dummyObj[i]);\n          } else if (!hasEmpty && alw.isSingleRow()) {\n            if (hasAnyFiltered(alias, alw.rowIter().first())) {\n              hasEmpty = true;\n            }\n          } else {\n            mayHasMoreThanOne = true;\n            if (!hasEmpty) {\n              AbstractRowContainer.RowIterator<List<Object>> iter = alw.rowIter();\n              for (List<Object> row = iter.first(); row != null; row = iter.next()) {\n                reportProgress();\n                if (hasAnyFiltered(alias, row)) {\n                  hasEmpty = true;\n                  break;\n                }\n              }\n            }\n          }\n        }\n      }\n\n      if (!hasEmpty && !mayHasMoreThanOne) {\n        genAllOneUniqueJoinObject();\n      } else if (!hasEmpty && !hasLeftSemiJoin) {\n        genUniqueJoinObject(0, 0);\n      } else {\n        genJoinObject();\n      }\n    }\n    Arrays.fill(aliasFilterTags, (byte)0xff);\n  }",
                "code_after_change": "  protected void checkAndGenObject() throws HiveException {\n    if (state == State.CLOSE) {\n      LOG.warn(\"checkAndGenObject is called after operator \" +\n          id + \" \" + getName() + \" closed\");\n      return;\n    }\n\n    if (condn[0].getType() == JoinDesc.UNIQUE_JOIN) {\n\n      // Check if results need to be emitted.\n      // Results only need to be emitted if there is a non-null entry in a table\n      // that is preserved or if there are no non-null entries\n      boolean preserve = false; // Will be true if there is a non-null entry\n      // in a preserved table\n      boolean hasNulls = false; // Will be true if there are null entries\n      boolean allOne = true;\n      for (int i = 0; i < numAliases; i++) {\n        Byte alias = order[i];\n        AbstractRowContainer<List<Object>> alw = storage[alias];\n\n        if (!alw.isSingleRow()) {\n          allOne = false;\n        }\n\n        if (!alw.hasRows()) {\n          alw.addRow(dummyObj[i]);\n          hasNulls = true;\n        } else if (condn[i].getPreserved()) {\n          preserve = true;\n        }\n      }\n\n      if (hasNulls && !preserve) {\n        return;\n      }\n\n      if (allOne) {\n        genAllOneUniqueJoinObject();\n      } else {\n        genUniqueJoinObject(0, 0);\n      }\n    } else {\n      // does any result need to be emitted\n      boolean mayHasMoreThanOne = false;\n      boolean hasEmpty = false;\n      for (int i = 0; i < numAliases; i++) {\n        Byte alias = order[i];\n        AbstractRowContainer<List<Object>> alw = storage[alias];\n\n        if (noOuterJoin) {\n          if (!alw.hasRows()) {\n            return;\n          } else if (!alw.isSingleRow()) {\n            mayHasMoreThanOne = true;\n          }\n        } else {\n          if (!alw.hasRows()) {\n            hasEmpty = true;\n            alw.addRow(dummyObj[i]);\n          } else if (!hasEmpty && alw.isSingleRow()) {\n            if (hasAnyFiltered(alias, alw.rowIter().first())) {\n              hasEmpty = true;\n            }\n          } else {\n            mayHasMoreThanOne = true;\n            if (!hasEmpty) {\n              AbstractRowContainer.RowIterator<List<Object>> iter = alw.rowIter();\n              for (List<Object> row = iter.first(); row != null; row = iter.next()) {\n                reportProgress();\n                if (hasAnyFiltered(alias, row)) {\n                  hasEmpty = true;\n                  break;\n                }\n              }\n            }\n          }\n        }\n      }\n\n      if (!hasEmpty && !mayHasMoreThanOne) {\n        genAllOneUniqueJoinObject();\n      } else if (!hasEmpty && !hasLeftSemiJoin) {\n        genUniqueJoinObject(0, 0);\n      } else {\n        genJoinObject();\n      }\n    }\n    Arrays.fill(aliasFilterTags, (byte)0xff);\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Correct",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report precisely identifies the root cause of the bug in the method 'CommonJoinOperator.checkAndGenObject', which is part of the ground truth methods. The report suggests that the method should return directly to avoid NPE if 'ExecReducer.close' is called twice, which matches the developer's fix of adding a state check to prevent operations after closure. The problem location is also precisely identified as 'CommonJoinOperator.checkAndGenObject', which is a ground truth method. There is no wrong information in the bug report as all details are consistent with the context of the bug and the provided stack trace."
        }
    },
    {
        "filename": "HIVE-7167.json",
        "code_diff": {
            "metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.createDefaultDB_core": {
                "code_before_change": "    private void createDefaultDB_core(RawStore ms) throws MetaException, InvalidObjectException {\n      try {\n        ms.getDatabase(DEFAULT_DATABASE_NAME);\n      } catch (NoSuchObjectException e) {\n        Database db = new Database(DEFAULT_DATABASE_NAME, DEFAULT_DATABASE_COMMENT,\n          wh.getDefaultDatabasePath(DEFAULT_DATABASE_NAME).toString(), null);\n        db.setOwnerName(PUBLIC);\n        db.setOwnerType(PrincipalType.ROLE);\n        ms.createDatabase(db);\n      }\n      HMSHandler.createDefaultDB = true;\n    }",
                "code_after_change": "    private void createDefaultDB_core(RawStore ms) throws MetaException, InvalidObjectException {\n      try {\n        ms.getDatabase(DEFAULT_DATABASE_NAME);\n      } catch (NoSuchObjectException e) {\n        Database db = new Database(DEFAULT_DATABASE_NAME, DEFAULT_DATABASE_COMMENT,\n          wh.getDefaultDatabasePath(DEFAULT_DATABASE_NAME).toString(), null);\n        db.setOwnerName(PUBLIC);\n        db.setOwnerType(PrincipalType.ROLE);\n        ms.createDatabase(db);\n      }\n    }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.createDefaultRoles_core": {
                "code_before_change": [],
                "code_after_change": "    private void createDefaultRoles_core() throws MetaException {\n\n      RawStore ms = getMS();\n      try {\n        ms.addRole(ADMIN, ADMIN);\n      } catch (InvalidObjectException e) {\n        LOG.debug(ADMIN +\" role already exists\",e);\n      } catch (NoSuchObjectException e) {\n        // This should never be thrown.\n        LOG.warn(\"Unexpected exception while adding \" +ADMIN+\" roles\" , e);\n      }\n      LOG.info(\"Added \"+ ADMIN+ \" role in metastore\");\n      try {\n        ms.addRole(PUBLIC, PUBLIC);\n      } catch (InvalidObjectException e) {\n        LOG.debug(PUBLIC + \" role already exists\",e);\n      } catch (NoSuchObjectException e) {\n        // This should never be thrown.\n        LOG.warn(\"Unexpected exception while adding \"+PUBLIC +\" roles\" , e);\n      }\n      LOG.info(\"Added \"+PUBLIC+ \" role in metastore\");\n      // now grant all privs to admin\n      PrivilegeBag privs = new PrivilegeBag();\n      privs.addToPrivileges(new HiveObjectPrivilege( new HiveObjectRef(HiveObjectType.GLOBAL, null,\n        null, null, null), ADMIN, PrincipalType.ROLE, new PrivilegeGrantInfo(\"All\", 0, ADMIN,\n        PrincipalType.ROLE, true)));\n      try {\n        ms.grantPrivileges(privs);\n      } catch (InvalidObjectException e) {\n        // Surprisingly these privs are already granted.\n        LOG.debug(\"Failed while granting global privs to admin\", e);\n      } catch (NoSuchObjectException e) {\n        // Unlikely to be thrown.\n        LOG.warn(\"Failed while granting global privs to admin\", e);\n      }\n    }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report describes a connection issue with the Hive Metastore, which is related to the stack trace context where the ground truth methods are involved. However, it does not precisely identify the root cause at the ground truth methods. The report does not provide any fix suggestion, hence 'Missing' for fix suggestion. The problem location is identified in the shared stack trace context but not precisely at the ground truth methods. There is no wrong information in the bug report as it accurately describes the error context."
        }
    },
    {
        "filename": "HIVE-12360.json",
        "code_diff": {
            "ql.src.java.org.apache.hadoop.hive.ql.io.orc.MetadataReader.readRowIndex": {
                "code_before_change": "  RecordReaderImpl.Index readRowIndex(StripeInformation stripe, StripeFooter footer,\n      boolean[] included, RowIndex[] indexes, boolean[] sargColumns,\n      BloomFilterIndex[] bloomFilterIndices) throws IOException;\n\n  StripeFooter readStripeFooter(StripeInformation stripe) throws IOException;\n\n  void close() throws IOException;\n}",
                "code_after_change": "  public RecordReaderImpl.Index readRowIndex(StripeInformation stripe, OrcProto.StripeFooter footer,\n      boolean[] included, OrcProto.RowIndex[] indexes, boolean[] sargColumns,\n      OrcProto.BloomFilterIndex[] bloomFilterIndices) throws IOException {\n    if (footer == null) {\n      footer = readStripeFooter(stripe);\n    }\n    if (indexes == null) {\n      indexes = new OrcProto.RowIndex[typeCount];\n    }\n    if (bloomFilterIndices == null) {\n      bloomFilterIndices = new OrcProto.BloomFilterIndex[typeCount];\n    }\n    long offset = stripe.getOffset();\n    List<OrcProto.Stream> streams = footer.getStreamsList();\n    for (int i = 0; i < streams.size(); i++) {\n      OrcProto.Stream stream = streams.get(i);\n      OrcProto.Stream nextStream = null;\n      if (i < streams.size() - 1) {\n        nextStream = streams.get(i+1);\n      }\n      int col = stream.getColumn();\n      int len = (int) stream.getLength();\n      // row index stream and bloom filter are interlaced, check if the sarg column contains bloom\n      // filter and combine the io to read row index and bloom filters for that column together\n      if (stream.hasKind() && (stream.getKind() == OrcProto.Stream.Kind.ROW_INDEX)) {\n        boolean readBloomFilter = false;\n        if (sargColumns != null && sargColumns[col] &&\n            nextStream.getKind() == OrcProto.Stream.Kind.BLOOM_FILTER) {\n          len += nextStream.getLength();\n          i += 1;\n          readBloomFilter = true;\n        }\n        if ((included == null || included[col]) && indexes[col] == null) {\n          byte[] buffer = new byte[len];\n          file.readFully(offset, buffer, 0, buffer.length);\n          ByteBuffer[] bb = new ByteBuffer[] {ByteBuffer.wrap(buffer)};\n          bb[0].limit((int) (bb[0].position() + stream.getLength()));\n          indexes[col] = OrcProto.RowIndex.parseFrom(InStream.create(\"index\",\n              bb, new long[]{0}, stream.getLength(), codec, bufferSize));\n          if (readBloomFilter) {\n            bb[0].position((int) stream.getLength());\n            bb[0].limit((int) (bb[0].position() + nextStream.getLength()));\n            bloomFilterIndices[col] = OrcProto.BloomFilterIndex.parseFrom(\n                InStream.create(\"bloom_filter\", bb, new long[]{0}, nextStream.getLength(),\n                    codec, bufferSize));\n          }\n        }\n      }\n      offset += len;\n    }\n\n    RecordReaderImpl.Index index = new RecordReaderImpl.Index(indexes, bloomFilterIndices);\n    return index;\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the issue occurring in the context of ORC file reading with predicate pushdown, which is related to the stack trace context where the ground truth method 'MetadataReader.readRowIndex' is involved. However, it does not precisely identify the root cause at the ground truth method. The report does not provide any fix suggestion, hence 'Missing' for fix suggestion. The problem location is partially identified as it mentions methods in the stack trace context but not the exact ground truth method. There is no wrong information in the report as it accurately describes the error and its context."
        }
    },
    {
        "filename": "HIVE-13160.json",
        "code_diff": {
            "ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.registerAllFunctionsOnce": {
                "code_before_change": "  private void registerAllFunctionsOnce() {\n    boolean breakLoop = false;\n    while (!breakLoop) {\n      int val = didRegisterAllFuncs.get();\n      switch (val) {\n      case REG_FUNCS_NO: {\n        if (didRegisterAllFuncs.compareAndSet(val, REG_FUNCS_PENDING)) {\n          breakLoop = true;\n          break;\n        }\n        continue;\n      }\n      case REG_FUNCS_PENDING: {\n        synchronized (didRegisterAllFuncs) {\n          try {\n            didRegisterAllFuncs.wait(100);\n          } catch (InterruptedException e) {\n            Thread.currentThread().interrupt();\n            return;\n          }\n        }\n        continue;\n      }\n      case REG_FUNCS_DONE: return;\n      default: throw new AssertionError(val);\n      }\n    }\n    try {\n      reloadFunctions();\n    } catch (Exception e) {\n      LOG.warn(\"Failed to register all functions.\", e);\n    } finally {\n      boolean result = didRegisterAllFuncs.compareAndSet(REG_FUNCS_PENDING, REG_FUNCS_DONE);\n      assert result;\n      synchronized (didRegisterAllFuncs) {\n        didRegisterAllFuncs.notifyAll();\n      }\n    }\n  }",
                "code_after_change": "  private void registerAllFunctionsOnce() throws HiveException {\n    boolean breakLoop = false;\n    while (!breakLoop) {\n      int val = didRegisterAllFuncs.get();\n      switch (val) {\n      case REG_FUNCS_NO: {\n        if (didRegisterAllFuncs.compareAndSet(val, REG_FUNCS_PENDING)) {\n          breakLoop = true;\n          break;\n        }\n        continue;\n      }\n      case REG_FUNCS_PENDING: {\n        synchronized (didRegisterAllFuncs) {\n          try {\n            didRegisterAllFuncs.wait(100);\n          } catch (InterruptedException e) {\n            Thread.currentThread().interrupt();\n            return;\n          }\n        }\n        continue;\n      }\n      case REG_FUNCS_DONE: return;\n      default: throw new AssertionError(val);\n      }\n    }\n    try {\n      reloadFunctions();\n      didRegisterAllFuncs.compareAndSet(REG_FUNCS_PENDING, REG_FUNCS_DONE);\n    } catch (Exception e) {\n      LOG.warn(\"Failed to register all functions.\", e);\n      didRegisterAllFuncs.compareAndSet(REG_FUNCS_PENDING, REG_FUNCS_NO);\n      throw new HiveException(e);\n    } finally {\n      synchronized (didRegisterAllFuncs) {\n        didRegisterAllFuncs.notifyAll();\n      }\n    }\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.remove": {
                "code_before_change": "    public synchronized void remove() {\n      if (this.get() != null) {\n        this.get().close();\n      }\n      super.remove();\n    }",
                "code_after_change": "    public synchronized void remove() {\n      if (this.get() != null) {\n        this.get().close();\n      }\n      super.remove();\n    }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Alternative Fix",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the issue with HiveServer2 (HS2) not being able to load UDFs when the Hive Metastore (HMS) is not ready, which is related to the stack trace context but does not precisely identify the root cause in the ground truth methods. The suggestion to load the function list when each Hive session is created is an alternative fix that could resolve the issue by decoupling the function list initialization from the HS2 startup. The problem location is partially identified as it mentions the context of the issue but not the exact ground truth methods. There is no wrong information as the report accurately describes the problem and provides a plausible solution."
        }
    },
    {
        "filename": "HIVE-12008.json",
        "code_diff": {
            "ql.src.java.org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcCtx.genColLists": {
                "code_before_change": "  public List<String> genColLists(Operator<? extends OperatorDesc> curOp)\n      throws SemanticException {\n    if (curOp.getChildOperators() == null) {\n      return null;\n    }\n    List<String> colList = null;\n    for (Operator<? extends OperatorDesc> child : curOp.getChildOperators()) {\n      List<String> prunList = null;\n      if (child instanceof CommonJoinOperator) {\n        int tag = child.getParentOperators().indexOf(curOp);\n        prunList = joinPrunedColLists.get(child).get((byte) tag);\n      } else if (child instanceof UnionOperator) {\n        List<Integer> positions = unionPrunedColLists.get(child);\n        if (positions != null && positions.size() > 0) {\n          prunList = new ArrayList<>();\n          RowSchema oldRS = curOp.getSchema();\n          for (Integer pos : positions) {\n            ColumnInfo colInfo = oldRS.getSignature().get(pos);\n            prunList.add(colInfo.getInternalName());\n          }\n        }\n      } else {\n        prunList = prunedColLists.get(child);\n      }\n      if (prunList == null) {\n        continue;\n      }\n      if (colList == null) {\n        colList = new ArrayList<String>(prunList);\n      } else {\n        colList = Utilities.mergeUniqElems(colList, prunList);\n      }\n    }\n    return colList;\n  }",
                "code_after_change": "  public List<String> genColLists(Operator<? extends OperatorDesc> curOp)\n      throws SemanticException {\n    if (curOp.getChildOperators() == null) {\n      return null;\n    }\n    List<String> colList = null;\n    for (Operator<? extends OperatorDesc> child : curOp.getChildOperators()) {\n      List<String> prunList = null;\n      if (child instanceof CommonJoinOperator) {\n        int tag = child.getParentOperators().indexOf(curOp);\n        prunList = joinPrunedColLists.get(child).get((byte) tag);\n      } else if (child instanceof UnionOperator) {\n        List<Integer> positions = unionPrunedColLists.get(child);\n        if (positions != null) {\n          prunList = new ArrayList<>();\n          RowSchema oldRS = curOp.getSchema();\n          for (Integer pos : positions) {\n            ColumnInfo colInfo = oldRS.getSignature().get(pos);\n            prunList.add(colInfo.getInternalName());\n          }\n        }\n      } else if (child instanceof FileSinkOperator) {\n        prunList = new ArrayList<>();\n        RowSchema oldRS = curOp.getSchema();\n        for (ColumnInfo colInfo : oldRS.getSignature()) {\n          prunList.add(colInfo.getInternalName());\n        }\n      } else {\n        prunList = prunedColLists.get(child);\n      }\n      if (prunList == null) {\n        continue;\n      }\n      if (colList == null) {\n        colList = new ArrayList<String>(prunList);\n      } else {\n        colList = Utilities.mergeUniqElems(colList, prunList);\n      }\n    }\n    return colList;\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.exec.UnionOperator.initializeOp": {
                "code_before_change": "  protected Collection<Future<?>> initializeOp(Configuration hconf) throws HiveException {\n    Collection<Future<?>> result = super.initializeOp(hconf);\n\n    int parents = parentOperators.size();\n    parentObjInspectors = new StructObjectInspector[parents];\n    parentFields = new List[parents];\n    for (int p = 0; p < parents; p++) {\n      parentObjInspectors[p] = (StructObjectInspector) inputObjInspectors[p];\n      parentFields[p] = parentObjInspectors[p].getAllStructFieldRefs();\n    }\n\n    // Get columnNames from the first parent\n    int columns = parentFields[0].size();\n    ArrayList<String> columnNames = new ArrayList<String>(columns);\n    for (int c = 0; c < columns; c++) {\n      columnNames.add(parentFields[0].get(c).getFieldName());\n    }\n\n    // Get outputFieldOIs\n    columnTypeResolvers = new ReturnObjectInspectorResolver[columns];\n    for (int c = 0; c < columns; c++) {\n      columnTypeResolvers[c] = new ReturnObjectInspectorResolver(true);\n    }\n\n    for (int p = 0; p < parents; p++) {\n      assert (parentFields[p].size() == columns);\n      for (int c = 0; c < columns; c++) {\n        if (!columnTypeResolvers[c].updateForUnionAll(parentFields[p].get(c)\n            .getFieldObjectInspector())) {\n          // checked in SemanticAnalyzer. Should not happen\n          throw new HiveException(\"Incompatible types for union operator\");\n        }\n      }\n    }\n\n    ArrayList<ObjectInspector> outputFieldOIs = new ArrayList<ObjectInspector>(\n        columns);\n    for (int c = 0; c < columns; c++) {\n      // can be null for void type\n      ObjectInspector fieldOI = parentFields[0].get(c).getFieldObjectInspector();\n      outputFieldOIs.add(columnTypeResolvers[c].get(fieldOI));\n    }\n\n    // create output row ObjectInspector\n    outputObjInspector = ObjectInspectorFactory\n        .getStandardStructObjectInspector(columnNames, outputFieldOIs);\n    outputRow = new ArrayList<Object>(columns);\n    for (int c = 0; c < columns; c++) {\n      outputRow.add(null);\n    }\n\n    // whether we need to do transformation for each parent\n    needsTransform = new boolean[parents];\n    for (int p = 0; p < parents; p++) {\n      // Testing using != is good enough, because we use ObjectInspectorFactory\n      // to\n      // create ObjectInspectors.\n      needsTransform[p] = (inputObjInspectors[p] != outputObjInspector);\n      if (isLogInfoEnabled && needsTransform[p]) {\n        LOG.info(\"Union Operator needs to transform row from parent[\" + p\n            + \"] from \" + inputObjInspectors[p] + \" to \" + outputObjInspector);\n      }\n    }\n    return result;\n  }",
                "code_after_change": "  protected void initializeOp(Configuration hconf) throws HiveException {\n    super.initializeOp(hconf);\n\n    int parents = parentOperators.size();\n    parentObjInspectors = new StructObjectInspector[parents];\n    parentFields = new List[parents];\n    int columns = 0;\n    for (int p = 0; p < parents; p++) {\n      parentObjInspectors[p] = (StructObjectInspector) inputObjInspectors[p];\n      parentFields[p] = parentObjInspectors[p].getAllStructFieldRefs();\n      if (p == 0 || parentFields[p].size() < columns) {\n        columns = parentFields[p].size();\n      }\n    }\n\n    // Get columnNames from the first parent\n    ArrayList<String> columnNames = new ArrayList<String>(columns);\n    for (int c = 0; c < columns; c++) {\n      columnNames.add(parentFields[0].get(c).getFieldName());\n    }\n\n    // Get outputFieldOIs\n    columnTypeResolvers = new ReturnObjectInspectorResolver[columns];\n    for (int c = 0; c < columns; c++) {\n      columnTypeResolvers[c] = new ReturnObjectInspectorResolver(true);\n    }\n\n    for (int p = 0; p < parents; p++) {\n      //When columns is 0, the union operator is empty.\n      assert (columns == 0 || parentFields[p].size() == columns);\n      for (int c = 0; c < columns; c++) {\n        if (!columnTypeResolvers[c].updateForUnionAll(parentFields[p].get(c)\n            .getFieldObjectInspector())) {\n          // checked in SemanticAnalyzer. Should not happen\n          throw new HiveException(\"Incompatible types for union operator\");\n        }\n      }\n    }\n\n    ArrayList<ObjectInspector> outputFieldOIs = new ArrayList<ObjectInspector>(\n        columns);\n    for (int c = 0; c < columns; c++) {\n      // can be null for void type\n      ObjectInspector fieldOI = parentFields[0].get(c).getFieldObjectInspector();\n      outputFieldOIs.add(columnTypeResolvers[c].get(fieldOI));\n    }\n\n    // create output row ObjectInspector\n    outputObjInspector = ObjectInspectorFactory\n        .getStandardStructObjectInspector(columnNames, outputFieldOIs);\n    outputRow = new ArrayList<Object>(columns);\n    for (int c = 0; c < columns; c++) {\n      outputRow.add(null);\n    }\n\n    // whether we need to do transformation for each parent\n    needsTransform = new boolean[parents];\n    for (int p = 0; p < parents; p++) {\n      // Testing using != is good enough, because we use ObjectInspectorFactory\n      // to\n      // create ObjectInspectors.\n      needsTransform[p] = (inputObjInspectors[p] != outputObjInspector);\n      if (isLogInfoEnabled && needsTransform[p]) {\n        LOG.info(\"Union Operator needs to transform row from parent[\" + p\n            + \"] from \" + inputObjInspectors[p] + \" to \" + outputObjInspector);\n      }\n    }\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Missing",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report mentions a stack trace that includes methods like 'ExecMapper.configure', which is in the shared stack trace context with the ground truth methods. However, it does not precisely identify the root cause in the ground truth methods. There is no fix suggestion provided in the bug report. The problem location is not precisely identified as the report does not mention any ground truth methods or related methods outside of the stack trace. All information in the bug report is relevant to the context of the bug, so there is no wrong information."
        }
    },
    {
        "filename": "HIVE-6205.json",
        "code_diff": {
            "ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeAlterTableDropParts": {
                "code_before_change": "  private void analyzeAlterTableDropParts(ASTNode ast, boolean expectView)\n      throws SemanticException {\n\n    String tblName = getUnescapedName((ASTNode) ast.getChild(0));\n    // get table metadata\n    List<PartitionSpec> partSpecs = getFullPartitionSpecs(ast);\n    Table tab = getTable(tblName, true);\n    validateAlterTableType(tab, AlterTableTypes.DROPPARTITION, expectView);\n    inputs.add(new ReadEntity(tab));\n\n    // Find out if all partition columns are strings. This is needed for JDO\n    boolean stringPartitionColumns = true;\n    List<FieldSchema> partCols = tab.getPartCols();\n\n    for (FieldSchema partCol : partCols) {\n      if (!partCol.getType().toLowerCase().equals(\"string\")) {\n        stringPartitionColumns = false;\n        break;\n      }\n    }\n\n    // Only equality is supported for non-string partition columns\n    if (!stringPartitionColumns) {\n      for (PartitionSpec partSpec : partSpecs) {\n        if (partSpec.isNonEqualityOperator()) {\n          throw new SemanticException(\n              ErrorMsg.DROP_PARTITION_NON_STRING_PARTCOLS_NONEQUALITY.getMsg());\n        }\n      }\n    }\n\n    boolean ignoreProtection = (ast.getFirstChildWithType(HiveParser.TOK_IGNOREPROTECTION) != null);\n    if (partSpecs != null) {\n      boolean ifExists = (ast.getFirstChildWithType(HiveParser.TOK_IFEXISTS) != null);\n      // we want to signal an error if the partition doesn't exist and we're\n      // configured not to fail silently\n      boolean throwException =\n          !ifExists && !HiveConf.getBoolVar(conf, ConfVars.DROPIGNORESNONEXISTENT);\n      addTableDropPartsOutputs(tblName, partSpecs, throwException,\n                                stringPartitionColumns, ignoreProtection);\n    }\n    DropTableDesc dropTblDesc =\n        new DropTableDesc(tblName, partSpecs, expectView, stringPartitionColumns, ignoreProtection);\n\n    rootTasks.add(TaskFactory.get(new DDLWork(getInputs(), getOutputs(),\n        dropTblDesc), conf));\n  }",
                "code_after_change": "  private void analyzeAlterTableDropParts(ASTNode ast, boolean expectView)\n      throws SemanticException {\n\n    String tblName = getUnescapedName((ASTNode) ast.getChild(0));\n    // get table metadata\n    List<PartitionSpec> partSpecs = getFullPartitionSpecs(ast);\n    Table tab = getTable(tblName, true);\n    validateAlterTableType(tab, AlterTableTypes.DROPPARTITION, expectView);\n    inputs.add(new ReadEntity(tab));\n\n    // Find out if all partition columns are strings. This is needed for JDO\n    boolean stringPartitionColumns = true;\n    List<FieldSchema> partCols = tab.getPartCols();\n\n    for (FieldSchema partCol : partCols) {\n      if (!partCol.getType().toLowerCase().equals(\"string\")) {\n        stringPartitionColumns = false;\n        break;\n      }\n    }\n\n    // Only equality is supported for non-string partition columns\n    if (!stringPartitionColumns) {\n      for (PartitionSpec partSpec : partSpecs) {\n        if (partSpec.isNonEqualityOperator()) {\n          throw new SemanticException(\n              ErrorMsg.DROP_PARTITION_NON_STRING_PARTCOLS_NONEQUALITY.getMsg());\n        }\n      }\n    }\n\n    boolean ignoreProtection = (ast.getFirstChildWithType(HiveParser.TOK_IGNOREPROTECTION) != null);\n    if (partSpecs != null) {\n      boolean ifExists = (ast.getFirstChildWithType(HiveParser.TOK_IFEXISTS) != null);\n      // we want to signal an error if the partition doesn't exist and we're\n      // configured not to fail silently\n      boolean throwException =\n          !ifExists && !HiveConf.getBoolVar(conf, ConfVars.DROPIGNORESNONEXISTENT);\n      addTableDropPartsOutputs(tblName, partSpecs, throwException,\n                                stringPartitionColumns, ignoreProtection);\n    }\n    DropTableDesc dropTblDesc =\n        new DropTableDesc(tblName, partSpecs, expectView, stringPartitionColumns, ignoreProtection);\n\n    rootTasks.add(TaskFactory.get(new DDLWork(getInputs(), getOutputs(),\n        dropTblDesc), conf));\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.Driver.doAuthorization": {
                "code_before_change": "  private void doAuthorization(BaseSemanticAnalyzer sem)\n    throws HiveException, AuthorizationException {\n    HashSet<ReadEntity> inputs = sem.getInputs();\n    HashSet<WriteEntity> outputs = sem.getOutputs();\n    SessionState ss = SessionState.get();\n    HiveOperation op = ss.getHiveOperation();\n    Hive db = sem.getDb();\n    if (op != null) {\n      if (op.equals(HiveOperation.CREATEDATABASE)) {\n        ss.getAuthorizer().authorize(\n            op.getInputRequiredPrivileges(), op.getOutputRequiredPrivileges());\n      } else if (op.equals(HiveOperation.CREATETABLE_AS_SELECT)\n          || op.equals(HiveOperation.CREATETABLE)) {\n        ss.getAuthorizer().authorize(\n            db.getDatabase(SessionState.get().getCurrentDatabase()), null,\n            HiveOperation.CREATETABLE_AS_SELECT.getOutputRequiredPrivileges());\n      } else {\n        if (op.equals(HiveOperation.IMPORT)) {\n          ImportSemanticAnalyzer isa = (ImportSemanticAnalyzer) sem;\n          if (!isa.existsTable()) {\n            ss.getAuthorizer().authorize(\n                db.getDatabase(SessionState.get().getCurrentDatabase()), null,\n                HiveOperation.CREATETABLE_AS_SELECT.getOutputRequiredPrivileges());\n          }\n        }\n      }\n      if (outputs != null && outputs.size() > 0) {\n        for (WriteEntity write : outputs) {\n          if (write.getType() == Entity.Type.DATABASE) {\n            ss.getAuthorizer().authorize(write.getDatabase(),\n                null, op.getOutputRequiredPrivileges());\n            continue;\n          }\n\n          if (write.getType() == WriteEntity.Type.PARTITION) {\n            Partition part = db.getPartition(write.getTable(), write\n                .getPartition().getSpec(), false);\n            if (part != null) {\n              ss.getAuthorizer().authorize(write.getPartition(), null,\n                      op.getOutputRequiredPrivileges());\n              continue;\n            }\n          }\n\n          if (write.getTable() != null) {\n            ss.getAuthorizer().authorize(write.getTable(), null,\n                    op.getOutputRequiredPrivileges());\n          }\n        }\n\n      }\n    }\n\n    if (inputs != null && inputs.size() > 0) {\n\n      Map<Table, List<String>> tab2Cols = new HashMap<Table, List<String>>();\n      Map<Partition, List<String>> part2Cols = new HashMap<Partition, List<String>>();\n\n      Map<String, Boolean> tableUsePartLevelAuth = new HashMap<String, Boolean>();\n      for (ReadEntity read : inputs) {\n        if (read.getType() == Entity.Type.DATABASE) {\n          continue;\n        }\n        Table tbl = read.getTable();\n        if ((read.getPartition() != null) || (tbl.isPartitioned())) {\n          String tblName = tbl.getTableName();\n          if (tableUsePartLevelAuth.get(tblName) == null) {\n            boolean usePartLevelPriv = (tbl.getParameters().get(\n                \"PARTITION_LEVEL_PRIVILEGE\") != null && (\"TRUE\"\n                .equalsIgnoreCase(tbl.getParameters().get(\n                    \"PARTITION_LEVEL_PRIVILEGE\"))));\n            if (usePartLevelPriv) {\n              tableUsePartLevelAuth.put(tblName, Boolean.TRUE);\n            } else {\n              tableUsePartLevelAuth.put(tblName, Boolean.FALSE);\n            }\n          }\n        }\n      }\n\n      if (op.equals(HiveOperation.CREATETABLE_AS_SELECT)\n          || op.equals(HiveOperation.QUERY)) {\n        SemanticAnalyzer querySem = (SemanticAnalyzer) sem;\n        ParseContext parseCtx = querySem.getParseContext();\n        Map<TableScanOperator, Table> tsoTopMap = parseCtx.getTopToTable();\n\n        for (Map.Entry<String, Operator<? extends OperatorDesc>> topOpMap : querySem\n            .getParseContext().getTopOps().entrySet()) {\n          Operator<? extends OperatorDesc> topOp = topOpMap.getValue();\n          if (topOp instanceof TableScanOperator\n              && tsoTopMap.containsKey(topOp)) {\n            TableScanOperator tableScanOp = (TableScanOperator) topOp;\n            Table tbl = tsoTopMap.get(tableScanOp);\n            List<Integer> neededColumnIds = tableScanOp.getNeededColumnIDs();\n            List<FieldSchema> columns = tbl.getCols();\n            List<String> cols = new ArrayList<String>();\n            for (int i = 0; i < neededColumnIds.size(); i++) {\n              cols.add(columns.get(neededColumnIds.get(i)).getName());\n            }\n            //map may not contain all sources, since input list may have been optimized out\n            //or non-existent tho such sources may still be referenced by the TableScanOperator\n            //if it's null then the partition probably doesn't exist so let's use table permission\n            if (tbl.isPartitioned() &&\n                tableUsePartLevelAuth.get(tbl.getTableName()) == Boolean.TRUE) {\n              String alias_id = topOpMap.getKey();\n\n              PrunedPartitionList partsList = PartitionPruner.prune(tableScanOp,\n                  parseCtx, alias_id);\n              Set<Partition> parts = partsList.getPartitions();\n              for (Partition part : parts) {\n                List<String> existingCols = part2Cols.get(part);\n                if (existingCols == null) {\n                  existingCols = new ArrayList<String>();\n                }\n                existingCols.addAll(cols);\n                part2Cols.put(part, existingCols);\n              }\n            } else {\n              List<String> existingCols = tab2Cols.get(tbl);\n              if (existingCols == null) {\n                existingCols = new ArrayList<String>();\n              }\n              existingCols.addAll(cols);\n              tab2Cols.put(tbl, existingCols);\n            }\n          }\n        }\n      }\n\n      // cache the results for table authorization\n      Set<String> tableAuthChecked = new HashSet<String>();\n      for (ReadEntity read : inputs) {\n        if (read.getType() == Entity.Type.DATABASE) {\n          ss.getAuthorizer().authorize(read.getDatabase(), op.getInputRequiredPrivileges(), null);\n          continue;\n        }\n        Table tbl = read.getTable();\n        if (read.getPartition() != null) {\n          Partition partition = read.getPartition();\n          tbl = partition.getTable();\n          // use partition level authorization\n          if (tableUsePartLevelAuth.get(tbl.getTableName()) == Boolean.TRUE) {\n            List<String> cols = part2Cols.get(partition);\n            if (cols != null && cols.size() > 0) {\n              ss.getAuthorizer().authorize(partition.getTable(),\n                  partition, cols, op.getInputRequiredPrivileges(),\n                  null);\n            } else {\n              ss.getAuthorizer().authorize(partition,\n                  op.getInputRequiredPrivileges(), null);\n            }\n            continue;\n          }\n        }\n\n        // if we reach here, it means it needs to do a table authorization\n        // check, and the table authorization may already happened because of other\n        // partitions\n        if (tbl != null && !tableAuthChecked.contains(tbl.getTableName()) &&\n            !(tableUsePartLevelAuth.get(tbl.getTableName()) == Boolean.TRUE)) {\n          List<String> cols = tab2Cols.get(tbl);\n          if (cols != null && cols.size() > 0) {\n            ss.getAuthorizer().authorize(tbl, null, cols,\n                op.getInputRequiredPrivileges(), null);\n          } else {\n            ss.getAuthorizer().authorize(tbl, op.getInputRequiredPrivileges(),\n                null);\n          }\n          tableAuthChecked.add(tbl.getTableName());\n        }\n      }\n\n    }\n  }",
                "code_after_change": "  private void doAuthorization(BaseSemanticAnalyzer sem)\n    throws HiveException, AuthorizationException {\n    HashSet<ReadEntity> inputs = sem.getInputs();\n    HashSet<WriteEntity> outputs = sem.getOutputs();\n    SessionState ss = SessionState.get();\n    HiveOperation op = ss.getHiveOperation();\n    Hive db = sem.getDb();\n    if (ss.isAuthorizationModeV2()) {\n      doAuthorizationV2(ss, op, inputs, outputs);\n      return;\n    }\n\n    if (op == null) {\n      throw new HiveException(\"Operation should not be null\");\n    }\n    if (op.equals(HiveOperation.CREATEDATABASE)) {\n      ss.getAuthorizer().authorize(\n          op.getInputRequiredPrivileges(), op.getOutputRequiredPrivileges());\n    } else if (op.equals(HiveOperation.CREATETABLE_AS_SELECT)\n        || op.equals(HiveOperation.CREATETABLE)) {\n      ss.getAuthorizer().authorize(\n          db.getDatabase(SessionState.get().getCurrentDatabase()), null,\n          HiveOperation.CREATETABLE_AS_SELECT.getOutputRequiredPrivileges());\n    } else {\n      if (op.equals(HiveOperation.IMPORT)) {\n        ImportSemanticAnalyzer isa = (ImportSemanticAnalyzer) sem;\n        if (!isa.existsTable()) {\n          ss.getAuthorizer().authorize(\n              db.getDatabase(SessionState.get().getCurrentDatabase()), null,\n              HiveOperation.CREATETABLE_AS_SELECT.getOutputRequiredPrivileges());\n        }\n      }\n    }\n    if (outputs != null && outputs.size() > 0) {\n      for (WriteEntity write : outputs) {\n        if (write.getType() == Entity.Type.DATABASE) {\n          ss.getAuthorizer().authorize(write.getDatabase(),\n              null, op.getOutputRequiredPrivileges());\n          continue;\n        }\n\n        if (write.getType() == WriteEntity.Type.PARTITION) {\n          Partition part = db.getPartition(write.getTable(), write\n              .getPartition().getSpec(), false);\n          if (part != null) {\n            ss.getAuthorizer().authorize(write.getPartition(), null,\n                    op.getOutputRequiredPrivileges());\n            continue;\n          }\n        }\n\n        if (write.getTable() != null) {\n          ss.getAuthorizer().authorize(write.getTable(), null,\n                  op.getOutputRequiredPrivileges());\n        }\n      }\n    }\n\n    if (inputs != null && inputs.size() > 0) {\n      Map<Table, List<String>> tab2Cols = new HashMap<Table, List<String>>();\n      Map<Partition, List<String>> part2Cols = new HashMap<Partition, List<String>>();\n\n      //determine if partition level privileges should be checked for input tables\n      Map<String, Boolean> tableUsePartLevelAuth = new HashMap<String, Boolean>();\n      for (ReadEntity read : inputs) {\n        if (read.getType() == Entity.Type.DATABASE) {\n          continue;\n        }\n        Table tbl = read.getTable();\n        if ((read.getPartition() != null) || (tbl.isPartitioned())) {\n          String tblName = tbl.getTableName();\n          if (tableUsePartLevelAuth.get(tblName) == null) {\n            boolean usePartLevelPriv = (tbl.getParameters().get(\n                \"PARTITION_LEVEL_PRIVILEGE\") != null && (\"TRUE\"\n                .equalsIgnoreCase(tbl.getParameters().get(\n                    \"PARTITION_LEVEL_PRIVILEGE\"))));\n            if (usePartLevelPriv) {\n              tableUsePartLevelAuth.put(tblName, Boolean.TRUE);\n            } else {\n              tableUsePartLevelAuth.put(tblName, Boolean.FALSE);\n            }\n          }\n        }\n      }\n\n      //for a select or create-as-select query, populate the partition to column (par2Cols) or\n      // table to columns mapping (tab2Cols)\n      if (op.equals(HiveOperation.CREATETABLE_AS_SELECT)\n          || op.equals(HiveOperation.QUERY)) {\n        SemanticAnalyzer querySem = (SemanticAnalyzer) sem;\n        ParseContext parseCtx = querySem.getParseContext();\n        Map<TableScanOperator, Table> tsoTopMap = parseCtx.getTopToTable();\n\n        for (Map.Entry<String, Operator<? extends OperatorDesc>> topOpMap : querySem\n            .getParseContext().getTopOps().entrySet()) {\n          Operator<? extends OperatorDesc> topOp = topOpMap.getValue();\n          if (topOp instanceof TableScanOperator\n              && tsoTopMap.containsKey(topOp)) {\n            TableScanOperator tableScanOp = (TableScanOperator) topOp;\n            Table tbl = tsoTopMap.get(tableScanOp);\n            List<Integer> neededColumnIds = tableScanOp.getNeededColumnIDs();\n            List<FieldSchema> columns = tbl.getCols();\n            List<String> cols = new ArrayList<String>();\n            for (int i = 0; i < neededColumnIds.size(); i++) {\n              cols.add(columns.get(neededColumnIds.get(i)).getName());\n            }\n            //map may not contain all sources, since input list may have been optimized out\n            //or non-existent tho such sources may still be referenced by the TableScanOperator\n            //if it's null then the partition probably doesn't exist so let's use table permission\n            if (tbl.isPartitioned() &&\n                tableUsePartLevelAuth.get(tbl.getTableName()) == Boolean.TRUE) {\n              String alias_id = topOpMap.getKey();\n\n              PrunedPartitionList partsList = PartitionPruner.prune(tableScanOp,\n                  parseCtx, alias_id);\n              Set<Partition> parts = partsList.getPartitions();\n              for (Partition part : parts) {\n                List<String> existingCols = part2Cols.get(part);\n                if (existingCols == null) {\n                  existingCols = new ArrayList<String>();\n                }\n                existingCols.addAll(cols);\n                part2Cols.put(part, existingCols);\n              }\n            } else {\n              List<String> existingCols = tab2Cols.get(tbl);\n              if (existingCols == null) {\n                existingCols = new ArrayList<String>();\n              }\n              existingCols.addAll(cols);\n              tab2Cols.put(tbl, existingCols);\n            }\n          }\n        }\n      }\n\n      // cache the results for table authorization\n      Set<String> tableAuthChecked = new HashSet<String>();\n      for (ReadEntity read : inputs) {\n        if (read.getType() == Entity.Type.DATABASE) {\n          ss.getAuthorizer().authorize(read.getDatabase(), op.getInputRequiredPrivileges(), null);\n          continue;\n        }\n        Table tbl = read.getTable();\n        if (read.getPartition() != null) {\n          Partition partition = read.getPartition();\n          tbl = partition.getTable();\n          // use partition level authorization\n          if (tableUsePartLevelAuth.get(tbl.getTableName()) == Boolean.TRUE) {\n            List<String> cols = part2Cols.get(partition);\n            if (cols != null && cols.size() > 0) {\n              ss.getAuthorizer().authorize(partition.getTable(),\n                  partition, cols, op.getInputRequiredPrivileges(),\n                  null);\n            } else {\n              ss.getAuthorizer().authorize(partition,\n                  op.getInputRequiredPrivileges(), null);\n            }\n            continue;\n          }\n        }\n\n        // if we reach here, it means it needs to do a table authorization\n        // check, and the table authorization may already happened because of other\n        // partitions\n        if (tbl != null && !tableAuthChecked.contains(tbl.getTableName()) &&\n            !(tableUsePartLevelAuth.get(tbl.getTableName()) == Boolean.TRUE)) {\n          List<String> cols = tab2Cols.get(tbl);\n          if (cols != null && cols.size() > 0) {\n            ss.getAuthorizer().authorize(tbl, null, cols,\n                op.getInputRequiredPrivileges(), null);\n          } else {\n            ss.getAuthorizer().authorize(tbl, op.getInputRequiredPrivileges(),\n                null);\n          }\n          tableAuthChecked.add(tbl.getTableName());\n        }\n      }\n\n    }\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeInternal": {
                "code_before_change": "  public void analyzeInternal(ASTNode ast) throws SemanticException {\n\n    switch (ast.getToken().getType()) {\n    case HiveParser.TOK_ALTERTABLE_PARTITION: {\n      ASTNode tablePart = (ASTNode) ast.getChild(0);\n      TablePartition tblPart = new TablePartition(tablePart);\n      String tableName = tblPart.tableName;\n      HashMap<String, String> partSpec = tblPart.partSpec;\n      ast = (ASTNode) ast.getChild(1);\n      if (ast.getToken().getType() == HiveParser.TOK_ALTERTABLE_FILEFORMAT) {\n        analyzeAlterTableFileFormat(ast, tableName, partSpec);\n      } else if (ast.getToken().getType() == HiveParser.TOK_ALTERTABLE_ALTERPARTS_PROTECTMODE) {\n        analyzeAlterTableProtectMode(ast, tableName, partSpec);\n      } else if (ast.getToken().getType() == HiveParser.TOK_ALTERTABLE_LOCATION) {\n        analyzeAlterTableLocation(ast, tableName, partSpec);\n      } else if (ast.getToken().getType() == HiveParser.TOK_ALTERTABLE_ALTERPARTS_MERGEFILES) {\n        analyzeAlterTablePartMergeFiles(tablePart, ast, tableName, partSpec);\n      } else if (ast.getToken().getType() == HiveParser.TOK_ALTERTABLE_SERIALIZER) {\n        analyzeAlterTableSerde(ast, tableName, partSpec);\n      } else if (ast.getToken().getType() == HiveParser.TOK_ALTERTABLE_SERDEPROPERTIES) {\n        analyzeAlterTableSerdeProps(ast, tableName, partSpec);\n      } else if (ast.getToken().getType() == HiveParser.TOK_ALTERTABLE_RENAMEPART) {\n        analyzeAlterTableRenamePart(ast, tableName, partSpec);\n      } else if (ast.getToken().getType() == HiveParser.TOK_ALTERTBLPART_SKEWED_LOCATION) {\n        analyzeAlterTableSkewedLocation(ast, tableName, partSpec);\n      } else if (ast.getToken().getType() == HiveParser.TOK_TABLEBUCKETS) {\n        analyzeAlterTableBucketNum(ast, tableName, partSpec);\n      } else if (ast.getToken().getType() == HiveParser.TOK_ALTERTABLE_CLUSTER_SORT) {\n        analyzeAlterTableClusterSort(ast, tableName, partSpec);\n      }\n      break;\n    }\n    case HiveParser.TOK_DROPTABLE:\n      analyzeDropTable(ast, false);\n      break;\n    case HiveParser.TOK_TRUNCATETABLE:\n      analyzeTruncateTable(ast);\n      break;\n    case HiveParser.TOK_CREATEINDEX:\n      analyzeCreateIndex(ast);\n      break;\n    case HiveParser.TOK_DROPINDEX:\n      analyzeDropIndex(ast);\n      break;\n    case HiveParser.TOK_DESCTABLE:\n      ctx.setResFile(new Path(ctx.getLocalTmpFileURI()));\n      analyzeDescribeTable(ast);\n      break;\n    case HiveParser.TOK_SHOWDATABASES:\n      ctx.setResFile(new Path(ctx.getLocalTmpFileURI()));\n      analyzeShowDatabases(ast);\n      break;\n    case HiveParser.TOK_SHOWTABLES:\n      ctx.setResFile(new Path(ctx.getLocalTmpFileURI()));\n      analyzeShowTables(ast);\n      break;\n    case HiveParser.TOK_SHOWCOLUMNS:\n      ctx.setResFile(new Path(ctx.getLocalTmpFileURI()));\n      analyzeShowColumns(ast);\n      break;\n    case HiveParser.TOK_SHOW_TABLESTATUS:\n      ctx.setResFile(new Path(ctx.getLocalTmpFileURI()));\n      analyzeShowTableStatus(ast);\n      break;\n    case HiveParser.TOK_SHOW_TBLPROPERTIES:\n      ctx.setResFile(new Path(ctx.getLocalTmpFileURI()));\n      analyzeShowTableProperties(ast);\n      break;\n    case HiveParser.TOK_SHOWFUNCTIONS:\n      ctx.setResFile(new Path(ctx.getLocalTmpFileURI()));\n      analyzeShowFunctions(ast);\n      break;\n    case HiveParser.TOK_SHOWLOCKS:\n      ctx.setResFile(new Path(ctx.getLocalTmpFileURI()));\n      analyzeShowLocks(ast);\n      break;\n    case HiveParser.TOK_SHOWDBLOCKS:\n      ctx.setResFile(new Path(ctx.getLocalTmpFileURI()));\n      analyzeShowDbLocks(ast);\n      break;\n    case HiveParser.TOK_DESCFUNCTION:\n      ctx.setResFile(new Path(ctx.getLocalTmpFileURI()));\n      analyzeDescFunction(ast);\n      break;\n    case HiveParser.TOK_DESCDATABASE:\n      ctx.setResFile(new Path(ctx.getLocalTmpFileURI()));\n      analyzeDescDatabase(ast);\n      break;\n    case HiveParser.TOK_MSCK:\n      ctx.setResFile(new Path(ctx.getLocalTmpFileURI()));\n      analyzeMetastoreCheck(ast);\n      break;\n    case HiveParser.TOK_DROPVIEW:\n      analyzeDropTable(ast, true);\n      break;\n    case HiveParser.TOK_ALTERVIEW_PROPERTIES:\n      analyzeAlterTableProps(ast, true, false);\n      break;\n    case HiveParser.TOK_DROPVIEW_PROPERTIES:\n      analyzeAlterTableProps(ast, true, true);\n      break;\n    case HiveParser.TOK_ALTERVIEW_ADDPARTS:\n      // for ALTER VIEW ADD PARTITION, we wrapped the ADD to discriminate\n      // view from table; unwrap it now\n      analyzeAlterTableAddParts((ASTNode) ast.getChild(0), true);\n      break;\n    case HiveParser.TOK_ALTERVIEW_DROPPARTS:\n      // for ALTER VIEW DROP PARTITION, we wrapped the DROP to discriminate\n      // view from table; unwrap it now\n      analyzeAlterTableDropParts((ASTNode) ast.getChild(0), true);\n      break;\n    case HiveParser.TOK_ALTERVIEW_RENAME:\n      // for ALTER VIEW RENAME, we wrapped the RENAME to discriminate\n      // view from table; unwrap it now\n      analyzeAlterTableRename(((ASTNode) ast.getChild(0)), true);\n      break;\n    case HiveParser.TOK_ALTERTABLE_RENAME:\n      analyzeAlterTableRename(ast, false);\n      break;\n    case HiveParser.TOK_ALTERTABLE_TOUCH:\n      analyzeAlterTableTouch(ast);\n      break;\n    case HiveParser.TOK_ALTERTABLE_ARCHIVE:\n      analyzeAlterTableArchive(ast, false);\n      break;\n    case HiveParser.TOK_ALTERTABLE_UNARCHIVE:\n      analyzeAlterTableArchive(ast, true);\n      break;\n    case HiveParser.TOK_ALTERTABLE_ADDCOLS:\n      analyzeAlterTableModifyCols(ast, AlterTableTypes.ADDCOLS);\n      break;\n    case HiveParser.TOK_ALTERTABLE_REPLACECOLS:\n      analyzeAlterTableModifyCols(ast, AlterTableTypes.REPLACECOLS);\n      break;\n    case HiveParser.TOK_ALTERTABLE_RENAMECOL:\n      analyzeAlterTableRenameCol(ast);\n      break;\n    case HiveParser.TOK_ALTERTABLE_ADDPARTS:\n      analyzeAlterTableAddParts(ast, false);\n      break;\n    case HiveParser.TOK_ALTERTABLE_DROPPARTS:\n      analyzeAlterTableDropParts(ast, false);\n      break;\n    case HiveParser.TOK_ALTERTABLE_ALTERPARTS:\n      analyzeAlterTableAlterParts(ast);\n      break;\n    case HiveParser.TOK_ALTERTABLE_PROPERTIES:\n      analyzeAlterTableProps(ast, false, false);\n      break;\n    case HiveParser.TOK_DROPTABLE_PROPERTIES:\n      analyzeAlterTableProps(ast, false, true);\n      break;\n    case HiveParser.TOK_ALTERINDEX_REBUILD:\n      analyzeAlterIndexRebuild(ast);\n      break;\n    case HiveParser.TOK_ALTERINDEX_PROPERTIES:\n      analyzeAlterIndexProps(ast);\n      break;\n    case HiveParser.TOK_SHOWPARTITIONS:\n      ctx.setResFile(new Path(ctx.getLocalTmpFileURI()));\n      analyzeShowPartitions(ast);\n      break;\n    case HiveParser.TOK_SHOW_CREATETABLE:\n      ctx.setResFile(new Path(ctx.getLocalTmpFileURI()));\n      analyzeShowCreateTable(ast);\n      break;\n    case HiveParser.TOK_SHOWINDEXES:\n      ctx.setResFile(new Path(ctx.getLocalTmpFileURI()));\n      analyzeShowIndexes(ast);\n      break;\n    case HiveParser.TOK_LOCKTABLE:\n      analyzeLockTable(ast);\n      break;\n    case HiveParser.TOK_UNLOCKTABLE:\n      analyzeUnlockTable(ast);\n      break;\n    case HiveParser.TOK_LOCKDB:\n      analyzeLockDatabase(ast);\n      break;\n    case HiveParser.TOK_UNLOCKDB:\n      analyzeUnlockDatabase(ast);\n      break;\n    case HiveParser.TOK_CREATEDATABASE:\n      analyzeCreateDatabase(ast);\n      break;\n    case HiveParser.TOK_DROPDATABASE:\n      analyzeDropDatabase(ast);\n      break;\n    case HiveParser.TOK_SWITCHDATABASE:\n      analyzeSwitchDatabase(ast);\n      break;\n    case HiveParser.TOK_ALTERDATABASE_PROPERTIES:\n      analyzeAlterDatabase(ast);\n      break;\n    case HiveParser.TOK_CREATEROLE:\n      analyzeCreateRole(ast);\n      break;\n    case HiveParser.TOK_DROPROLE:\n      analyzeDropRole(ast);\n      break;\n    case HiveParser.TOK_SHOW_ROLE_GRANT:\n      ctx.setResFile(new Path(ctx.getLocalTmpFileURI()));\n      analyzeShowRoleGrant(ast);\n      break;\n    case HiveParser.TOK_SHOW_ROLES:\n      ctx.setResFile(new Path(ctx.getLocalTmpFileURI()));\n      analyzeShowRoles(ast);\n      break;\n    case HiveParser.TOK_GRANT_ROLE:\n      analyzeGrantRevokeRole(true, ast);\n      break;\n    case HiveParser.TOK_REVOKE_ROLE:\n      analyzeGrantRevokeRole(false, ast);\n      break;\n    case HiveParser.TOK_GRANT:\n      analyzeGrant(ast);\n      break;\n    case HiveParser.TOK_SHOW_GRANT:\n      ctx.setResFile(new Path(ctx.getLocalTmpFileURI()));\n      analyzeShowGrant(ast);\n      break;\n    case HiveParser.TOK_REVOKE:\n      analyzeRevoke(ast);\n      break;\n    case HiveParser.TOK_ALTERTABLE_SKEWED:\n      analyzeAltertableSkewedby(ast);\n      break;\n   case HiveParser.TOK_EXCHANGEPARTITION:\n      analyzeExchangePartition(ast);\n      break;\n    default:\n      throw new SemanticException(\"Unsupported command.\");\n    }\n  }",
                "code_after_change": "  public void analyzeInternal(ASTNode ast) throws SemanticException {\n\n    switch (ast.getToken().getType()) {\n    case HiveParser.TOK_ALTERTABLE_PARTITION: {\n      ASTNode tablePart = (ASTNode) ast.getChild(0);\n      TablePartition tblPart = new TablePartition(tablePart);\n      String tableName = tblPart.tableName;\n      HashMap<String, String> partSpec = tblPart.partSpec;\n      ast = (ASTNode) ast.getChild(1);\n      if (ast.getToken().getType() == HiveParser.TOK_ALTERTABLE_FILEFORMAT) {\n        analyzeAlterTableFileFormat(ast, tableName, partSpec);\n      } else if (ast.getToken().getType() == HiveParser.TOK_ALTERTABLE_PROTECTMODE) {\n        analyzeAlterTableProtectMode(ast, tableName, partSpec);\n      } else if (ast.getToken().getType() == HiveParser.TOK_ALTERTABLE_LOCATION) {\n        analyzeAlterTableLocation(ast, tableName, partSpec);\n      } else if (ast.getToken().getType() == HiveParser.TOK_ALTERTABLE_MERGEFILES) {\n        analyzeAlterTablePartMergeFiles(tablePart, ast, tableName, partSpec);\n      } else if (ast.getToken().getType() == HiveParser.TOK_ALTERTABLE_SERIALIZER) {\n        analyzeAlterTableSerde(ast, tableName, partSpec);\n      } else if (ast.getToken().getType() == HiveParser.TOK_ALTERTABLE_SERDEPROPERTIES) {\n        analyzeAlterTableSerdeProps(ast, tableName, partSpec);\n      } else if (ast.getToken().getType() == HiveParser.TOK_ALTERTABLE_RENAMEPART) {\n        analyzeAlterTableRenamePart(ast, tableName, partSpec);\n      } else if (ast.getToken().getType() == HiveParser.TOK_ALTERTBLPART_SKEWED_LOCATION) {\n        analyzeAlterTableSkewedLocation(ast, tableName, partSpec);\n      } else if (ast.getToken().getType() == HiveParser.TOK_TABLEBUCKETS) {\n        analyzeAlterTableBucketNum(ast, tableName, partSpec);\n      } else if (ast.getToken().getType() == HiveParser.TOK_ALTERTABLE_CLUSTER_SORT) {\n        analyzeAlterTableClusterSort(ast, tableName, partSpec);\n      }\n      break;\n    }\n    case HiveParser.TOK_DROPTABLE:\n      analyzeDropTable(ast, false);\n      break;\n    case HiveParser.TOK_TRUNCATETABLE:\n      analyzeTruncateTable(ast);\n      break;\n    case HiveParser.TOK_CREATEINDEX:\n      analyzeCreateIndex(ast);\n      break;\n    case HiveParser.TOK_DROPINDEX:\n      analyzeDropIndex(ast);\n      break;\n    case HiveParser.TOK_DESCTABLE:\n      ctx.setResFile(ctx.getLocalTmpPath());\n      analyzeDescribeTable(ast);\n      break;\n    case HiveParser.TOK_SHOWDATABASES:\n      ctx.setResFile(ctx.getLocalTmpPath());\n      analyzeShowDatabases(ast);\n      break;\n    case HiveParser.TOK_SHOWTABLES:\n      ctx.setResFile(ctx.getLocalTmpPath());\n      analyzeShowTables(ast);\n      break;\n    case HiveParser.TOK_SHOWCOLUMNS:\n      ctx.setResFile(ctx.getLocalTmpPath());\n      analyzeShowColumns(ast);\n      break;\n    case HiveParser.TOK_SHOW_TABLESTATUS:\n      ctx.setResFile(ctx.getLocalTmpPath());\n      analyzeShowTableStatus(ast);\n      break;\n    case HiveParser.TOK_SHOW_TBLPROPERTIES:\n      ctx.setResFile(ctx.getLocalTmpPath());\n      analyzeShowTableProperties(ast);\n      break;\n    case HiveParser.TOK_SHOWFUNCTIONS:\n      ctx.setResFile(ctx.getLocalTmpPath());\n      analyzeShowFunctions(ast);\n      break;\n    case HiveParser.TOK_SHOWLOCKS:\n      ctx.setResFile(ctx.getLocalTmpPath());\n      analyzeShowLocks(ast);\n      break;\n    case HiveParser.TOK_SHOWDBLOCKS:\n      ctx.setResFile(ctx.getLocalTmpPath());\n      analyzeShowDbLocks(ast);\n      break;\n    case HiveParser.TOK_DESCFUNCTION:\n      ctx.setResFile(ctx.getLocalTmpPath());\n      analyzeDescFunction(ast);\n      break;\n    case HiveParser.TOK_DESCDATABASE:\n      ctx.setResFile(ctx.getLocalTmpPath());\n      analyzeDescDatabase(ast);\n      break;\n    case HiveParser.TOK_MSCK:\n      ctx.setResFile(ctx.getLocalTmpPath());\n      analyzeMetastoreCheck(ast);\n      break;\n    case HiveParser.TOK_DROPVIEW:\n      analyzeDropTable(ast, true);\n      break;\n    case HiveParser.TOK_ALTERVIEW_PROPERTIES:\n      analyzeAlterTableProps(ast, true, false);\n      break;\n    case HiveParser.TOK_DROPVIEW_PROPERTIES:\n      analyzeAlterTableProps(ast, true, true);\n      break;\n    case HiveParser.TOK_ALTERVIEW_ADDPARTS:\n      // for ALTER VIEW ADD PARTITION, we wrapped the ADD to discriminate\n      // view from table; unwrap it now\n      analyzeAlterTableAddParts((ASTNode) ast.getChild(0), true);\n      break;\n    case HiveParser.TOK_ALTERVIEW_DROPPARTS:\n      // for ALTER VIEW DROP PARTITION, we wrapped the DROP to discriminate\n      // view from table; unwrap it now\n      analyzeAlterTableDropParts((ASTNode) ast.getChild(0), true);\n      break;\n    case HiveParser.TOK_ALTERVIEW_RENAME:\n      // for ALTER VIEW RENAME, we wrapped the RENAME to discriminate\n      // view from table; unwrap it now\n      analyzeAlterTableRename(((ASTNode) ast.getChild(0)), true);\n      break;\n    case HiveParser.TOK_ALTERTABLE_RENAME:\n      analyzeAlterTableRename(ast, false);\n      break;\n    case HiveParser.TOK_ALTERTABLE_TOUCH:\n      analyzeAlterTableTouch(ast);\n      break;\n    case HiveParser.TOK_ALTERTABLE_ARCHIVE:\n      analyzeAlterTableArchive(ast, false);\n      break;\n    case HiveParser.TOK_ALTERTABLE_UNARCHIVE:\n      analyzeAlterTableArchive(ast, true);\n      break;\n    case HiveParser.TOK_ALTERTABLE_ADDCOLS:\n      analyzeAlterTableModifyCols(ast, AlterTableTypes.ADDCOLS);\n      break;\n    case HiveParser.TOK_ALTERTABLE_REPLACECOLS:\n      analyzeAlterTableModifyCols(ast, AlterTableTypes.REPLACECOLS);\n      break;\n    case HiveParser.TOK_ALTERTABLE_RENAMECOL:\n      analyzeAlterTableRenameCol(ast);\n      break;\n    case HiveParser.TOK_ALTERTABLE_ADDPARTS:\n      analyzeAlterTableAddParts(ast, false);\n      break;\n    case HiveParser.TOK_ALTERTABLE_DROPPARTS:\n      analyzeAlterTableDropParts(ast, false);\n      break;\n    case HiveParser.TOK_ALTERTABLE_PARTCOLTYPE:\n      analyzeAlterTablePartColType(ast);\n      break;\n    case HiveParser.TOK_ALTERTABLE_PROPERTIES:\n      analyzeAlterTableProps(ast, false, false);\n      break;\n    case HiveParser.TOK_DROPTABLE_PROPERTIES:\n      analyzeAlterTableProps(ast, false, true);\n      break;\n    case HiveParser.TOK_ALTERINDEX_REBUILD:\n      analyzeAlterIndexRebuild(ast);\n      break;\n    case HiveParser.TOK_ALTERINDEX_PROPERTIES:\n      analyzeAlterIndexProps(ast);\n      break;\n    case HiveParser.TOK_SHOWPARTITIONS:\n      ctx.setResFile(ctx.getLocalTmpPath());\n      analyzeShowPartitions(ast);\n      break;\n    case HiveParser.TOK_SHOW_CREATETABLE:\n      ctx.setResFile(ctx.getLocalTmpPath());\n      analyzeShowCreateTable(ast);\n      break;\n    case HiveParser.TOK_SHOWINDEXES:\n      ctx.setResFile(ctx.getLocalTmpPath());\n      analyzeShowIndexes(ast);\n      break;\n    case HiveParser.TOK_LOCKTABLE:\n      analyzeLockTable(ast);\n      break;\n    case HiveParser.TOK_UNLOCKTABLE:\n      analyzeUnlockTable(ast);\n      break;\n    case HiveParser.TOK_LOCKDB:\n      analyzeLockDatabase(ast);\n      break;\n    case HiveParser.TOK_UNLOCKDB:\n      analyzeUnlockDatabase(ast);\n      break;\n    case HiveParser.TOK_CREATEDATABASE:\n      analyzeCreateDatabase(ast);\n      break;\n    case HiveParser.TOK_DROPDATABASE:\n      analyzeDropDatabase(ast);\n      break;\n    case HiveParser.TOK_SWITCHDATABASE:\n      analyzeSwitchDatabase(ast);\n      break;\n    case HiveParser.TOK_ALTERDATABASE_PROPERTIES:\n      analyzeAlterDatabase(ast);\n      break;\n    case HiveParser.TOK_CREATEROLE:\n      analyzeCreateRole(ast);\n      break;\n    case HiveParser.TOK_DROPROLE:\n      analyzeDropRole(ast);\n      break;\n    case HiveParser.TOK_SHOW_ROLE_GRANT:\n      ctx.setResFile(ctx.getLocalTmpPath());\n      analyzeShowRoleGrant(ast);\n      break;\n    case HiveParser.TOK_SHOW_ROLES:\n      ctx.setResFile(ctx.getLocalTmpPath());\n      analyzeShowRoles(ast);\n      break;\n    case HiveParser.TOK_GRANT_ROLE:\n      analyzeGrantRevokeRole(true, ast);\n      break;\n    case HiveParser.TOK_REVOKE_ROLE:\n      analyzeGrantRevokeRole(false, ast);\n      break;\n    case HiveParser.TOK_GRANT:\n      analyzeGrant(ast);\n      break;\n    case HiveParser.TOK_SHOW_GRANT:\n      ctx.setResFile(ctx.getLocalTmpPath());\n      analyzeShowGrant(ast);\n      break;\n    case HiveParser.TOK_REVOKE:\n      analyzeRevoke(ast);\n      break;\n    case HiveParser.TOK_ALTERTABLE_SKEWED:\n      analyzeAltertableSkewedby(ast);\n      break;\n   case HiveParser.TOK_EXCHANGEPARTITION:\n      analyzeExchangePartition(ast);\n      break;\n    default:\n      throw new SemanticException(\"Unsupported command.\");\n    }\n  }"
            },
            "hcatalog.core.src.main.java.org.apache.hcatalog.cli.SemanticAnalysis.HCatSemanticAnalyzer.preAnalyze": {
                "code_before_change": "  public ASTNode preAnalyze(HiveSemanticAnalyzerHookContext context, ASTNode ast)\n    throws SemanticException {\n\n    this.ast = ast;\n    switch (ast.getToken().getType()) {\n\n    // HCat wants to intercept following tokens and special-handle them.\n    case HiveParser.TOK_CREATETABLE:\n      hook = new CreateTableHook();\n      return hook.preAnalyze(context, ast);\n\n    case HiveParser.TOK_CREATEDATABASE:\n      hook = new CreateDatabaseHook();\n      return hook.preAnalyze(context, ast);\n\n    case HiveParser.TOK_ALTERTABLE_PARTITION:\n      if (((ASTNode) ast.getChild(1)).getToken().getType() == HiveParser.TOK_ALTERTABLE_FILEFORMAT) {\n        return ast;\n      } else if (((ASTNode) ast.getChild(1)).getToken().getType() == HiveParser.TOK_ALTERTABLE_ALTERPARTS_MERGEFILES) {\n        // unsupported\n        throw new SemanticException(\"Operation not supported.\");\n      } else {\n        return ast;\n      }\n\n      // HCat will allow these operations to be performed.\n      // Database DDL\n    case HiveParser.TOK_SHOWDATABASES:\n    case HiveParser.TOK_DROPDATABASE:\n    case HiveParser.TOK_SWITCHDATABASE:\n    case HiveParser.TOK_DESCDATABASE:\n    case HiveParser.TOK_ALTERDATABASE_PROPERTIES:\n\n      // Index DDL\n    case HiveParser.TOK_ALTERINDEX_PROPERTIES:\n    case HiveParser.TOK_CREATEINDEX:\n    case HiveParser.TOK_DROPINDEX:\n    case HiveParser.TOK_SHOWINDEXES:\n\n      // View DDL\n      // \"alter view add partition\" does not work because of the nature of implementation\n      // of the DDL in hive. Hive will internally invoke another Driver on the select statement,\n      // and HCat does not let \"select\" statement through. I cannot find a way to get around it\n      // without modifying hive code. So just leave it unsupported.\n      //case HiveParser.TOK_ALTERVIEW_ADDPARTS:\n    case HiveParser.TOK_ALTERVIEW_DROPPARTS:\n    case HiveParser.TOK_ALTERVIEW_PROPERTIES:\n    case HiveParser.TOK_ALTERVIEW_RENAME:\n    case HiveParser.TOK_CREATEVIEW:\n    case HiveParser.TOK_DROPVIEW:\n\n      // Authorization DDL\n    case HiveParser.TOK_CREATEROLE:\n    case HiveParser.TOK_DROPROLE:\n    case HiveParser.TOK_GRANT_ROLE:\n    case HiveParser.TOK_GRANT_WITH_OPTION:\n    case HiveParser.TOK_GRANT:\n    case HiveParser.TOK_REVOKE_ROLE:\n    case HiveParser.TOK_REVOKE:\n    case HiveParser.TOK_SHOW_GRANT:\n    case HiveParser.TOK_SHOW_ROLE_GRANT:\n\n      // Misc DDL\n    case HiveParser.TOK_LOCKTABLE:\n    case HiveParser.TOK_UNLOCKTABLE:\n    case HiveParser.TOK_SHOWLOCKS:\n    case HiveParser.TOK_DESCFUNCTION:\n    case HiveParser.TOK_SHOWFUNCTIONS:\n    case HiveParser.TOK_EXPLAIN:\n\n      // Table DDL\n    case HiveParser.TOK_ALTERTABLE_ADDPARTS:\n    case HiveParser.TOK_ALTERTABLE_ADDCOLS:\n    case HiveParser.TOK_ALTERTABLE_CHANGECOL_AFTER_POSITION:\n    case HiveParser.TOK_ALTERTABLE_SERDEPROPERTIES:\n    case HiveParser.TOK_ALTERTABLE_CLUSTER_SORT:\n    case HiveParser.TOK_ALTERTABLE_DROPPARTS:\n    case HiveParser.TOK_ALTERTABLE_PROPERTIES:\n    case HiveParser.TOK_ALTERTABLE_RENAME:\n    case HiveParser.TOK_ALTERTABLE_RENAMECOL:\n    case HiveParser.TOK_ALTERTABLE_REPLACECOLS:\n    case HiveParser.TOK_ALTERTABLE_SERIALIZER:\n    case HiveParser.TOK_ALTERTABLE_TOUCH:\n    case HiveParser.TOK_DESCTABLE:\n    case HiveParser.TOK_DROPTABLE:\n    case HiveParser.TOK_SHOW_TABLESTATUS:\n    case HiveParser.TOK_SHOWPARTITIONS:\n    case HiveParser.TOK_SHOWTABLES:\n      return ast;\n\n    // In all other cases, throw an exception. Its a white-list of allowed operations.\n    default:\n      throw new SemanticException(\"Operation not supported.\");\n\n    }\n  }",
                "code_after_change": "  public ASTNode preAnalyze(HiveSemanticAnalyzerHookContext context, ASTNode ast)\n    throws SemanticException {\n\n    this.ast = ast;\n    switch (ast.getToken().getType()) {\n\n    // HCat wants to intercept following tokens and special-handle them.\n    case HiveParser.TOK_CREATETABLE:\n      hook = new CreateTableHook();\n      return hook.preAnalyze(context, ast);\n\n    case HiveParser.TOK_CREATEDATABASE:\n      hook = new CreateDatabaseHook();\n      return hook.preAnalyze(context, ast);\n\n    case HiveParser.TOK_ALTERTABLE_PARTITION:\n      if (((ASTNode) ast.getChild(1)).getToken().getType() == HiveParser.TOK_ALTERTABLE_FILEFORMAT) {\n        return ast;\n      } else if (((ASTNode) ast.getChild(1)).getToken().getType() == HiveParser.TOK_ALTERTABLE_MERGEFILES) {\n        // unsupported\n        throw new SemanticException(\"Operation not supported.\");\n      } else {\n        return ast;\n      }\n\n      // HCat will allow these operations to be performed.\n      // Database DDL\n    case HiveParser.TOK_SHOWDATABASES:\n    case HiveParser.TOK_DROPDATABASE:\n    case HiveParser.TOK_SWITCHDATABASE:\n    case HiveParser.TOK_DESCDATABASE:\n    case HiveParser.TOK_ALTERDATABASE_PROPERTIES:\n\n      // Index DDL\n    case HiveParser.TOK_ALTERINDEX_PROPERTIES:\n    case HiveParser.TOK_CREATEINDEX:\n    case HiveParser.TOK_DROPINDEX:\n    case HiveParser.TOK_SHOWINDEXES:\n\n      // View DDL\n      // \"alter view add partition\" does not work because of the nature of implementation\n      // of the DDL in hive. Hive will internally invoke another Driver on the select statement,\n      // and HCat does not let \"select\" statement through. I cannot find a way to get around it\n      // without modifying hive code. So just leave it unsupported.\n      //case HiveParser.TOK_ALTERVIEW_ADDPARTS:\n    case HiveParser.TOK_ALTERVIEW_DROPPARTS:\n    case HiveParser.TOK_ALTERVIEW_PROPERTIES:\n    case HiveParser.TOK_ALTERVIEW_RENAME:\n    case HiveParser.TOK_CREATEVIEW:\n    case HiveParser.TOK_DROPVIEW:\n\n      // Authorization DDL\n    case HiveParser.TOK_CREATEROLE:\n    case HiveParser.TOK_DROPROLE:\n    case HiveParser.TOK_GRANT_ROLE:\n    case HiveParser.TOK_GRANT_WITH_OPTION:\n    case HiveParser.TOK_GRANT:\n    case HiveParser.TOK_REVOKE_ROLE:\n    case HiveParser.TOK_REVOKE:\n    case HiveParser.TOK_SHOW_GRANT:\n    case HiveParser.TOK_SHOW_ROLE_GRANT:\n\n      // Misc DDL\n    case HiveParser.TOK_LOCKTABLE:\n    case HiveParser.TOK_UNLOCKTABLE:\n    case HiveParser.TOK_SHOWLOCKS:\n    case HiveParser.TOK_DESCFUNCTION:\n    case HiveParser.TOK_SHOWFUNCTIONS:\n    case HiveParser.TOK_EXPLAIN:\n\n      // Table DDL\n    case HiveParser.TOK_ALTERTABLE_ADDPARTS:\n    case HiveParser.TOK_ALTERTABLE_ADDCOLS:\n    case HiveParser.TOK_ALTERTABLE_CHANGECOL_AFTER_POSITION:\n    case HiveParser.TOK_ALTERTABLE_SERDEPROPERTIES:\n    case HiveParser.TOK_ALTERTABLE_CLUSTER_SORT:\n    case HiveParser.TOK_ALTERTABLE_DROPPARTS:\n    case HiveParser.TOK_ALTERTABLE_PROPERTIES:\n    case HiveParser.TOK_ALTERTABLE_RENAME:\n    case HiveParser.TOK_ALTERTABLE_RENAMECOL:\n    case HiveParser.TOK_ALTERTABLE_REPLACECOLS:\n    case HiveParser.TOK_ALTERTABLE_SERIALIZER:\n    case HiveParser.TOK_ALTERTABLE_TOUCH:\n    case HiveParser.TOK_DESCTABLE:\n    case HiveParser.TOK_DROPTABLE:\n    case HiveParser.TOK_SHOW_TABLESTATUS:\n    case HiveParser.TOK_SHOWPARTITIONS:\n    case HiveParser.TOK_SHOWTABLES:\n      return ast;\n\n    // In all other cases, throw an exception. Its a white-list of allowed operations.\n    default:\n      throw new SemanticException(\"Operation not supported.\");\n\n    }\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzerFactory.get": {
                "code_before_change": "  public static BaseSemanticAnalyzer get(HiveConf conf, ASTNode tree)\n      throws SemanticException {\n    if (tree.getToken() == null) {\n      throw new RuntimeException(\"Empty Syntax Tree\");\n    } else {\n      setSessionCommandType(commandType.get(tree.getToken().getType()));\n\n      switch (tree.getToken().getType()) {\n      case HiveParser.TOK_EXPLAIN:\n        return new ExplainSemanticAnalyzer(conf);\n      case HiveParser.TOK_LOAD:\n        return new LoadSemanticAnalyzer(conf);\n      case HiveParser.TOK_EXPORT:\n        return new ExportSemanticAnalyzer(conf);\n      case HiveParser.TOK_IMPORT:\n        return new ImportSemanticAnalyzer(conf);\n      case HiveParser.TOK_CREATEDATABASE:\n      case HiveParser.TOK_DROPDATABASE:\n      case HiveParser.TOK_SWITCHDATABASE:\n      case HiveParser.TOK_DROPTABLE:\n      case HiveParser.TOK_DROPVIEW:\n      case HiveParser.TOK_DESCDATABASE:\n      case HiveParser.TOK_DESCTABLE:\n      case HiveParser.TOK_DESCFUNCTION:\n      case HiveParser.TOK_MSCK:\n      case HiveParser.TOK_ALTERTABLE_ADDCOLS:\n      case HiveParser.TOK_ALTERTABLE_RENAMECOL:\n      case HiveParser.TOK_ALTERTABLE_REPLACECOLS:\n      case HiveParser.TOK_ALTERTABLE_RENAME:\n      case HiveParser.TOK_ALTERTABLE_DROPPARTS:\n      case HiveParser.TOK_ALTERTABLE_ADDPARTS:\n      case HiveParser.TOK_ALTERTABLE_PROPERTIES:\n      case HiveParser.TOK_DROPTABLE_PROPERTIES:\n      case HiveParser.TOK_ALTERTABLE_SERIALIZER:\n      case HiveParser.TOK_ALTERTABLE_SERDEPROPERTIES:\n      case HiveParser.TOK_ALTERINDEX_REBUILD:\n      case HiveParser.TOK_ALTERINDEX_PROPERTIES:\n      case HiveParser.TOK_ALTERVIEW_PROPERTIES:\n      case HiveParser.TOK_DROPVIEW_PROPERTIES:\n      case HiveParser.TOK_ALTERVIEW_ADDPARTS:\n      case HiveParser.TOK_ALTERVIEW_DROPPARTS:\n      case HiveParser.TOK_ALTERVIEW_RENAME:\n      case HiveParser.TOK_SHOWDATABASES:\n      case HiveParser.TOK_SHOWTABLES:\n      case HiveParser.TOK_SHOWCOLUMNS:\n      case HiveParser.TOK_SHOW_TABLESTATUS:\n      case HiveParser.TOK_SHOW_TBLPROPERTIES:\n      case HiveParser.TOK_SHOW_CREATETABLE:\n      case HiveParser.TOK_SHOWFUNCTIONS:\n      case HiveParser.TOK_SHOWPARTITIONS:\n      case HiveParser.TOK_SHOWINDEXES:\n      case HiveParser.TOK_SHOWLOCKS:\n      case HiveParser.TOK_SHOWDBLOCKS:\n      case HiveParser.TOK_CREATEINDEX:\n      case HiveParser.TOK_DROPINDEX:\n      case HiveParser.TOK_ALTERTABLE_CLUSTER_SORT:\n      case HiveParser.TOK_ALTERTABLE_TOUCH:\n      case HiveParser.TOK_ALTERTABLE_ARCHIVE:\n      case HiveParser.TOK_ALTERTABLE_UNARCHIVE:\n      case HiveParser.TOK_ALTERTABLE_ALTERPARTS:\n      case HiveParser.TOK_LOCKTABLE:\n      case HiveParser.TOK_UNLOCKTABLE:\n      case HiveParser.TOK_LOCKDB:\n      case HiveParser.TOK_UNLOCKDB:\n      case HiveParser.TOK_CREATEROLE:\n      case HiveParser.TOK_DROPROLE:\n      case HiveParser.TOK_GRANT:\n      case HiveParser.TOK_REVOKE:\n      case HiveParser.TOK_SHOW_GRANT:\n      case HiveParser.TOK_GRANT_ROLE:\n      case HiveParser.TOK_REVOKE_ROLE:\n      case HiveParser.TOK_SHOW_ROLE_GRANT:\n      case HiveParser.TOK_SHOW_ROLES:\n      case HiveParser.TOK_ALTERDATABASE_PROPERTIES:\n      case HiveParser.TOK_ALTERTABLE_SKEWED:\n      case HiveParser.TOK_TRUNCATETABLE:\n      case HiveParser.TOK_EXCHANGEPARTITION:\n        return new DDLSemanticAnalyzer(conf);\n      case HiveParser.TOK_ALTERTABLE_PARTITION:\n        HiveOperation commandType = null;\n        Integer type = ((ASTNode) tree.getChild(1)).getToken().getType();\n        if (tree.getChild(0).getChildCount() > 1) {\n          commandType = tablePartitionCommandType.get(type)[1];\n        } else {\n          commandType = tablePartitionCommandType.get(type)[0];\n        }\n        setSessionCommandType(commandType);\n        return new DDLSemanticAnalyzer(conf);\n\n      case HiveParser.TOK_CREATEFUNCTION:\n      case HiveParser.TOK_DROPFUNCTION:\n        return new FunctionSemanticAnalyzer(conf);\n\n      case HiveParser.TOK_ANALYZE:\n        return new ColumnStatsSemanticAnalyzer(conf, tree);\n\n      case HiveParser.TOK_CREATEMACRO:\n      case HiveParser.TOK_DROPMACRO:\n        return new MacroSemanticAnalyzer(conf);\n      default:\n        return new SemanticAnalyzer(conf);\n      }\n    }\n  }",
                "code_after_change": "  public static BaseSemanticAnalyzer get(HiveConf conf, ASTNode tree)\n      throws SemanticException {\n    if (tree.getToken() == null) {\n      throw new RuntimeException(\"Empty Syntax Tree\");\n    } else {\n      setSessionCommandType(commandType.get(tree.getToken().getType()));\n\n      switch (tree.getToken().getType()) {\n      case HiveParser.TOK_EXPLAIN:\n        return new ExplainSemanticAnalyzer(conf);\n      case HiveParser.TOK_LOAD:\n        return new LoadSemanticAnalyzer(conf);\n      case HiveParser.TOK_EXPORT:\n        return new ExportSemanticAnalyzer(conf);\n      case HiveParser.TOK_IMPORT:\n        return new ImportSemanticAnalyzer(conf);\n      case HiveParser.TOK_CREATEDATABASE:\n      case HiveParser.TOK_DROPDATABASE:\n      case HiveParser.TOK_SWITCHDATABASE:\n      case HiveParser.TOK_DROPTABLE:\n      case HiveParser.TOK_DROPVIEW:\n      case HiveParser.TOK_DESCDATABASE:\n      case HiveParser.TOK_DESCTABLE:\n      case HiveParser.TOK_DESCFUNCTION:\n      case HiveParser.TOK_MSCK:\n      case HiveParser.TOK_ALTERTABLE_ADDCOLS:\n      case HiveParser.TOK_ALTERTABLE_RENAMECOL:\n      case HiveParser.TOK_ALTERTABLE_REPLACECOLS:\n      case HiveParser.TOK_ALTERTABLE_RENAME:\n      case HiveParser.TOK_ALTERTABLE_DROPPARTS:\n      case HiveParser.TOK_ALTERTABLE_ADDPARTS:\n      case HiveParser.TOK_ALTERTABLE_PROPERTIES:\n      case HiveParser.TOK_DROPTABLE_PROPERTIES:\n      case HiveParser.TOK_ALTERTABLE_SERIALIZER:\n      case HiveParser.TOK_ALTERTABLE_SERDEPROPERTIES:\n      case HiveParser.TOK_ALTERTABLE_PARTCOLTYPE:\n      case HiveParser.TOK_ALTERINDEX_REBUILD:\n      case HiveParser.TOK_ALTERINDEX_PROPERTIES:\n      case HiveParser.TOK_ALTERVIEW_PROPERTIES:\n      case HiveParser.TOK_DROPVIEW_PROPERTIES:\n      case HiveParser.TOK_ALTERVIEW_ADDPARTS:\n      case HiveParser.TOK_ALTERVIEW_DROPPARTS:\n      case HiveParser.TOK_ALTERVIEW_RENAME:\n      case HiveParser.TOK_SHOWDATABASES:\n      case HiveParser.TOK_SHOWTABLES:\n      case HiveParser.TOK_SHOWCOLUMNS:\n      case HiveParser.TOK_SHOW_TABLESTATUS:\n      case HiveParser.TOK_SHOW_TBLPROPERTIES:\n      case HiveParser.TOK_SHOW_CREATETABLE:\n      case HiveParser.TOK_SHOWFUNCTIONS:\n      case HiveParser.TOK_SHOWPARTITIONS:\n      case HiveParser.TOK_SHOWINDEXES:\n      case HiveParser.TOK_SHOWLOCKS:\n      case HiveParser.TOK_SHOWDBLOCKS:\n      case HiveParser.TOK_CREATEINDEX:\n      case HiveParser.TOK_DROPINDEX:\n      case HiveParser.TOK_ALTERTABLE_CLUSTER_SORT:\n      case HiveParser.TOK_ALTERTABLE_TOUCH:\n      case HiveParser.TOK_ALTERTABLE_ARCHIVE:\n      case HiveParser.TOK_ALTERTABLE_UNARCHIVE:\n      case HiveParser.TOK_LOCKTABLE:\n      case HiveParser.TOK_UNLOCKTABLE:\n      case HiveParser.TOK_LOCKDB:\n      case HiveParser.TOK_UNLOCKDB:\n      case HiveParser.TOK_CREATEROLE:\n      case HiveParser.TOK_DROPROLE:\n      case HiveParser.TOK_GRANT:\n      case HiveParser.TOK_REVOKE:\n      case HiveParser.TOK_SHOW_GRANT:\n      case HiveParser.TOK_GRANT_ROLE:\n      case HiveParser.TOK_REVOKE_ROLE:\n      case HiveParser.TOK_SHOW_ROLE_GRANT:\n      case HiveParser.TOK_SHOW_ROLES:\n      case HiveParser.TOK_ALTERDATABASE_PROPERTIES:\n      case HiveParser.TOK_ALTERTABLE_SKEWED:\n      case HiveParser.TOK_TRUNCATETABLE:\n      case HiveParser.TOK_EXCHANGEPARTITION:\n        return new DDLSemanticAnalyzer(conf);\n      case HiveParser.TOK_ALTERTABLE_PARTITION:\n        HiveOperation commandType = null;\n        Integer type = ((ASTNode) tree.getChild(1)).getToken().getType();\n        if (tree.getChild(0).getChildCount() > 1) {\n          commandType = tablePartitionCommandType.get(type)[1];\n        } else {\n          commandType = tablePartitionCommandType.get(type)[0];\n        }\n        setSessionCommandType(commandType);\n        return new DDLSemanticAnalyzer(conf);\n\n      case HiveParser.TOK_CREATEFUNCTION:\n      case HiveParser.TOK_DROPFUNCTION:\n        return new FunctionSemanticAnalyzer(conf);\n\n      case HiveParser.TOK_ANALYZE:\n        return new ColumnStatsSemanticAnalyzer(conf, tree);\n\n      case HiveParser.TOK_CREATEMACRO:\n      case HiveParser.TOK_DROPMACRO:\n        return new MacroSemanticAnalyzer(conf);\n      default:\n        return new SemanticAnalyzer(conf);\n      }\n    }\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report precisely identifies the root cause by mentioning the NullPointerException in the 'doAuthorization' method, which is one of the ground truth methods. However, there is no fix suggestion provided in the bug report. The problem location is also precisely identified as the stack trace includes the 'doAuthorization' method, which is part of the ground truth methods. There is no wrong information in the bug report as all the details are consistent with the context of the bug."
        }
    },
    {
        "filename": "HIVE-15309.json",
        "code_diff": {
            "metastore.src.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.acquireLock": {
                "code_before_change": "  public LockHandle acquireLock(String key) throws MetaException {\n    /**\n     * The implementation here is a bit kludgey but done so that code exercised by unit tests\n     * (which run against Derby which has no support for select for update) is as similar to\n     * production code as possible.\n     * In particular, with Derby we always run in a single process with a single metastore and\n     * the absence of For Update is handled via a Semaphore.  The later would strictly speaking\n     * make the SQL statments below unnecessary (for Derby), but then they would not be tested.\n     */\n    Connection dbConn = null;\n    Statement stmt = null;\n    ResultSet rs = null;\n    try {\n      try {\n        String sqlStmt = sqlGenerator.addForUpdateClause(\"select MT_COMMENT from AUX_TABLE where MT_KEY1=\" + quoteString(key) + \" and MT_KEY2=0\");\n        lockInternal();\n        dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);\n        stmt = dbConn.createStatement();\n        if(LOG.isDebugEnabled()) {\n          LOG.debug(\"About to execute SQL: \" + sqlStmt);\n        }\n        rs = stmt.executeQuery(sqlStmt);\n        if (!rs.next()) {\n          close(rs);\n          try {\n            stmt.executeUpdate(\"insert into AUX_TABLE(MT_KEY1,MT_KEY2) values(\" + quoteString(key) + \", 0)\");\n            dbConn.commit();\n          } catch (SQLException ex) {\n            if (!isDuplicateKeyError(ex)) {\n              throw new RuntimeException(\"Unable to lock \" + quoteString(key) + \" due to: \" + getMessage(ex), ex);\n            }\n          }\n          rs = stmt.executeQuery(sqlStmt);\n          if (!rs.next()) {\n            throw new IllegalStateException(\"Unable to lock \" + quoteString(key) + \".  Expected row in AUX_TABLE is missing.\");\n          }\n        }\n        Semaphore derbySemaphore = null;\n        if(dbProduct == DatabaseProduct.DERBY) {\n          derbyKey2Lock.putIfAbsent(key, new Semaphore(1));\n          derbySemaphore =  derbyKey2Lock.get(key);\n          derbySemaphore.acquire();\n        }\n        LOG.info(quoteString(key) + \" locked by \" + quoteString(TxnHandler.hostname));\n        //OK, so now we have a lock\n        return new LockHandleImpl(dbConn, stmt, rs, key, derbySemaphore);\n      } catch (SQLException ex) {\n        rollbackDBConn(dbConn);\n        close(rs, stmt, dbConn);\n        checkRetryable(dbConn, ex, \"acquireLock(\" + key + \")\");\n        throw new MetaException(\"Unable to lock \" + quoteString(key) + \" due to: \" + getMessage(ex) + \"; \" + StringUtils.stringifyException(ex));\n      }\n      catch(InterruptedException ex) {\n        rollbackDBConn(dbConn);\n        close(rs, stmt, dbConn);\n        throw new MetaException(\"Unable to lock \" + quoteString(key) + \" due to: \" + ex.getMessage() + StringUtils.stringifyException(ex));\n      }\n      finally {\n        unlockInternal();\n      }\n    }\n    catch(RetryException ex) {\n      return acquireLock(key);\n    }\n  }",
                "code_after_change": "  public LockHandle acquireLock(String key) throws MetaException {\n    /**\n     * The implementation here is a bit kludgey but done so that code exercised by unit tests\n     * (which run against Derby which has no support for select for update) is as similar to\n     * production code as possible.\n     * In particular, with Derby we always run in a single process with a single metastore and\n     * the absence of For Update is handled via a Semaphore.  The later would strictly speaking\n     * make the SQL statements below unnecessary (for Derby), but then they would not be tested.\n     */\n    Connection dbConn = null;\n    Statement stmt = null;\n    ResultSet rs = null;\n    try {\n      try {\n        String sqlStmt = sqlGenerator.addForUpdateClause(\"select MT_COMMENT from AUX_TABLE where MT_KEY1=\" + quoteString(key) + \" and MT_KEY2=0\");\n        lockInternal();\n        dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);\n        stmt = dbConn.createStatement();\n        if(LOG.isDebugEnabled()) {\n          LOG.debug(\"About to execute SQL: \" + sqlStmt);\n        }\n        rs = stmt.executeQuery(sqlStmt);\n        if (!rs.next()) {\n          close(rs);\n          try {\n            stmt.executeUpdate(\"insert into AUX_TABLE(MT_KEY1,MT_KEY2) values(\" + quoteString(key) + \", 0)\");\n            dbConn.commit();\n          } catch (SQLException ex) {\n            if (!isDuplicateKeyError(ex)) {\n              throw new RuntimeException(\"Unable to lock \" + quoteString(key) + \" due to: \" + getMessage(ex), ex);\n            }\n            //if here, it means a concrurrent acquireLock() inserted the 'key'\n\n            //rollback is done for the benefit of Postgres which throws (SQLState=25P02, ErrorCode=0) if\n            //you attempt any stmt in a txn which had an error.\n            dbConn.rollback();\n          }\n          rs = stmt.executeQuery(sqlStmt);\n          if (!rs.next()) {\n            throw new IllegalStateException(\"Unable to lock \" + quoteString(key) + \".  Expected row in AUX_TABLE is missing.\");\n          }\n        }\n        Semaphore derbySemaphore = null;\n        if(dbProduct == DatabaseProduct.DERBY) {\n          derbyKey2Lock.putIfAbsent(key, new Semaphore(1));\n          derbySemaphore =  derbyKey2Lock.get(key);\n          derbySemaphore.acquire();\n        }\n        LOG.debug(quoteString(key) + \" locked by \" + quoteString(TxnHandler.hostname));\n        //OK, so now we have a lock\n        return new LockHandleImpl(dbConn, stmt, rs, key, derbySemaphore);\n      } catch (SQLException ex) {\n        rollbackDBConn(dbConn);\n        close(rs, stmt, dbConn);\n        checkRetryable(dbConn, ex, \"acquireLock(\" + key + \")\");\n        throw new MetaException(\"Unable to lock \" + quoteString(key) + \" due to: \" + getMessage(ex) + \"; \" + StringUtils.stringifyException(ex));\n      }\n      catch(InterruptedException ex) {\n        rollbackDBConn(dbConn);\n        close(rs, stmt, dbConn);\n        throw new MetaException(\"Unable to lock \" + quoteString(key) + \" due to: \" + ex.getMessage() + StringUtils.stringifyException(ex));\n      }\n      finally {\n        unlockInternal();\n      }\n    }\n    catch(RetryException ex) {\n      return acquireLock(key);\n    }\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.releaseLocks": {
                "code_before_change": "    public void releaseLocks() {\n      rollbackDBConn(dbConn);\n      close(rs, stmt, dbConn);\n      if(derbySemaphore != null) {\n        derbySemaphore.release();\n      }\n      for(String key : keys) {\n        LOG.info(quoteString(key) + \" unlocked by \" + quoteString(TxnHandler.hostname));\n      }\n    }",
                "code_after_change": "    public void releaseLocks() {\n      rollbackDBConn(dbConn);\n      close(rs, stmt, dbConn);\n      if(derbySemaphore != null) {\n        derbySemaphore.release();\n      }\n      for(String key : keys) {\n        LOG.debug(quoteString(key) + \" unlocked by \" + quoteString(TxnHandler.hostname));\n      }\n    }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.timeOutLocks": {
                "code_before_change": "  private void timeOutLocks(Connection dbConn, long now) {\n    Statement stmt = null;\n    ResultSet rs = null;\n    try {\n      stmt = dbConn.createStatement();\n      long maxHeartbeatTime = now - timeout;\n      //doing a SELECT first is less efficient but makes it easier to debug things\n      String s = \"select distinct hl_lock_ext_id from HIVE_LOCKS where hl_last_heartbeat < \" +\n        maxHeartbeatTime + \" and hl_txnid = 0\";//when txnid is <> 0, the lock is\n      //associated with a txn and is handled by performTimeOuts()\n      //want to avoid expiring locks for a txn w/o expiring the txn itself\n      List<Long> extLockIDs = new ArrayList<>();\n      rs = stmt.executeQuery(s);\n      while(rs.next()) {\n        extLockIDs.add(rs.getLong(1));\n      }\n      rs.close();\n      dbConn.commit();\n      if(extLockIDs.size() <= 0) {\n        return;\n      }\n\n      List<String> queries = new ArrayList<String>();\n\n      StringBuilder prefix = new StringBuilder();\n      StringBuilder suffix = new StringBuilder();\n\n      //include same hl_last_heartbeat condition in case someone heartbeated since the select\n      prefix.append(\"delete from HIVE_LOCKS where hl_last_heartbeat < \");\n      prefix.append(maxHeartbeatTime);\n      prefix.append(\" and hl_txnid = 0 and \");\n      suffix.append(\"\");\n\n      TxnUtils.buildQueryWithINClause(conf, queries, prefix, suffix, extLockIDs, \"hl_lock_ext_id\", true, false);\n\n      int deletedLocks = 0;\n      for (String query : queries) {\n        LOG.debug(\"Removing expired locks via: \" + query);\n        deletedLocks += stmt.executeUpdate(query);\n      }\n      if(deletedLocks > 0) {\n        Collections.sort(extLockIDs);////easier to read logs\n        LOG.info(\"Deleted \" + deletedLocks + \" ext locks from HIVE_LOCKS due to timeout (vs. \" +\n            extLockIDs.size() + \" found. List: \" + extLockIDs + \") maxHeartbeatTime=\" + maxHeartbeatTime);\n      }\n      LOG.debug(\"Going to commit\");\n      dbConn.commit();\n    }\n    catch(SQLException ex) {\n      LOG.error(\"Failed to purge timedout locks due to: \" + getMessage(ex), ex);\n    }\n    catch(Exception ex) {\n      LOG.error(\"Failed to purge timedout locks due to: \" + ex.getMessage(), ex);\n    } finally {\n      close(rs);\n      closeStmt(stmt);\n    }\n  }",
                "code_after_change": "  private void timeOutLocks(Connection dbConn, long now) {\n    Statement stmt = null;\n    ResultSet rs = null;\n    try {\n      stmt = dbConn.createStatement();\n      long maxHeartbeatTime = now - timeout;\n      //doing a SELECT first is less efficient but makes it easier to debug things\n      String s = \"select distinct hl_lock_ext_id from HIVE_LOCKS where hl_last_heartbeat < \" +\n        maxHeartbeatTime + \" and hl_txnid = 0\";//when txnid is <> 0, the lock is\n      //associated with a txn and is handled by performTimeOuts()\n      //want to avoid expiring locks for a txn w/o expiring the txn itself\n      List<Long> extLockIDs = new ArrayList<>();\n      rs = stmt.executeQuery(s);\n      while(rs.next()) {\n        extLockIDs.add(rs.getLong(1));\n      }\n      rs.close();\n      dbConn.commit();\n      if(extLockIDs.size() <= 0) {\n        return;\n      }\n\n      List<String> queries = new ArrayList<String>();\n\n      StringBuilder prefix = new StringBuilder();\n      StringBuilder suffix = new StringBuilder();\n\n      //include same hl_last_heartbeat condition in case someone heartbeated since the select\n      prefix.append(\"delete from HIVE_LOCKS where hl_last_heartbeat < \");\n      prefix.append(maxHeartbeatTime);\n      prefix.append(\" and hl_txnid = 0 and \");\n      suffix.append(\"\");\n\n      TxnUtils.buildQueryWithINClause(conf, queries, prefix, suffix, extLockIDs, \"hl_lock_ext_id\", true, false);\n\n      int deletedLocks = 0;\n      for (String query : queries) {\n        LOG.debug(\"Removing expired locks via: \" + query);\n        deletedLocks += stmt.executeUpdate(query);\n      }\n      if(deletedLocks > 0) {\n        Collections.sort(extLockIDs);//easier to read logs\n        LOG.info(\"Deleted \" + deletedLocks + \" int locks from HIVE_LOCKS due to timeout (\" +\n          \"HL_LOCK_EXT_ID list:  \" + extLockIDs + \") maxHeartbeatTime=\" + maxHeartbeatTime);\n      }\n      LOG.debug(\"Going to commit\");\n      dbConn.commit();\n    }\n    catch(SQLException ex) {\n      LOG.error(\"Failed to purge timedout locks due to: \" + getMessage(ex), ex);\n    }\n    catch(Exception ex) {\n      LOG.error(\"Failed to purge timedout locks due to: \" + ex.getMessage(), ex);\n    } finally {\n      close(rs);\n      closeStmt(stmt);\n    }\n  }"
            },
            "orc.src.java.org.apache.orc.impl.OrcAcidUtils.getLastFlushLength": {
                "code_before_change": "  public static long getLastFlushLength(FileSystem fs,\n                                        Path deltaFile) throws IOException {\n    Path lengths = getSideFile(deltaFile);\n    long result = Long.MAX_VALUE;\n    try (FSDataInputStream stream = fs.open(lengths)) {\n      result = -1;\n      while (stream.available() > 0) {\n        result = stream.readLong();\n      }\n      return result;\n    } catch (IOException ioe) {\n      return result;\n    }\n  }",
                "code_after_change": "  public static long getLastFlushLength(FileSystem fs,\n                                        Path deltaFile) throws IOException {\n    Path lengths = getSideFile(deltaFile);\n    long result = Long.MAX_VALUE;\n    if(!fs.exists(lengths)) {\n      return result;\n    }\n    try (FSDataInputStream stream = fs.open(lengths)) {\n      result = -1;\n      while (stream.available() > 0) {\n        result = stream.readLong();\n      }\n      return result;\n    } catch (IOException ioe) {\n      return result;\n    }\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Correct",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report precisely identifies the root cause by mentioning 'OrcAcidUtils.getLastFlushLength()' and suggests checking for file existence, which matches the developer's fix. The problem location is also precisely identified as it directly mentions the method 'OrcAcidUtils.getLastFlushLength()' from the ground truth list. There is no wrong information in the bug report as all details are relevant to the context of the bug."
        }
    },
    {
        "filename": "HIVE-10808.json",
        "code_diff": {
            "serde.src.java.org.apache.hadoop.hive.serde2.NullStructSerDe.getTypeName": {
                "code_before_change": "    public String getTypeName() {\n      return \"null\";\n    }",
                "code_after_change": "    public String getTypeName() {\n      return \"null\";\n    }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report mentions a ClassCastException related to NullStructSerDeObjectInspector, which is in the same stack trace context as the ground truth method 'serde.src.java.org.apache.hadoop.hive.serde2.NullStructSerDe.getTypeName'. However, it does not precisely identify the root cause at the ground truth method. There is no fix suggestion provided in the bug report. The problem location is partially identified as it is in the shared stack trace context with the ground truth method. There is no wrong information in the bug report as it correctly describes the exception and its context."
        }
    },
    {
        "filename": "HIVE-18429.json",
        "code_diff": {
            "ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.commitJob": {
                "code_before_change": "    public void commitJob(JobContext context) throws IOException {\n      JobConf conf = ShimLoader.getHadoopShims().getJobConf(context);\n      Path tmpLocation = new Path(conf.get(TMP_LOCATION));//this contains base_xxx or delta_xxx_yyy\n      Path finalLocation = new Path(conf.get(FINAL_LOCATION));\n      FileSystem fs = tmpLocation.getFileSystem(conf);\n      LOG.debug(\"Moving contents of \" + tmpLocation.toString() + \" to \" +\n          finalLocation.toString());\n\n      FileStatus[] contents = fs.listStatus(tmpLocation);//expect 1 base or delta dir in this list\n      //we have MIN_TXN, MAX_TXN and IS_MAJOR in JobConf so we could figure out exactly what the dir\n      //name is that we want to rename; leave it for another day\n      for (int i = 0; i < contents.length; i++) {\n        Path newPath = new Path(finalLocation, contents[i].getPath().getName());\n        fs.rename(contents[i].getPath(), newPath);\n      }\n      fs.delete(tmpLocation, true);\n    }",
                "code_after_change": "    public void commitJob(JobContext context) throws IOException {\n      JobConf conf = ShimLoader.getHadoopShims().getJobConf(context);\n      Path tmpLocation = new Path(conf.get(TMP_LOCATION));//this contains base_xxx or delta_xxx_yyy\n      Path finalLocation = new Path(conf.get(FINAL_LOCATION));\n      FileSystem fs = tmpLocation.getFileSystem(conf);\n      LOG.debug(\"Moving contents of \" + tmpLocation.toString() + \" to \" +\n          finalLocation.toString());\n      if(!fs.exists(tmpLocation)) {\n        /**\n         * No 'tmpLocation' may happen if job generated created 0 splits, which happens if all\n         * input delta and/or base files were empty or had\n         * only {@link org.apache.orc.impl.OrcAcidUtils#getSideFile(Path)} files.\n         * So make sure the new base/delta is created.\n         */\n        AcidOutputFormat.Options options = new AcidOutputFormat.Options(conf)\n            .writingBase(conf.getBoolean(IS_MAJOR, false))\n            .isCompressed(conf.getBoolean(IS_COMPRESSED, false))\n            .minimumTransactionId(conf.getLong(MIN_TXN, Long.MAX_VALUE))\n            .maximumTransactionId(conf.getLong(MAX_TXN, Long.MIN_VALUE))\n            .bucket(0)\n            .statementId(-1);\n        Path newDeltaDir = AcidUtils.createFilename(finalLocation, options).getParent();\n        LOG.info(context.getJobID() + \": \" + tmpLocation +\n            \" not found.  Assuming 0 splits.  Creating \" + newDeltaDir);\n        fs.mkdirs(newDeltaDir);\n        return;\n      }\n      FileStatus[] contents = fs.listStatus(tmpLocation);//expect 1 base or delta dir in this list\n      //we have MIN_TXN, MAX_TXN and IS_MAJOR in JobConf so we could figure out exactly what the dir\n      //name is that we want to rename; leave it for another day\n      for (FileStatus fileStatus : contents) {\n        Path newPath = new Path(finalLocation, fileStatus.getPath().getName());\n        fs.rename(fileStatus.getPath(), newPath);\n      }\n      fs.delete(tmpLocation, true);\n    }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.run": {
                "code_before_change": "  void run(HiveConf conf, String jobName, Table t, StorageDescriptor sd,\n           ValidTxnList txns, CompactionInfo ci, Worker.StatsUpdater su, TxnStore txnHandler) throws IOException {\n\n    if(conf.getBoolVar(HiveConf.ConfVars.HIVE_IN_TEST) && conf.getBoolVar(HiveConf.ConfVars.HIVETESTMODEFAILCOMPACTION)) {\n      throw new RuntimeException(HiveConf.ConfVars.HIVETESTMODEFAILCOMPACTION.name() + \"=true\");\n    }\n\n    // For MM tables we don't need to launch MR jobs as there is no compaction needed.\n    // We just need to delete the directories for aborted transactions.\n    if (AcidUtils.isInsertOnlyTable(t.getParameters())) {\n      LOG.debug(\"Going to delete directories for aborted transactions for MM table \"\n          + t.getDbName() + \".\" + t.getTableName());\n      removeFiles(conf, sd.getLocation(), txns, t);\n      return;\n    }\n\n    JobConf job = createBaseJobConf(conf, jobName, t, sd, txns, ci);\n\n    // Figure out and encode what files we need to read.  We do this here (rather than in\n    // getSplits below) because as part of this we discover our minimum and maximum transactions,\n    // and discovering that in getSplits is too late as we then have no way to pass it to our\n    // mapper.\n\n    AcidUtils.Directory dir = AcidUtils.getAcidState(new Path(sd.getLocation()), conf, txns, false, true);\n    List<AcidUtils.ParsedDelta> parsedDeltas = dir.getCurrentDirectories();\n    int maxDeltastoHandle = conf.getIntVar(HiveConf.ConfVars.COMPACTOR_MAX_NUM_DELTA);\n    if(parsedDeltas.size() > maxDeltastoHandle) {\n      /**\n       * if here, that means we have very high number of delta files.  This may be sign of a temporary\n       * glitch or a real issue.  For example, if transaction batch size or transaction size is set too\n       * low for the event flow rate in Streaming API, it may generate lots of delta files very\n       * quickly.  Another possibility is that Compaction is repeatedly failing and not actually compacting.\n       * Thus, force N minor compactions first to reduce number of deltas and then follow up with\n       * the compaction actually requested in {@link ci} which now needs to compact a lot fewer deltas\n       */\n      LOG.warn(parsedDeltas.size() + \" delta files found for \" + ci.getFullPartitionName()\n        + \" located at \" + sd.getLocation() + \"! This is likely a sign of misconfiguration, \" +\n        \"especially if this message repeats.  Check that compaction is running properly.  Check for any \" +\n        \"runaway/mis-configured process writing to ACID tables, especially using Streaming Ingest API.\");\n      int numMinorCompactions = parsedDeltas.size() / maxDeltastoHandle;\n      for(int jobSubId = 0; jobSubId < numMinorCompactions; jobSubId++) {\n        JobConf jobMinorCompact = createBaseJobConf(conf, jobName + \"_\" + jobSubId, t, sd, txns, ci);\n        launchCompactionJob(jobMinorCompact,\n          null, CompactionType.MINOR, null,\n          parsedDeltas.subList(jobSubId * maxDeltastoHandle, (jobSubId + 1) * maxDeltastoHandle),\n          maxDeltastoHandle, -1, conf, txnHandler, ci.id, jobName);\n      }\n      //now recompute state since we've done minor compactions and have different 'best' set of deltas\n      dir = AcidUtils.getAcidState(new Path(sd.getLocation()), conf, txns);\n    }\n\n    StringableList dirsToSearch = new StringableList();\n    Path baseDir = null;\n    if (ci.isMajorCompaction()) {\n      // There may not be a base dir if the partition was empty before inserts or if this\n      // partition is just now being converted to ACID.\n      baseDir = dir.getBaseDirectory();\n      if (baseDir == null) {\n        List<HdfsFileStatusWithId> originalFiles = dir.getOriginalFiles();\n        if (!(originalFiles == null) && !(originalFiles.size() == 0)) {\n          // There are original format files\n          for (HdfsFileStatusWithId stat : originalFiles) {\n            Path path = stat.getFileStatus().getPath();\n            //note that originalFiles are all original files recursively not dirs\n            dirsToSearch.add(path);\n            LOG.debug(\"Adding original file \" + path + \" to dirs to search\");\n          }\n          // Set base to the location so that the input format reads the original files.\n          baseDir = new Path(sd.getLocation());\n        }\n      } else {\n        // add our base to the list of directories to search for files in.\n        LOG.debug(\"Adding base directory \" + baseDir + \" to dirs to search\");\n        dirsToSearch.add(baseDir);\n      }\n    }\n    if (parsedDeltas.size() == 0 && dir.getOriginalFiles().size() == 0) {\n      // Skip compaction if there's no delta files AND there's no original files\n      String minOpenInfo = \".\";\n      if(txns.getMinOpenTxn() != null) {\n        minOpenInfo = \" with min Open \" + JavaUtils.txnIdToString(txns.getMinOpenTxn()) +\n          \".  Compaction cannot compact above this txnid\";\n      }\n      LOG.error(\"No delta files or original files found to compact in \" + sd.getLocation() +\n        \" for compactionId=\" + ci.id + minOpenInfo);\n      return;\n    }\n\n    launchCompactionJob(job, baseDir, ci.type, dirsToSearch, dir.getCurrentDirectories(),\n      dir.getCurrentDirectories().size(), dir.getObsolete().size(), conf, txnHandler, ci.id, jobName);\n\n    su.gatherStats();\n  }",
                "code_after_change": "  void run(HiveConf conf, String jobName, Table t, StorageDescriptor sd,\n           ValidTxnList txns, CompactionInfo ci, Worker.StatsUpdater su, TxnStore txnHandler) throws IOException {\n\n    if(conf.getBoolVar(HiveConf.ConfVars.HIVE_IN_TEST) && conf.getBoolVar(HiveConf.ConfVars.HIVETESTMODEFAILCOMPACTION)) {\n      throw new RuntimeException(HiveConf.ConfVars.HIVETESTMODEFAILCOMPACTION.name() + \"=true\");\n    }\n\n    // For MM tables we don't need to launch MR jobs as there is no compaction needed.\n    // We just need to delete the directories for aborted transactions.\n    if (AcidUtils.isInsertOnlyTable(t.getParameters())) {\n      LOG.debug(\"Going to delete directories for aborted transactions for MM table \"\n          + t.getDbName() + \".\" + t.getTableName());\n      removeFiles(conf, sd.getLocation(), txns, t);\n      return;\n    }\n\n    JobConf job = createBaseJobConf(conf, jobName, t, sd, txns, ci);\n\n    // Figure out and encode what files we need to read.  We do this here (rather than in\n    // getSplits below) because as part of this we discover our minimum and maximum transactions,\n    // and discovering that in getSplits is too late as we then have no way to pass it to our\n    // mapper.\n\n    AcidUtils.Directory dir = AcidUtils.getAcidState(new Path(sd.getLocation()), conf, txns, false, true);\n    List<AcidUtils.ParsedDelta> parsedDeltas = dir.getCurrentDirectories();\n    int maxDeltastoHandle = conf.getIntVar(HiveConf.ConfVars.COMPACTOR_MAX_NUM_DELTA);\n    if(parsedDeltas.size() > maxDeltastoHandle) {\n      /**\n       * if here, that means we have very high number of delta files.  This may be sign of a temporary\n       * glitch or a real issue.  For example, if transaction batch size or transaction size is set too\n       * low for the event flow rate in Streaming API, it may generate lots of delta files very\n       * quickly.  Another possibility is that Compaction is repeatedly failing and not actually compacting.\n       * Thus, force N minor compactions first to reduce number of deltas and then follow up with\n       * the compaction actually requested in {@link ci} which now needs to compact a lot fewer deltas\n       */\n      LOG.warn(parsedDeltas.size() + \" delta files found for \" + ci.getFullPartitionName()\n        + \" located at \" + sd.getLocation() + \"! This is likely a sign of misconfiguration, \" +\n        \"especially if this message repeats.  Check that compaction is running properly.  Check for any \" +\n        \"runaway/mis-configured process writing to ACID tables, especially using Streaming Ingest API.\");\n      int numMinorCompactions = parsedDeltas.size() / maxDeltastoHandle;\n      for(int jobSubId = 0; jobSubId < numMinorCompactions; jobSubId++) {\n        JobConf jobMinorCompact = createBaseJobConf(conf, jobName + \"_\" + jobSubId, t, sd, txns, ci);\n        launchCompactionJob(jobMinorCompact,\n          null, CompactionType.MINOR, null,\n          parsedDeltas.subList(jobSubId * maxDeltastoHandle, (jobSubId + 1) * maxDeltastoHandle),\n          maxDeltastoHandle, -1, conf, txnHandler, ci.id, jobName);\n      }\n      //now recompute state since we've done minor compactions and have different 'best' set of deltas\n      dir = AcidUtils.getAcidState(new Path(sd.getLocation()), conf, txns);\n    }\n\n    StringableList dirsToSearch = new StringableList();\n    Path baseDir = null;\n    if (ci.isMajorCompaction()) {\n      // There may not be a base dir if the partition was empty before inserts or if this\n      // partition is just now being converted to ACID.\n      baseDir = dir.getBaseDirectory();\n      if (baseDir == null) {\n        List<HdfsFileStatusWithId> originalFiles = dir.getOriginalFiles();\n        if (!(originalFiles == null) && !(originalFiles.size() == 0)) {\n          // There are original format files\n          for (HdfsFileStatusWithId stat : originalFiles) {\n            Path path = stat.getFileStatus().getPath();\n            //note that originalFiles are all original files recursively not dirs\n            dirsToSearch.add(path);\n            LOG.debug(\"Adding original file \" + path + \" to dirs to search\");\n          }\n          // Set base to the location so that the input format reads the original files.\n          baseDir = new Path(sd.getLocation());\n        }\n      } else {\n        // add our base to the list of directories to search for files in.\n        LOG.debug(\"Adding base directory \" + baseDir + \" to dirs to search\");\n        dirsToSearch.add(baseDir);\n      }\n    }\n    if (parsedDeltas.size() == 0 && dir.getOriginalFiles().size() == 0) {\n      // Skip compaction if there's no delta files AND there's no original files\n      String minOpenInfo = \".\";\n      if(txns.getMinOpenTxn() != null) {\n        minOpenInfo = \" with min Open \" + JavaUtils.txnIdToString(txns.getMinOpenTxn()) +\n          \".  Compaction cannot compact above this txnid\";\n      }\n      LOG.error(\"No delta files or original files found to compact in \" + sd.getLocation() +\n        \" for compactionId=\" + ci.id + minOpenInfo);\n      return;\n    }\n\n    launchCompactionJob(job, baseDir, ci.type, dirsToSearch, dir.getCurrentDirectories(),\n      dir.getCurrentDirectories().size(), dir.getObsolete().size(), conf, txnHandler, ci.id, jobName);\n\n    su.gatherStats();\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Correct",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report precisely identifies the root cause related to the ground truth method 'CompactorMR.commitJob'. It describes the issue with 'tmpLocation' not being created, which aligns with the changes made in the 'commitJob' method. The fix suggestion is correct as it matches the developer's fix, which involves checking for the existence of 'tmpLocation' and creating a new delta directory if it doesn't exist. The problem location identification is partial because the report mentions 'CompactorOutputCommitted.commitJob()', which is in the same stack trace context as the ground truth method but not the exact method. There is no wrong information in the bug report as all details are relevant to the context of the bug."
        }
    },
    {
        "filename": "HIVE-10776.json",
        "code_diff": {
            "ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.isRegex": {
                "code_before_change": "  static boolean isRegex(String pattern, HiveConf conf) {\n    String qIdSupport = HiveConf.getVar(conf, HiveConf.ConfVars.HIVE_QUOTEDID_SUPPORT);\n    if ( \"column\".equals(qIdSupport)) {\n      return false;\n    }\n    for (int i = 0; i < pattern.length(); i++) {\n      if (!Character.isLetterOrDigit(pattern.charAt(i))\n          && pattern.charAt(i) != '_') {\n        return true;\n      }\n    }\n    return false;\n  }",
                "code_after_change": "  static boolean isRegex(String pattern, HiveConf conf) {\n    String qIdSupport = HiveConf.getVar(conf, HiveConf.ConfVars.HIVE_QUOTEDID_SUPPORT);\n    if ( \"column\".equals(qIdSupport)) {\n      return false;\n    }\n    for (int i = 0; i < pattern.length(); i++) {\n      if (!Character.isLetterOrDigit(pattern.charAt(i))\n          && pattern.charAt(i) != '_') {\n        return true;\n      }\n    }\n    return false;\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.handleInsertStatementSpec": {
                "code_before_change": "  private void handleInsertStatementSpec(List<ExprNodeDesc> col_list, String dest,\n                                         RowResolver out_rwsch, RowResolver inputRR, QB qb,\n                                         ASTNode selExprList) throws SemanticException {\n    //(z,x)\n    List<String> targetTableSchema = qb.getParseInfo().getDestSchemaForClause(dest);//specified in the query\n    if(targetTableSchema == null) {\n      //no insert schema was specified\n      return;\n    }\n    if(targetTableSchema.size() != col_list.size()) {\n      Table target = qb.getMetaData().getDestTableForAlias(dest);\n      Partition partition = target == null ? qb.getMetaData().getDestPartitionForAlias(dest) : null;\n      throw new SemanticException(generateErrorMessage(selExprList,\n        \"Expected \" + targetTableSchema.size() + \" columns for \" + dest +\n          (target != null ? \"/\" + target.getCompleteName() : (partition != null ? \"/\" + partition.getCompleteName() : \"\")) +\n          \"; select produces \" + col_list.size() + \" columns\"));\n    }\n    //e.g. map z->expr for a\n    Map<String, ExprNodeDesc> targetCol2Projection = new HashMap<String, ExprNodeDesc>();\n    //e.g. map z->ColumnInfo for a\n    Map<String, ColumnInfo> targetCol2ColumnInfo = new HashMap<String, ColumnInfo>();\n    int colListPos = 0;\n    for(String targetCol : targetTableSchema) {\n      targetCol2ColumnInfo.put(targetCol, out_rwsch.getColumnInfos().get(colListPos));\n      targetCol2Projection.put(targetCol, col_list.get(colListPos++));\n    }\n    Table target = qb.getMetaData().getDestTableForAlias(dest);\n    Partition partition = target == null ? qb.getMetaData().getDestPartitionForAlias(dest) : null;\n    if(target == null && partition == null) {\n      throw new SemanticException(generateErrorMessage(selExprList,\n        \"No table/partition found in QB metadata for dest='\" + dest + \"'\"));\n    }\n    ArrayList<ExprNodeDesc> new_col_list = new ArrayList<ExprNodeDesc>();\n    ArrayList<ColumnInfo> newSchema = new ArrayList<ColumnInfo>();\n    colListPos = 0;\n    List<FieldSchema> targetTableCols = target != null ? target.getCols() : partition.getCols();\n    List<String> targetTableColNames = new ArrayList<String>();\n    for(FieldSchema fs : targetTableCols) {\n      targetTableColNames.add(fs.getName());\n    }\n    Map<String, String> partSpec = qb.getMetaData().getPartSpecForAlias(dest);\n    if(partSpec != null) {\n      //find dynamic partition columns\n      //relies on consistent order via LinkedHashMap\n      for(Map.Entry<String, String> partKeyVal : partSpec.entrySet()) {\n        if (partKeyVal.getValue() == null) {\n          targetTableColNames.add(partKeyVal.getKey());//these must be after non-partition cols\n        }\n      }\n    }\n    //now make the select produce <regular columns>,<dynamic partition columns> with\n    //where missing columns are NULL-filled\n    for(String f : targetTableColNames) {\n      if(targetCol2Projection.containsKey(f)) {\n        //put existing column in new list to make sure it is in the right position\n        new_col_list.add(targetCol2Projection.get(f));\n        ColumnInfo ci = targetCol2ColumnInfo.get(f);//todo: is this OK?\n        ci.setInternalName(getColumnInternalName(colListPos));\n        newSchema.add(ci);\n      }\n      else {\n        //add new 'synthetic' columns for projections not provided by Select\n        TypeCheckCtx tcCtx = new TypeCheckCtx(inputRR);\n        CommonToken t = new CommonToken(HiveParser.TOK_NULL);\n        t.setText(\"TOK_NULL\");\n        ExprNodeDesc exp = genExprNodeDesc(new ASTNode(t), inputRR, tcCtx);\n        new_col_list.add(exp);\n        final String tableAlias = \"\";//is this OK? this column doesn't come from any table\n        ColumnInfo colInfo = new ColumnInfo(getColumnInternalName(colListPos),\n          exp.getWritableObjectInspector(), tableAlias, false);\n        newSchema.add(colInfo);\n      }\n      colListPos++;\n    }\n    col_list.clear();\n    col_list.addAll(new_col_list);\n    out_rwsch.setRowSchema(new RowSchema(newSchema));\n  }",
                "code_after_change": "  private void handleInsertStatementSpec(List<ExprNodeDesc> col_list, String dest,\n                                         RowResolver outputRR, RowResolver inputRR, QB qb,\n                                         ASTNode selExprList) throws SemanticException {\n    //(z,x)\n    List<String> targetTableSchema = qb.getParseInfo().getDestSchemaForClause(dest);//specified in the query\n    if(targetTableSchema == null) {\n      //no insert schema was specified\n      return;\n    }\n    if(targetTableSchema.size() != col_list.size()) {\n      Table target = qb.getMetaData().getDestTableForAlias(dest);\n      Partition partition = target == null ? qb.getMetaData().getDestPartitionForAlias(dest) : null;\n      throw new SemanticException(generateErrorMessage(selExprList,\n        \"Expected \" + targetTableSchema.size() + \" columns for \" + dest +\n          (target != null ? \"/\" + target.getCompleteName() : (partition != null ? \"/\" + partition.getCompleteName() : \"\")) +\n          \"; select produces \" + col_list.size() + \" columns\"));\n    }\n    //e.g. map z->expr for a\n    Map<String, ExprNodeDesc> targetCol2Projection = new HashMap<String, ExprNodeDesc>();\n    //e.g. map z->ColumnInfo for a\n    Map<String, ColumnInfo> targetCol2ColumnInfo = new HashMap<String, ColumnInfo>();\n    int colListPos = 0;\n    for(String targetCol : targetTableSchema) {\n      targetCol2ColumnInfo.put(targetCol, outputRR.getColumnInfos().get(colListPos));\n      targetCol2Projection.put(targetCol, col_list.get(colListPos++));\n    }\n    Table target = qb.getMetaData().getDestTableForAlias(dest);\n    Partition partition = target == null ? qb.getMetaData().getDestPartitionForAlias(dest) : null;\n    if(target == null && partition == null) {\n      throw new SemanticException(generateErrorMessage(selExprList,\n        \"No table/partition found in QB metadata for dest='\" + dest + \"'\"));\n    }\n    ArrayList<ExprNodeDesc> new_col_list = new ArrayList<ExprNodeDesc>();\n    ArrayList<ColumnInfo> newSchema = new ArrayList<ColumnInfo>();\n    colListPos = 0;\n    List<FieldSchema> targetTableCols = target != null ? target.getCols() : partition.getCols();\n    List<String> targetTableColNames = new ArrayList<String>();\n    for(FieldSchema fs : targetTableCols) {\n      targetTableColNames.add(fs.getName());\n    }\n    Map<String, String> partSpec = qb.getMetaData().getPartSpecForAlias(dest);\n    if(partSpec != null) {\n      //find dynamic partition columns\n      //relies on consistent order via LinkedHashMap\n      for(Map.Entry<String, String> partKeyVal : partSpec.entrySet()) {\n        if (partKeyVal.getValue() == null) {\n          targetTableColNames.add(partKeyVal.getKey());//these must be after non-partition cols\n        }\n      }\n    }\n    //now make the select produce <regular columns>,<dynamic partition columns> with\n    //where missing columns are NULL-filled\n    for(String f : targetTableColNames) {\n      if(targetCol2Projection.containsKey(f)) {\n        //put existing column in new list to make sure it is in the right position\n        new_col_list.add(targetCol2Projection.get(f));\n        ColumnInfo ci = targetCol2ColumnInfo.get(f);//todo: is this OK?\n        ci.setInternalName(getColumnInternalName(colListPos));\n        newSchema.add(ci);\n      }\n      else {\n        //add new 'synthetic' columns for projections not provided by Select\n        TypeCheckCtx tcCtx = new TypeCheckCtx(inputRR);\n        CommonToken t = new CommonToken(HiveParser.TOK_NULL);\n        t.setText(\"TOK_NULL\");\n        ExprNodeDesc exp = genExprNodeDesc(new ASTNode(t), inputRR, tcCtx);\n        new_col_list.add(exp);\n        final String tableAlias = null;//this column doesn't come from any table\n        ColumnInfo colInfo = new ColumnInfo(getColumnInternalName(colListPos),\n          exp.getWritableObjectInspector(), tableAlias, false);\n        newSchema.add(colInfo);\n        outputRR.addMappingOnly(colInfo.getTabAlias(), colInfo.getInternalName(), colInfo);\n      }\n      colListPos++;\n    }\n    col_list.clear();\n    col_list.addAll(new_col_list);\n    outputRR.setRowSchema(new RowSchema(newSchema));\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report mentions methods in the stack trace that are related to the ground truth methods, but it does not identify the exact root cause. The methods 'genReduceSinkPlan', 'genBucketingSortingDest', and 'genFileSinkPlan' are in the stack trace and are related to the ground truth methods, but they are not the exact methods where the bug was fixed. There is no fix suggestion provided in the bug report. The problem location is partially identified as it mentions methods in the stack trace that are related to the ground truth methods. There is no wrong information in the bug report as it accurately describes the error and the context in which it occurs."
        }
    },
    {
        "filename": "HIVE-6301.json",
        "code_diff": {
            "ql.src.java.org.apache.hadoop.hive.ql.udf.UDFJson.extract": {
                "code_before_change": "  private Object extract(Object json, String path) {\n\n    // Cache patternkey.matcher(path).matches()\n    Matcher mKey = null;\n    Boolean mKeyMatches = mKeyMatchesCache.get(path);\n    if (mKeyMatches == null) {\n      mKey = patternKey.matcher(path);\n      mKeyMatches = mKey.matches() ? Boolean.TRUE : Boolean.FALSE;\n      mKeyMatchesCache.put(path, mKeyMatches);\n    }\n    if (!mKeyMatches.booleanValue()) {\n      return null;\n    }\n\n    // Cache mkey.group(1)\n    String mKeyGroup1 = mKeyGroup1Cache.get(path);\n    if (mKeyGroup1 == null) {\n      if (mKey == null) {\n        mKey = patternKey.matcher(path);\n      }\n      mKeyGroup1 = mKey.group(1);\n      mKeyGroup1Cache.put(path, mKeyGroup1);\n    }\n    json = extract_json_withkey(json, mKeyGroup1);\n\n    // Cache indexList\n    ArrayList<String> indexList = indexListCache.get(path);\n    if (indexList == null) {\n      Matcher mIndex = patternIndex.matcher(path);\n      indexList = new ArrayList<String>();\n      while (mIndex.find()) {\n        indexList.add(mIndex.group(1));\n      }\n      indexListCache.put(path, indexList);\n    }\n\n    if (indexList.size() > 0) {\n      json = extract_json_withindex(json, indexList);\n    }\n\n    return json;\n  }",
                "code_after_change": "  private Object extract(Object json, String path) {\n\n    // Cache patternkey.matcher(path).matches()\n    Matcher mKey = null;\n    Boolean mKeyMatches = mKeyMatchesCache.get(path);\n    if (mKeyMatches == null) {\n      mKey = patternKey.matcher(path);\n      mKeyMatches = mKey.matches() ? Boolean.TRUE : Boolean.FALSE;\n      mKeyMatchesCache.put(path, mKeyMatches);\n    }\n    if (!mKeyMatches.booleanValue()) {\n      return null;\n    }\n\n    // Cache mkey.group(1)\n    String mKeyGroup1 = mKeyGroup1Cache.get(path);\n    if (mKeyGroup1 == null) {\n      if (mKey == null) {\n        mKey = patternKey.matcher(path);\n        mKeyMatches = mKey.matches() ? Boolean.TRUE : Boolean.FALSE;\n        mKeyMatchesCache.put(path, mKeyMatches);\n        if (!mKeyMatches.booleanValue()) {\n          return null;\n        }\n      }\n      mKeyGroup1 = mKey.group(1);\n      mKeyGroup1Cache.put(path, mKeyGroup1);\n    }\n    json = extract_json_withkey(json, mKeyGroup1);\n\n    // Cache indexList\n    ArrayList<String> indexList = indexListCache.get(path);\n    if (indexList == null) {\n      Matcher mIndex = patternIndex.matcher(path);\n      indexList = new ArrayList<String>();\n      while (mIndex.find()) {\n        indexList.add(mIndex.group(1));\n      }\n      indexListCache.put(path, indexList);\n    }\n\n    if (indexList.size() > 0) {\n      json = extract_json_withindex(json, indexList);\n    }\n\n    return json;\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": ""
            },
            "fix_suggestion": "Correct",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": ""
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report precisely identifies the root cause by pointing out the missing call to mKey.matches() before using mKey.group(1), which is exactly the issue in the ground truth method 'ql.src.java.org.apache.hadoop.hive.ql.udf.UDFJson.extract'. The fix suggestion is correct as it aligns with the developer's fix, which involves ensuring mKey.matches() is called before mKey.group(1) is used. The problem location is also precise as the report directly references the method 'UDFJson.extract' where the bug occurs. There is no wrong information in the bug report as all details are relevant and accurate."
        }
    },
    {
        "filename": "HIVE-8295.json",
        "code_diff": {
            "metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreDirectSql.executeNoResult": {
                "code_before_change": [],
                "code_after_change": "  private void executeNoResult(final String queryText) throws SQLException {\n    JDOConnection jdoConn = pm.getDataStoreConnection();\n    boolean doTrace = LOG.isDebugEnabled();\n    try {\n      long start = doTrace ? System.nanoTime() : 0;\n      ((Connection)jdoConn.getNativeConnection()).createStatement().execute(queryText);\n      timingTrace(doTrace, queryText, start, doTrace ? System.nanoTime() : 0);\n    } finally {\n      jdoConn.close(); // We must release the connection before we call other pm methods.\n    }\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getPartitionsFromPartitionIds": {
                "code_before_change": [],
                "code_after_change": "  private List<Partition> getPartitionsFromPartitionIds(String dbName, String tblName,\n      Boolean isView, List<Object> partIdList) throws MetaException {\n    boolean doTrace = LOG.isDebugEnabled();\n    int idStringWidth = (int)Math.ceil(Math.log10(partIdList.size())) + 1; // 1 for comma\n    int sbCapacity = partIdList.size() * idStringWidth;\n    // Prepare StringBuilder for \"PART_ID in (...)\" to use in future queries.\n    StringBuilder partSb = new StringBuilder(sbCapacity);\n    for (Object partitionId : partIdList) {\n      partSb.append(extractSqlLong(partitionId)).append(\",\");\n    }\n    String partIds = trimCommaList(partSb);\n\n    // Get most of the fields for the IDs provided.\n    // Assume db and table names are the same for all partition, as provided in arguments.\n    String queryText =\n      \"select \\\"PARTITIONS\\\".\\\"PART_ID\\\", \\\"SDS\\\".\\\"SD_ID\\\", \\\"SDS\\\".\\\"CD_ID\\\",\"\n    + \" \\\"SERDES\\\".\\\"SERDE_ID\\\", \\\"PARTITIONS\\\".\\\"CREATE_TIME\\\",\"\n    + \" \\\"PARTITIONS\\\".\\\"LAST_ACCESS_TIME\\\", \\\"SDS\\\".\\\"INPUT_FORMAT\\\", \\\"SDS\\\".\\\"IS_COMPRESSED\\\",\"\n    + \" \\\"SDS\\\".\\\"IS_STOREDASSUBDIRECTORIES\\\", \\\"SDS\\\".\\\"LOCATION\\\", \\\"SDS\\\".\\\"NUM_BUCKETS\\\",\"\n    + \" \\\"SDS\\\".\\\"OUTPUT_FORMAT\\\", \\\"SERDES\\\".\\\"NAME\\\", \\\"SERDES\\\".\\\"SLIB\\\" \"\n    + \"from \\\"PARTITIONS\\\"\"\n    + \"  left outer join \\\"SDS\\\" on \\\"PARTITIONS\\\".\\\"SD_ID\\\" = \\\"SDS\\\".\\\"SD_ID\\\" \"\n    + \"  left outer join \\\"SERDES\\\" on \\\"SDS\\\".\\\"SERDE_ID\\\" = \\\"SERDES\\\".\\\"SERDE_ID\\\" \"\n    + \"where \\\"PART_ID\\\" in (\" + partIds + \") order by \\\"PART_NAME\\\" asc\";\n    long start = doTrace ? System.nanoTime() : 0;\n    Query query = pm.newQuery(\"javax.jdo.query.SQL\", queryText);\n    @SuppressWarnings(\"unchecked\")\n    List<Object[]> sqlResult = (List<Object[]>)query.execute();\n    long queryTime = doTrace ? System.nanoTime() : 0;\n\n    // Read all the fields and create partitions, SDs and serdes.\n    TreeMap<Long, Partition> partitions = new TreeMap<Long, Partition>();\n    TreeMap<Long, StorageDescriptor> sds = new TreeMap<Long, StorageDescriptor>();\n    TreeMap<Long, SerDeInfo> serdes = new TreeMap<Long, SerDeInfo>();\n    TreeMap<Long, List<FieldSchema>> colss = new TreeMap<Long, List<FieldSchema>>();\n    // Keep order by name, consistent with JDO.\n    ArrayList<Partition> orderedResult = new ArrayList<Partition>(partIdList.size());\n\n    // Prepare StringBuilder-s for \"in (...)\" lists to use in one-to-many queries.\n    StringBuilder sdSb = new StringBuilder(sbCapacity), serdeSb = new StringBuilder(sbCapacity);\n    StringBuilder colsSb = new StringBuilder(7); // We expect that there's only one field schema.\n    tblName = tblName.toLowerCase();\n    dbName = dbName.toLowerCase();\n    for (Object[] fields : sqlResult) {\n      // Here comes the ugly part...\n      long partitionId = extractSqlLong(fields[0]);\n      Long sdId = extractSqlLong(fields[1]);\n      Long colId = extractSqlLong(fields[2]);\n      Long serdeId = extractSqlLong(fields[3]);\n      // A partition must have either everything set, or nothing set if it's a view.\n      if (sdId == null || colId == null || serdeId == null) {\n        if (isView == null) {\n          isView = isViewTable(dbName, tblName);\n        }\n        if ((sdId != null || colId != null || serdeId != null) || !isView) {\n          throw new MetaException(\"Unexpected null for one of the IDs, SD \" + sdId + \", column \"\n              + colId + \", serde \" + serdeId + \" for a \" + (isView ? \"\" : \"non-\") + \" view\");\n        }\n      }\n\n      Partition part = new Partition();\n      orderedResult.add(part);\n      // Set the collection fields; some code might not check presence before accessing them.\n      part.setParameters(new HashMap<String, String>());\n      part.setValues(new ArrayList<String>());\n      part.setDbName(dbName);\n      part.setTableName(tblName);\n      if (fields[4] != null) part.setCreateTime(extractSqlInt(fields[4]));\n      if (fields[5] != null) part.setLastAccessTime(extractSqlInt(fields[5]));\n      partitions.put(partitionId, part);\n\n      if (sdId == null) continue; // Probably a view.\n      assert colId != null && serdeId != null;\n\n      // We assume each partition has an unique SD.\n      StorageDescriptor sd = new StorageDescriptor();\n      StorageDescriptor oldSd = sds.put(sdId, sd);\n      if (oldSd != null) {\n        throw new MetaException(\"Partitions reuse SDs; we don't expect that\");\n      }\n      // Set the collection fields; some code might not check presence before accessing them.\n      sd.setSortCols(new ArrayList<Order>());\n      sd.setBucketCols(new ArrayList<String>());\n      sd.setParameters(new HashMap<String, String>());\n      sd.setSkewedInfo(new SkewedInfo(new ArrayList<String>(),\n          new ArrayList<List<String>>(), new HashMap<List<String>, String>()));\n      sd.setInputFormat((String)fields[6]);\n      Boolean tmpBoolean = extractSqlBoolean(fields[7]);\n      if (tmpBoolean != null) sd.setCompressed(tmpBoolean);\n      tmpBoolean = extractSqlBoolean(fields[8]);\n      if (tmpBoolean != null) sd.setStoredAsSubDirectories(tmpBoolean);\n      sd.setLocation((String)fields[9]);\n      if (fields[10] != null) sd.setNumBuckets(extractSqlInt(fields[10]));\n      sd.setOutputFormat((String)fields[11]);\n      sdSb.append(sdId).append(\",\");\n      part.setSd(sd);\n\n      List<FieldSchema> cols = colss.get(colId);\n      // We expect that colId will be the same for all (or many) SDs.\n      if (cols == null) {\n        cols = new ArrayList<FieldSchema>();\n        colss.put(colId, cols);\n        colsSb.append(colId).append(\",\");\n      }\n      sd.setCols(cols);\n\n      // We assume each SD has an unique serde.\n      SerDeInfo serde = new SerDeInfo();\n      SerDeInfo oldSerde = serdes.put(serdeId, serde);\n      if (oldSerde != null) {\n        throw new MetaException(\"SDs reuse serdes; we don't expect that\");\n      }\n      serde.setParameters(new HashMap<String, String>());\n      serde.setName((String)fields[12]);\n      serde.setSerializationLib((String)fields[13]);\n      serdeSb.append(serdeId).append(\",\");\n      sd.setSerdeInfo(serde);\n    }\n    query.closeAll();\n    timingTrace(doTrace, queryText, start, queryTime);\n\n    // Now get all the one-to-many things. Start with partitions.\n    queryText = \"select \\\"PART_ID\\\", \\\"PARAM_KEY\\\", \\\"PARAM_VALUE\\\" from \\\"PARTITION_PARAMS\\\"\"\n        + \" where \\\"PART_ID\\\" in (\" + partIds + \") and \\\"PARAM_KEY\\\" is not null\"\n        + \" order by \\\"PART_ID\\\" asc\";\n    loopJoinOrderedResult(partitions, queryText, 0, new ApplyFunc<Partition>() {\n      @Override\n      public void apply(Partition t, Object[] fields) {\n        t.putToParameters((String)fields[1], (String)fields[2]);\n      }});\n\n    queryText = \"select \\\"PART_ID\\\", \\\"PART_KEY_VAL\\\" from \\\"PARTITION_KEY_VALS\\\"\"\n        + \" where \\\"PART_ID\\\" in (\" + partIds + \") and \\\"INTEGER_IDX\\\" >= 0\"\n        + \" order by \\\"PART_ID\\\" asc, \\\"INTEGER_IDX\\\" asc\";\n    loopJoinOrderedResult(partitions, queryText, 0, new ApplyFunc<Partition>() {\n      @Override\n      public void apply(Partition t, Object[] fields) {\n        t.addToValues((String)fields[1]);\n      }});\n\n    // Prepare IN (blah) lists for the following queries. Cut off the final ','s.\n    if (sdSb.length() == 0) {\n      assert serdeSb.length() == 0 && colsSb.length() == 0;\n      return orderedResult; // No SDs, probably a view.\n    }\n    String sdIds = trimCommaList(sdSb), serdeIds = trimCommaList(serdeSb),\n        colIds = trimCommaList(colsSb);\n\n    // Get all the stuff for SD. Don't do empty-list check - we expect partitions do have SDs.\n    queryText = \"select \\\"SD_ID\\\", \\\"PARAM_KEY\\\", \\\"PARAM_VALUE\\\" from \\\"SD_PARAMS\\\"\"\n        + \" where \\\"SD_ID\\\" in (\" + sdIds + \") and \\\"PARAM_KEY\\\" is not null\"\n        + \" order by \\\"SD_ID\\\" asc\";\n    loopJoinOrderedResult(sds, queryText, 0, new ApplyFunc<StorageDescriptor>() {\n      @Override\n      public void apply(StorageDescriptor t, Object[] fields) {\n        t.putToParameters((String)fields[1], (String)fields[2]);\n      }});\n\n    queryText = \"select \\\"SD_ID\\\", \\\"COLUMN_NAME\\\", \\\"SORT_COLS\\\".\\\"ORDER\\\" from \\\"SORT_COLS\\\"\"\n        + \" where \\\"SD_ID\\\" in (\" + sdIds + \") and \\\"INTEGER_IDX\\\" >= 0\"\n        + \" order by \\\"SD_ID\\\" asc, \\\"INTEGER_IDX\\\" asc\";\n    loopJoinOrderedResult(sds, queryText, 0, new ApplyFunc<StorageDescriptor>() {\n      @Override\n      public void apply(StorageDescriptor t, Object[] fields) {\n        if (fields[2] == null) return;\n        t.addToSortCols(new Order((String)fields[1], extractSqlInt(fields[2])));\n      }});\n\n    queryText = \"select \\\"SD_ID\\\", \\\"BUCKET_COL_NAME\\\" from \\\"BUCKETING_COLS\\\"\"\n        + \" where \\\"SD_ID\\\" in (\" + sdIds + \") and \\\"INTEGER_IDX\\\" >= 0\"\n        + \" order by \\\"SD_ID\\\" asc, \\\"INTEGER_IDX\\\" asc\";\n    loopJoinOrderedResult(sds, queryText, 0, new ApplyFunc<StorageDescriptor>() {\n      @Override\n      public void apply(StorageDescriptor t, Object[] fields) {\n        t.addToBucketCols((String)fields[1]);\n      }});\n\n    // Skewed columns stuff.\n    queryText = \"select \\\"SD_ID\\\", \\\"SKEWED_COL_NAME\\\" from \\\"SKEWED_COL_NAMES\\\"\"\n        + \" where \\\"SD_ID\\\" in (\" + sdIds + \") and \\\"INTEGER_IDX\\\" >= 0\"\n        + \" order by \\\"SD_ID\\\" asc, \\\"INTEGER_IDX\\\" asc\";\n    boolean hasSkewedColumns =\n      loopJoinOrderedResult(sds, queryText, 0, new ApplyFunc<StorageDescriptor>() {\n        @Override\n        public void apply(StorageDescriptor t, Object[] fields) {\n          if (!t.isSetSkewedInfo()) t.setSkewedInfo(new SkewedInfo());\n          t.getSkewedInfo().addToSkewedColNames((String)fields[1]);\n        }}) > 0;\n\n    // Assume we don't need to fetch the rest of the skewed column data if we have no columns.\n    if (hasSkewedColumns) {\n      // We are skipping the SKEWED_STRING_LIST table here, as it seems to be totally useless.\n      queryText =\n            \"select \\\"SKEWED_VALUES\\\".\\\"SD_ID_OID\\\",\"\n          + \"  \\\"SKEWED_STRING_LIST_VALUES\\\".\\\"STRING_LIST_ID\\\",\"\n          + \"  \\\"SKEWED_STRING_LIST_VALUES\\\".\\\"STRING_LIST_VALUE\\\" \"\n          + \"from \\\"SKEWED_VALUES\\\" \"\n          + \"  left outer join \\\"SKEWED_STRING_LIST_VALUES\\\" on \\\"SKEWED_VALUES\\\".\"\n          + \"\\\"STRING_LIST_ID_EID\\\" = \\\"SKEWED_STRING_LIST_VALUES\\\".\\\"STRING_LIST_ID\\\" \"\n          + \"where \\\"SKEWED_VALUES\\\".\\\"SD_ID_OID\\\" in (\" + sdIds + \") \"\n          + \"  and \\\"SKEWED_VALUES\\\".\\\"STRING_LIST_ID_EID\\\" is not null \"\n          + \"  and \\\"SKEWED_VALUES\\\".\\\"INTEGER_IDX\\\" >= 0 \"\n          + \"order by \\\"SKEWED_VALUES\\\".\\\"SD_ID_OID\\\" asc, \\\"SKEWED_VALUES\\\".\\\"INTEGER_IDX\\\" asc,\"\n          + \"  \\\"SKEWED_STRING_LIST_VALUES\\\".\\\"INTEGER_IDX\\\" asc\";\n      loopJoinOrderedResult(sds, queryText, 0, new ApplyFunc<StorageDescriptor>() {\n        private Long currentListId;\n        private List<String> currentList;\n        @Override\n        public void apply(StorageDescriptor t, Object[] fields) throws MetaException {\n          if (!t.isSetSkewedInfo()) t.setSkewedInfo(new SkewedInfo());\n          // Note that this is not a typical list accumulator - there's no call to finalize\n          // the last list. Instead we add list to SD first, as well as locally to add elements.\n          if (fields[1] == null) {\n            currentList = null; // left outer join produced a list with no values\n            currentListId = null;\n            t.getSkewedInfo().addToSkewedColValues(new ArrayList<String>());\n          } else {\n            long fieldsListId = extractSqlLong(fields[1]);\n            if (currentListId == null || fieldsListId != currentListId) {\n              currentList = new ArrayList<String>();\n              currentListId = fieldsListId;\n              t.getSkewedInfo().addToSkewedColValues(currentList);\n            }\n            currentList.add((String)fields[2]);\n          }\n        }});\n\n      // We are skipping the SKEWED_STRING_LIST table here, as it seems to be totally useless.\n      queryText =\n            \"select \\\"SKEWED_COL_VALUE_LOC_MAP\\\".\\\"SD_ID\\\",\"\n          + \" \\\"SKEWED_STRING_LIST_VALUES\\\".STRING_LIST_ID,\"\n          + \" \\\"SKEWED_COL_VALUE_LOC_MAP\\\".\\\"LOCATION\\\",\"\n          + \" \\\"SKEWED_STRING_LIST_VALUES\\\".\\\"STRING_LIST_VALUE\\\" \"\n          + \"from \\\"SKEWED_COL_VALUE_LOC_MAP\\\"\"\n          + \"  left outer join \\\"SKEWED_STRING_LIST_VALUES\\\" on \\\"SKEWED_COL_VALUE_LOC_MAP\\\".\"\n          + \"\\\"STRING_LIST_ID_KID\\\" = \\\"SKEWED_STRING_LIST_VALUES\\\".\\\"STRING_LIST_ID\\\" \"\n          + \"where \\\"SKEWED_COL_VALUE_LOC_MAP\\\".\\\"SD_ID\\\" in (\" + sdIds + \")\"\n          + \"  and \\\"SKEWED_COL_VALUE_LOC_MAP\\\".\\\"STRING_LIST_ID_KID\\\" is not null \"\n          + \"order by \\\"SKEWED_COL_VALUE_LOC_MAP\\\".\\\"SD_ID\\\" asc,\"\n          + \"  \\\"SKEWED_STRING_LIST_VALUES\\\".\\\"STRING_LIST_ID\\\" asc,\"\n          + \"  \\\"SKEWED_STRING_LIST_VALUES\\\".\\\"INTEGER_IDX\\\" asc\";\n\n      loopJoinOrderedResult(sds, queryText, 0, new ApplyFunc<StorageDescriptor>() {\n        private Long currentListId;\n        private List<String> currentList;\n        @Override\n        public void apply(StorageDescriptor t, Object[] fields) throws MetaException {\n          if (!t.isSetSkewedInfo()) {\n            SkewedInfo skewedInfo = new SkewedInfo();\n            skewedInfo.setSkewedColValueLocationMaps(new HashMap<List<String>, String>());\n            t.setSkewedInfo(skewedInfo);\n          }\n          Map<List<String>, String> skewMap = t.getSkewedInfo().getSkewedColValueLocationMaps();\n          // Note that this is not a typical list accumulator - there's no call to finalize\n          // the last list. Instead we add list to SD first, as well as locally to add elements.\n          if (fields[1] == null) {\n            currentList = new ArrayList<String>(); // left outer join produced a list with no values\n            currentListId = null;\n          } else {\n            long fieldsListId = extractSqlLong(fields[1]);\n            if (currentListId == null || fieldsListId != currentListId) {\n              currentList = new ArrayList<String>();\n              currentListId = fieldsListId;\n            } else {\n              skewMap.remove(currentList); // value based compare.. remove first\n            }\n            currentList.add((String)fields[3]);\n          }\n          skewMap.put(currentList, (String)fields[2]);\n        }});\n    } // if (hasSkewedColumns)\n\n    // Get FieldSchema stuff if any.\n    if (!colss.isEmpty()) {\n      // We are skipping the CDS table here, as it seems to be totally useless.\n      queryText = \"select \\\"CD_ID\\\", \\\"COMMENT\\\", \\\"COLUMN_NAME\\\", \\\"TYPE_NAME\\\"\"\n          + \" from \\\"COLUMNS_V2\\\" where \\\"CD_ID\\\" in (\" + colIds + \") and \\\"INTEGER_IDX\\\" >= 0\"\n          + \" order by \\\"CD_ID\\\" asc, \\\"INTEGER_IDX\\\" asc\";\n      loopJoinOrderedResult(colss, queryText, 0, new ApplyFunc<List<FieldSchema>>() {\n        @Override\n        public void apply(List<FieldSchema> t, Object[] fields) {\n          t.add(new FieldSchema((String)fields[2], (String)fields[3], (String)fields[1]));\n        }});\n    }\n\n    // Finally, get all the stuff for serdes - just the params.\n    queryText = \"select \\\"SERDE_ID\\\", \\\"PARAM_KEY\\\", \\\"PARAM_VALUE\\\" from \\\"SERDE_PARAMS\\\"\"\n        + \" where \\\"SERDE_ID\\\" in (\" + serdeIds + \") and \\\"PARAM_KEY\\\" is not null\"\n        + \" order by \\\"SERDE_ID\\\" asc\";\n    loopJoinOrderedResult(serdes, queryText, 0, new ApplyFunc<SerDeInfo>() {\n      @Override\n      public void apply(SerDeInfo t, Object[] fields) {\n        t.putToParameters((String)fields[1], (String)fields[2]);\n      }});\n\n    return orderedResult;\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getDatabase": {
                "code_before_change": [],
                "code_after_change": "  public Database getDatabase(String dbName) throws MetaException{\n    Query queryDbSelector = null;\n    Query queryDbParams = null;\n    try {\n      dbName = dbName.toLowerCase();\n\n      doDbSpecificInitializationsBeforeQuery();\n\n      String queryTextDbSelector= \"select \"\n          + \"\\\"DB_ID\\\", \\\"NAME\\\", \\\"DB_LOCATION_URI\\\", \\\"DESC\\\", \"\n          + \"\\\"OWNER_NAME\\\", \\\"OWNER_TYPE\\\" \"\n          + \"FROM \\\"DBS\\\" where \\\"NAME\\\" = ? \";\n      Object[] params = new Object[] { dbName };\n      queryDbSelector = pm.newQuery(\"javax.jdo.query.SQL\", queryTextDbSelector);\n\n      if (LOG.isTraceEnabled()) {\n        LOG.trace(\"getDatabase:query instantiated : \" + queryTextDbSelector\n            + \" with param [\" + params[0] + \"]\");\n      }\n\n      @SuppressWarnings(\"unchecked\")\n      List<Object[]> sqlResult = (List<Object[]>)queryDbSelector.executeWithArray(params);\n      if ((sqlResult == null) || sqlResult.isEmpty()) {\n        return null;\n      }\n\n      assert(sqlResult.size() == 1);\n      if (sqlResult.get(0) == null) {\n        return null;\n      }\n\n      Object[] dbline = sqlResult.get(0);\n      Long dbid = extractSqlLong(dbline[0]);\n\n      String queryTextDbParams = \"select \\\"PARAM_KEY\\\", \\\"PARAM_VALUE\\\" \"\n          + \" FROM \\\"DATABASE_PARAMS\\\" \"\n          + \" WHERE \\\"DB_ID\\\" = ? \"\n          + \" AND \\\"PARAM_KEY\\\" IS NOT NULL\";\n      params[0] = dbid;\n      queryDbParams = pm.newQuery(\"javax.jdo.query.SQL\", queryTextDbParams);\n      if (LOG.isTraceEnabled()) {\n        LOG.trace(\"getDatabase:query2 instantiated : \" + queryTextDbParams\n            + \" with param [\" + params[0] + \"]\");\n      }\n\n      Map<String,String> dbParams = new HashMap<String,String>();\n      List<Object[]> sqlResult2 = ensureList(queryDbParams.executeWithArray(params));\n      if (!sqlResult2.isEmpty()) {\n        for (Object[] line : sqlResult2) {\n          dbParams.put(extractSqlString(line[0]),extractSqlString(line[1]));\n        }\n      }\n      Database db = new Database();\n      db.setName(extractSqlString(dbline[1]));\n      db.setLocationUri(extractSqlString(dbline[2]));\n      db.setDescription(extractSqlString(dbline[3]));\n      db.setOwnerName(extractSqlString(dbline[4]));\n      String type = extractSqlString(dbline[5]);\n      db.setOwnerType(\n          (null == type || type.trim().isEmpty()) ? null : PrincipalType.valueOf(type));\n      db.setParameters(dbParams);\n      if (LOG.isDebugEnabled()){\n        LOG.debug(\"getDatabase: directsql returning db \" + db.getName()\n            + \" locn[\"+db.getLocationUri()  +\"] desc [\" +db.getDescription()\n            + \"] owner [\" + db.getOwnerName() + \"] ownertype [\"+ db.getOwnerType() +\"]\");\n      }\n      return db;\n    } finally {\n      if (queryDbSelector != null){\n        queryDbSelector.closeAll();\n      }\n      if (queryDbParams != null){\n        queryDbParams.closeAll();\n      }\n    }\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreDirectSql.apply": {
                "code_before_change": "      public void apply(Partition t, Object[] fields) {\n        t.putToParameters((String)fields[1], (String)fields[2]);\n      }});\n\n    queryText = \"select \\\"PART_ID\\\", \\\"PART_KEY_VAL\\\" from \\\"PARTITION_KEY_VALS\\\"\"\n        + \" where \\\"PART_ID\\\" in (\" + partIds + \") and \\\"INTEGER_IDX\\\" >= 0\"\n        + \" order by \\\"PART_ID\\\" asc, \\\"INTEGER_IDX\\\" asc\";\n    loopJoinOrderedResult(partitions, queryText, 0, new ApplyFunc<Partition>() {",
                "code_after_change": "      public void apply(Partition t, Object[] fields) {\n        t.putToParameters((String)fields[1], (String)fields[2]);\n      }});\n\n    queryText = \"select \\\"PART_ID\\\", \\\"PART_KEY_VAL\\\" from \\\"PARTITION_KEY_VALS\\\"\"\n        + \" where \\\"PART_ID\\\" in (\" + partIds + \") and \\\"INTEGER_IDX\\\" >= 0\"\n        + \" order by \\\"PART_ID\\\" asc, \\\"INTEGER_IDX\\\" asc\";\n    loopJoinOrderedResult(partitions, queryText, 0, new ApplyFunc<Partition>() {"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.getSqlResult": {
                "code_before_change": "      protected List<Partition> getSqlResult(GetHelper<List<Partition>> ctx) throws MetaException {\n        Integer max = (maxParts < 0) ? null : maxParts;\n        return directSql.getPartitions(dbName, tblName, max);\n      }",
                "code_after_change": "      protected Database getSqlResult(GetHelper<Database> ctx) throws MetaException {\n        return directSql.getDatabase(dbName);\n      }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsByNamesInternal": {
                "code_before_change": "  protected List<Partition> getPartitionsByNamesInternal(String dbName, String tblName,\n      final List<String> partNames, boolean allowSql, boolean allowJdo)\n          throws MetaException, NoSuchObjectException {\n    return new GetListHelper<Partition>(dbName, tblName, allowSql, allowJdo) {\n      @Override\n      protected List<Partition> getSqlResult(GetHelper<List<Partition>> ctx) throws MetaException {\n        return directSql.getPartitionsViaSqlFilter(dbName, tblName, partNames, null);\n      }\n      @Override\n      protected List<Partition> getJdoResult(\n          GetHelper<List<Partition>> ctx) throws MetaException, NoSuchObjectException {\n        return getPartitionsViaOrmFilter(dbName, tblName, partNames);\n      }\n    }.run(false);\n  }",
                "code_after_change": "  protected List<Partition> getPartitionsByNamesInternal(String dbName, String tblName,\n      final List<String> partNames, boolean allowSql, boolean allowJdo)\n          throws MetaException, NoSuchObjectException {\n    return new GetListHelper<Partition>(dbName, tblName, allowSql, allowJdo) {\n      @Override\n      protected List<Partition> getSqlResult(GetHelper<List<Partition>> ctx) throws MetaException {\n        return directSql.getPartitionsViaSqlFilter(dbName, tblName, partNames);\n      }\n      @Override\n      protected List<Partition> getJdoResult(\n          GetHelper<List<Partition>> ctx) throws MetaException, NoSuchObjectException {\n        return getPartitionsViaOrmFilter(dbName, tblName, partNames);\n      }\n    }.run(false);\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreDirectSql.isCompatibleDatastore": {
                "code_before_change": "  public boolean isCompatibleDatastore() {\n    return isCompatibleDatastore;\n  }",
                "code_after_change": "  public boolean isCompatibleDatastore() {\n    return isCompatibleDatastore;\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getPartitionsViaSqlFilterInternal": {
                "code_before_change": "  private List<Partition> getPartitionsViaSqlFilterInternal(String dbName, String tblName,\n      Boolean isView, String sqlFilter, List<? extends Object> paramsForFilter,\n      List<String> joinsForFilter, Integer max) throws MetaException {\n    boolean doTrace = LOG.isDebugEnabled();\n    dbName = dbName.toLowerCase();\n    tblName = tblName.toLowerCase();\n    // We have to be mindful of order during filtering if we are not returning all partitions.\n    String orderForFilter = (max != null) ? \" order by \\\"PART_NAME\\\" asc\" : \"\";\n    if (isMySql) {\n      assert pm.currentTransaction().isActive();\n      setAnsiQuotesForMysql(); // must be inside tx together with queries\n    }\n\n    // Get all simple fields for partitions and related objects, which we can map one-on-one.\n    // We will do this in 2 queries to use different existing indices for each one.\n    // We do not get table and DB name, assuming they are the same as we are using to filter.\n    // TODO: We might want to tune the indexes instead. With current ones MySQL performs\n    // poorly, esp. with 'order by' w/o index on large tables, even if the number of actual\n    // results is small (query that returns 8 out of 32k partitions can go 4sec. to 0sec. by\n    // just adding a \\\"PART_ID\\\" IN (...) filter that doesn't alter the results to it, probably\n    // causing it to not sort the entire table due to not knowing how selective the filter is.\n    String queryText =\n        \"select \\\"PARTITIONS\\\".\\\"PART_ID\\\" from \\\"PARTITIONS\\\"\"\n      + \"  inner join \\\"TBLS\\\" on \\\"PARTITIONS\\\".\\\"TBL_ID\\\" = \\\"TBLS\\\".\\\"TBL_ID\\\" \"\n      + \"    and \\\"TBLS\\\".\\\"TBL_NAME\\\" = ? \"\n      + \"  inner join \\\"DBS\\\" on \\\"TBLS\\\".\\\"DB_ID\\\" = \\\"DBS\\\".\\\"DB_ID\\\" \"\n      + \"     and \\\"DBS\\\".\\\"NAME\\\" = ? \"\n      + join(joinsForFilter, ' ')\n      + (StringUtils.isBlank(sqlFilter) ? \"\" : (\" where \" + sqlFilter)) + orderForFilter;\n    Object[] params = new Object[paramsForFilter.size() + 2];\n    params[0] = tblName;\n    params[1] = dbName;\n    for (int i = 0; i < paramsForFilter.size(); ++i) {\n      params[i + 2] = paramsForFilter.get(i);\n    }\n\n    long start = doTrace ? System.nanoTime() : 0;\n    Query query = pm.newQuery(\"javax.jdo.query.SQL\", queryText);\n    if (max != null) {\n      query.setRange(0, max.shortValue());\n    }\n    @SuppressWarnings(\"unchecked\")\n    List<Object> sqlResult = (List<Object>)query.executeWithArray(params);\n    long queryTime = doTrace ? System.nanoTime() : 0;\n    if (sqlResult.isEmpty()) {\n      timingTrace(doTrace, queryText, start, queryTime);\n      return new ArrayList<Partition>(); // no partitions, bail early.\n    }\n\n    // Prepare StringBuilder for \"PART_ID in (...)\" to use in future queries.\n    int sbCapacity = sqlResult.size() * 7; // if there are 100k things => 6 chars, plus comma\n    StringBuilder partSb = new StringBuilder(sbCapacity);\n    // Assume db and table names are the same for all partition, that's what we're selecting for.\n    for (Object partitionId : sqlResult) {\n      partSb.append(StatObjectConverter.extractSqlLong(partitionId)).append(\",\");\n    }\n    String partIds = trimCommaList(partSb);\n    timingTrace(doTrace, queryText, start, queryTime);\n\n    // Now get most of the other fields.\n    queryText =\n      \"select \\\"PARTITIONS\\\".\\\"PART_ID\\\", \\\"SDS\\\".\\\"SD_ID\\\", \\\"SDS\\\".\\\"CD_ID\\\",\"\n    + \" \\\"SERDES\\\".\\\"SERDE_ID\\\", \\\"PARTITIONS\\\".\\\"CREATE_TIME\\\",\"\n    + \" \\\"PARTITIONS\\\".\\\"LAST_ACCESS_TIME\\\", \\\"SDS\\\".\\\"INPUT_FORMAT\\\", \\\"SDS\\\".\\\"IS_COMPRESSED\\\",\"\n    + \" \\\"SDS\\\".\\\"IS_STOREDASSUBDIRECTORIES\\\", \\\"SDS\\\".\\\"LOCATION\\\", \\\"SDS\\\".\\\"NUM_BUCKETS\\\",\"\n    + \" \\\"SDS\\\".\\\"OUTPUT_FORMAT\\\", \\\"SERDES\\\".\\\"NAME\\\", \\\"SERDES\\\".\\\"SLIB\\\" \"\n    + \"from \\\"PARTITIONS\\\"\"\n    + \"  left outer join \\\"SDS\\\" on \\\"PARTITIONS\\\".\\\"SD_ID\\\" = \\\"SDS\\\".\\\"SD_ID\\\" \"\n    + \"  left outer join \\\"SERDES\\\" on \\\"SDS\\\".\\\"SERDE_ID\\\" = \\\"SERDES\\\".\\\"SERDE_ID\\\" \"\n    + \"where \\\"PART_ID\\\" in (\" + partIds + \") order by \\\"PART_NAME\\\" asc\";\n    start = doTrace ? System.nanoTime() : 0;\n    query = pm.newQuery(\"javax.jdo.query.SQL\", queryText);\n    @SuppressWarnings(\"unchecked\")\n    List<Object[]> sqlResult2 = (List<Object[]>)query.executeWithArray(params);\n    queryTime = doTrace ? System.nanoTime() : 0;\n\n    // Read all the fields and create partitions, SDs and serdes.\n    TreeMap<Long, Partition> partitions = new TreeMap<Long, Partition>();\n    TreeMap<Long, StorageDescriptor> sds = new TreeMap<Long, StorageDescriptor>();\n    TreeMap<Long, SerDeInfo> serdes = new TreeMap<Long, SerDeInfo>();\n    TreeMap<Long, List<FieldSchema>> colss = new TreeMap<Long, List<FieldSchema>>();\n    // Keep order by name, consistent with JDO.\n    ArrayList<Partition> orderedResult = new ArrayList<Partition>(sqlResult.size());\n\n    // Prepare StringBuilder-s for \"in (...)\" lists to use in one-to-many queries.\n    StringBuilder sdSb = new StringBuilder(sbCapacity), serdeSb = new StringBuilder(sbCapacity);\n    StringBuilder colsSb = new StringBuilder(7); // We expect that there's only one field schema.\n    tblName = tblName.toLowerCase();\n    dbName = dbName.toLowerCase();\n    for (Object[] fields : sqlResult2) {\n      // Here comes the ugly part...\n      long partitionId = StatObjectConverter.extractSqlLong(fields[0]);\n      Long sdId = StatObjectConverter.extractSqlLong(fields[1]);\n      Long colId = StatObjectConverter.extractSqlLong(fields[2]);\n      Long serdeId = StatObjectConverter.extractSqlLong(fields[3]);\n      // A partition must have either everything set, or nothing set if it's a view.\n      if (sdId == null || colId == null || serdeId == null) {\n        if (isView == null) {\n          isView = isViewTable(dbName, tblName);\n        }\n        if ((sdId != null || colId != null || serdeId != null) || !isView) {\n          throw new MetaException(\"Unexpected null for one of the IDs, SD \" + sdId + \", column \"\n              + colId + \", serde \" + serdeId + \" for a \" + (isView ? \"\" : \"non-\") + \" view\");\n        }\n      }\n\n      Partition part = new Partition();\n      orderedResult.add(part);\n      // Set the collection fields; some code might not check presence before accessing them.\n      part.setParameters(new HashMap<String, String>());\n      part.setValues(new ArrayList<String>());\n      part.setDbName(dbName);\n      part.setTableName(tblName);\n      if (fields[4] != null) part.setCreateTime(extractSqlInt(fields[4]));\n      if (fields[5] != null) part.setLastAccessTime(extractSqlInt(fields[5]));\n      partitions.put(partitionId, part);\n\n      if (sdId == null) continue; // Probably a view.\n      assert colId != null && serdeId != null;\n\n      // We assume each partition has an unique SD.\n      StorageDescriptor sd = new StorageDescriptor();\n      StorageDescriptor oldSd = sds.put(sdId, sd);\n      if (oldSd != null) {\n        throw new MetaException(\"Partitions reuse SDs; we don't expect that\");\n      }\n      // Set the collection fields; some code might not check presence before accessing them.\n      sd.setSortCols(new ArrayList<Order>());\n      sd.setBucketCols(new ArrayList<String>());\n      sd.setParameters(new HashMap<String, String>());\n      sd.setSkewedInfo(new SkewedInfo(new ArrayList<String>(),\n          new ArrayList<List<String>>(), new HashMap<List<String>, String>()));\n      sd.setInputFormat((String)fields[6]);\n      Boolean tmpBoolean = extractSqlBoolean(fields[7]);\n      if (tmpBoolean != null) sd.setCompressed(tmpBoolean);\n      tmpBoolean = extractSqlBoolean(fields[8]);\n      if (tmpBoolean != null) sd.setStoredAsSubDirectories(tmpBoolean);\n      sd.setLocation((String)fields[9]);\n      if (fields[10] != null) sd.setNumBuckets(extractSqlInt(fields[10]));\n      sd.setOutputFormat((String)fields[11]);\n      sdSb.append(sdId).append(\",\");\n      part.setSd(sd);\n\n      List<FieldSchema> cols = colss.get(colId);\n      // We expect that colId will be the same for all (or many) SDs.\n      if (cols == null) {\n        cols = new ArrayList<FieldSchema>();\n        colss.put(colId, cols);\n        colsSb.append(colId).append(\",\");\n      }\n      sd.setCols(cols);\n\n      // We assume each SD has an unique serde.\n      SerDeInfo serde = new SerDeInfo();\n      SerDeInfo oldSerde = serdes.put(serdeId, serde);\n      if (oldSerde != null) {\n        throw new MetaException(\"SDs reuse serdes; we don't expect that\");\n      }\n      serde.setParameters(new HashMap<String, String>());\n      serde.setName((String)fields[12]);\n      serde.setSerializationLib((String)fields[13]);\n      serdeSb.append(serdeId).append(\",\");\n      sd.setSerdeInfo(serde);\n    }\n    query.closeAll();\n    timingTrace(doTrace, queryText, start, queryTime);\n\n    // Now get all the one-to-many things. Start with partitions.\n    queryText = \"select \\\"PART_ID\\\", \\\"PARAM_KEY\\\", \\\"PARAM_VALUE\\\" from \\\"PARTITION_PARAMS\\\"\"\n        + \" where \\\"PART_ID\\\" in (\" + partIds + \") and \\\"PARAM_KEY\\\" is not null\"\n        + \" order by \\\"PART_ID\\\" asc\";\n    loopJoinOrderedResult(partitions, queryText, 0, new ApplyFunc<Partition>() {\n      @Override\n      public void apply(Partition t, Object[] fields) {\n        t.putToParameters((String)fields[1], (String)fields[2]);\n      }});\n\n    queryText = \"select \\\"PART_ID\\\", \\\"PART_KEY_VAL\\\" from \\\"PARTITION_KEY_VALS\\\"\"\n        + \" where \\\"PART_ID\\\" in (\" + partIds + \") and \\\"INTEGER_IDX\\\" >= 0\"\n        + \" order by \\\"PART_ID\\\" asc, \\\"INTEGER_IDX\\\" asc\";\n    loopJoinOrderedResult(partitions, queryText, 0, new ApplyFunc<Partition>() {\n      @Override\n      public void apply(Partition t, Object[] fields) {\n        t.addToValues((String)fields[1]);\n      }});\n\n    // Prepare IN (blah) lists for the following queries. Cut off the final ','s.\n    if (sdSb.length() == 0) {\n      assert serdeSb.length() == 0 && colsSb.length() == 0;\n      return orderedResult; // No SDs, probably a view.\n    }\n    String sdIds = trimCommaList(sdSb), serdeIds = trimCommaList(serdeSb),\n        colIds = trimCommaList(colsSb);\n\n    // Get all the stuff for SD. Don't do empty-list check - we expect partitions do have SDs.\n    queryText = \"select \\\"SD_ID\\\", \\\"PARAM_KEY\\\", \\\"PARAM_VALUE\\\" from \\\"SD_PARAMS\\\"\"\n        + \" where \\\"SD_ID\\\" in (\" + sdIds + \") and \\\"PARAM_KEY\\\" is not null\"\n        + \" order by \\\"SD_ID\\\" asc\";\n    loopJoinOrderedResult(sds, queryText, 0, new ApplyFunc<StorageDescriptor>() {\n      @Override\n      public void apply(StorageDescriptor t, Object[] fields) {\n        t.putToParameters((String)fields[1], (String)fields[2]);\n      }});\n\n    queryText = \"select \\\"SD_ID\\\", \\\"COLUMN_NAME\\\", \\\"SORT_COLS\\\".\\\"ORDER\\\" from \\\"SORT_COLS\\\"\"\n        + \" where \\\"SD_ID\\\" in (\" + sdIds + \") and \\\"INTEGER_IDX\\\" >= 0\"\n        + \" order by \\\"SD_ID\\\" asc, \\\"INTEGER_IDX\\\" asc\";\n    loopJoinOrderedResult(sds, queryText, 0, new ApplyFunc<StorageDescriptor>() {\n      @Override\n      public void apply(StorageDescriptor t, Object[] fields) {\n        if (fields[2] == null) return;\n        t.addToSortCols(new Order((String)fields[1], extractSqlInt(fields[2])));\n      }});\n\n    queryText = \"select \\\"SD_ID\\\", \\\"BUCKET_COL_NAME\\\" from \\\"BUCKETING_COLS\\\"\"\n        + \" where \\\"SD_ID\\\" in (\" + sdIds + \") and \\\"INTEGER_IDX\\\" >= 0\"\n        + \" order by \\\"SD_ID\\\" asc, \\\"INTEGER_IDX\\\" asc\";\n    loopJoinOrderedResult(sds, queryText, 0, new ApplyFunc<StorageDescriptor>() {\n      @Override\n      public void apply(StorageDescriptor t, Object[] fields) {\n        t.addToBucketCols((String)fields[1]);\n      }});\n\n    // Skewed columns stuff.\n    queryText = \"select \\\"SD_ID\\\", \\\"SKEWED_COL_NAME\\\" from \\\"SKEWED_COL_NAMES\\\"\"\n        + \" where \\\"SD_ID\\\" in (\" + sdIds + \") and \\\"INTEGER_IDX\\\" >= 0\"\n        + \" order by \\\"SD_ID\\\" asc, \\\"INTEGER_IDX\\\" asc\";\n    boolean hasSkewedColumns =\n      loopJoinOrderedResult(sds, queryText, 0, new ApplyFunc<StorageDescriptor>() {\n        @Override\n        public void apply(StorageDescriptor t, Object[] fields) {\n          if (!t.isSetSkewedInfo()) t.setSkewedInfo(new SkewedInfo());\n          t.getSkewedInfo().addToSkewedColNames((String)fields[1]);\n        }}) > 0;\n\n    // Assume we don't need to fetch the rest of the skewed column data if we have no columns.\n    if (hasSkewedColumns) {\n      // We are skipping the SKEWED_STRING_LIST table here, as it seems to be totally useless.\n      queryText =\n            \"select \\\"SKEWED_VALUES\\\".\\\"SD_ID_OID\\\",\"\n          + \"  \\\"SKEWED_STRING_LIST_VALUES\\\".\\\"STRING_LIST_ID\\\",\"\n          + \"  \\\"SKEWED_STRING_LIST_VALUES\\\".\\\"STRING_LIST_VALUE\\\" \"\n          + \"from \\\"SKEWED_VALUES\\\" \"\n          + \"  left outer join \\\"SKEWED_STRING_LIST_VALUES\\\" on \\\"SKEWED_VALUES\\\".\"\n          + \"\\\"STRING_LIST_ID_EID\\\" = \\\"SKEWED_STRING_LIST_VALUES\\\".\\\"STRING_LIST_ID\\\" \"\n          + \"where \\\"SKEWED_VALUES\\\".\\\"SD_ID_OID\\\" in (\" + sdIds + \") \"\n          + \"  and \\\"SKEWED_VALUES\\\".\\\"STRING_LIST_ID_EID\\\" is not null \"\n          + \"  and \\\"SKEWED_VALUES\\\".\\\"INTEGER_IDX\\\" >= 0 \"\n          + \"order by \\\"SKEWED_VALUES\\\".\\\"SD_ID_OID\\\" asc, \\\"SKEWED_VALUES\\\".\\\"INTEGER_IDX\\\" asc,\"\n          + \"  \\\"SKEWED_STRING_LIST_VALUES\\\".\\\"INTEGER_IDX\\\" asc\";\n      loopJoinOrderedResult(sds, queryText, 0, new ApplyFunc<StorageDescriptor>() {\n        private Long currentListId;\n        private List<String> currentList;\n        @Override\n        public void apply(StorageDescriptor t, Object[] fields) throws MetaException {\n          if (!t.isSetSkewedInfo()) t.setSkewedInfo(new SkewedInfo());\n          // Note that this is not a typical list accumulator - there's no call to finalize\n          // the last list. Instead we add list to SD first, as well as locally to add elements.\n          if (fields[1] == null) {\n            currentList = null; // left outer join produced a list with no values\n            currentListId = null;\n            t.getSkewedInfo().addToSkewedColValues(new ArrayList<String>());\n          } else {\n            long fieldsListId = StatObjectConverter.extractSqlLong(fields[1]);\n            if (currentListId == null || fieldsListId != currentListId) {\n              currentList = new ArrayList<String>();\n              currentListId = fieldsListId;\n              t.getSkewedInfo().addToSkewedColValues(currentList);\n            }\n            currentList.add((String)fields[2]);\n          }\n        }});\n\n      // We are skipping the SKEWED_STRING_LIST table here, as it seems to be totally useless.\n      queryText =\n            \"select \\\"SKEWED_COL_VALUE_LOC_MAP\\\".\\\"SD_ID\\\",\"\n          + \" \\\"SKEWED_STRING_LIST_VALUES\\\".STRING_LIST_ID,\"\n          + \" \\\"SKEWED_COL_VALUE_LOC_MAP\\\".\\\"LOCATION\\\",\"\n          + \" \\\"SKEWED_STRING_LIST_VALUES\\\".\\\"STRING_LIST_VALUE\\\" \"\n          + \"from \\\"SKEWED_COL_VALUE_LOC_MAP\\\"\"\n          + \"  left outer join \\\"SKEWED_STRING_LIST_VALUES\\\" on \\\"SKEWED_COL_VALUE_LOC_MAP\\\".\"\n          + \"\\\"STRING_LIST_ID_KID\\\" = \\\"SKEWED_STRING_LIST_VALUES\\\".\\\"STRING_LIST_ID\\\" \"\n          + \"where \\\"SKEWED_COL_VALUE_LOC_MAP\\\".\\\"SD_ID\\\" in (\" + sdIds + \")\"\n          + \"  and \\\"SKEWED_COL_VALUE_LOC_MAP\\\".\\\"STRING_LIST_ID_KID\\\" is not null \"\n          + \"order by \\\"SKEWED_COL_VALUE_LOC_MAP\\\".\\\"SD_ID\\\" asc,\"\n          + \"  \\\"SKEWED_STRING_LIST_VALUES\\\".\\\"STRING_LIST_ID\\\" asc,\"\n          + \"  \\\"SKEWED_STRING_LIST_VALUES\\\".\\\"INTEGER_IDX\\\" asc\";\n\n      loopJoinOrderedResult(sds, queryText, 0, new ApplyFunc<StorageDescriptor>() {\n        private Long currentListId;\n        private List<String> currentList;\n        @Override\n        public void apply(StorageDescriptor t, Object[] fields) throws MetaException {\n          if (!t.isSetSkewedInfo()) {\n            SkewedInfo skewedInfo = new SkewedInfo();\n            skewedInfo.setSkewedColValueLocationMaps(new HashMap<List<String>, String>());\n            t.setSkewedInfo(skewedInfo);\n          }\n          Map<List<String>, String> skewMap = t.getSkewedInfo().getSkewedColValueLocationMaps();\n          // Note that this is not a typical list accumulator - there's no call to finalize\n          // the last list. Instead we add list to SD first, as well as locally to add elements.\n          if (fields[1] == null) {\n            currentList = new ArrayList<String>(); // left outer join produced a list with no values\n            currentListId = null;\n          } else {\n            long fieldsListId = StatObjectConverter.extractSqlLong(fields[1]);\n            if (currentListId == null || fieldsListId != currentListId) {\n              currentList = new ArrayList<String>();\n              currentListId = fieldsListId;\n            } else {\n              skewMap.remove(currentList); // value based compare.. remove first\n            }\n            currentList.add((String)fields[3]);\n          }\n          skewMap.put(currentList, (String)fields[2]);\n        }});\n    } // if (hasSkewedColumns)\n\n    // Get FieldSchema stuff if any.\n    if (!colss.isEmpty()) {\n      // We are skipping the CDS table here, as it seems to be totally useless.\n      queryText = \"select \\\"CD_ID\\\", \\\"COMMENT\\\", \\\"COLUMN_NAME\\\", \\\"TYPE_NAME\\\"\"\n          + \" from \\\"COLUMNS_V2\\\" where \\\"CD_ID\\\" in (\" + colIds + \") and \\\"INTEGER_IDX\\\" >= 0\"\n          + \" order by \\\"CD_ID\\\" asc, \\\"INTEGER_IDX\\\" asc\";\n      loopJoinOrderedResult(colss, queryText, 0, new ApplyFunc<List<FieldSchema>>() {\n        @Override\n        public void apply(List<FieldSchema> t, Object[] fields) {\n          t.add(new FieldSchema((String)fields[2], (String)fields[3], (String)fields[1]));\n        }});\n    }\n\n    // Finally, get all the stuff for serdes - just the params.\n    queryText = \"select \\\"SERDE_ID\\\", \\\"PARAM_KEY\\\", \\\"PARAM_VALUE\\\" from \\\"SERDE_PARAMS\\\"\"\n        + \" where \\\"SERDE_ID\\\" in (\" + serdeIds + \") and \\\"PARAM_KEY\\\" is not null\"\n        + \" order by \\\"SERDE_ID\\\" asc\";\n    loopJoinOrderedResult(serdes, queryText, 0, new ApplyFunc<SerDeInfo>() {\n      @Override\n      public void apply(SerDeInfo t, Object[] fields) {\n        t.putToParameters((String)fields[1], (String)fields[2]);\n      }});\n\n    return orderedResult;\n  }",
                "code_after_change": "  private List<Partition> getPartitionsViaSqlFilterInternal(String dbName, String tblName,\n      Boolean isView, String sqlFilter, List<? extends Object> paramsForFilter,\n      List<String> joinsForFilter, Integer max) throws MetaException {\n    boolean doTrace = LOG.isDebugEnabled();\n    dbName = dbName.toLowerCase();\n    tblName = tblName.toLowerCase();\n    // We have to be mindful of order during filtering if we are not returning all partitions.\n    String orderForFilter = (max != null) ? \" order by \\\"PART_NAME\\\" asc\" : \"\";\n\n    doDbSpecificInitializationsBeforeQuery();\n\n    // Get all simple fields for partitions and related objects, which we can map one-on-one.\n    // We will do this in 2 queries to use different existing indices for each one.\n    // We do not get table and DB name, assuming they are the same as we are using to filter.\n    // TODO: We might want to tune the indexes instead. With current ones MySQL performs\n    // poorly, esp. with 'order by' w/o index on large tables, even if the number of actual\n    // results is small (query that returns 8 out of 32k partitions can go 4sec. to 0sec. by\n    // just adding a \\\"PART_ID\\\" IN (...) filter that doesn't alter the results to it, probably\n    // causing it to not sort the entire table due to not knowing how selective the filter is.\n    String queryText =\n        \"select \\\"PARTITIONS\\\".\\\"PART_ID\\\" from \\\"PARTITIONS\\\"\"\n      + \"  inner join \\\"TBLS\\\" on \\\"PARTITIONS\\\".\\\"TBL_ID\\\" = \\\"TBLS\\\".\\\"TBL_ID\\\" \"\n      + \"    and \\\"TBLS\\\".\\\"TBL_NAME\\\" = ? \"\n      + \"  inner join \\\"DBS\\\" on \\\"TBLS\\\".\\\"DB_ID\\\" = \\\"DBS\\\".\\\"DB_ID\\\" \"\n      + \"     and \\\"DBS\\\".\\\"NAME\\\" = ? \"\n      + join(joinsForFilter, ' ')\n      + (StringUtils.isBlank(sqlFilter) ? \"\" : (\" where \" + sqlFilter)) + orderForFilter;\n    Object[] params = new Object[paramsForFilter.size() + 2];\n    params[0] = tblName;\n    params[1] = dbName;\n    for (int i = 0; i < paramsForFilter.size(); ++i) {\n      params[i + 2] = paramsForFilter.get(i);\n    }\n\n    long start = doTrace ? System.nanoTime() : 0;\n    Query query = pm.newQuery(\"javax.jdo.query.SQL\", queryText);\n    if (max != null) {\n      query.setRange(0, max.shortValue());\n    }\n    @SuppressWarnings(\"unchecked\")\n    List<Object> sqlResult = (List<Object>)query.executeWithArray(params);\n    long queryTime = doTrace ? System.nanoTime() : 0;\n    if (sqlResult.isEmpty()) {\n      timingTrace(doTrace, queryText, start, queryTime);\n      return new ArrayList<Partition>(); // no partitions, bail early.\n    }\n\n    // Get full objects. For Oracle, do it in batches.\n    List<Partition> result = null;\n    if (batchSize != NO_BATCHING && batchSize < sqlResult.size()) {\n      result = new ArrayList<Partition>(sqlResult.size());\n      while (result.size() < sqlResult.size()) {\n        int toIndex = Math.min(result.size() + batchSize, sqlResult.size());\n        List<Object> batchedSqlResult = sqlResult.subList(result.size(), toIndex);\n        result.addAll(getPartitionsFromPartitionIds(dbName, tblName, isView, batchedSqlResult));\n      }\n    } else {\n      result = getPartitionsFromPartitionIds(dbName, tblName, isView, sqlResult);\n    }\n \n    timingTrace(doTrace, queryText, start, queryTime);\n    query.closeAll();\n    return result;\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.StatObjectConverter.createThriftDecimal": {
                "code_before_change": "  private static Decimal createThriftDecimal(String s) {\n    BigDecimal d = new BigDecimal(s);\n    return new Decimal(ByteBuffer.wrap(d.unscaledValue().toByteArray()), (short)d.scale());\n  }",
                "code_after_change": "  private static Decimal createThriftDecimal(String s) {\n    BigDecimal d = new BigDecimal(s);\n    return new Decimal(ByteBuffer.wrap(d.unscaledValue().toByteArray()), (short)d.scale());\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreDirectSql.timingTrace": {
                "code_before_change": "  private void timingTrace(boolean doTrace, String queryText, long start, long queryTime) {\n    if (!doTrace) return;\n    LOG.debug(\"Direct SQL query in \" + (queryTime - start) / 1000000.0 + \"ms + \" +\n        (System.nanoTime() - queryTime) / 1000000.0 + \"ms, the query is [\" + queryText + \"]\");\n  }",
                "code_after_change": "  private void timingTrace(boolean doTrace, String queryText, long start, long queryTime) {\n    if (!doTrace) return;\n    LOG.debug(\"Direct SQL query in \" + (queryTime - start) / 1000000.0 + \"ms + \" +\n        (System.nanoTime() - queryTime) / 1000000.0 + \"ms, the query is [\" + queryText + \"]\");\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreDirectSql.loopJoinOrderedResult": {
                "code_before_change": "  private <T> int loopJoinOrderedResult(TreeMap<Long, T> tree,\n      String queryText, int keyIndex, ApplyFunc<T> func) throws MetaException {\n    boolean doTrace = LOG.isDebugEnabled();\n    long start = doTrace ? System.nanoTime() : 0;\n    Query query = pm.newQuery(\"javax.jdo.query.SQL\", queryText);\n    Object result = query.execute();\n    long queryTime = doTrace ? System.nanoTime() : 0;\n    if (result == null) {\n      query.closeAll();\n      return 0;\n    }\n    List<Object[]> list = ensureList(result);\n    Iterator<Object[]> iter = list.iterator();\n    Object[] fields = null;\n    for (Map.Entry<Long, T> entry : tree.entrySet()) {\n      if (fields == null && !iter.hasNext()) break;\n      long id = entry.getKey();\n      while (fields != null || iter.hasNext()) {\n        if (fields == null) {\n          fields = iter.next();\n        }\n        long nestedId = StatObjectConverter.extractSqlLong(fields[keyIndex]);\n        if (nestedId < id) throw new MetaException(\"Found entries for unknown ID \" + nestedId);\n        if (nestedId > id) break; // fields belong to one of the next entries\n        func.apply(entry.getValue(), fields);\n        fields = null;\n      }\n    }\n    int rv = list.size();\n    query.closeAll();\n    timingTrace(doTrace, queryText, start, queryTime);\n    return rv;\n  }",
                "code_after_change": "  private <T> int loopJoinOrderedResult(TreeMap<Long, T> tree,\n      String queryText, int keyIndex, ApplyFunc<T> func) throws MetaException {\n    boolean doTrace = LOG.isDebugEnabled();\n    long start = doTrace ? System.nanoTime() : 0;\n    Query query = pm.newQuery(\"javax.jdo.query.SQL\", queryText);\n    Object result = query.execute();\n    long queryTime = doTrace ? System.nanoTime() : 0;\n    if (result == null) {\n      query.closeAll();\n      return 0;\n    }\n    List<Object[]> list = ensureList(result);\n    Iterator<Object[]> iter = list.iterator();\n    Object[] fields = null;\n    for (Map.Entry<Long, T> entry : tree.entrySet()) {\n      if (fields == null && !iter.hasNext()) break;\n      long id = entry.getKey();\n      while (fields != null || iter.hasNext()) {\n        if (fields == null) {\n          fields = iter.next();\n        }\n        long nestedId = extractSqlLong(fields[keyIndex]);\n        if (nestedId < id) throw new MetaException(\"Found entries for unknown ID \" + nestedId);\n        if (nestedId > id) break; // fields belong to one of the next entries\n        func.apply(entry.getValue(), fields);\n        fields = null;\n      }\n    }\n    int rv = list.size();\n    query.closeAll();\n    timingTrace(doTrace, queryText, start, queryTime);\n    return rv;\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreDirectSql.makeColumnStats": {
                "code_before_change": "  private ColumnStatistics makeColumnStats(\n      List<Object[]> list, ColumnStatisticsDesc csd, int offset) throws MetaException {\n    ColumnStatistics result = new ColumnStatistics();\n    result.setStatsDesc(csd);\n    List<ColumnStatisticsObj> csos = new ArrayList<ColumnStatisticsObj>(list.size());\n    for (Object[] row : list) {\n      // LastAnalyzed is stored per column but thrift has it per several;\n      // get the lowest for now as nobody actually uses this field.\n      Object laObj = row[offset + 14];\n      if (laObj != null && (!csd.isSetLastAnalyzed() || csd.getLastAnalyzed() > StatObjectConverter.extractSqlLong(laObj))) {\n        csd.setLastAnalyzed(StatObjectConverter.extractSqlLong(laObj));\n      }\n      csos.add(prepareCSObj(row, offset));\n    }\n    result.setStatsObj(csos);\n    return result;\n  }",
                "code_after_change": "  private ColumnStatistics makeColumnStats(\n      List<Object[]> list, ColumnStatisticsDesc csd, int offset) throws MetaException {\n    ColumnStatistics result = new ColumnStatistics();\n    result.setStatsDesc(csd);\n    List<ColumnStatisticsObj> csos = new ArrayList<ColumnStatisticsObj>(list.size());\n    for (Object[] row : list) {\n      // LastAnalyzed is stored per column but thrift has it per several;\n      // get the lowest for now as nobody actually uses this field.\n      Object laObj = row[offset + 14];\n      if (laObj != null && (!csd.isSetLastAnalyzed() || csd.getLastAnalyzed() > extractSqlLong(laObj))) {\n        csd.setLastAnalyzed(extractSqlLong(laObj));\n      }\n      csos.add(prepareCSObj(row, offset));\n    }\n    result.setStatsObj(csos);\n    return result;\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getTableStats": {
                "code_before_change": "  public ColumnStatistics getTableStats(\n      String dbName, String tableName, List<String> colNames) throws MetaException {\n    if (colNames.isEmpty()) {\n      return null;\n    }\n    boolean doTrace = LOG.isDebugEnabled();\n    long start = doTrace ? System.nanoTime() : 0;\n    String queryText = \"select \" + STATS_COLLIST + \" from \\\"TAB_COL_STATS\\\" \"\n      + \" where \\\"DB_NAME\\\" = ? and \\\"TABLE_NAME\\\" = ? and \\\"COLUMN_NAME\\\" in (\"\n      + makeParams(colNames.size()) + \")\";\n    Query query = pm.newQuery(\"javax.jdo.query.SQL\", queryText);\n    Object[] params = new Object[colNames.size() + 2];\n    params[0] = dbName;\n    params[1] = tableName;\n    for (int i = 0; i < colNames.size(); ++i) {\n      params[i + 2] = colNames.get(i);\n    }\n    Object qResult = query.executeWithArray(params);\n    long queryTime = doTrace ? System.nanoTime() : 0;\n    if (qResult == null) {\n      query.closeAll();\n      return null;\n    }\n    List<Object[]> list = ensureList(qResult);\n    if (list.isEmpty()) return null;\n    ColumnStatisticsDesc csd = new ColumnStatisticsDesc(true, dbName, tableName);\n    ColumnStatistics result = makeColumnStats(list, csd, 0);\n    timingTrace(doTrace, queryText, start, queryTime);\n    query.closeAll();\n    return result;\n  }",
                "code_after_change": "  public ColumnStatistics getTableStats(\n      String dbName, String tableName, List<String> colNames) throws MetaException {\n    if (colNames.isEmpty()) {\n      return null;\n    }\n    boolean doTrace = LOG.isDebugEnabled();\n    long start = doTrace ? System.nanoTime() : 0;\n    String queryText = \"select \" + STATS_COLLIST + \" from \\\"TAB_COL_STATS\\\" \"\n      + \" where \\\"DB_NAME\\\" = ? and \\\"TABLE_NAME\\\" = ? and \\\"COLUMN_NAME\\\" in (\"\n      + makeParams(colNames.size()) + \")\";\n    Query query = pm.newQuery(\"javax.jdo.query.SQL\", queryText);\n    Object[] params = new Object[colNames.size() + 2];\n    params[0] = dbName;\n    params[1] = tableName;\n    for (int i = 0; i < colNames.size(); ++i) {\n      params[i + 2] = colNames.get(i);\n    }\n    Object qResult = query.executeWithArray(params);\n    long queryTime = doTrace ? System.nanoTime() : 0;\n    if (qResult == null) {\n      query.closeAll();\n      return null;\n    }\n    List<Object[]> list = ensureList(qResult);\n    if (list.isEmpty()) return null;\n    ColumnStatisticsDesc csd = new ColumnStatisticsDesc(true, dbName, tableName);\n    ColumnStatistics result = makeColumnStats(list, csd, 0);\n    timingTrace(doTrace, queryText, start, queryTime);\n    query.closeAll();\n    return result;\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreDirectSql.partsFoundForPartitions": {
                "code_before_change": "  private long partsFoundForPartitions(String dbName, String tableName,\n      List<String> partNames, List<String> colNames) throws MetaException {\n    long partsFound = 0;\n    boolean doTrace = LOG.isDebugEnabled();\n    String qText = \"select count(\\\"COLUMN_NAME\\\") from \\\"PART_COL_STATS\\\"\"\n        + \" where \\\"DB_NAME\\\" = ? and \\\"TABLE_NAME\\\" = ? \"\n        + \" and \\\"COLUMN_NAME\\\" in (\" + makeParams(colNames.size()) + \")\"\n        + \" and \\\"PARTITION_NAME\\\" in (\" + makeParams(partNames.size()) + \")\"\n        + \" group by \\\"PARTITION_NAME\\\"\";\n    long start = doTrace ? System.nanoTime() : 0;\n    Query query = pm.newQuery(\"javax.jdo.query.SQL\", qText);\n    Object qResult = query.executeWithArray(prepareParams(dbName, tableName,\n        partNames, colNames));\n    long end = doTrace ? System.nanoTime() : 0;\n    timingTrace(doTrace, qText, start, end);\n    ForwardQueryResult fqr = (ForwardQueryResult) qResult;\n    Iterator<?> iter = fqr.iterator();\n    while (iter.hasNext()) {\n      if (StatObjectConverter.extractSqlLong(iter.next()) == colNames.size()) {\n        partsFound++;\n      }\n    }\n    return partsFound;\n  }",
                "code_after_change": "  private long partsFoundForPartitions(String dbName, String tableName,\n      List<String> partNames, List<String> colNames) throws MetaException {\n    long partsFound = 0;\n    boolean doTrace = LOG.isDebugEnabled();\n    String qText = \"select count(\\\"COLUMN_NAME\\\") from \\\"PART_COL_STATS\\\"\"\n        + \" where \\\"DB_NAME\\\" = ? and \\\"TABLE_NAME\\\" = ? \"\n        + \" and \\\"COLUMN_NAME\\\" in (\" + makeParams(colNames.size()) + \")\"\n        + \" and \\\"PARTITION_NAME\\\" in (\" + makeParams(partNames.size()) + \")\"\n        + \" group by \\\"PARTITION_NAME\\\"\";\n    long start = doTrace ? System.nanoTime() : 0;\n    Query query = pm.newQuery(\"javax.jdo.query.SQL\", qText);\n    Object qResult = query.executeWithArray(prepareParams(dbName, tableName,\n        partNames, colNames));\n    long end = doTrace ? System.nanoTime() : 0;\n    timingTrace(doTrace, qText, start, end);\n    ForwardQueryResult fqr = (ForwardQueryResult) qResult;\n    Iterator<?> iter = fqr.iterator();\n    while (iter.hasNext()) {\n      if (extractSqlLong(iter.next()) == colNames.size()) {\n        partsFound++;\n      }\n    }\n    return partsFound;\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreDirectSql.columnStatisticsObjForPartitions": {
                "code_before_change": "  private List<ColumnStatisticsObj> columnStatisticsObjForPartitions(\n      String dbName, String tableName, List<String> partNames,\n      List<String> colNames, long partsFound) throws MetaException {\n    String commonPrefix = \"select \\\"COLUMN_NAME\\\", \\\"COLUMN_TYPE\\\", \"\n        + \"min(\\\"LONG_LOW_VALUE\\\"), max(\\\"LONG_HIGH_VALUE\\\"), min(\\\"DOUBLE_LOW_VALUE\\\"), max(\\\"DOUBLE_HIGH_VALUE\\\"), \"\n        + \"min(\\\"BIG_DECIMAL_LOW_VALUE\\\"), max(\\\"BIG_DECIMAL_HIGH_VALUE\\\"), sum(\\\"NUM_NULLS\\\"), max(\\\"NUM_DISTINCTS\\\"), \"\n        + \"max(\\\"AVG_COL_LEN\\\"), max(\\\"MAX_COL_LEN\\\"), sum(\\\"NUM_TRUES\\\"), sum(\\\"NUM_FALSES\\\") from \\\"PART_COL_STATS\\\"\"\n        + \" where \\\"DB_NAME\\\" = ? and \\\"TABLE_NAME\\\" = ? \";\n    String qText = null;\n    long start = 0;\n    long end = 0;\n    Query query = null;\n    boolean doTrace = LOG.isDebugEnabled();\n    Object qResult = null;\n    ForwardQueryResult fqr = null;\n    // Check if the status of all the columns of all the partitions exists\n    // Extrapolation is not needed.\n    if (partsFound == partNames.size()) {\n      qText = commonPrefix \n          + \" and \\\"COLUMN_NAME\\\" in (\" + makeParams(colNames.size()) + \")\"\n          + \" and \\\"PARTITION_NAME\\\" in (\" + makeParams(partNames.size()) + \")\"\n          + \" group by \\\"COLUMN_NAME\\\", \\\"COLUMN_TYPE\\\"\";\n      start = doTrace ? System.nanoTime() : 0;\n      query = pm.newQuery(\"javax.jdo.query.SQL\", qText);\n      qResult = query.executeWithArray(prepareParams(dbName, tableName,\n          partNames, colNames));      \n      if (qResult == null) {\n        query.closeAll();\n        return Lists.newArrayList();\n      }\n      end = doTrace ? System.nanoTime() : 0;\n      timingTrace(doTrace, qText, start, end);\n      List<Object[]> list = ensureList(qResult);\n      List<ColumnStatisticsObj> colStats = new ArrayList<ColumnStatisticsObj>(\n          list.size());\n      for (Object[] row : list) {\n        colStats.add(prepareCSObj(row, 0));\n      }\n      query.closeAll();\n      return colStats;\n    } else {\n      // Extrapolation is needed for some columns.\n      // In this case, at least a column status for a partition is missing.\n      // We need to extrapolate this partition based on the other partitions\n      List<ColumnStatisticsObj> colStats = new ArrayList<ColumnStatisticsObj>(\n          colNames.size());\n      qText = \"select \\\"COLUMN_NAME\\\", \\\"COLUMN_TYPE\\\", count(\\\"PARTITION_NAME\\\") \"\n          + \" from \\\"PART_COL_STATS\\\"\"\n          + \" where \\\"DB_NAME\\\" = ? and \\\"TABLE_NAME\\\" = ? \"\n          + \" and \\\"COLUMN_NAME\\\" in (\" + makeParams(colNames.size()) + \")\"\n          + \" and \\\"PARTITION_NAME\\\" in (\" + makeParams(partNames.size()) + \")\"\n          + \" group by \\\"COLUMN_NAME\\\", \\\"COLUMN_TYPE\\\"\";\n      start = doTrace ? System.nanoTime() : 0;\n      query = pm.newQuery(\"javax.jdo.query.SQL\", qText);\n      qResult = query.executeWithArray(prepareParams(dbName, tableName,\n          partNames, colNames));\n      end = doTrace ? System.nanoTime() : 0;\n      timingTrace(doTrace, qText, start, end);\n      if (qResult == null) {\n        query.closeAll();\n        return Lists.newArrayList();\n      }\n      List<String> noExtraColumnNames = new ArrayList<String>();\n      Map<String, String[]> extraColumnNameTypeParts = new HashMap<String, String[]>();\n      List<Object[]> list = ensureList(qResult);\n      for (Object[] row : list) {\n        String colName = (String) row[0];\n        String colType = (String) row[1];\n        // Extrapolation is not needed for this column if\n        // count(\\\"PARTITION_NAME\\\")==partNames.size()\n        // Or, extrapolation is not possible for this column if\n        // count(\\\"PARTITION_NAME\\\")<2\n        Long count = StatObjectConverter.extractSqlLong(row[2]);\n        if (count == partNames.size() || count < 2) {\n          noExtraColumnNames.add(colName);\n        } else {\n          extraColumnNameTypeParts.put(colName, new String[] { colType, String.valueOf(count) });\n        }\n      }\n      query.closeAll();\n      // Extrapolation is not needed for columns noExtraColumnNames\n      if (noExtraColumnNames.size() != 0) {\n        qText = commonPrefix \n            + \" and \\\"COLUMN_NAME\\\" in (\"+ makeParams(noExtraColumnNames.size()) + \")\"\n            + \" and \\\"PARTITION_NAME\\\" in (\"+ makeParams(partNames.size()) +\")\"\n            + \" group by \\\"COLUMN_NAME\\\", \\\"COLUMN_TYPE\\\"\";\n        start = doTrace ? System.nanoTime() : 0;\n        query = pm.newQuery(\"javax.jdo.query.SQL\", qText);\n        qResult = query.executeWithArray(prepareParams(dbName, tableName,\n            partNames, noExtraColumnNames));\n        if (qResult == null) {\n          query.closeAll();\n          return Lists.newArrayList();\n        }\n        list = ensureList(qResult);\n        for (Object[] row : list) {\n          colStats.add(prepareCSObj(row, 0));\n        }\n        end = doTrace ? System.nanoTime() : 0;\n        timingTrace(doTrace, qText, start, end);\n        query.closeAll();\n      }\n      // Extrapolation is needed for extraColumnNames.\n      // give a sequence number for all the partitions\n      if (extraColumnNameTypeParts.size() != 0) {\n        Map<String, Integer> indexMap = new HashMap<String, Integer>();\n        for (int index = 0; index < partNames.size(); index++) {\n          indexMap.put(partNames.get(index), index);\n        }\n        // get sum for all columns to reduce the number of queries\n        Map<String, Map<Integer, Object>> sumMap = new HashMap<String, Map<Integer, Object>>();\n        qText = \"select \\\"COLUMN_NAME\\\", sum(\\\"NUM_NULLS\\\"), sum(\\\"NUM_TRUES\\\"), sum(\\\"NUM_FALSES\\\")\"\n            + \" from \\\"PART_COL_STATS\\\"\"\n            + \" where \\\"DB_NAME\\\" = ? and \\\"TABLE_NAME\\\" = ? \"\n            + \" and \\\"COLUMN_NAME\\\" in (\" +makeParams(extraColumnNameTypeParts.size())+ \")\"\n            + \" and \\\"PARTITION_NAME\\\" in (\" + makeParams(partNames.size()) + \")\"\n            + \" group by \\\"COLUMN_NAME\\\"\";\n        start = doTrace ? System.nanoTime() : 0;\n        query = pm.newQuery(\"javax.jdo.query.SQL\", qText);\n        List<String> extraColumnNames = new ArrayList<String>();\n        extraColumnNames.addAll(extraColumnNameTypeParts.keySet());\n        qResult = query.executeWithArray(prepareParams(dbName, tableName,\n            partNames, extraColumnNames));\n        if (qResult == null) {\n          query.closeAll();\n          return Lists.newArrayList();\n        }\n        list = ensureList(qResult);\n        // see the indexes for colstats in IExtrapolatePartStatus\n        Integer[] sumIndex = new Integer[] { 6, 10, 11 };\n        for (Object[] row : list) {\n          Map<Integer, Object> indexToObject = new HashMap<Integer, Object>();\n          for (int ind = 1; ind < row.length; ind++) {\n            indexToObject.put(sumIndex[ind - 1], row[ind]);\n          }\n          sumMap.put((String) row[0], indexToObject);\n        }\n        end = doTrace ? System.nanoTime() : 0;\n        timingTrace(doTrace, qText, start, end);\n        query.closeAll();\n        for (Map.Entry<String, String[]> entry : extraColumnNameTypeParts\n            .entrySet()) {\n          Object[] row = new Object[IExtrapolatePartStatus.colStatNames.length + 2];\n          String colName = entry.getKey();\n          String colType = entry.getValue()[0];\n          Long sumVal = Long.parseLong(entry.getValue()[1]);\n          // fill in colname\n          row[0] = colName;\n          // fill in coltype\n          row[1] = colType;\n          // use linear extrapolation. more complicated one can be added in the future.\n          IExtrapolatePartStatus extrapolateMethod = new LinearExtrapolatePartStatus();\n          // fill in colstatus\n          Integer[] index = IExtrapolatePartStatus.indexMaps.get(colType\n              .toLowerCase());\n          //if the colType is not the known type, long, double, etc, then get all index.\n          if (index == null) {\n            index = IExtrapolatePartStatus.indexMaps.get(\"default\");\n          }\n          for (int colStatIndex : index) {\n            String colStatName = IExtrapolatePartStatus.colStatNames[colStatIndex];\n            // if the aggregation type is sum, we do a scale-up\n            if (IExtrapolatePartStatus.aggrTypes[colStatIndex] == IExtrapolatePartStatus.AggrType.Sum) {\n              Object o = sumMap.get(colName).get(colStatIndex);\n              if (o == null) {\n                row[2 + colStatIndex] = null;\n              } else {\n                Long val = StatObjectConverter.extractSqlLong(o);\n                row[2 + colStatIndex] = (Long) (val / sumVal * (partNames.size()));\n              }\n            } else {\n              // if the aggregation type is min/max, we extrapolate from the\n              // left/right borders\n              qText = \"select \\\"\"\n                  + colStatName\n                  + \"\\\",\\\"PARTITION_NAME\\\" from \\\"PART_COL_STATS\\\"\"\n                  + \" where \\\"DB_NAME\\\" = ? and \\\"TABLE_NAME\\\" = ?\"\n                  + \" and \\\"COLUMN_NAME\\\" in (\" +makeParams(1)+ \")\"\n                  + \" and \\\"PARTITION_NAME\\\" in (\" + makeParams(partNames.size()) + \")\"\n                  + \" order by \\'\" + colStatName + \"\\'\";\n              start = doTrace ? System.nanoTime() : 0;\n              query = pm.newQuery(\"javax.jdo.query.SQL\", qText);\n              qResult = query.executeWithArray(prepareParams(dbName,\n                  tableName, partNames, Arrays.asList(colName)));\n              if (qResult == null) {\n                query.closeAll();\n                return Lists.newArrayList();\n              }\n              fqr = (ForwardQueryResult) qResult;\n              Object[] min = (Object[]) (fqr.get(0));\n              Object[] max = (Object[]) (fqr.get(fqr.size() - 1));\n              end = doTrace ? System.nanoTime() : 0;\n              timingTrace(doTrace, qText, start, end);\n              query.closeAll();\n              if (min[0] == null || max[0] == null) {\n                row[2 + colStatIndex] = null;\n              } else {\n                row[2 + colStatIndex] = extrapolateMethod.extrapolate(min, max,\n                    colStatIndex, indexMap);\n              }\n            }\n          }\n          colStats.add(prepareCSObj(row, 0));\n        }\n      }\n      return colStats;\n    }\n  }",
                "code_after_change": "  private List<ColumnStatisticsObj> columnStatisticsObjForPartitions(\n      String dbName, String tableName, List<String> partNames,\n      List<String> colNames, long partsFound) throws MetaException {\n    // TODO: all the extrapolation logic should be moved out of this class,\n    //       only mechanical data retrieval should remain here.\n    String commonPrefix = \"select \\\"COLUMN_NAME\\\", \\\"COLUMN_TYPE\\\", \"\n        + \"min(\\\"LONG_LOW_VALUE\\\"), max(\\\"LONG_HIGH_VALUE\\\"), min(\\\"DOUBLE_LOW_VALUE\\\"), max(\\\"DOUBLE_HIGH_VALUE\\\"), \"\n        + \"min(\\\"BIG_DECIMAL_LOW_VALUE\\\"), max(\\\"BIG_DECIMAL_HIGH_VALUE\\\"), sum(\\\"NUM_NULLS\\\"), max(\\\"NUM_DISTINCTS\\\"), \"\n        + \"max(\\\"AVG_COL_LEN\\\"), max(\\\"MAX_COL_LEN\\\"), sum(\\\"NUM_TRUES\\\"), sum(\\\"NUM_FALSES\\\") from \\\"PART_COL_STATS\\\"\"\n        + \" where \\\"DB_NAME\\\" = ? and \\\"TABLE_NAME\\\" = ? \";\n    String qText = null;\n    long start = 0;\n    long end = 0;\n    Query query = null;\n    boolean doTrace = LOG.isDebugEnabled();\n    Object qResult = null;\n    ForwardQueryResult fqr = null;\n    // Check if the status of all the columns of all the partitions exists\n    // Extrapolation is not needed.\n    if (partsFound == partNames.size()) {\n      qText = commonPrefix \n          + \" and \\\"COLUMN_NAME\\\" in (\" + makeParams(colNames.size()) + \")\"\n          + \" and \\\"PARTITION_NAME\\\" in (\" + makeParams(partNames.size()) + \")\"\n          + \" group by \\\"COLUMN_NAME\\\", \\\"COLUMN_TYPE\\\"\";\n      start = doTrace ? System.nanoTime() : 0;\n      query = pm.newQuery(\"javax.jdo.query.SQL\", qText);\n      qResult = query.executeWithArray(prepareParams(dbName, tableName,\n          partNames, colNames));      \n      if (qResult == null) {\n        query.closeAll();\n        return Lists.newArrayList();\n      }\n      end = doTrace ? System.nanoTime() : 0;\n      timingTrace(doTrace, qText, start, end);\n      List<Object[]> list = ensureList(qResult);\n      List<ColumnStatisticsObj> colStats = new ArrayList<ColumnStatisticsObj>(\n          list.size());\n      for (Object[] row : list) {\n        colStats.add(prepareCSObj(row, 0));\n      }\n      query.closeAll();\n      return colStats;\n    } else {\n      // Extrapolation is needed for some columns.\n      // In this case, at least a column status for a partition is missing.\n      // We need to extrapolate this partition based on the other partitions\n      List<ColumnStatisticsObj> colStats = new ArrayList<ColumnStatisticsObj>(\n          colNames.size());\n      qText = \"select \\\"COLUMN_NAME\\\", \\\"COLUMN_TYPE\\\", count(\\\"PARTITION_NAME\\\") \"\n          + \" from \\\"PART_COL_STATS\\\"\"\n          + \" where \\\"DB_NAME\\\" = ? and \\\"TABLE_NAME\\\" = ? \"\n          + \" and \\\"COLUMN_NAME\\\" in (\" + makeParams(colNames.size()) + \")\"\n          + \" and \\\"PARTITION_NAME\\\" in (\" + makeParams(partNames.size()) + \")\"\n          + \" group by \\\"COLUMN_NAME\\\", \\\"COLUMN_TYPE\\\"\";\n      start = doTrace ? System.nanoTime() : 0;\n      query = pm.newQuery(\"javax.jdo.query.SQL\", qText);\n      qResult = query.executeWithArray(prepareParams(dbName, tableName,\n          partNames, colNames));\n      end = doTrace ? System.nanoTime() : 0;\n      timingTrace(doTrace, qText, start, end);\n      if (qResult == null) {\n        query.closeAll();\n        return Lists.newArrayList();\n      }\n      List<String> noExtraColumnNames = new ArrayList<String>();\n      Map<String, String[]> extraColumnNameTypeParts = new HashMap<String, String[]>();\n      List<Object[]> list = ensureList(qResult);\n      for (Object[] row : list) {\n        String colName = (String) row[0];\n        String colType = (String) row[1];\n        // Extrapolation is not needed for this column if\n        // count(\\\"PARTITION_NAME\\\")==partNames.size()\n        // Or, extrapolation is not possible for this column if\n        // count(\\\"PARTITION_NAME\\\")<2\n        Long count = extractSqlLong(row[2]);\n        if (count == partNames.size() || count < 2) {\n          noExtraColumnNames.add(colName);\n        } else {\n          extraColumnNameTypeParts.put(colName, new String[] { colType, String.valueOf(count) });\n        }\n      }\n      query.closeAll();\n      // Extrapolation is not needed for columns noExtraColumnNames\n      if (noExtraColumnNames.size() != 0) {\n        qText = commonPrefix \n            + \" and \\\"COLUMN_NAME\\\" in (\"+ makeParams(noExtraColumnNames.size()) + \")\"\n            + \" and \\\"PARTITION_NAME\\\" in (\"+ makeParams(partNames.size()) +\")\"\n            + \" group by \\\"COLUMN_NAME\\\", \\\"COLUMN_TYPE\\\"\";\n        start = doTrace ? System.nanoTime() : 0;\n        query = pm.newQuery(\"javax.jdo.query.SQL\", qText);\n        qResult = query.executeWithArray(prepareParams(dbName, tableName,\n            partNames, noExtraColumnNames));\n        if (qResult == null) {\n          query.closeAll();\n          return Lists.newArrayList();\n        }\n        list = ensureList(qResult);\n        for (Object[] row : list) {\n          colStats.add(prepareCSObj(row, 0));\n        }\n        end = doTrace ? System.nanoTime() : 0;\n        timingTrace(doTrace, qText, start, end);\n        query.closeAll();\n      }\n      // Extrapolation is needed for extraColumnNames.\n      // give a sequence number for all the partitions\n      if (extraColumnNameTypeParts.size() != 0) {\n        Map<String, Integer> indexMap = new HashMap<String, Integer>();\n        for (int index = 0; index < partNames.size(); index++) {\n          indexMap.put(partNames.get(index), index);\n        }\n        // get sum for all columns to reduce the number of queries\n        Map<String, Map<Integer, Object>> sumMap = new HashMap<String, Map<Integer, Object>>();\n        qText = \"select \\\"COLUMN_NAME\\\", sum(\\\"NUM_NULLS\\\"), sum(\\\"NUM_TRUES\\\"), sum(\\\"NUM_FALSES\\\")\"\n            + \" from \\\"PART_COL_STATS\\\"\"\n            + \" where \\\"DB_NAME\\\" = ? and \\\"TABLE_NAME\\\" = ? \"\n            + \" and \\\"COLUMN_NAME\\\" in (\" +makeParams(extraColumnNameTypeParts.size())+ \")\"\n            + \" and \\\"PARTITION_NAME\\\" in (\" + makeParams(partNames.size()) + \")\"\n            + \" group by \\\"COLUMN_NAME\\\"\";\n        start = doTrace ? System.nanoTime() : 0;\n        query = pm.newQuery(\"javax.jdo.query.SQL\", qText);\n        List<String> extraColumnNames = new ArrayList<String>();\n        extraColumnNames.addAll(extraColumnNameTypeParts.keySet());\n        qResult = query.executeWithArray(prepareParams(dbName, tableName,\n            partNames, extraColumnNames));\n        if (qResult == null) {\n          query.closeAll();\n          return Lists.newArrayList();\n        }\n        list = ensureList(qResult);\n        // see the indexes for colstats in IExtrapolatePartStatus\n        Integer[] sumIndex = new Integer[] { 6, 10, 11 };\n        for (Object[] row : list) {\n          Map<Integer, Object> indexToObject = new HashMap<Integer, Object>();\n          for (int ind = 1; ind < row.length; ind++) {\n            indexToObject.put(sumIndex[ind - 1], row[ind]);\n          }\n          sumMap.put((String) row[0], indexToObject);\n        }\n        end = doTrace ? System.nanoTime() : 0;\n        timingTrace(doTrace, qText, start, end);\n        query.closeAll();\n        for (Map.Entry<String, String[]> entry : extraColumnNameTypeParts\n            .entrySet()) {\n          Object[] row = new Object[IExtrapolatePartStatus.colStatNames.length + 2];\n          String colName = entry.getKey();\n          String colType = entry.getValue()[0];\n          Long sumVal = Long.parseLong(entry.getValue()[1]);\n          // fill in colname\n          row[0] = colName;\n          // fill in coltype\n          row[1] = colType;\n          // use linear extrapolation. more complicated one can be added in the future.\n          IExtrapolatePartStatus extrapolateMethod = new LinearExtrapolatePartStatus();\n          // fill in colstatus\n          Integer[] index = IExtrapolatePartStatus.indexMaps.get(colType\n              .toLowerCase());\n          //if the colType is not the known type, long, double, etc, then get all index.\n          if (index == null) {\n            index = IExtrapolatePartStatus.indexMaps.get(\"default\");\n          }\n          for (int colStatIndex : index) {\n            String colStatName = IExtrapolatePartStatus.colStatNames[colStatIndex];\n            // if the aggregation type is sum, we do a scale-up\n            if (IExtrapolatePartStatus.aggrTypes[colStatIndex] == IExtrapolatePartStatus.AggrType.Sum) {\n              Object o = sumMap.get(colName).get(colStatIndex);\n              if (o == null) {\n                row[2 + colStatIndex] = null;\n              } else {\n                Long val = extractSqlLong(o);\n                row[2 + colStatIndex] = (Long) (val / sumVal * (partNames.size()));\n              }\n            } else {\n              // if the aggregation type is min/max, we extrapolate from the\n              // left/right borders\n              qText = \"select \\\"\"\n                  + colStatName\n                  + \"\\\",\\\"PARTITION_NAME\\\" from \\\"PART_COL_STATS\\\"\"\n                  + \" where \\\"DB_NAME\\\" = ? and \\\"TABLE_NAME\\\" = ?\"\n                  + \" and \\\"COLUMN_NAME\\\" = ?\"\n                  + \" and \\\"PARTITION_NAME\\\" in (\" + makeParams(partNames.size()) + \")\"\n                  + \" order by \\'\" + colStatName + \"\\'\";\n              start = doTrace ? System.nanoTime() : 0;\n              query = pm.newQuery(\"javax.jdo.query.SQL\", qText);\n              qResult = query.executeWithArray(prepareParams(dbName,\n                  tableName, partNames, Arrays.asList(colName)));\n              if (qResult == null) {\n                query.closeAll();\n                return Lists.newArrayList();\n              }\n              fqr = (ForwardQueryResult) qResult;\n              Object[] min = (Object[]) (fqr.get(0));\n              Object[] max = (Object[]) (fqr.get(fqr.size() - 1));\n              end = doTrace ? System.nanoTime() : 0;\n              timingTrace(doTrace, qText, start, end);\n              query.closeAll();\n              if (min[0] == null || max[0] == null) {\n                row[2 + colStatIndex] = null;\n              } else {\n                row[2 + colStatIndex] = extrapolateMethod.extrapolate(min, max,\n                    colStatIndex, indexMap);\n              }\n            }\n          }\n          colStats.add(prepareCSObj(row, 0));\n        }\n      }\n      return colStats;\n    }\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.StatObjectConverter.fillColumnStatisticsData": {
                "code_before_change": "  public static void fillColumnStatisticsData(String colType, ColumnStatisticsData data,\n      Object llow, Object lhigh, Object dlow, Object dhigh, Object declow, Object dechigh,\n      Object nulls, Object dist, Object avglen, Object maxlen, Object trues, Object falses) throws MetaException {\n    if (colType.equals(\"boolean\")) {\n      BooleanColumnStatsData boolStats = new BooleanColumnStatsData();\n      boolStats.setNumFalses(extractSqlLong(falses));\n      boolStats.setNumTrues(extractSqlLong(trues));\n      boolStats.setNumNulls(extractSqlLong(nulls));\n      data.setBooleanStats(boolStats);\n    } else if (colType.equals(\"string\") ||\n        colType.startsWith(\"varchar\") || colType.startsWith(\"char\")) {\n      StringColumnStatsData stringStats = new StringColumnStatsData();\n      stringStats.setNumNulls(extractSqlLong(nulls));\n      stringStats.setAvgColLen((Double)avglen);\n      stringStats.setMaxColLen(extractSqlLong(maxlen));\n      stringStats.setNumDVs(extractSqlLong(dist));\n      data.setStringStats(stringStats);\n    } else if (colType.equals(\"binary\")) {\n      BinaryColumnStatsData binaryStats = new BinaryColumnStatsData();\n      binaryStats.setNumNulls(extractSqlLong(nulls));\n      binaryStats.setAvgColLen((Double)avglen);\n      binaryStats.setMaxColLen(extractSqlLong(maxlen));\n      data.setBinaryStats(binaryStats);\n    } else if (colType.equals(\"bigint\") || colType.equals(\"int\") ||\n        colType.equals(\"smallint\") || colType.equals(\"tinyint\") ||\n        colType.equals(\"timestamp\")) {\n      LongColumnStatsData longStats = new LongColumnStatsData();\n      longStats.setNumNulls(extractSqlLong(nulls));\n      if (lhigh != null) {\n        longStats.setHighValue(extractSqlLong(lhigh));\n      }\n      if (llow != null) {\n        longStats.setLowValue(extractSqlLong(llow));\n      }\n      longStats.setNumDVs(extractSqlLong(dist));\n      data.setLongStats(longStats);\n    } else if (colType.equals(\"double\") || colType.equals(\"float\")) {\n      DoubleColumnStatsData doubleStats = new DoubleColumnStatsData();\n      doubleStats.setNumNulls(extractSqlLong(nulls));\n      if (dhigh != null) {\n        doubleStats.setHighValue((Double)dhigh);\n      }\n      if (dlow != null) {\n        doubleStats.setLowValue((Double)dlow);\n      }\n      doubleStats.setNumDVs(extractSqlLong(dist));\n      data.setDoubleStats(doubleStats);\n    } else if (colType.startsWith(\"decimal\")) {\n      DecimalColumnStatsData decimalStats = new DecimalColumnStatsData();\n      decimalStats.setNumNulls(extractSqlLong(nulls));\n      if (dechigh != null) {\n        decimalStats.setHighValue(createThriftDecimal((String)dechigh));\n      }\n      if (declow != null) {\n        decimalStats.setLowValue(createThriftDecimal((String)declow));\n      }\n      decimalStats.setNumDVs(extractSqlLong(dist));\n      data.setDecimalStats(decimalStats);\n    }\n  }",
                "code_after_change": "  public static void fillColumnStatisticsData(String colType, ColumnStatisticsData data,\n      Object llow, Object lhigh, Object dlow, Object dhigh, Object declow, Object dechigh,\n      Object nulls, Object dist, Object avglen, Object maxlen, Object trues, Object falses) throws MetaException {\n    colType = colType.toLowerCase();\n    if (colType.equals(\"boolean\")) {\n      BooleanColumnStatsData boolStats = new BooleanColumnStatsData();\n      boolStats.setNumFalses(MetaStoreDirectSql.extractSqlLong(falses));\n      boolStats.setNumTrues(MetaStoreDirectSql.extractSqlLong(trues));\n      boolStats.setNumNulls(MetaStoreDirectSql.extractSqlLong(nulls));\n      data.setBooleanStats(boolStats);\n    } else if (colType.equals(\"string\") ||\n        colType.startsWith(\"varchar\") || colType.startsWith(\"char\")) {\n      StringColumnStatsData stringStats = new StringColumnStatsData();\n      stringStats.setNumNulls(MetaStoreDirectSql.extractSqlLong(nulls));\n      stringStats.setAvgColLen((Double)avglen);\n      stringStats.setMaxColLen(MetaStoreDirectSql.extractSqlLong(maxlen));\n      stringStats.setNumDVs(MetaStoreDirectSql.extractSqlLong(dist));\n      data.setStringStats(stringStats);\n    } else if (colType.equals(\"binary\")) {\n      BinaryColumnStatsData binaryStats = new BinaryColumnStatsData();\n      binaryStats.setNumNulls(MetaStoreDirectSql.extractSqlLong(nulls));\n      binaryStats.setAvgColLen((Double)avglen);\n      binaryStats.setMaxColLen(MetaStoreDirectSql.extractSqlLong(maxlen));\n      data.setBinaryStats(binaryStats);\n    } else if (colType.equals(\"bigint\") || colType.equals(\"int\") ||\n        colType.equals(\"smallint\") || colType.equals(\"tinyint\") ||\n        colType.equals(\"timestamp\")) {\n      LongColumnStatsData longStats = new LongColumnStatsData();\n      longStats.setNumNulls(MetaStoreDirectSql.extractSqlLong(nulls));\n      if (lhigh != null) {\n        longStats.setHighValue(MetaStoreDirectSql.extractSqlLong(lhigh));\n      }\n      if (llow != null) {\n        longStats.setLowValue(MetaStoreDirectSql.extractSqlLong(llow));\n      }\n      longStats.setNumDVs(MetaStoreDirectSql.extractSqlLong(dist));\n      data.setLongStats(longStats);\n    } else if (colType.equals(\"double\") || colType.equals(\"float\")) {\n      DoubleColumnStatsData doubleStats = new DoubleColumnStatsData();\n      doubleStats.setNumNulls(MetaStoreDirectSql.extractSqlLong(nulls));\n      if (dhigh != null) {\n        doubleStats.setHighValue((Double)dhigh);\n      }\n      if (dlow != null) {\n        doubleStats.setLowValue((Double)dlow);\n      }\n      doubleStats.setNumDVs(MetaStoreDirectSql.extractSqlLong(dist));\n      data.setDoubleStats(doubleStats);\n    } else if (colType.startsWith(\"decimal\")) {\n      DecimalColumnStatsData decimalStats = new DecimalColumnStatsData();\n      decimalStats.setNumNulls(MetaStoreDirectSql.extractSqlLong(nulls));\n      if (dechigh != null) {\n        decimalStats.setHighValue(createThriftDecimal((String)dechigh));\n      }\n      if (declow != null) {\n        decimalStats.setLowValue(createThriftDecimal((String)declow));\n      }\n      decimalStats.setNumDVs(MetaStoreDirectSql.extractSqlLong(dist));\n      data.setDecimalStats(decimalStats);\n    }\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.initialize": {
                "code_before_change": "  private void initialize(Properties dsProps) {\n    LOG.info(\"ObjectStore, initialize called\");\n    prop = dsProps;\n    pm = getPersistenceManager();\n    isInitialized = pm != null;\n    if (isInitialized) {\n      expressionProxy = createExpressionProxy(hiveConf);\n      directSql = new MetaStoreDirectSql(pm);\n    }\n    LOG.debug(\"RawStore: \" + this + \", with PersistenceManager: \" + pm +\n        \" created in the thread with id: \" + Thread.currentThread().getId());\n  }",
                "code_after_change": "  private void initialize(Properties dsProps) {\n    LOG.info(\"ObjectStore, initialize called\");\n    prop = dsProps;\n    pm = getPersistenceManager();\n    isInitialized = pm != null;\n    if (isInitialized) {\n      expressionProxy = createExpressionProxy(hiveConf);\n      directSql = new MetaStoreDirectSql(pm, hiveConf);\n    }\n    LOG.debug(\"RawStore: \" + this + \", with PersistenceManager: \" + pm +\n        \" created in the thread with id: \" + Thread.currentThread().getId());\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Correct",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the issue with the SQL query failing due to a limit on the number of expressions in a list, which is related to the methods in the stack trace, particularly 'getPartitionsViaSqlFilterInternal'. However, it does not pinpoint the exact root cause in the ground truth methods, hence 'Partial' with 'Shared Stack Trace Context'. The fix suggestion to retrieve partition objects in batches aligns with the developer's fix, which involves batching in 'getPartitionsViaSqlFilterInternal', thus 'Correct'. The problem location is also 'Partial' with 'Shared Stack Trace Context' as it mentions 'MetaStoreDirectSql' methods in the stack trace but not the exact ground truth methods. There is no wrong information as the report accurately describes the problem and its context."
        }
    },
    {
        "filename": "HIVE-8915.json",
        "code_diff": {
            "ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.Cleaner.run": {
                "code_before_change": "  public void run() {\n    if (cleanerCheckInterval == 0) {\n      cleanerCheckInterval = conf.getTimeVar(\n          HiveConf.ConfVars.HIVE_COMPACTOR_CLEANER_RUN_INTERVAL, TimeUnit.MILLISECONDS);\n    }\n\n    do {\n      // This is solely for testing.  It checks if the test has set the looped value to false,\n      // and if so remembers that and then sets it to true at the end.  We have to check here\n      // first to make sure we go through a complete iteration of the loop before resetting it.\n      boolean setLooped = !looped.get();\n      // Make sure nothing escapes this run method and kills the metastore at large,\n      // so wrap it in a big catch Throwable statement.\n      try {\n        long startedAt = System.currentTimeMillis();\n\n        // First look for all the compactions that are waiting to be cleaned.  If we have not\n        // seen an entry before, look for all the locks held on that table or partition and\n        // record them.  We will then only clean the partition once all of those locks have been\n        // released.  This way we avoid removing the files while they are in use,\n        // while at the same time avoiding starving the cleaner as new readers come along.\n        // This works because we know that any reader who comes along after the worker thread has\n        // done the compaction will read the more up to date version of the data (either in a\n        // newer delta or in a newer base).\n        List<CompactionInfo> toClean = txnHandler.findReadyToClean();\n        if (toClean.size() > 0 || compactId2LockMap.size() > 0) {\n          ShowLocksResponse locksResponse = txnHandler.showLocks(new ShowLocksRequest());\n\n          for (CompactionInfo ci : toClean) {\n            // Check to see if we have seen this request before.  If so, ignore it.  If not,\n            // add it to our queue.\n            if (!compactId2LockMap.containsKey(ci.id)) {\n              compactId2LockMap.put(ci.id, findRelatedLocks(ci, locksResponse));\n              compactId2CompactInfoMap.put(ci.id, ci);\n            }\n          }\n\n          // Now, for each entry in the queue, see if all of the associated locks are clear so we\n          // can clean\n          Set<Long> currentLocks = buildCurrentLockSet(locksResponse);\n          List<Long> expiredLocks = new ArrayList<Long>();\n          List<Long> compactionsCleaned = new ArrayList<Long>();\n          try {\n            for (Map.Entry<Long, Set<Long>> queueEntry : compactId2LockMap.entrySet()) {\n              boolean sawLock = false;\n              for (Long lockId : queueEntry.getValue()) {\n                if (currentLocks.contains(lockId)) {\n                  sawLock = true;\n                  break;\n                } else {\n                  expiredLocks.add(lockId);\n                }\n              }\n\n              if (!sawLock) {\n                // Remember to remove this when we're out of the loop,\n                // we can't do it in the loop or we'll get a concurrent modification exception.\n                compactionsCleaned.add(queueEntry.getKey());\n                clean(compactId2CompactInfoMap.get(queueEntry.getKey()));\n              } else {\n                // Remove the locks we didn't see so we don't look for them again next time\n                for (Long lockId : expiredLocks) {\n                  queueEntry.getValue().remove(lockId);\n                }\n              }\n            }\n          } finally {\n            if (compactionsCleaned.size() > 0) {\n              for (Long compactId : compactionsCleaned) {\n                compactId2LockMap.remove(compactId);\n                compactId2CompactInfoMap.remove(compactId);\n              }\n            }\n          }\n        }\n\n        // Now, go back to bed until it's time to do this again\n        long elapsedTime = System.currentTimeMillis() - startedAt;\n        if (elapsedTime >= cleanerCheckInterval || stop.get())  continue;\n        else Thread.sleep(cleanerCheckInterval - elapsedTime);\n      } catch (Throwable t) {\n        LOG.error(\"Caught an exception in the main loop of compactor cleaner, \" +\n            StringUtils.stringifyException(t));\n      }\n      if (setLooped) {\n        looped.set(true);\n      }\n    } while (!stop.get());\n  }",
                "code_after_change": "  public void run() {\n    if (cleanerCheckInterval == 0) {\n      cleanerCheckInterval = conf.getTimeVar(\n          HiveConf.ConfVars.HIVE_COMPACTOR_CLEANER_RUN_INTERVAL, TimeUnit.MILLISECONDS);\n    }\n\n    do {\n      // This is solely for testing.  It checks if the test has set the looped value to false,\n      // and if so remembers that and then sets it to true at the end.  We have to check here\n      // first to make sure we go through a complete iteration of the loop before resetting it.\n      boolean setLooped = !looped.get();\n      long startedAt = System.currentTimeMillis();\n      // Make sure nothing escapes this run method and kills the metastore at large,\n      // so wrap it in a big catch Throwable statement.\n      try {\n\n        // First look for all the compactions that are waiting to be cleaned.  If we have not\n        // seen an entry before, look for all the locks held on that table or partition and\n        // record them.  We will then only clean the partition once all of those locks have been\n        // released.  This way we avoid removing the files while they are in use,\n        // while at the same time avoiding starving the cleaner as new readers come along.\n        // This works because we know that any reader who comes along after the worker thread has\n        // done the compaction will read the more up to date version of the data (either in a\n        // newer delta or in a newer base).\n        List<CompactionInfo> toClean = txnHandler.findReadyToClean();\n        if (toClean.size() > 0 || compactId2LockMap.size() > 0) {\n          ShowLocksResponse locksResponse = txnHandler.showLocks(new ShowLocksRequest());\n\n          for (CompactionInfo ci : toClean) {\n            // Check to see if we have seen this request before.  If so, ignore it.  If not,\n            // add it to our queue.\n            if (!compactId2LockMap.containsKey(ci.id)) {\n              compactId2LockMap.put(ci.id, findRelatedLocks(ci, locksResponse));\n              compactId2CompactInfoMap.put(ci.id, ci);\n            }\n          }\n\n          // Now, for each entry in the queue, see if all of the associated locks are clear so we\n          // can clean\n          Set<Long> currentLocks = buildCurrentLockSet(locksResponse);\n          List<Long> expiredLocks = new ArrayList<Long>();\n          List<Long> compactionsCleaned = new ArrayList<Long>();\n          try {\n            for (Map.Entry<Long, Set<Long>> queueEntry : compactId2LockMap.entrySet()) {\n              boolean sawLock = false;\n              for (Long lockId : queueEntry.getValue()) {\n                if (currentLocks.contains(lockId)) {\n                  sawLock = true;\n                  break;\n                } else {\n                  expiredLocks.add(lockId);\n                }\n              }\n\n              if (!sawLock) {\n                // Remember to remove this when we're out of the loop,\n                // we can't do it in the loop or we'll get a concurrent modification exception.\n                compactionsCleaned.add(queueEntry.getKey());\n                clean(compactId2CompactInfoMap.get(queueEntry.getKey()));\n              } else {\n                // Remove the locks we didn't see so we don't look for them again next time\n                for (Long lockId : expiredLocks) {\n                  queueEntry.getValue().remove(lockId);\n                }\n              }\n            }\n          } finally {\n            if (compactionsCleaned.size() > 0) {\n              for (Long compactId : compactionsCleaned) {\n                compactId2LockMap.remove(compactId);\n                compactId2CompactInfoMap.remove(compactId);\n              }\n            }\n          }\n        }\n      } catch (Throwable t) {\n        LOG.error(\"Caught an exception in the main loop of compactor cleaner, \" +\n            StringUtils.stringifyException(t));\n      }\n      if (setLooped) {\n        looped.set(true);\n      }\n      // Now, go back to bed until it's time to do this again\n      long elapsedTime = System.currentTimeMillis() - startedAt;\n      if (elapsedTime >= cleanerCheckInterval || stop.get())  {\n        continue;\n      } else {\n        try {\n          Thread.sleep(cleanerCheckInterval - elapsedTime);\n        } catch (InterruptedException ie) {\n          // What can I do about it?\n        }\n      }\n    } while (!stop.get());\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.Worker.hostname": {
                "code_before_change": "  public static String hostname() {\n    try {\n      return InetAddress.getLocalHost().getHostName();\n    } catch (UnknownHostException e) {\n      LOG.error(\"Unable to resolve my host name \" + e.getMessage());\n      throw new RuntimeException(e);\n    }\n  }",
                "code_after_change": "  public static String hostname() {\n    try {\n      return InetAddress.getLocalHost().getHostName();\n    } catch (UnknownHostException e) {\n      LOG.error(\"Unable to resolve my host name \" + e.getMessage());\n      throw new RuntimeException(e);\n    }\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.Worker.run": {
                "code_before_change": "  public void run() {\n    // Make sure nothing escapes this run method and kills the metastore at large,\n    // so wrap it in a big catch Throwable statement.\n    try {\n      do {\n        CompactionInfo ci = txnHandler.findNextToCompact(name);\n\n        if (ci == null && !stop.get()) {\n          try {\n            Thread.sleep(SLEEP_TIME);\n            continue;\n          } catch (InterruptedException e) {\n            LOG.warn(\"Worker thread sleep interrupted \" + e.getMessage());\n            continue;\n          }\n        }\n\n        // Find the table we will be working with.\n        Table t1 = null;\n        try {\n          t1 = resolveTable(ci);\n        } catch (MetaException e) {\n          txnHandler.markCleaned(ci);\n          continue;\n        }\n        // This chicanery is to get around the fact that the table needs to be final in order to\n        // go into the doAs below.\n        final Table t = t1;\n\n        // Find the partition we will be working with, if there is one.\n        Partition p = null;\n        try {\n          p = resolvePartition(ci);\n        } catch (Exception e) {\n          txnHandler.markCleaned(ci);\n          continue;\n        }\n\n        // Find the appropriate storage descriptor\n        final StorageDescriptor sd =  resolveStorageDescriptor(t, p);\n\n        // Check that the table or partition isn't sorted, as we don't yet support that.\n        if (sd.getSortCols() != null && !sd.getSortCols().isEmpty()) {\n          LOG.error(\"Attempt to compact sorted table, which is not yet supported!\");\n          txnHandler.markCleaned(ci);\n          continue;\n        }\n\n        final boolean isMajor = ci.isMajorCompaction();\n        final ValidTxnList txns =\n            TxnHandler.createValidTxnList(txnHandler.getOpenTxns(), 0);\n        final StringBuffer jobName = new StringBuffer(name);\n        jobName.append(\"-compactor-\");\n        jobName.append(ci.getFullPartitionName());\n\n        // Determine who to run as\n        String runAs;\n        if (ci.runAs == null) {\n          runAs = findUserToRunAs(sd.getLocation(), t);\n          txnHandler.setRunAs(ci.id, runAs);\n        } else {\n          runAs = ci.runAs;\n        }\n\n        LOG.info(\"Starting \" + ci.type.toString() + \" compaction for \" +\n            ci.getFullPartitionName());\n\n        final StatsUpdater su = StatsUpdater.init(ci, txnHandler.findColumnsWithStats(ci), conf,\n          runJobAsSelf(runAs) ? runAs : t.getOwner());\n        final CompactorMR mr = new CompactorMR();\n        try {\n          if (runJobAsSelf(runAs)) {\n            mr.run(conf, jobName.toString(), t, sd, txns, isMajor, su);\n          } else {\n            UserGroupInformation ugi = UserGroupInformation.createProxyUser(t.getOwner(),\n              UserGroupInformation.getLoginUser());\n            ugi.doAs(new PrivilegedExceptionAction<Object>() {\n              @Override\n              public Object run() throws Exception {\n                mr.run(conf, jobName.toString(), t, sd, txns, isMajor, su);\n                return null;\n              }\n            });\n          }\n          txnHandler.markCompacted(ci);\n        } catch (Exception e) {\n          LOG.error(\"Caught exception while trying to compact \" + ci.getFullPartitionName() +\n              \".  Marking clean to avoid repeated failures, \" + StringUtils.stringifyException(e));\n          txnHandler.markCleaned(ci);\n        }\n      } while (!stop.get());\n    } catch (Throwable t) {\n      LOG.error(\"Caught an exception in the main loop of compactor worker \" + name +\n          \", exiting \" + StringUtils.stringifyException(t));\n    }\n  }",
                "code_after_change": "  public void run() {\n    do {\n      boolean launchedJob = false;\n      // Make sure nothing escapes this run method and kills the metastore at large,\n      // so wrap it in a big catch Throwable statement.\n      try {\n        CompactionInfo ci = txnHandler.findNextToCompact(name);\n\n        if (ci == null && !stop.get()) {\n          try {\n            Thread.sleep(SLEEP_TIME);\n            continue;\n          } catch (InterruptedException e) {\n            LOG.warn(\"Worker thread sleep interrupted \" + e.getMessage());\n            continue;\n          }\n        }\n\n        // Find the table we will be working with.\n        Table t1 = null;\n        try {\n          t1 = resolveTable(ci);\n        } catch (MetaException e) {\n          txnHandler.markCleaned(ci);\n          continue;\n        }\n        // This chicanery is to get around the fact that the table needs to be final in order to\n        // go into the doAs below.\n        final Table t = t1;\n\n        // Find the partition we will be working with, if there is one.\n        Partition p = null;\n        try {\n          p = resolvePartition(ci);\n        } catch (Exception e) {\n          txnHandler.markCleaned(ci);\n          continue;\n        }\n\n        // Find the appropriate storage descriptor\n        final StorageDescriptor sd =  resolveStorageDescriptor(t, p);\n\n        // Check that the table or partition isn't sorted, as we don't yet support that.\n        if (sd.getSortCols() != null && !sd.getSortCols().isEmpty()) {\n          LOG.error(\"Attempt to compact sorted table, which is not yet supported!\");\n          txnHandler.markCleaned(ci);\n          continue;\n        }\n\n        final boolean isMajor = ci.isMajorCompaction();\n        final ValidTxnList txns =\n            CompactionTxnHandler.createValidCompactTxnList(txnHandler.getOpenTxnsInfo());\n        LOG.debug(\"ValidCompactTxnList: \" + txns.writeToString());\n        final StringBuffer jobName = new StringBuffer(name);\n        jobName.append(\"-compactor-\");\n        jobName.append(ci.getFullPartitionName());\n\n        // Determine who to run as\n        String runAs;\n        if (ci.runAs == null) {\n          runAs = findUserToRunAs(sd.getLocation(), t);\n          txnHandler.setRunAs(ci.id, runAs);\n        } else {\n          runAs = ci.runAs;\n        }\n\n        LOG.info(\"Starting \" + ci.type.toString() + \" compaction for \" +\n            ci.getFullPartitionName());\n\n        final StatsUpdater su = StatsUpdater.init(ci, txnHandler.findColumnsWithStats(ci), conf,\n          runJobAsSelf(runAs) ? runAs : t.getOwner());\n        final CompactorMR mr = new CompactorMR();\n        launchedJob = true;\n        try {\n          if (runJobAsSelf(runAs)) {\n            mr.run(conf, jobName.toString(), t, sd, txns, isMajor, su);\n          } else {\n            UserGroupInformation ugi = UserGroupInformation.createProxyUser(t.getOwner(),\n              UserGroupInformation.getLoginUser());\n            ugi.doAs(new PrivilegedExceptionAction<Object>() {\n              @Override\n              public Object run() throws Exception {\n                mr.run(conf, jobName.toString(), t, sd, txns, isMajor, su);\n                return null;\n              }\n            });\n          }\n          txnHandler.markCompacted(ci);\n        } catch (Exception e) {\n          LOG.error(\"Caught exception while trying to compact \" + ci.getFullPartitionName() +\n              \".  Marking clean to avoid repeated failures, \" + StringUtils.stringifyException(e));\n          txnHandler.markCleaned(ci);\n        }\n      } catch (Throwable t) {\n        LOG.error(\"Caught an exception in the main loop of compactor worker \" + name + \", \" +\n            StringUtils.stringifyException(t));\n      }\n\n      // If we didn't try to launch a job it either means there was no work to do or we got\n      // here as the result of a communication failure with the DB.  Either way we want to wait\n      // a bit before we restart the loop.\n      if (!launchedJob && !stop.get()) {\n        try {\n          Thread.sleep(SLEEP_TIME);\n        } catch (InterruptedException e) {\n        }\n      }\n    } while (!stop.get());\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Preventive",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the issue as an endless loop of errors due to the non-existence of the 'COMPACTION_QUEUE' table, which is related to the stack trace context but not the precise root cause in the ground truth methods. The suggestion to add a delay or fail if unable to start is a preventive measure to mitigate the issue, though it doesn't match the exact developer fix. The problem location is partially identified as it mentions the stack trace context, which includes the ground truth method 'Cleaner.run'. There is no wrong information as the report accurately describes the symptoms and context of the bug."
        }
    },
    {
        "filename": "HIVE-7249.json",
        "code_diff": {
            "ql.src.java.org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.commitTxn": {
                "code_before_change": "  public void commitTxn() throws LockException {\n    if (txnId == 0) {\n      throw new RuntimeException(\"Attempt to commit before opening a \" +\n          \"transaction\");\n    }\n    try {\n      LOG.debug(\"Committing txn \" + txnId);\n      client.commitTxn(txnId);\n    } catch (NoSuchTxnException e) {\n      LOG.error(\"Metastore could not find txn \" + txnId);\n      throw new LockException(ErrorMsg.TXN_NO_SUCH_TRANSACTION.getMsg() , e);\n    } catch (TxnAbortedException e) {\n      LOG.error(\"Transaction \" + txnId + \" aborted\");\n      throw new LockException(ErrorMsg.TXN_ABORTED.getMsg(), e);\n    } catch (TException e) {\n      throw new LockException(ErrorMsg.METASTORE_COMMUNICATION_FAILED.getMsg(),\n          e);\n    } finally {\n      txnId = 0;\n    }\n  }",
                "code_after_change": "  public void commitTxn() throws LockException {\n    if (txnId == 0) {\n      throw new RuntimeException(\"Attempt to commit before opening a \" +\n          \"transaction\");\n    }\n    try {\n      lockMgr.clearLocalLockRecords();\n      LOG.debug(\"Committing txn \" + txnId);\n      client.commitTxn(txnId);\n    } catch (NoSuchTxnException e) {\n      LOG.error(\"Metastore could not find txn \" + txnId);\n      throw new LockException(ErrorMsg.TXN_NO_SUCH_TRANSACTION.getMsg() , e);\n    } catch (TxnAbortedException e) {\n      LOG.error(\"Transaction \" + txnId + \" aborted\");\n      throw new LockException(ErrorMsg.TXN_ABORTED.getMsg(), e);\n    } catch (TException e) {\n      throw new LockException(ErrorMsg.METASTORE_COMMUNICATION_FAILED.getMsg(),\n          e);\n    } finally {\n      txnId = 0;\n    }\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.lockmgr.DbTxnManager.rollbackTxn": {
                "code_before_change": "  public void rollbackTxn() throws LockException {\n    if (txnId == 0) {\n      throw new RuntimeException(\"Attempt to rollback before opening a \" +\n          \"transaction\");\n    }\n    try {\n      LOG.debug(\"Rolling back txn \" + txnId);\n      client.rollbackTxn(txnId);\n    } catch (NoSuchTxnException e) {\n      LOG.error(\"Metastore could not find txn \" + txnId);\n      throw new LockException(ErrorMsg.TXN_NO_SUCH_TRANSACTION.getMsg() , e);\n    } catch (TException e) {\n      throw new LockException(ErrorMsg.METASTORE_COMMUNICATION_FAILED.getMsg(),\n          e);\n    } finally {\n      txnId = 0;\n    }\n  }",
                "code_after_change": "  public void rollbackTxn() throws LockException {\n    if (txnId == 0) {\n      throw new RuntimeException(\"Attempt to rollback before opening a \" +\n          \"transaction\");\n    }\n    try {\n      lockMgr.clearLocalLockRecords();\n      LOG.debug(\"Rolling back txn \" + txnId);\n      client.rollbackTxn(txnId);\n    } catch (NoSuchTxnException e) {\n      LOG.error(\"Metastore could not find txn \" + txnId);\n      throw new LockException(ErrorMsg.TXN_NO_SUCH_TRANSACTION.getMsg() , e);\n    } catch (TException e) {\n      throw new LockException(ErrorMsg.METASTORE_COMMUNICATION_FAILED.getMsg(),\n          e);\n    } finally {\n      txnId = 0;\n    }\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.lockmgr.DbLockManager.hashCode": {
                "code_before_change": "    public int hashCode() {\n      return (int)(lockId % Integer.MAX_VALUE);\n    }",
                "code_after_change": "    public int hashCode() {\n      return (int)(lockId % Integer.MAX_VALUE);\n    }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the issue occurring after calling commitTxn() and then closeTxnManager(), which is related to the ground truth methods. However, it does not precisely identify the root cause in the ground truth methods but mentions methods in the same stack trace context. The report does not provide any fix suggestion, hence 'Missing' for fix suggestion. The problem location is partially identified as it mentions methods in the shared stack trace context but not the exact ground truth methods. There is no wrong information in the report as it accurately describes the observed behavior and related methods."
        }
    },
    {
        "filename": "HIVE-11540.json",
        "code_diff": {
            "ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.createBaseJobConf": {
                "code_before_change": [],
                "code_after_change": "  private JobConf createBaseJobConf(HiveConf conf, String jobName, Table t, StorageDescriptor sd,\n                                    ValidTxnList txns) {\n    JobConf job = new JobConf(conf);\n    job.setJobName(jobName);\n    job.setOutputKeyClass(NullWritable.class);\n    job.setOutputValueClass(NullWritable.class);\n    job.setJarByClass(CompactorMR.class);\n    LOG.debug(\"User jar set to \" + job.getJar());\n    job.setMapperClass(CompactorMap.class);\n    job.setNumReduceTasks(0);\n    job.setInputFormat(CompactorInputFormat.class);\n    job.setOutputFormat(NullOutputFormat.class);\n    job.setOutputCommitter(CompactorOutputCommitter.class);\n\n    String queueName = conf.getVar(HiveConf.ConfVars.COMPACTOR_JOB_QUEUE);\n    if(queueName != null && queueName.length() > 0) {\n      job.setQueueName(queueName);\n    }\n\n    job.set(FINAL_LOCATION, sd.getLocation());\n    job.set(TMP_LOCATION, sd.getLocation() + \"/\" + TMPDIR + \"_\" + UUID.randomUUID().toString());\n    job.set(INPUT_FORMAT_CLASS_NAME, sd.getInputFormat());\n    job.set(OUTPUT_FORMAT_CLASS_NAME, sd.getOutputFormat());\n    job.setBoolean(IS_COMPRESSED, sd.isCompressed());\n    job.set(TABLE_PROPS, new StringableMap(t.getParameters()).toString());\n    job.setInt(NUM_BUCKETS, sd.getNumBuckets());\n    job.set(ValidTxnList.VALID_TXNS_KEY, txns.toString());\n    setColumnTypes(job, sd.getCols());\n    return job;\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.io.AcidUtils.deltaSubdir": {
                "code_before_change": "  public static String deltaSubdir(long min, long max) {\n    return DELTA_PREFIX + String.format(DELTA_DIGITS, min) + \"_\" +\n        String.format(DELTA_DIGITS, max);\n  }",
                "code_after_change": "  public static String deltaSubdir(long min, long max) {\n    return DELTA_PREFIX + String.format(DELTA_DIGITS, min) + \"_\" +\n        String.format(DELTA_DIGITS, max);\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.Worker.run": {
                "code_before_change": "  public void run() {\n    do {\n      boolean launchedJob = false;\n      // Make sure nothing escapes this run method and kills the metastore at large,\n      // so wrap it in a big catch Throwable statement.\n      try {\n        CompactionInfo ci = txnHandler.findNextToCompact(name);\n\n        if (ci == null && !stop.get()) {\n          try {\n            Thread.sleep(SLEEP_TIME);\n            continue;\n          } catch (InterruptedException e) {\n            LOG.warn(\"Worker thread sleep interrupted \" + e.getMessage());\n            continue;\n          }\n        }\n\n        // Find the table we will be working with.\n        Table t1 = null;\n        try {\n          t1 = resolveTable(ci);\n          if (t1 == null) {\n            LOG.info(\"Unable to find table \" + ci.getFullTableName() +\n                \", assuming it was dropped and moving on.\");\n            txnHandler.markCleaned(ci);\n            continue;\n          }\n        } catch (MetaException e) {\n          txnHandler.markCleaned(ci);\n          continue;\n        }\n        // This chicanery is to get around the fact that the table needs to be final in order to\n        // go into the doAs below.\n        final Table t = t1;\n\n        // Find the partition we will be working with, if there is one.\n        Partition p = null;\n        try {\n          p = resolvePartition(ci);\n          if (p == null && ci.partName != null) {\n            LOG.info(\"Unable to find partition \" + ci.getFullPartitionName() +\n                \", assuming it was dropped and moving on.\");\n            txnHandler.markCleaned(ci);\n            continue;\n          }\n        } catch (Exception e) {\n          txnHandler.markCleaned(ci);\n          continue;\n        }\n\n        // Find the appropriate storage descriptor\n        final StorageDescriptor sd =  resolveStorageDescriptor(t, p);\n\n        // Check that the table or partition isn't sorted, as we don't yet support that.\n        if (sd.getSortCols() != null && !sd.getSortCols().isEmpty()) {\n          LOG.error(\"Attempt to compact sorted table, which is not yet supported!\");\n          txnHandler.markCleaned(ci);\n          continue;\n        }\n\n        final boolean isMajor = ci.isMajorCompaction();\n        final ValidTxnList txns =\n            CompactionTxnHandler.createValidCompactTxnList(txnHandler.getOpenTxnsInfo());\n        LOG.debug(\"ValidCompactTxnList: \" + txns.writeToString());\n        final StringBuilder jobName = new StringBuilder(name);\n        jobName.append(\"-compactor-\");\n        jobName.append(ci.getFullPartitionName());\n\n        // Determine who to run as\n        String runAs;\n        if (ci.runAs == null) {\n          runAs = findUserToRunAs(sd.getLocation(), t);\n          txnHandler.setRunAs(ci.id, runAs);\n        } else {\n          runAs = ci.runAs;\n        }\n\n        LOG.info(\"Starting \" + ci.type.toString() + \" compaction for \" +\n            ci.getFullPartitionName());\n\n        final StatsUpdater su = StatsUpdater.init(ci, txnHandler.findColumnsWithStats(ci), conf,\n          runJobAsSelf(runAs) ? runAs : t.getOwner());\n        final CompactorMR mr = new CompactorMR();\n        launchedJob = true;\n        try {\n          if (runJobAsSelf(runAs)) {\n            mr.run(conf, jobName.toString(), t, sd, txns, isMajor, su);\n          } else {\n            UserGroupInformation ugi = UserGroupInformation.createProxyUser(t.getOwner(),\n              UserGroupInformation.getLoginUser());\n            ugi.doAs(new PrivilegedExceptionAction<Object>() {\n              @Override\n              public Object run() throws Exception {\n                mr.run(conf, jobName.toString(), t, sd, txns, isMajor, su);\n                return null;\n              }\n            });\n          }\n          txnHandler.markCompacted(ci);\n        } catch (Exception e) {\n          LOG.error(\"Caught exception while trying to compact \" + ci.getFullPartitionName() +\n              \".  Marking clean to avoid repeated failures, \" + StringUtils.stringifyException(e));\n          txnHandler.markCleaned(ci);\n        }\n      } catch (Throwable t) {\n        LOG.error(\"Caught an exception in the main loop of compactor worker \" + name + \", \" +\n            StringUtils.stringifyException(t));\n      }\n\n      // If we didn't try to launch a job it either means there was no work to do or we got\n      // here as the result of a communication failure with the DB.  Either way we want to wait\n      // a bit before we restart the loop.\n      if (!launchedJob && !stop.get()) {\n        try {\n          Thread.sleep(SLEEP_TIME);\n        } catch (InterruptedException e) {\n        }\n      }\n    } while (!stop.get());\n  }",
                "code_after_change": "  public void run() {\n    do {\n      boolean launchedJob = false;\n      // Make sure nothing escapes this run method and kills the metastore at large,\n      // so wrap it in a big catch Throwable statement.\n      try {\n        final CompactionInfo ci = txnHandler.findNextToCompact(name);\n\n        if (ci == null && !stop.get()) {\n          try {\n            Thread.sleep(SLEEP_TIME);\n            continue;\n          } catch (InterruptedException e) {\n            LOG.warn(\"Worker thread sleep interrupted \" + e.getMessage());\n            continue;\n          }\n        }\n\n        // Find the table we will be working with.\n        Table t1 = null;\n        try {\n          t1 = resolveTable(ci);\n          if (t1 == null) {\n            LOG.info(\"Unable to find table \" + ci.getFullTableName() +\n                \", assuming it was dropped and moving on.\");\n            txnHandler.markCleaned(ci);\n            continue;\n          }\n        } catch (MetaException e) {\n          txnHandler.markCleaned(ci);\n          continue;\n        }\n        // This chicanery is to get around the fact that the table needs to be final in order to\n        // go into the doAs below.\n        final Table t = t1;\n\n        // Find the partition we will be working with, if there is one.\n        Partition p = null;\n        try {\n          p = resolvePartition(ci);\n          if (p == null && ci.partName != null) {\n            LOG.info(\"Unable to find partition \" + ci.getFullPartitionName() +\n                \", assuming it was dropped and moving on.\");\n            txnHandler.markCleaned(ci);\n            continue;\n          }\n        } catch (Exception e) {\n          txnHandler.markCleaned(ci);\n          continue;\n        }\n\n        // Find the appropriate storage descriptor\n        final StorageDescriptor sd =  resolveStorageDescriptor(t, p);\n\n        // Check that the table or partition isn't sorted, as we don't yet support that.\n        if (sd.getSortCols() != null && !sd.getSortCols().isEmpty()) {\n          LOG.error(\"Attempt to compact sorted table, which is not yet supported!\");\n          txnHandler.markCleaned(ci);\n          continue;\n        }\n\n        final boolean isMajor = ci.isMajorCompaction();\n        final ValidTxnList txns =\n            CompactionTxnHandler.createValidCompactTxnList(txnHandler.getOpenTxnsInfo());\n        LOG.debug(\"ValidCompactTxnList: \" + txns.writeToString());\n        final StringBuilder jobName = new StringBuilder(name);\n        jobName.append(\"-compactor-\");\n        jobName.append(ci.getFullPartitionName());\n\n        // Determine who to run as\n        String runAs;\n        if (ci.runAs == null) {\n          runAs = findUserToRunAs(sd.getLocation(), t);\n          txnHandler.setRunAs(ci.id, runAs);\n        } else {\n          runAs = ci.runAs;\n        }\n\n        LOG.info(\"Starting \" + ci.type.toString() + \" compaction for \" +\n            ci.getFullPartitionName());\n\n        final StatsUpdater su = StatsUpdater.init(ci, txnHandler.findColumnsWithStats(ci), conf,\n          runJobAsSelf(runAs) ? runAs : t.getOwner());\n        final CompactorMR mr = new CompactorMR();\n        launchedJob = true;\n        try {\n          if (runJobAsSelf(runAs)) {\n            mr.run(conf, jobName.toString(), t, sd, txns, ci, su);\n          } else {\n            UserGroupInformation ugi = UserGroupInformation.createProxyUser(t.getOwner(),\n              UserGroupInformation.getLoginUser());\n            ugi.doAs(new PrivilegedExceptionAction<Object>() {\n              @Override\n              public Object run() throws Exception {\n                mr.run(conf, jobName.toString(), t, sd, txns, ci, su);\n                return null;\n              }\n            });\n          }\n          txnHandler.markCompacted(ci);\n        } catch (Exception e) {\n          LOG.error(\"Caught exception while trying to compact \" + ci.getFullPartitionName() +\n              \".  Marking clean to avoid repeated failures, \" + StringUtils.stringifyException(e));\n          txnHandler.markCleaned(ci);\n        }\n      } catch (Throwable t) {\n        LOG.error(\"Caught an exception in the main loop of compactor worker \" + name + \", \" +\n            StringUtils.stringifyException(t));\n      }\n\n      // If we didn't try to launch a job it either means there was no work to do or we got\n      // here as the result of a communication failure with the DB.  Either way we want to wait\n      // a bit before we restart the loop.\n      if (!launchedJob && !stop.get()) {\n        try {\n          Thread.sleep(SLEEP_TIME);\n        } catch (InterruptedException e) {\n        }\n      }\n    } while (!stop.get());\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.run": {
                "code_before_change": "  void run(HiveConf conf, String jobName, Table t, StorageDescriptor sd,\n           ValidTxnList txns, boolean isMajor, Worker.StatsUpdater su) throws IOException {\n    JobConf job = new JobConf(conf);\n    job.setJobName(jobName);\n    job.setOutputKeyClass(NullWritable.class);\n    job.setOutputValueClass(NullWritable.class);\n    job.setJarByClass(CompactorMR.class);\n    LOG.debug(\"User jar set to \" + job.getJar());\n    job.setMapperClass(CompactorMap.class);\n    job.setNumReduceTasks(0);\n    job.setInputFormat(CompactorInputFormat.class);\n    job.setOutputFormat(NullOutputFormat.class);\n    job.setOutputCommitter(CompactorOutputCommitter.class);\n\n    job.set(FINAL_LOCATION, sd.getLocation());\n    job.set(TMP_LOCATION, sd.getLocation() + \"/\" + TMPDIR + \"_\" + UUID.randomUUID().toString());\n    job.set(INPUT_FORMAT_CLASS_NAME, sd.getInputFormat());\n    job.set(OUTPUT_FORMAT_CLASS_NAME, sd.getOutputFormat());\n    job.setBoolean(IS_MAJOR, isMajor);\n    job.setBoolean(IS_COMPRESSED, sd.isCompressed());\n    job.set(TABLE_PROPS, new StringableMap(t.getParameters()).toString());\n    job.setInt(NUM_BUCKETS, sd.getNumBuckets());\n    job.set(ValidTxnList.VALID_TXNS_KEY, txns.toString());\n    setColumnTypes(job, sd.getCols());\n\n    // Figure out and encode what files we need to read.  We do this here (rather than in\n    // getSplits below) because as part of this we discover our minimum and maximum transactions,\n    // and discovering that in getSplits is too late as we then have no way to pass it to our\n    // mapper.\n\n    AcidUtils.Directory dir = AcidUtils.getAcidState(new Path(sd.getLocation()), conf, txns);\n    StringableList dirsToSearch = new StringableList();\n    Path baseDir = null;\n    if (isMajor) {\n      // There may not be a base dir if the partition was empty before inserts or if this\n      // partition is just now being converted to ACID.\n      baseDir = dir.getBaseDirectory();\n      if (baseDir == null) {\n        List<FileStatus> originalFiles = dir.getOriginalFiles();\n        if (!(originalFiles == null) && !(originalFiles.size() == 0)) {\n          // There are original format files\n          for (FileStatus stat : originalFiles) {\n            dirsToSearch.add(stat.getPath());\n            LOG.debug(\"Adding original file \" + stat.getPath().toString() + \" to dirs to search\");\n          }\n          // Set base to the location so that the input format reads the original files.\n          baseDir = new Path(sd.getLocation());\n        }\n      } else {\n        // add our base to the list of directories to search for files in.\n        LOG.debug(\"Adding base directory \" + baseDir + \" to dirs to search\");\n        dirsToSearch.add(baseDir);\n      }\n    }\n\n    List<AcidUtils.ParsedDelta> parsedDeltas = dir.getCurrentDirectories();\n\n    if (parsedDeltas == null || parsedDeltas.size() == 0) {\n      // Seriously, no deltas?  Can't compact that.\n      LOG.error(  \"No delta files found to compact in \" + sd.getLocation());\n      return;\n    }\n\n    StringableList deltaDirs = new StringableList();\n    long minTxn = Long.MAX_VALUE;\n    long maxTxn = Long.MIN_VALUE;\n    for (AcidUtils.ParsedDelta delta : parsedDeltas) {\n      LOG.debug(\"Adding delta \" + delta.getPath() + \" to directories to search\");\n      dirsToSearch.add(delta.getPath());\n      deltaDirs.add(delta.getPath());\n      minTxn = Math.min(minTxn, delta.getMinTransaction());\n      maxTxn = Math.max(maxTxn, delta.getMaxTransaction());\n    }\n\n    if (baseDir != null) job.set(BASE_DIR, baseDir.toString());\n    job.set(DELTA_DIRS, deltaDirs.toString());\n    job.set(DIRS_TO_SEARCH, dirsToSearch.toString());\n    job.setLong(MIN_TXN, minTxn);\n    job.setLong(MAX_TXN, maxTxn);\n    LOG.debug(\"Setting minimum transaction to \" + minTxn);\n    LOG.debug(\"Setting maximume transaction to \" + maxTxn);\n\n    JobClient.runJob(job).waitForCompletion();\n    su.gatherStats();\n  }",
                "code_after_change": "  void run(HiveConf conf, String jobName, Table t, StorageDescriptor sd,\n           ValidTxnList txns, CompactionInfo ci, Worker.StatsUpdater su) throws IOException {\n    JobConf job = createBaseJobConf(conf, jobName, t, sd, txns);\n\n    // Figure out and encode what files we need to read.  We do this here (rather than in\n    // getSplits below) because as part of this we discover our minimum and maximum transactions,\n    // and discovering that in getSplits is too late as we then have no way to pass it to our\n    // mapper.\n\n    AcidUtils.Directory dir = AcidUtils.getAcidState(new Path(sd.getLocation()), conf, txns, false);\n    List<AcidUtils.ParsedDelta> parsedDeltas = dir.getCurrentDirectories();\n    int maxDeltastoHandle = conf.getIntVar(HiveConf.ConfVars.COMPACTOR_MAX_NUM_DELTA);\n    if(parsedDeltas.size() > maxDeltastoHandle) {\n      /**\n       * if here, that means we have very high number of delta files.  This may be sign of a temporary\n       * glitch or a real issue.  For example, if transaction batch size or transaction size is set too\n       * low for the event flow rate in Streaming API, it may generate lots of delta files very\n       * quickly.  Another possibility is that Compaction is repeatedly failing and not actually compacting.\n       * Thus, force N minor compactions first to reduce number of deltas and then follow up with\n       * the compaction actually requested in {@link ci} which now needs to compact a lot fewer deltas\n       */\n      LOG.warn(parsedDeltas.size() + \" delta files found for \" + ci.getFullPartitionName()\n        + \" located at \" + sd.getLocation() + \"! This is likely a sign of misconfiguration, \" +\n        \"especially if this message repeats.  Check that compaction is running properly.  Check for any \" +\n        \"runaway/mis-configured process writing to ACID tables, especially using Streaming Ingest API.\");\n      int numMinorCompactions = parsedDeltas.size() / maxDeltastoHandle;\n      for(int jobSubId = 0; jobSubId < numMinorCompactions; jobSubId++) {\n        JobConf jobMinorCompact = createBaseJobConf(conf, jobName + \"_\" + jobSubId, t, sd, txns);\n        launchCompactionJob(jobMinorCompact,\n          null, CompactionType.MINOR, null,\n          parsedDeltas.subList(jobSubId * maxDeltastoHandle, (jobSubId + 1) * maxDeltastoHandle),\n          maxDeltastoHandle, -1);\n      }\n      //now recompute state since we've done minor compactions and have different 'best' set of deltas\n      dir = AcidUtils.getAcidState(new Path(sd.getLocation()), conf, txns);\n    }\n\n    StringableList dirsToSearch = new StringableList();\n    Path baseDir = null;\n    if (ci.isMajorCompaction()) {\n      // There may not be a base dir if the partition was empty before inserts or if this\n      // partition is just now being converted to ACID.\n      baseDir = dir.getBaseDirectory();\n      if (baseDir == null) {\n        List<HdfsFileStatusWithId> originalFiles = dir.getOriginalFiles();\n        if (!(originalFiles == null) && !(originalFiles.size() == 0)) {\n          // There are original format files\n          for (HdfsFileStatusWithId stat : originalFiles) {\n            Path path = stat.getFileStatus().getPath();\n            dirsToSearch.add(path);\n            LOG.debug(\"Adding original file \" + path + \" to dirs to search\");\n          }\n          // Set base to the location so that the input format reads the original files.\n          baseDir = new Path(sd.getLocation());\n        }\n      } else {\n        // add our base to the list of directories to search for files in.\n        LOG.debug(\"Adding base directory \" + baseDir + \" to dirs to search\");\n        dirsToSearch.add(baseDir);\n      }\n    }\n\n    if (parsedDeltas.size() == 0) {\n      // Seriously, no deltas?  Can't compact that.\n      LOG.error(  \"No delta files found to compact in \" + sd.getLocation());\n      //couldn't someone want to run a Major compaction to convert old table to ACID?\n      return;\n    }\n\n    launchCompactionJob(job, baseDir, ci.type, dirsToSearch, dir.getCurrentDirectories(),\n      dir.getCurrentDirectories().size(), dir.getObsolete().size());\n\n    su.gatherStats();\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.io.AcidUtils.getStatementId": {
                "code_before_change": "    public int getStatementId() {\n      return statementId == -1 ? 0 : statementId;\n    }",
                "code_after_change": "    public int getStatementId() {\n      return statementId == -1 ? 0 : statementId;\n    }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.launchCompactionJob": {
                "code_before_change": [],
                "code_after_change": "  private void launchCompactionJob(JobConf job, Path baseDir, CompactionType compactionType,\n                                   StringableList dirsToSearch,\n                                   List<AcidUtils.ParsedDelta> parsedDeltas,\n                                   int curDirNumber, int obsoleteDirNumber) throws IOException {\n    job.setBoolean(IS_MAJOR, compactionType == CompactionType.MAJOR);\n    if(dirsToSearch == null) {\n      dirsToSearch = new StringableList();\n    }\n    StringableList deltaDirs = new StringableList();\n    long minTxn = Long.MAX_VALUE;\n    long maxTxn = Long.MIN_VALUE;\n    for (AcidUtils.ParsedDelta delta : parsedDeltas) {\n      LOG.debug(\"Adding delta \" + delta.getPath() + \" to directories to search\");\n      dirsToSearch.add(delta.getPath());\n      deltaDirs.add(delta.getPath());\n      minTxn = Math.min(minTxn, delta.getMinTransaction());\n      maxTxn = Math.max(maxTxn, delta.getMaxTransaction());\n    }\n\n    if (baseDir != null) job.set(BASE_DIR, baseDir.toString());\n    job.set(DELTA_DIRS, deltaDirs.toString());\n    job.set(DIRS_TO_SEARCH, dirsToSearch.toString());\n    job.setLong(MIN_TXN, minTxn);\n    job.setLong(MAX_TXN, maxTxn);\n\n    LOG.info(\"Submitting \" + compactionType + \" compaction job '\" +\n      job.getJobName() + \"' to \" + job.getQueueName() + \" queue.  \" +\n      \"(current delta dirs count=\" + curDirNumber +\n      \", obsolete delta dirs count=\" + obsoleteDirNumber + \". TxnIdRange[\" + minTxn + \",\" + maxTxn + \"]\");\n    RunningJob rj = JobClient.runJob(job);\n    LOG.info(\"Submitted compaction job '\" + job.getJobName() + \"' with jobID=\" + rj.getID());\n    rj.waitForCompletion();\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.io.AcidUtils.getAcidState": {
                "code_before_change": "  public static Directory getAcidState(Path directory,\n                                       Configuration conf,\n                                       ValidTxnList txnList\n                                       ) throws IOException {\n    FileSystem fs = directory.getFileSystem(conf);\n    FileStatus bestBase = null;\n    long bestBaseTxn = 0;\n    final List<ParsedDelta> deltas = new ArrayList<ParsedDelta>();\n    List<ParsedDelta> working = new ArrayList<ParsedDelta>();\n    List<FileStatus> originalDirectories = new ArrayList<FileStatus>();\n    final List<FileStatus> obsolete = new ArrayList<FileStatus>();\n    List<FileStatus> children = SHIMS.listLocatedStatus(fs, directory,\n        hiddenFileFilter);\n    for(FileStatus child: children) {\n      Path p = child.getPath();\n      String fn = p.getName();\n      if (fn.startsWith(BASE_PREFIX) && child.isDir()) {\n        long txn = parseBase(p);\n        if (bestBase == null) {\n          bestBase = child;\n          bestBaseTxn = txn;\n        } else if (bestBaseTxn < txn) {\n          obsolete.add(bestBase);\n          bestBase = child;\n          bestBaseTxn = txn;\n        } else {\n          obsolete.add(child);\n        }\n      } else if (fn.startsWith(DELTA_PREFIX) && child.isDir()) {\n        ParsedDelta delta = parseDelta(child);\n        if (txnList.isTxnRangeValid(delta.minTransaction,\n            delta.maxTransaction) !=\n            ValidTxnList.RangeResponse.NONE) {\n          working.add(delta);\n        }\n      } else {\n        // This is just the directory.  We need to recurse and find the actual files.  But don't\n        // do this until we have determined there is no base.  This saves time.  Plus,\n        // it is possible that the cleaner is running and removing these original files,\n        // in which case recursing through them could cause us to get an error.\n        originalDirectories.add(child);\n      }\n    }\n\n    final List<FileStatus> original = new ArrayList<FileStatus>();\n    // if we have a base, the original files are obsolete.\n    if (bestBase != null) {\n      // remove the entries so we don't get confused later and think we should\n      // use them.\n      original.clear();\n    } else {\n      // Okay, we're going to need these originals.  Recurse through them and figure out what we\n      // really need.\n      for (FileStatus origDir : originalDirectories) {\n        findOriginals(fs, origDir, original);\n      }\n    }\n\n    Collections.sort(working);\n    long current = bestBaseTxn;\n    int lastStmtId = -1;\n    for(ParsedDelta next: working) {\n      if (next.maxTransaction > current) {\n        // are any of the new transactions ones that we care about?\n        if (txnList.isTxnRangeValid(current+1, next.maxTransaction) !=\n          ValidTxnList.RangeResponse.NONE) {\n          deltas.add(next);\n          current = next.maxTransaction;\n          lastStmtId = next.statementId;\n        }\n      }\n      else if(next.maxTransaction == current && lastStmtId >= 0) {\n        //make sure to get all deltas within a single transaction;  multi-statement txn\n        //generate multiple delta files with the same txnId range\n        //of course, if maxTransaction has already been minor compacted, all per statement deltas are obsolete\n        deltas.add(next);\n      }\n      else {\n        obsolete.add(next.path);\n      }\n    }\n\n    final Path base = bestBase == null ? null : bestBase.getPath();\n    LOG.debug(\"in directory \" + directory.toUri().toString() + \" base = \" + base + \" deltas = \" +\n        deltas.size());\n\n    return new Directory(){\n\n      @Override\n      public Path getBaseDirectory() {\n        return base;\n      }\n\n      @Override\n      public List<FileStatus> getOriginalFiles() {\n        return original;\n      }\n\n      @Override\n      public List<ParsedDelta> getCurrentDirectories() {\n        return deltas;\n      }\n\n      @Override\n      public List<FileStatus> getObsolete() {\n        return obsolete;\n      }\n    };\n  }",
                "code_after_change": "  public static Directory getAcidState(Path directory,\n      Configuration conf,\n      ValidTxnList txnList\n      ) throws IOException {\n    return getAcidState(directory, conf, txnList, false);\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report mentions an OutOfMemoryError related to compaction, which is relevant to the ground truth methods involved in compaction processes. However, it does not precisely identify the root cause at the specific ground truth methods. The stack trace context includes methods like 'CompactorMR.run' and 'Worker.run', which are part of the ground truth, hence the 'Shared Stack Trace Context' sub-category. There is no specific fix suggestion provided in the bug report, leading to a 'Missing' evaluation for fix suggestion. The problem location is also identified as 'Partial' with 'Shared Stack Trace Context' because the report does not directly mention the ground truth methods but is related to them through the stack trace. There is no wrong information in the report, as all details are relevant to the context of the bug."
        }
    },
    {
        "filename": "HIVE-15755.json",
        "code_diff": {
            "ql.src.java.org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.addColumn2Table": {
                "code_before_change": "    private void addColumn2Table(String tableName, String columnName) {\n      tableName = tableName.toLowerCase();//normalize name for mapping\n      tableNamesFound.add(tableName);\n      List<String> cols = table2column.get(tableName);\n      if(cols == null) {\n        cols = new ArrayList<>();\n        table2column.put(tableName, cols);\n      }\n      //we want to preserve 'columnName' as it was in original input query so that rewrite\n      //looks as much as possible like original query\n      cols.add(columnName);\n    }",
                "code_after_change": "    private void addColumn2Table(String tableName, String columnName) {\n      tableName = tableName.toLowerCase();//normalize name for mapping\n      tableNamesFound.add(tableName);\n      List<String> cols = table2column.get(tableName);\n      if(cols == null) {\n        cols = new ArrayList<>();\n        table2column.put(tableName, cols);\n      }\n      //we want to preserve 'columnName' as it was in original input query so that rewrite\n      //looks as much as possible like original query\n      cols.add(columnName);\n    }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.getPredicate": {
                "code_before_change": "    private String getPredicate() {\n      //normilize table name for mapping\n      List<String> targetCols = table2column.get(targetTableNameInSourceQuery.toLowerCase());\n      StringBuilder sb = new StringBuilder();\n      for(String col : targetCols) {\n        if(sb.length() > 0) {\n          sb.append(\" AND \");\n        }\n        //but preserve table name in SQL\n        sb.append(HiveUtils.unparseIdentifier(targetTableNameInSourceQuery, conf)).append(\".\").append(HiveUtils.unparseIdentifier(col, conf)).append(\" IS NULL\");\n      }\n      return sb.toString();\n    }",
                "code_after_change": "    private String getPredicate() {\n      //normilize table name for mapping\n      List<String> targetCols = table2column.get(targetTableNameInSourceQuery.toLowerCase());\n      if(targetCols == null) {\n        /*e.g. ON source.t=1\n        * this is not strictly speaking invlaid but it does ensure that all columns from target\n        * table are all NULL for every row.  This would make any WHEN MATCHED clause invalid since\n        * we don't have a ROW__ID.  The WHEN NOT MATCHED could be meaningful but it's just data from\n        * source satisfying source.t=1...  not worth the effort to support this*/\n        throw new IllegalArgumentException(ErrorMsg.INVALID_TABLE_IN_ON_CLAUSE_OF_MERGE\n          .format(targetTableNameInSourceQuery, onClauseAsString));\n      }\n      StringBuilder sb = new StringBuilder();\n      for(String col : targetCols) {\n        if(sb.length() > 0) {\n          sb.append(\" AND \");\n        }\n        //but preserve table name in SQL\n        sb.append(HiveUtils.unparseIdentifier(targetTableNameInSourceQuery, conf)).append(\".\").append(HiveUtils.unparseIdentifier(col, conf)).append(\" IS NULL\");\n      }\n      return sb.toString();\n    }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report mentions a NullPointerException occurring in the context of a merge statement with an invalid table name, which is related to the ground truth method 'getPredicate' as seen in the stack trace. However, it does not precisely identify the root cause at the ground truth method level. The fix suggestion is missing as there is no suggestion provided in the bug report. The problem location is partially identified as it is related to the stack trace context where the ground truth method 'getPredicate' is mentioned. There is no wrong information in the bug report as it accurately describes the error context."
        }
    },
    {
        "filename": "HIVE-9390.json",
        "code_diff": {
            "metastore.src.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.checkRetryable": {
                "code_before_change": [],
                "code_after_change": "  protected void checkRetryable(Connection conn,\n                                SQLException e,\n                                String caller) throws RetryException, MetaException {\n\n    // If you change this function, remove the @Ignore from TestTxnHandler.deadlockIsDetected()\n    // to test these changes.\n    // MySQL and MSSQL use 40001 as the state code for rollback.  Postgres uses 40001 and 40P01.\n    // Oracle seems to return different SQLStates and messages each time,\n    // so I've tried to capture the different error messages (there appear to be fewer different\n    // error messages than SQL states).\n    // Derby and newer MySQL driver use the new SQLTransactionRollbackException\n    if (dbProduct == null) {\n      determineDatabaseProduct(conn);\n    }\n    if (e instanceof SQLTransactionRollbackException ||\n      ((dbProduct == DatabaseProduct.MYSQL || dbProduct == DatabaseProduct.POSTGRES ||\n        dbProduct == DatabaseProduct.SQLSERVER) && e.getSQLState().equals(\"40001\")) ||\n      (dbProduct == DatabaseProduct.POSTGRES && e.getSQLState().equals(\"40P01\")) ||\n      (dbProduct == DatabaseProduct.ORACLE && (e.getMessage().contains(\"deadlock detected\")\n        || e.getMessage().contains(\"can't serialize access for this transaction\")))) {\n      if (deadlockCnt++ < ALLOWED_REPEATED_DEADLOCKS) {\n        LOG.warn(\"Deadlock detected in \" + caller + \", trying again.\");\n        throw new RetryException();\n      } else {\n        LOG.error(\"Too many repeated deadlocks in \" + caller + \", giving up.\");\n        deadlockCnt = 0;\n      }\n    }\n    else if(isRetryable(e)) {\n      //in MSSQL this means Communication Link Failure\n      if(retryNum++ < retryLimit) {\n        try {\n          Thread.sleep(retryInterval);\n        }\n        catch(InterruptedException ex) {\n          //\n        }\n        LOG.warn(\"Retryable error detected in \" + caller + \", trying again: \" + getMessage(e));\n        throw new RetryException();\n      }\n      else {\n        LOG.error(\"Fatal error. Retry limit (\" + retryLimit + \") reached. Last error: \" + getMessage(e));\n        retryNum = 0;\n      }\n    }\n    else {\n      //if here, we got something that will propagate the error (rather than retry), so reset counters\n      deadlockCnt = 0;\n      retryNum = 0;\n    }\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler.revokeTimedoutWorkers": {
                "code_before_change": "  public void revokeTimedoutWorkers(long timeout) throws MetaException {\n    try {\n      Connection dbConn = getDbConn(Connection.TRANSACTION_SERIALIZABLE);\n      long latestValidStart = getDbTime(dbConn) - timeout;\n      Statement stmt = null;\n      try {\n        stmt = dbConn.createStatement();\n        String s = \"update COMPACTION_QUEUE set cq_worker_id = null, cq_start = null, cq_state = '\"\n            + INITIATED_STATE+ \"' where cq_state = '\" + WORKING_STATE + \"' and cq_start < \"\n            +  latestValidStart;\n        LOG.debug(\"Going to execute update <\" + s + \">\");\n        // It isn't an error if the following returns no rows, as the local workers could have died\n        // with  nothing assigned to them.\n        stmt.executeUpdate(s);\n        LOG.debug(\"Going to commit\");\n        dbConn.commit();\n      } catch (SQLException e) {\n        try {\n          LOG.error(\"Unable to change dead worker's records back to initiated state \" +\n              e.getMessage());\n          LOG.debug(\"Going to rollback\");\n          dbConn.rollback();\n        } catch (SQLException e1) {\n        }\n        detectDeadlock(dbConn, e, \"revokeTimedoutWorkers\");\n        throw new MetaException(\"Unable to connect to transaction database \" +\n            StringUtils.stringifyException(e));\n      } finally {\n        closeDbConn(dbConn);\n        closeStmt(stmt);\n      }\n    } catch (DeadlockException e) {\n      revokeTimedoutWorkers(timeout);\n    } finally {\n      deadlockCnt = 0;\n    }\n  }",
                "code_after_change": "  public void revokeTimedoutWorkers(long timeout) throws MetaException {\n    try {\n      Connection dbConn = null;\n      Statement stmt = null;\n      try {\n        dbConn = getDbConn(Connection.TRANSACTION_SERIALIZABLE);\n        long latestValidStart = getDbTime(dbConn) - timeout;\n        stmt = dbConn.createStatement();\n        String s = \"update COMPACTION_QUEUE set cq_worker_id = null, cq_start = null, cq_state = '\"\n          + INITIATED_STATE+ \"' where cq_state = '\" + WORKING_STATE + \"' and cq_start < \"\n          +  latestValidStart;\n        LOG.debug(\"Going to execute update <\" + s + \">\");\n        // It isn't an error if the following returns no rows, as the local workers could have died\n        // with  nothing assigned to them.\n        stmt.executeUpdate(s);\n        LOG.debug(\"Going to commit\");\n        dbConn.commit();\n      } catch (SQLException e) {\n        LOG.error(\"Unable to change dead worker's records back to initiated state \" +\n          e.getMessage());\n        LOG.debug(\"Going to rollback\");\n        rollbackDBConn(dbConn);\n        checkRetryable(dbConn, e, \"revokeTimedoutWorkers\");\n        throw new MetaException(\"Unable to connect to transaction database \" +\n          StringUtils.stringifyException(e));\n      } finally {\n        closeDbConn(dbConn);\n        closeStmt(stmt);\n      }\n    } catch (RetryException e) {\n      revokeTimedoutWorkers(timeout);\n    }\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.abortTxn": {
                "code_before_change": "  public void abortTxn(AbortTxnRequest rqst) throws NoSuchTxnException, MetaException {\n    long txnid = rqst.getTxnid();\n    try {\n      Connection dbConn = getDbConn(Connection.TRANSACTION_SERIALIZABLE);\n      try {\n        List<Long> txnids = new ArrayList<Long>(1);\n        txnids.add(txnid);\n        if (abortTxns(dbConn, txnids) != 1) {\n          LOG.debug(\"Going to rollback\");\n          dbConn.rollback();\n          throw new NoSuchTxnException(\"No such transaction: \" + txnid);\n        }\n\n        LOG.debug(\"Going to commit\");\n        dbConn.commit();\n      } catch (SQLException e) {\n        try {\n          LOG.debug(\"Going to rollback\");\n          dbConn.rollback();\n        } catch (SQLException e1) {\n        }\n        detectDeadlock(dbConn, e, \"abortTxn\");\n        throw new MetaException(\"Unable to update transaction database \"\n          + StringUtils.stringifyException(e));\n      } finally {\n        closeDbConn(dbConn);\n      }\n    } catch (DeadlockException e) {\n      abortTxn(rqst);\n    } finally {\n      deadlockCnt = 0;\n    }\n  }",
                "code_after_change": "  public void abortTxn(AbortTxnRequest rqst) throws NoSuchTxnException, MetaException {\n    long txnid = rqst.getTxnid();\n    try {\n      Connection dbConn = null;\n      try {\n        dbConn = getDbConn(Connection.TRANSACTION_SERIALIZABLE);\n        List<Long> txnids = new ArrayList<Long>(1);\n        txnids.add(txnid);\n        if (abortTxns(dbConn, txnids) != 1) {\n          LOG.debug(\"Going to rollback\");\n          dbConn.rollback();\n          throw new NoSuchTxnException(\"No such transaction: \" + txnid);\n        }\n\n        LOG.debug(\"Going to commit\");\n        dbConn.commit();\n      } catch (SQLException e) {\n        LOG.debug(\"Going to rollback\");\n        rollbackDBConn(dbConn);\n        checkRetryable(dbConn, e, \"abortTxn\");\n        throw new MetaException(\"Unable to update transaction database \"\n          + StringUtils.stringifyException(e));\n      } finally {\n        closeDbConn(dbConn);\n      }\n    } catch (RetryException e) {\n      abortTxn(rqst);\n    }\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.getLockInfoFromLockId": {
                "code_before_change": "  private List<LockInfo> getLockInfoFromLockId(Connection dbConn, long extLockId)\n      throws NoSuchLockException, MetaException, SQLException {\n    Statement stmt = null;\n    try {\n      stmt = dbConn.createStatement();\n      String s = \"select hl_lock_ext_id, hl_lock_int_id, hl_db, hl_table, \" +\n          \"hl_partition, hl_lock_state, hl_lock_type from HIVE_LOCKS where \" +\n          \"hl_lock_ext_id = \" + extLockId;\n      LOG.debug(\"Going to execute query <\" + s + \">\");\n      ResultSet rs = stmt.executeQuery(s);\n      boolean sawAtLeastOne = false;\n      List<LockInfo> ourLockInfo = new ArrayList<LockInfo>();\n      while (rs.next()) {\n        ourLockInfo.add(new LockInfo(rs));\n        sawAtLeastOne = true;\n      }\n      if (!sawAtLeastOne) {\n        throw new MetaException(\"This should never happen!  We already \" +\n            \"checked the lock existed but now we can't find it!\");\n      }\n      return ourLockInfo;\n    } finally {\n      closeStmt(stmt);\n    }\n  }",
                "code_after_change": "  private List<LockInfo> getLockInfoFromLockId(Connection dbConn, long extLockId)\n    throws NoSuchLockException, MetaException, SQLException {\n    Statement stmt = null;\n    try {\n      stmt = dbConn.createStatement();\n      String s = \"select hl_lock_ext_id, hl_lock_int_id, hl_db, hl_table, \" +\n        \"hl_partition, hl_lock_state, hl_lock_type from HIVE_LOCKS where \" +\n        \"hl_lock_ext_id = \" + extLockId;\n      LOG.debug(\"Going to execute query <\" + s + \">\");\n      ResultSet rs = stmt.executeQuery(s);\n      boolean sawAtLeastOne = false;\n      List<LockInfo> ourLockInfo = new ArrayList<LockInfo>();\n      while (rs.next()) {\n        ourLockInfo.add(new LockInfo(rs));\n        sawAtLeastOne = true;\n      }\n      if (!sawAtLeastOne) {\n        throw new MetaException(\"This should never happen!  We already \" +\n          \"checked the lock existed but now we can't find it!\");\n      }\n      return ourLockInfo;\n    } finally {\n      closeStmt(stmt);\n    }\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.buildJumpTable": {
                "code_before_change": " private static synchronized void buildJumpTable() {\n    if (jumpTable != null) return;\n\n    jumpTable =\n        new HashMap<LockType, Map<LockType, Map<LockState,  LockAction>>>(3);\n\n    // SR: Lock we are trying to acquire is shared read\n    Map<LockType, Map<LockState, LockAction>> m =\n        new HashMap<LockType, Map<LockState, LockAction>>(3);\n    jumpTable.put(LockType.SHARED_READ, m);\n\n    // SR.SR: Lock we are examining is shared read\n    Map<LockState, LockAction> m2 = new HashMap<LockState, LockAction>(2);\n    m.put(LockType.SHARED_READ, m2);\n\n    // SR.SR.acquired Lock we are examining is acquired;  We can acquire\n    // because two shared reads can acquire together and there must be\n    // nothing in front of this one to prevent acquisition.\n    m2.put(LockState.ACQUIRED, LockAction.ACQUIRE);\n\n    // SR.SR.wait Lock we are examining is waiting.  In this case we keep\n    // looking, as it's possible that something in front is blocking it or\n    // that the other locker hasn't checked yet and he could lock as well.\n    m2.put(LockState.WAITING, LockAction.KEEP_LOOKING);\n\n    // SR.SW: Lock we are examining is shared write\n    m2 = new HashMap<LockState, LockAction>(2);\n    m.put(LockType.SHARED_WRITE, m2);\n\n    // SR.SW.acquired Lock we are examining is acquired;  We can acquire\n    // because a read can share with a write, and there must be\n    // nothing in front of this one to prevent acquisition.\n    m2.put(LockState.ACQUIRED, LockAction.ACQUIRE);\n\n    // SR.SW.wait Lock we are examining is waiting.  In this case we keep\n    // looking, as it's possible that something in front is blocking it or\n    // that the other locker hasn't checked yet and he could lock as well or\n    // that something is blocking it that would not block a read.\n    m2.put(LockState.WAITING, LockAction.KEEP_LOOKING);\n\n     // SR.E: Lock we are examining is exclusive\n    m2 = new HashMap<LockState, LockAction>(2);\n    m.put(LockType.EXCLUSIVE, m2);\n\n    // No matter whether it has acquired or not, we cannot pass an exclusive.\n    m2.put(LockState.ACQUIRED, LockAction.WAIT);\n    m2.put(LockState.WAITING, LockAction.WAIT);\n\n    // SW: Lock we are trying to acquire is shared write\n    m = new HashMap<LockType, Map<LockState, LockAction>>(3);\n    jumpTable.put(LockType.SHARED_WRITE, m);\n\n    // SW.SR: Lock we are examining is shared read\n    m2 = new HashMap<LockState, LockAction>(2);\n    m.put(LockType.SHARED_READ, m2);\n\n    // SW.SR.acquired Lock we are examining is acquired;  We need to keep\n    // looking, because there may or may not be another shared write in front\n    // that would block us.\n    m2.put(LockState.ACQUIRED, LockAction.KEEP_LOOKING);\n\n    // SW.SR.wait Lock we are examining is waiting.  In this case we keep\n    // looking, as it's possible that something in front is blocking it or\n    // that the other locker hasn't checked yet and he could lock as well.\n    m2.put(LockState.WAITING, LockAction.KEEP_LOOKING);\n\n    // SW.SW: Lock we are examining is shared write\n    m2 = new HashMap<LockState, LockAction>(2);\n    m.put(LockType.SHARED_WRITE, m2);\n\n    // Regardless of acquired or waiting, one shared write cannot pass another.\n    m2.put(LockState.ACQUIRED, LockAction.WAIT);\n    m2.put(LockState.WAITING, LockAction.WAIT);\n\n     // SW.E: Lock we are examining is exclusive\n    m2 = new HashMap<LockState, LockAction>(2);\n    m.put(LockType.EXCLUSIVE, m2);\n\n    // No matter whether it has acquired or not, we cannot pass an exclusive.\n    m2.put(LockState.ACQUIRED, LockAction.WAIT);\n    m2.put(LockState.WAITING, LockAction.WAIT);\n\n    // E: Lock we are trying to acquire is exclusive\n    m = new HashMap<LockType, Map<LockState, LockAction>>(3);\n    jumpTable.put(LockType.EXCLUSIVE, m);\n\n    // E.SR: Lock we are examining is shared read\n    m2 = new HashMap<LockState, LockAction>(2);\n    m.put(LockType.SHARED_READ, m2);\n\n    // Exclusives can never pass\n    m2.put(LockState.ACQUIRED, LockAction.WAIT);\n    m2.put(LockState.WAITING, LockAction.WAIT);\n\n    // E.SW: Lock we are examining is shared write\n    m2 = new HashMap<LockState, LockAction>(2);\n    m.put(LockType.SHARED_WRITE, m2);\n\n    // Exclusives can never pass\n    m2.put(LockState.ACQUIRED, LockAction.WAIT);\n    m2.put(LockState.WAITING, LockAction.WAIT);\n\n     // E.E: Lock we are examining is exclusive\n    m2 = new HashMap<LockState, LockAction>(2);\n    m.put(LockType.EXCLUSIVE, m2);\n\n    // No matter whether it has acquired or not, we cannot pass an exclusive.\n    m2.put(LockState.ACQUIRED, LockAction.WAIT);\n    m2.put(LockState.WAITING, LockAction.WAIT);\n  }",
                "code_after_change": "  private static synchronized void buildJumpTable() {\n    if (jumpTable != null) return;\n\n    jumpTable =\n      new HashMap<LockType, Map<LockType, Map<LockState,  LockAction>>>(3);\n\n    // SR: Lock we are trying to acquire is shared read\n    Map<LockType, Map<LockState, LockAction>> m =\n      new HashMap<LockType, Map<LockState, LockAction>>(3);\n    jumpTable.put(LockType.SHARED_READ, m);\n\n    // SR.SR: Lock we are examining is shared read\n    Map<LockState, LockAction> m2 = new HashMap<LockState, LockAction>(2);\n    m.put(LockType.SHARED_READ, m2);\n\n    // SR.SR.acquired Lock we are examining is acquired;  We can acquire\n    // because two shared reads can acquire together and there must be\n    // nothing in front of this one to prevent acquisition.\n    m2.put(LockState.ACQUIRED, LockAction.ACQUIRE);\n\n    // SR.SR.wait Lock we are examining is waiting.  In this case we keep\n    // looking, as it's possible that something in front is blocking it or\n    // that the other locker hasn't checked yet and he could lock as well.\n    m2.put(LockState.WAITING, LockAction.KEEP_LOOKING);\n\n    // SR.SW: Lock we are examining is shared write\n    m2 = new HashMap<LockState, LockAction>(2);\n    m.put(LockType.SHARED_WRITE, m2);\n\n    // SR.SW.acquired Lock we are examining is acquired;  We can acquire\n    // because a read can share with a write, and there must be\n    // nothing in front of this one to prevent acquisition.\n    m2.put(LockState.ACQUIRED, LockAction.ACQUIRE);\n\n    // SR.SW.wait Lock we are examining is waiting.  In this case we keep\n    // looking, as it's possible that something in front is blocking it or\n    // that the other locker hasn't checked yet and he could lock as well or\n    // that something is blocking it that would not block a read.\n    m2.put(LockState.WAITING, LockAction.KEEP_LOOKING);\n\n    // SR.E: Lock we are examining is exclusive\n    m2 = new HashMap<LockState, LockAction>(2);\n    m.put(LockType.EXCLUSIVE, m2);\n\n    // No matter whether it has acquired or not, we cannot pass an exclusive.\n    m2.put(LockState.ACQUIRED, LockAction.WAIT);\n    m2.put(LockState.WAITING, LockAction.WAIT);\n\n    // SW: Lock we are trying to acquire is shared write\n    m = new HashMap<LockType, Map<LockState, LockAction>>(3);\n    jumpTable.put(LockType.SHARED_WRITE, m);\n\n    // SW.SR: Lock we are examining is shared read\n    m2 = new HashMap<LockState, LockAction>(2);\n    m.put(LockType.SHARED_READ, m2);\n\n    // SW.SR.acquired Lock we are examining is acquired;  We need to keep\n    // looking, because there may or may not be another shared write in front\n    // that would block us.\n    m2.put(LockState.ACQUIRED, LockAction.KEEP_LOOKING);\n\n    // SW.SR.wait Lock we are examining is waiting.  In this case we keep\n    // looking, as it's possible that something in front is blocking it or\n    // that the other locker hasn't checked yet and he could lock as well.\n    m2.put(LockState.WAITING, LockAction.KEEP_LOOKING);\n\n    // SW.SW: Lock we are examining is shared write\n    m2 = new HashMap<LockState, LockAction>(2);\n    m.put(LockType.SHARED_WRITE, m2);\n\n    // Regardless of acquired or waiting, one shared write cannot pass another.\n    m2.put(LockState.ACQUIRED, LockAction.WAIT);\n    m2.put(LockState.WAITING, LockAction.WAIT);\n\n    // SW.E: Lock we are examining is exclusive\n    m2 = new HashMap<LockState, LockAction>(2);\n    m.put(LockType.EXCLUSIVE, m2);\n\n    // No matter whether it has acquired or not, we cannot pass an exclusive.\n    m2.put(LockState.ACQUIRED, LockAction.WAIT);\n    m2.put(LockState.WAITING, LockAction.WAIT);\n\n    // E: Lock we are trying to acquire is exclusive\n    m = new HashMap<LockType, Map<LockState, LockAction>>(3);\n    jumpTable.put(LockType.EXCLUSIVE, m);\n\n    // E.SR: Lock we are examining is shared read\n    m2 = new HashMap<LockState, LockAction>(2);\n    m.put(LockType.SHARED_READ, m2);\n\n    // Exclusives can never pass\n    m2.put(LockState.ACQUIRED, LockAction.WAIT);\n    m2.put(LockState.WAITING, LockAction.WAIT);\n\n    // E.SW: Lock we are examining is shared write\n    m2 = new HashMap<LockState, LockAction>(2);\n    m.put(LockType.SHARED_WRITE, m2);\n\n    // Exclusives can never pass\n    m2.put(LockState.ACQUIRED, LockAction.WAIT);\n    m2.put(LockState.WAITING, LockAction.WAIT);\n\n    // E.E: Lock we are examining is exclusive\n    m2 = new HashMap<LockState, LockAction>(2);\n    m.put(LockType.EXCLUSIVE, m2);\n\n    // No matter whether it has acquired or not, we cannot pass an exclusive.\n    m2.put(LockState.ACQUIRED, LockAction.WAIT);\n    m2.put(LockState.WAITING, LockAction.WAIT);\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler.revokeFromLocalWorkers": {
                "code_before_change": "  public void revokeFromLocalWorkers(String hostname) throws MetaException {\n    try {\n      Connection dbConn = getDbConn(Connection.TRANSACTION_SERIALIZABLE);\n      Statement stmt = null;\n      try {\n        stmt = dbConn.createStatement();\n        String s = \"update COMPACTION_QUEUE set cq_worker_id = null, cq_start = null, cq_state = '\"\n            + INITIATED_STATE+ \"' where cq_state = '\" + WORKING_STATE + \"' and cq_worker_id like '\"\n            +  hostname + \"%'\";\n        LOG.debug(\"Going to execute update <\" + s + \">\");\n        // It isn't an error if the following returns no rows, as the local workers could have died\n        // with  nothing assigned to them.\n        stmt.executeUpdate(s);\n        LOG.debug(\"Going to commit\");\n        dbConn.commit();\n      } catch (SQLException e) {\n        try {\n          LOG.error(\"Unable to change dead worker's records back to initiated state \" +\n              e.getMessage());\n          LOG.debug(\"Going to rollback\");\n          dbConn.rollback();\n        } catch (SQLException e1) {\n        }\n        detectDeadlock(dbConn, e, \"revokeFromLocalWorkers\");\n        throw new MetaException(\"Unable to connect to transaction database \" +\n            StringUtils.stringifyException(e));\n      } finally {\n        closeDbConn(dbConn);\n        closeStmt(stmt);\n      }\n    } catch (DeadlockException e) {\n      revokeFromLocalWorkers(hostname);\n    } finally {\n      deadlockCnt = 0;\n    }\n  }",
                "code_after_change": "  public void revokeFromLocalWorkers(String hostname) throws MetaException {\n    try {\n      Connection dbConn = null;\n      Statement stmt = null;\n      try {\n        dbConn = getDbConn(Connection.TRANSACTION_SERIALIZABLE);\n        stmt = dbConn.createStatement();\n        String s = \"update COMPACTION_QUEUE set cq_worker_id = null, cq_start = null, cq_state = '\"\n          + INITIATED_STATE+ \"' where cq_state = '\" + WORKING_STATE + \"' and cq_worker_id like '\"\n          +  hostname + \"%'\";\n        LOG.debug(\"Going to execute update <\" + s + \">\");\n        // It isn't an error if the following returns no rows, as the local workers could have died\n        // with  nothing assigned to them.\n        stmt.executeUpdate(s);\n        LOG.debug(\"Going to commit\");\n        dbConn.commit();\n      } catch (SQLException e) {\n        LOG.error(\"Unable to change dead worker's records back to initiated state \" +\n          e.getMessage());\n        LOG.debug(\"Going to rollback\");\n        rollbackDBConn(dbConn);\n        checkRetryable(dbConn, e, \"revokeFromLocalWorkers\");\n        throw new MetaException(\"Unable to connect to transaction database \" +\n          StringUtils.stringifyException(e));\n      } finally {\n        closeDbConn(dbConn);\n        closeStmt(stmt);\n      }\n    } catch (RetryException e) {\n      revokeFromLocalWorkers(hostname);\n    }\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.wait": {
                "code_before_change": "  private void wait(Connection dbConn, Savepoint save) throws SQLException {\n    // Need to rollback because we did a select that acquired locks but we didn't\n    // actually update anything.  Also, we may have locked some locks as\n    // acquired that we now want to not acquire.  It's ok to rollback because\n    // once we see one wait, we're done, we won't look for more.\n    // Only rollback to savepoint because we want to commit our heartbeat\n    // changes.\n    LOG.debug(\"Going to rollback to savepoint\");\n    dbConn.rollback(save);\n  }",
                "code_after_change": "  private void wait(Connection dbConn, Savepoint save) throws SQLException {\n    // Need to rollback because we did a select that acquired locks but we didn't\n    // actually update anything.  Also, we may have locked some locks as\n    // acquired that we now want to not acquire.  It's ok to rollback because\n    // once we see one wait, we're done, we won't look for more.\n    // Only rollback to savepoint because we want to commit our heartbeat\n    // changes.\n    LOG.debug(\"Going to rollback to savepoint\");\n    dbConn.rollback(save);\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler.findPotentialCompactions": {
                "code_before_change": "  public Set<CompactionInfo> findPotentialCompactions(int maxAborted) throws MetaException {\n    Connection dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);\n    Set<CompactionInfo> response = new HashSet<CompactionInfo>();\n    Statement stmt = null;\n    try {\n      stmt = dbConn.createStatement();\n      // Check for completed transactions\n      String s = \"select distinct ctc_database, ctc_table, \" +\n          \"ctc_partition from COMPLETED_TXN_COMPONENTS\";\n      LOG.debug(\"Going to execute query <\" + s + \">\");\n      ResultSet rs = stmt.executeQuery(s);\n      while (rs.next()) {\n        CompactionInfo info = new CompactionInfo();\n        info.dbname = rs.getString(1);\n        info.tableName = rs.getString(2);\n        info.partName = rs.getString(3);\n        response.add(info);\n      }\n\n      // Check for aborted txns\n      s = \"select tc_database, tc_table, tc_partition \" +\n          \"from TXNS, TXN_COMPONENTS \" +\n          \"where txn_id = tc_txnid and txn_state = '\" + TXN_ABORTED + \"' \" +\n          \"group by tc_database, tc_table, tc_partition \" +\n          \"having count(*) > \" + maxAborted;\n\n      LOG.debug(\"Going to execute query <\" + s + \">\");\n      rs = stmt.executeQuery(s);\n      while (rs.next()) {\n        CompactionInfo info = new CompactionInfo();\n        info.dbname = rs.getString(1);\n        info.tableName = rs.getString(2);\n        info.partName = rs.getString(3);\n        info.tooManyAborts = true;\n        response.add(info);\n      }\n\n      LOG.debug(\"Going to rollback\");\n      dbConn.rollback();\n    } catch (SQLException e) {\n      LOG.error(\"Unable to connect to transaction database \" + e.getMessage());\n    } finally {\n      closeDbConn(dbConn);\n      closeStmt(stmt);\n    }\n    return response;\n  }",
                "code_after_change": "  public Set<CompactionInfo> findPotentialCompactions(int maxAborted) throws MetaException {\n    Connection dbConn = null;\n    Set<CompactionInfo> response = new HashSet<CompactionInfo>();\n    Statement stmt = null;\n    try {\n      try {\n        dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);\n        stmt = dbConn.createStatement();\n        // Check for completed transactions\n        String s = \"select distinct ctc_database, ctc_table, \" +\n          \"ctc_partition from COMPLETED_TXN_COMPONENTS\";\n        LOG.debug(\"Going to execute query <\" + s + \">\");\n        ResultSet rs = stmt.executeQuery(s);\n        while (rs.next()) {\n          CompactionInfo info = new CompactionInfo();\n          info.dbname = rs.getString(1);\n          info.tableName = rs.getString(2);\n          info.partName = rs.getString(3);\n          response.add(info);\n        }\n\n        // Check for aborted txns\n        s = \"select tc_database, tc_table, tc_partition \" +\n          \"from TXNS, TXN_COMPONENTS \" +\n          \"where txn_id = tc_txnid and txn_state = '\" + TXN_ABORTED + \"' \" +\n          \"group by tc_database, tc_table, tc_partition \" +\n          \"having count(*) > \" + maxAborted;\n\n        LOG.debug(\"Going to execute query <\" + s + \">\");\n        rs = stmt.executeQuery(s);\n        while (rs.next()) {\n          CompactionInfo info = new CompactionInfo();\n          info.dbname = rs.getString(1);\n          info.tableName = rs.getString(2);\n          info.partName = rs.getString(3);\n          info.tooManyAborts = true;\n          response.add(info);\n        }\n\n        LOG.debug(\"Going to rollback\");\n        dbConn.rollback();\n      } catch (SQLException e) {\n        LOG.error(\"Unable to connect to transaction database \" + e.getMessage());\n        checkRetryable(dbConn, e, \"findPotentialCompactions\");\n      } finally {\n        closeDbConn(dbConn);\n        closeStmt(stmt);\n      }\n      return response;\n    }\n    catch (RetryException e) {\n      return findPotentialCompactions(maxAborted);\n    }\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.heartbeat": {
                "code_before_change": "  public void heartbeat(HeartbeatRequest ids)\n      throws NoSuchTxnException,  NoSuchLockException, TxnAbortedException, MetaException {\n    try {\n      Connection dbConn = getDbConn(Connection.TRANSACTION_SERIALIZABLE);\n      try {\n        heartbeatLock(dbConn, ids.getLockid());\n        heartbeatTxn(dbConn, ids.getTxnid());\n      } catch (SQLException e) {\n        try {\n          LOG.debug(\"Going to rollback\");\n          dbConn.rollback();\n        } catch (SQLException e1) {\n        }\n        detectDeadlock(dbConn, e, \"heartbeat\");\n        throw new MetaException(\"Unable to select from transaction database \" +\n            StringUtils.stringifyException(e));\n      } finally {\n        closeDbConn(dbConn);\n      }\n    } catch (DeadlockException e) {\n      heartbeat(ids);\n    } finally {\n      deadlockCnt = 0;\n    }\n  }",
                "code_after_change": "  public void heartbeat(HeartbeatRequest ids)\n    throws NoSuchTxnException,  NoSuchLockException, TxnAbortedException, MetaException {\n    try {\n      Connection dbConn = null;\n      try {\n        dbConn = getDbConn(Connection.TRANSACTION_SERIALIZABLE);\n        heartbeatLock(dbConn, ids.getLockid());\n        heartbeatTxn(dbConn, ids.getTxnid());\n      } catch (SQLException e) {\n        LOG.debug(\"Going to rollback\");\n        rollbackDBConn(dbConn);\n        checkRetryable(dbConn, e, \"heartbeat\");\n        throw new MetaException(\"Unable to select from transaction database \" +\n          StringUtils.stringifyException(e));\n      } finally {\n        closeDbConn(dbConn);\n      }\n    } catch (RetryException e) {\n      heartbeat(ids);\n    } finally {\n      deadlockCnt = 0;\n    }\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.openTxns": {
                "code_before_change": "  public OpenTxnsResponse openTxns(OpenTxnRequest rqst) throws MetaException {\n    int numTxns = rqst.getNum_txns();\n    try {\n      Connection dbConn = getDbConn(Connection.TRANSACTION_SERIALIZABLE);\n      Statement stmt = null;\n      try {\n        // Make sure the user has not requested an insane amount of txns.\n        int maxTxns = HiveConf.getIntVar(conf,\n            HiveConf.ConfVars.HIVE_TXN_MAX_OPEN_BATCH);\n        if (numTxns > maxTxns) numTxns = maxTxns;\n\n        stmt = dbConn.createStatement();\n        String s = \"select ntxn_next from NEXT_TXN_ID\";\n        LOG.debug(\"Going to execute query <\" + s + \">\");\n        ResultSet rs = stmt.executeQuery(s);\n        if (!rs.next()) {\n          throw new MetaException(\"Transaction database not properly \" +\n              \"configured, can't find next transaction id.\");\n        }\n        long first = rs.getLong(1);\n        s = \"update NEXT_TXN_ID set ntxn_next = \" + (first + numTxns);\n        LOG.debug(\"Going to execute update <\" + s + \">\");\n        stmt.executeUpdate(s);\n        long now = getDbTime(dbConn);\n        s = \"insert into TXNS (txn_id, txn_state, txn_started, \" +\n            \"txn_last_heartbeat, txn_user, txn_host) values (?, 'o', \" + now + \", \" +\n            now + \", '\" + rqst.getUser() + \"', '\" + rqst.getHostname() + \"')\";\n        LOG.debug(\"Going to prepare statement <\" + s + \">\");\n        PreparedStatement ps = dbConn.prepareStatement(s);\n        List<Long> txnIds = new ArrayList<Long>(numTxns);\n        for (long i = first; i < first + numTxns; i++) {\n          ps.setLong(1, i);\n          ps.executeUpdate();\n          txnIds.add(i);\n        }\n\n        LOG.debug(\"Going to commit\");\n        dbConn.commit();\n        return new OpenTxnsResponse(txnIds);\n      } catch (SQLException e) {\n        try {\n          LOG.debug(\"Going to rollback\");\n          dbConn.rollback();\n        } catch (SQLException e1) {\n        }\n        detectDeadlock(dbConn, e, \"openTxns\");\n        throw new MetaException(\"Unable to select from transaction database \"\n          + StringUtils.stringifyException(e));\n      } finally {\n        closeStmt(stmt);\n        closeDbConn(dbConn);\n      }\n    } catch (DeadlockException e) {\n      return openTxns(rqst);\n    } finally {\n      deadlockCnt = 0;\n    }\n  }",
                "code_after_change": "  public OpenTxnsResponse openTxns(OpenTxnRequest rqst) throws MetaException {\n    int numTxns = rqst.getNum_txns();\n    try {\n      Connection dbConn = null;\n      Statement stmt = null;\n      try {\n        dbConn = getDbConn(Connection.TRANSACTION_SERIALIZABLE);\n        // Make sure the user has not requested an insane amount of txns.\n        int maxTxns = HiveConf.getIntVar(conf,\n          HiveConf.ConfVars.HIVE_TXN_MAX_OPEN_BATCH);\n        if (numTxns > maxTxns) numTxns = maxTxns;\n\n        stmt = dbConn.createStatement();\n        String s = \"select ntxn_next from NEXT_TXN_ID\";\n        LOG.debug(\"Going to execute query <\" + s + \">\");\n        ResultSet rs = stmt.executeQuery(s);\n        if (!rs.next()) {\n          throw new MetaException(\"Transaction database not properly \" +\n            \"configured, can't find next transaction id.\");\n        }\n        long first = rs.getLong(1);\n        s = \"update NEXT_TXN_ID set ntxn_next = \" + (first + numTxns);\n        LOG.debug(\"Going to execute update <\" + s + \">\");\n        stmt.executeUpdate(s);\n        long now = getDbTime(dbConn);\n        s = \"insert into TXNS (txn_id, txn_state, txn_started, \" +\n          \"txn_last_heartbeat, txn_user, txn_host) values (?, 'o', \" + now + \", \" +\n          now + \", '\" + rqst.getUser() + \"', '\" + rqst.getHostname() + \"')\";\n        LOG.debug(\"Going to prepare statement <\" + s + \">\");\n        PreparedStatement ps = dbConn.prepareStatement(s);\n        List<Long> txnIds = new ArrayList<Long>(numTxns);\n        for (long i = first; i < first + numTxns; i++) {\n          ps.setLong(1, i);\n          ps.executeUpdate();\n          txnIds.add(i);\n        }\n\n        LOG.debug(\"Going to commit\");\n        dbConn.commit();\n        return new OpenTxnsResponse(txnIds);\n      } catch (SQLException e) {\n        LOG.debug(\"Going to rollback\");\n        rollbackDBConn(dbConn);\n        checkRetryable(dbConn, e, \"openTxns\");\n        throw new MetaException(\"Unable to select from transaction database \"\n          + StringUtils.stringifyException(e));\n      } finally {\n        closeStmt(stmt);\n        closeDbConn(dbConn);\n      }\n    } catch (RetryException e) {\n      return openTxns(rqst);\n    }\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler.markCompacted": {
                "code_before_change": "  public void markCompacted(CompactionInfo info) throws MetaException {\n    try {\n      Connection dbConn = getDbConn(Connection.TRANSACTION_SERIALIZABLE);\n      Statement stmt = null;\n      try {\n        stmt = dbConn.createStatement();\n        String s = \"update COMPACTION_QUEUE set cq_state = '\" + READY_FOR_CLEANING + \"', \" +\n            \"cq_worker_id = null where cq_id = \" + info.id;\n        LOG.debug(\"Going to execute update <\" + s + \">\");\n        if (stmt.executeUpdate(s) != 1) {\n          LOG.error(\"Unable to update compaction record\");\n          LOG.debug(\"Going to rollback\");\n          dbConn.rollback();\n        }\n        LOG.debug(\"Going to commit\");\n        dbConn.commit();\n      } catch (SQLException e) {\n        try {\n          LOG.error(\"Unable to update compaction queue \" + e.getMessage());\n          LOG.debug(\"Going to rollback\");\n          dbConn.rollback();\n        } catch (SQLException e1) {\n        }\n        detectDeadlock(dbConn, e, \"markCompacted\");\n        throw new MetaException(\"Unable to connect to transaction database \" +\n            StringUtils.stringifyException(e));\n      } finally {\n        closeDbConn(dbConn);\n        closeStmt(stmt);\n      }\n    } catch (DeadlockException e) {\n      markCompacted(info);\n    } finally {\n      deadlockCnt = 0;\n    }\n  }",
                "code_after_change": "  public void markCompacted(CompactionInfo info) throws MetaException {\n    try {\n      Connection dbConn = null;\n      Statement stmt = null;\n      try {\n        dbConn = getDbConn(Connection.TRANSACTION_SERIALIZABLE);\n        stmt = dbConn.createStatement();\n        String s = \"update COMPACTION_QUEUE set cq_state = '\" + READY_FOR_CLEANING + \"', \" +\n          \"cq_worker_id = null where cq_id = \" + info.id;\n        LOG.debug(\"Going to execute update <\" + s + \">\");\n        if (stmt.executeUpdate(s) != 1) {\n          LOG.error(\"Unable to update compaction record\");\n          LOG.debug(\"Going to rollback\");\n          dbConn.rollback();\n        }\n        LOG.debug(\"Going to commit\");\n        dbConn.commit();\n      } catch (SQLException e) {\n        LOG.error(\"Unable to update compaction queue \" + e.getMessage());\n        LOG.debug(\"Going to rollback\");\n        rollbackDBConn(dbConn);\n        checkRetryable(dbConn, e, \"markCompacted\");\n        throw new MetaException(\"Unable to connect to transaction database \" +\n          StringUtils.stringifyException(e));\n      } finally {\n        closeDbConn(dbConn);\n        closeStmt(stmt);\n      }\n    } catch (RetryException e) {\n      markCompacted(info);\n    }\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.setTimeout": {
                "code_before_change": "  long setTimeout(long milliseconds) {\n    long previous_timeout = timeout;\n    timeout = milliseconds;\n    return previous_timeout;\n  }",
                "code_after_change": "  long setTimeout(long milliseconds) {\n    long previous_timeout = timeout;\n    timeout = milliseconds;\n    return previous_timeout;\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.checkQFileTestHack": {
                "code_before_change": "  private void checkQFileTestHack() {\n    boolean hackOn = HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVE_IN_TEST) ||\n        HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVE_IN_TEZ_TEST);\n    if (hackOn) {\n      LOG.info(\"Hacking in canned values for transaction manager\");\n      // Set up the transaction/locking db in the derby metastore\n      TxnDbUtil.setConfValues(conf);\n      try {\n        TxnDbUtil.prepDb();\n      } catch (Exception e) {\n        // We may have already created the tables and thus don't need to redo it.\n        if (!e.getMessage().contains(\"already exists\")) {\n          throw new RuntimeException(\"Unable to set up transaction database for\" +\n              \" testing: \" + e.getMessage());\n        }\n      }\n    }\n  }",
                "code_after_change": "  private void checkQFileTestHack() {\n    boolean hackOn = HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVE_IN_TEST) ||\n      HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVE_IN_TEZ_TEST);\n    if (hackOn) {\n      LOG.info(\"Hacking in canned values for transaction manager\");\n      // Set up the transaction/locking db in the derby metastore\n      TxnDbUtil.setConfValues(conf);\n      try {\n        TxnDbUtil.prepDb();\n      } catch (Exception e) {\n        // We may have already created the tables and thus don't need to redo it.\n        if (!e.getMessage().contains(\"already exists\")) {\n          throw new RuntimeException(\"Unable to set up transaction database for\" +\n            \" testing: \" + e.getMessage());\n        }\n      }\n    }\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.abortTxns": {
                "code_before_change": "  private int abortTxns(Connection dbConn, List<Long> txnids) throws SQLException {\n    Statement stmt = null;\n    int updateCnt = 0;\n    try {\n      stmt = dbConn.createStatement();\n  \n      // delete from HIVE_LOCKS first, we always access HIVE_LOCKS before TXNS\n      StringBuilder buf = new StringBuilder(\"delete from HIVE_LOCKS where hl_txnid in (\");\n      boolean first = true;\n      for (Long id : txnids) {\n        if (first) first = false;\n        else buf.append(',');\n        buf.append(id);\n      }\n      buf.append(')');\n      LOG.debug(\"Going to execute update <\" + buf.toString() + \">\");\n      stmt.executeUpdate(buf.toString());\n  \n      buf = new StringBuilder(\"update TXNS set txn_state = '\" + TXN_ABORTED + \"' where txn_id in (\");\n      first = true;\n      for (Long id : txnids) {\n        if (first) first = false;\n        else buf.append(',');\n        buf.append(id);\n      }\n      buf.append(')');\n      LOG.debug(\"Going to execute update <\" + buf.toString() + \">\");\n      updateCnt = stmt.executeUpdate(buf.toString());\n  \n      LOG.debug(\"Going to commit\");\n      dbConn.commit();\n    } finally {\n      closeStmt(stmt);\n    }\n    return updateCnt;\n  }",
                "code_after_change": "  private int abortTxns(Connection dbConn, List<Long> txnids) throws SQLException {\n    Statement stmt = null;\n    int updateCnt = 0;\n    try {\n      stmt = dbConn.createStatement();\n\n      // delete from HIVE_LOCKS first, we always access HIVE_LOCKS before TXNS\n      StringBuilder buf = new StringBuilder(\"delete from HIVE_LOCKS where hl_txnid in (\");\n      boolean first = true;\n      for (Long id : txnids) {\n        if (first) first = false;\n        else buf.append(',');\n        buf.append(id);\n      }\n      buf.append(')');\n      LOG.debug(\"Going to execute update <\" + buf.toString() + \">\");\n      stmt.executeUpdate(buf.toString());\n\n      buf = new StringBuilder(\"update TXNS set txn_state = '\" + TXN_ABORTED + \"' where txn_id in (\");\n      first = true;\n      for (Long id : txnids) {\n        if (first) first = false;\n        else buf.append(',');\n        buf.append(id);\n      }\n      buf.append(')');\n      LOG.debug(\"Going to execute update <\" + buf.toString() + \">\");\n      updateCnt = stmt.executeUpdate(buf.toString());\n\n      LOG.debug(\"Going to commit\");\n      dbConn.commit();\n    } finally {\n      closeStmt(stmt);\n    }\n    return updateCnt;\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler.findReadyToClean": {
                "code_before_change": "  public List<CompactionInfo> findReadyToClean() throws MetaException {\n    Connection dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);\n    List<CompactionInfo> rc = new ArrayList<CompactionInfo>();\n\n    Statement stmt = null;\n    try {\n      stmt = dbConn.createStatement();\n      String s = \"select cq_id, cq_database, cq_table, cq_partition, \" +\n          \"cq_type, cq_run_as from COMPACTION_QUEUE where cq_state = '\" + READY_FOR_CLEANING + \"'\";\n      LOG.debug(\"Going to execute query <\" + s + \">\");\n      ResultSet rs = stmt.executeQuery(s);\n      while (rs.next()) {\n        CompactionInfo info = new CompactionInfo();\n        info.id = rs.getLong(1);\n        info.dbname = rs.getString(2);\n        info.tableName = rs.getString(3);\n        info.partName = rs.getString(4);\n        switch (rs.getString(5).charAt(0)) {\n          case MAJOR_TYPE: info.type = CompactionType.MAJOR; break;\n          case MINOR_TYPE: info.type = CompactionType.MINOR; break;\n          default: throw new MetaException(\"Unexpected compaction type \" + rs.getString(5));\n        }\n        info.runAs = rs.getString(6);\n        rc.add(info);\n      }\n      LOG.debug(\"Going to rollback\");\n      dbConn.rollback();\n      return rc;\n    } catch (SQLException e) {\n      LOG.error(\"Unable to select next element for cleaning, \" + e.getMessage());\n      try {\n        LOG.debug(\"Going to rollback\");\n        dbConn.rollback();\n      } catch (SQLException e1) {\n      }\n      throw new MetaException(\"Unable to connect to transaction database \" +\n          StringUtils.stringifyException(e));\n    } finally {\n      closeDbConn(dbConn);\n      closeStmt(stmt);\n    }\n  }",
                "code_after_change": "  public List<CompactionInfo> findReadyToClean() throws MetaException {\n    Connection dbConn = null;\n    List<CompactionInfo> rc = new ArrayList<CompactionInfo>();\n\n    Statement stmt = null;\n    try {\n      try {\n        dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);\n        stmt = dbConn.createStatement();\n        String s = \"select cq_id, cq_database, cq_table, cq_partition, \" +\n          \"cq_type, cq_run_as from COMPACTION_QUEUE where cq_state = '\" + READY_FOR_CLEANING + \"'\";\n        LOG.debug(\"Going to execute query <\" + s + \">\");\n        ResultSet rs = stmt.executeQuery(s);\n        while (rs.next()) {\n          CompactionInfo info = new CompactionInfo();\n          info.id = rs.getLong(1);\n          info.dbname = rs.getString(2);\n          info.tableName = rs.getString(3);\n          info.partName = rs.getString(4);\n          switch (rs.getString(5).charAt(0)) {\n            case MAJOR_TYPE: info.type = CompactionType.MAJOR; break;\n            case MINOR_TYPE: info.type = CompactionType.MINOR; break;\n            default: throw new MetaException(\"Unexpected compaction type \" + rs.getString(5));\n          }\n          info.runAs = rs.getString(6);\n          rc.add(info);\n        }\n        LOG.debug(\"Going to rollback\");\n        dbConn.rollback();\n        return rc;\n      } catch (SQLException e) {\n        LOG.error(\"Unable to select next element for cleaning, \" + e.getMessage());\n        LOG.debug(\"Going to rollback\");\n        rollbackDBConn(dbConn);\n        checkRetryable(dbConn, e, \"findReadyToClean\");\n        throw new MetaException(\"Unable to connect to transaction database \" +\n          StringUtils.stringifyException(e));\n      } finally {\n        closeDbConn(dbConn);\n        closeStmt(stmt);\n      }\n    } catch (RetryException e) {\n      return findReadyToClean();\n    }\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.equals": {
                "code_before_change": "    public boolean equals(Object other) {\n      if (!(other instanceof LockInfo)) return false;\n      LockInfo o = (LockInfo)other;\n      // Lock ids are unique across the system.\n      return extLockId == o.extLockId && intLockId == o.intLockId;\n    }",
                "code_after_change": "    public boolean equals(Object other) {\n      if (!(other instanceof LockInfo)) return false;\n      LockInfo o = (LockInfo)other;\n      // Lock ids are unique across the system.\n      return extLockId == o.extLockId && intLockId == o.intLockId;\n    }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.setupJdbcConnectionPool": {
                "code_before_change": "  private static synchronized void setupJdbcConnectionPool(HiveConf conf) throws SQLException {\n    if (connPool != null) return;\n\n    String driverUrl = HiveConf.getVar(conf, HiveConf.ConfVars.METASTORECONNECTURLKEY);\n    String user = HiveConf.getVar(conf, HiveConf.ConfVars.METASTORE_CONNECTION_USER_NAME);\n    String passwd;\n    try {\n      passwd = ShimLoader.getHadoopShims().getPassword(conf,\n          HiveConf.ConfVars.METASTOREPWD.varname);\n    } catch (IOException err) {\n      throw new SQLException(\"Error getting metastore password\", err);\n    }\n    String connectionPooler = HiveConf.getVar(conf,\n        HiveConf.ConfVars.METASTORE_CONNECTION_POOLING_TYPE).toLowerCase();\n\n    if (\"bonecp\".equals(connectionPooler)) {\n      BoneCPConfig config = new BoneCPConfig();\n      config.setJdbcUrl(driverUrl);\n      config.setMaxConnectionsPerPartition(10);\n      config.setPartitionCount(1);\n      config.setUser(user);\n      config.setPassword(passwd);\n      connPool = new BoneCPDataSource(config);\n    } else if (\"dbcp\".equals(connectionPooler)) {\n      ObjectPool objectPool = new GenericObjectPool();\n      ConnectionFactory connFactory = new DriverManagerConnectionFactory(driverUrl, user, passwd);\n      // This doesn't get used, but it's still necessary, see\n      // http://svn.apache.org/viewvc/commons/proper/dbcp/branches/DBCP_1_4_x_BRANCH/doc/ManualPoolingDataSourceExample.java?view=markup\n      PoolableConnectionFactory poolConnFactory =\n          new PoolableConnectionFactory(connFactory, objectPool, null, null, false, true);\n      connPool = new PoolingDataSource(objectPool);\n    } else {\n      throw new RuntimeException(\"Unknown JDBC connection pooling \" + connectionPooler);\n    }\n  }",
                "code_after_change": "  private static synchronized void setupJdbcConnectionPool(HiveConf conf) throws SQLException {\n    if (connPool != null) return;\n\n    String driverUrl = HiveConf.getVar(conf, HiveConf.ConfVars.METASTORECONNECTURLKEY);\n    String user = HiveConf.getVar(conf, HiveConf.ConfVars.METASTORE_CONNECTION_USER_NAME);\n    String passwd;\n    try {\n      passwd = ShimLoader.getHadoopShims().getPassword(conf,\n        HiveConf.ConfVars.METASTOREPWD.varname);\n    } catch (IOException err) {\n      throw new SQLException(\"Error getting metastore password\", err);\n    }\n    String connectionPooler = HiveConf.getVar(conf,\n      HiveConf.ConfVars.METASTORE_CONNECTION_POOLING_TYPE).toLowerCase();\n\n    if (\"bonecp\".equals(connectionPooler)) {\n      BoneCPConfig config = new BoneCPConfig();\n      config.setJdbcUrl(driverUrl);\n      config.setMaxConnectionsPerPartition(10);\n      config.setPartitionCount(1);\n      config.setUser(user);\n      config.setPassword(passwd);\n      connPool = new BoneCPDataSource(config);\n    } else if (\"dbcp\".equals(connectionPooler)) {\n      ObjectPool objectPool = new GenericObjectPool();\n      ConnectionFactory connFactory = new DriverManagerConnectionFactory(driverUrl, user, passwd);\n      // This doesn't get used, but it's still necessary, see\n      // http://svn.apache.org/viewvc/commons/proper/dbcp/branches/DBCP_1_4_x_BRANCH/doc/ManualPoolingDataSourceExample.java?view=markup\n      PoolableConnectionFactory poolConnFactory =\n        new PoolableConnectionFactory(connFactory, objectPool, null, null, false, true);\n      connPool = new PoolingDataSource(objectPool);\n    } else {\n      throw new RuntimeException(\"Unknown JDBC connection pooling \" + connectionPooler);\n    }\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler.cleanEmptyAbortedTxns": {
                "code_before_change": "  public void cleanEmptyAbortedTxns() throws MetaException {\n    try {\n      Connection dbConn = getDbConn(Connection.TRANSACTION_SERIALIZABLE);\n      Statement stmt = null;\n      try {\n        stmt = dbConn.createStatement();\n        String s = \"select txn_id from TXNS where \" +\n            \"txn_id not in (select tc_txnid from TXN_COMPONENTS) and \" +\n            \"txn_state = '\" + TXN_ABORTED + \"'\";\n        LOG.debug(\"Going to execute query <\" + s + \">\");\n        ResultSet rs = stmt.executeQuery(s);\n        Set<Long> txnids = new HashSet<Long>();\n        while (rs.next()) txnids.add(rs.getLong(1));\n        if (txnids.size() > 0) {\n          StringBuffer buf = new StringBuffer(\"delete from TXNS where txn_id in (\");\n          boolean first = true;\n          for (long tid : txnids) {\n            if (first) first = false;\n            else buf.append(\", \");\n            buf.append(tid);\n          }\n          buf.append(\")\");\n          LOG.debug(\"Going to execute update <\" + buf.toString() + \">\");\n          int rc = stmt.executeUpdate(buf.toString());\n          LOG.debug(\"Removed \" + rc + \" records from txns\");\n          LOG.debug(\"Going to commit\");\n          dbConn.commit();\n        }\n      } catch (SQLException e) {\n        LOG.error(\"Unable to delete from txns table \" + e.getMessage());\n        LOG.debug(\"Going to rollback\");\n        try {\n          dbConn.rollback();\n        } catch (SQLException e1) {\n        }\n        detectDeadlock(dbConn, e, \"cleanEmptyAbortedTxns\");\n        throw new MetaException(\"Unable to connect to transaction database \" +\n            StringUtils.stringifyException(e));\n      } finally {\n        closeDbConn(dbConn);\n        closeStmt(stmt);\n      }\n    } catch (DeadlockException e) {\n      cleanEmptyAbortedTxns();\n    } finally {\n      deadlockCnt = 0;\n    }\n  }",
                "code_after_change": "  public void cleanEmptyAbortedTxns() throws MetaException {\n    try {\n      Connection dbConn = null;\n      Statement stmt = null;\n      try {\n        dbConn = getDbConn(Connection.TRANSACTION_SERIALIZABLE);\n        stmt = dbConn.createStatement();\n        String s = \"select txn_id from TXNS where \" +\n          \"txn_id not in (select tc_txnid from TXN_COMPONENTS) and \" +\n          \"txn_state = '\" + TXN_ABORTED + \"'\";\n        LOG.debug(\"Going to execute query <\" + s + \">\");\n        ResultSet rs = stmt.executeQuery(s);\n        Set<Long> txnids = new HashSet<Long>();\n        while (rs.next()) txnids.add(rs.getLong(1));\n        if (txnids.size() > 0) {\n          StringBuffer buf = new StringBuffer(\"delete from TXNS where txn_id in (\");\n          boolean first = true;\n          for (long tid : txnids) {\n            if (first) first = false;\n            else buf.append(\", \");\n            buf.append(tid);\n          }\n          buf.append(\")\");\n          LOG.debug(\"Going to execute update <\" + buf.toString() + \">\");\n          int rc = stmt.executeUpdate(buf.toString());\n          LOG.debug(\"Removed \" + rc + \" records from txns\");\n          LOG.debug(\"Going to commit\");\n          dbConn.commit();\n        }\n      } catch (SQLException e) {\n        LOG.error(\"Unable to delete from txns table \" + e.getMessage());\n        LOG.debug(\"Going to rollback\");\n        rollbackDBConn(dbConn);\n        checkRetryable(dbConn, e, \"cleanEmptyAbortedTxns\");\n        throw new MetaException(\"Unable to connect to transaction database \" +\n          StringUtils.stringifyException(e));\n      } finally {\n        closeDbConn(dbConn);\n        closeStmt(stmt);\n      }\n    } catch (RetryException e) {\n      cleanEmptyAbortedTxns();\n    }\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler.findNextToCompact": {
                "code_before_change": "  public CompactionInfo findNextToCompact(String workerId) throws MetaException {\n    try {\n      Connection dbConn = getDbConn(Connection.TRANSACTION_SERIALIZABLE);\n      CompactionInfo info = new CompactionInfo();\n\n      Statement stmt = null;\n      try {\n        stmt = dbConn.createStatement();\n        String s = \"select cq_id, cq_database, cq_table, cq_partition, \" +\n            \"cq_type from COMPACTION_QUEUE where cq_state = '\" + INITIATED_STATE + \"'\";\n        LOG.debug(\"Going to execute query <\" + s + \">\");\n        ResultSet rs = stmt.executeQuery(s);\n        if (!rs.next()) {\n          LOG.debug(\"No compactions found ready to compact\");\n          dbConn.rollback();\n          return null;\n        }\n        info.id = rs.getLong(1);\n        info.dbname = rs.getString(2);\n        info.tableName = rs.getString(3);\n        info.partName = rs.getString(4);\n        switch (rs.getString(5).charAt(0)) {\n          case MAJOR_TYPE: info.type = CompactionType.MAJOR; break;\n          case MINOR_TYPE: info.type = CompactionType.MINOR; break;\n          default: throw new MetaException(\"Unexpected compaction type \" + rs.getString(5));\n        }\n\n        // Now, update this record as being worked on by this worker.\n        long now = getDbTime(dbConn);\n        s = \"update COMPACTION_QUEUE set cq_worker_id = '\" + workerId + \"', \" +\n            \"cq_start = \" + now + \", cq_state = '\" + WORKING_STATE + \"' where cq_id = \" + info.id;\n        LOG.debug(\"Going to execute update <\" + s + \">\");\n        if (stmt.executeUpdate(s) != 1) {\n          LOG.error(\"Unable to update compaction record\");\n          LOG.debug(\"Going to rollback\");\n          dbConn.rollback();\n        }\n        LOG.debug(\"Going to commit\");\n        dbConn.commit();\n        return info;\n      } catch (SQLException e) {\n        LOG.error(\"Unable to select next element for compaction, \" + e.getMessage());\n        try {\n          LOG.debug(\"Going to rollback\");\n          dbConn.rollback();\n        } catch (SQLException e1) {\n        }\n        detectDeadlock(dbConn, e, \"findNextToCompact\");\n        throw new MetaException(\"Unable to connect to transaction database \" +\n            StringUtils.stringifyException(e));\n      } finally {\n        closeDbConn(dbConn);\n        closeStmt(stmt);\n      }\n    } catch (DeadlockException e) {\n      return findNextToCompact(workerId);\n    } finally {\n      deadlockCnt = 0;\n    }\n  }",
                "code_after_change": "  public CompactionInfo findNextToCompact(String workerId) throws MetaException {\n    try {\n      Connection dbConn = null;\n      CompactionInfo info = new CompactionInfo();\n\n      Statement stmt = null;\n      try {\n        dbConn = getDbConn(Connection.TRANSACTION_SERIALIZABLE);\n        stmt = dbConn.createStatement();\n        String s = \"select cq_id, cq_database, cq_table, cq_partition, \" +\n          \"cq_type from COMPACTION_QUEUE where cq_state = '\" + INITIATED_STATE + \"'\";\n        LOG.debug(\"Going to execute query <\" + s + \">\");\n        ResultSet rs = stmt.executeQuery(s);\n        if (!rs.next()) {\n          LOG.debug(\"No compactions found ready to compact\");\n          dbConn.rollback();\n          return null;\n        }\n        info.id = rs.getLong(1);\n        info.dbname = rs.getString(2);\n        info.tableName = rs.getString(3);\n        info.partName = rs.getString(4);\n        switch (rs.getString(5).charAt(0)) {\n          case MAJOR_TYPE: info.type = CompactionType.MAJOR; break;\n          case MINOR_TYPE: info.type = CompactionType.MINOR; break;\n          default: throw new MetaException(\"Unexpected compaction type \" + rs.getString(5));\n        }\n\n        // Now, update this record as being worked on by this worker.\n        long now = getDbTime(dbConn);\n        s = \"update COMPACTION_QUEUE set cq_worker_id = '\" + workerId + \"', \" +\n          \"cq_start = \" + now + \", cq_state = '\" + WORKING_STATE + \"' where cq_id = \" + info.id;\n        LOG.debug(\"Going to execute update <\" + s + \">\");\n        if (stmt.executeUpdate(s) != 1) {\n          LOG.error(\"Unable to update compaction record\");\n          LOG.debug(\"Going to rollback\");\n          dbConn.rollback();\n        }\n        LOG.debug(\"Going to commit\");\n        dbConn.commit();\n        return info;\n      } catch (SQLException e) {\n        LOG.error(\"Unable to select next element for compaction, \" + e.getMessage());\n        LOG.debug(\"Going to rollback\");\n        rollbackDBConn(dbConn);\n        checkRetryable(dbConn, e, \"findNextToCompact\");\n        throw new MetaException(\"Unable to connect to transaction database \" +\n          StringUtils.stringifyException(e));\n      } finally {\n        closeDbConn(dbConn);\n        closeStmt(stmt);\n      }\n    } catch (RetryException e) {\n      return findNextToCompact(workerId);\n    }\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.createValidTxnList": {
                "code_before_change": "  public static ValidTxnList createValidTxnList(GetOpenTxnsResponse txns, long currentTxn) {\n    long highWater = txns.getTxn_high_water_mark();\n    Set<Long> open = txns.getOpen_txns();\n    long[] exceptions = new long[open.size() - (currentTxn > 0 ? 1 : 0)];\n    int i = 0;\n    for(long txn: open) {\n      if (currentTxn > 0 && currentTxn == txn) continue;\n      exceptions[i++] = txn;\n    }\n    return new ValidTxnListImpl(exceptions, highWater);\n  }",
                "code_after_change": "  public static ValidTxnList createValidTxnList(GetOpenTxnsResponse txns, long currentTxn) {\n    long highWater = txns.getTxn_high_water_mark();\n    Set<Long> open = txns.getOpen_txns();\n    long[] exceptions = new long[open.size() - (currentTxn > 0 ? 1 : 0)];\n    int i = 0;\n    for(long txn: open) {\n      if (currentTxn > 0 && currentTxn == txn) continue;\n      exceptions[i++] = txn;\n    }\n    return new ValidTxnListImpl(exceptions, highWater);\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.checkLock": {
                "code_before_change": "  public LockResponse checkLock(CheckLockRequest rqst)\n      throws NoSuchTxnException, NoSuchLockException, TxnAbortedException, MetaException {\n    try {\n      Connection dbConn = getDbConn(Connection.TRANSACTION_SERIALIZABLE);\n      try {\n        long extLockId = rqst.getLockid();\n        // Clean up timed out locks\n        timeOutLocks(dbConn);\n\n        // Heartbeat on the lockid first, to assure that our lock is still valid.\n        // Then look up the lock info (hopefully in the cache).  If these locks\n        // are associated with a transaction then heartbeat on that as well.\n        heartbeatLock(dbConn, extLockId);\n        long txnid = getTxnIdFromLockId(dbConn, extLockId);\n        if (txnid > 0)  heartbeatTxn(dbConn, txnid);\n        return checkLock(dbConn, extLockId, true);\n      } catch (SQLException e) {\n        try {\n          LOG.debug(\"Going to rollback\");\n          dbConn.rollback();\n        } catch (SQLException e1) {\n        }\n        detectDeadlock(dbConn, e, \"checkLock\");\n        throw new MetaException(\"Unable to update transaction database \" +\n            StringUtils.stringifyException(e));\n      } finally {\n        closeDbConn(dbConn);\n      }\n    } catch (DeadlockException e) {\n      return checkLock(rqst);\n    } finally {\n      deadlockCnt = 0;\n    }\n\n  }",
                "code_after_change": "  public LockResponse checkLock(CheckLockRequest rqst)\n    throws NoSuchTxnException, NoSuchLockException, TxnAbortedException, MetaException {\n    try {\n      Connection dbConn = null;\n      try {\n        dbConn = getDbConn(Connection.TRANSACTION_SERIALIZABLE);\n        long extLockId = rqst.getLockid();\n        // Clean up timed out locks\n        timeOutLocks(dbConn);\n\n        // Heartbeat on the lockid first, to assure that our lock is still valid.\n        // Then look up the lock info (hopefully in the cache).  If these locks\n        // are associated with a transaction then heartbeat on that as well.\n        heartbeatLock(dbConn, extLockId);\n        long txnid = getTxnIdFromLockId(dbConn, extLockId);\n        if (txnid > 0)  heartbeatTxn(dbConn, txnid);\n        return checkLock(dbConn, extLockId, true);\n      } catch (SQLException e) {\n        LOG.debug(\"Going to rollback\");\n        rollbackDBConn(dbConn);\n        checkRetryable(dbConn, e, \"checkLock\");\n        throw new MetaException(\"Unable to update transaction database \" +\n          StringUtils.stringifyException(e));\n      } finally {\n        closeDbConn(dbConn);\n      }\n    } catch (RetryException e) {\n      return checkLock(rqst);\n    }\n\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.close": {
                "code_before_change": "  void close(ResultSet rs) {\n    try {\n      if (rs != null && !rs.isClosed()) {\n        rs.close();\n      }\n    }\n    catch(SQLException ex) {\n      LOG.warn(\"Failed to close statement \" + ex.getMessage());\n    }\n  }",
                "code_after_change": "  void close(ResultSet rs) {\n    try {\n      if (rs != null && !rs.isClosed()) {\n        rs.close();\n      }\n    }\n    catch(SQLException ex) {\n      LOG.warn(\"Failed to close statement \" + getMessage(ex));\n    }\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.heartbeatLock": {
                "code_before_change": "  private void heartbeatLock(Connection dbConn, long extLockId)\n      throws NoSuchLockException, SQLException, MetaException {\n    // If the lock id is 0, then there are no locks in this heartbeat\n    if (extLockId == 0) return;\n    Statement stmt = null;\n    try {\n      stmt = dbConn.createStatement();\n      long now = getDbTime(dbConn);\n\n      String s = \"update HIVE_LOCKS set hl_last_heartbeat = \" +\n          now + \" where hl_lock_ext_id = \" + extLockId;\n      LOG.debug(\"Going to execute update <\" + s + \">\");\n      int rc = stmt.executeUpdate(s);\n      if (rc < 1) {\n        LOG.debug(\"Going to rollback\");\n        dbConn.rollback();\n        throw new NoSuchLockException(\"No such lock: \" + extLockId);\n      }\n      LOG.debug(\"Going to commit\");\n      dbConn.commit();\n    } finally {\n      closeStmt(stmt);\n    }\n  }",
                "code_after_change": "  private void heartbeatLock(Connection dbConn, long extLockId)\n    throws NoSuchLockException, SQLException, MetaException {\n    // If the lock id is 0, then there are no locks in this heartbeat\n    if (extLockId == 0) return;\n    Statement stmt = null;\n    try {\n      stmt = dbConn.createStatement();\n      long now = getDbTime(dbConn);\n\n      String s = \"update HIVE_LOCKS set hl_last_heartbeat = \" +\n        now + \" where hl_lock_ext_id = \" + extLockId;\n      LOG.debug(\"Going to execute update <\" + s + \">\");\n      int rc = stmt.executeUpdate(s);\n      if (rc < 1) {\n        LOG.debug(\"Going to rollback\");\n        dbConn.rollback();\n        throw new NoSuchLockException(\"No such lock: \" + extLockId);\n      }\n      LOG.debug(\"Going to commit\");\n      dbConn.commit();\n    } finally {\n      closeStmt(stmt);\n    }\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.timeOutLocks": {
                "code_before_change": "  private void timeOutLocks(Connection dbConn) throws SQLException, MetaException {\n    long now = getDbTime(dbConn);\n    Statement stmt = null;\n    try {\n      stmt = dbConn.createStatement();\n      // Remove any timed out locks from the table.\n      String s = \"delete from HIVE_LOCKS where hl_last_heartbeat < \" +\n          (now - timeout);\n      LOG.debug(\"Going to execute update <\" + s + \">\");\n      stmt.executeUpdate(s);\n      LOG.debug(\"Going to commit\");\n      dbConn.commit();\n    } finally {\n      closeStmt(stmt);\n    }\n  }",
                "code_after_change": "  private void timeOutLocks(Connection dbConn) throws SQLException, MetaException {\n    long now = getDbTime(dbConn);\n    Statement stmt = null;\n    try {\n      stmt = dbConn.createStatement();\n      // Remove any timed out locks from the table.\n      String s = \"delete from HIVE_LOCKS where hl_last_heartbeat < \" +\n        (now - timeout);\n      LOG.debug(\"Going to execute update <\" + s + \">\");\n      stmt.executeUpdate(s);\n      LOG.debug(\"Going to commit\");\n      dbConn.commit();\n    } finally {\n      closeStmt(stmt);\n    }\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.TxnHandler": {
                "code_before_change": [],
                "code_after_change": []
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler.markCleaned": {
                "code_before_change": "  public void markCleaned(CompactionInfo info) throws MetaException {\n    try {\n      Connection dbConn = getDbConn(Connection.TRANSACTION_SERIALIZABLE);\n      Statement stmt = null;\n      try {\n        stmt = dbConn.createStatement();\n        String s = \"delete from COMPACTION_QUEUE where cq_id = \" + info.id;\n        LOG.debug(\"Going to execute update <\" + s + \">\");\n        if (stmt.executeUpdate(s) != 1) {\n          LOG.error(\"Unable to delete compaction record\");\n          LOG.debug(\"Going to rollback\");\n          dbConn.rollback();\n        }\n\n        // Remove entries from completed_txn_components as well, so we don't start looking there\n        // again.\n        s = \"delete from COMPLETED_TXN_COMPONENTS where ctc_database = '\" + info.dbname + \"' and \" +\n            \"ctc_table = '\" + info.tableName + \"'\";\n        if (info.partName != null) {\n          s += \" and ctc_partition = '\" + info.partName + \"'\";\n        }\n        LOG.debug(\"Going to execute update <\" + s + \">\");\n        if (stmt.executeUpdate(s) < 1) {\n          LOG.error(\"Expected to remove at least one row from completed_txn_components when \" +\n              \"marking compaction entry as clean!\");\n        }\n\n\n        s = \"select txn_id from TXNS, TXN_COMPONENTS where txn_id = tc_txnid and txn_state = '\" +\n            TXN_ABORTED + \"' and tc_database = '\" + info.dbname + \"' and tc_table = '\" +\n            info.tableName + \"'\";\n        if (info.partName != null) s += \" and tc_partition = '\" + info.partName + \"'\";\n        LOG.debug(\"Going to execute update <\" + s + \">\");\n        ResultSet rs = stmt.executeQuery(s);\n        Set<Long> txnids = new HashSet<Long>();\n        while (rs.next()) txnids.add(rs.getLong(1));\n        if (txnids.size() > 0) {\n\n          // Remove entries from txn_components, as there may be aborted txn components\n          StringBuffer buf = new StringBuffer();\n          buf.append(\"delete from TXN_COMPONENTS where tc_txnid in (\");\n          boolean first = true;\n          for (long id : txnids) {\n            if (first) first = false;\n            else buf.append(\", \");\n            buf.append(id);\n          }\n\n          buf.append(\") and tc_database = '\");\n          buf.append(info.dbname);\n          buf.append(\"' and tc_table = '\");\n          buf.append(info.tableName);\n          buf.append(\"'\");\n          if (info.partName != null) {\n            buf.append(\" and tc_partition = '\");\n            buf.append(info.partName);\n            buf.append(\"'\");\n          }\n          LOG.debug(\"Going to execute update <\" + buf.toString() + \">\");\n          int rc = stmt.executeUpdate(buf.toString());\n          LOG.debug(\"Removed \" + rc + \" records from txn_components\");\n\n          // Don't bother cleaning from the txns table.  A separate call will do that.  We don't\n          // know here which txns still have components from other tables or partitions in the\n          // table, so we don't know which ones we can and cannot clean.\n        }\n\n        LOG.debug(\"Going to commit\");\n        dbConn.commit();\n      } catch (SQLException e) {\n        try {\n          LOG.error(\"Unable to delete from compaction queue \" + e.getMessage());\n          LOG.debug(\"Going to rollback\");\n          dbConn.rollback();\n        } catch (SQLException e1) {\n        }\n        detectDeadlock(dbConn, e, \"markCleaned\");\n        throw new MetaException(\"Unable to connect to transaction database \" +\n            StringUtils.stringifyException(e));\n      } finally {\n        closeDbConn(dbConn);\n        closeStmt(stmt);\n      }\n    } catch (DeadlockException e) {\n      markCleaned(info);\n    } finally {\n      deadlockCnt = 0;\n    }\n  }",
                "code_after_change": "  public void markCleaned(CompactionInfo info) throws MetaException {\n    try {\n      Connection dbConn = null;\n      Statement stmt = null;\n      try {\n        dbConn = getDbConn(Connection.TRANSACTION_SERIALIZABLE);\n        stmt = dbConn.createStatement();\n        String s = \"delete from COMPACTION_QUEUE where cq_id = \" + info.id;\n        LOG.debug(\"Going to execute update <\" + s + \">\");\n        if (stmt.executeUpdate(s) != 1) {\n          LOG.error(\"Unable to delete compaction record\");\n          LOG.debug(\"Going to rollback\");\n          dbConn.rollback();\n        }\n\n        // Remove entries from completed_txn_components as well, so we don't start looking there\n        // again.\n        s = \"delete from COMPLETED_TXN_COMPONENTS where ctc_database = '\" + info.dbname + \"' and \" +\n          \"ctc_table = '\" + info.tableName + \"'\";\n        if (info.partName != null) {\n          s += \" and ctc_partition = '\" + info.partName + \"'\";\n        }\n        LOG.debug(\"Going to execute update <\" + s + \">\");\n        if (stmt.executeUpdate(s) < 1) {\n          LOG.error(\"Expected to remove at least one row from completed_txn_components when \" +\n            \"marking compaction entry as clean!\");\n        }\n\n\n        s = \"select txn_id from TXNS, TXN_COMPONENTS where txn_id = tc_txnid and txn_state = '\" +\n          TXN_ABORTED + \"' and tc_database = '\" + info.dbname + \"' and tc_table = '\" +\n          info.tableName + \"'\";\n        if (info.partName != null) s += \" and tc_partition = '\" + info.partName + \"'\";\n        LOG.debug(\"Going to execute update <\" + s + \">\");\n        ResultSet rs = stmt.executeQuery(s);\n        Set<Long> txnids = new HashSet<Long>();\n        while (rs.next()) txnids.add(rs.getLong(1));\n        if (txnids.size() > 0) {\n\n          // Remove entries from txn_components, as there may be aborted txn components\n          StringBuffer buf = new StringBuffer();\n          buf.append(\"delete from TXN_COMPONENTS where tc_txnid in (\");\n          boolean first = true;\n          for (long id : txnids) {\n            if (first) first = false;\n            else buf.append(\", \");\n            buf.append(id);\n          }\n\n          buf.append(\") and tc_database = '\");\n          buf.append(info.dbname);\n          buf.append(\"' and tc_table = '\");\n          buf.append(info.tableName);\n          buf.append(\"'\");\n          if (info.partName != null) {\n            buf.append(\" and tc_partition = '\");\n            buf.append(info.partName);\n            buf.append(\"'\");\n          }\n          LOG.debug(\"Going to execute update <\" + buf.toString() + \">\");\n          int rc = stmt.executeUpdate(buf.toString());\n          LOG.debug(\"Removed \" + rc + \" records from txn_components\");\n\n          // Don't bother cleaning from the txns table.  A separate call will do that.  We don't\n          // know here which txns still have components from other tables or partitions in the\n          // table, so we don't know which ones we can and cannot clean.\n        }\n\n        LOG.debug(\"Going to commit\");\n        dbConn.commit();\n      } catch (SQLException e) {\n        LOG.error(\"Unable to delete from compaction queue \" + e.getMessage());\n        LOG.debug(\"Going to rollback\");\n        rollbackDBConn(dbConn);\n        checkRetryable(dbConn, e, \"markCleaned\");\n        throw new MetaException(\"Unable to connect to transaction database \" +\n          StringUtils.stringifyException(e));\n      } finally {\n        closeDbConn(dbConn);\n        closeStmt(stmt);\n      }\n    } catch (RetryException e) {\n      markCleaned(info);\n    }\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.commitTxn": {
                "code_before_change": "  public void commitTxn(CommitTxnRequest rqst)\n      throws NoSuchTxnException, TxnAbortedException,  MetaException {\n    long txnid = rqst.getTxnid();\n    try {\n      Connection dbConn = getDbConn(Connection.TRANSACTION_SERIALIZABLE);\n      Statement stmt = null;\n      try {\n        stmt = dbConn.createStatement();\n        // Before we do the commit heartbeat the txn.  This is slightly odd in that we're going to\n        // commit it, but it does two things.  One, it makes sure the transaction is still valid.\n        // Two, it avoids the race condition where we time out between now and when we actually\n        // commit the transaction below.  And it does this all in a dead-lock safe way by\n        // committing the heartbeat back to the database.\n        heartbeatTxn(dbConn, txnid);\n\n        // Move the record from txn_components into completed_txn_components so that the compactor\n        // knows where to look to compact.\n        String s = \"insert into COMPLETED_TXN_COMPONENTS select tc_txnid, tc_database, tc_table, \" +\n            \"tc_partition from TXN_COMPONENTS where tc_txnid = \" + txnid;\n        LOG.debug(\"Going to execute insert <\" + s + \">\");\n        if (stmt.executeUpdate(s) < 1) {\n          LOG.warn(\"Expected to move at least one record from txn_components to \" +\n              \"completed_txn_components when committing txn!\");\n        }\n\n        // Always access TXN_COMPONENTS before HIVE_LOCKS;\n        s = \"delete from TXN_COMPONENTS where tc_txnid = \" + txnid;\n        LOG.debug(\"Going to execute update <\" + s + \">\");\n        stmt.executeUpdate(s);\n        // Always access HIVE_LOCKS before TXNS\n        s = \"delete from HIVE_LOCKS where hl_txnid = \" + txnid;\n        LOG.debug(\"Going to execute update <\" + s + \">\");\n        stmt.executeUpdate(s);\n        s = \"delete from TXNS where txn_id = \" + txnid;\n        LOG.debug(\"Going to execute update <\" + s + \">\");\n        stmt.executeUpdate(s);\n        LOG.debug(\"Going to commit\");\n        dbConn.commit();\n      } catch (SQLException e) {\n        try {\n          LOG.debug(\"Going to rollback\");\n          dbConn.rollback();\n        } catch (SQLException e1) {\n        }\n        detectDeadlock(dbConn, e, \"commitTxn\");\n        throw new MetaException(\"Unable to update transaction database \"\n          + StringUtils.stringifyException(e));\n      } finally {\n        closeStmt(stmt);\n        closeDbConn(dbConn);\n      }\n    } catch (DeadlockException e) {\n      commitTxn(rqst);\n    } finally {\n      deadlockCnt = 0;\n    }\n  }",
                "code_after_change": "  public void commitTxn(CommitTxnRequest rqst)\n    throws NoSuchTxnException, TxnAbortedException,  MetaException {\n    long txnid = rqst.getTxnid();\n    try {\n      Connection dbConn = null;\n      Statement stmt = null;\n      try {\n        dbConn = getDbConn(Connection.TRANSACTION_SERIALIZABLE);\n        stmt = dbConn.createStatement();\n        // Before we do the commit heartbeat the txn.  This is slightly odd in that we're going to\n        // commit it, but it does two things.  One, it makes sure the transaction is still valid.\n        // Two, it avoids the race condition where we time out between now and when we actually\n        // commit the transaction below.  And it does this all in a dead-lock safe way by\n        // committing the heartbeat back to the database.\n        heartbeatTxn(dbConn, txnid);\n\n        // Move the record from txn_components into completed_txn_components so that the compactor\n        // knows where to look to compact.\n        String s = \"insert into COMPLETED_TXN_COMPONENTS select tc_txnid, tc_database, tc_table, \" +\n          \"tc_partition from TXN_COMPONENTS where tc_txnid = \" + txnid;\n        LOG.debug(\"Going to execute insert <\" + s + \">\");\n        if (stmt.executeUpdate(s) < 1) {\n          LOG.warn(\"Expected to move at least one record from txn_components to \" +\n            \"completed_txn_components when committing txn!\");\n        }\n\n        // Always access TXN_COMPONENTS before HIVE_LOCKS;\n        s = \"delete from TXN_COMPONENTS where tc_txnid = \" + txnid;\n        LOG.debug(\"Going to execute update <\" + s + \">\");\n        stmt.executeUpdate(s);\n        // Always access HIVE_LOCKS before TXNS\n        s = \"delete from HIVE_LOCKS where hl_txnid = \" + txnid;\n        LOG.debug(\"Going to execute update <\" + s + \">\");\n        stmt.executeUpdate(s);\n        s = \"delete from TXNS where txn_id = \" + txnid;\n        LOG.debug(\"Going to execute update <\" + s + \">\");\n        stmt.executeUpdate(s);\n        LOG.debug(\"Going to commit\");\n        dbConn.commit();\n      } catch (SQLException e) {\n        LOG.debug(\"Going to rollback\");\n        rollbackDBConn(dbConn);\n        checkRetryable(dbConn, e, \"commitTxn\");\n        throw new MetaException(\"Unable to update transaction database \"\n          + StringUtils.stringifyException(e));\n      } finally {\n        closeStmt(stmt);\n        closeDbConn(dbConn);\n      }\n    } catch (RetryException e) {\n      commitTxn(rqst);\n    }\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.heartbeatTxn": {
                "code_before_change": "  private void heartbeatTxn(Connection dbConn, long txnid)\n      throws NoSuchTxnException, TxnAbortedException, SQLException, MetaException {\n    // If the txnid is 0, then there are no transactions in this heartbeat\n    if (txnid == 0) return;\n    Statement stmt = null;\n    try {\n      stmt = dbConn.createStatement();\n      long now = getDbTime(dbConn);\n      // We need to check whether this transaction is valid and open\n      String s = \"select txn_state from TXNS where txn_id = \" + txnid;\n      LOG.debug(\"Going to execute query <\" + s + \">\");\n      ResultSet rs = stmt.executeQuery(s);\n      if (!rs.next()) {\n        LOG.debug(\"Going to rollback\");\n        dbConn.rollback();\n        throw new NoSuchTxnException(\"No such transaction: \" + txnid);\n      }\n      if (rs.getString(1).charAt(0) == TXN_ABORTED) {\n        LOG.debug(\"Going to rollback\");\n        dbConn.rollback();\n        throw new TxnAbortedException(\"Transaction \" + txnid +\n            \" already aborted\");\n      }\n      s = \"update TXNS set txn_last_heartbeat = \" + now +\n          \" where txn_id = \" + txnid;\n      LOG.debug(\"Going to execute update <\" + s + \">\");\n      stmt.executeUpdate(s);\n      LOG.debug(\"Going to commit\");\n      dbConn.commit();\n    } finally {\n      closeStmt(stmt);\n    }\n  }",
                "code_after_change": "  private void heartbeatTxn(Connection dbConn, long txnid)\n    throws NoSuchTxnException, TxnAbortedException, SQLException, MetaException {\n    // If the txnid is 0, then there are no transactions in this heartbeat\n    if (txnid == 0) return;\n    Statement stmt = null;\n    try {\n      stmt = dbConn.createStatement();\n      long now = getDbTime(dbConn);\n      // We need to check whether this transaction is valid and open\n      String s = \"select txn_state from TXNS where txn_id = \" + txnid;\n      LOG.debug(\"Going to execute query <\" + s + \">\");\n      ResultSet rs = stmt.executeQuery(s);\n      if (!rs.next()) {\n        LOG.debug(\"Going to rollback\");\n        dbConn.rollback();\n        throw new NoSuchTxnException(\"No such transaction: \" + txnid);\n      }\n      if (rs.getString(1).charAt(0) == TXN_ABORTED) {\n        LOG.debug(\"Going to rollback\");\n        dbConn.rollback();\n        throw new TxnAbortedException(\"Transaction \" + txnid +\n          \" already aborted\");\n      }\n      s = \"update TXNS set txn_last_heartbeat = \" + now +\n        \" where txn_id = \" + txnid;\n      LOG.debug(\"Going to execute update <\" + s + \">\");\n      stmt.executeUpdate(s);\n      LOG.debug(\"Going to commit\");\n      dbConn.commit();\n    } finally {\n      closeStmt(stmt);\n    }\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler.CompactionTxnHandler": {
                "code_before_change": [],
                "code_after_change": []
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.timeOutTxns": {
                "code_before_change": "  private void timeOutTxns(Connection dbConn) throws SQLException, MetaException {\n    long now = getDbTime(dbConn);\n    Statement stmt = null;\n    try {\n      stmt = dbConn.createStatement();\n      // Abort any timed out locks from the table.\n      String s = \"select txn_id from TXNS where txn_state = '\" + TXN_OPEN +\n          \"' and txn_last_heartbeat <  \" + (now - timeout);\n      LOG.debug(\"Going to execute query <\" + s + \">\");\n      ResultSet rs = stmt.executeQuery(s);\n      List<Long> deadTxns = new ArrayList<Long>();\n      // Limit the number of timed out transactions we do in one pass to keep from generating a\n      // huge delete statement\n      for (int i = 0; i < 20 && rs.next(); i++) deadTxns.add(rs.getLong(1));\n      // We don't care whether all of the transactions get deleted or not,\n      // if some didn't it most likely means someone else deleted them in the interum\n      if (deadTxns.size() > 0) abortTxns(dbConn, deadTxns);\n    } finally {\n      closeStmt(stmt);\n    }\n  }",
                "code_after_change": "  private void timeOutTxns(Connection dbConn) throws SQLException, MetaException {\n    long now = getDbTime(dbConn);\n    Statement stmt = null;\n    try {\n      stmt = dbConn.createStatement();\n      // Abort any timed out locks from the table.\n      String s = \"select txn_id from TXNS where txn_state = '\" + TXN_OPEN +\n        \"' and txn_last_heartbeat <  \" + (now - timeout);\n      LOG.debug(\"Going to execute query <\" + s + \">\");\n      ResultSet rs = stmt.executeQuery(s);\n      List<Long> deadTxns = new ArrayList<Long>();\n      // Limit the number of timed out transactions we do in one pass to keep from generating a\n      // huge delete statement\n      for (int i = 0; i < 20 && rs.next(); i++) deadTxns.add(rs.getLong(1));\n      // We don't care whether all of the transactions get deleted or not,\n      // if some didn't it most likely means someone else deleted them in the interum\n      if (deadTxns.size() > 0) abortTxns(dbConn, deadTxns);\n    } finally {\n      closeStmt(stmt);\n    }\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.heartbeatTxnRange": {
                "code_before_change": "  public HeartbeatTxnRangeResponse heartbeatTxnRange(HeartbeatTxnRangeRequest rqst)\n      throws MetaException {\n    try {\n      Connection dbConn = getDbConn(Connection.TRANSACTION_SERIALIZABLE);\n      HeartbeatTxnRangeResponse rsp = new HeartbeatTxnRangeResponse();\n      Set<Long> nosuch = new HashSet<Long>();\n      Set<Long> aborted = new HashSet<Long>();\n      rsp.setNosuch(nosuch);\n      rsp.setAborted(aborted);\n      try {\n        for (long txn = rqst.getMin(); txn <= rqst.getMax(); txn++) {\n          try {\n            heartbeatTxn(dbConn, txn);\n          } catch (NoSuchTxnException e) {\n            nosuch.add(txn);\n          } catch (TxnAbortedException e) {\n            aborted.add(txn);\n          }\n        }\n        return rsp;\n      } catch (SQLException e) {\n        try {\n          LOG.debug(\"Going to rollback\");\n          dbConn.rollback();\n        } catch (SQLException e1) {\n        }\n        detectDeadlock(dbConn, e, \"heartbeatTxnRange\");\n        throw new MetaException(\"Unable to select from transaction database \" +\n            StringUtils.stringifyException(e));\n      } finally {\n        closeDbConn(dbConn);\n      }\n    } catch (DeadlockException e) {\n      return heartbeatTxnRange(rqst);\n    }\n  }",
                "code_after_change": "  public HeartbeatTxnRangeResponse heartbeatTxnRange(HeartbeatTxnRangeRequest rqst)\n    throws MetaException {\n    try {\n      Connection dbConn = null;\n      HeartbeatTxnRangeResponse rsp = new HeartbeatTxnRangeResponse();\n      Set<Long> nosuch = new HashSet<Long>();\n      Set<Long> aborted = new HashSet<Long>();\n      rsp.setNosuch(nosuch);\n      rsp.setAborted(aborted);\n      try {\n        dbConn = getDbConn(Connection.TRANSACTION_SERIALIZABLE);\n        for (long txn = rqst.getMin(); txn <= rqst.getMax(); txn++) {\n          try {\n            heartbeatTxn(dbConn, txn);\n          } catch (NoSuchTxnException e) {\n            nosuch.add(txn);\n          } catch (TxnAbortedException e) {\n            aborted.add(txn);\n          }\n        }\n        return rsp;\n      } catch (SQLException e) {\n        LOG.debug(\"Going to rollback\");\n        rollbackDBConn(dbConn);\n        checkRetryable(dbConn, e, \"heartbeatTxnRange\");\n        throw new MetaException(\"Unable to select from transaction database \" +\n          StringUtils.stringifyException(e));\n      } finally {\n        closeDbConn(dbConn);\n      }\n    } catch (RetryException e) {\n      return heartbeatTxnRange(rqst);\n    }\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.lock": {
                "code_before_change": "  public LockResponse lock(LockRequest rqst)\n      throws NoSuchTxnException, TxnAbortedException, MetaException {\n    try {\n      Connection dbConn = getDbConn(Connection.TRANSACTION_SERIALIZABLE);\n      try {\n        return lock(dbConn, rqst, true);\n      } catch (SQLException e) {\n        try {\n          LOG.debug(\"Going to rollback\");\n          dbConn.rollback();\n        } catch (SQLException e1) {\n        }\n        detectDeadlock(dbConn, e, \"lock\");\n        throw new MetaException(\"Unable to update transaction database \" +\n            StringUtils.stringifyException(e));\n      } finally {\n        closeDbConn(dbConn);\n      }\n    } catch (DeadlockException e) {\n      return lock(rqst);\n    } finally {\n      deadlockCnt = 0;\n    }\n  }",
                "code_after_change": "  public LockResponse lock(LockRequest rqst)\n    throws NoSuchTxnException, TxnAbortedException, MetaException {\n    try {\n      Connection dbConn = null;\n      try {\n        dbConn = getDbConn(Connection.TRANSACTION_SERIALIZABLE);\n        return lock(dbConn, rqst, true);\n      } catch (SQLException e) {\n        LOG.debug(\"Going to rollback\");\n        rollbackDBConn(dbConn);\n        checkRetryable(dbConn, e, \"lock\");\n        throw new MetaException(\"Unable to update transaction database \" +\n          StringUtils.stringifyException(e));\n      } finally {\n        closeDbConn(dbConn);\n      }\n    } catch (RetryException e) {\n      return lock(rqst);\n    }\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler.setRunAs": {
                "code_before_change": "  public void setRunAs(long cq_id, String user) throws MetaException {\n    try {\n      Connection dbConn = getDbConn(Connection.TRANSACTION_SERIALIZABLE);\n      Statement stmt = null;\n      try {\n       stmt = dbConn.createStatement();\n       String s = \"update COMPACTION_QUEUE set cq_run_as = '\" + user + \"' where cq_id = \" + cq_id;\n       LOG.debug(\"Going to execute update <\" + s + \">\");\n       if (stmt.executeUpdate(s) != 1) {\n         LOG.error(\"Unable to update compaction record\");\n         LOG.debug(\"Going to rollback\");\n         dbConn.rollback();\n       }\n       LOG.debug(\"Going to commit\");\n       dbConn.commit();\n     } catch (SQLException e) {\n       LOG.error(\"Unable to update compaction queue, \" + e.getMessage());\n       try {\n         LOG.debug(\"Going to rollback\");\n         dbConn.rollback();\n       } catch (SQLException e1) {\n       }\n       detectDeadlock(dbConn, e, \"setRunAs\");\n     } finally {\n       closeDbConn(dbConn);\n       closeStmt(stmt);\n     }\n    } catch (DeadlockException e) {\n      setRunAs(cq_id, user);\n    } finally {\n      deadlockCnt = 0;\n    }\n  }",
                "code_after_change": "  public void setRunAs(long cq_id, String user) throws MetaException {\n    try {\n      Connection dbConn = null;\n      Statement stmt = null;\n      try {\n        dbConn = getDbConn(Connection.TRANSACTION_SERIALIZABLE);\n        stmt = dbConn.createStatement();\n        String s = \"update COMPACTION_QUEUE set cq_run_as = '\" + user + \"' where cq_id = \" + cq_id;\n        LOG.debug(\"Going to execute update <\" + s + \">\");\n        if (stmt.executeUpdate(s) != 1) {\n          LOG.error(\"Unable to update compaction record\");\n          LOG.debug(\"Going to rollback\");\n          dbConn.rollback();\n        }\n        LOG.debug(\"Going to commit\");\n        dbConn.commit();\n      } catch (SQLException e) {\n        LOG.error(\"Unable to update compaction queue, \" + e.getMessage());\n        LOG.debug(\"Going to rollback\");\n        rollbackDBConn(dbConn);\n        checkRetryable(dbConn, e, \"setRunAs\");\n      } finally {\n        closeDbConn(dbConn);\n        closeStmt(stmt);\n      }\n    } catch (RetryException e) {\n      setRunAs(cq_id, user);\n    }\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.unlock": {
                "code_before_change": "  public void unlock(UnlockRequest rqst)\n      throws NoSuchLockException, TxnOpenException, MetaException {\n    try {\n      Connection dbConn = getDbConn(Connection.TRANSACTION_SERIALIZABLE);\n      Statement stmt = null;\n      try {\n        // Odd as it seems, we need to heartbeat first because this touches the\n        // lock table and assures that our locks our still valid.  If they are\n        // not, this will throw an exception and the heartbeat will fail.\n        long extLockId = rqst.getLockid();\n        heartbeatLock(dbConn, extLockId);\n        long txnid = getTxnIdFromLockId(dbConn, extLockId);\n        // If there is a valid txnid, throw an exception,\n        // as locks associated with transactions should be unlocked only when the\n        // transaction is committed or aborted.\n        if (txnid > 0) {\n          LOG.debug(\"Going to rollback\");\n          dbConn.rollback();\n          String msg = \"Unlocking locks associated with transaction\" +\n              \" not permitted.  Lockid \" + extLockId + \" is associated with \" +\n              \"transaction \" + txnid;\n          LOG.error(msg);\n          throw new TxnOpenException(msg);\n        }\n        stmt = dbConn.createStatement();\n        String s = \"delete from HIVE_LOCKS where hl_lock_ext_id = \" + extLockId;\n        LOG.debug(\"Going to execute update <\" + s + \">\");\n        int rc = stmt.executeUpdate(s);\n        if (rc < 1) {\n          LOG.debug(\"Going to rollback\");\n          dbConn.rollback();\n          throw new NoSuchLockException(\"No such lock: \" + extLockId);\n        }\n        LOG.debug(\"Going to commit\");\n        dbConn.commit();\n      } catch (SQLException e) {\n        try {\n          LOG.debug(\"Going to rollback\");\n          dbConn.rollback();\n        } catch (SQLException e1) {\n        }\n        detectDeadlock(dbConn, e, \"unlock\");\n        throw new MetaException(\"Unable to update transaction database \" +\n            StringUtils.stringifyException(e));\n      } finally {\n        closeStmt(stmt);\n        closeDbConn(dbConn);\n      }\n    } catch (DeadlockException e) {\n      unlock(rqst);\n    } finally {\n      deadlockCnt = 0;\n    }\n  }",
                "code_after_change": "  public void unlock(UnlockRequest rqst)\n    throws NoSuchLockException, TxnOpenException, MetaException {\n    try {\n      Connection dbConn = null;\n      Statement stmt = null;\n      try {\n        dbConn = getDbConn(Connection.TRANSACTION_SERIALIZABLE);\n        // Odd as it seems, we need to heartbeat first because this touches the\n        // lock table and assures that our locks our still valid.  If they are\n        // not, this will throw an exception and the heartbeat will fail.\n        long extLockId = rqst.getLockid();\n        heartbeatLock(dbConn, extLockId);\n        long txnid = getTxnIdFromLockId(dbConn, extLockId);\n        // If there is a valid txnid, throw an exception,\n        // as locks associated with transactions should be unlocked only when the\n        // transaction is committed or aborted.\n        if (txnid > 0) {\n          LOG.debug(\"Going to rollback\");\n          dbConn.rollback();\n          String msg = \"Unlocking locks associated with transaction\" +\n            \" not permitted.  Lockid \" + extLockId + \" is associated with \" +\n            \"transaction \" + txnid;\n          LOG.error(msg);\n          throw new TxnOpenException(msg);\n        }\n        stmt = dbConn.createStatement();\n        String s = \"delete from HIVE_LOCKS where hl_lock_ext_id = \" + extLockId;\n        LOG.debug(\"Going to execute update <\" + s + \">\");\n        int rc = stmt.executeUpdate(s);\n        if (rc < 1) {\n          LOG.debug(\"Going to rollback\");\n          dbConn.rollback();\n          throw new NoSuchLockException(\"No such lock: \" + extLockId);\n        }\n        LOG.debug(\"Going to commit\");\n        dbConn.commit();\n      } catch (SQLException e) {\n        LOG.debug(\"Going to rollback\");\n        rollbackDBConn(dbConn);\n        checkRetryable(dbConn, e, \"unlock\");\n        throw new MetaException(\"Unable to update transaction database \" +\n          StringUtils.stringifyException(e));\n      } finally {\n        closeStmt(stmt);\n        closeDbConn(dbConn);\n      }\n    } catch (RetryException e) {\n      unlock(rqst);\n    }\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.closeStmt": {
                "code_before_change": "  protected void closeStmt(Statement stmt) {\n    try {\n      if (stmt != null) stmt.close();\n    } catch (SQLException e) {\n      LOG.warn(\"Failed to close statement \" + e.getMessage());\n    }\n  }",
                "code_after_change": "  protected void closeStmt(Statement stmt) {\n    try {\n      if (stmt != null) stmt.close();\n    } catch (SQLException e) {\n      LOG.warn(\"Failed to close statement \" + getMessage(e));\n    }\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.compact": {
                "code_before_change": "  public void compact(CompactionRequest rqst) throws MetaException {\n    // Put a compaction request in the queue.\n    try {\n      Connection dbConn = getDbConn(Connection.TRANSACTION_SERIALIZABLE);\n      Statement stmt = null;\n      try {\n        stmt = dbConn.createStatement();\n\n        // Get the id for the next entry in the queue\n        String s = \"select ncq_next from NEXT_COMPACTION_QUEUE_ID\";\n        LOG.debug(\"going to execute query <\" + s + \">\");\n        ResultSet rs = stmt.executeQuery(s);\n        if (!rs.next()) {\n          LOG.debug(\"Going to rollback\");\n          dbConn.rollback();\n          throw new MetaException(\"Transaction tables not properly initiated, \" +\n              \"no record found in next_compaction_queue_id\");\n        }\n        long id = rs.getLong(1);\n        s = \"update NEXT_COMPACTION_QUEUE_ID set ncq_next = \" + (id + 1);\n        LOG.debug(\"Going to execute update <\" + s + \">\");\n        stmt.executeUpdate(s);\n\n        StringBuilder buf = new StringBuilder(\"insert into COMPACTION_QUEUE (cq_id, cq_database, \" +\n            \"cq_table, \");\n        String partName = rqst.getPartitionname();\n        if (partName != null) buf.append(\"cq_partition, \");\n        buf.append(\"cq_state, cq_type\");\n        if (rqst.getRunas() != null) buf.append(\", cq_run_as\");\n        buf.append(\") values (\");\n        buf.append(id);\n        buf.append(\", '\");\n        buf.append(rqst.getDbname());\n        buf.append(\"', '\");\n        buf.append(rqst.getTablename());\n        buf.append(\"', '\");\n        if (partName != null) {\n          buf.append(partName);\n          buf.append(\"', '\");\n        }\n        buf.append(INITIATED_STATE);\n        buf.append(\"', '\");\n        switch (rqst.getType()) {\n          case MAJOR:\n            buf.append(MAJOR_TYPE);\n            break;\n\n          case MINOR:\n            buf.append(MINOR_TYPE);\n            break;\n\n          default:\n            LOG.debug(\"Going to rollback\");\n            dbConn.rollback();\n            throw new MetaException(\"Unexpected compaction type \" + rqst.getType().toString());\n        }\n        if (rqst.getRunas() != null) {\n          buf.append(\"', '\");\n          buf.append(rqst.getRunas());\n        }\n        buf.append(\"')\");\n        s = buf.toString();\n        LOG.debug(\"Going to execute update <\" + s + \">\");\n        stmt.executeUpdate(s);\n        LOG.debug(\"Going to commit\");\n        dbConn.commit();\n      } catch (SQLException e) {\n        try {\n          LOG.debug(\"Going to rollback\");\n          dbConn.rollback();\n        } catch (SQLException e1) {\n        }\n        detectDeadlock(dbConn, e, \"compact\");\n        throw new MetaException(\"Unable to select from transaction database \" +\n            StringUtils.stringifyException(e));\n      } finally {\n        closeStmt(stmt);\n        closeDbConn(dbConn);\n      }\n    } catch (DeadlockException e) {\n      compact(rqst);\n    } finally {\n      deadlockCnt = 0;\n    }\n  }",
                "code_after_change": "  public void compact(CompactionRequest rqst) throws MetaException {\n    // Put a compaction request in the queue.\n    try {\n      Connection dbConn = null;\n      Statement stmt = null;\n      try {\n        dbConn = getDbConn(Connection.TRANSACTION_SERIALIZABLE);\n        stmt = dbConn.createStatement();\n\n        // Get the id for the next entry in the queue\n        String s = \"select ncq_next from NEXT_COMPACTION_QUEUE_ID\";\n        LOG.debug(\"going to execute query <\" + s + \">\");\n        ResultSet rs = stmt.executeQuery(s);\n        if (!rs.next()) {\n          LOG.debug(\"Going to rollback\");\n          dbConn.rollback();\n          throw new MetaException(\"Transaction tables not properly initiated, \" +\n            \"no record found in next_compaction_queue_id\");\n        }\n        long id = rs.getLong(1);\n        s = \"update NEXT_COMPACTION_QUEUE_ID set ncq_next = \" + (id + 1);\n        LOG.debug(\"Going to execute update <\" + s + \">\");\n        stmt.executeUpdate(s);\n\n        StringBuilder buf = new StringBuilder(\"insert into COMPACTION_QUEUE (cq_id, cq_database, \" +\n          \"cq_table, \");\n        String partName = rqst.getPartitionname();\n        if (partName != null) buf.append(\"cq_partition, \");\n        buf.append(\"cq_state, cq_type\");\n        if (rqst.getRunas() != null) buf.append(\", cq_run_as\");\n        buf.append(\") values (\");\n        buf.append(id);\n        buf.append(\", '\");\n        buf.append(rqst.getDbname());\n        buf.append(\"', '\");\n        buf.append(rqst.getTablename());\n        buf.append(\"', '\");\n        if (partName != null) {\n          buf.append(partName);\n          buf.append(\"', '\");\n        }\n        buf.append(INITIATED_STATE);\n        buf.append(\"', '\");\n        switch (rqst.getType()) {\n          case MAJOR:\n            buf.append(MAJOR_TYPE);\n            break;\n\n          case MINOR:\n            buf.append(MINOR_TYPE);\n            break;\n\n          default:\n            LOG.debug(\"Going to rollback\");\n            dbConn.rollback();\n            throw new MetaException(\"Unexpected compaction type \" + rqst.getType().toString());\n        }\n        if (rqst.getRunas() != null) {\n          buf.append(\"', '\");\n          buf.append(rqst.getRunas());\n        }\n        buf.append(\"')\");\n        s = buf.toString();\n        LOG.debug(\"Going to execute update <\" + s + \">\");\n        stmt.executeUpdate(s);\n        LOG.debug(\"Going to commit\");\n        dbConn.commit();\n      } catch (SQLException e) {\n        LOG.debug(\"Going to rollback\");\n        rollbackDBConn(dbConn);\n        checkRetryable(dbConn, e, \"compact\");\n        throw new MetaException(\"Unable to select from transaction database \" +\n          StringUtils.stringifyException(e));\n      } finally {\n        closeStmt(stmt);\n        closeDbConn(dbConn);\n      }\n    } catch (RetryException e) {\n      compact(rqst);\n    }\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.acquire": {
                "code_before_change": "  private void acquire(Connection dbConn, Statement stmt, long extLockId, long intLockId)\n      throws SQLException, NoSuchLockException, MetaException {\n    long now = getDbTime(dbConn);\n    String s = \"update HIVE_LOCKS set hl_lock_state = '\" + LOCK_ACQUIRED + \"', \" +\n        \"hl_last_heartbeat = \" + now + \", hl_acquired_at = \" + now + \" where hl_lock_ext_id = \" +\n        extLockId + \" and hl_lock_int_id = \" + intLockId;\n    LOG.debug(\"Going to execute update <\" + s + \">\");\n    int rc = stmt.executeUpdate(s);\n    if (rc < 1) {\n      LOG.debug(\"Going to rollback\");\n      dbConn.rollback();\n      throw new NoSuchLockException(\"No such lock: (\" + extLockId + \",\" +\n          + intLockId + \")\");\n    }\n    // We update the database, but we don't commit because there may be other\n    // locks together with this, and we only want to acquire one if we can\n    // acquire all.\n  }",
                "code_after_change": "  private void acquire(Connection dbConn, Statement stmt, long extLockId, long intLockId)\n    throws SQLException, NoSuchLockException, MetaException {\n    long now = getDbTime(dbConn);\n    String s = \"update HIVE_LOCKS set hl_lock_state = '\" + LOCK_ACQUIRED + \"', \" +\n      \"hl_last_heartbeat = \" + now + \", hl_acquired_at = \" + now + \" where hl_lock_ext_id = \" +\n      extLockId + \" and hl_lock_int_id = \" + intLockId;\n    LOG.debug(\"Going to execute update <\" + s + \">\");\n    int rc = stmt.executeUpdate(s);\n    if (rc < 1) {\n      LOG.debug(\"Going to rollback\");\n      dbConn.rollback();\n      throw new NoSuchLockException(\"No such lock: (\" + extLockId + \",\" +\n        + intLockId + \")\");\n    }\n    // We update the database, but we don't commit because there may be other\n    // locks together with this, and we only want to acquire one if we can\n    // acquire all.\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.getTxnIdFromLockId": {
                "code_before_change": "  private long getTxnIdFromLockId(Connection dbConn, long extLockId)\n      throws NoSuchLockException, MetaException, SQLException {\n    Statement stmt = null;\n    try {\n      stmt = dbConn.createStatement();\n      String s = \"select hl_txnid from HIVE_LOCKS where hl_lock_ext_id = \" +\n          extLockId;\n      LOG.debug(\"Going to execute query <\" + s + \">\");\n      ResultSet rs = stmt.executeQuery(s);\n      if (!rs.next()) {\n        throw new MetaException(\"This should never happen!  We already \" +\n            \"checked the lock existed but now we can't find it!\");\n      }\n      long txnid = rs.getLong(1);\n      LOG.debug(\"Return txnid \" + (rs.wasNull() ? -1 : txnid));\n      return (rs.wasNull() ? -1 : txnid);\n    } finally {\n      closeStmt(stmt);\n    }\n  }",
                "code_after_change": "  private long getTxnIdFromLockId(Connection dbConn, long extLockId)\n    throws NoSuchLockException, MetaException, SQLException {\n    Statement stmt = null;\n    try {\n      stmt = dbConn.createStatement();\n      String s = \"select hl_txnid from HIVE_LOCKS where hl_lock_ext_id = \" +\n        extLockId;\n      LOG.debug(\"Going to execute query <\" + s + \">\");\n      ResultSet rs = stmt.executeQuery(s);\n      if (!rs.next()) {\n        throw new MetaException(\"This should never happen!  We already \" +\n          \"checked the lock existed but now we can't find it!\");\n      }\n      long txnid = rs.getLong(1);\n      LOG.debug(\"Return txnid \" + (rs.wasNull() ? -1 : txnid));\n      return (rs.wasNull() ? -1 : txnid);\n    } finally {\n      closeStmt(stmt);\n    }\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.get_function": {
                "code_before_change": "    public Function get_function(String dbName, String funcName)\n        throws MetaException, NoSuchObjectException, TException {\n      startFunction(\"get_function\", \": \" + dbName + \".\" + funcName);\n\n      RawStore ms = getMS();\n      Function func = null;\n      Exception ex = null;\n\n      try {\n        func = ms.getFunction(dbName, funcName);\n        if (func == null) {\n          throw new NoSuchObjectException(\n              \"Function \" + dbName + \".\" + funcName + \" does not exist\");\n        }\n      } catch (Exception e) {\n        ex = e;\n        throw newMetaException(e);\n      } finally {\n        endFunction(\"get_database\", func != null, ex);\n      }\n\n      return func;\n    }",
                "code_after_change": "    public Function get_function(String dbName, String funcName)\n        throws MetaException, NoSuchObjectException, TException {\n      startFunction(\"get_function\", \": \" + dbName + \".\" + funcName);\n\n      RawStore ms = getMS();\n      Function func = null;\n      Exception ex = null;\n\n      try {\n        func = ms.getFunction(dbName, funcName);\n        if (func == null) {\n          throw new NoSuchObjectException(\n              \"Function \" + dbName + \".\" + funcName + \" does not exist\");\n        }\n      } catch (Exception e) {\n        ex = e;\n        throw newMetaException(e);\n      } finally {\n        endFunction(\"get_database\", func != null, ex);\n      }\n\n      return func;\n    }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the error related to database connection issues, which is mentioned in the stack trace context, but it does not precisely identify the root cause in the ground truth methods. The stack trace includes methods like 'TxnHandler.getDbConn' and 'TxnHandler.getOpenTxns', which are related to the ground truth methods but not the exact ones. There is no fix suggestion provided in the bug report. The problem location is partially identified as it mentions methods in the stack trace that are related to the ground truth methods. There is no wrong information in the bug report as it accurately describes the error context."
        }
    },
    {
        "filename": "HIVE-7623.json",
        "code_diff": {
            "metastore.src.java.org.apache.hadoop.hive.metastore.HiveAlterHandler.alterPartition": {
                "code_before_change": "  public Partition alterPartition(final RawStore msdb, Warehouse wh, final String dbname,\n      final String name, final List<String> part_vals, final Partition new_part)\n      throws InvalidOperationException, InvalidObjectException, AlreadyExistsException,\n      MetaException {\n    boolean success = false;\n\n    Path srcPath = null;\n    Path destPath = null;\n    FileSystem srcFs = null;\n    FileSystem destFs = null;\n    Partition oldPart = null;\n    String oldPartLoc = null;\n    String newPartLoc = null;\n\n    // Set DDL time to now if not specified\n    if (new_part.getParameters() == null ||\n        new_part.getParameters().get(hive_metastoreConstants.DDL_TIME) == null ||\n        Integer.parseInt(new_part.getParameters().get(hive_metastoreConstants.DDL_TIME)) == 0) {\n      new_part.putToParameters(hive_metastoreConstants.DDL_TIME, Long.toString(System\n          .currentTimeMillis() / 1000));\n    }\n\n    Table tbl = msdb.getTable(dbname, name);\n    //alter partition\n    if (part_vals == null || part_vals.size() == 0) {\n      try {\n        oldPart = msdb.getPartition(dbname, name, new_part.getValues());\n        if (MetaStoreUtils.requireCalStats(hiveConf, oldPart, new_part, tbl)) {\n          MetaStoreUtils.updatePartitionStatsFast(new_part, wh, false, true);\n        }\n        msdb.alterPartition(dbname, name, new_part.getValues(), new_part);\n      } catch (InvalidObjectException e) {\n        throw new InvalidOperationException(\"alter is not possible\");\n      } catch (NoSuchObjectException e){\n        //old partition does not exist\n        throw new InvalidOperationException(\"alter is not possible\");\n      }\n      return oldPart;\n    }\n    //rename partition\n    try {\n      msdb.openTransaction();\n      try {\n        oldPart = msdb.getPartition(dbname, name, part_vals);\n      } catch (NoSuchObjectException e) {\n        // this means there is no existing partition\n        throw new InvalidObjectException(\n            \"Unable to rename partition because old partition does not exist\");\n      }\n      Partition check_part = null;\n      try {\n        check_part = msdb.getPartition(dbname, name, new_part.getValues());\n      } catch(NoSuchObjectException e) {\n        // this means there is no existing partition\n        check_part = null;\n      }\n      if (check_part != null) {\n        throw new AlreadyExistsException(\"Partition already exists:\" + dbname + \".\" + name + \".\" +\n            new_part.getValues());\n      }\n      if (tbl == null) {\n        throw new InvalidObjectException(\n            \"Unable to rename partition because table or database do not exist\");\n      }\n\n      // if the external partition is renamed, the file should not change\n      if (tbl.getTableType().equals(TableType.EXTERNAL_TABLE.toString())) {\n        new_part.getSd().setLocation(oldPart.getSd().getLocation());\n        msdb.alterPartition(dbname, name, part_vals, new_part);\n      } else {\n        try {\n          destPath = new Path(wh.getTablePath(msdb.getDatabase(dbname), name),\n            Warehouse.makePartName(tbl.getPartitionKeys(), new_part.getValues()));\n          destPath = constructRenamedPath(destPath, new Path(new_part.getSd().getLocation()));\n        } catch (NoSuchObjectException e) {\n          LOG.debug(e);\n          throw new InvalidOperationException(\n            \"Unable to change partition or table. Database \" + dbname + \" does not exist\"\n              + \" Check metastore logs for detailed stack.\" + e.getMessage());\n        }\n        if (destPath != null) {\n          newPartLoc = destPath.toString();\n          oldPartLoc = oldPart.getSd().getLocation();\n\n          srcPath = new Path(oldPartLoc);\n\n          LOG.info(\"srcPath:\" + oldPartLoc);\n          LOG.info(\"descPath:\" + newPartLoc);\n          srcFs = wh.getFs(srcPath);\n          destFs = wh.getFs(destPath);\n          // check that src and dest are on the same file system\n          if (srcFs != destFs) {\n            throw new InvalidOperationException(\"table new location \" + destPath\n              + \" is on a different file system than the old location \"\n              + srcPath + \". This operation is not supported\");\n          }\n          try {\n            srcFs.exists(srcPath); // check that src exists and also checks\n            if (newPartLoc.compareTo(oldPartLoc) != 0 && destFs.exists(destPath)) {\n              throw new InvalidOperationException(\"New location for this table \"\n                + tbl.getDbName() + \".\" + tbl.getTableName()\n                + \" already exists : \" + destPath);\n            }\n          } catch (IOException e) {\n            Warehouse.closeFs(srcFs);\n            Warehouse.closeFs(destFs);\n            throw new InvalidOperationException(\"Unable to access new location \"\n              + destPath + \" for partition \" + tbl.getDbName() + \".\"\n              + tbl.getTableName() + \" \" + new_part.getValues());\n          }\n          new_part.getSd().setLocation(newPartLoc);\n          if (MetaStoreUtils.requireCalStats(hiveConf, oldPart, new_part, tbl)) {\n            MetaStoreUtils.updatePartitionStatsFast(new_part, wh, false, true);\n          }\n          msdb.alterPartition(dbname, name, part_vals, new_part);\n        }\n      }\n\n      success = msdb.commitTransaction();\n    } finally {\n      if (!success) {\n        msdb.rollbackTransaction();\n      }\n      if (success && newPartLoc != null && newPartLoc.compareTo(oldPartLoc) != 0) {\n        //rename the data directory\n        try{\n          if (srcFs.exists(srcPath)) {\n            //if destPath's parent path doesn't exist, we should mkdir it\n            Path destParentPath = destPath.getParent();\n            if (!wh.mkdirs(destParentPath, true)) {\n                throw new IOException(\"Unable to create path \" + destParentPath);\n            }\n            wh.renameDir(srcPath, destPath, true);\n            LOG.info(\"rename done!\");\n          }\n        } catch (IOException e) {\n          boolean revertMetaDataTransaction = false;\n          try {\n            msdb.openTransaction();\n            msdb.alterPartition(dbname, name, new_part.getValues(), oldPart);\n            revertMetaDataTransaction = msdb.commitTransaction();\n          } catch (Exception e1) {\n            LOG.error(\"Reverting metadata opeation failed During HDFS operation failed\", e1);\n            if (!revertMetaDataTransaction) {\n              msdb.rollbackTransaction();\n            }\n          }\n          throw new InvalidOperationException(\"Unable to access old location \"\n              + srcPath + \" for partition \" + tbl.getDbName() + \".\"\n              + tbl.getTableName() + \" \" + part_vals);\n        }\n      }\n    }\n    return oldPart;\n  }",
                "code_after_change": "  public Partition alterPartition(final RawStore msdb, Warehouse wh, final String dbname,\n      final String name, final List<String> part_vals, final Partition new_part)\n      throws InvalidOperationException, InvalidObjectException, AlreadyExistsException,\n      MetaException {\n    boolean success = false;\n\n    Path srcPath = null;\n    Path destPath = null;\n    FileSystem srcFs = null;\n    FileSystem destFs = null;\n    Partition oldPart = null;\n    String oldPartLoc = null;\n    String newPartLoc = null;\n\n    // Set DDL time to now if not specified\n    if (new_part.getParameters() == null ||\n        new_part.getParameters().get(hive_metastoreConstants.DDL_TIME) == null ||\n        Integer.parseInt(new_part.getParameters().get(hive_metastoreConstants.DDL_TIME)) == 0) {\n      new_part.putToParameters(hive_metastoreConstants.DDL_TIME, Long.toString(System\n          .currentTimeMillis() / 1000));\n    }\n\n    Table tbl = msdb.getTable(dbname, name);\n    //alter partition\n    if (part_vals == null || part_vals.size() == 0) {\n      try {\n        oldPart = msdb.getPartition(dbname, name, new_part.getValues());\n        if (MetaStoreUtils.requireCalStats(hiveConf, oldPart, new_part, tbl)) {\n          MetaStoreUtils.updatePartitionStatsFast(new_part, wh, false, true);\n        }\n        msdb.alterPartition(dbname, name, new_part.getValues(), new_part);\n      } catch (InvalidObjectException e) {\n        throw new InvalidOperationException(\"alter is not possible\");\n      } catch (NoSuchObjectException e){\n        //old partition does not exist\n        throw new InvalidOperationException(\"alter is not possible\");\n      }\n      return oldPart;\n    }\n    //rename partition\n    try {\n      msdb.openTransaction();\n      try {\n        oldPart = msdb.getPartition(dbname, name, part_vals);\n      } catch (NoSuchObjectException e) {\n        // this means there is no existing partition\n        throw new InvalidObjectException(\n            \"Unable to rename partition because old partition does not exist\");\n      }\n      Partition check_part = null;\n      try {\n        check_part = msdb.getPartition(dbname, name, new_part.getValues());\n      } catch(NoSuchObjectException e) {\n        // this means there is no existing partition\n        check_part = null;\n      }\n      if (check_part != null) {\n        throw new AlreadyExistsException(\"Partition already exists:\" + dbname + \".\" + name + \".\" +\n            new_part.getValues());\n      }\n      if (tbl == null) {\n        throw new InvalidObjectException(\n            \"Unable to rename partition because table or database do not exist\");\n      }\n\n      // if the external partition is renamed, the file should not change\n      if (tbl.getTableType().equals(TableType.EXTERNAL_TABLE.toString())) {\n        new_part.getSd().setLocation(oldPart.getSd().getLocation());\n        msdb.alterPartition(dbname, name, part_vals, new_part);\n      } else {\n        try {\n          destPath = new Path(wh.getTablePath(msdb.getDatabase(dbname), name),\n            Warehouse.makePartName(tbl.getPartitionKeys(), new_part.getValues()));\n          destPath = constructRenamedPath(destPath, new Path(new_part.getSd().getLocation()));\n        } catch (NoSuchObjectException e) {\n          LOG.debug(e);\n          throw new InvalidOperationException(\n            \"Unable to change partition or table. Database \" + dbname + \" does not exist\"\n              + \" Check metastore logs for detailed stack.\" + e.getMessage());\n        }\n        if (destPath != null) {\n          newPartLoc = destPath.toString();\n          oldPartLoc = oldPart.getSd().getLocation();\n\n          srcPath = new Path(oldPartLoc);\n\n          LOG.info(\"srcPath:\" + oldPartLoc);\n          LOG.info(\"descPath:\" + newPartLoc);\n          srcFs = wh.getFs(srcPath);\n          destFs = wh.getFs(destPath);\n          // check that src and dest are on the same file system\n          if (!FileUtils.equalsFileSystem(srcFs, destFs)) {\n            throw new InvalidOperationException(\"table new location \" + destPath\n              + \" is on a different file system than the old location \"\n              + srcPath + \". This operation is not supported\");\n          }\n          try {\n            srcFs.exists(srcPath); // check that src exists and also checks\n            if (newPartLoc.compareTo(oldPartLoc) != 0 && destFs.exists(destPath)) {\n              throw new InvalidOperationException(\"New location for this table \"\n                + tbl.getDbName() + \".\" + tbl.getTableName()\n                + \" already exists : \" + destPath);\n            }\n          } catch (IOException e) {\n            Warehouse.closeFs(srcFs);\n            Warehouse.closeFs(destFs);\n            throw new InvalidOperationException(\"Unable to access new location \"\n              + destPath + \" for partition \" + tbl.getDbName() + \".\"\n              + tbl.getTableName() + \" \" + new_part.getValues());\n          }\n          new_part.getSd().setLocation(newPartLoc);\n          if (MetaStoreUtils.requireCalStats(hiveConf, oldPart, new_part, tbl)) {\n            MetaStoreUtils.updatePartitionStatsFast(new_part, wh, false, true);\n          }\n          msdb.alterPartition(dbname, name, part_vals, new_part);\n        }\n      }\n\n      success = msdb.commitTransaction();\n    } finally {\n      if (!success) {\n        msdb.rollbackTransaction();\n      }\n      if (success && newPartLoc != null && newPartLoc.compareTo(oldPartLoc) != 0) {\n        //rename the data directory\n        try{\n          if (srcFs.exists(srcPath)) {\n            //if destPath's parent path doesn't exist, we should mkdir it\n            Path destParentPath = destPath.getParent();\n            if (!wh.mkdirs(destParentPath, true)) {\n                throw new IOException(\"Unable to create path \" + destParentPath);\n            }\n            wh.renameDir(srcPath, destPath, true);\n            LOG.info(\"rename done!\");\n          }\n        } catch (IOException e) {\n          boolean revertMetaDataTransaction = false;\n          try {\n            msdb.openTransaction();\n            msdb.alterPartition(dbname, name, new_part.getValues(), oldPart);\n            revertMetaDataTransaction = msdb.commitTransaction();\n          } catch (Exception e1) {\n            LOG.error(\"Reverting metadata opeation failed During HDFS operation failed\", e1);\n            if (!revertMetaDataTransaction) {\n              msdb.rollbackTransaction();\n            }\n          }\n          throw new InvalidOperationException(\"Unable to access old location \"\n              + srcPath + \" for partition \" + tbl.getDbName() + \".\"\n              + tbl.getTableName() + \" \" + part_vals);\n        }\n      }\n    }\n    return oldPart;\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Correct",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report precisely identifies the root cause by pointing out the use of '!=' to compare filesystem objects in the 'HiveAlterHandler.alterPartition' method, which is the ground truth method. The fix suggestion is correct as it aligns with the developer's fix, which changes the comparison to use 'FileUtils.equalsFileSystem'. The problem location is also precisely identified as the report mentions the exact method 'HiveAlterHandler.alterPartition' where the issue occurs. There is no wrong information in the bug report as all details are relevant and accurate."
        }
    },
    {
        "filename": "HIVE-15997.json",
        "code_diff": {
            "ql.src.java.org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager.unlockPrimitive": {
                "code_before_change": "  static void unlockPrimitive(HiveLock hiveLock, String parent, CuratorFramework curatorFramework) throws LockException {\n    ZooKeeperHiveLock zLock = (ZooKeeperHiveLock)hiveLock;\n    HiveLockMode lMode = hiveLock.getHiveLockMode();\n    HiveLockObject obj = zLock.getHiveLockObject();\n    String name  = getLastObjectName(parent, obj);\n    try {\n      curatorFramework.delete().forPath(zLock.getPath());\n\n      // Delete the parent node if all the children have been deleted\n      List<String> children = curatorFramework.getChildren().forPath(name);\n      if (children == null || children.isEmpty()) {\n        curatorFramework.delete().forPath(name);\n      }\n      Metrics metrics = MetricsFactory.getInstance();\n      if (metrics != null) {\n        try {\n          switch(lMode) {\n          case EXCLUSIVE:\n            metrics.decrementCounter(MetricsConstant.ZOOKEEPER_HIVE_EXCLUSIVELOCKS);\n            break;\n          case SEMI_SHARED:\n            metrics.decrementCounter(MetricsConstant.ZOOKEEPER_HIVE_SEMISHAREDLOCKS);\n            break;\n          default:\n            metrics.decrementCounter(MetricsConstant.ZOOKEEPER_HIVE_SHAREDLOCKS);\n            break;\n          }\n        } catch (Exception e) {\n          LOG.warn(\"Error Reporting hive client zookeeper unlock operation to Metrics system\", e);\n        }\n      }\n    } catch (KeeperException.NoNodeException nne) {\n      //can happen in retrying deleting the zLock after exceptions like InterruptedException\n      //or in a race condition where parent has already been deleted by other process when it\n      //is to be deleted. Both cases should not raise error\n      LOG.debug(\"Node \" + zLock.getPath() + \" or its parent has already been deleted.\");\n    } catch (KeeperException.NotEmptyException nee) {\n      //can happen in a race condition where another process adds a zLock under this parent\n      //just before it is about to be deleted. It should not be a problem since this parent\n      //can eventually be deleted by the process which hold its last child zLock\n      LOG.debug(\"Node \" + name + \" to be deleted is not empty.\");\n    } catch (Exception e) {\n      //exceptions including InterruptException and other KeeperException\n      LOG.error(\"Failed to release ZooKeeper lock: \", e);\n      throw new LockException(e);\n    }\n  }",
                "code_after_change": "  static void unlockPrimitive(HiveLock hiveLock, String parent, CuratorFramework curatorFramework) throws LockException {\n    ZooKeeperHiveLock zLock = (ZooKeeperHiveLock)hiveLock;\n    HiveLockMode lMode = hiveLock.getHiveLockMode();\n    HiveLockObject obj = zLock.getHiveLockObject();\n    String name  = getLastObjectName(parent, obj);\n    try {\n      //catch InterruptedException to make sure locks can be released when the query is cancelled.\n      try {\n        curatorFramework.delete().forPath(zLock.getPath());\n      } catch (InterruptedException ie) {\n        curatorFramework.delete().forPath(zLock.getPath());\n      }\n\n      // Delete the parent node if all the children have been deleted\n      List<String> children = null;\n      try {\n        children = curatorFramework.getChildren().forPath(name);\n      } catch (InterruptedException ie) {\n        children = curatorFramework.getChildren().forPath(name);\n      }\n      if (children == null || children.isEmpty()) {\n        try {\n          curatorFramework.delete().forPath(name);\n        } catch (InterruptedException ie) {\n          curatorFramework.delete().forPath(name);\n        }\n      }\n      Metrics metrics = MetricsFactory.getInstance();\n      if (metrics != null) {\n        try {\n          switch(lMode) {\n          case EXCLUSIVE:\n            metrics.decrementCounter(MetricsConstant.ZOOKEEPER_HIVE_EXCLUSIVELOCKS);\n            break;\n          case SEMI_SHARED:\n            metrics.decrementCounter(MetricsConstant.ZOOKEEPER_HIVE_SEMISHAREDLOCKS);\n            break;\n          default:\n            metrics.decrementCounter(MetricsConstant.ZOOKEEPER_HIVE_SHAREDLOCKS);\n            break;\n          }\n        } catch (Exception e) {\n          LOG.warn(\"Error Reporting hive client zookeeper unlock operation to Metrics system\", e);\n        }\n      }\n    } catch (KeeperException.NoNodeException nne) {\n      //can happen in retrying deleting the zLock after exceptions like InterruptedException\n      //or in a race condition where parent has already been deleted by other process when it\n      //is to be deleted. Both cases should not raise error\n      LOG.debug(\"Node \" + zLock.getPath() + \" or its parent has already been deleted.\");\n    } catch (KeeperException.NotEmptyException nee) {\n      //can happen in a race condition where another process adds a zLock under this parent\n      //just before it is about to be deleted. It should not be a problem since this parent\n      //can eventually be deleted by the process which hold its last child zLock\n      LOG.debug(\"Node \" + name + \" to be deleted is not empty.\");\n    } catch (Exception e) {\n      //exceptions including InterruptException and other KeeperException\n      LOG.error(\"Failed to release ZooKeeper lock: \", e);\n      throw new LockException(e);\n    }\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.exec.spark.RemoteHiveSparkClient.submit": {
                "code_before_change": "  private SparkJobRef submit(final DriverContext driverContext, final SparkWork sparkWork) throws Exception {\n    final Context ctx = driverContext.getCtx();\n    final HiveConf hiveConf = (HiveConf) ctx.getConf();\n    refreshLocalResources(sparkWork, hiveConf);\n    final JobConf jobConf = new JobConf(hiveConf);\n\n    //update the credential provider location in the jobConf\n    HiveConfUtil.updateJobCredentialProviders(jobConf);\n\n    // Create temporary scratch dir\n    final Path emptyScratchDir = ctx.getMRTmpPath();\n    FileSystem fs = emptyScratchDir.getFileSystem(jobConf);\n    fs.mkdirs(emptyScratchDir);\n\n    byte[] jobConfBytes = KryoSerializer.serializeJobConf(jobConf);\n    byte[] scratchDirBytes = KryoSerializer.serialize(emptyScratchDir);\n    byte[] sparkWorkBytes = KryoSerializer.serialize(sparkWork);\n\n    JobStatusJob job = new JobStatusJob(jobConfBytes, scratchDirBytes, sparkWorkBytes);\n    JobHandle<Serializable> jobHandle = remoteClient.submit(job);\n    RemoteSparkJobStatus sparkJobStatus = new RemoteSparkJobStatus(remoteClient, jobHandle, sparkClientTimtout);\n    return new RemoteSparkJobRef(hiveConf, jobHandle, sparkJobStatus);\n  }",
                "code_after_change": "  private SparkJobRef submit(final DriverContext driverContext, final SparkWork sparkWork) throws Exception {\n    final Context ctx = driverContext.getCtx();\n    final HiveConf hiveConf = (HiveConf) ctx.getConf();\n    refreshLocalResources(sparkWork, hiveConf);\n    final JobConf jobConf = new JobConf(hiveConf);\n\n    //update the credential provider location in the jobConf\n    HiveConfUtil.updateJobCredentialProviders(jobConf);\n\n    // Create temporary scratch dir\n    final Path emptyScratchDir = ctx.getMRTmpPath();\n    FileSystem fs = emptyScratchDir.getFileSystem(jobConf);\n    fs.mkdirs(emptyScratchDir);\n\n    byte[] jobConfBytes = KryoSerializer.serializeJobConf(jobConf);\n    byte[] scratchDirBytes = KryoSerializer.serialize(emptyScratchDir);\n    byte[] sparkWorkBytes = KryoSerializer.serialize(sparkWork);\n\n    JobStatusJob job = new JobStatusJob(jobConfBytes, scratchDirBytes, sparkWorkBytes);\n    if (driverContext.isShutdown()) {\n      throw new HiveException(\"Operation is cancelled.\");\n    }\n\n    JobHandle<Serializable> jobHandle = remoteClient.submit(job);\n    RemoteSparkJobStatus sparkJobStatus = new RemoteSparkJobStatus(remoteClient, jobHandle, sparkClientTimtout);\n    return new RemoteSparkJobRef(hiveConf, jobHandle, sparkJobStatus);\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.exec.spark.LocalHiveSparkClient.execute": {
                "code_before_change": "  public SparkJobRef execute(DriverContext driverContext, SparkWork sparkWork) throws Exception {\n    Context ctx = driverContext.getCtx();\n    HiveConf hiveConf = (HiveConf) ctx.getConf();\n    refreshLocalResources(sparkWork, hiveConf);\n    JobConf jobConf = new JobConf(hiveConf);\n\n    // Create temporary scratch dir\n    Path emptyScratchDir;\n    emptyScratchDir = ctx.getMRTmpPath();\n    FileSystem fs = emptyScratchDir.getFileSystem(jobConf);\n    fs.mkdirs(emptyScratchDir);\n\n    // Update credential provider location\n    // the password to the credential provider in already set in the sparkConf\n    // in HiveSparkClientFactory\n    HiveConfUtil.updateJobCredentialProviders(jobConf);\n\n    SparkCounters sparkCounters = new SparkCounters(sc);\n    Map<String, List<String>> prefixes = sparkWork.getRequiredCounterPrefix();\n    if (prefixes != null) {\n      for (String group : prefixes.keySet()) {\n        for (String counterName : prefixes.get(group)) {\n          sparkCounters.createCounter(group, counterName);\n        }\n      }\n    }\n    SparkReporter sparkReporter = new SparkReporter(sparkCounters);\n\n    // Generate Spark plan\n    SparkPlanGenerator gen =\n      new SparkPlanGenerator(sc, ctx, jobConf, emptyScratchDir, sparkReporter);\n    SparkPlan plan = gen.generate(sparkWork);\n\n    // Execute generated plan.\n    JavaPairRDD<HiveKey, BytesWritable> finalRDD = plan.generateGraph();\n    // We use Spark RDD async action to submit job as it's the only way to get jobId now.\n    JavaFutureAction<Void> future = finalRDD.foreachAsync(HiveVoidFunction.getInstance());\n    // As we always use foreach action to submit RDD graph, it would only trigger one job.\n    int jobId = future.jobIds().get(0);\n    LocalSparkJobStatus sparkJobStatus = new LocalSparkJobStatus(\n      sc, jobId, jobMetricsListener, sparkCounters, plan.getCachedRDDIds(), future);\n    return new LocalSparkJobRef(Integer.toString(jobId), hiveConf,  sparkJobStatus, sc);\n  }",
                "code_after_change": "  public SparkJobRef execute(DriverContext driverContext, SparkWork sparkWork) throws Exception {\n    Context ctx = driverContext.getCtx();\n    HiveConf hiveConf = (HiveConf) ctx.getConf();\n    refreshLocalResources(sparkWork, hiveConf);\n    JobConf jobConf = new JobConf(hiveConf);\n\n    // Create temporary scratch dir\n    Path emptyScratchDir;\n    emptyScratchDir = ctx.getMRTmpPath();\n    FileSystem fs = emptyScratchDir.getFileSystem(jobConf);\n    fs.mkdirs(emptyScratchDir);\n\n    // Update credential provider location\n    // the password to the credential provider in already set in the sparkConf\n    // in HiveSparkClientFactory\n    HiveConfUtil.updateJobCredentialProviders(jobConf);\n\n    SparkCounters sparkCounters = new SparkCounters(sc);\n    Map<String, List<String>> prefixes = sparkWork.getRequiredCounterPrefix();\n    if (prefixes != null) {\n      for (String group : prefixes.keySet()) {\n        for (String counterName : prefixes.get(group)) {\n          sparkCounters.createCounter(group, counterName);\n        }\n      }\n    }\n    SparkReporter sparkReporter = new SparkReporter(sparkCounters);\n\n    // Generate Spark plan\n    SparkPlanGenerator gen =\n      new SparkPlanGenerator(sc, ctx, jobConf, emptyScratchDir, sparkReporter);\n    SparkPlan plan = gen.generate(sparkWork);\n\n    if (driverContext.isShutdown()) {\n      throw new HiveException(\"Operation is cancelled.\");\n    }\n\n    // Execute generated plan.\n    JavaPairRDD<HiveKey, BytesWritable> finalRDD = plan.generateGraph();\n    // We use Spark RDD async action to submit job as it's the only way to get jobId now.\n    JavaFutureAction<Void> future = finalRDD.foreachAsync(HiveVoidFunction.getInstance());\n    // As we always use foreach action to submit RDD graph, it would only trigger one job.\n    int jobId = future.jobIds().get(0);\n    LocalSparkJobStatus sparkJobStatus = new LocalSparkJobStatus(\n      sc, jobId, jobMetricsListener, sparkCounters, plan.getCachedRDDIds(), future);\n    return new LocalSparkJobRef(Integer.toString(jobId), hiveConf,  sparkJobStatus, sc);\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.Driver.isInterrupted": {
                "code_before_change": "  private boolean isInterrupted() {\n    lDrvState.stateLock.lock();\n    try {\n      if (lDrvState.driverState == DriverState.INTERRUPT) {\n        Thread.currentThread().interrupt();\n        return true;\n      } else {\n        return false;\n      }\n    } finally {\n      lDrvState.stateLock.unlock();\n    }\n  }",
                "code_after_change": "  private boolean isInterrupted() {\n    lDrvState.stateLock.lock();\n    try {\n      if (lDrvState.driverState == DriverState.INTERRUPT) {\n        return true;\n      } else {\n        return false;\n      }\n    } finally {\n      lDrvState.stateLock.unlock();\n    }\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.exec.mr.ExecDriver.execute": {
                "code_before_change": "  public int execute(DriverContext driverContext) {\n\n    IOPrepareCache ioPrepareCache = IOPrepareCache.get();\n    ioPrepareCache.clear();\n\n    boolean success = true;\n\n    Context ctx = driverContext.getCtx();\n    boolean ctxCreated = false;\n    Path emptyScratchDir;\n    JobClient jc = null;\n\n    MapWork mWork = work.getMapWork();\n    ReduceWork rWork = work.getReduceWork();\n\n    try {\n      if (ctx == null) {\n        ctx = new Context(job);\n        ctxCreated = true;\n      }\n\n      emptyScratchDir = ctx.getMRTmpPath();\n      FileSystem fs = emptyScratchDir.getFileSystem(job);\n      fs.mkdirs(emptyScratchDir);\n    } catch (IOException e) {\n      e.printStackTrace();\n      console.printError(\"Error launching map-reduce job\", \"\\n\"\n          + org.apache.hadoop.util.StringUtils.stringifyException(e));\n      return 5;\n    }\n\n    HiveFileFormatUtils.prepareJobOutput(job);\n    //See the javadoc on HiveOutputFormatImpl and HadoopShims.prepareJobOutput()\n    job.setOutputFormat(HiveOutputFormatImpl.class);\n\n    job.setMapperClass(ExecMapper.class);\n\n    job.setMapOutputKeyClass(HiveKey.class);\n    job.setMapOutputValueClass(BytesWritable.class);\n\n    try {\n      String partitioner = HiveConf.getVar(job, ConfVars.HIVEPARTITIONER);\n      job.setPartitionerClass(JavaUtils.loadClass(partitioner));\n    } catch (ClassNotFoundException e) {\n      throw new RuntimeException(e.getMessage(), e);\n    }\n\n    propagateSplitSettings(job, mWork);\n\n    job.setNumReduceTasks(rWork != null ? rWork.getNumReduceTasks().intValue() : 0);\n    job.setReducerClass(ExecReducer.class);\n\n    // set input format information if necessary\n    setInputAttributes(job);\n\n    // Turn on speculative execution for reducers\n    boolean useSpeculativeExecReducers = HiveConf.getBoolVar(job,\n        HiveConf.ConfVars.HIVESPECULATIVEEXECREDUCERS);\n    job.setBoolean(MRJobConfig.REDUCE_SPECULATIVE, useSpeculativeExecReducers);\n\n    String inpFormat = HiveConf.getVar(job, HiveConf.ConfVars.HIVEINPUTFORMAT);\n\n    if (mWork.isUseBucketizedHiveInputFormat()) {\n      inpFormat = BucketizedHiveInputFormat.class.getName();\n    }\n\n    LOG.info(\"Using \" + inpFormat);\n\n    try {\n      job.setInputFormat(JavaUtils.loadClass(inpFormat));\n    } catch (ClassNotFoundException e) {\n      throw new RuntimeException(e.getMessage(), e);\n    }\n\n    // No-Op - we don't really write anything here ..\n    job.setOutputKeyClass(Text.class);\n    job.setOutputValueClass(Text.class);\n\n    int returnVal = 0;\n    boolean noName = StringUtils.isEmpty(job.get(MRJobConfig.JOB_NAME));\n\n    if (noName) {\n      // This is for a special case to ensure unit tests pass\n      job.set(MRJobConfig.JOB_NAME, \"JOB\" + Utilities.randGen.nextInt());\n    }\n\n    try{\n      MapredLocalWork localwork = mWork.getMapRedLocalWork();\n      if (localwork != null && localwork.hasStagedAlias()) {\n        if (!ShimLoader.getHadoopShims().isLocalMode(job)) {\n          Path localPath = localwork.getTmpPath();\n          Path hdfsPath = mWork.getTmpHDFSPath();\n\n          FileSystem hdfs = hdfsPath.getFileSystem(job);\n          FileSystem localFS = localPath.getFileSystem(job);\n          FileStatus[] hashtableFiles = localFS.listStatus(localPath);\n          int fileNumber = hashtableFiles.length;\n          String[] fileNames = new String[fileNumber];\n\n          for ( int i = 0; i < fileNumber; i++){\n            fileNames[i] = hashtableFiles[i].getPath().getName();\n          }\n\n          //package and compress all the hashtable files to an archive file\n          String stageId = this.getId();\n          String archiveFileName = Utilities.generateTarFileName(stageId);\n          localwork.setStageID(stageId);\n\n          CompressionUtils.tar(localPath.toUri().getPath(), fileNames,archiveFileName);\n          Path archivePath = Utilities.generateTarPath(localPath, stageId);\n          LOG.info(\"Archive \"+ hashtableFiles.length+\" hash table files to \" + archivePath);\n\n          //upload archive file to hdfs\n          Path hdfsFilePath =Utilities.generateTarPath(hdfsPath, stageId);\n          short replication = (short) job.getInt(\"mapred.submit.replication\", 10);\n          hdfs.copyFromLocalFile(archivePath, hdfsFilePath);\n          hdfs.setReplication(hdfsFilePath, replication);\n          LOG.info(\"Upload 1 archive file  from\" + archivePath + \" to: \" + hdfsFilePath);\n\n          //add the archive file to distributed cache\n          DistributedCache.createSymlink(job);\n          DistributedCache.addCacheArchive(hdfsFilePath.toUri(), job);\n          LOG.info(\"Add 1 archive file to distributed cache. Archive file: \" + hdfsFilePath.toUri());\n        }\n      }\n      work.configureJobConf(job);\n      List<Path> inputPaths = Utilities.getInputPaths(job, mWork, emptyScratchDir, ctx, false);\n      Utilities.setInputPaths(job, inputPaths);\n\n      Utilities.setMapRedWork(job, work, ctx.getMRTmpPath());\n\n      if (mWork.getSamplingType() > 0 && rWork != null && job.getNumReduceTasks() > 1) {\n        try {\n          handleSampling(ctx, mWork, job);\n          job.setPartitionerClass(HiveTotalOrderPartitioner.class);\n        } catch (IllegalStateException e) {\n          console.printInfo(\"Not enough sampling data.. Rolling back to single reducer task\");\n          rWork.setNumReduceTasks(1);\n          job.setNumReduceTasks(1);\n        } catch (Exception e) {\n          LOG.error(\"Sampling error\", e);\n          console.printError(e.toString(),\n              \"\\n\" + org.apache.hadoop.util.StringUtils.stringifyException(e));\n          rWork.setNumReduceTasks(1);\n          job.setNumReduceTasks(1);\n        }\n      }\n\n      jc = new JobClient(job);\n      // make this client wait if job tracker is not behaving well.\n      Throttle.checkJobTracker(job, LOG);\n\n      if (mWork.isGatheringStats() || (rWork != null && rWork.isGatheringStats())) {\n        // initialize stats publishing table\n        StatsPublisher statsPublisher;\n        StatsFactory factory = StatsFactory.newFactory(job);\n        if (factory != null) {\n          statsPublisher = factory.getStatsPublisher();\n          List<String> statsTmpDir = Utilities.getStatsTmpDirs(mWork, job);\n          if (rWork != null) {\n            statsTmpDir.addAll(Utilities.getStatsTmpDirs(rWork, job));\n          }\n          StatsCollectionContext sc = new StatsCollectionContext(job);\n          sc.setStatsTmpDirs(statsTmpDir);\n          if (!statsPublisher.init(sc)) { // creating stats table if not exists\n            if (HiveConf.getBoolVar(job, HiveConf.ConfVars.HIVE_STATS_RELIABLE)) {\n              throw\n                new HiveException(ErrorMsg.STATSPUBLISHER_INITIALIZATION_ERROR.getErrorCodedMsg());\n            }\n          }\n        }\n      }\n\n      Utilities.createTmpDirs(job, mWork);\n      Utilities.createTmpDirs(job, rWork);\n\n      SessionState ss = SessionState.get();\n      if (HiveConf.getVar(job, HiveConf.ConfVars.HIVE_EXECUTION_ENGINE).equals(\"tez\")\n          && ss != null) {\n        TezSessionState session = ss.getTezSession();\n        TezSessionPoolManager.getInstance().closeIfNotDefault(session, true);\n      }\n\n      HiveConfUtil.updateJobCredentialProviders(job);\n      // Finally SUBMIT the JOB!\n      rj = jc.submitJob(job);\n      this.jobID = rj.getJobID();\n      updateStatusInQueryDisplay();\n      returnVal = jobExecHelper.progress(rj, jc, ctx);\n      success = (returnVal == 0);\n    } catch (Exception e) {\n      e.printStackTrace();\n      setException(e);\n      String mesg = \" with exception '\" + Utilities.getNameMessage(e) + \"'\";\n      if (rj != null) {\n        mesg = \"Ended Job = \" + rj.getJobID() + mesg;\n      } else {\n        mesg = \"Job Submission failed\" + mesg;\n      }\n\n      // Has to use full name to make sure it does not conflict with\n      // org.apache.commons.lang.StringUtils\n      console.printError(mesg, \"\\n\" + org.apache.hadoop.util.StringUtils.stringifyException(e));\n\n      success = false;\n      returnVal = 1;\n    } finally {\n      Utilities.clearWork(job);\n      try {\n        if (ctxCreated) {\n          ctx.clear();\n        }\n\n        if (rj != null) {\n          if (returnVal != 0) {\n            rj.killJob();\n          }\n          jobID = rj.getID().toString();\n        }\n        if (jc!=null) {\n          jc.close();\n        }\n      } catch (Exception e) {\n\tLOG.warn(\"Failed while cleaning up \", e);\n      } finally {\n\tHadoopJobExecHelper.runningJobs.remove(rj);\n      }\n    }\n\n    // get the list of Dynamic partition paths\n    try {\n      if (rj != null) {\n        if (mWork.getAliasToWork() != null) {\n          for (Operator<? extends OperatorDesc> op : mWork.getAliasToWork().values()) {\n            op.jobClose(job, success);\n          }\n        }\n        if (rWork != null) {\n          rWork.getReducer().jobClose(job, success);\n        }\n      }\n    } catch (Exception e) {\n      // jobClose needs to execute successfully otherwise fail task\n      if (success) {\n        setException(e);\n        success = false;\n        returnVal = 3;\n        String mesg = \"Job Commit failed with exception '\" + Utilities.getNameMessage(e) + \"'\";\n        console.printError(mesg, \"\\n\" + org.apache.hadoop.util.StringUtils.stringifyException(e));\n      }\n    }\n\n    return (returnVal);\n  }",
                "code_after_change": "  public int execute(DriverContext driverContext) {\n\n    IOPrepareCache ioPrepareCache = IOPrepareCache.get();\n    ioPrepareCache.clear();\n\n    boolean success = true;\n\n    Context ctx = driverContext.getCtx();\n    boolean ctxCreated = false;\n    Path emptyScratchDir;\n    JobClient jc = null;\n\n    if (driverContext.isShutdown()) {\n      LOG.warn(\"Task was cancelled\");\n      return 5;\n    }\n\n    MapWork mWork = work.getMapWork();\n    ReduceWork rWork = work.getReduceWork();\n\n    try {\n      if (ctx == null) {\n        ctx = new Context(job);\n        ctxCreated = true;\n      }\n\n      emptyScratchDir = ctx.getMRTmpPath();\n      FileSystem fs = emptyScratchDir.getFileSystem(job);\n      fs.mkdirs(emptyScratchDir);\n    } catch (IOException e) {\n      e.printStackTrace();\n      console.printError(\"Error launching map-reduce job\", \"\\n\"\n          + org.apache.hadoop.util.StringUtils.stringifyException(e));\n      return 5;\n    }\n\n    HiveFileFormatUtils.prepareJobOutput(job);\n    //See the javadoc on HiveOutputFormatImpl and HadoopShims.prepareJobOutput()\n    job.setOutputFormat(HiveOutputFormatImpl.class);\n\n    job.setMapperClass(ExecMapper.class);\n\n    job.setMapOutputKeyClass(HiveKey.class);\n    job.setMapOutputValueClass(BytesWritable.class);\n\n    try {\n      String partitioner = HiveConf.getVar(job, ConfVars.HIVEPARTITIONER);\n      job.setPartitionerClass(JavaUtils.loadClass(partitioner));\n    } catch (ClassNotFoundException e) {\n      throw new RuntimeException(e.getMessage(), e);\n    }\n\n    propagateSplitSettings(job, mWork);\n\n    job.setNumReduceTasks(rWork != null ? rWork.getNumReduceTasks().intValue() : 0);\n    job.setReducerClass(ExecReducer.class);\n\n    // set input format information if necessary\n    setInputAttributes(job);\n\n    // Turn on speculative execution for reducers\n    boolean useSpeculativeExecReducers = HiveConf.getBoolVar(job,\n        HiveConf.ConfVars.HIVESPECULATIVEEXECREDUCERS);\n    job.setBoolean(MRJobConfig.REDUCE_SPECULATIVE, useSpeculativeExecReducers);\n\n    String inpFormat = HiveConf.getVar(job, HiveConf.ConfVars.HIVEINPUTFORMAT);\n\n    if (mWork.isUseBucketizedHiveInputFormat()) {\n      inpFormat = BucketizedHiveInputFormat.class.getName();\n    }\n\n    LOG.info(\"Using \" + inpFormat);\n\n    try {\n      job.setInputFormat(JavaUtils.loadClass(inpFormat));\n    } catch (ClassNotFoundException e) {\n      throw new RuntimeException(e.getMessage(), e);\n    }\n\n    // No-Op - we don't really write anything here ..\n    job.setOutputKeyClass(Text.class);\n    job.setOutputValueClass(Text.class);\n\n    int returnVal = 0;\n    boolean noName = StringUtils.isEmpty(job.get(MRJobConfig.JOB_NAME));\n\n    if (noName) {\n      // This is for a special case to ensure unit tests pass\n      job.set(MRJobConfig.JOB_NAME, \"JOB\" + Utilities.randGen.nextInt());\n    }\n\n    try{\n      MapredLocalWork localwork = mWork.getMapRedLocalWork();\n      if (localwork != null && localwork.hasStagedAlias()) {\n        if (!ShimLoader.getHadoopShims().isLocalMode(job)) {\n          Path localPath = localwork.getTmpPath();\n          Path hdfsPath = mWork.getTmpHDFSPath();\n\n          FileSystem hdfs = hdfsPath.getFileSystem(job);\n          FileSystem localFS = localPath.getFileSystem(job);\n          FileStatus[] hashtableFiles = localFS.listStatus(localPath);\n          int fileNumber = hashtableFiles.length;\n          String[] fileNames = new String[fileNumber];\n\n          for ( int i = 0; i < fileNumber; i++){\n            fileNames[i] = hashtableFiles[i].getPath().getName();\n          }\n\n          //package and compress all the hashtable files to an archive file\n          String stageId = this.getId();\n          String archiveFileName = Utilities.generateTarFileName(stageId);\n          localwork.setStageID(stageId);\n\n          CompressionUtils.tar(localPath.toUri().getPath(), fileNames,archiveFileName);\n          Path archivePath = Utilities.generateTarPath(localPath, stageId);\n          LOG.info(\"Archive \"+ hashtableFiles.length+\" hash table files to \" + archivePath);\n\n          //upload archive file to hdfs\n          Path hdfsFilePath =Utilities.generateTarPath(hdfsPath, stageId);\n          short replication = (short) job.getInt(\"mapred.submit.replication\", 10);\n          hdfs.copyFromLocalFile(archivePath, hdfsFilePath);\n          hdfs.setReplication(hdfsFilePath, replication);\n          LOG.info(\"Upload 1 archive file  from\" + archivePath + \" to: \" + hdfsFilePath);\n\n          //add the archive file to distributed cache\n          DistributedCache.createSymlink(job);\n          DistributedCache.addCacheArchive(hdfsFilePath.toUri(), job);\n          LOG.info(\"Add 1 archive file to distributed cache. Archive file: \" + hdfsFilePath.toUri());\n        }\n      }\n      work.configureJobConf(job);\n      List<Path> inputPaths = Utilities.getInputPaths(job, mWork, emptyScratchDir, ctx, false);\n      Utilities.setInputPaths(job, inputPaths);\n\n      Utilities.setMapRedWork(job, work, ctx.getMRTmpPath());\n\n      if (mWork.getSamplingType() > 0 && rWork != null && job.getNumReduceTasks() > 1) {\n        try {\n          handleSampling(ctx, mWork, job);\n          job.setPartitionerClass(HiveTotalOrderPartitioner.class);\n        } catch (IllegalStateException e) {\n          console.printInfo(\"Not enough sampling data.. Rolling back to single reducer task\");\n          rWork.setNumReduceTasks(1);\n          job.setNumReduceTasks(1);\n        } catch (Exception e) {\n          LOG.error(\"Sampling error\", e);\n          console.printError(e.toString(),\n              \"\\n\" + org.apache.hadoop.util.StringUtils.stringifyException(e));\n          rWork.setNumReduceTasks(1);\n          job.setNumReduceTasks(1);\n        }\n      }\n\n      jc = new JobClient(job);\n      // make this client wait if job tracker is not behaving well.\n      Throttle.checkJobTracker(job, LOG);\n\n      if (mWork.isGatheringStats() || (rWork != null && rWork.isGatheringStats())) {\n        // initialize stats publishing table\n        StatsPublisher statsPublisher;\n        StatsFactory factory = StatsFactory.newFactory(job);\n        if (factory != null) {\n          statsPublisher = factory.getStatsPublisher();\n          List<String> statsTmpDir = Utilities.getStatsTmpDirs(mWork, job);\n          if (rWork != null) {\n            statsTmpDir.addAll(Utilities.getStatsTmpDirs(rWork, job));\n          }\n          StatsCollectionContext sc = new StatsCollectionContext(job);\n          sc.setStatsTmpDirs(statsTmpDir);\n          if (!statsPublisher.init(sc)) { // creating stats table if not exists\n            if (HiveConf.getBoolVar(job, HiveConf.ConfVars.HIVE_STATS_RELIABLE)) {\n              throw\n                new HiveException(ErrorMsg.STATSPUBLISHER_INITIALIZATION_ERROR.getErrorCodedMsg());\n            }\n          }\n        }\n      }\n\n      Utilities.createTmpDirs(job, mWork);\n      Utilities.createTmpDirs(job, rWork);\n\n      SessionState ss = SessionState.get();\n      if (HiveConf.getVar(job, HiveConf.ConfVars.HIVE_EXECUTION_ENGINE).equals(\"tez\")\n          && ss != null) {\n        TezSessionState session = ss.getTezSession();\n        TezSessionPoolManager.getInstance().closeIfNotDefault(session, true);\n      }\n\n      HiveConfUtil.updateJobCredentialProviders(job);\n      // Finally SUBMIT the JOB!\n      if (driverContext.isShutdown()) {\n        LOG.warn(\"Task was cancelled\");\n        return 5;\n      }\n\n      rj = jc.submitJob(job);\n\n      if (driverContext.isShutdown()) {\n        LOG.warn(\"Task was cancelled\");\n        if (rj != null) {\n          rj.killJob();\n          rj = null;\n        }\n        return 5;\n      }\n\n      this.jobID = rj.getJobID();\n      updateStatusInQueryDisplay();\n      returnVal = jobExecHelper.progress(rj, jc, ctx);\n      success = (returnVal == 0);\n    } catch (Exception e) {\n      e.printStackTrace();\n      setException(e);\n      String mesg = \" with exception '\" + Utilities.getNameMessage(e) + \"'\";\n      if (rj != null) {\n        mesg = \"Ended Job = \" + rj.getJobID() + mesg;\n      } else {\n        mesg = \"Job Submission failed\" + mesg;\n      }\n\n      // Has to use full name to make sure it does not conflict with\n      // org.apache.commons.lang.StringUtils\n      console.printError(mesg, \"\\n\" + org.apache.hadoop.util.StringUtils.stringifyException(e));\n\n      success = false;\n      returnVal = 1;\n    } finally {\n      Utilities.clearWork(job);\n      try {\n        if (ctxCreated) {\n          ctx.clear();\n        }\n\n        if (rj != null) {\n          if (returnVal != 0) {\n            rj.killJob();\n          }\n          jobID = rj.getID().toString();\n        }\n        if (jc!=null) {\n          jc.close();\n        }\n      } catch (Exception e) {\n\tLOG.warn(\"Failed while cleaning up \", e);\n      } finally {\n\tHadoopJobExecHelper.runningJobs.remove(rj);\n      }\n    }\n\n    // get the list of Dynamic partition paths\n    try {\n      if (rj != null) {\n        if (mWork.getAliasToWork() != null) {\n          for (Operator<? extends OperatorDesc> op : mWork.getAliasToWork().values()) {\n            op.jobClose(job, success);\n          }\n        }\n        if (rWork != null) {\n          rWork.getReducer().jobClose(job, success);\n        }\n      }\n    } catch (Exception e) {\n      // jobClose needs to execute successfully otherwise fail task\n      if (success) {\n        setException(e);\n        success = false;\n        returnVal = 3;\n        String mesg = \"Job Commit failed with exception '\" + Utilities.getNameMessage(e) + \"'\";\n        console.printError(mesg, \"\\n\" + org.apache.hadoop.util.StringUtils.stringifyException(e));\n      }\n    }\n\n    return (returnVal);\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report mentions resource leaks and provides stack traces that include methods like 'org.apache.hadoop.hive.ql.Context.removeScratchDir' and 'org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager.unlockPrimitive'. These methods are in the same stack trace context as the ground truth methods, but the report does not precisely identify the root cause or problem location at the ground truth methods. There is no fix suggestion provided in the bug report. All information in the report is relevant to the context of the bug, so there is no wrong information."
        }
    },
    {
        "filename": "HIVE-7009.json",
        "code_diff": {
            "ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DagUtils.checkPreExisting": {
                "code_before_change": "  private boolean checkPreExisting(Path src, Path dest, Configuration conf)\n    throws IOException {\n    FileSystem destFS = dest.getFileSystem(conf);\n    FileSystem sourceFS = src.getFileSystem(conf);\n    if (destFS.exists(dest)) {\n      return (sourceFS.getFileStatus(src).getLen() == destFS.getFileStatus(dest).getLen());\n    }\n    return false;\n  }",
                "code_after_change": "  private boolean checkPreExisting(Path src, Path dest, Configuration conf)\n    throws IOException {\n    FileSystem destFS = dest.getFileSystem(conf);\n    FileSystem sourceFS = src.getFileSystem(conf);\n    if (destFS.exists(dest)) {\n      return (sourceFS.getFileStatus(src).getLen() == destFS.getFileStatus(dest).getLen());\n    }\n    return false;\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DagUtils.getDefaultDestDir": {
                "code_before_change": "  public Path getDefaultDestDir(Configuration conf) throws LoginException, IOException {\n    UserGroupInformation ugi = ShimLoader.getHadoopShims().getUGIForConf(conf);\n    String userName = ShimLoader.getHadoopShims().getShortUserName(ugi);\n    String userPathStr = HiveConf.getVar(conf, HiveConf.ConfVars.HIVE_USER_INSTALL_DIR);\n    Path userPath = new Path(userPathStr);\n    FileSystem fs = userPath.getFileSystem(conf);\n    if (!(fs instanceof DistributedFileSystem)) {\n      throw new IOException(ErrorMsg.INVALID_HDFS_URI.format(userPathStr));\n    }\n\n    String jarPathStr = userPathStr + \"/\" + userName;\n    String hdfsDirPathStr = jarPathStr;\n    Path hdfsDirPath = new Path(hdfsDirPathStr);\n\n    FileStatus fstatus = fs.getFileStatus(hdfsDirPath);\n    if (!fstatus.isDir()) {\n      throw new IOException(ErrorMsg.INVALID_DIR.format(hdfsDirPath.toString()));\n    }\n\n    Path retPath = new Path(hdfsDirPath.toString() + \"/.hiveJars\");\n\n    fs.mkdirs(retPath);\n    return retPath;\n  }",
                "code_after_change": "  public Path getDefaultDestDir(Configuration conf) throws LoginException, IOException {\n    UserGroupInformation ugi = ShimLoader.getHadoopShims().getUGIForConf(conf);\n    String userName = ShimLoader.getHadoopShims().getShortUserName(ugi);\n    String userPathStr = HiveConf.getVar(conf, HiveConf.ConfVars.HIVE_USER_INSTALL_DIR);\n    Path userPath = new Path(userPathStr);\n    FileSystem fs = userPath.getFileSystem(conf);\n\n    String jarPathStr = userPathStr + \"/\" + userName;\n    String hdfsDirPathStr = jarPathStr;\n    Path hdfsDirPath = new Path(hdfsDirPathStr);\n\n    FileStatus fstatus = fs.getFileStatus(hdfsDirPath);\n    if (!fstatus.isDir()) {\n      throw new IOException(ErrorMsg.INVALID_DIR.format(hdfsDirPath.toString()));\n    }\n\n    Path retPath = new Path(hdfsDirPath.toString() + \"/.hiveJars\");\n\n    fs.mkdirs(retPath);\n    return retPath;\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DagUtils.getHiveJarDirectory": {
                "code_before_change": "  public FileStatus getHiveJarDirectory(Configuration conf) throws IOException, LoginException {\n    FileStatus fstatus = null;\n    String hdfsDirPathStr = HiveConf.getVar(conf, HiveConf.ConfVars.HIVE_JAR_DIRECTORY, null);\n    if (hdfsDirPathStr != null) {\n      LOG.info(\"Hive jar directory is \" + hdfsDirPathStr);\n      fstatus = validateTargetDir(new Path(hdfsDirPathStr), conf);\n    }\n\n    if (fstatus == null) {\n      Path destDir = getDefaultDestDir(conf);\n      LOG.info(\"Jar dir is null/directory doesn't exist. Choosing HIVE_INSTALL_DIR - \" + destDir);\n      fstatus = validateTargetDir(destDir, conf);\n    }\n\n    if (fstatus == null) {\n      throw new IOException(ErrorMsg.NO_VALID_LOCATIONS.getMsg());\n    }\n    return fstatus;\n  }",
                "code_after_change": "  public FileStatus getHiveJarDirectory(Configuration conf) throws IOException, LoginException {\n    FileStatus fstatus = null;\n    String hdfsDirPathStr = HiveConf.getVar(conf, HiveConf.ConfVars.HIVE_JAR_DIRECTORY, null);\n    if (hdfsDirPathStr != null) {\n      LOG.info(\"Hive jar directory is \" + hdfsDirPathStr);\n      fstatus = validateTargetDir(new Path(hdfsDirPathStr), conf);\n    }\n\n    if (fstatus == null) {\n      Path destDir = getDefaultDestDir(conf);\n      LOG.info(\"Jar dir is null/directory doesn't exist. Choosing HIVE_INSTALL_DIR - \" + destDir);\n      fstatus = validateTargetDir(destDir, conf);\n    }\n\n    if (fstatus == null) {\n      throw new IOException(ErrorMsg.NO_VALID_LOCATIONS.getMsg());\n    }\n    return fstatus;\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report precisely identifies the root cause in the method 'ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DagUtils.getDefaultDestDir', which is part of the ground truth methods. The report describes the issue with enforcing the HDFS file system, which aligns with the changes made in the 'after' version of the method. However, the bug report does not provide any fix suggestion, hence it is marked as 'Missing' for fix suggestion. The problem location is also precisely identified as it directly mentions the method 'getDefaultDestDir' in the description. There is no wrong information in the bug report as all the details are relevant and accurate."
        }
    },
    {
        "filename": "HIVE-2031.json",
        "code_diff": {
            "ql.src.java.org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.tableSpec": {
                "code_before_change": [],
                "code_after_change": []
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report precisely identifies the root cause by mentioning the exception message related to the 'BaseSemanticAnalyzer.tableSpec' method, which is the ground truth method. However, there is no fix suggestion provided in the bug report, as it only states that the exception message needs to be corrected for better traceability. The problem location is also precise as the stack trace directly points to the 'BaseSemanticAnalyzer.tableSpec' method. There is no wrong information in the bug report as all the details are consistent with the context of the bug."
        }
    },
    {
        "filename": "HIVE-4018.json",
        "code_diff": {
            "ql.src.java.org.apache.hadoop.hive.ql.exec.MapJoinMetaData.getList": {
                "code_before_change": "  public static ArrayList<Object> getList(){\n    list.clear();\n    return list;\n  }",
                "code_after_change": "  public static ArrayList<Object> getList(){\n    list.clear();\n    return list;\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.MapJoinObjectKey.readExternal": {
                "code_before_change": "  public void readExternal(ObjectInput in) throws IOException,\n      ClassNotFoundException {\n    try {\n      // get the tableDesc from the map stored in the mapjoin operator\n      HashTableSinkObjectCtx ctx = MapJoinMetaData.get(\n          Integer.valueOf(metadataTag));\n\n      Writable val = ctx.getSerDe().getSerializedClass().newInstance();\n      val.readFields(in);\n      ArrayList<Object> list = (ArrayList<Object>) ObjectInspectorUtils.copyToStandardObject(ctx\n          .getSerDe().deserialize(val), ctx.getSerDe().getObjectInspector(),\n          ObjectInspectorCopyOption.WRITABLE);\n      if(list == null){\n        obj = new ArrayList(0).toArray();\n      }else{\n        obj = list.toArray();\n      }\n\n    } catch (Exception e) {\n      throw new IOException(e);\n    }\n\n  }",
                "code_after_change": "  public void readExternal(ObjectInput in) throws IOException,\n      ClassNotFoundException {\n    try {\n      // get the tableDesc from the map stored in the mapjoin operator\n      HashTableSinkObjectCtx ctx = MapJoinOperator.getMetadata().get(\n          Integer.valueOf(metadataTag));\n\n      Writable val = ctx.getSerDe().getSerializedClass().newInstance();\n      val.readFields(in);\n      ArrayList<Object> list = (ArrayList<Object>) ObjectInspectorUtils.copyToStandardObject(ctx\n          .getSerDe().deserialize(val), ctx.getSerDe().getObjectInspector(),\n          ObjectInspectorCopyOption.WRITABLE);\n      if(list == null){\n        obj = new ArrayList(0).toArray();\n      }else{\n        obj = list.toArray();\n      }\n\n    } catch (Exception e) {\n      throw new IOException(e);\n    }\n\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.exec.HashTableSinkOperator.getConf": {
                "code_before_change": "    public Configuration getConf() {\n      return conf;\n    }",
                "code_after_change": "    public Configuration getConf() {\n      return conf;\n    }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.exec.HashTableSinkOperator.setValueMetaData": {
                "code_before_change": "  private void setValueMetaData(int tag) throws SerDeException {\n    TableDesc valueTableDesc = conf.getValueTblFilteredDescs().get(tag);\n    SerDe valueSerDe = (SerDe) ReflectionUtils.newInstance(valueTableDesc.getDeserializerClass(),\n        null);\n\n    valueSerDe.initialize(null, valueTableDesc.getProperties());\n\n    List<ObjectInspector> newFields = rowContainerStandardObjectInspectors.get((Byte) alias);\n    int length = newFields.size();\n    List<String> newNames = new ArrayList<String>(length);\n    for (int i = 0; i < length; i++) {\n      String tmp = new String(\"tmp_\" + i);\n      newNames.add(tmp);\n    }\n    StandardStructObjectInspector standardOI = ObjectInspectorFactory\n        .getStandardStructObjectInspector(newNames, newFields);\n\n    MapJoinMetaData.put(Integer.valueOf(metadataValueTag[tag]), new HashTableSinkObjectCtx(\n        standardOI, valueSerDe, valueTableDesc, hconf));\n  }",
                "code_after_change": "  private void setValueMetaData(int tag) throws SerDeException {\n    TableDesc valueTableDesc = conf.getValueTblFilteredDescs().get(tag);\n    SerDe valueSerDe = (SerDe) ReflectionUtils.newInstance(valueTableDesc.getDeserializerClass(),\n        null);\n\n    valueSerDe.initialize(null, valueTableDesc.getProperties());\n\n    List<ObjectInspector> newFields = rowContainerStandardObjectInspectors[alias];\n    int length = newFields.size();\n    List<String> newNames = new ArrayList<String>(length);\n    for (int i = 0; i < length; i++) {\n      String tmp = new String(\"tmp_\" + i);\n      newNames.add(tmp);\n    }\n    StandardStructObjectInspector standardOI = ObjectInspectorFactory\n        .getStandardStructObjectInspector(newNames, newFields);\n\n    int alias = Integer.valueOf(metadataValueTag[tag]);\n    metadata.put(Integer.valueOf(metadataValueTag[tag]), new HashTableSinkObjectCtx(\n        standardOI, valueSerDe, valueTableDesc, hasFilter(alias), hconf));\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.exec.MapJoinOperator.generateMapMetaData": {
                "code_before_change": "  public void generateMapMetaData() throws HiveException, SerDeException {\n    // generate the meta data for key\n    // index for key is -1\n    TableDesc keyTableDesc = conf.getKeyTblDesc();\n    SerDe keySerializer = (SerDe) ReflectionUtils.newInstance(keyTableDesc.getDeserializerClass(),\n        null);\n    keySerializer.initialize(null, keyTableDesc.getProperties());\n    MapJoinMetaData.put(Integer.valueOf(metadataKeyTag), new HashTableSinkObjectCtx(\n        ObjectInspectorUtils.getStandardObjectInspector(keySerializer.getObjectInspector(),\n            ObjectInspectorCopyOption.WRITABLE), keySerializer, keyTableDesc, hconf));\n\n    for (int pos = 0; pos < order.length; pos++) {\n      if (pos == posBigTable) {\n        continue;\n      }\n      TableDesc valueTableDesc;\n      if (conf.getNoOuterJoin()) {\n        valueTableDesc = conf.getValueTblDescs().get(pos);\n      } else {\n        valueTableDesc = conf.getValueFilteredTblDescs().get(pos);\n      }\n      SerDe valueSerDe = (SerDe) ReflectionUtils.newInstance(valueTableDesc.getDeserializerClass(),\n          null);\n      valueSerDe.initialize(null, valueTableDesc.getProperties());\n\n      MapJoinMetaData.put(Integer.valueOf(pos), new HashTableSinkObjectCtx(ObjectInspectorUtils\n          .getStandardObjectInspector(valueSerDe.getObjectInspector(),\n              ObjectInspectorCopyOption.WRITABLE), valueSerDe, valueTableDesc, hconf));\n    }\n  }",
                "code_after_change": "  public void generateMapMetaData() throws HiveException, SerDeException {\n    // generate the meta data for key\n    // index for key is -1\n    TableDesc keyTableDesc = conf.getKeyTblDesc();\n    SerDe keySerializer = (SerDe) ReflectionUtils.newInstance(keyTableDesc.getDeserializerClass(),\n        null);\n    keySerializer.initialize(null, keyTableDesc.getProperties());\n    metadata.put(Integer.valueOf(metadataKeyTag), new HashTableSinkObjectCtx(\n        ObjectInspectorUtils.getStandardObjectInspector(keySerializer.getObjectInspector(),\n            ObjectInspectorCopyOption.WRITABLE), keySerializer, keyTableDesc, false, hconf));\n\n    for (int pos = 0; pos < order.length; pos++) {\n      if (pos == posBigTable) {\n        continue;\n      }\n      TableDesc valueTableDesc;\n      if (conf.getNoOuterJoin()) {\n        valueTableDesc = conf.getValueTblDescs().get(pos);\n      } else {\n        valueTableDesc = conf.getValueFilteredTblDescs().get(pos);\n      }\n      SerDe valueSerDe = (SerDe) ReflectionUtils.newInstance(valueTableDesc.getDeserializerClass(),\n          null);\n      valueSerDe.initialize(null, valueTableDesc.getProperties());\n\n      ObjectInspector inspector = valueSerDe.getObjectInspector();\n      metadata.put(Integer.valueOf(pos), new HashTableSinkObjectCtx(ObjectInspectorUtils\n          .getStandardObjectInspector(inspector, ObjectInspectorCopyOption.WRITABLE),\n          valueSerDe, valueTableDesc, hasFilter(pos), hconf));\n    }\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.MapJoinSingleKey.hashCode": {
                "code_before_change": "  public int hashCode() {\n    int hashCode;\n    if (obj == null) {\n      hashCode = metadataTag;\n    } else {\n      hashCode = 31 + obj.hashCode();\n    }\n    return hashCode;\n  }",
                "code_after_change": "  public int hashCode() {\n    int hashCode;\n    if (obj == null) {\n      hashCode = metadataTag;\n    } else {\n      hashCode = 31 + obj.hashCode();\n    }\n    return hashCode;\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.MapJoinDoubleKeys.hashCode": {
                "code_before_change": "  public int hashCode() {\n    int hashCode = 1;\n    if (obj1 == null) {\n      hashCode = metadataTag;\n    } else {\n      hashCode += (31 + obj1.hashCode());\n    }\n    if (obj2 == null) {\n      hashCode += metadataTag;\n    } else {\n      hashCode += (31 + obj2.hashCode());\n    }\n    return hashCode;\n  }",
                "code_after_change": "  public int hashCode() {\n    int hashCode = 1;\n    if (obj1 == null) {\n      hashCode = metadataTag;\n    } else {\n      hashCode += (31 + obj1.hashCode());\n    }\n    if (obj2 == null) {\n      hashCode += metadataTag;\n    } else {\n      hashCode += (31 + obj2.hashCode());\n    }\n    return hashCode;\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.exec.HashTableSinkOperator.processOp": {
                "code_before_change": "  public void processOp(Object row, int tag) throws HiveException {\n    // let the mapJoinOp process these small tables\n    try {\n      if (firstRow) {\n        // generate the map metadata\n        setKeyMetaData();\n        firstRow = false;\n      }\n      alias = (byte)tag;\n\n      // compute keys and values as StandardObjects\n      AbstractMapJoinKey keyMap = JoinUtil.computeMapJoinKeys(row, joinKeys.get(alias),\n          joinKeysObjectInspectors.get(alias));\n\n      Object[] value = JoinUtil.computeMapJoinValues(row, joinValues.get(alias),\n          joinValuesObjectInspectors.get(alias), joinFilters.get(alias), joinFilterObjectInspectors\n              .get(alias), filterMap == null ? null : filterMap[alias]);\n\n\n      HashMapWrapper<AbstractMapJoinKey, MapJoinObjectValue> hashTable = mapJoinTables.get(alias);\n\n      MapJoinObjectValue o = hashTable.get(keyMap);\n      MapJoinRowContainer<Object[]> res = null;\n\n      boolean needNewKey = true;\n      if (o == null) {\n        res = new MapJoinRowContainer<Object[]>();\n        res.add(value);\n\n        if (metadataValueTag[tag] == -1) {\n          metadataValueTag[tag] = order[tag];\n          setValueMetaData(tag);\n        }\n\n        // Construct externalizable objects for key and value\n        if (needNewKey) {\n          MapJoinObjectValue valueObj = new MapJoinObjectValue(metadataValueTag[tag], res);\n\n          rowNumber++;\n          if (rowNumber > hashTableScale && rowNumber % hashTableScale == 0) {\n            isAbort = hashTable.isAbort(rowNumber, console);\n            if (isAbort) {\n              throw new HiveException(\"RunOutOfMeomoryUsage\");\n            }\n          }\n          hashTable.put(keyMap, valueObj);\n        }\n\n      } else {\n        res = o.getObj();\n        res.add(value);\n      }\n\n\n    } catch (SerDeException e) {\n      throw new HiveException(e);\n    }\n\n  }",
                "code_after_change": "  public void processOp(Object row, int tag) throws HiveException {\n    // let the mapJoinOp process these small tables\n    try {\n      if (firstRow) {\n        // generate the map metadata\n        setKeyMetaData();\n        firstRow = false;\n      }\n      alias = (byte)tag;\n\n      // compute keys and values as StandardObjects\n      AbstractMapJoinKey keyMap = JoinUtil.computeMapJoinKeys(row, joinKeys[alias],\n          joinKeysObjectInspectors[alias]);\n\n      Object[] value = JoinUtil.computeMapJoinValues(row, joinValues[alias],\n          joinValuesObjectInspectors[alias], joinFilters[alias], joinFilterObjectInspectors[alias],\n          filterMaps == null ? null : filterMaps[alias]);\n\n      HashMapWrapper<AbstractMapJoinKey, MapJoinObjectValue> hashTable = mapJoinTables[alias];\n\n      MapJoinObjectValue o = hashTable.get(keyMap);\n      MapJoinRowContainer<Object[]> res = null;\n\n      boolean needNewKey = true;\n      if (o == null) {\n        res = new MapJoinRowContainer<Object[]>();\n        res.add(value);\n\n        if (metadataValueTag[tag] == -1) {\n          metadataValueTag[tag] = order[tag];\n          setValueMetaData(tag);\n        }\n\n        // Construct externalizable objects for key and value\n        if (needNewKey) {\n          MapJoinObjectValue valueObj = new MapJoinObjectValue(\n              metadataValueTag[tag], res);\n\n          rowNumber++;\n          if (rowNumber > hashTableScale && rowNumber % hashTableScale == 0) {\n            isAbort = hashTable.isAbort(rowNumber, console);\n            if (isAbort) {\n              throw new HiveException(\"RunOutOfMeomoryUsage\");\n            }\n          }\n          hashTable.put(keyMap, valueObj);\n        }\n\n      } else {\n        res = o.getObj();\n        res.add(value);\n      }\n\n\n    } catch (SerDeException e) {\n      throw new HiveException(e);\n    }\n\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.exec.MapJoinOperator.loadHashTable": {
                "code_before_change": "  private void loadHashTable() throws HiveException {\n\n    if (!this.getExecContext().getLocalWork().getInputFileChangeSensitive()) {\n      if (hashTblInitedOnce) {\n        return;\n      } else {\n        hashTblInitedOnce = true;\n      }\n    }\n\n    String baseDir = null;\n\n    String currentInputFile = getExecContext().getCurrentInputFile();\n    LOG.info(\"******* Load from HashTable File: input : \" + currentInputFile);\n\n    String fileName = getExecContext().getLocalWork().getBucketFileName(currentInputFile);\n\n    try {\n      if (ShimLoader.getHadoopShims().isLocalMode(hconf)) {\n        baseDir = this.getExecContext().getLocalWork().getTmpFileURI();\n      } else {\n        Path[] localArchives;\n        String stageID = this.getExecContext().getLocalWork().getStageID();\n        String suffix = Utilities.generateTarFileName(stageID);\n        FileSystem localFs = FileSystem.getLocal(hconf);\n        localArchives = DistributedCache.getLocalCacheArchives(this.hconf);\n        Path archive;\n        for (int j = 0; j < localArchives.length; j++) {\n          archive = localArchives[j];\n          if (!archive.getName().endsWith(suffix)) {\n            continue;\n          }\n          Path archiveLocalLink = archive.makeQualified(localFs);\n          baseDir = archiveLocalLink.toUri().getPath();\n        }\n      }\n      for (Map.Entry<Byte, HashMapWrapper<AbstractMapJoinKey, MapJoinObjectValue>> entry : mapJoinTables\n          .entrySet()) {\n        Byte pos = entry.getKey();\n        HashMapWrapper<AbstractMapJoinKey, MapJoinObjectValue> hashtable = entry.getValue();\n        String filePath = Utilities.generatePath(baseDir, conf.getDumpFilePrefix(), pos, fileName);\n        Path path = new Path(filePath);\n        LOG.info(\"\\tLoad back 1 hashtable file from tmp file uri:\" + path.toString());\n        hashtable.initilizePersistentHash(path.toUri().getPath());\n      }\n    } catch (Exception e) {\n      LOG.error(\"Load Distributed Cache Error\");\n      throw new HiveException(e.getMessage());\n    }\n  }",
                "code_after_change": "  private void loadHashTable() throws HiveException {\n\n    if (!this.getExecContext().getLocalWork().getInputFileChangeSensitive()) {\n      if (hashTblInitedOnce) {\n        return;\n      } else {\n        hashTblInitedOnce = true;\n      }\n    }\n\n    String baseDir = null;\n\n    String currentInputFile = getExecContext().getCurrentInputFile();\n    LOG.info(\"******* Load from HashTable File: input : \" + currentInputFile);\n\n    String fileName = getExecContext().getLocalWork().getBucketFileName(currentInputFile);\n\n    try {\n      if (ShimLoader.getHadoopShims().isLocalMode(hconf)) {\n        baseDir = this.getExecContext().getLocalWork().getTmpFileURI();\n      } else {\n        Path[] localArchives;\n        String stageID = this.getExecContext().getLocalWork().getStageID();\n        String suffix = Utilities.generateTarFileName(stageID);\n        FileSystem localFs = FileSystem.getLocal(hconf);\n        localArchives = DistributedCache.getLocalCacheArchives(this.hconf);\n        Path archive;\n        for (int j = 0; j < localArchives.length; j++) {\n          archive = localArchives[j];\n          if (!archive.getName().endsWith(suffix)) {\n            continue;\n          }\n          Path archiveLocalLink = archive.makeQualified(localFs);\n          baseDir = archiveLocalLink.toUri().getPath();\n        }\n      }\n      for (byte pos = 0; pos < mapJoinTables.length; pos++) {\n        HashMapWrapper<AbstractMapJoinKey, MapJoinObjectValue> hashtable = mapJoinTables[pos];\n        if (hashtable == null) {\n          continue;\n        }\n        String filePath = Utilities.generatePath(baseDir, conf.getDumpFilePrefix(), pos, fileName);\n        Path path = new Path(filePath);\n        LOG.info(\"\\tLoad back 1 hashtable file from tmp file uri:\" + path.toString());\n        hashtable.initilizePersistentHash(path.toUri().getPath());\n      }\n    } catch (Exception e) {\n      LOG.error(\"Load Distributed Cache Error\", e);\n      throw new HiveException(e);\n    }\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.MapJoinObjectValue.writeExternal": {
                "code_before_change": "  public void writeExternal(ObjectOutput out) throws IOException {\n    try {\n\n      out.writeInt(metadataTag);\n\n      // get the tableDesc from the map stored in the mapjoin operator\n      HashTableSinkObjectCtx ctx = MapJoinMetaData.get(Integer.valueOf(metadataTag));\n\n      // Different processing for key and value\n      MapJoinRowContainer<Object[]> v = obj;\n      out.writeInt(v.size());\n      if (v.size() > 0) {\n        Object[] row = v.first();\n        out.writeInt(row.length);\n\n        if (row.length > 0) {\n          for (; row != null; row = v.next()) {\n            Writable outVal = ctx.getSerDe().serialize(row, ctx.getStandardOI());\n            outVal.write(out);\n          }\n        }\n      }\n    } catch (SerDeException e) {\n      throw new IOException(e);\n    } catch (HiveException e) {\n      throw new IOException(e);\n    }\n  }",
                "code_after_change": "  public void writeExternal(ObjectOutput out) throws IOException {\n    try {\n\n      out.writeInt(metadataTag);\n\n      // get the tableDesc from the map stored in the mapjoin operator\n      HashTableSinkObjectCtx ctx = HashTableSinkOperator.getMetadata().get(\n          Integer.valueOf(metadataTag));\n\n      // Different processing for key and value\n      MapJoinRowContainer<Object[]> v = obj;\n      out.writeInt(v.size());\n      if (v.size() > 0) {\n        Object[] row = v.first();\n        out.writeInt(row.length);\n\n        if (row.length > 0) {\n          for (; row != null; row = v.next()) {\n            Writable outVal = ctx.getSerDe().serialize(row, ctx.getStandardOI());\n            outVal.write(out);\n          }\n        }\n      }\n    } catch (SerDeException e) {\n      throw new IOException(e);\n    } catch (HiveException e) {\n      throw new IOException(e);\n    }\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.MapJoinObjectValue.readExternal": {
                "code_before_change": "  public void readExternal(ObjectInput in) throws IOException, ClassNotFoundException {\n    try {\n\n      metadataTag = in.readInt();\n\n      // get the tableDesc from the map stored in the mapjoin operator\n      HashTableSinkObjectCtx ctx = MapJoinMetaData.get(Integer.valueOf(metadataTag));\n      int sz = in.readInt();\n\n      MapJoinRowContainer<Object[]> res = new MapJoinRowContainer<Object[]>();\n      if (sz > 0) {\n        int numCols = in.readInt();\n        if (numCols > 0) {\n          for (int pos = 0; pos < sz; pos++) {\n            Writable val = ctx.getSerDe().getSerializedClass().newInstance();\n            val.readFields(in);\n\n            ArrayList<Object> memObj = (ArrayList<Object>) ObjectInspectorUtils\n                .copyToStandardObject(ctx.getSerDe().deserialize(val), ctx.getSerDe()\n                    .getObjectInspector(), ObjectInspectorCopyOption.WRITABLE);\n\n            if (memObj == null) {\n              res.add(new ArrayList<Object>(0).toArray());\n            } else {\n              res.add(memObj.toArray());\n            }\n          }\n        } else {\n          for (int i = 0; i < sz; i++) {\n            res.add(new ArrayList<Object>(0).toArray());\n          }\n        }\n      }\n      obj = res;\n    } catch (Exception e) {\n      throw new IOException(e);\n    }\n  }",
                "code_after_change": "  public void readExternal(ObjectInput in) throws IOException, ClassNotFoundException {\n    try {\n\n      metadataTag = in.readInt();\n\n      // get the tableDesc from the map stored in the mapjoin operator\n      HashTableSinkObjectCtx ctx = MapJoinOperator.getMetadata().get(\n          Integer.valueOf(metadataTag));\n      int sz = in.readInt();\n      MapJoinRowContainer<Object[]> res = new MapJoinRowContainer<Object[]>();\n      if (sz > 0) {\n        int numCols = in.readInt();\n        if (numCols > 0) {\n          for (int pos = 0; pos < sz; pos++) {\n            Writable val = ctx.getSerDe().getSerializedClass().newInstance();\n            val.readFields(in);\n\n            ArrayList<Object> memObj = (ArrayList<Object>) ObjectInspectorUtils\n                .copyToStandardObject(ctx.getSerDe().deserialize(val), ctx.getSerDe()\n                    .getObjectInspector(), ObjectInspectorCopyOption.WRITABLE);\n\n            if (memObj == null) {\n              res.add(new ArrayList<Object>(0).toArray());\n            } else {\n              Object[] array = memObj.toArray();\n              res.add(array);\n              if (ctx.hasFilterTag()) {\n                aliasFilter &= ((ShortWritable)array[array.length - 1]).get();\n              }\n            }\n          }\n        } else {\n          for (int i = 0; i < sz; i++) {\n            res.add(new ArrayList<Object>(0).toArray());\n          }\n        }\n      }\n      obj = res;\n    } catch (Exception e) {\n      throw new IOException(e);\n    }\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.MapJoinSingleKey.writeExternal": {
                "code_before_change": "  public void writeExternal(ObjectOutput out) throws IOException {\n    try {\n      // out.writeInt(metadataTag);\n      // get the tableDesc from the map stored in the mapjoin operator\n      HashTableSinkObjectCtx ctx = MapJoinMetaData.get(Integer.valueOf(metadataTag));\n\n      ArrayList<Object> list = MapJoinMetaData.getList();\n      list.add(obj);\n\n      // Different processing for key and value\n      Writable outVal = ctx.getSerDe().serialize(list, ctx.getStandardOI());\n      outVal.write(out);\n\n    } catch (SerDeException e) {\n      throw new IOException(e);\n    }\n  }",
                "code_after_change": "  public void writeExternal(ObjectOutput out) throws IOException {\n    try {\n      // out.writeInt(metadataTag);\n      // get the tableDesc from the map stored in the mapjoin operator\n      HashTableSinkObjectCtx ctx = HashTableSinkOperator.getMetadata().get(\n          Integer.valueOf(metadataTag));\n\n      ArrayList<Object> list = MapJoinMetaData.getList();\n      list.add(obj);\n\n      // Different processing for key and value\n      Writable outVal = ctx.getSerDe().serialize(list, ctx.getStandardOI());\n      outVal.write(out);\n\n    } catch (SerDeException e) {\n      throw new IOException(e);\n    }\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.exec.HashTableSinkOperator.setKeyMetaData": {
                "code_before_change": "  private void setKeyMetaData() throws SerDeException {\n    TableDesc keyTableDesc = conf.getKeyTblDesc();\n    SerDe keySerializer = (SerDe) ReflectionUtils.newInstance(keyTableDesc.getDeserializerClass(),\n        null);\n    keySerializer.initialize(null, keyTableDesc.getProperties());\n\n    MapJoinMetaData.clear();\n    MapJoinMetaData.put(Integer.valueOf(metadataKeyTag), new HashTableSinkObjectCtx(\n        ObjectInspectorUtils.getStandardObjectInspector(keySerializer.getObjectInspector(),\n            ObjectInspectorCopyOption.WRITABLE), keySerializer, keyTableDesc, hconf));\n  }",
                "code_after_change": "  private void setKeyMetaData() throws SerDeException {\n    TableDesc keyTableDesc = conf.getKeyTblDesc();\n    SerDe keySerializer = (SerDe) ReflectionUtils.newInstance(keyTableDesc.getDeserializerClass(),\n        null);\n    keySerializer.initialize(null, keyTableDesc.getProperties());\n\n    metadata.put(Integer.valueOf(metadataKeyTag), new HashTableSinkObjectCtx(\n        ObjectInspectorUtils.getStandardObjectInspector(keySerializer.getObjectInspector(),\n            ObjectInspectorCopyOption.WRITABLE), keySerializer, keyTableDesc, false, hconf));\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.exec.HashTableSinkOperator.closeOp": {
                "code_before_change": "  public void closeOp(boolean abort) throws HiveException {\n    try {\n      if (mapJoinTables != null) {\n        // get tmp file URI\n        String tmpURI = this.getExecContext().getLocalWork().getTmpFileURI();\n        LOG.info(\"Get TMP URI: \" + tmpURI);\n        long fileLength;\n        for (Map.Entry<Byte, HashMapWrapper<AbstractMapJoinKey, MapJoinObjectValue>> hashTables : mapJoinTables\n            .entrySet()) {\n          // get the key and value\n          Byte tag = hashTables.getKey();\n          HashMapWrapper<AbstractMapJoinKey, MapJoinObjectValue> hashTable = hashTables.getValue();\n\n          // get current input file name\n          String bigBucketFileName = getExecContext().getCurrentBigBucketFile();\n\n          String fileName = getExecContext().getLocalWork().getBucketFileName(bigBucketFileName);\n\n          // get the tmp URI path; it will be a hdfs path if not local mode\n          String dumpFilePrefix = conf.getDumpFilePrefix();\n          String tmpURIPath = Utilities.generatePath(tmpURI, dumpFilePrefix, tag, fileName);\n          hashTable.isAbort(rowNumber, console);\n          console.printInfo(Utilities.now() + \"\\tDump the hashtable into file: \" + tmpURIPath);\n          // get the hashtable file and path\n          Path path = new Path(tmpURIPath);\n          FileSystem fs = path.getFileSystem(hconf);\n          File file = new File(path.toUri().getPath());\n          fs.create(path);\n          fileLength = hashTable.flushMemoryCacheToPersistent(file);\n          console.printInfo(Utilities.now() + \"\\tUpload 1 File to: \" + tmpURIPath + \" File size: \"\n              + fileLength);\n\n          hashTable.close();\n        }\n      }\n\n      super.closeOp(abort);\n    } catch (Exception e) {\n      LOG.error(\"Generate Hashtable error\");\n      e.printStackTrace();\n    }\n  }",
                "code_after_change": "  public void closeOp(boolean abort) throws HiveException {\n    try {\n      if (mapJoinTables != null) {\n        // get tmp file URI\n        String tmpURI = this.getExecContext().getLocalWork().getTmpFileURI();\n        LOG.info(\"Get TMP URI: \" + tmpURI);\n        long fileLength;\n        for (byte tag = 0; tag < mapJoinTables.length; tag++) {\n          // get the key and value\n          HashMapWrapper<AbstractMapJoinKey, MapJoinObjectValue> hashTable = mapJoinTables[tag];\n          if (hashTable == null) {\n            continue;\n          }\n\n          // get current input file name\n          String bigBucketFileName = getExecContext().getCurrentBigBucketFile();\n\n          String fileName = getExecContext().getLocalWork().getBucketFileName(bigBucketFileName);\n\n          // get the tmp URI path; it will be a hdfs path if not local mode\n          String dumpFilePrefix = conf.getDumpFilePrefix();\n          String tmpURIPath = Utilities.generatePath(tmpURI, dumpFilePrefix, tag, fileName);\n          hashTable.isAbort(rowNumber, console);\n          console.printInfo(Utilities.now() + \"\\tDump the hashtable into file: \" + tmpURIPath);\n          // get the hashtable file and path\n          Path path = new Path(tmpURIPath);\n          FileSystem fs = path.getFileSystem(hconf);\n          File file = new File(path.toUri().getPath());\n          fs.create(path);\n          fileLength = hashTable.flushMemoryCacheToPersistent(file);\n          console.printInfo(Utilities.now() + \"\\tUpload 1 File to: \" + tmpURIPath + \" File size: \"\n              + fileLength);\n\n          hashTable.close();\n        }\n      }\n\n      super.closeOp(abort);\n    } catch (Exception e) {\n      LOG.error(\"Generate Hashtable error\", e);\n      e.printStackTrace();\n    }\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.MapJoinDoubleKeys.writeExternal": {
                "code_before_change": "  public void writeExternal(ObjectOutput out) throws IOException {\n    try {\n      // out.writeInt(metadataTag);\n      // get the tableDesc from the map stored in the mapjoin operator\n      HashTableSinkObjectCtx ctx = MapJoinMetaData.get(Integer.valueOf(metadataTag));\n\n      ArrayList<Object> list = MapJoinMetaData.getList();\n      list.add(obj1);\n      list.add(obj2);\n      // Different processing for key and value\n      Writable outVal = ctx.getSerDe().serialize(list, ctx.getStandardOI());\n      outVal.write(out);\n\n    } catch (SerDeException e) {\n      throw new IOException(e);\n    }\n  }",
                "code_after_change": "  public void writeExternal(ObjectOutput out) throws IOException {\n    try {\n      // out.writeInt(metadataTag);\n      // get the tableDesc from the map stored in the mapjoin operator\n      HashTableSinkObjectCtx ctx = HashTableSinkOperator.getMetadata().get(\n          Integer.valueOf(metadataTag));\n\n      ArrayList<Object> list = MapJoinMetaData.getList();\n      list.add(obj1);\n      list.add(obj2);\n      // Different processing for key and value\n      Writable outVal = ctx.getSerDe().serialize(list, ctx.getStandardOI());\n      outVal.write(out);\n\n    } catch (SerDeException e) {\n      throw new IOException(e);\n    }\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": ""
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": ""
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report precisely identifies the root cause by mentioning the 'MapJoinOperator.loadHashTable' method, which is part of the ground truth methods. The error message 'Load Distributed Cache Error' is directly related to this method, indicating a precise identification of the root cause. However, the bug report does not provide any fix suggestion, hence it is marked as 'Missing' for fix suggestion. The problem location is also precisely identified as it mentions 'MapJoinOperator.loadHashTable' in the stack trace, which is a ground truth method. There is no wrong information in the bug report as all the details are relevant to the context of the bug."
        }
    },
    {
        "filename": "HIVE-11255.json",
        "code_diff": {
            "metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.get_table_objects_by_name": {
                "code_before_change": "    public List<Table> get_table_objects_by_name(final String dbname, final List<String> names)\n        throws MetaException, InvalidOperationException, UnknownDBException {\n      List<Table> tables = null;\n      startMultiTableFunction(\"get_multi_table\", dbname, names);\n      Exception ex = null;\n      try {\n\n        if (dbname == null || dbname.isEmpty()) {\n          throw new UnknownDBException(\"DB name is null or empty\");\n        }\n        if (names == null)\n        {\n          throw new InvalidOperationException(dbname + \" cannot find null tables\");\n        }\n        tables = getMS().getTableObjectsByName(dbname, names);\n      } catch (Exception e) {\n        ex = e;\n        if (e instanceof MetaException) {\n          throw (MetaException) e;\n        } else if (e instanceof InvalidOperationException) {\n          throw (InvalidOperationException) e;\n        } else if (e instanceof UnknownDBException) {\n          throw (UnknownDBException) e;\n        } else {\n          throw newMetaException(e);\n        }\n      } finally {\n        endFunction(\"get_multi_table\", tables != null, ex, join(names, \",\"));\n      }\n      return tables;\n    }",
                "code_after_change": "    public List<Table> get_table_objects_by_name(final String dbName, final List<String> tableNames)\n        throws MetaException, InvalidOperationException, UnknownDBException {\n      List<Table> tables = new ArrayList<Table>();\n      startMultiTableFunction(\"get_multi_table\", dbName, tableNames);\n      Exception ex = null;\n      int tableBatchSize = HiveConf.getIntVar(hiveConf,\n          ConfVars.METASTORE_BATCH_RETRIEVE_MAX);\n\n      try {\n        if (dbName == null || dbName.isEmpty()) {\n          throw new UnknownDBException(\"DB name is null or empty\");\n        }\n        if (tableNames == null)\n        {\n          throw new InvalidOperationException(dbName + \" cannot find null tables\");\n        }\n\n        // The list of table names could contain duplicates. RawStore.getTableObjectsByName()\n        // only guarantees returning no duplicate table objects in one batch. If we need\n        // to break into multiple batches, remove duplicates first.\n        List<String> distinctTableNames = tableNames;\n        if (distinctTableNames.size() > tableBatchSize) {\n          List<String> lowercaseTableNames = new ArrayList<String>();\n          for (String tableName : tableNames) {\n            lowercaseTableNames.add(HiveStringUtils.normalizeIdentifier(tableName));\n          }\n          distinctTableNames = new ArrayList<String>(new HashSet<String>(lowercaseTableNames));\n        }\n\n        RawStore ms = getMS();\n        int startIndex = 0;\n        // Retrieve the tables from the metastore in batches. Some databases like\n        // Oracle cannot have over 1000 expressions in a in-list\n        while (startIndex < distinctTableNames.size()) {\n          int endIndex = Math.min(startIndex + tableBatchSize, distinctTableNames.size());\n          tables.addAll(ms.getTableObjectsByName(dbName, distinctTableNames.subList(startIndex, endIndex)));\n          startIndex = endIndex;\n        }\n      } catch (Exception e) {\n        ex = e;\n        if (e instanceof MetaException) {\n          throw (MetaException) e;\n        } else if (e instanceof InvalidOperationException) {\n          throw (InvalidOperationException) e;\n        } else if (e instanceof UnknownDBException) {\n          throw (UnknownDBException) e;\n        } else {\n          throw newMetaException(e);\n        }\n      } finally {\n        endFunction(\"get_multi_table\", tables != null, ex, join(tableNames, \",\"));\n      }\n      return tables;\n    }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.get_table_core": {
                "code_before_change": "    public Table get_table_core(final String dbname, final String name) throws MetaException,\n        NoSuchObjectException {\n      Table t;\n      try {\n        t = getMS().getTable(dbname, name);\n        if (t == null) {\n          throw new NoSuchObjectException(dbname + \".\" + name\n              + \" table not found\");\n        }\n      } catch (Exception e) {\n        if (e instanceof MetaException) {\n          throw (MetaException) e;\n        } else if (e instanceof NoSuchObjectException) {\n          throw (NoSuchObjectException) e;\n        } else {\n          throw newMetaException(e);\n        }\n      }\n      return t;\n    }",
                "code_after_change": "    public Table get_table_core(final String dbname, final String name) throws MetaException,\n        NoSuchObjectException {\n      Table t;\n      try {\n        t = getMS().getTable(dbname, name);\n        if (t == null) {\n          throw new NoSuchObjectException(dbname + \".\" + name\n              + \" table not found\");\n        }\n      } catch (Exception e) {\n        if (e instanceof MetaException) {\n          throw (MetaException) e;\n        } else if (e instanceof NoSuchObjectException) {\n          throw (NoSuchObjectException) e;\n        } else {\n          throw newMetaException(e);\n        }\n      }\n      return t;\n    }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Preventive",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report precisely identifies the root cause by mentioning the method 'get_table_objects_by_name' in HiveMetaStore.java, which is one of the ground truth methods. The fix suggestion is preventive as it suggests breaking the table list into multiple sublists to avoid the SQLSyntaxErrorException, which aligns with the developer's fix of batching the table retrieval. The problem location is also precise as it directly mentions the method 'get_table_objects_by_name' in HiveMetaStore.java. There is no wrong information in the bug report as all details are relevant and accurate."
        }
    },
    {
        "filename": "HIVE-10151.json",
        "code_diff": {
            "ql.src.java.org.apache.hadoop.hive.ql.optimizer.BucketingSortingReduceSinkOptimizer.process": {
                "code_before_change": "      public Object process(Node nd, Stack<Node> stack,\n          NodeProcessorCtx procCtx, Object... nodeOutputs) throws SemanticException {\n        return null;\n      }",
                "code_after_change": "      public Object process(Node nd, Stack<Node> stack,\n          NodeProcessorCtx procCtx, Object... nodeOutputs) throws SemanticException {\n        return null;\n      }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.exec.Operator.isUseBucketizedHiveInputFormat": {
                "code_before_change": "  public boolean isUseBucketizedHiveInputFormat() {\n    return useBucketizedHiveInputFormat;\n  }",
                "code_after_change": "  public boolean isUseBucketizedHiveInputFormat() {\n    return useBucketizedHiveInputFormat;\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.optimizer.BucketingSortingReduceSinkOptimizer.checkTable": {
                "code_before_change": "    private boolean checkTable(Table table,\n        List<Integer> bucketPositionsDest,\n        List<Integer> sortPositionsDest,\n        List<Integer> sortOrderDest,\n        int numBucketsDest) {\n      // The bucketing and sorting positions should exactly match\n      int numBuckets = table.getNumBuckets();\n      if (numBucketsDest != numBuckets) {\n        return false;\n      }\n\n      List<Integer> tableBucketPositions =\n          getBucketPositions(table.getBucketCols(), table.getCols());\n      ObjectPair<List<Integer>, List<Integer>> tableSortPositionsOrder =\n          getSortPositionsOrder(table.getSortCols(), table.getCols());\n      return bucketPositionsDest.equals(tableBucketPositions) &&\n          sortPositionsDest.equals(tableSortPositionsOrder.getFirst()) &&\n          sortOrderDest.equals(tableSortPositionsOrder.getSecond());\n    }",
                "code_after_change": "    private boolean checkTable(Table table,\n        List<Integer> bucketPositionsDest,\n        List<Integer> sortPositionsDest,\n        List<Integer> sortOrderDest,\n        int numBucketsDest) {\n      // The bucketing and sorting positions should exactly match\n      int numBuckets = table.getNumBuckets();\n      if (numBucketsDest != numBuckets) {\n        return false;\n      }\n\n      List<Integer> tableBucketPositions =\n          getBucketPositions(table.getBucketCols(), table.getCols());\n      ObjectPair<List<Integer>, List<Integer>> tableSortPositionsOrder =\n          getSortPositionsOrder(table.getSortCols(), table.getCols());\n      return bucketPositionsDest.equals(tableBucketPositions) &&\n          sortPositionsDest.equals(tableSortPositionsOrder.getFirst()) &&\n          sortOrderDest.equals(tableSortPositionsOrder.getSecond());\n    }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the issue with the use of BucketizedHiveInputFormat and its interaction with ORC, which is related to the stack trace context but does not precisely identify the root cause in the ground truth methods. The report does not provide any fix suggestion, hence 'Missing' for fix suggestion. The problem location is partially identified as it mentions methods in the stack trace context but not the exact ground truth methods. There is no wrong information as the report accurately describes the problem context."
        }
    },
    {
        "filename": "HIVE-13546.json",
        "code_diff": {
            "ql.src.java.org.apache.hadoop.hive.ql.optimizer.Optimizer.initialize": {
                "code_before_change": "  public void initialize(HiveConf hiveConf) {\n\n    boolean isTezExecEngine = HiveConf.getVar(hiveConf, HiveConf.ConfVars.HIVE_EXECUTION_ENGINE).equals(\"tez\");\n    boolean isSparkExecEngine = HiveConf.getVar(hiveConf, HiveConf.ConfVars.HIVE_EXECUTION_ENGINE).equals(\"spark\");\n    boolean bucketMapJoinOptimizer = false;\n\n    transformations = new ArrayList<Transform>();\n\n    // Add the additional postprocessing transformations needed if\n    // we are translating Calcite operators into Hive operators.\n    transformations.add(new HiveOpConverterPostProc());\n\n    // Add the transformation that computes the lineage information.\n    Set<String> postExecHooks = Sets.newHashSet(\n      Splitter.on(\",\").trimResults().omitEmptyStrings().split(\n        Strings.nullToEmpty(HiveConf.getVar(hiveConf, HiveConf.ConfVars.POSTEXECHOOKS))));\n    if (postExecHooks.contains(\"org.apache.hadoop.hive.ql.hooks.PostExecutePrinter\")\n        || postExecHooks.contains(\"org.apache.hadoop.hive.ql.hooks.LineageLogger\")) {\n      transformations.add(new Generator());\n    }\n\n    // Try to transform OR predicates in Filter into simpler IN clauses first\n    if (HiveConf.getBoolVar(hiveConf, HiveConf.ConfVars.HIVEPOINTLOOKUPOPTIMIZER) &&\n            !pctx.getContext().isCboSucceeded()) {\n      final int min = HiveConf.getIntVar(hiveConf,\n          HiveConf.ConfVars.HIVEPOINTLOOKUPOPTIMIZERMIN);\n      transformations.add(new PointLookupOptimizer(min));\n    }\n\n    if (HiveConf.getBoolVar(hiveConf, HiveConf.ConfVars.HIVEPARTITIONCOLUMNSEPARATOR)) {\n        transformations.add(new PartitionColumnsSeparator());\n    }\n\n    if (HiveConf.getBoolVar(hiveConf, HiveConf.ConfVars.HIVEOPTPPD) &&\n            !pctx.getContext().isCboSucceeded()) {\n      transformations.add(new PredicateTransitivePropagate());\n      if (HiveConf.getBoolVar(hiveConf, HiveConf.ConfVars.HIVEOPTCONSTANTPROPAGATION)) {\n        transformations.add(new ConstantPropagate());\n      }\n      transformations.add(new SyntheticJoinPredicate());\n      transformations.add(new PredicatePushDown());\n    } else if (HiveConf.getBoolVar(hiveConf, HiveConf.ConfVars.HIVEOPTPPD) &&\n            pctx.getContext().isCboSucceeded()) {\n      transformations.add(new SyntheticJoinPredicate());\n      transformations.add(new SimplePredicatePushDown());\n      transformations.add(new RedundantDynamicPruningConditionsRemoval());\n    }\n\n    if (HiveConf.getBoolVar(hiveConf, HiveConf.ConfVars.HIVEOPTCONSTANTPROPAGATION) &&\n            !pctx.getContext().isCboSucceeded()) {    \n      // We run constant propagation twice because after predicate pushdown, filter expressions   \n      // are combined and may become eligible for reduction (like is not null filter).    \n      transformations.add(new ConstantPropagate());\n    }\n\n    if(HiveConf.getBoolVar(hiveConf, HiveConf.ConfVars.DYNAMICPARTITIONING) &&\n        HiveConf.getVar(hiveConf, HiveConf.ConfVars.DYNAMICPARTITIONINGMODE).equals(\"nonstrict\") &&\n        HiveConf.getBoolVar(hiveConf, HiveConf.ConfVars.HIVEOPTSORTDYNAMICPARTITION) &&\n        !HiveConf.getBoolVar(hiveConf, HiveConf.ConfVars.HIVEOPTLISTBUCKETING)) {\n      transformations.add(new SortedDynPartitionOptimizer());\n    }\n\n    if (HiveConf.getBoolVar(hiveConf, HiveConf.ConfVars.HIVEOPTPPD)) {\n      transformations.add(new PartitionPruner());\n      transformations.add(new PartitionConditionRemover());\n      if (HiveConf.getBoolVar(hiveConf, HiveConf.ConfVars.HIVEOPTLISTBUCKETING)) {\n        /* Add list bucketing pruner. */\n        transformations.add(new ListBucketingPruner());\n      }\n    }\n    if ((HiveConf.getBoolVar(hiveConf, HiveConf.ConfVars.HIVEOPTPPD)\n            && HiveConf.getBoolVar(hiveConf, HiveConf.ConfVars.HIVEOPTCONSTANTPROPAGATION)) ||\n            (HiveConf.getBoolVar(hiveConf, HiveConf.ConfVars.HIVEOPTCONSTANTPROPAGATION)\n                    && pctx.getContext().isCboSucceeded())) {\n      // PartitionPruner may create more folding opportunities, run ConstantPropagate again.\n      transformations.add(new ConstantPropagate());\n    }\n\n    if (HiveConf.getBoolVar(hiveConf, HiveConf.ConfVars.HIVEOPTGROUPBY) ||\n        HiveConf.getBoolVar(hiveConf, HiveConf.ConfVars.HIVE_MAP_GROUPBY_SORT)) {\n      transformations.add(new GroupByOptimizer());\n    }\n    transformations.add(new ColumnPruner());\n    if (HiveConf.getBoolVar(hiveConf, HiveConf.ConfVars.HIVE_OPTIMIZE_SKEWJOIN_COMPILETIME)) {\n      if (!isTezExecEngine) {\n        transformations.add(new SkewJoinOptimizer());\n      } else {\n        LOG.warn(\"Skew join is currently not supported in tez! Disabling the skew join optimization.\");\n      }\n    }\n    if (HiveConf.getBoolVar(hiveConf, HiveConf.ConfVars.HIVEOPTGBYUSINGINDEX)) {\n      transformations.add(new RewriteGBUsingIndex());\n    }\n    transformations.add(new SamplePruner());\n\n    MapJoinProcessor mapJoinProcessor = isSparkExecEngine ? new SparkMapJoinProcessor()\n      : new MapJoinProcessor();\n    transformations.add(mapJoinProcessor);\n\n    if ((HiveConf.getBoolVar(hiveConf, HiveConf.ConfVars.HIVEOPTBUCKETMAPJOIN))\n      && !isTezExecEngine && !isSparkExecEngine) {\n      transformations.add(new BucketMapJoinOptimizer());\n      bucketMapJoinOptimizer = true;\n    }\n\n    // If optimize hive.optimize.bucketmapjoin.sortedmerge is set, add both\n    // BucketMapJoinOptimizer and SortedMergeBucketMapJoinOptimizer\n    if ((HiveConf.getBoolVar(hiveConf, HiveConf.ConfVars.HIVEOPTSORTMERGEBUCKETMAPJOIN))\n        && !isTezExecEngine && !isSparkExecEngine) {\n      if (!bucketMapJoinOptimizer) {\n        // No need to add BucketMapJoinOptimizer twice\n        transformations.add(new BucketMapJoinOptimizer());\n      }\n      transformations.add(new SortedMergeBucketMapJoinOptimizer());\n    }\n\n    if (HiveConf.getBoolVar(hiveConf, HiveConf.ConfVars.HIVEOPTIMIZEBUCKETINGSORTING)) {\n      transformations.add(new BucketingSortingReduceSinkOptimizer());\n    }\n\n    transformations.add(new UnionProcessor());\n\n    if (HiveConf.getBoolVar(hiveConf, HiveConf.ConfVars.NWAYJOINREORDER)) {\n      transformations.add(new JoinReorder());\n    }\n\n    if (HiveConf.getBoolVar(hiveConf,\n        HiveConf.ConfVars.TEZ_OPTIMIZE_BUCKET_PRUNING)\n        && HiveConf.getBoolVar(hiveConf, HiveConf.ConfVars.HIVEOPTPPD)\n        && HiveConf.getBoolVar(hiveConf, HiveConf.ConfVars.HIVEOPTINDEXFILTER)) {\n      final boolean compatMode =\n          HiveConf.getBoolVar(hiveConf, HiveConf.ConfVars.TEZ_OPTIMIZE_BUCKET_PRUNING_COMPAT);\n      transformations.add(new FixedBucketPruningOptimizer(compatMode));\n    }\n\n    if(HiveConf.getBoolVar(hiveConf, HiveConf.ConfVars.HIVEOPTREDUCEDEDUPLICATION)) {\n      transformations.add(new ReduceSinkDeDuplication());\n    }\n    transformations.add(new NonBlockingOpDeDupProc());\n    if (HiveConf.getBoolVar(hiveConf, HiveConf.ConfVars.HIVEIDENTITYPROJECTREMOVER)\n        && !HiveConf.getBoolVar(hiveConf, HiveConf.ConfVars.HIVE_CBO_RETPATH_HIVEOP)) {\n      transformations.add(new IdentityProjectRemover());\n    }\n    if (HiveConf.getBoolVar(hiveConf, HiveConf.ConfVars.HIVELIMITOPTENABLE)) {\n      transformations.add(new GlobalLimitOptimizer());\n    }\n    if(HiveConf.getBoolVar(hiveConf, HiveConf.ConfVars.HIVEOPTCORRELATION) &&\n        !HiveConf.getBoolVar(hiveConf, HiveConf.ConfVars.HIVEGROUPBYSKEW) &&\n        !HiveConf.getBoolVar(hiveConf, HiveConf.ConfVars.HIVE_OPTIMIZE_SKEWJOIN_COMPILETIME) &&\n        !isTezExecEngine) {\n      transformations.add(new CorrelationOptimizer());\n    }\n    if (HiveConf.getFloatVar(hiveConf, HiveConf.ConfVars.HIVELIMITPUSHDOWNMEMORYUSAGE) > 0) {\n      transformations.add(new LimitPushdownOptimizer());\n    }\n    if(HiveConf.getBoolVar(hiveConf, HiveConf.ConfVars.HIVEOPTIMIZEMETADATAQUERIES)) {\n      transformations.add(new StatsOptimizer());\n    }\n    if (pctx.getContext().getExplain() && !isTezExecEngine && !isSparkExecEngine) {\n      transformations.add(new AnnotateWithStatistics());\n      transformations.add(new AnnotateWithOpTraits());\n    }\n\n    if (!HiveConf.getVar(hiveConf, HiveConf.ConfVars.HIVEFETCHTASKCONVERSION).equals(\"none\")) {\n      transformations.add(new SimpleFetchOptimizer()); // must be called last\n    }\n\n    if (HiveConf.getBoolVar(hiveConf, HiveConf.ConfVars.HIVEFETCHTASKAGGR)) {\n      transformations.add(new SimpleFetchAggregation());\n    }\n  }",
                "code_after_change": "  public void initialize(HiveConf hiveConf) {\n\n    boolean isTezExecEngine = HiveConf.getVar(hiveConf, HiveConf.ConfVars.HIVE_EXECUTION_ENGINE).equals(\"tez\");\n    boolean isSparkExecEngine = HiveConf.getVar(hiveConf, HiveConf.ConfVars.HIVE_EXECUTION_ENGINE).equals(\"spark\");\n    boolean bucketMapJoinOptimizer = false;\n\n    transformations = new ArrayList<Transform>();\n\n    // Add the additional postprocessing transformations needed if\n    // we are translating Calcite operators into Hive operators.\n    transformations.add(new HiveOpConverterPostProc());\n\n    // Add the transformation that computes the lineage information.\n    Set<String> postExecHooks = Sets.newHashSet(\n      Splitter.on(\",\").trimResults().omitEmptyStrings().split(\n        Strings.nullToEmpty(HiveConf.getVar(hiveConf, HiveConf.ConfVars.POSTEXECHOOKS))));\n    if (postExecHooks.contains(\"org.apache.hadoop.hive.ql.hooks.PostExecutePrinter\")\n        || postExecHooks.contains(\"org.apache.hadoop.hive.ql.hooks.LineageLogger\")) {\n      transformations.add(new Generator());\n    }\n\n    if (HiveConf.getBoolVar(hiveConf, HiveConf.ConfVars.HIVEOPTPPD)) {\n      transformations.add(new PredicateTransitivePropagate());\n      if (HiveConf.getBoolVar(hiveConf, HiveConf.ConfVars.HIVEOPTCONSTANTPROPAGATION)) {\n        transformations.add(new ConstantPropagate());\n      }\n      transformations.add(new SyntheticJoinPredicate());\n      transformations.add(new PredicatePushDown());\n    }\n\n    if (HiveConf.getBoolVar(hiveConf, HiveConf.ConfVars.HIVEOPTCONSTANTPROPAGATION)) {\n      // We run constant propagation twice because after predicate pushdown, filter expressions\n      // are combined and may become eligible for reduction (like is not null filter).\n        transformations.add(new ConstantPropagate());\n    }\n\n    if(HiveConf.getBoolVar(hiveConf, HiveConf.ConfVars.DYNAMICPARTITIONING) &&\n        HiveConf.getVar(hiveConf, HiveConf.ConfVars.DYNAMICPARTITIONINGMODE).equals(\"nonstrict\") &&\n        HiveConf.getBoolVar(hiveConf, HiveConf.ConfVars.HIVEOPTSORTDYNAMICPARTITION) &&\n        !HiveConf.getBoolVar(hiveConf, HiveConf.ConfVars.HIVEOPTLISTBUCKETING)) {\n      transformations.add(new SortedDynPartitionOptimizer());\n    }\n\n    if (HiveConf.getBoolVar(hiveConf, HiveConf.ConfVars.HIVEOPTPPD)) {\n      transformations.add(new PartitionPruner());\n      transformations.add(new PartitionConditionRemover());\n      if (HiveConf.getBoolVar(hiveConf, HiveConf.ConfVars.HIVEOPTLISTBUCKETING)) {\n        /* Add list bucketing pruner. */\n        transformations.add(new ListBucketingPruner());\n      }\n    }\n\n    if (HiveConf.getBoolVar(hiveConf, HiveConf.ConfVars.HIVEOPTGROUPBY) ||\n        HiveConf.getBoolVar(hiveConf, HiveConf.ConfVars.HIVE_MAP_GROUPBY_SORT)) {\n      transformations.add(new GroupByOptimizer());\n    }\n    transformations.add(new ColumnPruner());\n    if (HiveConf.getBoolVar(hiveConf, HiveConf.ConfVars.HIVE_OPTIMIZE_SKEWJOIN_COMPILETIME)) {\n      if (!isTezExecEngine) {\n        transformations.add(new SkewJoinOptimizer());\n      } else {\n        LOG.warn(\"Skew join is currently not supported in tez! Disabling the skew join optimization.\");\n      }\n    }\n    if (HiveConf.getBoolVar(hiveConf, HiveConf.ConfVars.HIVEOPTGBYUSINGINDEX)) {\n      transformations.add(new RewriteGBUsingIndex());\n    }\n    transformations.add(new SamplePruner());\n\n    MapJoinProcessor mapJoinProcessor = isSparkExecEngine ? new SparkMapJoinProcessor()\n      : new MapJoinProcessor();\n    transformations.add(mapJoinProcessor);\n\n    if ((HiveConf.getBoolVar(hiveConf, HiveConf.ConfVars.HIVEOPTBUCKETMAPJOIN))\n      && !isTezExecEngine && !isSparkExecEngine) {\n      transformations.add(new BucketMapJoinOptimizer());\n      bucketMapJoinOptimizer = true;\n    }\n\n    // If optimize hive.optimize.bucketmapjoin.sortedmerge is set, add both\n    // BucketMapJoinOptimizer and SortedMergeBucketMapJoinOptimizer\n    if ((HiveConf.getBoolVar(hiveConf, HiveConf.ConfVars.HIVEOPTSORTMERGEBUCKETMAPJOIN))\n        && !isTezExecEngine && !isSparkExecEngine) {\n      if (!bucketMapJoinOptimizer) {\n        // No need to add BucketMapJoinOptimizer twice\n        transformations.add(new BucketMapJoinOptimizer());\n      }\n      transformations.add(new SortedMergeBucketMapJoinOptimizer());\n    }\n\n    if (HiveConf.getBoolVar(hiveConf, HiveConf.ConfVars.HIVEOPTIMIZEBUCKETINGSORTING)) {\n      transformations.add(new BucketingSortingReduceSinkOptimizer());\n    }\n\n    transformations.add(new UnionProcessor());\n\n    if (HiveConf.getBoolVar(hiveConf, HiveConf.ConfVars.NWAYJOINREORDER)) {\n      transformations.add(new JoinReorder());\n    }\n\n    if(HiveConf.getBoolVar(hiveConf, HiveConf.ConfVars.HIVEOPTREDUCEDEDUPLICATION)) {\n      transformations.add(new ReduceSinkDeDuplication());\n    }\n    transformations.add(new NonBlockingOpDeDupProc());\n    if (HiveConf.getBoolVar(hiveConf, HiveConf.ConfVars.HIVEIDENTITYPROJECTREMOVER)\n        && !HiveConf.getBoolVar(hiveConf, HiveConf.ConfVars.HIVE_CBO_RETPATH_HIVEOP)) {\n      transformations.add(new IdentityProjectRemover());\n    }\n    if (HiveConf.getBoolVar(hiveConf, HiveConf.ConfVars.HIVELIMITOPTENABLE)) {\n      transformations.add(new GlobalLimitOptimizer());\n    }\n    if(HiveConf.getBoolVar(hiveConf, HiveConf.ConfVars.HIVEOPTCORRELATION) &&\n        !HiveConf.getBoolVar(hiveConf, HiveConf.ConfVars.HIVEGROUPBYSKEW) &&\n        !HiveConf.getBoolVar(hiveConf, HiveConf.ConfVars.HIVE_OPTIMIZE_SKEWJOIN_COMPILETIME) &&\n        !isTezExecEngine) {\n      transformations.add(new CorrelationOptimizer());\n    }\n    if (HiveConf.getFloatVar(hiveConf, HiveConf.ConfVars.HIVELIMITPUSHDOWNMEMORYUSAGE) > 0) {\n      transformations.add(new LimitPushdownOptimizer());\n    }\n    if(HiveConf.getBoolVar(hiveConf, HiveConf.ConfVars.HIVEOPTIMIZEMETADATAQUERIES)) {\n      transformations.add(new StatsOptimizer());\n    }\n    if (pctx.getContext().getExplain() && !isTezExecEngine && !isSparkExecEngine) {\n      transformations.add(new AnnotateWithStatistics());\n      transformations.add(new AnnotateWithOpTraits());\n    }\n\n    if (!HiveConf.getVar(hiveConf, HiveConf.ConfVars.HIVEFETCHTASKCONVERSION).equals(\"none\")) {\n      transformations.add(new SimpleFetchOptimizer()); // must be called last\n    }\n\n    if (HiveConf.getBoolVar(hiveConf, HiveConf.ConfVars.HIVEFETCHTASKAGGR)) {\n      transformations.add(new SimpleFetchAggregation());\n    }\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Missing",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report mentions a Hive Runtime Error and provides a stack trace that includes methods like 'ExecReducer.reduce' and 'FileUtils.makePartName', which are in the shared stack trace context with the ground truth method 'Optimizer.initialize'. However, it does not precisely identify the root cause in the 'Optimizer.initialize' method. There is no fix suggestion provided in the bug report, as it lacks any 'Suggestions' or 'possible_fix' fields, nor does the description suggest a fix. The problem location is not precisely identified as the report does not mention the ground truth method or any related methods outside of the stack trace. There is no wrong information in the report; it accurately describes the error and the context in which it occurs."
        }
    },
    {
        "filename": "HIVE-7049.json",
        "code_diff": {
            "serde.src.java.org.apache.hadoop.hive.serde2.avro.AvroDeserializer.workerBase": {
                "code_before_change": "  private List<Object> workerBase(List<Object> objectRow, Schema fileSchema, List<String> columnNames,\n                                  List<TypeInfo> columnTypes, GenericRecord record)\n          throws AvroSerdeException {\n    for(int i = 0; i < columnNames.size(); i++) {\n      TypeInfo columnType = columnTypes.get(i);\n      String columnName = columnNames.get(i);\n      Object datum = record.get(columnName);\n      Schema datumSchema = record.getSchema().getField(columnName).schema();\n      Schema.Field field = fileSchema.getField(columnName);\n      objectRow.add(worker(datum, field == null ? null : field.schema(), datumSchema, columnType));\n    }\n\n    return objectRow;\n  }",
                "code_after_change": "  private List<Object> workerBase(List<Object> objectRow, Schema fileSchema, List<String> columnNames,\n                                  List<TypeInfo> columnTypes, GenericRecord record)\n          throws AvroSerdeException {\n    for(int i = 0; i < columnNames.size(); i++) {\n      TypeInfo columnType = columnTypes.get(i);\n      String columnName = columnNames.get(i);\n      Object datum = record.get(columnName);\n      Schema datumSchema = record.getSchema().getField(columnName).schema();\n      Schema.Field field = AvroSerdeUtils.isNullableType(fileSchema)?AvroSerdeUtils.getOtherTypeFromNullableType(fileSchema).getField(columnName):fileSchema.getField(columnName);\n      objectRow.add(worker(datum, field == null ? null : field.schema(), datumSchema, columnType));\n    }\n\n    return objectRow;\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Preventive",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the issue with nullable types in schemas, which is related to the ground truth method 'workerBase' as it deals with schema handling. However, it specifically mentions 'deserializeNullableUnion', which is in the same stack trace context but not the exact method where the fix was applied. The fix suggestion to check the file schema for nullability is preventive, as it aligns with the developer's fix of checking the file schema's nullability. The problem location is partially identified as it mentions 'deserializeNullableUnion', which is in the same stack trace context as 'workerBase'. There is no wrong information in the bug report as it correctly describes the issue and its context."
        }
    },
    {
        "filename": "HIVE-9755.json",
        "code_diff": {
            "ql.src.java.org.apache.hadoop.hive.ql.udf.generic.GenericUDAFnGrams.merge": {
                "code_before_change": "    public void merge(AggregationBuffer agg, Object partial) throws HiveException {\n      if(partial == null) {\n        return;\n      }\n      NGramAggBuf myagg = (NGramAggBuf) agg;\n      List<Text> partialNGrams = (List<Text>) loi.getList(partial);\n      int n = Integer.parseInt(partialNGrams.get(partialNGrams.size()-1).toString());\n      if(myagg.n > 0 && myagg.n != n) {\n        throw new HiveException(getClass().getSimpleName() + \": mismatch in value for 'n'\"\n            + \", which usually is caused by a non-constant expression. Found '\"+n+\"' and '\"\n            + myagg.n + \"'.\");\n      }\n      myagg.n = n;\n      partialNGrams.remove(partialNGrams.size()-1);\n      myagg.nge.merge(partialNGrams);\n    }",
                "code_after_change": "    public void merge(AggregationBuffer agg, Object partial) throws HiveException {\n      if(partial == null) {\n        return;\n      }\n      NGramAggBuf myagg = (NGramAggBuf) agg;\n      List<Text> partialNGrams = (List<Text>) loi.getList(partial);\n      int n = Integer.parseInt(partialNGrams.get(partialNGrams.size()-1).toString());\n\n      // A value of 0 for n indicates that the mapper processed data that does not meet\n      // filter criteria, so merge() should be NO-OP.\n      if (n == 0) {\n        return;\n      }\n\n      if(myagg.n > 0 && myagg.n != n) {\n        throw new HiveException(getClass().getSimpleName() + \": mismatch in value for 'n'\"\n            + \", which usually is caused by a non-constant expression. Found '\"+n+\"' and '\"\n            + myagg.n + \"'.\");\n      }\n      myagg.n = n;\n      partialNGrams.remove(partialNGrams.size()-1);\n      myagg.nge.merge(partialNGrams);\n    }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Missing",
                "sub_category": ""
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report mentions a HiveException related to the 'GenericUDAFnGramEvaluator' in the stack trace, which is part of the ground truth method's context, but it does not precisely identify the root cause in the 'merge' method. The report does not provide any fix suggestion, hence 'Missing' for fix suggestion. The problem location is not precisely identified as the report does not mention the 'merge' method directly, nor does it provide any related method in the 'problem_location' field, hence 'Missing'. There is no wrong information in the bug report as all the details are relevant to the context of the bug."
        }
    },
    {
        "filename": "HIVE-19130.json",
        "code_diff": {
            "ql.src.java.org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer.createReplImportTasks": {
                "code_before_change": "  private static void createReplImportTasks(\n      ImportTableDesc tblDesc,\n      List<AddPartitionDesc> partitionDescs,\n      ReplicationSpec replicationSpec, boolean waitOnPrecursor,\n      Table table, URI fromURI, FileSystem fs, Warehouse wh,\n      EximUtil.SemanticAnalyzerWrapperContext x, Long writeId, int stmtId, boolean isSourceMm,\n      UpdatedMetaDataTracker updatedMetadata)\n      throws HiveException, URISyntaxException, IOException, MetaException {\n\n    Task<?> dr = null;\n    WriteEntity.WriteType lockType = WriteEntity.WriteType.DDL_NO_LOCK;\n\n    // Normally, on import, trying to create a table or a partition in a db that does not yet exist\n    // is a error condition. However, in the case of a REPL LOAD, it is possible that we are trying\n    // to create tasks to create a table inside a db that as-of-now does not exist, but there is\n    // a precursor Task waiting that will create it before this is encountered. Thus, we instantiate\n    // defaults and do not error out in that case.\n    Database parentDb = x.getHive().getDatabase(tblDesc.getDatabaseName());\n    if (parentDb == null){\n      if (!waitOnPrecursor){\n        throw new SemanticException(ErrorMsg.DATABASE_NOT_EXISTS.getMsg(tblDesc.getDatabaseName()));\n      }\n    }\n\n    if (table != null) {\n      if (!replicationSpec.allowReplacementInto(table.getParameters())) {\n        // If the target table exists and is newer or same as current update based on repl.last.id, then just noop it.\n        x.getLOG().info(\"Table {}.{} is not replaced as it is newer than the update\",\n                tblDesc.getDatabaseName(), tblDesc.getTableName());\n        return;\n      }\n    } else {\n      // If table doesn't exist, allow creating a new one only if the database state is older than the update.\n      if ((parentDb != null) && (!replicationSpec.allowReplacementInto(parentDb.getParameters()))) {\n        // If the target table exists and is newer or same as current update based on repl.last.id, then just noop it.\n        x.getLOG().info(\"Table {}.{} is not created as the database is newer than the update\",\n                tblDesc.getDatabaseName(), tblDesc.getTableName());\n        return;\n      }\n    }\n\n    if (updatedMetadata != null) {\n      updatedMetadata.set(replicationSpec.getReplicationState(),\n                          tblDesc.getDatabaseName(),\n                          tblDesc.getTableName(),\n                          null);\n    }\n\n    if (tblDesc.getLocation() == null) {\n      if (!waitOnPrecursor){\n        tblDesc.setLocation(wh.getDefaultTablePath(parentDb, tblDesc.getTableName()).toString());\n      } else {\n        tblDesc.setLocation(\n            wh.getDnsPath(new Path(\n                wh.getDefaultDatabasePath(tblDesc.getDatabaseName()),\n                org.apache.hadoop.hive.metastore.utils.MetaStoreUtils.encodeTableName(tblDesc.getTableName().toLowerCase())\n            )\n        ).toString());\n\n      }\n    }\n\n    /* Note: In the following section, Metadata-only import handling logic is\n       interleaved with regular repl-import logic. The rule of thumb being\n       followed here is that MD-only imports are essentially ALTERs. They do\n       not load data, and should not be \"creating\" any metadata - they should\n       be replacing instead. The only place it makes sense for a MD-only import\n       to create is in the case of a table that's been dropped and recreated,\n       or in the case of an unpartitioned table. In all other cases, it should\n       behave like a noop or a pure MD alter.\n    */\n    if (table == null) {\n      if (lockType == WriteEntity.WriteType.DDL_NO_LOCK){\n        lockType = WriteEntity.WriteType.DDL_SHARED;\n      }\n\n      Task t = createTableTask(tblDesc, x);\n      table = new Table(tblDesc.getDatabaseName(), tblDesc.getTableName());\n\n      if (!replicationSpec.isMetadataOnly()) {\n        if (isPartitioned(tblDesc)) {\n          Task<?> ict = createImportCommitTask(x.getConf(),\n              tblDesc.getDatabaseName(), tblDesc.getTableName(), writeId, stmtId,\n              AcidUtils.isInsertOnlyTable(tblDesc.getTblProps()));\n          for (AddPartitionDesc addPartitionDesc : partitionDescs) {\n            addPartitionDesc.setReplicationSpec(replicationSpec);\n            t.addDependentTask(\n                addSinglePartition(fromURI, fs, tblDesc, table, wh, addPartitionDesc, replicationSpec, x, writeId, stmtId, isSourceMm, ict));\n            if (updatedMetadata != null) {\n              updatedMetadata.addPartition(addPartitionDesc.getPartition(0).getPartSpec());\n            }\n          }\n        } else {\n          x.getLOG().debug(\"adding dependent CopyWork/MoveWork for table\");\n          t.addDependentTask(loadTable(fromURI, table, true, new Path(tblDesc.getLocation()), replicationSpec, x, writeId, stmtId, isSourceMm));\n        }\n      }\n      // Simply create\n      x.getTasks().add(t);\n    } else {\n      // Table existed, and is okay to replicate into, not dropping and re-creating.\n      if (table.isPartitioned()) {\n        x.getLOG().debug(\"table partitioned\");\n        for (AddPartitionDesc addPartitionDesc : partitionDescs) {\n          addPartitionDesc.setReplicationSpec(replicationSpec);\n          Map<String, String> partSpec = addPartitionDesc.getPartition(0).getPartSpec();\n          org.apache.hadoop.hive.ql.metadata.Partition ptn = null;\n          Task<?> ict = replicationSpec.isMetadataOnly() ? null : createImportCommitTask(\n                  x.getConf(), tblDesc.getDatabaseName(), tblDesc.getTableName(), writeId, stmtId,\n                  AcidUtils.isInsertOnlyTable(tblDesc.getTblProps()));\n          if ((ptn = x.getHive().getPartition(table, partSpec, false)) == null) {\n            if (!replicationSpec.isMetadataOnly()){\n              x.getTasks().add(addSinglePartition(\n                  fromURI, fs, tblDesc, table, wh, addPartitionDesc, replicationSpec, x, writeId, stmtId, isSourceMm, ict));\n              if (updatedMetadata != null) {\n                updatedMetadata.addPartition(addPartitionDesc.getPartition(0).getPartSpec());\n              }\n            } else {\n              x.getTasks().add(alterSinglePartition(\n                      fromURI, fs, tblDesc, table, wh, addPartitionDesc, replicationSpec, null, x));\n              if (updatedMetadata != null) {\n                updatedMetadata.addPartition(addPartitionDesc.getPartition(0).getPartSpec());\n              }\n            }\n          } else {\n            // If replicating, then the partition already existing means we need to replace, maybe, if\n            // the destination ptn's repl.last.id is older than the replacement's.\n            if (replicationSpec.allowReplacementInto(ptn.getParameters())){\n              if (!replicationSpec.isMetadataOnly()){\n                x.getTasks().add(addSinglePartition(\n                    fromURI, fs, tblDesc, table, wh, addPartitionDesc, replicationSpec, x, writeId, stmtId, isSourceMm, ict));\n              } else {\n                x.getTasks().add(alterSinglePartition(\n                    fromURI, fs, tblDesc, table, wh, addPartitionDesc, replicationSpec, ptn, x));\n              }\n              if (updatedMetadata != null) {\n                updatedMetadata.addPartition(addPartitionDesc.getPartition(0).getPartSpec());\n              }\n              if (lockType == WriteEntity.WriteType.DDL_NO_LOCK){\n                lockType = WriteEntity.WriteType.DDL_SHARED;\n              }\n            }\n          }\n        }\n        if (replicationSpec.isMetadataOnly() && partitionDescs.isEmpty()){\n          // MD-ONLY table alter\n          x.getTasks().add(alterTableTask(tblDesc, x,replicationSpec));\n          if (lockType == WriteEntity.WriteType.DDL_NO_LOCK){\n            lockType = WriteEntity.WriteType.DDL_SHARED;\n          }\n        }\n      } else {\n        x.getLOG().debug(\"table non-partitioned\");\n        if (!replicationSpec.isMetadataOnly()) {\n          // repl-imports are replace-into unless the event is insert-into\n          loadTable(fromURI, table, replicationSpec.isReplace(), new Path(fromURI),\n            replicationSpec, x, writeId, stmtId, isSourceMm);\n        } else {\n          x.getTasks().add(alterTableTask(tblDesc, x, replicationSpec));\n        }\n        if (lockType == WriteEntity.WriteType.DDL_NO_LOCK){\n          lockType = WriteEntity.WriteType.DDL_SHARED;\n        }\n      }\n    }\n    x.getOutputs().add(new WriteEntity(table,lockType));\n  }",
                "code_after_change": "  private static void createReplImportTasks(\n      ImportTableDesc tblDesc,\n      List<AddPartitionDesc> partitionDescs,\n      ReplicationSpec replicationSpec, boolean waitOnPrecursor,\n      Table table, URI fromURI, FileSystem fs, Warehouse wh,\n      EximUtil.SemanticAnalyzerWrapperContext x, Long writeId, int stmtId, boolean isSourceMm,\n      UpdatedMetaDataTracker updatedMetadata)\n      throws HiveException, URISyntaxException, IOException, MetaException {\n\n    Task<?> dropTblTask = null;\n    WriteEntity.WriteType lockType = WriteEntity.WriteType.DDL_NO_LOCK;\n\n    // Normally, on import, trying to create a table or a partition in a db that does not yet exist\n    // is a error condition. However, in the case of a REPL LOAD, it is possible that we are trying\n    // to create tasks to create a table inside a db that as-of-now does not exist, but there is\n    // a precursor Task waiting that will create it before this is encountered. Thus, we instantiate\n    // defaults and do not error out in that case.\n    Database parentDb = x.getHive().getDatabase(tblDesc.getDatabaseName());\n    if (parentDb == null){\n      if (!waitOnPrecursor){\n        throw new SemanticException(ErrorMsg.DATABASE_NOT_EXISTS.getMsg(tblDesc.getDatabaseName()));\n      }\n    }\n\n    if (table != null) {\n      if (!replicationSpec.allowReplacementInto(table.getParameters())) {\n        // If the target table exists and is newer or same as current update based on repl.last.id, then just noop it.\n        x.getLOG().info(\"Table {}.{} is not replaced as it is newer than the update\",\n                tblDesc.getDatabaseName(), tblDesc.getTableName());\n        return;\n      }\n\n      // If the table exists and we found a valid create table event, then need to drop the table first\n      // and then create it. This case is possible if the event sequence is drop_table(t1) -> create_table(t1).\n      // We need to drop here to handle the case where the previous incremental load created the table but\n      // didn't set the last repl ID due to some failure.\n      if (x.getEventType() == DumpType.EVENT_CREATE_TABLE) {\n        dropTblTask = dropTableTask(table, x, replicationSpec);\n        table = null;\n      }\n    } else {\n      // If table doesn't exist, allow creating a new one only if the database state is older than the update.\n      if ((parentDb != null) && (!replicationSpec.allowReplacementInto(parentDb.getParameters()))) {\n        // If the target table exists and is newer or same as current update based on repl.last.id, then just noop it.\n        x.getLOG().info(\"Table {}.{} is not created as the database is newer than the update\",\n                tblDesc.getDatabaseName(), tblDesc.getTableName());\n        return;\n      }\n    }\n\n    if (updatedMetadata != null) {\n      updatedMetadata.set(replicationSpec.getReplicationState(),\n                          tblDesc.getDatabaseName(),\n                          tblDesc.getTableName(),\n                          null);\n    }\n\n    if (tblDesc.getLocation() == null) {\n      if (!waitOnPrecursor){\n        tblDesc.setLocation(wh.getDefaultTablePath(parentDb, tblDesc.getTableName()).toString());\n      } else {\n        tblDesc.setLocation(\n            wh.getDnsPath(new Path(\n                wh.getDefaultDatabasePath(tblDesc.getDatabaseName()),\n                org.apache.hadoop.hive.metastore.utils.MetaStoreUtils.encodeTableName(tblDesc.getTableName().toLowerCase())\n            )\n        ).toString());\n\n      }\n    }\n\n    /* Note: In the following section, Metadata-only import handling logic is\n       interleaved with regular repl-import logic. The rule of thumb being\n       followed here is that MD-only imports are essentially ALTERs. They do\n       not load data, and should not be \"creating\" any metadata - they should\n       be replacing instead. The only place it makes sense for a MD-only import\n       to create is in the case of a table that's been dropped and recreated,\n       or in the case of an unpartitioned table. In all other cases, it should\n       behave like a noop or a pure MD alter.\n    */\n    if (table == null) {\n      if (lockType == WriteEntity.WriteType.DDL_NO_LOCK){\n        lockType = WriteEntity.WriteType.DDL_SHARED;\n      }\n\n      Task t = createTableTask(tblDesc, x);\n      table = new Table(tblDesc.getDatabaseName(), tblDesc.getTableName());\n\n      if (!replicationSpec.isMetadataOnly()) {\n        if (isPartitioned(tblDesc)) {\n          Task<?> ict = createImportCommitTask(x.getConf(),\n              tblDesc.getDatabaseName(), tblDesc.getTableName(), writeId, stmtId,\n              AcidUtils.isInsertOnlyTable(tblDesc.getTblProps()));\n          for (AddPartitionDesc addPartitionDesc : partitionDescs) {\n            addPartitionDesc.setReplicationSpec(replicationSpec);\n            t.addDependentTask(\n                addSinglePartition(fromURI, fs, tblDesc, table, wh, addPartitionDesc, replicationSpec, x, writeId, stmtId, isSourceMm, ict));\n            if (updatedMetadata != null) {\n              updatedMetadata.addPartition(addPartitionDesc.getPartition(0).getPartSpec());\n            }\n          }\n        } else {\n          x.getLOG().debug(\"adding dependent CopyWork/MoveWork for table\");\n          t.addDependentTask(loadTable(fromURI, table, true, new Path(tblDesc.getLocation()), replicationSpec, x, writeId, stmtId, isSourceMm));\n        }\n      }\n\n      if (dropTblTask != null) {\n        // Drop first and then create\n        dropTblTask.addDependentTask(t);\n        x.getTasks().add(dropTblTask);\n      } else {\n        // Simply create\n        x.getTasks().add(t);\n      }\n    } else {\n      // Table existed, and is okay to replicate into, not dropping and re-creating.\n      if (table.isPartitioned()) {\n        x.getLOG().debug(\"table partitioned\");\n        for (AddPartitionDesc addPartitionDesc : partitionDescs) {\n          addPartitionDesc.setReplicationSpec(replicationSpec);\n          Map<String, String> partSpec = addPartitionDesc.getPartition(0).getPartSpec();\n          org.apache.hadoop.hive.ql.metadata.Partition ptn = null;\n          Task<?> ict = replicationSpec.isMetadataOnly() ? null : createImportCommitTask(\n                  x.getConf(), tblDesc.getDatabaseName(), tblDesc.getTableName(), writeId, stmtId,\n                  AcidUtils.isInsertOnlyTable(tblDesc.getTblProps()));\n          if ((ptn = x.getHive().getPartition(table, partSpec, false)) == null) {\n            if (!replicationSpec.isMetadataOnly()){\n              x.getTasks().add(addSinglePartition(\n                  fromURI, fs, tblDesc, table, wh, addPartitionDesc, replicationSpec, x, writeId, stmtId, isSourceMm, ict));\n              if (updatedMetadata != null) {\n                updatedMetadata.addPartition(addPartitionDesc.getPartition(0).getPartSpec());\n              }\n            } else {\n              x.getTasks().add(alterSinglePartition(\n                      fromURI, fs, tblDesc, table, wh, addPartitionDesc, replicationSpec, null, x));\n              if (updatedMetadata != null) {\n                updatedMetadata.addPartition(addPartitionDesc.getPartition(0).getPartSpec());\n              }\n            }\n          } else {\n            // If replicating, then the partition already existing means we need to replace, maybe, if\n            // the destination ptn's repl.last.id is older than the replacement's.\n            if (replicationSpec.allowReplacementInto(ptn.getParameters())){\n              if (!replicationSpec.isMetadataOnly()){\n                x.getTasks().add(addSinglePartition(\n                    fromURI, fs, tblDesc, table, wh, addPartitionDesc, replicationSpec, x, writeId, stmtId, isSourceMm, ict));\n              } else {\n                x.getTasks().add(alterSinglePartition(\n                    fromURI, fs, tblDesc, table, wh, addPartitionDesc, replicationSpec, ptn, x));\n              }\n              if (updatedMetadata != null) {\n                updatedMetadata.addPartition(addPartitionDesc.getPartition(0).getPartSpec());\n              }\n              if (lockType == WriteEntity.WriteType.DDL_NO_LOCK){\n                lockType = WriteEntity.WriteType.DDL_SHARED;\n              }\n            }\n          }\n        }\n        if (replicationSpec.isMetadataOnly() && partitionDescs.isEmpty()){\n          // MD-ONLY table alter\n          x.getTasks().add(alterTableTask(tblDesc, x,replicationSpec));\n          if (lockType == WriteEntity.WriteType.DDL_NO_LOCK){\n            lockType = WriteEntity.WriteType.DDL_SHARED;\n          }\n        }\n      } else {\n        x.getLOG().debug(\"table non-partitioned\");\n        if (!replicationSpec.isMetadataOnly()) {\n          // repl-imports are replace-into unless the event is insert-into\n          loadTable(fromURI, table, replicationSpec.isReplace(), new Path(fromURI),\n            replicationSpec, x, writeId, stmtId, isSourceMm);\n        } else {\n          x.getTasks().add(alterTableTask(tblDesc, x, replicationSpec));\n        }\n        if (lockType == WriteEntity.WriteType.DDL_NO_LOCK){\n          lockType = WriteEntity.WriteType.DDL_SHARED;\n        }\n      }\n    }\n    x.getOutputs().add(new WriteEntity(table,lockType));\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.parse.EximUtil.getCtx": {
                "code_before_change": "    public Context getCtx() {\n      return ctx;\n    }",
                "code_after_change": "    public Context getCtx() {\n      return ctx;\n    }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.dropPartitions": {
                "code_before_change": "  private void dropPartitions(Hive db, Table tbl, DropTableDesc dropTbl) throws HiveException {\n\n    ReplicationSpec replicationSpec = dropTbl.getReplicationSpec();\n    if (replicationSpec.isInReplicationScope()){\n      /**\n       * ALTER TABLE DROP PARTITION ... FOR REPLICATION(x) behaves as a DROP PARTITION IF OLDER THAN x\n       *\n       * So, we check each partition that matches our DropTableDesc.getPartSpecs(), and drop it only\n       * if it's older than the event that spawned this replicated request to drop partition\n       */\n      // TODO: Current implementation of replication will result in DROP_PARTITION under replication\n      // scope being called per-partition instead of multiple partitions. However, to be robust, we\n      // must still handle the case of multiple partitions in case this assumption changes in the\n      // future. However, if this assumption changes, we will not be very performant if we fetch\n      // each partition one-by-one, and then decide on inspection whether or not this is a candidate\n      // for dropping. Thus, we need a way to push this filter (replicationSpec.allowEventReplacementInto)\n      // to the  metastore to allow it to do drop a partition or not, depending on a Predicate on the\n      // parameter key values.\n      for (DropTableDesc.PartSpec partSpec : dropTbl.getPartSpecs()){\n        List<Partition> partitions = new ArrayList<>();\n        try {\n          db.getPartitionsByExpr(tbl, partSpec.getPartSpec(), conf, partitions);\n          for (Partition p : Iterables.filter(partitions,\n              replicationSpec.allowEventReplacementInto())){\n            db.dropPartition(tbl.getDbName(),tbl.getTableName(),p.getValues(),true);\n          }\n        } catch (NoSuchObjectException e){\n          // ignore NSOE because that means there's nothing to drop.\n        } catch (Exception e) {\n          throw new HiveException(e.getMessage(), e);\n        }\n      }\n      return;\n    }\n\n    // ifExists is currently verified in DDLSemanticAnalyzer\n    List<Partition> droppedParts\n        = db.dropPartitions(dropTbl.getTableName(),\n                            dropTbl.getPartSpecs(),\n                            PartitionDropOptions.instance()\n                                                .deleteData(true)\n                                                .ifExists(true)\n                                                .purgeData(dropTbl.getIfPurge()));\n    for (Partition partition : droppedParts) {\n      console.printInfo(\"Dropped the partition \" + partition.getName());\n      // We have already locked the table, don't lock the partitions.\n      addIfAbsentByName(new WriteEntity(partition, WriteEntity.WriteType.DDL_NO_LOCK));\n    };\n  }",
                "code_after_change": "  private void dropPartitions(Hive db, Table tbl, DropTableDesc dropTbl) throws HiveException {\n\n    ReplicationSpec replicationSpec = dropTbl.getReplicationSpec();\n    if (replicationSpec.isInReplicationScope()){\n      /**\n       * ALTER TABLE DROP PARTITION ... FOR REPLICATION(x) behaves as a DROP PARTITION IF OLDER THAN x\n       *\n       * So, we check each partition that matches our DropTableDesc.getPartSpecs(), and drop it only\n       * if it's older than the event that spawned this replicated request to drop partition\n       */\n      // TODO: Current implementation of replication will result in DROP_PARTITION under replication\n      // scope being called per-partition instead of multiple partitions. However, to be robust, we\n      // must still handle the case of multiple partitions in case this assumption changes in the\n      // future. However, if this assumption changes, we will not be very performant if we fetch\n      // each partition one-by-one, and then decide on inspection whether or not this is a candidate\n      // for dropping. Thus, we need a way to push this filter (replicationSpec.allowEventReplacementInto)\n      // to the  metastore to allow it to do drop a partition or not, depending on a Predicate on the\n      // parameter key values.\n\n      if (tbl == null) {\n        // If table is missing, then partitions are also would've been dropped. Just no-op.\n        return;\n      }\n\n      for (DropTableDesc.PartSpec partSpec : dropTbl.getPartSpecs()){\n        List<Partition> partitions = new ArrayList<>();\n        try {\n          db.getPartitionsByExpr(tbl, partSpec.getPartSpec(), conf, partitions);\n          for (Partition p : Iterables.filter(partitions,\n              replicationSpec.allowEventReplacementInto())){\n            db.dropPartition(tbl.getDbName(),tbl.getTableName(),p.getValues(),true);\n          }\n        } catch (NoSuchObjectException e){\n          // ignore NSOE because that means there's nothing to drop.\n        } catch (Exception e) {\n          throw new HiveException(e.getMessage(), e);\n        }\n      }\n      return;\n    }\n\n    // ifExists is currently verified in DDLSemanticAnalyzer\n    List<Partition> droppedParts\n        = db.dropPartitions(dropTbl.getTableName(),\n                            dropTbl.getPartSpecs(),\n                            PartitionDropOptions.instance()\n                                                .deleteData(true)\n                                                .ifExists(true)\n                                                .purgeData(dropTbl.getIfPurge()));\n    for (Partition partition : droppedParts) {\n      console.printInfo(\"Dropped the partition \" + partition.getName());\n      // We have already locked the table, don't lock the partitions.\n      addIfAbsentByName(new WriteEntity(partition, WriteEntity.WriteType.DDL_NO_LOCK));\n    }\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.parse.repl.load.message.TableHandler.handle": {
                "code_before_change": "  public List<Task<? extends Serializable>> handle(Context context) throws SemanticException {\n    try {\n      List<Task<? extends Serializable>> importTasks = new ArrayList<>();\n\n      EximUtil.SemanticAnalyzerWrapperContext x =\n          new EximUtil.SemanticAnalyzerWrapperContext(\n              context.hiveConf, context.db, readEntitySet, writeEntitySet, importTasks, context.log,\n              context.nestedContext);\n\n      // REPL LOAD is not partition level. It is always DB or table level. So, passing null for partition specs.\n      // Also, REPL LOAD doesn't support external table and hence no location set as well.\n      ImportSemanticAnalyzer.prepareImport(false, false, false, false,\n          (context.precursor != null), null, context.tableName, context.dbName,\n          null, context.location, x, updatedMetadata);\n\n      return importTasks;\n    } catch (Exception e) {\n      throw new SemanticException(e);\n    }\n  }",
                "code_after_change": "  public List<Task<? extends Serializable>> handle(Context context) throws SemanticException {\n    try {\n      List<Task<? extends Serializable>> importTasks = new ArrayList<>();\n\n      EximUtil.SemanticAnalyzerWrapperContext x =\n          new EximUtil.SemanticAnalyzerWrapperContext(\n              context.hiveConf, context.db, readEntitySet, writeEntitySet, importTasks, context.log,\n              context.nestedContext);\n      x.setEventType(context.dmd.getDumpType());\n\n      // REPL LOAD is not partition level. It is always DB or table level. So, passing null for partition specs.\n      // Also, REPL LOAD doesn't support external table and hence no location set as well.\n      ImportSemanticAnalyzer.prepareImport(false, false, false, false,\n          (context.precursor != null), null, context.tableName, context.dbName,\n          null, context.location, x, updatedMetadata);\n\n      return importTasks;\n    } catch (Exception e) {\n      throw new SemanticException(e);\n    }\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.parse.ImportSemanticAnalyzer.createTableTask": {
                "code_before_change": "  private static Task<?> createTableTask(ImportTableDesc tableDesc, EximUtil.SemanticAnalyzerWrapperContext x){\n    return tableDesc.getCreateTableTask(x.getInputs(), x.getOutputs(), x.getConf());\n  }",
                "code_after_change": "  private static Task<?> createTableTask(ImportTableDesc tableDesc, EximUtil.SemanticAnalyzerWrapperContext x){\n    return tableDesc.getCreateTableTask(x.getInputs(), x.getOutputs(), x.getConf());\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report precisely identifies the root cause as a NullPointerException (NPE) occurring in the method 'org.apache.hadoop.hive.ql.exec.DDLTask.dropPartitions', which is one of the ground truth methods. The problem location is also precisely identified as the stack trace clearly shows the NPE occurring in the 'dropPartitions' method. However, the bug report does not provide any fix suggestion, hence it is marked as 'Missing' for fix suggestion. There is no wrong information in the bug report as all the details provided are consistent with the context of the bug."
        }
    },
    {
        "filename": "HIVE-13090.json",
        "code_diff": {
            "shims.common.src.main.java.org.apache.hadoop.hive.thrift.ZooKeeperTokenStore.removeToken": {
                "code_before_change": "  public boolean removeToken(DelegationTokenIdentifier tokenIdentifier) {\n    String tokenPath = getTokenPath(tokenIdentifier);\n    zkDelete(tokenPath);\n    return true;\n  }",
                "code_after_change": "  public boolean removeToken(DelegationTokenIdentifier tokenIdentifier) {\n    String tokenPath = getTokenPath(tokenIdentifier);\n    zkDelete(tokenPath);\n    return true;\n  }"
            },
            "shims.common.src.main.java.org.apache.hadoop.hive.thrift.TokenStoreDelegationTokenSecretManager.run": {
                "code_before_change": "    public void run() {\n      LOGGER.info(\"Starting expired delegation token remover thread, \"\n          + \"tokenRemoverScanInterval=\" + tokenRemoverScanInterval\n          / (60 * 1000) + \" min(s)\");\n      try {\n        while (running) {\n          long now = System.currentTimeMillis();\n          if (lastMasterKeyUpdate + keyUpdateInterval < now) {\n            try {\n              rollMasterKeyExt();\n              lastMasterKeyUpdate = now;\n            } catch (IOException e) {\n              LOGGER.error(\"Master key updating failed. \"\n                  + StringUtils.stringifyException(e));\n            }\n          }\n          if (lastTokenCacheCleanup + tokenRemoverScanInterval < now) {\n            removeExpiredTokens();\n            lastTokenCacheCleanup = now;\n          }\n          try {\n            Thread.sleep(5000); // 5 seconds\n          } catch (InterruptedException ie) {\n            LOGGER\n            .error(\"InterruptedExcpetion recieved for ExpiredTokenRemover thread \"\n                + ie);\n          }\n        }\n      } catch (Throwable t) {\n        LOGGER.error(\"ExpiredTokenRemover thread received unexpected exception. \"\n            + t, t);\n        Runtime.getRuntime().exit(-1);\n      }\n    }",
                "code_after_change": "    public void run() {\n      LOGGER.info(\"Starting expired delegation token remover thread, \"\n          + \"tokenRemoverScanInterval=\" + tokenRemoverScanInterval\n          / (60 * 1000) + \" min(s)\");\n      try {\n        while (running) {\n          long now = System.currentTimeMillis();\n          if (lastMasterKeyUpdate + keyUpdateInterval < now) {\n            try {\n              rollMasterKeyExt();\n              lastMasterKeyUpdate = now;\n            } catch (IOException e) {\n              LOGGER.error(\"Master key updating failed. \"\n                  + StringUtils.stringifyException(e));\n            }\n          }\n          if (lastTokenCacheCleanup + tokenRemoverScanInterval < now) {\n            removeExpiredTokens();\n            lastTokenCacheCleanup = now;\n          }\n          try {\n            Thread.sleep(5000); // 5 seconds\n          } catch (InterruptedException ie) {\n            LOGGER\n            .error(\"InterruptedExcpetion recieved for ExpiredTokenRemover thread \"\n                + ie);\n          }\n        }\n      } catch (Throwable t) {\n        LOGGER.error(\"ExpiredTokenRemover thread received unexpected exception. \"\n            + t, t);\n      }\n    }"
            },
            "shims.common.src.main.java.org.apache.hadoop.hive.thrift.ZooKeeperTokenStore.getToken": {
                "code_before_change": "  public DelegationTokenInformation getToken(DelegationTokenIdentifier tokenIdentifier) {\n    byte[] tokenBytes = zkGetData(getTokenPath(tokenIdentifier));\n    try {\n      return HiveDelegationTokenSupport.decodeDelegationTokenInformation(tokenBytes);\n    } catch (Exception ex) {\n      throw new TokenStoreException(\"Failed to decode token\", ex);\n    }\n  }",
                "code_after_change": "  public DelegationTokenInformation getToken(DelegationTokenIdentifier tokenIdentifier) {\n    byte[] tokenBytes = zkGetData(getTokenPath(tokenIdentifier));\n    if(tokenBytes == null) {\n      // The token is already removed.\n      return null;\n    }\n    try {\n      return HiveDelegationTokenSupport.decodeDelegationTokenInformation(tokenBytes);\n    } catch (Exception ex) {\n      throw new TokenStoreException(\"Failed to decode token\", ex);\n    }\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Buggy Method"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Buggy Method"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the method 'ZooKeeperTokenStore.getToken' where the error occurred, which is a ground truth method, but it does not specify the exact root cause related to the fix. Therefore, it is classified as 'Partial' under 'Buggy Method' for root cause identification. There is no fix suggestion provided in the bug report, so it is marked as 'Missing' for fix suggestion. The problem location is identified as 'Partial' under 'Buggy Method' because the report mentions 'ZooKeeperTokenStore.getToken', which is where the error occurred, but not where the actual fix was made. There is no wrong information in the bug report as all the information is relevant to the context of the bug."
        }
    },
    {
        "filename": "HIVE-5664.json",
        "code_diff": {
            "metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase": {
                "code_before_change": "  public void dropDatabase(String name)\n      throws NoSuchObjectException, InvalidOperationException, MetaException, TException {\n    dropDatabase(name, true, false, false);\n  }",
                "code_after_change": "  public void dropDatabase(String name)\n      throws NoSuchObjectException, InvalidOperationException, MetaException, TException {\n    dropDatabase(name, true, false, false);\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the error occurring during the drop database operation and mentions the stack trace, which includes the method 'org.apache.hadoop.hive.ql.exec.DDLTask.dropDatabase'. This method is in the shared stack trace context with the ground truth method 'metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.dropDatabase'. However, it does not precisely identify the root cause at the ground truth method. There is no fix suggestion provided in the bug report. The problem location is partially identified as it mentions methods in the stack trace but not the exact ground truth method. There is no wrong information in the bug report as it accurately describes the error and the context in which it occurs."
        }
    },
    {
        "filename": "HIVE-15778.json",
        "code_diff": {
            "metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.drop_index_by_name_core": {
                "code_before_change": "    private boolean drop_index_by_name_core(final RawStore ms,\n        final String dbName, final String tblName,\n        final String indexName, final boolean deleteData) throws NoSuchObjectException,\n        MetaException, TException, IOException, InvalidObjectException, InvalidInputException {\n      boolean success = false;\n      Index index = null;\n      Path tblPath = null;\n      List<Path> partPaths = null;\n      try {\n        ms.openTransaction();\n        // drop the underlying index table\n        index = get_index_by_name(dbName, tblName, indexName);  // throws exception if not exists\n        firePreEvent(new PreDropIndexEvent(index, this));\n        ms.dropIndex(dbName, tblName, indexName);\n        String idxTblName = index.getIndexTableName();\n        if (idxTblName != null) {\n          String[] qualified = MetaStoreUtils.getQualifiedName(index.getDbName(), idxTblName);\n          Table tbl = get_table_core(qualified[0], qualified[1]);\n          if (tbl.getSd() == null) {\n            throw new MetaException(\"Table metadata is corrupted\");\n          }\n\n          if (tbl.getSd().getLocation() != null) {\n            tblPath = new Path(tbl.getSd().getLocation());\n            if (!wh.isWritable(tblPath.getParent())) {\n              throw new MetaException(\"Index table metadata not deleted since \" +\n                  tblPath.getParent() + \" is not writable by \" +\n                  hiveConf.getUser());\n            }\n          }\n\n          // Drop the partitions and get a list of partition locations which need to be deleted\n          partPaths = dropPartitionsAndGetLocations(ms, qualified[0], qualified[1], tblPath,\n              tbl.getPartitionKeys(), deleteData);\n          if (!ms.dropTable(qualified[0], qualified[1])) {\n            throw new MetaException(\"Unable to drop underlying data table \"\n                + qualified[0] + \".\" + qualified[1] + \" for index \" + indexName);\n          }\n        }\n\n        if (transactionalListeners.size() > 0) {\n          DropIndexEvent dropIndexEvent = new DropIndexEvent(index, true, this);\n          for (MetaStoreEventListener transactionalListener : transactionalListeners) {\n            transactionalListener.onDropIndex(dropIndexEvent);\n          }\n        }\n\n        success = ms.commitTransaction();\n      } finally {\n        if (!success) {\n          ms.rollbackTransaction();\n        } else if (deleteData && tblPath != null) {\n          deletePartitionData(partPaths);\n          deleteTableData(tblPath);\n          // ok even if the data is not deleted\n        }\n        for (MetaStoreEventListener listener : listeners) {\n          DropIndexEvent dropIndexEvent = new DropIndexEvent(index, success, this);\n          listener.onDropIndex(dropIndexEvent);\n        }\n      }\n      return success;\n    }",
                "code_after_change": "    private boolean drop_index_by_name_core(final RawStore ms,\n        final String dbName, final String tblName,\n        final String indexName, final boolean deleteData) throws NoSuchObjectException,\n        MetaException, TException, IOException, InvalidObjectException, InvalidInputException {\n      boolean success = false;\n      Index index = null;\n      Path tblPath = null;\n      List<Path> partPaths = null;\n      try {\n        ms.openTransaction();\n        // drop the underlying index table\n        index = get_index_by_name(dbName, tblName, indexName);  // throws exception if not exists\n        firePreEvent(new PreDropIndexEvent(index, this));\n        ms.dropIndex(dbName, tblName, indexName);\n        String idxTblName = index.getIndexTableName();\n        if (idxTblName != null) {\n          String[] qualified = MetaStoreUtils.getQualifiedName(index.getDbName(), idxTblName);\n          Table tbl = get_table_core(qualified[0], qualified[1]);\n          if (tbl.getSd() == null) {\n            throw new MetaException(\"Table metadata is corrupted\");\n          }\n\n          if (tbl.getSd().getLocation() != null) {\n            tblPath = new Path(tbl.getSd().getLocation());\n            if (!wh.isWritable(tblPath.getParent())) {\n              throw new MetaException(\"Index table metadata not deleted since \" +\n                  tblPath.getParent() + \" is not writable by \" +\n                  hiveConf.getUser());\n            }\n          }\n\n          // Drop the partitions and get a list of partition locations which need to be deleted\n          partPaths = dropPartitionsAndGetLocations(ms, qualified[0], qualified[1], tblPath,\n              tbl.getPartitionKeys(), deleteData);\n          if (!ms.dropTable(qualified[0], qualified[1])) {\n            throw new MetaException(\"Unable to drop underlying data table \"\n                + qualified[0] + \".\" + qualified[1] + \" for index \" + indexName);\n          }\n        }\n\n        if (transactionalListeners.size() > 0) {\n          DropIndexEvent dropIndexEvent = new DropIndexEvent(index, true, this);\n          for (MetaStoreEventListener transactionalListener : transactionalListeners) {\n            transactionalListener.onDropIndex(dropIndexEvent);\n          }\n        }\n\n        success = ms.commitTransaction();\n      } finally {\n        if (!success) {\n          ms.rollbackTransaction();\n        } else if (deleteData && tblPath != null) {\n          deletePartitionData(partPaths);\n          deleteTableData(tblPath);\n          // ok even if the data is not deleted\n        }\n        // Skip the event listeners if the index is NULL\n        if (index != null) {\n          for (MetaStoreEventListener listener : listeners) {\n            DropIndexEvent dropIndexEvent = new DropIndexEvent(index, success, this);\n            listener.onDropIndex(dropIndexEvent);\n          }\n        }\n      }\n      return success;\n    }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Buggy Method"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Buggy Method"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the method 'drop_index_by_name' and 'drop_index_by_name_core' in the stack trace, which are related to the ground truth method 'drop_index_by_name_core'. However, it does not precisely identify the root cause at the ground truth method. The report does not provide a fix suggestion, hence 'Missing' for fix suggestion. The problem location is identified as 'drop_index_by_name_core', which is the buggy method but not where the actual fix was made, hence 'Partial' with 'Buggy Method' sub-category. There is no wrong information in the bug report as all details are relevant to the context of the bug."
        }
    },
    {
        "filename": "HIVE-8386.json",
        "code_diff": {
            "serde.src.java.org.apache.hadoop.hive.serde2.typeinfo.StructTypeInfo.getAllStructFieldTypeInfos": {
                "code_before_change": "  public ArrayList<TypeInfo> getAllStructFieldTypeInfos() {\n    return allStructFieldTypeInfos;\n  }",
                "code_after_change": "  public ArrayList<TypeInfo> getAllStructFieldTypeInfos() {\n    return allStructFieldTypeInfos;\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Direct Caller/Callee"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Direct Caller/Callee"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the issue with case sensitivity in the HCat API, specifically mentioning the method 'getStructFieldTypeInfo' in the stack trace, which directly calls the ground truth method 'getAllStructFieldTypeInfos'. This makes it a 'Direct Caller/Callee' for both root cause and problem location identification. There is no fix suggestion provided in the bug report, hence it is marked as 'Missing'. All information in the bug report is relevant and related to the context of the bug, so there is no wrong information."
        }
    },
    {
        "filename": "HIVE-14714.json",
        "code_diff": {
            "spark-client.src.main.java.org.apache.hive.spark.client.SparkClientImpl.run": {
                "code_before_change": "  public <T extends Serializable> Future<T> run(Job<T> job) {\n    return protocol.run(job);\n  }",
                "code_after_change": "  public <T extends Serializable> Future<T> run(Job<T> job) {\n    return protocol.run(job);\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report mentions the IOException occurring in the context of the SparkClientImpl class, specifically in the run method, which is part of the stack trace. However, it does not precisely identify the root cause at the ground truth method 'spark-client.src.main.java.org.apache.hive.spark.client.SparkClientImpl.run'. The report lacks any fix suggestion, as there is no 'Suggestions' or 'problem_location' field, nor is there any suggestion in the 'Description'. The problem location is partially identified as it mentions the SparkClientImpl class and the run method in the stack trace, which shares context with the ground truth method. There is no wrong information in the report as all details are relevant to the context of the bug."
        }
    },
    {
        "filename": "HIVE-5428.json",
        "code_diff": {
            "metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreDirectSql.MetaStoreDirectSql": {
                "code_before_change": [],
                "code_after_change": []
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsByNamesInternal": {
                "code_before_change": "  protected List<Partition> getPartitionsByNamesInternal(String dbName, String tblName,\n      List<String> partNames, boolean allowSql, boolean allowJdo)\n          throws MetaException, NoSuchObjectException {\n    assert allowSql || allowJdo;\n    dbName = dbName.toLowerCase();\n    tblName = tblName.toLowerCase();\n    boolean doTrace = LOG.isDebugEnabled();\n    boolean doUseDirectSql = canUseDirectSql(allowSql);\n\n    boolean success = false;\n    List<Partition> results = null;\n    try {\n      long start = doTrace ? System.nanoTime() : 0;\n      openTransaction();\n      if (doUseDirectSql) {\n        try {\n          results = directSql.getPartitionsViaSqlFilter(dbName, tblName, partNames, null);\n        } catch (Exception ex) {\n          handleDirectSqlError(allowJdo, ex);\n          doUseDirectSql = false;\n          start = doTrace ? System.nanoTime() : 0;\n        }\n      }\n\n      if (!doUseDirectSql) {\n        results = getPartitionsViaOrmFilter(dbName, tblName, partNames);\n      }\n      success = commitTransaction();\n      if (doTrace) {\n        LOG.debug(results.size() + \" partition retrieved using \" + (doUseDirectSql ? \"SQL\" : \"ORM\")\n            + \" in \" + ((System.nanoTime() - start) / 1000000.0) + \"ms\");\n      }\n    } finally {\n      if (!success) {\n        rollbackTransaction();\n      }\n    }\n    return results;\n  }",
                "code_after_change": "  protected List<Partition> getPartitionsByNamesInternal(String dbName, String tblName,\n      List<String> partNames, boolean allowSql, boolean allowJdo)\n          throws MetaException, NoSuchObjectException {\n    assert allowSql || allowJdo;\n    dbName = dbName.toLowerCase();\n    tblName = tblName.toLowerCase();\n    boolean doTrace = LOG.isDebugEnabled();\n    boolean doUseDirectSql = canUseDirectSql(allowSql, allowJdo);\n\n    boolean success = false;\n    List<Partition> results = null;\n    try {\n      long start = doTrace ? System.nanoTime() : 0;\n      openTransaction();\n      if (doUseDirectSql) {\n        try {\n          results = directSql.getPartitionsViaSqlFilter(dbName, tblName, partNames, null);\n        } catch (Exception ex) {\n          handleDirectSqlError(allowJdo, ex);\n          doUseDirectSql = false;\n          start = doTrace ? System.nanoTime() : 0;\n        }\n      }\n\n      if (!doUseDirectSql) {\n        results = getPartitionsViaOrmFilter(dbName, tblName, partNames);\n      }\n      success = commitTransaction();\n      if (doTrace) {\n        LOG.debug(results.size() + \" partition retrieved using \" + (doUseDirectSql ? \"SQL\" : \"ORM\")\n            + \" in \" + ((System.nanoTime() - start) / 1000000.0) + \"ms\");\n      }\n    } finally {\n      if (!success) {\n        rollbackTransaction();\n      }\n    }\n    return results;\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsInternal": {
                "code_before_change": "  protected List<Partition> getPartitionsInternal(String dbName, String tableName,\n      int maxParts, boolean allowSql, boolean allowJdo) throws MetaException {\n    assert allowSql || allowJdo;\n    boolean doTrace = LOG.isDebugEnabled();\n    boolean doUseDirectSql = canUseDirectSql(allowSql);\n\n    boolean success = false;\n    List<Partition> parts = null;\n    try {\n      long start = doTrace ? System.nanoTime() : 0;\n      openTransaction();\n      if (doUseDirectSql) {\n        try {\n          Integer max = (maxParts < 0) ? null : maxParts;\n          parts = directSql.getPartitions(dbName, tableName, max);\n        } catch (Exception ex) {\n          handleDirectSqlError(allowJdo, ex);\n          doUseDirectSql = false;\n          start = doTrace ? System.nanoTime() : 0;\n        }\n      }\n\n      if (!doUseDirectSql) {\n        parts = convertToParts(listMPartitions(dbName, tableName, maxParts));\n      }\n      success = commitTransaction();\n      if (doTrace) {\n        LOG.debug(parts.size() + \" partition retrieved using \" + (doUseDirectSql ? \"SQL\" : \"ORM\")\n            + \" in \" + ((System.nanoTime() - start) / 1000000.0) + \"ms\");\n      }\n      return parts;\n    } finally {\n      if (!success) {\n        rollbackTransaction();\n      }\n    }\n  }",
                "code_after_change": "  protected List<Partition> getPartitionsInternal(String dbName, String tableName,\n      int maxParts, boolean allowSql, boolean allowJdo) throws MetaException {\n    assert allowSql || allowJdo;\n    boolean doTrace = LOG.isDebugEnabled();\n    boolean doUseDirectSql = canUseDirectSql(allowSql, allowJdo);\n\n    boolean success = false;\n    List<Partition> parts = null;\n    try {\n      long start = doTrace ? System.nanoTime() : 0;\n      openTransaction();\n      if (doUseDirectSql) {\n        try {\n          Integer max = (maxParts < 0) ? null : maxParts;\n          parts = directSql.getPartitions(dbName, tableName, max);\n        } catch (Exception ex) {\n          handleDirectSqlError(allowJdo, ex);\n          doUseDirectSql = false;\n          start = doTrace ? System.nanoTime() : 0;\n        }\n      }\n\n      if (!doUseDirectSql) {\n        parts = convertToParts(listMPartitions(dbName, tableName, maxParts));\n      }\n      success = commitTransaction();\n      if (doTrace) {\n        LOG.debug(parts.size() + \" partition retrieved using \" + (doUseDirectSql ? \"SQL\" : \"ORM\")\n            + \" in \" + ((System.nanoTime() - start) / 1000000.0) + \"ms\");\n      }\n      return parts;\n    } finally {\n      if (!success) {\n        rollbackTransaction();\n      }\n    }\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsByFilterInternal": {
                "code_before_change": "  protected List<Partition> getPartitionsByFilterInternal(String dbName, String tblName,\n      String filter, short maxParts, boolean allowSql, boolean allowJdo)\n      throws MetaException, NoSuchObjectException {\n    assert allowSql || allowJdo;\n    boolean doTrace = LOG.isDebugEnabled();\n    boolean doUseDirectSql = canUseDirectSql(allowSql);\n\n    dbName = dbName.toLowerCase();\n    tblName = tblName.toLowerCase();\n    ExpressionTree tree = (filter != null && !filter.isEmpty())\n        ? getFilterParser(filter).tree : ExpressionTree.EMPTY_TREE;\n\n    List<Partition> results = null;\n    boolean success = false;\n    try {\n      long start = doTrace ? System.nanoTime() : 0;\n      openTransaction();\n      Table table = ensureGetTable(dbName, tblName);\n      if (doUseDirectSql) {\n        try {\n          Integer max = (maxParts < 0) ? null : (int)maxParts;\n          results = directSql.getPartitionsViaSqlFilter(table, tree, max);\n          if (results == null) {\n            // Cannot push down SQL filter. The message has been logged internally.\n            // This is not an error so don't roll back, just go to JDO.\n            doUseDirectSql = false;\n          }\n        } catch (Exception ex) {\n          handleDirectSqlError(allowJdo, ex);\n          doUseDirectSql = false;\n          start = doTrace ? System.nanoTime() : 0;\n          table = ensureGetTable(dbName, tblName); // detached on rollback, get again\n        }\n      }\n\n      if (!doUseDirectSql) {\n        results = getPartitionsViaOrmFilter(table, tree, maxParts, true);\n      }\n      success = commitTransaction();\n      if (doTrace) {\n        LOG.debug(results.size() + \" partition retrieved using \" + (doUseDirectSql ? \"SQL\" : \"ORM\")\n            + \" in \" + ((System.nanoTime() - start) / 1000000.0) + \"ms\");\n      }\n      return results;\n    } finally {\n      if (!success) {\n        rollbackTransaction();\n      }\n    }\n  }",
                "code_after_change": "  protected List<Partition> getPartitionsByFilterInternal(String dbName, String tblName,\n      String filter, short maxParts, boolean allowSql, boolean allowJdo)\n      throws MetaException, NoSuchObjectException {\n    assert allowSql || allowJdo;\n    boolean doTrace = LOG.isDebugEnabled();\n    boolean doUseDirectSql = canUseDirectSql(allowSql, allowJdo);\n\n    dbName = dbName.toLowerCase();\n    tblName = tblName.toLowerCase();\n    ExpressionTree tree = (filter != null && !filter.isEmpty())\n        ? getFilterParser(filter).tree : ExpressionTree.EMPTY_TREE;\n\n    List<Partition> results = null;\n    boolean success = false;\n    try {\n      long start = doTrace ? System.nanoTime() : 0;\n      openTransaction();\n      Table table = ensureGetTable(dbName, tblName);\n      if (doUseDirectSql) {\n        try {\n          Integer max = (maxParts < 0) ? null : (int)maxParts;\n          results = directSql.getPartitionsViaSqlFilter(table, tree, max);\n          if (results == null) {\n            // Cannot push down SQL filter. The message has been logged internally.\n            // This is not an error so don't roll back, just go to JDO.\n            doUseDirectSql = false;\n          }\n        } catch (Exception ex) {\n          handleDirectSqlError(allowJdo, ex);\n          doUseDirectSql = false;\n          start = doTrace ? System.nanoTime() : 0;\n          table = ensureGetTable(dbName, tblName); // detached on rollback, get again\n        }\n      }\n\n      if (!doUseDirectSql) {\n        results = getPartitionsViaOrmFilter(table, tree, maxParts, true);\n      }\n      success = commitTransaction();\n      if (doTrace) {\n        LOG.debug(results.size() + \" partition retrieved using \" + (doUseDirectSql ? \"SQL\" : \"ORM\")\n            + \" in \" + ((System.nanoTime() - start) / 1000000.0) + \"ms\");\n      }\n      return results;\n    } finally {\n      if (!success) {\n        rollbackTransaction();\n      }\n    }\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsByExprInternal": {
                "code_before_change": "  protected boolean getPartitionsByExprInternal(String dbName, String tblName,\n      byte[] expr, String defaultPartitionName, short maxParts, Set<Partition> result,\n      boolean allowSql, boolean allowJdo) throws TException {\n    assert allowSql || allowJdo;\n    assert result != null;\n    dbName = dbName.toLowerCase();\n    tblName = tblName.toLowerCase();\n\n    // We will try pushdown first, so make the filter. This will also validate the expression,\n    // if serialization fails we will throw incompatible metastore error to the client.\n    String filter = null;\n    try {\n      filter = expressionProxy.convertExprToFilter(expr);\n    } catch (MetaException ex) {\n      throw new IMetaStoreClient.IncompatibleMetastoreException(ex.getMessage());\n    }\n\n    // Make a tree out of the filter.\n    // TODO: this is all pretty ugly. The only reason we need all these transformations\n    //       is to maintain support for simple filters for HCat users that query metastore.\n    //       If forcing everyone to use thick client is out of the question, maybe we could\n    //       parse the filter into standard hive expressions and not all this separate tree\n    //       Filter.g stuff. That way this method and ...ByFilter would just be merged.\n    ExpressionTree exprTree = makeExpressionTree(filter);\n\n    boolean doUseDirectSql = allowSql && isDirectSqlEnabled(maxParts);\n    boolean doTrace = LOG.isDebugEnabled();\n    List<Partition> partitions = null;\n    boolean hasUnknownPartitions = false;\n    boolean success = false;\n    try {\n      long start = doTrace ? System.nanoTime() : 0;\n      openTransaction();\n      Table table = ensureGetTable(dbName, tblName);\n      if (doUseDirectSql) {\n        try {\n          if (exprTree != null) {\n            // We have some sort of expression tree, try SQL filter pushdown.\n            partitions = directSql.getPartitionsViaSqlFilter(table, exprTree, null);\n          }\n          if (partitions == null) {\n            // We couldn't do SQL filter pushdown. Get names via normal means.\n            List<String> partNames = new LinkedList<String>();\n            hasUnknownPartitions = getPartitionNamesPrunedByExprNoTxn(\n                table, expr, defaultPartitionName, maxParts, partNames);\n            partitions = directSql.getPartitionsViaSqlFilter(dbName, tblName, partNames, null);\n          }\n        } catch (Exception ex) {\n          handleDirectSqlError(allowJdo, ex);\n          doUseDirectSql = false;\n          table = ensureGetTable(dbName, tblName); // Get again, detached on rollback.\n        }\n      }\n\n      if (!doUseDirectSql) {\n        assert partitions == null;\n        if (exprTree != null) {\n          // We have some sort of expression tree, try JDOQL filter pushdown.\n          partitions = getPartitionsViaOrmFilter(table, exprTree, maxParts, false);\n        }\n        if (partitions == null) {\n          // We couldn't do JDOQL filter pushdown. Get names via normal means.\n          List<String> partNames = new ArrayList<String>();\n          hasUnknownPartitions = getPartitionNamesPrunedByExprNoTxn(\n              table, expr, defaultPartitionName, maxParts, partNames);\n          partitions = getPartitionsViaOrmFilter(dbName, tblName, partNames);\n        }\n      }\n      success = commitTransaction();\n      if (doTrace) {\n        double time = ((System.nanoTime() - start) / 1000000.0);\n        LOG.debug(partitions.size() + \" partition retrieved using \"\n          + (doUseDirectSql ? \"SQL\" : \"ORM\") + \" in \" + time + \"ms\");\n      }\n    } finally {\n      if (!success) {\n        rollbackTransaction();\n      }\n    }\n    result.addAll(partitions);\n    return hasUnknownPartitions;\n  }",
                "code_after_change": "  protected boolean getPartitionsByExprInternal(String dbName, String tblName,\n      byte[] expr, String defaultPartitionName, short maxParts, Set<Partition> result,\n      boolean allowSql, boolean allowJdo) throws TException {\n    assert allowSql || allowJdo;\n    assert result != null;\n    dbName = dbName.toLowerCase();\n    tblName = tblName.toLowerCase();\n\n    // We will try pushdown first, so make the filter. This will also validate the expression,\n    // if serialization fails we will throw incompatible metastore error to the client.\n    String filter = null;\n    try {\n      filter = expressionProxy.convertExprToFilter(expr);\n    } catch (MetaException ex) {\n      throw new IMetaStoreClient.IncompatibleMetastoreException(ex.getMessage());\n    }\n\n    // Make a tree out of the filter.\n    // TODO: this is all pretty ugly. The only reason we need all these transformations\n    //       is to maintain support for simple filters for HCat users that query metastore.\n    //       If forcing everyone to use thick client is out of the question, maybe we could\n    //       parse the filter into standard hive expressions and not all this separate tree\n    //       Filter.g stuff. That way this method and ...ByFilter would just be merged.\n    ExpressionTree exprTree = makeExpressionTree(filter);\n\n    boolean doUseDirectSql = canUseDirectSql(allowSql, allowJdo);\n    boolean doTrace = LOG.isDebugEnabled();\n    List<Partition> partitions = null;\n    boolean hasUnknownPartitions = false;\n    boolean success = false;\n    try {\n      long start = doTrace ? System.nanoTime() : 0;\n      openTransaction();\n      Table table = ensureGetTable(dbName, tblName);\n      if (doUseDirectSql) {\n        try {\n          if (exprTree != null) {\n            // We have some sort of expression tree, try SQL filter pushdown.\n            partitions = directSql.getPartitionsViaSqlFilter(table, exprTree, null);\n          }\n          if (partitions == null) {\n            // We couldn't do SQL filter pushdown. Get names via normal means.\n            List<String> partNames = new LinkedList<String>();\n            hasUnknownPartitions = getPartitionNamesPrunedByExprNoTxn(\n                table, expr, defaultPartitionName, maxParts, partNames);\n            partitions = directSql.getPartitionsViaSqlFilter(dbName, tblName, partNames, null);\n          }\n        } catch (Exception ex) {\n          handleDirectSqlError(allowJdo, ex);\n          doUseDirectSql = false;\n          table = ensureGetTable(dbName, tblName); // Get again, detached on rollback.\n        }\n      }\n\n      if (!doUseDirectSql) {\n        assert partitions == null;\n        if (exprTree != null) {\n          // We have some sort of expression tree, try JDOQL filter pushdown.\n          partitions = getPartitionsViaOrmFilter(table, exprTree, maxParts, false);\n        }\n        if (partitions == null) {\n          // We couldn't do JDOQL filter pushdown. Get names via normal means.\n          List<String> partNames = new ArrayList<String>();\n          hasUnknownPartitions = getPartitionNamesPrunedByExprNoTxn(\n              table, expr, defaultPartitionName, maxParts, partNames);\n          partitions = getPartitionsViaOrmFilter(dbName, tblName, partNames);\n        }\n      }\n      success = commitTransaction();\n      if (doTrace) {\n        double time = ((System.nanoTime() - start) / 1000000.0);\n        LOG.debug(partitions.size() + \" partition retrieved using \"\n          + (doUseDirectSql ? \"SQL\" : \"ORM\") + \" in \" + time + \"ms\");\n      }\n    } finally {\n      if (!success) {\n        rollbackTransaction();\n      }\n    }\n    result.addAll(partitions);\n    return hasUnknownPartitions;\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the issue occurring during the initialization of MetaStoreDirectSql, which is part of the stack trace context shared with the ground truth methods. However, it does not pinpoint the exact root cause within the ground truth methods. The report does not provide any fix suggestions, hence 'Missing' for fix suggestion. The problem location is identified in the shared stack trace context but not precisely at the ground truth methods, leading to a 'Partial' classification. There is no incorrect information in the report, so 'No' for wrong information."
        }
    },
    {
        "filename": "HIVE-12567.json",
        "code_diff": {
            "metastore.src.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.isRetryable": {
                "code_before_change": "  private static boolean isRetryable(Exception ex) {\n    if(ex instanceof SQLException) {\n      SQLException sqlException = (SQLException)ex;\n      if(\"08S01\".equalsIgnoreCase(sqlException.getSQLState())) {\n        //in MSSQL this means Communication Link Failure\n        return true;\n      }\n      //see https://issues.apache.org/jira/browse/HIVE-9938\n    }\n    return false;\n  }",
                "code_after_change": "  private static boolean isRetryable(Exception ex) {\n    if(ex instanceof SQLException) {\n      SQLException sqlException = (SQLException)ex;\n      if(\"08S01\".equalsIgnoreCase(sqlException.getSQLState())) {\n        //in MSSQL this means Communication Link Failure\n        return true;\n      }\n      if(\"ORA-08176\".equalsIgnoreCase(sqlException.getSQLState())) {\n        return true;\n      }\n      //see also https://issues.apache.org/jira/browse/HIVE-9938\n    }\n    return false;\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report mentions the error 'ORA-08176' and the stack trace includes methods related to the ground truth method 'TxnHandler.isRetryable', specifically 'TxnHandler.lock' and 'TxnHandler.checkLock'. These methods are in the same stack trace context as the ground truth method, but the report does not precisely identify 'TxnHandler.isRetryable' as the root cause. There is no fix suggestion provided in the bug report. The problem location is partially identified as it mentions methods in the shared stack trace context. There is no wrong information in the bug report as it accurately describes the error and its context."
        }
    },
    {
        "filename": "HIVE-6984.json",
        "code_diff": {
            "ql.src.java.org.apache.hadoop.hive.ql.exec.TableScanOperator.initializeOp": {
                "code_before_change": "  protected void initializeOp(Configuration hconf) throws HiveException {\n    initializeChildren(hconf);\n    inputFileChanged = false;\n\n    if (conf == null) {\n      return;\n    }\n    rowLimit = conf.getRowLimit();\n    if (!conf.isGatherStats()) {\n      return;\n    }\n\n    this.hconf = hconf;\n    if (hconf instanceof JobConf) {\n      jc = (JobConf) hconf;\n    } else {\n      // test code path\n      jc = new JobConf(hconf);\n    }\n\n    currentStat = null;\n    stats = new HashMap<String, Stat>();\n    if (conf.getPartColumns() == null || conf.getPartColumns().size() == 0) {\n      // NON PARTITIONED table\n      return;\n    }\n\n  }",
                "code_after_change": "  protected void initializeOp(Configuration hconf) throws HiveException {\n    initializeChildren(hconf);\n    inputFileChanged = false;\n\n    if (conf == null) {\n      return;\n    }\n    rowLimit = conf.getRowLimit();\n    if (!conf.isGatherStats()) {\n      return;\n    }\n\n    this.hconf = hconf;\n    if (hconf instanceof JobConf) {\n      jc = (JobConf) hconf;\n    } else {\n      // test code path\n      jc = new JobConf(hconf);\n    }\n\n    defaultPartitionName = HiveConf.getVar(hconf, HiveConf.ConfVars.DEFAULTPARTITIONNAME);\n    currentStat = null;\n    stats = new HashMap<String, Stat>();\n    if (conf.getPartColumns() == null || conf.getPartColumns().size() == 0) {\n      // NON PARTITIONED table\n      return;\n    }\n\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.exec.TableScanOperator.gatherStats": {
                "code_before_change": "  private void gatherStats(Object row) {\n    // first row/call or a new partition\n    if ((currentStat == null) || inputFileChanged) {\n      String partitionSpecs;\n      inputFileChanged = false;\n      if (conf.getPartColumns() == null || conf.getPartColumns().size() == 0) {\n        partitionSpecs = \"\"; // non-partitioned\n      } else {\n        // Figure out the partition spec from the input.\n        // This is only done once for the first row (when stat == null)\n        // since all rows in the same mapper should be from the same partition.\n        List<Object> writable;\n        List<String> values;\n        int dpStartCol; // the first position of partition column\n        assert inputObjInspectors[0].getCategory() == ObjectInspector.Category.STRUCT : \"input object inspector is not struct\";\n\n        writable = new ArrayList<Object>(conf.getPartColumns().size());\n        values = new ArrayList<String>(conf.getPartColumns().size());\n        dpStartCol = 0;\n        StructObjectInspector soi = (StructObjectInspector) inputObjInspectors[0];\n        for (StructField sf : soi.getAllStructFieldRefs()) {\n          String fn = sf.getFieldName();\n          if (!conf.getPartColumns().contains(fn)) {\n            dpStartCol++;\n          } else {\n            break;\n          }\n        }\n        ObjectInspectorUtils.partialCopyToStandardObject(writable, row, dpStartCol, conf\n            .getPartColumns().size(),\n            (StructObjectInspector) inputObjInspectors[0], ObjectInspectorCopyOption.WRITABLE);\n\n        for (Object o : writable) {\n          assert (o != null && o.toString().length() > 0);\n          values.add(o.toString());\n        }\n        partitionSpecs = FileUtils.makePartName(conf.getPartColumns(), values);\n        LOG.info(\"Stats Gathering found a new partition spec = \" + partitionSpecs);\n      }\n      // find which column contains the raw data size (both partitioned and non partitioned\n      int uSizeColumn = -1;\n      StructObjectInspector soi = (StructObjectInspector) inputObjInspectors[0];\n      for (int i = 0; i < soi.getAllStructFieldRefs().size(); i++) {\n        if (soi.getAllStructFieldRefs().get(i).getFieldName()\n            .equals(VirtualColumn.RAWDATASIZE.getName().toLowerCase())) {\n          uSizeColumn = i;\n          break;\n        }\n      }\n      currentStat = stats.get(partitionSpecs);\n      if (currentStat == null) {\n        currentStat = new Stat();\n        currentStat.setBookkeepingInfo(StatsSetupConst.RAW_DATA_SIZE, uSizeColumn);\n        stats.put(partitionSpecs, currentStat);\n      }\n    }\n\n    // increase the row count\n    currentStat.addToStat(StatsSetupConst.ROW_COUNT, 1);\n\n    // extract the raw data size, and update the stats for the current partition\n    int rdSizeColumn = currentStat.getBookkeepingInfo(StatsSetupConst.RAW_DATA_SIZE);\n    if(rdSizeColumn != -1) {\n      List<Object> rdSize = new ArrayList<Object>(1);\n      ObjectInspectorUtils.partialCopyToStandardObject(rdSize, row,\n          rdSizeColumn, 1, (StructObjectInspector) inputObjInspectors[0],\n          ObjectInspectorCopyOption.WRITABLE);\n      currentStat.addToStat(StatsSetupConst.RAW_DATA_SIZE, (((LongWritable)rdSize.get(0)).get()));\n    }\n\n  }",
                "code_after_change": "  private void gatherStats(Object row) {\n    // first row/call or a new partition\n    if ((currentStat == null) || inputFileChanged) {\n      String partitionSpecs;\n      inputFileChanged = false;\n      if (conf.getPartColumns() == null || conf.getPartColumns().size() == 0) {\n        partitionSpecs = \"\"; // non-partitioned\n      } else {\n        // Figure out the partition spec from the input.\n        // This is only done once for the first row (when stat == null)\n        // since all rows in the same mapper should be from the same partition.\n        List<Object> writable;\n        List<String> values;\n        int dpStartCol; // the first position of partition column\n        assert inputObjInspectors[0].getCategory() == ObjectInspector.Category.STRUCT : \"input object inspector is not struct\";\n\n        writable = new ArrayList<Object>(conf.getPartColumns().size());\n        values = new ArrayList<String>(conf.getPartColumns().size());\n        dpStartCol = 0;\n        StructObjectInspector soi = (StructObjectInspector) inputObjInspectors[0];\n        for (StructField sf : soi.getAllStructFieldRefs()) {\n          String fn = sf.getFieldName();\n          if (!conf.getPartColumns().contains(fn)) {\n            dpStartCol++;\n          } else {\n            break;\n          }\n        }\n        ObjectInspectorUtils.partialCopyToStandardObject(writable, row, dpStartCol, conf\n            .getPartColumns().size(),\n            (StructObjectInspector) inputObjInspectors[0], ObjectInspectorCopyOption.WRITABLE);\n\n        for (Object o : writable) {\n          // It's possible that a parition column may have NULL value, in which case the row belongs\n          // to the special partition, __HIVE_DEFAULT_PARTITION__.\n          values.add(o == null ? defaultPartitionName : o.toString());\n        }\n        partitionSpecs = FileUtils.makePartName(conf.getPartColumns(), values);\n        LOG.info(\"Stats Gathering found a new partition spec = \" + partitionSpecs);\n      }\n      // find which column contains the raw data size (both partitioned and non partitioned\n      int uSizeColumn = -1;\n      StructObjectInspector soi = (StructObjectInspector) inputObjInspectors[0];\n      for (int i = 0; i < soi.getAllStructFieldRefs().size(); i++) {\n        if (soi.getAllStructFieldRefs().get(i).getFieldName()\n            .equals(VirtualColumn.RAWDATASIZE.getName().toLowerCase())) {\n          uSizeColumn = i;\n          break;\n        }\n      }\n      currentStat = stats.get(partitionSpecs);\n      if (currentStat == null) {\n        currentStat = new Stat();\n        currentStat.setBookkeepingInfo(StatsSetupConst.RAW_DATA_SIZE, uSizeColumn);\n        stats.put(partitionSpecs, currentStat);\n      }\n    }\n\n    // increase the row count\n    currentStat.addToStat(StatsSetupConst.ROW_COUNT, 1);\n\n    // extract the raw data size, and update the stats for the current partition\n    int rdSizeColumn = currentStat.getBookkeepingInfo(StatsSetupConst.RAW_DATA_SIZE);\n    if(rdSizeColumn != -1) {\n      List<Object> rdSize = new ArrayList<Object>(1);\n      ObjectInspectorUtils.partialCopyToStandardObject(rdSize, row,\n          rdSizeColumn, 1, (StructObjectInspector) inputObjInspectors[0],\n          ObjectInspectorCopyOption.WRITABLE);\n      currentStat.addToStat(StatsSetupConst.RAW_DATA_SIZE, (((LongWritable)rdSize.get(0)).get()));\n    }\n\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report mentions a NullPointerException occurring in the method 'org.apache.hadoop.hive.ql.exec.TableScanOperator.gatherStats', which is part of the stack trace and is one of the ground truth methods. However, it does not explicitly identify the root cause related to the ground truth methods, hence it is classified as 'Partial' with 'Shared Stack Trace Context'. There is no fix suggestion provided in the bug report, so it is marked as 'Missing'. The problem location is also identified as 'Partial' with 'Shared Stack Trace Context' because the method where the error occurred is mentioned in the stack trace. There is no incorrect information in the bug report, so 'Wrong Information' is marked as 'No'."
        }
    },
    {
        "filename": "HIVE-10736.json",
        "code_diff": {
            "ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionState.open": {
                "code_before_change": "  public void open(HiveConf conf)\n      throws IOException, LoginException, URISyntaxException, TezException {\n    open(conf, null);\n  }",
                "code_after_change": "  public void open(HiveConf conf)\n      throws IOException, LoginException, URISyntaxException, TezException {\n    open(conf, null);\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezJobMonitor.run": {
                "code_before_change": "      public void run() {\n        for (DAGClient c: shutdownList) {\n          TezJobMonitor.killRunningJobs();\n        }\n        try {\n          for (TezSessionState s: TezSessionState.getOpenSessions()) {\n            System.err.println(\"Shutting down tez session.\");\n            TezSessionPoolManager.getInstance().close(s, false);\n          }\n        } catch (Exception e) {\n          // ignore\n        }\n      }",
                "code_after_change": "      public void run() {\n        for (DAGClient c: shutdownList) {\n          TezJobMonitor.killRunningJobs();\n        }\n        try {\n          for (TezSessionState s : TezSessionPoolManager.getInstance().getOpenSessions()) {\n            System.err.println(\"Shutting down tez session.\");\n            TezSessionPoolManager.getInstance().close(s, false);\n          }\n        } catch (Exception e) {\n          // ignore\n        }\n      }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionState.close": {
                "code_before_change": "  public void close(boolean keepTmpDir) throws Exception {\n    if (!isOpen()) {\n      return;\n    }\n\n    LOG.info(\"Closing Tez Session\");\n    try {\n      session.stop();\n      openSessions.remove(this);\n    } catch (SessionNotRunning nr) {\n      // ignore\n    }\n\n    if (!keepTmpDir) {\n      cleanupScratchDir();\n    }\n    session = null;\n    tezScratchDir = null;\n    conf = null;\n    appJarLr = null;\n    additionalFilesNotFromConf.clear();\n    localizedResources.clear();\n  }",
                "code_after_change": "  public void close(boolean keepTmpDir) throws TezException, IOException {\n    if (!isOpen()) {\n      return;\n    }\n\n    LOG.info(\"Closing Tez Session\");\n    try {\n      session.stop();\n    } catch (SessionNotRunning nr) {\n      // ignore\n    }\n\n    if (!keepTmpDir) {\n      cleanupScratchDir();\n    }\n    session = null;\n    tezScratchDir = null;\n    conf = null;\n    appJarLr = null;\n    additionalFilesNotFromConf.clear();\n    localizedResources.clear();\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionState.isOpen": {
                "code_before_change": "  public boolean isOpen() {\n    return session != null;\n  }",
                "code_after_change": "  public boolean isOpen() {\n    return session != null;\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolManager.closeAndOpen": {
                "code_before_change": "  public void closeAndOpen(TezSessionState sessionState, HiveConf conf, boolean keepTmpDir)\n      throws Exception {\n    closeAndOpen(sessionState, conf, null, keepTmpDir);\n  }",
                "code_after_change": "  public void closeAndOpen(TezSessionState sessionState, HiveConf conf, boolean keepTmpDir)\n      throws Exception {\n    closeAndOpen(sessionState, conf, null, keepTmpDir);\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolManager.getNewSessionState": {
                "code_before_change": "  private TezSessionState getNewSessionState(HiveConf conf,\n      String queueName, boolean doOpen) throws Exception {\n    TezSessionState retTezSessionState = createSession(TezSessionState.makeSessionId());\n    if (queueName != null) {\n      conf.set(\"tez.queue.name\", queueName);\n    }\n    String what = \"Created\";\n    if (doOpen) {\n      retTezSessionState.open(conf);\n      what = \"Started\";\n    }\n\n    LOG.info(what + \" a new session for queue: \" + queueName +\n        \" session id: \" + retTezSessionState.getSessionId());\n    return retTezSessionState;\n  }",
                "code_after_change": "  private TezSessionState getNewSessionState(HiveConf conf,\n      String queueName, boolean doOpen) throws Exception {\n    TezSessionState retTezSessionState = createSession(TezSessionState.makeSessionId());\n    if (queueName != null) {\n      conf.set(\"tez.queue.name\", queueName);\n    }\n    String what = \"Created\";\n    if (doOpen) {\n      retTezSessionState.open(conf);\n      openSessions.add(retTezSessionState);\n      what = \"Started\";\n    }\n\n    LOG.info(what + \" a new session for queue: \" + queueName +\n        \" session id: \" + retTezSessionState.getSessionId());\n    return retTezSessionState;\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolManager.startPool": {
                "code_before_change": "  public void startPool() throws Exception {\n    this.inited = true;\n    for (int i = 0; i < blockingQueueLength; i++) {\n      HiveConf newConf = new HiveConf(initConf);\n      TezSessionState sessionState = defaultQueuePool.take();\n      newConf.set(\"tez.queue.name\", sessionState.getQueueName());\n      sessionState.open(newConf);\n      defaultQueuePool.put(sessionState);\n    }\n  }",
                "code_after_change": "  public void startPool() throws Exception {\n    this.inited = true;\n    for (int i = 0; i < blockingQueueLength; i++) {\n      HiveConf newConf = new HiveConf(initConf);\n      TezSessionState sessionState = defaultQueuePool.take();\n      newConf.set(\"tez.queue.name\", sessionState.getQueueName());\n      sessionState.open(newConf);\n      openSessions.add(sessionState);\n      defaultQueuePool.put(sessionState);\n    }\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolManager.close": {
                "code_before_change": "  public void close(TezSessionState tezSessionState, boolean keepTmpDir) throws Exception {\n    LOG.info(\"Closing tez session default? \" + tezSessionState.isDefault());\n    if (!tezSessionState.isDefault()) {\n      tezSessionState.close(keepTmpDir);\n    }\n  }",
                "code_after_change": "  public void close(TezSessionState tezSessionState, boolean keepTmpDir) throws Exception {\n    LOG.info(\"Closing tez session default? \" + tezSessionState.isDefault());\n    if (!tezSessionState.isDefault()) {\n      tezSessionState.close(keepTmpDir);\n      openSessions.remove(tezSessionState);\n    }\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezSessionPoolManager.stop": {
                "code_before_change": "  public void stop() throws Exception {\n    if ((sessionPool == null) || (this.inited == false)) {\n      return;\n    }\n\n    // we can just stop all the sessions\n    for (TezSessionState sessionState: TezSessionState.getOpenSessions()) {\n      if (sessionState.isDefault()) {\n        sessionState.close(false);\n      }\n    }\n  }",
                "code_after_change": "  public void stop() throws Exception {\n    if ((sessionPool == null) || (this.inited == false)) {\n      return;\n    }\n\n    // we can just stop all the sessions\n    Iterator<TezSessionState> iter = openSessions.iterator();\n    while (iter.hasNext()) {\n      TezSessionState sessionState = iter.next();\n      if (sessionState.isDefault()) {\n        sessionState.close(false);\n        iter.remove();\n      }\n    }\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report mentions a ConcurrentModificationException occurring during the shutdown process, which is related to the TezSessionPoolManager.stop method. This method is part of the stack trace and is a ground truth method, but the report does not precisely identify the root cause within the method. The report lacks any fix suggestions, as there is no 'Suggestions' or 'problem_location' field, nor any suggestion in the 'Description'. The problem location is partially identified as it mentions the TezSessionPoolManager.stop method in the stack trace, which is a ground truth method. There is no wrong information in the bug report as all details are relevant to the context of the bug."
        }
    },
    {
        "filename": "HIVE-7710.json",
        "code_diff": {
            "metastore.src.java.org.apache.hadoop.hive.metastore.HiveAlterHandler.alterTable": {
                "code_before_change": "  public void alterTable(RawStore msdb, Warehouse wh, String dbname,\n      String name, Table newt) throws InvalidOperationException, MetaException {\n    if (newt == null) {\n      throw new InvalidOperationException(\"New table is invalid: \" + newt);\n    }\n\n    if (!MetaStoreUtils.validateName(newt.getTableName())) {\n      throw new InvalidOperationException(newt.getTableName()\n          + \" is not a valid object name\");\n    }\n    String validate = MetaStoreUtils.validateTblColumns(newt.getSd().getCols());\n    if (validate != null) {\n      throw new InvalidOperationException(\"Invalid column \" + validate);\n    }\n\n    Path srcPath = null;\n    FileSystem srcFs = null;\n    Path destPath = null;\n    FileSystem destFs = null;\n\n    boolean success = false;\n    boolean moveData = false;\n    boolean rename = false;\n    Table oldt = null;\n    List<ObjectPair<Partition, String>> altps = new ArrayList<ObjectPair<Partition, String>>();\n\n    try {\n      msdb.openTransaction();\n      name = name.toLowerCase();\n      dbname = dbname.toLowerCase();\n\n      // check if table with the new name already exists\n      if (!newt.getTableName().equalsIgnoreCase(name)\n          || !newt.getDbName().equalsIgnoreCase(dbname)) {\n        if (msdb.getTable(newt.getDbName(), newt.getTableName()) != null) {\n          throw new InvalidOperationException(\"new table \" + newt.getDbName()\n              + \".\" + newt.getTableName() + \" already exists\");\n        }\n        rename = true;\n      }\n\n      // get old table\n      oldt = msdb.getTable(dbname, name);\n      if (oldt == null) {\n        throw new InvalidOperationException(\"table \" + newt.getDbName() + \".\"\n            + newt.getTableName() + \" doesn't exist\");\n      }\n\n      if (HiveConf.getBoolVar(hiveConf,\n            HiveConf.ConfVars.METASTORE_DISALLOW_INCOMPATIBLE_COL_TYPE_CHANGES,\n            false)) {\n        // Throws InvalidOperationException if the new column types are not\n        // compatible with the current column types.\n        MetaStoreUtils.throwExceptionIfIncompatibleColTypeChange(\n            oldt.getSd().getCols(), newt.getSd().getCols());\n      }\n\n      //check that partition keys have not changed, except for virtual views\n      //however, allow the partition comments to change\n      boolean partKeysPartiallyEqual = checkPartialPartKeysEqual(oldt.getPartitionKeys(),\n          newt.getPartitionKeys());\n\n      if(!oldt.getTableType().equals(TableType.VIRTUAL_VIEW.toString())){\n        if (oldt.getPartitionKeys().size() != newt.getPartitionKeys().size()\n            || !partKeysPartiallyEqual) {\n          throw new InvalidOperationException(\n              \"partition keys can not be changed.\");\n        }\n      }\n\n      // if this alter is a rename, the table is not a virtual view, the user\n      // didn't change the default location (or new location is empty), and\n      // table is not an external table, that means user is asking metastore to\n      // move data to the new location corresponding to the new name\n      if (rename\n          && !oldt.getTableType().equals(TableType.VIRTUAL_VIEW.toString())\n          && (oldt.getSd().getLocation().compareTo(newt.getSd().getLocation()) == 0\n            || StringUtils.isEmpty(newt.getSd().getLocation()))\n          && !MetaStoreUtils.isExternalTable(oldt)) {\n\n        srcPath = new Path(oldt.getSd().getLocation());\n        srcFs = wh.getFs(srcPath);\n\n        // that means user is asking metastore to move data to new location\n        // corresponding to the new name\n        // get new location\n        Path databasePath = constructRenamedPath(\n            wh.getDefaultDatabasePath(newt.getDbName()), srcPath);\n        destPath = new Path(databasePath, newt.getTableName());\n        destFs = wh.getFs(destPath);\n\n        newt.getSd().setLocation(destPath.toString());\n        moveData = true;\n\n        // check that destination does not exist otherwise we will be\n        // overwriting data\n        // check that src and dest are on the same file system\n        if (!FileUtils.equalsFileSystem(srcFs, destFs)) {\n          throw new InvalidOperationException(\"table new location \" + destPath\n              + \" is on a different file system than the old location \"\n              + srcPath + \". This operation is not supported\");\n        }\n        try {\n          srcFs.exists(srcPath); // check that src exists and also checks\n                                 // permissions necessary\n          if (destFs.exists(destPath)) {\n            throw new InvalidOperationException(\"New location for this table \"\n                + newt.getDbName() + \".\" + newt.getTableName()\n                + \" already exists : \" + destPath);\n          }\n        } catch (IOException e) {\n          Warehouse.closeFs(srcFs);\n          Warehouse.closeFs(destFs);\n          throw new InvalidOperationException(\"Unable to access new location \"\n              + destPath + \" for table \" + newt.getDbName() + \".\"\n              + newt.getTableName());\n        }\n        String oldTblLocPath = srcPath.toUri().getPath();\n        String newTblLocPath = destPath.toUri().getPath();\n\n        // also the location field in partition\n        List<Partition> parts = msdb.getPartitions(dbname, name, -1);\n        for (Partition part : parts) {\n          String oldPartLoc = part.getSd().getLocation();\n          if (oldPartLoc.contains(oldTblLocPath)) {\n            URI oldUri = new Path(oldPartLoc).toUri();\n            String newPath = oldUri.getPath().replace(oldTblLocPath, newTblLocPath);\n            Path newPartLocPath = new Path(oldUri.getScheme(), oldUri.getAuthority(), newPath);\n            altps.add(ObjectPair.create(part, part.getSd().getLocation()));\n            part.getSd().setLocation(newPartLocPath.toString());\n            msdb.alterPartition(dbname, name, part.getValues(), part);\n          }\n        }\n      } else if (MetaStoreUtils.requireCalStats(hiveConf, null, null, newt) &&\n        (newt.getPartitionKeysSize() == 0)) {\n          Database db = msdb.getDatabase(newt.getDbName());\n          // Update table stats. For partitioned table, we update stats in\n          // alterPartition()\n          MetaStoreUtils.updateUnpartitionedTableStatsFast(db, newt, wh, false, true);\n      }\n      // now finally call alter table\n      msdb.alterTable(dbname, name, newt);\n      // commit the changes\n      success = msdb.commitTransaction();\n    } catch (InvalidObjectException e) {\n      LOG.debug(e);\n      throw new InvalidOperationException(\n          \"Unable to change partition or table.\"\n              + \" Check metastore logs for detailed stack.\" + e.getMessage());\n    } catch (NoSuchObjectException e) {\n      LOG.debug(e);\n      throw new InvalidOperationException(\n          \"Unable to change partition or table. Database \" + dbname + \" does not exist\"\n              + \" Check metastore logs for detailed stack.\" + e.getMessage());\n    } finally {\n      if (!success) {\n        msdb.rollbackTransaction();\n      }\n      if (success && moveData) {\n        // change the file name in hdfs\n        // check that src exists otherwise there is no need to copy the data\n        try {\n          if (srcFs.exists(srcPath)) {\n            // rename the src to destination\n            srcFs.rename(srcPath, destPath);\n          }\n        } catch (IOException e) {\n          boolean revertMetaDataTransaction = false;\n          try {\n            msdb.openTransaction();\n            msdb.alterTable(dbname, newt.getTableName(), oldt);\n            for (ObjectPair<Partition, String> pair : altps) {\n              Partition part = pair.getFirst();\n              part.getSd().setLocation(pair.getSecond());\n              msdb.alterPartition(dbname, name, part.getValues(), part);\n            }\n            revertMetaDataTransaction = msdb.commitTransaction();\n          } catch (Exception e1) {\n            // we should log this for manual rollback by administrator\n            LOG.error(\"Reverting metadata by HDFS operation failure failed During HDFS operation failed\", e1);\n            LOG.error(\"Table \" + Warehouse.getQualifiedName(newt) +\n                \" should be renamed to \" + Warehouse.getQualifiedName(oldt));\n            LOG.error(\"Table \" + Warehouse.getQualifiedName(newt) +\n                \" should have path \" + srcPath);\n            for (ObjectPair<Partition, String> pair : altps) {\n              LOG.error(\"Partition \" + Warehouse.getQualifiedName(pair.getFirst()) +\n                  \" should have path \" + pair.getSecond());\n            }\n            if (!revertMetaDataTransaction) {\n              msdb.rollbackTransaction();\n            }\n          }\n          throw new InvalidOperationException(\"Unable to access old location \"\n              + srcPath + \" for table \" + dbname + \".\" + name);\n        }\n      }\n    }\n    if (!success) {\n      throw new MetaException(\"Committing the alter table transaction was not successful.\");\n    }\n  }",
                "code_after_change": "  public void alterTable(RawStore msdb, Warehouse wh, String dbname,\n      String name, Table newt) throws InvalidOperationException, MetaException {\n    if (newt == null) {\n      throw new InvalidOperationException(\"New table is invalid: \" + newt);\n    }\n\n    if (!MetaStoreUtils.validateName(newt.getTableName())) {\n      throw new InvalidOperationException(newt.getTableName()\n          + \" is not a valid object name\");\n    }\n    String validate = MetaStoreUtils.validateTblColumns(newt.getSd().getCols());\n    if (validate != null) {\n      throw new InvalidOperationException(\"Invalid column \" + validate);\n    }\n\n    Path srcPath = null;\n    FileSystem srcFs = null;\n    Path destPath = null;\n    FileSystem destFs = null;\n\n    boolean success = false;\n    boolean moveData = false;\n    boolean rename = false;\n    Table oldt = null;\n    List<ObjectPair<Partition, String>> altps = new ArrayList<ObjectPair<Partition, String>>();\n\n    try {\n      msdb.openTransaction();\n      name = name.toLowerCase();\n      dbname = dbname.toLowerCase();\n\n      // check if table with the new name already exists\n      if (!newt.getTableName().equalsIgnoreCase(name)\n          || !newt.getDbName().equalsIgnoreCase(dbname)) {\n        if (msdb.getTable(newt.getDbName(), newt.getTableName()) != null) {\n          throw new InvalidOperationException(\"new table \" + newt.getDbName()\n              + \".\" + newt.getTableName() + \" already exists\");\n        }\n        rename = true;\n      }\n\n      // get old table\n      oldt = msdb.getTable(dbname, name);\n      if (oldt == null) {\n        throw new InvalidOperationException(\"table \" + newt.getDbName() + \".\"\n            + newt.getTableName() + \" doesn't exist\");\n      }\n\n      if (HiveConf.getBoolVar(hiveConf,\n            HiveConf.ConfVars.METASTORE_DISALLOW_INCOMPATIBLE_COL_TYPE_CHANGES,\n            false)) {\n        // Throws InvalidOperationException if the new column types are not\n        // compatible with the current column types.\n        MetaStoreUtils.throwExceptionIfIncompatibleColTypeChange(\n            oldt.getSd().getCols(), newt.getSd().getCols());\n      }\n\n      //check that partition keys have not changed, except for virtual views\n      //however, allow the partition comments to change\n      boolean partKeysPartiallyEqual = checkPartialPartKeysEqual(oldt.getPartitionKeys(),\n          newt.getPartitionKeys());\n\n      if(!oldt.getTableType().equals(TableType.VIRTUAL_VIEW.toString())){\n        if (oldt.getPartitionKeys().size() != newt.getPartitionKeys().size()\n            || !partKeysPartiallyEqual) {\n          throw new InvalidOperationException(\n              \"partition keys can not be changed.\");\n        }\n      }\n\n      // if this alter is a rename, the table is not a virtual view, the user\n      // didn't change the default location (or new location is empty), and\n      // table is not an external table, that means user is asking metastore to\n      // move data to the new location corresponding to the new name\n      if (rename\n          && !oldt.getTableType().equals(TableType.VIRTUAL_VIEW.toString())\n          && (oldt.getSd().getLocation().compareTo(newt.getSd().getLocation()) == 0\n            || StringUtils.isEmpty(newt.getSd().getLocation()))\n          && !MetaStoreUtils.isExternalTable(oldt)) {\n\n        srcPath = new Path(oldt.getSd().getLocation());\n        srcFs = wh.getFs(srcPath);\n\n        // that means user is asking metastore to move data to new location\n        // corresponding to the new name\n        // get new location\n        Path databasePath = constructRenamedPath(\n            wh.getDefaultDatabasePath(newt.getDbName()), srcPath);\n        destPath = new Path(databasePath, newt.getTableName());\n        destFs = wh.getFs(destPath);\n\n        newt.getSd().setLocation(destPath.toString());\n        moveData = true;\n\n        // check that destination does not exist otherwise we will be\n        // overwriting data\n        // check that src and dest are on the same file system\n        if (!FileUtils.equalsFileSystem(srcFs, destFs)) {\n          throw new InvalidOperationException(\"table new location \" + destPath\n              + \" is on a different file system than the old location \"\n              + srcPath + \". This operation is not supported\");\n        }\n        try {\n          srcFs.exists(srcPath); // check that src exists and also checks\n                                 // permissions necessary\n          if (destFs.exists(destPath)) {\n            throw new InvalidOperationException(\"New location for this table \"\n                + newt.getDbName() + \".\" + newt.getTableName()\n                + \" already exists : \" + destPath);\n          }\n        } catch (IOException e) {\n          Warehouse.closeFs(srcFs);\n          Warehouse.closeFs(destFs);\n          throw new InvalidOperationException(\"Unable to access new location \"\n              + destPath + \" for table \" + newt.getDbName() + \".\"\n              + newt.getTableName());\n        }\n        String oldTblLocPath = srcPath.toUri().getPath();\n        String newTblLocPath = destPath.toUri().getPath();\n\n        // also the location field in partition\n        List<Partition> parts = msdb.getPartitions(dbname, name, -1);\n        for (Partition part : parts) {\n          String oldPartLoc = part.getSd().getLocation();\n          if (oldPartLoc.contains(oldTblLocPath)) {\n            URI oldUri = new Path(oldPartLoc).toUri();\n            String newPath = oldUri.getPath().replace(oldTblLocPath, newTblLocPath);\n            Path newPartLocPath = new Path(oldUri.getScheme(), oldUri.getAuthority(), newPath);\n            altps.add(ObjectPair.create(part, part.getSd().getLocation()));\n            part.getSd().setLocation(newPartLocPath.toString());\n            msdb.alterPartition(dbname, name, part.getValues(), part);\n          }\n        }\n      } else if (MetaStoreUtils.requireCalStats(hiveConf, null, null, newt) &&\n        (newt.getPartitionKeysSize() == 0)) {\n          Database db = msdb.getDatabase(newt.getDbName());\n          // Update table stats. For partitioned table, we update stats in\n          // alterPartition()\n          MetaStoreUtils.updateUnpartitionedTableStatsFast(db, newt, wh, false, true);\n      }\n      // now finally call alter table\n      msdb.alterTable(dbname, name, newt);\n      // commit the changes\n      success = msdb.commitTransaction();\n    } catch (InvalidObjectException e) {\n      LOG.debug(e);\n      throw new InvalidOperationException(\n          \"Unable to change partition or table.\"\n              + \" Check metastore logs for detailed stack.\" + e.getMessage());\n    } catch (NoSuchObjectException e) {\n      LOG.debug(e);\n      throw new InvalidOperationException(\n          \"Unable to change partition or table. Database \" + dbname + \" does not exist\"\n              + \" Check metastore logs for detailed stack.\" + e.getMessage());\n    } finally {\n      if (!success) {\n        msdb.rollbackTransaction();\n      }\n      if (success && moveData) {\n        // change the file name in hdfs\n        // check that src exists otherwise there is no need to copy the data\n        // rename the src to destination\n        try {\n          if (srcFs.exists(srcPath) && !srcFs.rename(srcPath, destPath)) {\n            throw new IOException(\"Renaming \" + srcPath + \" to \" + destPath + \" is failed\");\n          }\n        } catch (IOException e) {\n          boolean revertMetaDataTransaction = false;\n          try {\n            msdb.openTransaction();\n            msdb.alterTable(dbname, newt.getTableName(), oldt);\n            for (ObjectPair<Partition, String> pair : altps) {\n              Partition part = pair.getFirst();\n              part.getSd().setLocation(pair.getSecond());\n              msdb.alterPartition(dbname, name, part.getValues(), part);\n            }\n            revertMetaDataTransaction = msdb.commitTransaction();\n          } catch (Exception e1) {\n            // we should log this for manual rollback by administrator\n            LOG.error(\"Reverting metadata by HDFS operation failure failed During HDFS operation failed\", e1);\n            LOG.error(\"Table \" + Warehouse.getQualifiedName(newt) +\n                \" should be renamed to \" + Warehouse.getQualifiedName(oldt));\n            LOG.error(\"Table \" + Warehouse.getQualifiedName(newt) +\n                \" should have path \" + srcPath);\n            for (ObjectPair<Partition, String> pair : altps) {\n              LOG.error(\"Partition \" + Warehouse.getQualifiedName(pair.getFirst()) +\n                  \" should have path \" + pair.getSecond());\n            }\n            if (!revertMetaDataTransaction) {\n              msdb.rollbackTransaction();\n            }\n          }\n          throw new InvalidOperationException(\"Unable to access old location \"\n              + srcPath + \" for table \" + dbname + \".\" + name);\n        }\n      }\n    }\n    if (!success) {\n      throw new MetaException(\"Committing the alter table transaction was not successful.\");\n    }\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.alterTable": {
                "code_before_change": "  public void alterTable(String dbname, String name, Table newTable)\n      throws InvalidObjectException, MetaException {\n    boolean success = false;\n    try {\n      openTransaction();\n      name = name.toLowerCase();\n      dbname = dbname.toLowerCase();\n      MTable newt = convertToMTable(newTable);\n      if (newt == null) {\n        throw new InvalidObjectException(\"new table is invalid\");\n      }\n\n      MTable oldt = getMTable(dbname, name);\n      if (oldt == null) {\n        throw new MetaException(\"table \" + name + \" doesn't exist\");\n      }\n\n      // For now only alter name, owner, paramters, cols, bucketcols are allowed\n      oldt.setTableName(newt.getTableName().toLowerCase());\n      oldt.setParameters(newt.getParameters());\n      oldt.setOwner(newt.getOwner());\n      // Fully copy over the contents of the new SD into the old SD,\n      // so we don't create an extra SD in the metastore db that has no references.\n      copyMSD(newt.getSd(), oldt.getSd());\n      oldt.setDatabase(newt.getDatabase());\n      oldt.setRetention(newt.getRetention());\n      oldt.setPartitionKeys(newt.getPartitionKeys());\n      oldt.setTableType(newt.getTableType());\n      oldt.setLastAccessTime(newt.getLastAccessTime());\n      oldt.setViewOriginalText(newt.getViewOriginalText());\n      oldt.setViewExpandedText(newt.getViewExpandedText());\n\n      // commit the changes\n      success = commitTransaction();\n    } finally {\n      if (!success) {\n        rollbackTransaction();\n      }\n    }\n  }",
                "code_after_change": "  public void alterTable(String dbname, String name, Table newTable)\n      throws InvalidObjectException, MetaException {\n    boolean success = false;\n    try {\n      openTransaction();\n      name = name.toLowerCase();\n      dbname = dbname.toLowerCase();\n      MTable newt = convertToMTable(newTable);\n      if (newt == null) {\n        throw new InvalidObjectException(\"new table is invalid\");\n      }\n\n      MTable oldt = getMTable(dbname, name);\n      if (oldt == null) {\n        throw new MetaException(\"table \" + name + \" doesn't exist\");\n      }\n\n      // For now only alter name, owner, paramters, cols, bucketcols are allowed\n      oldt.setDatabase(newt.getDatabase());\n      oldt.setTableName(newt.getTableName().toLowerCase());\n      oldt.setParameters(newt.getParameters());\n      oldt.setOwner(newt.getOwner());\n      // Fully copy over the contents of the new SD into the old SD,\n      // so we don't create an extra SD in the metastore db that has no references.\n      copyMSD(newt.getSd(), oldt.getSd());\n      oldt.setRetention(newt.getRetention());\n      oldt.setPartitionKeys(newt.getPartitionKeys());\n      oldt.setTableType(newt.getTableType());\n      oldt.setLastAccessTime(newt.getLastAccessTime());\n      oldt.setViewOriginalText(newt.getViewOriginalText());\n      oldt.setViewExpandedText(newt.getViewExpandedText());\n\n      // commit the changes\n      success = commitTransaction();\n    } finally {\n      if (!success) {\n        rollbackTransaction();\n      }\n    }\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Correct",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report precisely identifies the root cause by mentioning the method 'HiveAlterHandler#alterTable', which is one of the ground truth methods where the bug occurred. The fix suggestion is correct as it aligns with the developer's fix, which involves checking if the rename of the HDFS directory succeeds. The problem location identification is precise because the report directly mentions 'HiveAlterHandler#alterTable', which is a ground truth method. There is no wrong information in the bug report as all the details provided are relevant and accurate."
        }
    },
    {
        "filename": "HIVE-8735.json",
        "code_diff": {
            "ql.src.java.org.apache.hadoop.hive.ql.stats.jdbc.JDBCStatsPublisher.run": {
                "code_before_change": "      public Void run(PreparedStatement stmt) throws SQLException {\n        stmt.setQueryTimeout(timeout);\n        return null;\n      }",
                "code_after_change": "      public Void run(PreparedStatement stmt) throws SQLException {\n        stmt.setQueryTimeout(timeout);\n        return null;\n      }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.stats.jdbc.JDBCStatsUtils.getDeleteAggr": {
                "code_before_change": "  public static String getDeleteAggr(String rowID, String comment) {\n    String delete = \"DELETE /* \" + comment + \" */ \" +\n        \" FROM \" + getStatTableName() + \" WHERE \" + JDBCStatsUtils.getIdColumnName() +\n        \" LIKE ? ESCAPE ?\";\n    return delete;\n  }",
                "code_after_change": "  public static String getDeleteAggr(String rowID, String comment) {\n    String delete = \"DELETE /* \" + comment + \" */ \" +\n        \" FROM \" + getStatTableName() + \" WHERE \" + JDBCStatsUtils.getIdColumnName() +\n        \" LIKE ? ESCAPE ?\";\n    return delete;\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.stats.jdbc.JDBCStatsAggregator.run": {
                "code_before_change": "      public Void run(PreparedStatement stmt) throws SQLException {\n        stmt.setQueryTimeout(timeout);\n        return null;\n      }",
                "code_after_change": "      public Void run(PreparedStatement stmt) throws SQLException {\n        stmt.setQueryTimeout(timeout);\n        return null;\n      }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.stats.jdbc.JDBCStatsUtils.getBasicStat": {
                "code_before_change": "  public static String getBasicStat() {\n    return supportedStats.get(0);\n  }",
                "code_after_change": "  public static String getBasicStat() {\n    return supportedStats.get(0);\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.stats.jdbc.JDBCStatsPublisher.publishStat": {
                "code_before_change": "  public boolean publishStat(String fileID, Map<String, String> stats) {\n\n    if (stats.isEmpty()) {\n      // If there are no stats to publish, nothing to do.\n      return true;\n    }\n\n    if (conn == null) {\n      LOG.error(\"JDBC connection is null. Cannot publish stats without JDBC connection.\");\n      return false;\n    }\n\n    if (!JDBCStatsUtils.isValidStatisticSet(stats.keySet())) {\n      LOG.warn(\"Invalid statistic:\" + stats.keySet().toString() + \", supported \"\n          + \" stats: \" + JDBCStatsUtils.getSupportedStatistics());\n      return false;\n    }\n    LOG.info(\"Stats publishing for key \" + fileID);\n\n    Utilities.SQLCommand<Void> execUpdate = new Utilities.SQLCommand<Void>() {\n      @Override\n      public Void run(PreparedStatement stmt) throws SQLException {\n        stmt.executeUpdate();\n        return null;\n      }\n    };\n\n    List<String> supportedStatistics = JDBCStatsUtils.getSupportedStatistics();\n\n    for (int failures = 0;; failures++) {\n      try {\n        insStmt.setString(1, fileID);\n        for (int i = 0; i < JDBCStatsUtils.getSupportedStatistics().size(); i++) {\n          insStmt.setString(i + 2, stats.get(supportedStatistics.get(i)));\n        }\n        Utilities.executeWithRetry(execUpdate, insStmt, waitWindow, maxRetries);\n        return true;\n      } catch (SQLIntegrityConstraintViolationException e) {\n\n        // We assume that the table used for partial statistics has a primary key declared on the\n        // \"fileID\". The exception will be thrown if two tasks report results for the same fileID.\n        // In such case, we either update the row, or abandon changes depending on which statistic\n        // is newer.\n\n        for (int updateFailures = 0;; updateFailures++) {\n          try {\n            int i;\n            for (i = 0; i < JDBCStatsUtils.getSupportedStatistics().size(); i++) {\n              updStmt.setString(i + 1, stats.get(supportedStatistics.get(i)));\n            }\n            updStmt.setString(supportedStatistics.size() + 1, fileID);\n            updStmt.setString(supportedStatistics.size() + 2,\n                stats.get(JDBCStatsUtils.getBasicStat()));\n            updStmt.setString(supportedStatistics.size() + 3, fileID);\n            Utilities.executeWithRetry(execUpdate, updStmt, waitWindow, maxRetries);\n            return true;\n          } catch (SQLRecoverableException ue) {\n            // need to start from scratch (connection)\n            if (!handleSQLRecoverableException(ue, updateFailures)) {\n              return false;\n            }\n          } catch (SQLException ue) {\n            LOG.error(\"Error during publishing statistics. \", e);\n            return false;\n          }\n        }\n\n      } catch (SQLRecoverableException e) {\n        // need to start from scratch (connection)\n        if (!handleSQLRecoverableException(e, failures)) {\n          return false;\n        }\n      } catch (SQLException e) {\n        LOG.error(\"Error during publishing statistics. \", e);\n        return false;\n      }\n    }\n  }",
                "code_after_change": "  public boolean publishStat(String fileID, Map<String, String> stats) {\n\n    if (stats.isEmpty()) {\n      // If there are no stats to publish, nothing to do.\n      return true;\n    }\n\n    if (conn == null) {\n      LOG.error(\"JDBC connection is null. Cannot publish stats without JDBC connection.\");\n      return false;\n    }\n\n    if (!JDBCStatsUtils.isValidStatisticSet(stats.keySet())) {\n      LOG.warn(\"Invalid statistic:\" + stats.keySet().toString() + \", supported \"\n          + \" stats: \" + JDBCStatsUtils.getSupportedStatistics());\n      return false;\n    }\n    String rowId = JDBCStatsUtils.truncateRowId(fileID);\n    if (LOG.isInfoEnabled()) {\n      String truncateSuffix = (rowId != fileID) ? \" (from \" + fileID + \")\" : \"\"; // object equality\n      LOG.info(\"Stats publishing for key \" + rowId + truncateSuffix);\n    }\n\n    Utilities.SQLCommand<Void> execUpdate = new Utilities.SQLCommand<Void>() {\n      @Override\n      public Void run(PreparedStatement stmt) throws SQLException {\n        stmt.executeUpdate();\n        return null;\n      }\n    };\n\n    List<String> supportedStatistics = JDBCStatsUtils.getSupportedStatistics();\n\n    for (int failures = 0;; failures++) {\n      try {\n        insStmt.setString(1, rowId);\n        for (int i = 0; i < JDBCStatsUtils.getSupportedStatistics().size(); i++) {\n          insStmt.setString(i + 2, stats.get(supportedStatistics.get(i)));\n        }\n        Utilities.executeWithRetry(execUpdate, insStmt, waitWindow, maxRetries);\n        return true;\n      } catch (SQLIntegrityConstraintViolationException e) {\n\n        // We assume that the table used for partial statistics has a primary key declared on the\n        // \"fileID\". The exception will be thrown if two tasks report results for the same fileID.\n        // In such case, we either update the row, or abandon changes depending on which statistic\n        // is newer.\n\n        for (int updateFailures = 0;; updateFailures++) {\n          try {\n            int i;\n            for (i = 0; i < JDBCStatsUtils.getSupportedStatistics().size(); i++) {\n              updStmt.setString(i + 1, stats.get(supportedStatistics.get(i)));\n            }\n            updStmt.setString(supportedStatistics.size() + 1, rowId);\n            updStmt.setString(supportedStatistics.size() + 2,\n                stats.get(JDBCStatsUtils.getBasicStat()));\n            updStmt.setString(supportedStatistics.size() + 3, rowId);\n            Utilities.executeWithRetry(execUpdate, updStmt, waitWindow, maxRetries);\n            return true;\n          } catch (SQLRecoverableException ue) {\n            // need to start from scratch (connection)\n            if (!handleSQLRecoverableException(ue, updateFailures)) {\n              return false;\n            }\n          } catch (SQLException ue) {\n            LOG.error(\"Error during publishing statistics. \", e);\n            return false;\n          }\n        }\n\n      } catch (SQLRecoverableException e) {\n        // need to start from scratch (connection)\n        if (!handleSQLRecoverableException(e, failures)) {\n          return false;\n        }\n      } catch (SQLException e) {\n        LOG.error(\"Error during publishing statistics. \", e);\n        return false;\n      }\n    }\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Buggy Method"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Buggy Method"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the method 'JDBCStatsPublisher.publishStat' in the stack trace, which is where the error occurs, but not where the actual fix was made. Therefore, it is classified as 'Partial' under 'Buggy Method' for both root cause and problem location identification. There is no fix suggestion provided in the bug report, so it is marked as 'Missing'. All information in the bug report is relevant to the context of the bug, so there is no wrong information."
        }
    },
    {
        "filename": "HIVE-13209.json",
        "code_diff": {
            "metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.initialValue": {
                "code_before_change": "    protected DateFormat initialValue() {\n      DateFormat val = new SimpleDateFormat(\"yyyy-MM-dd\");\n      val.setLenient(false); // Without this, 2020-20-20 becomes 2021-08-20.\n      return val;\n    };",
                "code_after_change": "    protected DateFormat initialValue() {\n      DateFormat val = new SimpleDateFormat(\"yyyy-MM-dd\");\n      val.setLenient(false); // Without this, 2020-20-20 becomes 2021-08-20.\n      return val;\n    };"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.get_delegation_token": {
                "code_before_change": "    public String get_delegation_token(String token_owner,\n        String renewer_kerberos_principal_name)\n        throws MetaException, TException {\n      startFunction(\"get_delegation_token\");\n      String ret = null;\n      Exception ex = null;\n      try {\n        ret =\n            HiveMetaStore.getDelegationToken(token_owner,\n                renewer_kerberos_principal_name, getIpAddress());\n      } catch (IOException e) {\n        ex = e;\n        throw new MetaException(e.getMessage());\n      } catch (InterruptedException e) {\n        ex = e;\n        throw new MetaException(e.getMessage());\n      } catch (Exception e) {\n        ex = e;\n        if (e instanceof MetaException) {\n          throw (MetaException) e;\n        } else if (e instanceof TException) {\n          throw (TException) e;\n        } else {\n          throw newMetaException(e);\n        }\n      } finally {\n        endFunction(\"get_delegation_token\", ret != null, ex);\n      }\n      return ret;\n    }",
                "code_after_change": "    public String get_delegation_token(String token_owner,\n        String renewer_kerberos_principal_name)\n        throws MetaException, TException {\n      startFunction(\"get_delegation_token\");\n      String ret = null;\n      Exception ex = null;\n      try {\n        ret =\n            HiveMetaStore.getDelegationToken(token_owner,\n                renewer_kerberos_principal_name, getIPAddress());\n      } catch (IOException e) {\n        ex = e;\n        throw new MetaException(e.getMessage());\n      } catch (InterruptedException e) {\n        ex = e;\n        throw new MetaException(e.getMessage());\n      } catch (Exception e) {\n        ex = e;\n        if (e instanceof MetaException) {\n          throw (MetaException) e;\n        } else if (e instanceof TException) {\n          throw (TException) e;\n        } else {\n          throw newMetaException(e);\n        }\n      } finally {\n        endFunction(\"get_delegation_token\", ret != null, ex);\n      }\n      return ret;\n    }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.logInfo": {
                "code_before_change": "    private void logInfo(String m) {\n      LOG.info(threadLocalId.get().toString() + \": \" + m);\n      logAuditEvent(m);\n    }",
                "code_after_change": "    private void logInfo(String m) {\n      LOG.info(threadLocalId.get().toString() + \": \" + m);\n      logAuditEvent(m);\n    }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.logAuditEvent": {
                "code_before_change": "    private final void logAuditEvent(String cmd) {\n      if (cmd == null) {\n        return;\n      }\n\n      UserGroupInformation ugi;\n      try {\n        ugi = Utils.getUGI();\n      } catch (Exception ex) {\n        throw new RuntimeException(ex);\n      }\n      final Formatter fmt = auditFormatter.get();\n      ((StringBuilder) fmt.out()).setLength(0);\n\n      String address = null;\n      if (useSasl) {\n        if (saslServer != null && saslServer.getRemoteAddress() != null) {\n          address = String.valueOf(saslServer.getRemoteAddress());\n        }\n      } else {\n        address = getIpAddress();\n      }\n      if (address == null) {\n        address = \"unknown-ip-addr\";\n      }\n\n      auditLog.info(fmt.format(AUDIT_FORMAT, ugi.getUserName(),\n          address, cmd).toString());\n    }",
                "code_after_change": "    private final void logAuditEvent(String cmd) {\n      if (cmd == null) {\n        return;\n      }\n\n      UserGroupInformation ugi;\n      try {\n        ugi = Utils.getUGI();\n      } catch (Exception ex) {\n        throw new RuntimeException(ex);\n      }\n      final Formatter fmt = auditFormatter.get();\n      ((StringBuilder) fmt.out()).setLength(0);\n\n      String address = getIPAddress();\n      if (address == null) {\n        address = \"unknown-ip-addr\";\n      }\n\n      auditLog.info(fmt.format(AUDIT_FORMAT, ugi.getUserName(),\n          address, cmd).toString());\n    }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.TSetIpAddressProcessor.setIpAddress": {
                "code_before_change": "  protected void setIpAddress(final TProtocol in) {\n    TTransport transport = in.getTransport();\n    if (!(transport instanceof TSocket)) {\n      return;\n    }\n    setIpAddress(((TSocket)transport).getSocket());\n  }",
                "code_after_change": "  protected void setIpAddress(final TProtocol in) {\n    TTransport transport = in.getTransport();\n    if (!(transport instanceof TSocket)) {\n      return;\n    }\n    setIpAddress(((TSocket)transport).getSocket());\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the issue occurring in the 'get_delegation_token' method, which is part of the stack trace and is a ground truth method. However, it does not precisely identify the root cause, which involves the incorrect method call 'getIpAddress()' instead of 'getIPAddress()'. The report does not provide any fix suggestion, hence 'Missing' for fix suggestion. The problem location is partially identified as it mentions 'get_delegation_token' in the stack trace, which is a ground truth method. There is no wrong information in the bug report as it correctly describes the error context."
        }
    },
    {
        "filename": "HIVE-13065.json",
        "code_diff": {
            "hbase-handler.src.java.org.apache.hadoop.hive.hbase.HBaseRowSerializer.serialize": {
                "code_before_change": "  public Writable serialize(Object obj, ObjectInspector objInspector) throws Exception {\n    if (objInspector.getCategory() != ObjectInspector.Category.STRUCT) {\n      throw new SerDeException(getClass().toString()\n          + \" can only serialize struct types, but we got: \"\n          + objInspector.getTypeName());\n    }\n\n    // Prepare the field ObjectInspectors\n    StructObjectInspector soi = (StructObjectInspector) objInspector;\n    List<? extends StructField> fields = soi.getAllStructFieldRefs();\n    List<Object> values = soi.getStructFieldsDataAsList(obj);\n\n    StructField field = fields.get(keyIndex);\n    Object value = values.get(keyIndex);\n\n    byte[] key = keyFactory.serializeKey(value, field);\n    if (key == null) {\n      throw new SerDeException(\"HBase row key cannot be NULL\");\n    }\n    long timestamp = putTimestamp;\n    if (timestamp < 0 && timestampIndex >= 0) {\n      ObjectInspector inspector = fields.get(timestampIndex).getFieldObjectInspector();\n      value = values.get(timestampIndex);\n      if (inspector instanceof LongObjectInspector) {\n        timestamp = ((LongObjectInspector)inspector).get(value);\n      } else {\n        PrimitiveObjectInspector primitive = (PrimitiveObjectInspector) inspector;\n        timestamp = PrimitiveObjectInspectorUtils.getTimestamp(value, primitive).getTime();\n      }\n    }\n\n    Put put = timestamp >= 0 ? new Put(key, timestamp) : new Put(key);\n\n    // Serialize each field\n    for (int i = 0; i < fields.size(); i++) {\n      if (i == keyIndex || i == timestampIndex) {\n        continue;\n      }\n      field = fields.get(i);\n      value = values.get(i);\n      serializeField(value, field, columnMappings[i], put);\n    }\n\n    return new PutWritable(put);\n  }",
                "code_after_change": "  public Writable serialize(Object obj, ObjectInspector objInspector) throws Exception {\n    if (objInspector.getCategory() != ObjectInspector.Category.STRUCT) {\n      throw new SerDeException(getClass().toString()\n          + \" can only serialize struct types, but we got: \"\n          + objInspector.getTypeName());\n    }\n\n    // Prepare the field ObjectInspectors\n    StructObjectInspector soi = (StructObjectInspector) objInspector;\n    List<? extends StructField> fields = soi.getAllStructFieldRefs();\n    List<Object> values = soi.getStructFieldsDataAsList(obj);\n\n    StructField field = fields.get(keyIndex);\n    Object value = values.get(keyIndex);\n\n    byte[] key = keyFactory.serializeKey(value, field);\n    if (key == null) {\n      throw new SerDeException(\"HBase row key cannot be NULL\");\n    }\n    long timestamp = putTimestamp;\n    if (timestamp < 0 && timestampIndex >= 0) {\n      ObjectInspector inspector = fields.get(timestampIndex).getFieldObjectInspector();\n      value = values.get(timestampIndex);\n      if (inspector instanceof LongObjectInspector) {\n        timestamp = ((LongObjectInspector)inspector).get(value);\n      } else {\n        PrimitiveObjectInspector primitive = (PrimitiveObjectInspector) inspector;\n        timestamp = PrimitiveObjectInspectorUtils.getTimestamp(value, primitive).getTime();\n      }\n    }\n\n    Put put = timestamp >= 0 ? new Put(key, timestamp) : new Put(key);\n\n    // Serialize each field\n    for (int i = 0; i < fields.size(); i++) {\n      if (i == keyIndex || i == timestampIndex) {\n        continue;\n      }\n      field = fields.get(i);\n      value = values.get(i);\n      serializeField(value, field, columnMappings[i], put);\n    }\n\n    return new PutWritable(put);\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the issue as a NullPointerException occurring when writing map type data with null values to an HBase backed table. The stack trace in the description points to methods like `HBaseSerDe.serialize` and `LazyUtils.writePrimitiveUTF8`, which are in the same stack trace context as the ground truth method `HBaseRowSerializer.serialize`. However, it does not precisely identify the root cause at the ground truth method. There is no fix suggestion provided in the bug report. The problem location is partially identified through the stack trace context, but not precisely at the ground truth method. There is no wrong information in the bug report as all details are relevant to the context of the bug."
        }
    },
    {
        "filename": "HIVE-11470.json",
        "code_diff": {
            "hcatalog.core.src.main.java.org.apache.hive.hcatalog.mapreduce.DynamicPartitionFileRecordWriterContainer.getLocalFileWriter": {
                "code_before_change": "  protected LocalFileWriter getLocalFileWriter(HCatRecord value) throws IOException, HCatException {\n    OutputJobInfo localJobInfo = null;\n    // Calculate which writer to use from the remaining values - this needs to\n    // be done before we delete cols.\n    List<String> dynamicPartValues = new ArrayList<String>();\n    for (Integer colToAppend : dynamicPartCols) {\n      dynamicPartValues.add(value.get(colToAppend).toString());\n    }\n\n    String dynKey = dynamicPartValues.toString();\n    if (!baseDynamicWriters.containsKey(dynKey)) {\n      if ((maxDynamicPartitions != -1) && (baseDynamicWriters.size() > maxDynamicPartitions)) {\n        throw new HCatException(ErrorType.ERROR_TOO_MANY_DYNAMIC_PTNS,\n            \"Number of dynamic partitions being created \"\n                + \"exceeds configured max allowable partitions[\" + maxDynamicPartitions\n                + \"], increase parameter [\" + HiveConf.ConfVars.DYNAMICPARTITIONMAXPARTS.varname\n                + \"] if needed.\");\n      }\n\n      org.apache.hadoop.mapred.TaskAttemptContext currTaskContext =\n          HCatMapRedUtil.createTaskAttemptContext(context);\n      configureDynamicStorageHandler(currTaskContext, dynamicPartValues);\n      localJobInfo = HCatBaseOutputFormat.getJobInfo(currTaskContext.getConfiguration());\n\n      // Setup serDe.\n      SerDe currSerDe =\n          ReflectionUtils.newInstance(storageHandler.getSerDeClass(), currTaskContext.getJobConf());\n      try {\n        InternalUtil.initializeOutputSerDe(currSerDe, currTaskContext.getConfiguration(),\n            localJobInfo);\n      } catch (SerDeException e) {\n        throw new IOException(\"Failed to initialize SerDe\", e);\n      }\n\n      // create base OutputFormat\n      org.apache.hadoop.mapred.OutputFormat baseOF =\n          ReflectionUtils.newInstance(storageHandler.getOutputFormatClass(),\n              currTaskContext.getJobConf());\n\n      // We are skipping calling checkOutputSpecs() for each partition\n      // As it can throw a FileAlreadyExistsException when more than one\n      // mapper is writing to a partition.\n      // See HCATALOG-490, also to avoid contacting the namenode for each new\n      // FileOutputFormat instance.\n      // In general this should be ok for most FileOutputFormat implementations\n      // but may become an issue for cases when the method is used to perform\n      // other setup tasks.\n\n      // Get Output Committer\n      org.apache.hadoop.mapred.OutputCommitter baseOutputCommitter =\n          currTaskContext.getJobConf().getOutputCommitter();\n\n      // Create currJobContext the latest so it gets all the config changes\n      org.apache.hadoop.mapred.JobContext currJobContext =\n          HCatMapRedUtil.createJobContext(currTaskContext);\n\n      // Set up job.\n      baseOutputCommitter.setupJob(currJobContext);\n\n      // Recreate to refresh jobConf of currTask context.\n      currTaskContext =\n          HCatMapRedUtil.createTaskAttemptContext(currJobContext.getJobConf(),\n              currTaskContext.getTaskAttemptID(), currTaskContext.getProgressible());\n\n      // Set temp location.\n      currTaskContext.getConfiguration().set(\n          \"mapred.work.output.dir\",\n          new FileOutputCommitter(new Path(localJobInfo.getLocation()), currTaskContext)\n              .getWorkPath().toString());\n\n      // Set up task.\n      baseOutputCommitter.setupTask(currTaskContext);\n\n      Path parentDir = new Path(currTaskContext.getConfiguration().get(\"mapred.work.output.dir\"));\n      Path childPath =\n          new Path(parentDir, FileOutputFormat.getUniqueFile(currTaskContext, \"part\", \"\"));\n\n      RecordWriter baseRecordWriter =\n          baseOF.getRecordWriter(parentDir.getFileSystem(currTaskContext.getConfiguration()),\n              currTaskContext.getJobConf(), childPath.toString(),\n              InternalUtil.createReporter(currTaskContext));\n\n      baseDynamicWriters.put(dynKey, baseRecordWriter);\n      baseDynamicSerDe.put(dynKey, currSerDe);\n      baseDynamicCommitters.put(dynKey, baseOutputCommitter);\n      dynamicContexts.put(dynKey, currTaskContext);\n      dynamicObjectInspectors.put(dynKey,\n          InternalUtil.createStructObjectInspector(jobInfo.getOutputSchema()));\n      dynamicOutputJobInfo.put(dynKey,\n          HCatOutputFormat.getJobInfo(dynamicContexts.get(dynKey).getConfiguration()));\n    }\n\n    return new LocalFileWriter(baseDynamicWriters.get(dynKey), dynamicObjectInspectors.get(dynKey),\n        baseDynamicSerDe.get(dynKey), dynamicOutputJobInfo.get(dynKey));\n  }",
                "code_after_change": "  protected LocalFileWriter getLocalFileWriter(HCatRecord value) throws IOException, HCatException {\n    OutputJobInfo localJobInfo = null;\n    // Calculate which writer to use from the remaining values - this needs to\n    // be done before we delete cols.\n    List<String> dynamicPartValues = new ArrayList<String>();\n    for (Integer colToAppend : dynamicPartCols) {\n      Object partitionValue = value.get(colToAppend);\n      dynamicPartValues.add(partitionValue == null? HIVE_DEFAULT_PARTITION_VALUE : partitionValue.toString());\n    }\n\n    String dynKey = dynamicPartValues.toString();\n    if (!baseDynamicWriters.containsKey(dynKey)) {\n      if ((maxDynamicPartitions != -1) && (baseDynamicWriters.size() > maxDynamicPartitions)) {\n        throw new HCatException(ErrorType.ERROR_TOO_MANY_DYNAMIC_PTNS,\n            \"Number of dynamic partitions being created \"\n                + \"exceeds configured max allowable partitions[\" + maxDynamicPartitions\n                + \"], increase parameter [\" + HiveConf.ConfVars.DYNAMICPARTITIONMAXPARTS.varname\n                + \"] if needed.\");\n      }\n\n      org.apache.hadoop.mapred.TaskAttemptContext currTaskContext =\n          HCatMapRedUtil.createTaskAttemptContext(context);\n      configureDynamicStorageHandler(currTaskContext, dynamicPartValues);\n      localJobInfo = HCatBaseOutputFormat.getJobInfo(currTaskContext.getConfiguration());\n\n      // Setup serDe.\n      SerDe currSerDe =\n          ReflectionUtils.newInstance(storageHandler.getSerDeClass(), currTaskContext.getJobConf());\n      try {\n        InternalUtil.initializeOutputSerDe(currSerDe, currTaskContext.getConfiguration(),\n            localJobInfo);\n      } catch (SerDeException e) {\n        throw new IOException(\"Failed to initialize SerDe\", e);\n      }\n\n      // create base OutputFormat\n      org.apache.hadoop.mapred.OutputFormat baseOF =\n          ReflectionUtils.newInstance(storageHandler.getOutputFormatClass(),\n              currTaskContext.getJobConf());\n\n      // We are skipping calling checkOutputSpecs() for each partition\n      // As it can throw a FileAlreadyExistsException when more than one\n      // mapper is writing to a partition.\n      // See HCATALOG-490, also to avoid contacting the namenode for each new\n      // FileOutputFormat instance.\n      // In general this should be ok for most FileOutputFormat implementations\n      // but may become an issue for cases when the method is used to perform\n      // other setup tasks.\n\n      // Get Output Committer\n      org.apache.hadoop.mapred.OutputCommitter baseOutputCommitter =\n          currTaskContext.getJobConf().getOutputCommitter();\n\n      // Create currJobContext the latest so it gets all the config changes\n      org.apache.hadoop.mapred.JobContext currJobContext =\n          HCatMapRedUtil.createJobContext(currTaskContext);\n\n      // Set up job.\n      baseOutputCommitter.setupJob(currJobContext);\n\n      // Recreate to refresh jobConf of currTask context.\n      currTaskContext =\n          HCatMapRedUtil.createTaskAttemptContext(currJobContext.getJobConf(),\n              currTaskContext.getTaskAttemptID(), currTaskContext.getProgressible());\n\n      // Set temp location.\n      currTaskContext.getConfiguration().set(\n          \"mapred.work.output.dir\",\n          new FileOutputCommitter(new Path(localJobInfo.getLocation()), currTaskContext)\n              .getWorkPath().toString());\n\n      // Set up task.\n      baseOutputCommitter.setupTask(currTaskContext);\n\n      Path parentDir = new Path(currTaskContext.getConfiguration().get(\"mapred.work.output.dir\"));\n      Path childPath =\n          new Path(parentDir, FileOutputFormat.getUniqueFile(currTaskContext,\n              currTaskContext.getConfiguration().get(\"mapreduce.output.basename\", \"part\"), \"\"));\n\n      RecordWriter baseRecordWriter =\n          baseOF.getRecordWriter(parentDir.getFileSystem(currTaskContext.getConfiguration()),\n              currTaskContext.getJobConf(), childPath.toString(),\n              InternalUtil.createReporter(currTaskContext));\n\n      baseDynamicWriters.put(dynKey, baseRecordWriter);\n      baseDynamicSerDe.put(dynKey, currSerDe);\n      baseDynamicCommitters.put(dynKey, baseOutputCommitter);\n      dynamicContexts.put(dynKey, currTaskContext);\n      dynamicObjectInspectors.put(dynKey,\n          InternalUtil.createStructObjectInspector(jobInfo.getOutputSchema()));\n      dynamicOutputJobInfo.put(dynKey,\n          HCatOutputFormat.getJobInfo(dynamicContexts.get(dynKey).getConfiguration()));\n    }\n\n    return new LocalFileWriter(baseDynamicWriters.get(dynKey), dynamicObjectInspectors.get(dynKey),\n        baseDynamicSerDe.get(dynKey), dynamicOutputJobInfo.get(dynKey));\n  }"
            },
            "hcatalog.core.src.main.java.org.apache.hive.hcatalog.mapreduce.DynamicPartitionFileRecordWriterContainer.DynamicPartitionFileRecordWriterContainer": {
                "code_before_change": [],
                "code_after_change": []
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Correct",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report precisely identifies the root cause in the method 'DynamicPartitionFileRecordWriterContainer.getLocalFileWriter', which is part of the ground truth methods. The report suggests checking for null and substituting with a default value, which matches the developer's fix. The problem location is also precisely identified as it directly points to the method where the fix was made. There is no wrong information in the bug report as all details are relevant and accurate."
        }
    },
    {
        "filename": "HIVE-12476.json",
        "code_diff": {
            "metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getPartitionsFromPartitionIds": {
                "code_before_change": "  private List<Partition> getPartitionsFromPartitionIds(String dbName, String tblName,\n      Boolean isView, List<Object> partIdList) throws MetaException {\n    boolean doTrace = LOG.isDebugEnabled();\n    int idStringWidth = (int)Math.ceil(Math.log10(partIdList.size())) + 1; // 1 for comma\n    int sbCapacity = partIdList.size() * idStringWidth;\n    // Prepare StringBuilder for \"PART_ID in (...)\" to use in future queries.\n    StringBuilder partSb = new StringBuilder(sbCapacity);\n    for (Object partitionId : partIdList) {\n      partSb.append(extractSqlLong(partitionId)).append(\",\");\n    }\n    String partIds = trimCommaList(partSb);\n\n    // Get most of the fields for the IDs provided.\n    // Assume db and table names are the same for all partition, as provided in arguments.\n    String queryText =\n      \"select \\\"PARTITIONS\\\".\\\"PART_ID\\\", \\\"SDS\\\".\\\"SD_ID\\\", \\\"SDS\\\".\\\"CD_ID\\\",\"\n    + \" \\\"SERDES\\\".\\\"SERDE_ID\\\", \\\"PARTITIONS\\\".\\\"CREATE_TIME\\\",\"\n    + \" \\\"PARTITIONS\\\".\\\"LAST_ACCESS_TIME\\\", \\\"SDS\\\".\\\"INPUT_FORMAT\\\", \\\"SDS\\\".\\\"IS_COMPRESSED\\\",\"\n    + \" \\\"SDS\\\".\\\"IS_STOREDASSUBDIRECTORIES\\\", \\\"SDS\\\".\\\"LOCATION\\\", \\\"SDS\\\".\\\"NUM_BUCKETS\\\",\"\n    + \" \\\"SDS\\\".\\\"OUTPUT_FORMAT\\\", \\\"SERDES\\\".\\\"NAME\\\", \\\"SERDES\\\".\\\"SLIB\\\" \"\n    + \"from \\\"PARTITIONS\\\"\"\n    + \"  left outer join \\\"SDS\\\" on \\\"PARTITIONS\\\".\\\"SD_ID\\\" = \\\"SDS\\\".\\\"SD_ID\\\" \"\n    + \"  left outer join \\\"SERDES\\\" on \\\"SDS\\\".\\\"SERDE_ID\\\" = \\\"SERDES\\\".\\\"SERDE_ID\\\" \"\n    + \"where \\\"PART_ID\\\" in (\" + partIds + \") order by \\\"PART_NAME\\\" asc\";\n    long start = doTrace ? System.nanoTime() : 0;\n    Query query = pm.newQuery(\"javax.jdo.query.SQL\", queryText);\n    List<Object[]> sqlResult = executeWithArray(query, null, queryText);\n    long queryTime = doTrace ? System.nanoTime() : 0;\n    Deadline.checkTimeout();\n\n    // Read all the fields and create partitions, SDs and serdes.\n    TreeMap<Long, Partition> partitions = new TreeMap<Long, Partition>();\n    TreeMap<Long, StorageDescriptor> sds = new TreeMap<Long, StorageDescriptor>();\n    TreeMap<Long, SerDeInfo> serdes = new TreeMap<Long, SerDeInfo>();\n    TreeMap<Long, List<FieldSchema>> colss = new TreeMap<Long, List<FieldSchema>>();\n    // Keep order by name, consistent with JDO.\n    ArrayList<Partition> orderedResult = new ArrayList<Partition>(partIdList.size());\n\n    // Prepare StringBuilder-s for \"in (...)\" lists to use in one-to-many queries.\n    StringBuilder sdSb = new StringBuilder(sbCapacity), serdeSb = new StringBuilder(sbCapacity);\n    StringBuilder colsSb = new StringBuilder(7); // We expect that there's only one field schema.\n    tblName = tblName.toLowerCase();\n    dbName = dbName.toLowerCase();\n    for (Object[] fields : sqlResult) {\n      // Here comes the ugly part...\n      long partitionId = extractSqlLong(fields[0]);\n      Long sdId = extractSqlLong(fields[1]);\n      Long colId = extractSqlLong(fields[2]);\n      Long serdeId = extractSqlLong(fields[3]);\n      // A partition must have either everything set, or nothing set if it's a view.\n      if (sdId == null || colId == null || serdeId == null) {\n        if (isView == null) {\n          isView = isViewTable(dbName, tblName);\n        }\n        if ((sdId != null || colId != null || serdeId != null) || !isView) {\n          throw new MetaException(\"Unexpected null for one of the IDs, SD \" + sdId + \", column \"\n              + colId + \", serde \" + serdeId + \" for a \" + (isView ? \"\" : \"non-\") + \" view\");\n        }\n      }\n\n      Partition part = new Partition();\n      orderedResult.add(part);\n      // Set the collection fields; some code might not check presence before accessing them.\n      part.setParameters(new HashMap<String, String>());\n      part.setValues(new ArrayList<String>());\n      part.setDbName(dbName);\n      part.setTableName(tblName);\n      if (fields[4] != null) part.setCreateTime(extractSqlInt(fields[4]));\n      if (fields[5] != null) part.setLastAccessTime(extractSqlInt(fields[5]));\n      partitions.put(partitionId, part);\n\n      if (sdId == null) continue; // Probably a view.\n      assert colId != null && serdeId != null;\n\n      // We assume each partition has an unique SD.\n      StorageDescriptor sd = new StorageDescriptor();\n      StorageDescriptor oldSd = sds.put(sdId, sd);\n      if (oldSd != null) {\n        throw new MetaException(\"Partitions reuse SDs; we don't expect that\");\n      }\n      // Set the collection fields; some code might not check presence before accessing them.\n      sd.setSortCols(new ArrayList<Order>());\n      sd.setBucketCols(new ArrayList<String>());\n      sd.setParameters(new HashMap<String, String>());\n      sd.setSkewedInfo(new SkewedInfo(new ArrayList<String>(),\n          new ArrayList<List<String>>(), new HashMap<List<String>, String>()));\n      sd.setInputFormat((String)fields[6]);\n      Boolean tmpBoolean = extractSqlBoolean(fields[7]);\n      if (tmpBoolean != null) sd.setCompressed(tmpBoolean);\n      tmpBoolean = extractSqlBoolean(fields[8]);\n      if (tmpBoolean != null) sd.setStoredAsSubDirectories(tmpBoolean);\n      sd.setLocation((String)fields[9]);\n      if (fields[10] != null) sd.setNumBuckets(extractSqlInt(fields[10]));\n      sd.setOutputFormat((String)fields[11]);\n      sdSb.append(sdId).append(\",\");\n      part.setSd(sd);\n\n      List<FieldSchema> cols = colss.get(colId);\n      // We expect that colId will be the same for all (or many) SDs.\n      if (cols == null) {\n        cols = new ArrayList<FieldSchema>();\n        colss.put(colId, cols);\n        colsSb.append(colId).append(\",\");\n      }\n      sd.setCols(cols);\n\n      // We assume each SD has an unique serde.\n      SerDeInfo serde = new SerDeInfo();\n      SerDeInfo oldSerde = serdes.put(serdeId, serde);\n      if (oldSerde != null) {\n        throw new MetaException(\"SDs reuse serdes; we don't expect that\");\n      }\n      serde.setParameters(new HashMap<String, String>());\n      serde.setName((String)fields[12]);\n      serde.setSerializationLib((String)fields[13]);\n      serdeSb.append(serdeId).append(\",\");\n      sd.setSerdeInfo(serde);\n      Deadline.checkTimeout();\n    }\n    query.closeAll();\n    timingTrace(doTrace, queryText, start, queryTime);\n\n    // Now get all the one-to-many things. Start with partitions.\n    queryText = \"select \\\"PART_ID\\\", \\\"PARAM_KEY\\\", \\\"PARAM_VALUE\\\" from \\\"PARTITION_PARAMS\\\"\"\n        + \" where \\\"PART_ID\\\" in (\" + partIds + \") and \\\"PARAM_KEY\\\" is not null\"\n        + \" order by \\\"PART_ID\\\" asc\";\n    loopJoinOrderedResult(partitions, queryText, 0, new ApplyFunc<Partition>() {\n      @Override\n      public void apply(Partition t, Object[] fields) {\n        t.putToParameters((String)fields[1], (String)fields[2]);\n      }});\n\n    queryText = \"select \\\"PART_ID\\\", \\\"PART_KEY_VAL\\\" from \\\"PARTITION_KEY_VALS\\\"\"\n        + \" where \\\"PART_ID\\\" in (\" + partIds + \") and \\\"INTEGER_IDX\\\" >= 0\"\n        + \" order by \\\"PART_ID\\\" asc, \\\"INTEGER_IDX\\\" asc\";\n    loopJoinOrderedResult(partitions, queryText, 0, new ApplyFunc<Partition>() {\n      @Override\n      public void apply(Partition t, Object[] fields) {\n        t.addToValues((String)fields[1]);\n      }});\n\n    // Prepare IN (blah) lists for the following queries. Cut off the final ','s.\n    if (sdSb.length() == 0) {\n      assert serdeSb.length() == 0 && colsSb.length() == 0;\n      return orderedResult; // No SDs, probably a view.\n    }\n    String sdIds = trimCommaList(sdSb), serdeIds = trimCommaList(serdeSb),\n        colIds = trimCommaList(colsSb);\n\n    // Get all the stuff for SD. Don't do empty-list check - we expect partitions do have SDs.\n    queryText = \"select \\\"SD_ID\\\", \\\"PARAM_KEY\\\", \\\"PARAM_VALUE\\\" from \\\"SD_PARAMS\\\"\"\n        + \" where \\\"SD_ID\\\" in (\" + sdIds + \") and \\\"PARAM_KEY\\\" is not null\"\n        + \" order by \\\"SD_ID\\\" asc\";\n    loopJoinOrderedResult(sds, queryText, 0, new ApplyFunc<StorageDescriptor>() {\n      @Override\n      public void apply(StorageDescriptor t, Object[] fields) {\n        t.putToParameters((String)fields[1], (String)fields[2]);\n      }});\n\n    queryText = \"select \\\"SD_ID\\\", \\\"COLUMN_NAME\\\", \\\"SORT_COLS\\\".\\\"ORDER\\\" from \\\"SORT_COLS\\\"\"\n        + \" where \\\"SD_ID\\\" in (\" + sdIds + \") and \\\"INTEGER_IDX\\\" >= 0\"\n        + \" order by \\\"SD_ID\\\" asc, \\\"INTEGER_IDX\\\" asc\";\n    loopJoinOrderedResult(sds, queryText, 0, new ApplyFunc<StorageDescriptor>() {\n      @Override\n      public void apply(StorageDescriptor t, Object[] fields) {\n        if (fields[2] == null) return;\n        t.addToSortCols(new Order((String)fields[1], extractSqlInt(fields[2])));\n      }});\n\n    queryText = \"select \\\"SD_ID\\\", \\\"BUCKET_COL_NAME\\\" from \\\"BUCKETING_COLS\\\"\"\n        + \" where \\\"SD_ID\\\" in (\" + sdIds + \") and \\\"INTEGER_IDX\\\" >= 0\"\n        + \" order by \\\"SD_ID\\\" asc, \\\"INTEGER_IDX\\\" asc\";\n    loopJoinOrderedResult(sds, queryText, 0, new ApplyFunc<StorageDescriptor>() {\n      @Override\n      public void apply(StorageDescriptor t, Object[] fields) {\n        t.addToBucketCols((String)fields[1]);\n      }});\n\n    // Skewed columns stuff.\n    queryText = \"select \\\"SD_ID\\\", \\\"SKEWED_COL_NAME\\\" from \\\"SKEWED_COL_NAMES\\\"\"\n        + \" where \\\"SD_ID\\\" in (\" + sdIds + \") and \\\"INTEGER_IDX\\\" >= 0\"\n        + \" order by \\\"SD_ID\\\" asc, \\\"INTEGER_IDX\\\" asc\";\n    boolean hasSkewedColumns =\n      loopJoinOrderedResult(sds, queryText, 0, new ApplyFunc<StorageDescriptor>() {\n        @Override\n        public void apply(StorageDescriptor t, Object[] fields) {\n          if (!t.isSetSkewedInfo()) t.setSkewedInfo(new SkewedInfo());\n          t.getSkewedInfo().addToSkewedColNames((String)fields[1]);\n        }}) > 0;\n\n    // Assume we don't need to fetch the rest of the skewed column data if we have no columns.\n    if (hasSkewedColumns) {\n      // We are skipping the SKEWED_STRING_LIST table here, as it seems to be totally useless.\n      queryText =\n            \"select \\\"SKEWED_VALUES\\\".\\\"SD_ID_OID\\\",\"\n          + \"  \\\"SKEWED_STRING_LIST_VALUES\\\".\\\"STRING_LIST_ID\\\",\"\n          + \"  \\\"SKEWED_STRING_LIST_VALUES\\\".\\\"STRING_LIST_VALUE\\\" \"\n          + \"from \\\"SKEWED_VALUES\\\" \"\n          + \"  left outer join \\\"SKEWED_STRING_LIST_VALUES\\\" on \\\"SKEWED_VALUES\\\".\"\n          + \"\\\"STRING_LIST_ID_EID\\\" = \\\"SKEWED_STRING_LIST_VALUES\\\".\\\"STRING_LIST_ID\\\" \"\n          + \"where \\\"SKEWED_VALUES\\\".\\\"SD_ID_OID\\\" in (\" + sdIds + \") \"\n          + \"  and \\\"SKEWED_VALUES\\\".\\\"STRING_LIST_ID_EID\\\" is not null \"\n          + \"  and \\\"SKEWED_VALUES\\\".\\\"INTEGER_IDX\\\" >= 0 \"\n          + \"order by \\\"SKEWED_VALUES\\\".\\\"SD_ID_OID\\\" asc, \\\"SKEWED_VALUES\\\".\\\"INTEGER_IDX\\\" asc,\"\n          + \"  \\\"SKEWED_STRING_LIST_VALUES\\\".\\\"INTEGER_IDX\\\" asc\";\n      loopJoinOrderedResult(sds, queryText, 0, new ApplyFunc<StorageDescriptor>() {\n        private Long currentListId;\n        private List<String> currentList;\n        @Override\n        public void apply(StorageDescriptor t, Object[] fields) throws MetaException {\n          if (!t.isSetSkewedInfo()) t.setSkewedInfo(new SkewedInfo());\n          // Note that this is not a typical list accumulator - there's no call to finalize\n          // the last list. Instead we add list to SD first, as well as locally to add elements.\n          if (fields[1] == null) {\n            currentList = null; // left outer join produced a list with no values\n            currentListId = null;\n            t.getSkewedInfo().addToSkewedColValues(new ArrayList<String>());\n          } else {\n            long fieldsListId = extractSqlLong(fields[1]);\n            if (currentListId == null || fieldsListId != currentListId) {\n              currentList = new ArrayList<String>();\n              currentListId = fieldsListId;\n              t.getSkewedInfo().addToSkewedColValues(currentList);\n            }\n            currentList.add((String)fields[2]);\n          }\n        }});\n\n      // We are skipping the SKEWED_STRING_LIST table here, as it seems to be totally useless.\n      queryText =\n            \"select \\\"SKEWED_COL_VALUE_LOC_MAP\\\".\\\"SD_ID\\\",\"\n          + \" \\\"SKEWED_STRING_LIST_VALUES\\\".STRING_LIST_ID,\"\n          + \" \\\"SKEWED_COL_VALUE_LOC_MAP\\\".\\\"LOCATION\\\",\"\n          + \" \\\"SKEWED_STRING_LIST_VALUES\\\".\\\"STRING_LIST_VALUE\\\" \"\n          + \"from \\\"SKEWED_COL_VALUE_LOC_MAP\\\"\"\n          + \"  left outer join \\\"SKEWED_STRING_LIST_VALUES\\\" on \\\"SKEWED_COL_VALUE_LOC_MAP\\\".\"\n          + \"\\\"STRING_LIST_ID_KID\\\" = \\\"SKEWED_STRING_LIST_VALUES\\\".\\\"STRING_LIST_ID\\\" \"\n          + \"where \\\"SKEWED_COL_VALUE_LOC_MAP\\\".\\\"SD_ID\\\" in (\" + sdIds + \")\"\n          + \"  and \\\"SKEWED_COL_VALUE_LOC_MAP\\\".\\\"STRING_LIST_ID_KID\\\" is not null \"\n          + \"order by \\\"SKEWED_COL_VALUE_LOC_MAP\\\".\\\"SD_ID\\\" asc,\"\n          + \"  \\\"SKEWED_STRING_LIST_VALUES\\\".\\\"STRING_LIST_ID\\\" asc,\"\n          + \"  \\\"SKEWED_STRING_LIST_VALUES\\\".\\\"INTEGER_IDX\\\" asc\";\n\n      loopJoinOrderedResult(sds, queryText, 0, new ApplyFunc<StorageDescriptor>() {\n        private Long currentListId;\n        private List<String> currentList;\n        @Override\n        public void apply(StorageDescriptor t, Object[] fields) throws MetaException {\n          if (!t.isSetSkewedInfo()) {\n            SkewedInfo skewedInfo = new SkewedInfo();\n            skewedInfo.setSkewedColValueLocationMaps(new HashMap<List<String>, String>());\n            t.setSkewedInfo(skewedInfo);\n          }\n          Map<List<String>, String> skewMap = t.getSkewedInfo().getSkewedColValueLocationMaps();\n          // Note that this is not a typical list accumulator - there's no call to finalize\n          // the last list. Instead we add list to SD first, as well as locally to add elements.\n          if (fields[1] == null) {\n            currentList = new ArrayList<String>(); // left outer join produced a list with no values\n            currentListId = null;\n          } else {\n            long fieldsListId = extractSqlLong(fields[1]);\n            if (currentListId == null || fieldsListId != currentListId) {\n              currentList = new ArrayList<String>();\n              currentListId = fieldsListId;\n            } else {\n              skewMap.remove(currentList); // value based compare.. remove first\n            }\n            currentList.add((String)fields[3]);\n          }\n          skewMap.put(currentList, (String)fields[2]);\n        }});\n    } // if (hasSkewedColumns)\n\n    // Get FieldSchema stuff if any.\n    if (!colss.isEmpty()) {\n      // We are skipping the CDS table here, as it seems to be totally useless.\n      queryText = \"select \\\"CD_ID\\\", \\\"COMMENT\\\", \\\"COLUMN_NAME\\\", \\\"TYPE_NAME\\\"\"\n          + \" from \\\"COLUMNS_V2\\\" where \\\"CD_ID\\\" in (\" + colIds + \") and \\\"INTEGER_IDX\\\" >= 0\"\n          + \" order by \\\"CD_ID\\\" asc, \\\"INTEGER_IDX\\\" asc\";\n      loopJoinOrderedResult(colss, queryText, 0, new ApplyFunc<List<FieldSchema>>() {\n        @Override\n        public void apply(List<FieldSchema> t, Object[] fields) {\n          t.add(new FieldSchema((String)fields[2], (String)fields[3], (String)fields[1]));\n        }});\n    }\n\n    // Finally, get all the stuff for serdes - just the params.\n    queryText = \"select \\\"SERDE_ID\\\", \\\"PARAM_KEY\\\", \\\"PARAM_VALUE\\\" from \\\"SERDE_PARAMS\\\"\"\n        + \" where \\\"SERDE_ID\\\" in (\" + serdeIds + \") and \\\"PARAM_KEY\\\" is not null\"\n        + \" order by \\\"SERDE_ID\\\" asc\";\n    loopJoinOrderedResult(serdes, queryText, 0, new ApplyFunc<SerDeInfo>() {\n      @Override\n      public void apply(SerDeInfo t, Object[] fields) {\n        t.putToParameters((String)fields[1], (String)fields[2]);\n      }});\n\n    return orderedResult;\n  }",
                "code_after_change": "  private List<Partition> getPartitionsFromPartitionIds(String dbName, String tblName,\n      Boolean isView, List<Object> partIdList) throws MetaException {\n    boolean doTrace = LOG.isDebugEnabled();\n    int idStringWidth = (int)Math.ceil(Math.log10(partIdList.size())) + 1; // 1 for comma\n    int sbCapacity = partIdList.size() * idStringWidth;\n    // Prepare StringBuilder for \"PART_ID in (...)\" to use in future queries.\n    StringBuilder partSb = new StringBuilder(sbCapacity);\n    for (Object partitionId : partIdList) {\n      partSb.append(extractSqlLong(partitionId)).append(\",\");\n    }\n    String partIds = trimCommaList(partSb);\n\n    // Get most of the fields for the IDs provided.\n    // Assume db and table names are the same for all partition, as provided in arguments.\n    String queryText =\n      \"select \\\"PARTITIONS\\\".\\\"PART_ID\\\", \\\"SDS\\\".\\\"SD_ID\\\", \\\"SDS\\\".\\\"CD_ID\\\",\"\n    + \" \\\"SERDES\\\".\\\"SERDE_ID\\\", \\\"PARTITIONS\\\".\\\"CREATE_TIME\\\",\"\n    + \" \\\"PARTITIONS\\\".\\\"LAST_ACCESS_TIME\\\", \\\"SDS\\\".\\\"INPUT_FORMAT\\\", \\\"SDS\\\".\\\"IS_COMPRESSED\\\",\"\n    + \" \\\"SDS\\\".\\\"IS_STOREDASSUBDIRECTORIES\\\", \\\"SDS\\\".\\\"LOCATION\\\", \\\"SDS\\\".\\\"NUM_BUCKETS\\\",\"\n    + \" \\\"SDS\\\".\\\"OUTPUT_FORMAT\\\", \\\"SERDES\\\".\\\"NAME\\\", \\\"SERDES\\\".\\\"SLIB\\\" \"\n    + \"from \\\"PARTITIONS\\\"\"\n    + \"  left outer join \\\"SDS\\\" on \\\"PARTITIONS\\\".\\\"SD_ID\\\" = \\\"SDS\\\".\\\"SD_ID\\\" \"\n    + \"  left outer join \\\"SERDES\\\" on \\\"SDS\\\".\\\"SERDE_ID\\\" = \\\"SERDES\\\".\\\"SERDE_ID\\\" \"\n    + \"where \\\"PART_ID\\\" in (\" + partIds + \") order by \\\"PART_NAME\\\" asc\";\n    long start = doTrace ? System.nanoTime() : 0;\n    Query query = pm.newQuery(\"javax.jdo.query.SQL\", queryText);\n    @SuppressWarnings(\"unchecked\")\n    List<Object[]> sqlResult = executeWithArray(query, null, queryText);\n    long queryTime = doTrace ? System.nanoTime() : 0;\n    Deadline.checkTimeout();\n\n    // Read all the fields and create partitions, SDs and serdes.\n    TreeMap<Long, Partition> partitions = new TreeMap<Long, Partition>();\n    TreeMap<Long, StorageDescriptor> sds = new TreeMap<Long, StorageDescriptor>();\n    TreeMap<Long, SerDeInfo> serdes = new TreeMap<Long, SerDeInfo>();\n    TreeMap<Long, List<FieldSchema>> colss = new TreeMap<Long, List<FieldSchema>>();\n    // Keep order by name, consistent with JDO.\n    ArrayList<Partition> orderedResult = new ArrayList<Partition>(partIdList.size());\n\n    // Prepare StringBuilder-s for \"in (...)\" lists to use in one-to-many queries.\n    StringBuilder sdSb = new StringBuilder(sbCapacity), serdeSb = new StringBuilder(sbCapacity);\n    StringBuilder colsSb = new StringBuilder(7); // We expect that there's only one field schema.\n    tblName = tblName.toLowerCase();\n    dbName = dbName.toLowerCase();\n    for (Object[] fields : sqlResult) {\n      // Here comes the ugly part...\n      long partitionId = extractSqlLong(fields[0]);\n      Long sdId = extractSqlLong(fields[1]);\n      Long colId = extractSqlLong(fields[2]);\n      Long serdeId = extractSqlLong(fields[3]);\n      // A partition must have either everything set, or nothing set if it's a view.\n      if (sdId == null || colId == null || serdeId == null) {\n        if (isView == null) {\n          isView = isViewTable(dbName, tblName);\n        }\n        if ((sdId != null || colId != null || serdeId != null) || !isView) {\n          throw new MetaException(\"Unexpected null for one of the IDs, SD \" + sdId + \", column \"\n              + colId + \", serde \" + serdeId + \" for a \" + (isView ? \"\" : \"non-\") + \" view\");\n        }\n      }\n\n      Partition part = new Partition();\n      orderedResult.add(part);\n      // Set the collection fields; some code might not check presence before accessing them.\n      part.setParameters(new HashMap<String, String>());\n      part.setValues(new ArrayList<String>());\n      part.setDbName(dbName);\n      part.setTableName(tblName);\n      if (fields[4] != null) part.setCreateTime(extractSqlInt(fields[4]));\n      if (fields[5] != null) part.setLastAccessTime(extractSqlInt(fields[5]));\n      partitions.put(partitionId, part);\n\n      if (sdId == null) continue; // Probably a view.\n      assert colId != null && serdeId != null;\n\n      // We assume each partition has an unique SD.\n      StorageDescriptor sd = new StorageDescriptor();\n      StorageDescriptor oldSd = sds.put(sdId, sd);\n      if (oldSd != null) {\n        throw new MetaException(\"Partitions reuse SDs; we don't expect that\");\n      }\n      // Set the collection fields; some code might not check presence before accessing them.\n      sd.setSortCols(new ArrayList<Order>());\n      sd.setBucketCols(new ArrayList<String>());\n      sd.setParameters(new HashMap<String, String>());\n      sd.setSkewedInfo(new SkewedInfo(new ArrayList<String>(),\n          new ArrayList<List<String>>(), new HashMap<List<String>, String>()));\n      sd.setInputFormat((String)fields[6]);\n      Boolean tmpBoolean = extractSqlBoolean(fields[7]);\n      if (tmpBoolean != null) sd.setCompressed(tmpBoolean);\n      tmpBoolean = extractSqlBoolean(fields[8]);\n      if (tmpBoolean != null) sd.setStoredAsSubDirectories(tmpBoolean);\n      sd.setLocation((String)fields[9]);\n      if (fields[10] != null) sd.setNumBuckets(extractSqlInt(fields[10]));\n      sd.setOutputFormat((String)fields[11]);\n      sdSb.append(sdId).append(\",\");\n      part.setSd(sd);\n\n      List<FieldSchema> cols = colss.get(colId);\n      // We expect that colId will be the same for all (or many) SDs.\n      if (cols == null) {\n        cols = new ArrayList<FieldSchema>();\n        colss.put(colId, cols);\n        colsSb.append(colId).append(\",\");\n      }\n      sd.setCols(cols);\n\n      // We assume each SD has an unique serde.\n      SerDeInfo serde = new SerDeInfo();\n      SerDeInfo oldSerde = serdes.put(serdeId, serde);\n      if (oldSerde != null) {\n        throw new MetaException(\"SDs reuse serdes; we don't expect that\");\n      }\n      serde.setParameters(new HashMap<String, String>());\n      serde.setName((String)fields[12]);\n      serde.setSerializationLib((String)fields[13]);\n      serdeSb.append(serdeId).append(\",\");\n      sd.setSerdeInfo(serde);\n      Deadline.checkTimeout();\n    }\n    query.closeAll();\n    timingTrace(doTrace, queryText, start, queryTime);\n\n    // Now get all the one-to-many things. Start with partitions.\n    queryText = \"select \\\"PART_ID\\\", \\\"PARAM_KEY\\\", \\\"PARAM_VALUE\\\" from \\\"PARTITION_PARAMS\\\"\"\n        + \" where \\\"PART_ID\\\" in (\" + partIds + \") and \\\"PARAM_KEY\\\" is not null\"\n        + \" order by \\\"PART_ID\\\" asc\";\n    loopJoinOrderedResult(partitions, queryText, 0, new ApplyFunc<Partition>() {\n      @Override\n      public void apply(Partition t, Object[] fields) {\n        t.putToParameters((String)fields[1], (String)fields[2]);\n      }});\n    // Perform conversion of null map values\n    for (Partition t : partitions.values()) {\n      t.setParameters(MetaStoreUtils.trimMapNulls(t.getParameters(), convertMapNullsToEmptyStrings));\n    }\n\n    queryText = \"select \\\"PART_ID\\\", \\\"PART_KEY_VAL\\\" from \\\"PARTITION_KEY_VALS\\\"\"\n        + \" where \\\"PART_ID\\\" in (\" + partIds + \") and \\\"INTEGER_IDX\\\" >= 0\"\n        + \" order by \\\"PART_ID\\\" asc, \\\"INTEGER_IDX\\\" asc\";\n    loopJoinOrderedResult(partitions, queryText, 0, new ApplyFunc<Partition>() {\n      @Override\n      public void apply(Partition t, Object[] fields) {\n        t.addToValues((String)fields[1]);\n      }});\n\n    // Prepare IN (blah) lists for the following queries. Cut off the final ','s.\n    if (sdSb.length() == 0) {\n      assert serdeSb.length() == 0 && colsSb.length() == 0;\n      return orderedResult; // No SDs, probably a view.\n    }\n    String sdIds = trimCommaList(sdSb), serdeIds = trimCommaList(serdeSb),\n        colIds = trimCommaList(colsSb);\n\n    // Get all the stuff for SD. Don't do empty-list check - we expect partitions do have SDs.\n    queryText = \"select \\\"SD_ID\\\", \\\"PARAM_KEY\\\", \\\"PARAM_VALUE\\\" from \\\"SD_PARAMS\\\"\"\n        + \" where \\\"SD_ID\\\" in (\" + sdIds + \") and \\\"PARAM_KEY\\\" is not null\"\n        + \" order by \\\"SD_ID\\\" asc\";\n    loopJoinOrderedResult(sds, queryText, 0, new ApplyFunc<StorageDescriptor>() {\n      @Override\n      public void apply(StorageDescriptor t, Object[] fields) {\n        t.putToParameters((String)fields[1], (String)fields[2]);\n      }});\n    // Perform conversion of null map values\n    for (StorageDescriptor t : sds.values()) {\n      t.setParameters(MetaStoreUtils.trimMapNulls(t.getParameters(), convertMapNullsToEmptyStrings));\n    }\n\n    queryText = \"select \\\"SD_ID\\\", \\\"COLUMN_NAME\\\", \\\"SORT_COLS\\\".\\\"ORDER\\\" from \\\"SORT_COLS\\\"\"\n        + \" where \\\"SD_ID\\\" in (\" + sdIds + \") and \\\"INTEGER_IDX\\\" >= 0\"\n        + \" order by \\\"SD_ID\\\" asc, \\\"INTEGER_IDX\\\" asc\";\n    loopJoinOrderedResult(sds, queryText, 0, new ApplyFunc<StorageDescriptor>() {\n      @Override\n      public void apply(StorageDescriptor t, Object[] fields) {\n        if (fields[2] == null) return;\n        t.addToSortCols(new Order((String)fields[1], extractSqlInt(fields[2])));\n      }});\n\n    queryText = \"select \\\"SD_ID\\\", \\\"BUCKET_COL_NAME\\\" from \\\"BUCKETING_COLS\\\"\"\n        + \" where \\\"SD_ID\\\" in (\" + sdIds + \") and \\\"INTEGER_IDX\\\" >= 0\"\n        + \" order by \\\"SD_ID\\\" asc, \\\"INTEGER_IDX\\\" asc\";\n    loopJoinOrderedResult(sds, queryText, 0, new ApplyFunc<StorageDescriptor>() {\n      @Override\n      public void apply(StorageDescriptor t, Object[] fields) {\n        t.addToBucketCols((String)fields[1]);\n      }});\n\n    // Skewed columns stuff.\n    queryText = \"select \\\"SD_ID\\\", \\\"SKEWED_COL_NAME\\\" from \\\"SKEWED_COL_NAMES\\\"\"\n        + \" where \\\"SD_ID\\\" in (\" + sdIds + \") and \\\"INTEGER_IDX\\\" >= 0\"\n        + \" order by \\\"SD_ID\\\" asc, \\\"INTEGER_IDX\\\" asc\";\n    boolean hasSkewedColumns =\n      loopJoinOrderedResult(sds, queryText, 0, new ApplyFunc<StorageDescriptor>() {\n        @Override\n        public void apply(StorageDescriptor t, Object[] fields) {\n          if (!t.isSetSkewedInfo()) t.setSkewedInfo(new SkewedInfo());\n          t.getSkewedInfo().addToSkewedColNames((String)fields[1]);\n        }}) > 0;\n\n    // Assume we don't need to fetch the rest of the skewed column data if we have no columns.\n    if (hasSkewedColumns) {\n      // We are skipping the SKEWED_STRING_LIST table here, as it seems to be totally useless.\n      queryText =\n            \"select \\\"SKEWED_VALUES\\\".\\\"SD_ID_OID\\\",\"\n          + \"  \\\"SKEWED_STRING_LIST_VALUES\\\".\\\"STRING_LIST_ID\\\",\"\n          + \"  \\\"SKEWED_STRING_LIST_VALUES\\\".\\\"STRING_LIST_VALUE\\\" \"\n          + \"from \\\"SKEWED_VALUES\\\" \"\n          + \"  left outer join \\\"SKEWED_STRING_LIST_VALUES\\\" on \\\"SKEWED_VALUES\\\".\"\n          + \"\\\"STRING_LIST_ID_EID\\\" = \\\"SKEWED_STRING_LIST_VALUES\\\".\\\"STRING_LIST_ID\\\" \"\n          + \"where \\\"SKEWED_VALUES\\\".\\\"SD_ID_OID\\\" in (\" + sdIds + \") \"\n          + \"  and \\\"SKEWED_VALUES\\\".\\\"STRING_LIST_ID_EID\\\" is not null \"\n          + \"  and \\\"SKEWED_VALUES\\\".\\\"INTEGER_IDX\\\" >= 0 \"\n          + \"order by \\\"SKEWED_VALUES\\\".\\\"SD_ID_OID\\\" asc, \\\"SKEWED_VALUES\\\".\\\"INTEGER_IDX\\\" asc,\"\n          + \"  \\\"SKEWED_STRING_LIST_VALUES\\\".\\\"INTEGER_IDX\\\" asc\";\n      loopJoinOrderedResult(sds, queryText, 0, new ApplyFunc<StorageDescriptor>() {\n        private Long currentListId;\n        private List<String> currentList;\n        @Override\n        public void apply(StorageDescriptor t, Object[] fields) throws MetaException {\n          if (!t.isSetSkewedInfo()) t.setSkewedInfo(new SkewedInfo());\n          // Note that this is not a typical list accumulator - there's no call to finalize\n          // the last list. Instead we add list to SD first, as well as locally to add elements.\n          if (fields[1] == null) {\n            currentList = null; // left outer join produced a list with no values\n            currentListId = null;\n            t.getSkewedInfo().addToSkewedColValues(new ArrayList<String>());\n          } else {\n            long fieldsListId = extractSqlLong(fields[1]);\n            if (currentListId == null || fieldsListId != currentListId) {\n              currentList = new ArrayList<String>();\n              currentListId = fieldsListId;\n              t.getSkewedInfo().addToSkewedColValues(currentList);\n            }\n            currentList.add((String)fields[2]);\n          }\n        }});\n\n      // We are skipping the SKEWED_STRING_LIST table here, as it seems to be totally useless.\n      queryText =\n            \"select \\\"SKEWED_COL_VALUE_LOC_MAP\\\".\\\"SD_ID\\\",\"\n          + \" \\\"SKEWED_STRING_LIST_VALUES\\\".STRING_LIST_ID,\"\n          + \" \\\"SKEWED_COL_VALUE_LOC_MAP\\\".\\\"LOCATION\\\",\"\n          + \" \\\"SKEWED_STRING_LIST_VALUES\\\".\\\"STRING_LIST_VALUE\\\" \"\n          + \"from \\\"SKEWED_COL_VALUE_LOC_MAP\\\"\"\n          + \"  left outer join \\\"SKEWED_STRING_LIST_VALUES\\\" on \\\"SKEWED_COL_VALUE_LOC_MAP\\\".\"\n          + \"\\\"STRING_LIST_ID_KID\\\" = \\\"SKEWED_STRING_LIST_VALUES\\\".\\\"STRING_LIST_ID\\\" \"\n          + \"where \\\"SKEWED_COL_VALUE_LOC_MAP\\\".\\\"SD_ID\\\" in (\" + sdIds + \")\"\n          + \"  and \\\"SKEWED_COL_VALUE_LOC_MAP\\\".\\\"STRING_LIST_ID_KID\\\" is not null \"\n          + \"order by \\\"SKEWED_COL_VALUE_LOC_MAP\\\".\\\"SD_ID\\\" asc,\"\n          + \"  \\\"SKEWED_STRING_LIST_VALUES\\\".\\\"STRING_LIST_ID\\\" asc,\"\n          + \"  \\\"SKEWED_STRING_LIST_VALUES\\\".\\\"INTEGER_IDX\\\" asc\";\n\n      loopJoinOrderedResult(sds, queryText, 0, new ApplyFunc<StorageDescriptor>() {\n        private Long currentListId;\n        private List<String> currentList;\n        @Override\n        public void apply(StorageDescriptor t, Object[] fields) throws MetaException {\n          if (!t.isSetSkewedInfo()) {\n            SkewedInfo skewedInfo = new SkewedInfo();\n            skewedInfo.setSkewedColValueLocationMaps(new HashMap<List<String>, String>());\n            t.setSkewedInfo(skewedInfo);\n          }\n          Map<List<String>, String> skewMap = t.getSkewedInfo().getSkewedColValueLocationMaps();\n          // Note that this is not a typical list accumulator - there's no call to finalize\n          // the last list. Instead we add list to SD first, as well as locally to add elements.\n          if (fields[1] == null) {\n            currentList = new ArrayList<String>(); // left outer join produced a list with no values\n            currentListId = null;\n          } else {\n            long fieldsListId = extractSqlLong(fields[1]);\n            if (currentListId == null || fieldsListId != currentListId) {\n              currentList = new ArrayList<String>();\n              currentListId = fieldsListId;\n            } else {\n              skewMap.remove(currentList); // value based compare.. remove first\n            }\n            currentList.add((String)fields[3]);\n          }\n          skewMap.put(currentList, (String)fields[2]);\n        }});\n    } // if (hasSkewedColumns)\n\n    // Get FieldSchema stuff if any.\n    if (!colss.isEmpty()) {\n      // We are skipping the CDS table here, as it seems to be totally useless.\n      queryText = \"select \\\"CD_ID\\\", \\\"COMMENT\\\", \\\"COLUMN_NAME\\\", \\\"TYPE_NAME\\\"\"\n          + \" from \\\"COLUMNS_V2\\\" where \\\"CD_ID\\\" in (\" + colIds + \") and \\\"INTEGER_IDX\\\" >= 0\"\n          + \" order by \\\"CD_ID\\\" asc, \\\"INTEGER_IDX\\\" asc\";\n      loopJoinOrderedResult(colss, queryText, 0, new ApplyFunc<List<FieldSchema>>() {\n        @Override\n        public void apply(List<FieldSchema> t, Object[] fields) {\n          t.add(new FieldSchema((String)fields[2], (String)fields[3], (String)fields[1]));\n        }});\n    }\n\n    // Finally, get all the stuff for serdes - just the params.\n    queryText = \"select \\\"SERDE_ID\\\", \\\"PARAM_KEY\\\", \\\"PARAM_VALUE\\\" from \\\"SERDE_PARAMS\\\"\"\n        + \" where \\\"SERDE_ID\\\" in (\" + serdeIds + \") and \\\"PARAM_KEY\\\" is not null\"\n        + \" order by \\\"SERDE_ID\\\" asc\";\n    loopJoinOrderedResult(serdes, queryText, 0, new ApplyFunc<SerDeInfo>() {\n      @Override\n      public void apply(SerDeInfo t, Object[] fields) {\n        t.putToParameters((String)fields[1], (String)fields[2]);\n      }});\n    // Perform conversion of null map values\n    for (SerDeInfo t : serdes.values()) {\n      t.setParameters(MetaStoreUtils.trimMapNulls(t.getParameters(), convertMapNullsToEmptyStrings));\n    }\n\n    return orderedResult;\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreDirectSql.apply": {
                "code_before_change": "      public void apply(Partition t, Object[] fields) {\n        t.putToParameters((String)fields[1], (String)fields[2]);\n      }});\n\n    queryText = \"select \\\"PART_ID\\\", \\\"PART_KEY_VAL\\\" from \\\"PARTITION_KEY_VALS\\\"\"\n        + \" where \\\"PART_ID\\\" in (\" + partIds + \") and \\\"INTEGER_IDX\\\" >= 0\"\n        + \" order by \\\"PART_ID\\\" asc, \\\"INTEGER_IDX\\\" asc\";\n    loopJoinOrderedResult(partitions, queryText, 0, new ApplyFunc<Partition>() {",
                "code_after_change": "      public void apply(Partition t, Object[] fields) {\n        t.putToParameters((String)fields[1], (String)fields[2]);\n      }});\n    // Perform conversion of null map values\n    for (Partition t : partitions.values()) {"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report mentions a NullPointerException related to the metastore's Direct SQL mode, which is relevant to the ground truth methods in the context of the stack trace. However, it does not precisely identify the root cause in the ground truth methods. The report does not provide any fix suggestions, hence 'Missing' for fix suggestion. The problem location is partially identified as it refers to the metastore's Direct SQL mode, which is related to the ground truth methods but not precise. There is no wrong information as the report correctly identifies the context of the error."
        }
    },
    {
        "filename": "HIVE-10559.json",
        "code_diff": {
            "ql.src.java.org.apache.hadoop.hive.ql.parse.TezCompiler.runCycleAnalysisForPartitionPruning": {
                "code_before_change": "  private void runCycleAnalysisForPartitionPruning(OptimizeTezProcContext procCtx,\n      Set<ReadEntity> inputs, Set<WriteEntity> outputs) throws SemanticException {\n\n    if (!procCtx.conf.getBoolVar(ConfVars.TEZ_DYNAMIC_PARTITION_PRUNING)) {\n      return;\n    }\n\n    boolean cycleFree = false;\n    while (!cycleFree) {\n      cycleFree = true;\n      Set<Set<Operator<?>>> components = getComponents(procCtx);\n      for (Set<Operator<?>> component : components) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Component: \");\n          for (Operator<?> co : component) {\n            LOG.debug(\"Operator: \" + co.getName() + \", \" + co.getIdentifier());\n          }\n        }\n        if (component.size() != 1) {\n          LOG.info(\"Found cycle in operator plan...\");\n          cycleFree = false;\n          removeEventOperator(component);\n          break;\n        }\n      }\n      LOG.info(\"Cycle free: \" + cycleFree);\n    }\n  }",
                "code_after_change": "  private void runCycleAnalysisForPartitionPruning(OptimizeTezProcContext procCtx,\n      Set<ReadEntity> inputs, Set<WriteEntity> outputs) throws SemanticException {\n\n    if (!procCtx.conf.getBoolVar(ConfVars.TEZ_DYNAMIC_PARTITION_PRUNING)) {\n      return;\n    }\n\n    boolean cycleFree = false;\n    while (!cycleFree) {\n      cycleFree = true;\n      Set<Set<Operator<?>>> components = getComponents(procCtx);\n      for (Set<Operator<?>> component : components) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Component: \");\n          for (Operator<?> co : component) {\n            LOG.debug(\"Operator: \" + co.getName() + \", \" + co.getIdentifier());\n          }\n        }\n        if (component.size() != 1) {\n          LOG.info(\"Found cycle in operator plan...\");\n          cycleFree = false;\n          removeEventOperator(component, procCtx);\n          break;\n        }\n      }\n      LOG.info(\"Cycle free: \" + cycleFree);\n    }\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin.convertJoinMapJoin": {
                "code_before_change": "  public MapJoinOperator convertJoinMapJoin(JoinOperator joinOp, OptimizeTezProcContext context,\n      int bigTablePosition) throws SemanticException {\n    // bail on mux operator because currently the mux operator masks the emit keys\n    // of the constituent reduce sinks.\n    for (Operator<? extends OperatorDesc> parentOp : joinOp.getParentOperators()) {\n      if (parentOp instanceof MuxOperator) {\n        return null;\n      }\n    }\n\n    // can safely convert the join to a map join.\n    MapJoinOperator mapJoinOp =\n        MapJoinProcessor.convertJoinOpMapJoinOp(context.conf, joinOp,\n            joinOp.getConf().isLeftInputJoin(), joinOp.getConf().getBaseSrc(),\n            joinOp.getConf().getMapAliases(), bigTablePosition, true);\n\n    Operator<? extends OperatorDesc> parentBigTableOp =\n        mapJoinOp.getParentOperators().get(bigTablePosition);\n    if (parentBigTableOp instanceof ReduceSinkOperator) {\n      for (Operator<?> p : parentBigTableOp.getParentOperators()) {\n        // we might have generated a dynamic partition operator chain. Since\n        // we're removing the reduce sink we need do remove that too.\n        Set<Operator<?>> dynamicPartitionOperators = new HashSet<Operator<?>>();\n        for (Operator<?> c : p.getChildOperators()) {\n          if (hasDynamicPartitionBroadcast(c)) {\n            dynamicPartitionOperators.add(c);\n          }\n        }\n        for (Operator<?> c : dynamicPartitionOperators) {\n          p.removeChild(c);\n        }\n      }\n      mapJoinOp.getParentOperators().remove(bigTablePosition);\n      if (!(mapJoinOp.getParentOperators().contains(parentBigTableOp.getParentOperators().get(0)))) {\n        mapJoinOp.getParentOperators().add(bigTablePosition,\n            parentBigTableOp.getParentOperators().get(0));\n      }\n      parentBigTableOp.getParentOperators().get(0).removeChild(parentBigTableOp);\n      for (Operator<? extends OperatorDesc>op : mapJoinOp.getParentOperators()) {\n        if (!(op.getChildOperators().contains(mapJoinOp))) {\n          op.getChildOperators().add(mapJoinOp);\n        }\n        op.getChildOperators().remove(joinOp);\n      }\n    }\n\n    return mapJoinOp;\n  }",
                "code_after_change": "  public MapJoinOperator convertJoinMapJoin(JoinOperator joinOp, OptimizeTezProcContext context,\n      int bigTablePosition) throws SemanticException {\n    // bail on mux operator because currently the mux operator masks the emit keys\n    // of the constituent reduce sinks.\n    for (Operator<? extends OperatorDesc> parentOp : joinOp.getParentOperators()) {\n      if (parentOp instanceof MuxOperator) {\n        return null;\n      }\n    }\n\n    // can safely convert the join to a map join.\n    MapJoinOperator mapJoinOp =\n        MapJoinProcessor.convertJoinOpMapJoinOp(context.conf, joinOp,\n            joinOp.getConf().isLeftInputJoin(), joinOp.getConf().getBaseSrc(),\n            joinOp.getConf().getMapAliases(), bigTablePosition, true);\n    mapJoinOp.getConf().setHybridHashJoin(HiveConf.getBoolVar(context.conf,\n      HiveConf.ConfVars.HIVEUSEHYBRIDGRACEHASHJOIN));\n\n    Operator<? extends OperatorDesc> parentBigTableOp =\n        mapJoinOp.getParentOperators().get(bigTablePosition);\n    if (parentBigTableOp instanceof ReduceSinkOperator) {\n      for (Operator<?> p : parentBigTableOp.getParentOperators()) {\n        // we might have generated a dynamic partition operator chain. Since\n        // we're removing the reduce sink we need do remove that too.\n        Set<Operator<?>> dynamicPartitionOperators = new HashSet<Operator<?>>();\n        Map<Operator<?>, AppMasterEventOperator> opEventPairs = new HashMap<>();\n        for (Operator<?> c : p.getChildOperators()) {\n          AppMasterEventOperator event = findDynamicPartitionBroadcast(c);\n          if (event != null) {\n            dynamicPartitionOperators.add(c);\n            opEventPairs.put(c, event);\n          }\n        }\n        for (Operator<?> c : dynamicPartitionOperators) {\n          if (context.pruningOpsRemovedByPriorOpt.isEmpty() ||\n              !context.pruningOpsRemovedByPriorOpt.contains(opEventPairs.get(c))) {\n            p.removeChild(c);\n            // at this point we've found the fork in the op pipeline that has the pruning as a child plan.\n            LOG.info(\"Disabling dynamic pruning for: \"\n                + ((DynamicPruningEventDesc) opEventPairs.get(c).getConf()).getTableScan().getName()\n                + \". Need to be removed together with reduce sink\");\n          }\n        }\n        for (Operator<?> op : dynamicPartitionOperators) {\n          context.pruningOpsRemovedByPriorOpt.add(opEventPairs.get(op));\n        }\n      }\n      mapJoinOp.getParentOperators().remove(bigTablePosition);\n      if (!(mapJoinOp.getParentOperators().contains(parentBigTableOp.getParentOperators().get(0)))) {\n        mapJoinOp.getParentOperators().add(bigTablePosition,\n            parentBigTableOp.getParentOperators().get(0));\n      }\n      parentBigTableOp.getParentOperators().get(0).removeChild(parentBigTableOp);\n      for (Operator<? extends OperatorDesc>op : mapJoinOp.getParentOperators()) {\n        if (!(op.getChildOperators().contains(mapJoinOp))) {\n          op.getChildOperators().add(mapJoinOp);\n        }\n        op.getChildOperators().remove(joinOp);\n      }\n    }\n\n    return mapJoinOp;\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.parse.OptimizeTezProcContext.OptimizeTezProcContext": {
                "code_before_change": [],
                "code_after_change": []
            },
            "ql.src.java.org.apache.hadoop.hive.ql.optimizer.RemoveDynamicPruningBySize.process": {
                "code_before_change": "  public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procContext,\n      Object... nodeOutputs)\n      throws SemanticException {\n\n    OptimizeTezProcContext context = (OptimizeTezProcContext) procContext;\n\n    AppMasterEventOperator event = (AppMasterEventOperator) nd;\n    AppMasterEventDesc desc = event.getConf();\n\n    if (desc.getStatistics().getDataSize() > context.conf\n        .getLongVar(ConfVars.TEZ_DYNAMIC_PARTITION_PRUNING_MAX_DATA_SIZE)) {\n      Operator<?> child = event;\n      Operator<?> curr = event;\n\n      while (curr.getChildOperators().size() <= 1) {\n        child = curr;\n        curr = curr.getParentOperators().get(0);\n      }\n      // at this point we've found the fork in the op pipeline that has the\n      // pruning as a child plan.\n      LOG.info(\"Disabling dynamic pruning for: \"\n          + ((DynamicPruningEventDesc) desc).getTableScan().getName()\n          + \". Expected data size is too big: \" + desc.getStatistics().getDataSize());\n      curr.removeChild(child);\n    }\n    return false;\n  }",
                "code_after_change": "  public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procContext,\n      Object... nodeOutputs)\n      throws SemanticException {\n\n    OptimizeTezProcContext context = (OptimizeTezProcContext) procContext;\n\n    AppMasterEventOperator event = (AppMasterEventOperator) nd;\n    AppMasterEventDesc desc = event.getConf();\n\n    if (desc.getStatistics().getDataSize() > context.conf\n        .getLongVar(ConfVars.TEZ_DYNAMIC_PARTITION_PRUNING_MAX_DATA_SIZE) &&\n        (context.pruningOpsRemovedByPriorOpt.isEmpty() ||\n         !context.pruningOpsRemovedByPriorOpt.contains(event))) {\n      context.pruningOpsRemovedByPriorOpt.add(event);\n      GenTezUtils.getUtils().removeBranch(event);\n      // at this point we've found the fork in the op pipeline that has the pruning as a child plan.\n      LOG.info(\"Disabling dynamic pruning for: \"\n          + ((DynamicPruningEventDesc) desc).getTableScan().getName()\n          + \". Expected data size is too big: \" + desc.getStatistics().getDataSize());\n    }\n    return false;\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin.findDynamicPartitionBroadcast": {
                "code_before_change": [],
                "code_after_change": "  private AppMasterEventOperator findDynamicPartitionBroadcast(Operator<?> parent) {\n\n    for (Operator<?> op : parent.getChildOperators()) {\n      while (op != null) {\n        if (op instanceof AppMasterEventOperator && op.getConf() instanceof DynamicPruningEventDesc) {\n          // found dynamic partition pruning operator\n          return (AppMasterEventOperator)op;\n        }\n        if (op instanceof ReduceSinkOperator || op instanceof FileSinkOperator) {\n          // crossing reduce sink or file sink means the pruning isn't for this parent.\n          break;\n        }\n\n        if (op.getChildOperators().size() != 1) {\n          // dynamic partition pruning pipeline doesn't have multiple children\n          break;\n        }\n\n        op = op.getChildOperators().get(0);\n      }\n    }\n\n    return null;\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.parse.TezCompiler.removeEventOperator": {
                "code_before_change": "  private void removeEventOperator(Set<Operator<?>> component) {\n    AppMasterEventOperator victim = null;\n    for (Operator<?> o : component) {\n      if (o instanceof AppMasterEventOperator) {\n        if (victim == null\n            || o.getConf().getStatistics().getDataSize() < victim.getConf().getStatistics()\n                .getDataSize()) {\n          victim = (AppMasterEventOperator) o;\n        }\n      }\n    }\n\n    Operator<?> child = victim;\n    Operator<?> curr = victim;\n\n    while (curr.getChildOperators().size() <= 1) {\n      child = curr;\n      curr = curr.getParentOperators().get(0);\n    }\n\n    // at this point we've found the fork in the op pipeline that has the\n    // pruning as a child plan.\n    LOG.info(\"Disabling dynamic pruning for: \"\n        + ((DynamicPruningEventDesc) victim.getConf()).getTableScan().toString()\n        + \". Needed to break cyclic dependency\");\n    curr.removeChild(child);\n  }",
                "code_after_change": "  private void removeEventOperator(Set<Operator<?>> component, OptimizeTezProcContext context) {\n    AppMasterEventOperator victim = null;\n    for (Operator<?> o : component) {\n      if (o instanceof AppMasterEventOperator) {\n        if (victim == null\n            || o.getConf().getStatistics().getDataSize() < victim.getConf().getStatistics()\n                .getDataSize()) {\n          victim = (AppMasterEventOperator) o;\n        }\n      }\n    }\n\n    if (victim == null ||\n        (!context.pruningOpsRemovedByPriorOpt.isEmpty() &&\n         context.pruningOpsRemovedByPriorOpt.contains(victim))) {\n      return;\n    }\n\n    GenTezUtils.getUtils().removeBranch(victim);\n    // at this point we've found the fork in the op pipeline that has the pruning as a child plan.\n    LOG.info(\"Disabling dynamic pruning for: \"\n        + ((DynamicPruningEventDesc) victim.getConf()).getTableScan().toString()\n        + \". Needed to break cyclic dependency\");\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.parse.GenTezUtils.findRoots": {
                "code_before_change": "  private void findRoots(Operator<?> op, List<Operator<?>> ops) {\n    List<Operator<?>> parents = op.getParentOperators();\n    if (parents == null || parents.isEmpty()) {\n      ops.add(op);\n      return;\n    }\n    for (Operator<?> p : parents) {\n      findRoots(p, ops);\n    }\n  }",
                "code_after_change": "  private void findRoots(Operator<?> op, List<Operator<?>> ops) {\n    List<Operator<?>> parents = op.getParentOperators();\n    if (parents == null || parents.isEmpty()) {\n      ops.add(op);\n      return;\n    }\n    for (Operator<?> p : parents) {\n      findRoots(p, ops);\n    }\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report precisely identifies the root cause as the method 'RemoveDynamicPruningBySize.process', which is one of the ground truth methods. However, there is no fix suggestion provided in the bug report. The problem location is also precisely identified as 'RemoveDynamicPruningBySize.process', which matches the ground truth. There is no wrong information in the bug report as all the details are consistent with the context of the bug."
        }
    },
    {
        "filename": "HIVE-9721.json",
        "code_diff": {
            "shims.0.23.src.main.java.org.apache.hadoop.hive.shims.Hadoop23Shims.setFullFileStatus": {
                "code_before_change": [],
                "code_after_change": []
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report precisely identifies the root cause by mentioning the method 'Hadoop23Shims.setFullFileStatus', which is the ground truth method where the bug occurred. However, there is no fix suggestion provided in the bug report, as there is no 'Suggestions' or 'problem_location' field, nor is there any mention of a fix in the 'Description' field. The problem location is also precisely identified as it directly mentions the method 'Hadoop23Shims.setFullFileStatus'. There is no wrong information in the bug report as all the details are consistent with the context of the bug."
        }
    },
    {
        "filename": "HIVE-4216.json",
        "code_diff": {
            "shims.0.23.src.main.java.org.apache.hadoop.hive.shims.Hadoop23Shims.getJobTrackerState": {
                "code_before_change": [],
                "code_after_change": []
            },
            "shims.0.23.src.main.java.org.apache.hadoop.hive.shims.Hadoop23Shims.newTaskAttemptContext": {
                "code_before_change": [],
                "code_after_change": []
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Missing",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report mentions a stack trace that includes methods like 'org.apache.hadoop.hive.ql.exec.ExecReducer.reduce' and 'org.apache.hadoop.hive.ql.io.HiveFileFormatUtils.getHiveRecordWriter', which are in the shared stack trace context with the ground truth methods. However, it does not precisely identify the root cause related to the ground truth methods. There is no fix suggestion provided in the bug report. The problem location is not precisely identified as the report does not mention any ground truth methods or closely related methods outside of the stack trace. All information in the bug report is relevant to the context of the bug, so there is no wrong information."
        }
    },
    {
        "filename": "HIVE-13836.json",
        "code_diff": {
            "hcatalog.server-extensions.src.main.java.org.apache.hive.hcatalog.listener.DbNotificationListener.now": {
                "code_before_change": "  private int now() {\n    long millis = System.currentTimeMillis();\n    millis /= 1000;\n    if (millis > Integer.MAX_VALUE) {\n      LOG.warn(\"We've passed max int value in seconds since the epoch, \" +\n          \"all notification times will be the same!\");\n      return Integer.MAX_VALUE;\n    }\n    return (int)millis;\n  }",
                "code_after_change": "  private int now() {\n    long millis = System.currentTimeMillis();\n    millis /= 1000;\n    if (millis > Integer.MAX_VALUE) {\n      LOG.warn(\"We've passed max int value in seconds since the epoch, \" +\n          \"all notification times will be the same!\");\n      return Integer.MAX_VALUE;\n    }\n    return (int)millis;\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the error as occurring in the context of transactions, specifically mentioning methods like 'TransactionManager.begin' and 'ObjectStore.openTransaction', which are part of the stack trace. However, it does not precisely identify the root cause in the 'DbNotificationListener.now' method, which is the ground truth. The report lacks any fix suggestion, as there is no 'Suggestions' or 'problem_location' field, nor any suggestion in the 'Description'. The problem location is partially identified as it mentions methods in the same stack trace context as the ground truth method. There is no wrong information in the report; all details are relevant to the context of the bug."
        }
    },
    {
        "filename": "HIVE-9873.json",
        "code_diff": {
            "ql.src.java.org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.getSplit": {
                "code_before_change": "  protected ParquetInputSplit getSplit(\n      final InputSplit oldSplit,\n      final JobConf conf\n      ) throws IOException {\n    ParquetInputSplit split;\n    if (oldSplit instanceof FileSplit) {\n      final Path finalPath = ((FileSplit) oldSplit).getPath();\n      final JobConf cloneJob = projectionPusher.pushProjectionsAndFilters(conf, finalPath.getParent());\n\n      final ParquetMetadata parquetMetadata = ParquetFileReader.readFooter(cloneJob, finalPath);\n      final List<BlockMetaData> blocks = parquetMetadata.getBlocks();\n      final FileMetaData fileMetaData = parquetMetadata.getFileMetaData();\n\n      final ReadContext readContext = new DataWritableReadSupport()\n          .init(cloneJob, fileMetaData.getKeyValueMetaData(), fileMetaData.getSchema());\n      schemaSize = MessageTypeParser.parseMessageType(readContext.getReadSupportMetadata()\n          .get(DataWritableReadSupport.HIVE_SCHEMA_KEY)).getFieldCount();\n      final List<BlockMetaData> splitGroup = new ArrayList<BlockMetaData>();\n      final long splitStart = ((FileSplit) oldSplit).getStart();\n      final long splitLength = ((FileSplit) oldSplit).getLength();\n      for (final BlockMetaData block : blocks) {\n        final long firstDataPage = block.getColumns().get(0).getFirstDataPageOffset();\n        if (firstDataPage >= splitStart && firstDataPage < splitStart + splitLength) {\n          splitGroup.add(block);\n        }\n      }\n      if (splitGroup.isEmpty()) {\n        LOG.warn(\"Skipping split, could not find row group in: \" + (FileSplit) oldSplit);\n        split = null;\n      } else {\n        if (HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVE_PARQUET_TIMESTAMP_SKIP_CONVERSION)) {\n          skipTimestampConversion = !Strings.nullToEmpty(fileMetaData.getCreatedBy()).startsWith(\"parquet-mr\");\n        }\n        split = new ParquetInputSplit(finalPath,\n                splitStart,\n                splitLength,\n                ((FileSplit) oldSplit).getLocations(),\n                splitGroup,\n                readContext.getRequestedSchema().toString(),\n                fileMetaData.getSchema().toString(),\n                fileMetaData.getKeyValueMetaData(),\n                readContext.getReadSupportMetadata());\n      }\n    } else {\n      throw new IllegalArgumentException(\"Unknown split type: \" + oldSplit);\n    }\n    return split;\n  }",
                "code_after_change": "  protected ParquetInputSplit getSplit(\n      final InputSplit oldSplit,\n      final JobConf conf\n      ) throws IOException {\n    ParquetInputSplit split;\n    if (oldSplit instanceof FileSplit) {\n      final Path finalPath = ((FileSplit) oldSplit).getPath();\n      jobConf = projectionPusher.pushProjectionsAndFilters(conf, finalPath.getParent());\n\n      final ParquetMetadata parquetMetadata = ParquetFileReader.readFooter(jobConf, finalPath);\n      final List<BlockMetaData> blocks = parquetMetadata.getBlocks();\n      final FileMetaData fileMetaData = parquetMetadata.getFileMetaData();\n\n      final ReadContext readContext = new DataWritableReadSupport()\n          .init(jobConf, fileMetaData.getKeyValueMetaData(), fileMetaData.getSchema());\n      schemaSize = MessageTypeParser.parseMessageType(readContext.getReadSupportMetadata()\n          .get(DataWritableReadSupport.HIVE_SCHEMA_KEY)).getFieldCount();\n      final List<BlockMetaData> splitGroup = new ArrayList<BlockMetaData>();\n      final long splitStart = ((FileSplit) oldSplit).getStart();\n      final long splitLength = ((FileSplit) oldSplit).getLength();\n      for (final BlockMetaData block : blocks) {\n        final long firstDataPage = block.getColumns().get(0).getFirstDataPageOffset();\n        if (firstDataPage >= splitStart && firstDataPage < splitStart + splitLength) {\n          splitGroup.add(block);\n        }\n      }\n      if (splitGroup.isEmpty()) {\n        LOG.warn(\"Skipping split, could not find row group in: \" + (FileSplit) oldSplit);\n        split = null;\n      } else {\n        if (HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVE_PARQUET_TIMESTAMP_SKIP_CONVERSION)) {\n          skipTimestampConversion = !Strings.nullToEmpty(fileMetaData.getCreatedBy()).startsWith(\"parquet-mr\");\n        }\n        split = new ParquetInputSplit(finalPath,\n                splitStart,\n                splitLength,\n                ((FileSplit) oldSplit).getLocations(),\n                splitGroup,\n                readContext.getRequestedSchema().toString(),\n                fileMetaData.getSchema().toString(),\n                fileMetaData.getKeyValueMetaData(),\n                readContext.getReadSupportMetadata());\n      }\n    } else {\n      throw new IllegalArgumentException(\"Unknown split type: \" + oldSplit);\n    }\n    return split;\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.io.parquet.read.ParquetRecordReaderWrapper.ParquetRecordReaderWrapper": {
                "code_before_change": [],
                "code_after_change": []
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report precisely identifies the root cause by mentioning the issue with the ParquetRecordReaderWrapper and the incorrect metadata until the call to projectionPusher.pushProjectionsAndFilters, which aligns with the ground truth method ParquetRecordReaderWrapper.getSplit. However, the report does not provide a specific fix suggestion, hence 'Missing' for fix suggestion. The problem location is also precise as it mentions ParquetRecordReaderWrapper, which is part of the ground truth methods. There is no wrong information in the bug report as all details are relevant to the context of the bug."
        }
    },
    {
        "filename": "HIVE-13174.json",
        "code_diff": {
            "ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.validateAggregationDesc": {
                "code_before_change": "  private boolean validateAggregationDesc(List<AggregationDesc> descs, boolean isReduceMergePartial, boolean hasKeys) {\n    for (AggregationDesc d : descs) {\n      boolean ret = validateAggregationDesc(d, isReduceMergePartial, hasKeys);\n      if (!ret) {\n        return false;\n      }\n    }\n    return true;\n  }",
                "code_after_change": "  private boolean validateAggregationDesc(List<AggregationDesc> descs, boolean isReduceMergePartial, boolean hasKeys) {\n    for (AggregationDesc d : descs) {\n      boolean ret = validateAggregationDesc(d, isReduceMergePartial, hasKeys);\n      if (!ret) {\n        return false;\n      }\n    }\n    return true;\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.optimizer.physical.Vectorizer.validateExprNodeDesc": {
                "code_before_change": "  private boolean validateExprNodeDesc(List<ExprNodeDesc> descs) {\n    return validateExprNodeDesc(descs, VectorExpressionDescriptor.Mode.PROJECTION);\n  }",
                "code_after_change": "  private boolean validateExprNodeDesc(List<ExprNodeDesc> descs) {\n    return validateExprNodeDesc(descs, VectorExpressionDescriptor.Mode.PROJECTION);\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report mentions the method 'validateExprNodeDesc' which is part of the stack trace and is a ground truth method, but it does not precisely identify the root cause related to the ground truth methods. The report does not provide any fix suggestion, hence 'Missing' for fix suggestion. The problem location is identified as 'Partial' with 'Shared Stack Trace Context' because it mentions methods in the stack trace but not the exact ground truth methods. There is no wrong information in the bug report as all mentioned details are relevant to the context."
        }
    },
    {
        "filename": "HIVE-5431.json",
        "code_diff": {
            "ql.src.java.org.apache.hadoop.hive.ql.plan.PlanUtils.configureJobPropertiesForStorageHandler": {
                "code_before_change": "  private static void configureJobPropertiesForStorageHandler(boolean input,\n    TableDesc tableDesc) {\n\n    if (tableDesc == null) {\n      return;\n    }\n\n    try {\n      HiveStorageHandler storageHandler =\n        HiveUtils.getStorageHandler(\n          Hive.get().getConf(),\n          tableDesc.getProperties().getProperty(\n            org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.META_TABLE_STORAGE));\n      if (storageHandler != null) {\n        Map<String, String> jobProperties = new LinkedHashMap<String, String>();\n        if(input) {\n            try {\n                storageHandler.configureInputJobProperties(\n                  tableDesc,\n                  jobProperties);\n            } catch(AbstractMethodError e) {\n                LOG.debug(\"configureInputJobProperties not found \"+\n                    \"using configureTableJobProperties\",e);\n                storageHandler.configureTableJobProperties(tableDesc, jobProperties);\n            }\n        }\n        else {\n            try {\n                storageHandler.configureOutputJobProperties(\n                  tableDesc,\n                  jobProperties);\n            } catch(AbstractMethodError e) {\n                LOG.debug(\"configureOutputJobProperties not found\"+\n                    \"using configureTableJobProperties\",e);\n                storageHandler.configureTableJobProperties(tableDesc, jobProperties);\n            }\n        }\n        // Job properties are only relevant for non-native tables, so\n        // for native tables, leave it null to avoid cluttering up\n        // plans.\n        if (!jobProperties.isEmpty()) {\n          if (tableDesc.getOutputFileFormatClass().getName() == HivePassThroughOutputFormat.HIVE_PASSTHROUGH_OF_CLASSNAME) {\n            // get the real output format when we register this for the table\n            jobProperties.put(HivePassThroughOutputFormat.HIVE_PASSTHROUGH_STORAGEHANDLER_OF_JOBCONFKEY,HiveFileFormatUtils.getRealOutputFormatClassName());\n          }\n          tableDesc.setJobProperties(jobProperties);\n        }\n      }\n    } catch (HiveException ex) {\n      throw new RuntimeException(ex);\n    }\n  }",
                "code_after_change": "  private static void configureJobPropertiesForStorageHandler(boolean input,\n    TableDesc tableDesc) {\n\n    if (tableDesc == null) {\n      return;\n    }\n\n    try {\n      HiveStorageHandler storageHandler =\n        HiveUtils.getStorageHandler(\n          Hive.get().getConf(),\n          tableDesc.getProperties().getProperty(\n            org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.META_TABLE_STORAGE));\n      if (storageHandler != null) {\n        Map<String, String> jobProperties = new LinkedHashMap<String, String>();\n        if(input) {\n            try {\n                storageHandler.configureInputJobProperties(\n                  tableDesc,\n                  jobProperties);\n            } catch(AbstractMethodError e) {\n                LOG.debug(\"configureInputJobProperties not found \"+\n                    \"using configureTableJobProperties\",e);\n                storageHandler.configureTableJobProperties(tableDesc, jobProperties);\n            }\n        }\n        else {\n            try {\n                storageHandler.configureOutputJobProperties(\n                  tableDesc,\n                  jobProperties);\n            } catch(AbstractMethodError e) {\n                LOG.debug(\"configureOutputJobProperties not found\"+\n                    \"using configureTableJobProperties\",e);\n                storageHandler.configureTableJobProperties(tableDesc, jobProperties);\n            }\n            if (tableDesc.getOutputFileFormatClass().getName()\n                     == HivePassThroughOutputFormat.HIVE_PASSTHROUGH_OF_CLASSNAME) {\n             // get the real output format when we register this for the table\n             jobProperties.put(\n                 HivePassThroughOutputFormat.HIVE_PASSTHROUGH_STORAGEHANDLER_OF_JOBCONFKEY,\n                 HiveFileFormatUtils.getRealOutputFormatClassName());\n           }\n        }\n        // Job properties are only relevant for non-native tables, so\n        // for native tables, leave it null to avoid cluttering up\n        // plans.\n        if (!jobProperties.isEmpty()) {\n          tableDesc.setJobProperties(jobProperties);\n        }\n      }\n    } catch (HiveException ex) {\n      throw new RuntimeException(ex);\n    }\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report precisely identifies the root cause by mentioning that the issue is with the 'PlanUtils.configureJobPropertiesForStorageHandler' method, which is the ground truth method. However, the bug report does not provide any fix suggestion, hence it is marked as 'Missing' for fix suggestion. The problem location is also precisely identified as it directly points to the method where the bug occurred. There is no wrong information in the bug report as all the details are consistent with the context of the bug."
        }
    },
    {
        "filename": "HIVE-13115.json",
        "code_diff": {
            "metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getPartitionsFromPartitionIds": {
                "code_before_change": "  private List<Partition> getPartitionsFromPartitionIds(String dbName, String tblName,\n      Boolean isView, List<Object> partIdList) throws MetaException {\n    boolean doTrace = LOG.isDebugEnabled();\n    int idStringWidth = (int)Math.ceil(Math.log10(partIdList.size())) + 1; // 1 for comma\n    int sbCapacity = partIdList.size() * idStringWidth;\n    // Prepare StringBuilder for \"PART_ID in (...)\" to use in future queries.\n    StringBuilder partSb = new StringBuilder(sbCapacity);\n    for (Object partitionId : partIdList) {\n      partSb.append(extractSqlLong(partitionId)).append(\",\");\n    }\n    String partIds = trimCommaList(partSb);\n\n    // Get most of the fields for the IDs provided.\n    // Assume db and table names are the same for all partition, as provided in arguments.\n    String queryText =\n      \"select \\\"PARTITIONS\\\".\\\"PART_ID\\\", \\\"SDS\\\".\\\"SD_ID\\\", \\\"SDS\\\".\\\"CD_ID\\\",\"\n    + \" \\\"SERDES\\\".\\\"SERDE_ID\\\", \\\"PARTITIONS\\\".\\\"CREATE_TIME\\\",\"\n    + \" \\\"PARTITIONS\\\".\\\"LAST_ACCESS_TIME\\\", \\\"SDS\\\".\\\"INPUT_FORMAT\\\", \\\"SDS\\\".\\\"IS_COMPRESSED\\\",\"\n    + \" \\\"SDS\\\".\\\"IS_STOREDASSUBDIRECTORIES\\\", \\\"SDS\\\".\\\"LOCATION\\\", \\\"SDS\\\".\\\"NUM_BUCKETS\\\",\"\n    + \" \\\"SDS\\\".\\\"OUTPUT_FORMAT\\\", \\\"SERDES\\\".\\\"NAME\\\", \\\"SERDES\\\".\\\"SLIB\\\" \"\n    + \"from \\\"PARTITIONS\\\"\"\n    + \"  left outer join \\\"SDS\\\" on \\\"PARTITIONS\\\".\\\"SD_ID\\\" = \\\"SDS\\\".\\\"SD_ID\\\" \"\n    + \"  left outer join \\\"SERDES\\\" on \\\"SDS\\\".\\\"SERDE_ID\\\" = \\\"SERDES\\\".\\\"SERDE_ID\\\" \"\n    + \"where \\\"PART_ID\\\" in (\" + partIds + \") order by \\\"PART_NAME\\\" asc\";\n    long start = doTrace ? System.nanoTime() : 0;\n    Query query = pm.newQuery(\"javax.jdo.query.SQL\", queryText);\n    List<Object[]> sqlResult = executeWithArray(query, null, queryText);\n    long queryTime = doTrace ? System.nanoTime() : 0;\n    Deadline.checkTimeout();\n\n    // Read all the fields and create partitions, SDs and serdes.\n    TreeMap<Long, Partition> partitions = new TreeMap<Long, Partition>();\n    TreeMap<Long, StorageDescriptor> sds = new TreeMap<Long, StorageDescriptor>();\n    TreeMap<Long, SerDeInfo> serdes = new TreeMap<Long, SerDeInfo>();\n    TreeMap<Long, List<FieldSchema>> colss = new TreeMap<Long, List<FieldSchema>>();\n    // Keep order by name, consistent with JDO.\n    ArrayList<Partition> orderedResult = new ArrayList<Partition>(partIdList.size());\n\n    // Prepare StringBuilder-s for \"in (...)\" lists to use in one-to-many queries.\n    StringBuilder sdSb = new StringBuilder(sbCapacity), serdeSb = new StringBuilder(sbCapacity);\n    StringBuilder colsSb = new StringBuilder(7); // We expect that there's only one field schema.\n    tblName = tblName.toLowerCase();\n    dbName = dbName.toLowerCase();\n    for (Object[] fields : sqlResult) {\n      // Here comes the ugly part...\n      long partitionId = extractSqlLong(fields[0]);\n      Long sdId = extractSqlLong(fields[1]);\n      Long colId = extractSqlLong(fields[2]);\n      Long serdeId = extractSqlLong(fields[3]);\n      // A partition must have either everything set, or nothing set if it's a view.\n      if (sdId == null || colId == null || serdeId == null) {\n        if (isView == null) {\n          isView = isViewTable(dbName, tblName);\n        }\n        if ((sdId != null || colId != null || serdeId != null) || !isView) {\n          throw new MetaException(\"Unexpected null for one of the IDs, SD \" + sdId + \", column \"\n              + colId + \", serde \" + serdeId + \" for a \" + (isView ? \"\" : \"non-\") + \" view\");\n        }\n      }\n\n      Partition part = new Partition();\n      orderedResult.add(part);\n      // Set the collection fields; some code might not check presence before accessing them.\n      part.setParameters(new HashMap<String, String>());\n      part.setValues(new ArrayList<String>());\n      part.setDbName(dbName);\n      part.setTableName(tblName);\n      if (fields[4] != null) part.setCreateTime(extractSqlInt(fields[4]));\n      if (fields[5] != null) part.setLastAccessTime(extractSqlInt(fields[5]));\n      partitions.put(partitionId, part);\n\n      if (sdId == null) continue; // Probably a view.\n      assert colId != null && serdeId != null;\n\n      // We assume each partition has an unique SD.\n      StorageDescriptor sd = new StorageDescriptor();\n      StorageDescriptor oldSd = sds.put(sdId, sd);\n      if (oldSd != null) {\n        throw new MetaException(\"Partitions reuse SDs; we don't expect that\");\n      }\n      // Set the collection fields; some code might not check presence before accessing them.\n      sd.setSortCols(new ArrayList<Order>());\n      sd.setBucketCols(new ArrayList<String>());\n      sd.setParameters(new HashMap<String, String>());\n      sd.setSkewedInfo(new SkewedInfo(new ArrayList<String>(),\n          new ArrayList<List<String>>(), new HashMap<List<String>, String>()));\n      sd.setInputFormat((String)fields[6]);\n      Boolean tmpBoolean = extractSqlBoolean(fields[7]);\n      if (tmpBoolean != null) sd.setCompressed(tmpBoolean);\n      tmpBoolean = extractSqlBoolean(fields[8]);\n      if (tmpBoolean != null) sd.setStoredAsSubDirectories(tmpBoolean);\n      sd.setLocation((String)fields[9]);\n      if (fields[10] != null) sd.setNumBuckets(extractSqlInt(fields[10]));\n      sd.setOutputFormat((String)fields[11]);\n      sdSb.append(sdId).append(\",\");\n      part.setSd(sd);\n\n      List<FieldSchema> cols = colss.get(colId);\n      // We expect that colId will be the same for all (or many) SDs.\n      if (cols == null) {\n        cols = new ArrayList<FieldSchema>();\n        colss.put(colId, cols);\n        colsSb.append(colId).append(\",\");\n      }\n      sd.setCols(cols);\n\n      // We assume each SD has an unique serde.\n      SerDeInfo serde = new SerDeInfo();\n      SerDeInfo oldSerde = serdes.put(serdeId, serde);\n      if (oldSerde != null) {\n        throw new MetaException(\"SDs reuse serdes; we don't expect that\");\n      }\n      serde.setParameters(new HashMap<String, String>());\n      serde.setName((String)fields[12]);\n      serde.setSerializationLib((String)fields[13]);\n      serdeSb.append(serdeId).append(\",\");\n      sd.setSerdeInfo(serde);\n      Deadline.checkTimeout();\n    }\n    query.closeAll();\n    timingTrace(doTrace, queryText, start, queryTime);\n\n    // Now get all the one-to-many things. Start with partitions.\n    queryText = \"select \\\"PART_ID\\\", \\\"PARAM_KEY\\\", \\\"PARAM_VALUE\\\" from \\\"PARTITION_PARAMS\\\"\"\n        + \" where \\\"PART_ID\\\" in (\" + partIds + \") and \\\"PARAM_KEY\\\" is not null\"\n        + \" order by \\\"PART_ID\\\" asc\";\n    loopJoinOrderedResult(partitions, queryText, 0, new ApplyFunc<Partition>() {\n      @Override\n      public void apply(Partition t, Object[] fields) {\n        t.putToParameters((String)fields[1], (String)fields[2]);\n      }});\n    // Perform conversion of null map values\n    for (Partition t : partitions.values()) {\n      t.setParameters(MetaStoreUtils.trimMapNulls(t.getParameters(), convertMapNullsToEmptyStrings));\n    }\n\n    queryText = \"select \\\"PART_ID\\\", \\\"PART_KEY_VAL\\\" from \\\"PARTITION_KEY_VALS\\\"\"\n        + \" where \\\"PART_ID\\\" in (\" + partIds + \") and \\\"INTEGER_IDX\\\" >= 0\"\n        + \" order by \\\"PART_ID\\\" asc, \\\"INTEGER_IDX\\\" asc\";\n    loopJoinOrderedResult(partitions, queryText, 0, new ApplyFunc<Partition>() {\n      @Override\n      public void apply(Partition t, Object[] fields) {\n        t.addToValues((String)fields[1]);\n      }});\n\n    // Prepare IN (blah) lists for the following queries. Cut off the final ','s.\n    if (sdSb.length() == 0) {\n      assert serdeSb.length() == 0 && colsSb.length() == 0;\n      return orderedResult; // No SDs, probably a view.\n    }\n    String sdIds = trimCommaList(sdSb), serdeIds = trimCommaList(serdeSb),\n        colIds = trimCommaList(colsSb);\n\n    // Get all the stuff for SD. Don't do empty-list check - we expect partitions do have SDs.\n    queryText = \"select \\\"SD_ID\\\", \\\"PARAM_KEY\\\", \\\"PARAM_VALUE\\\" from \\\"SD_PARAMS\\\"\"\n        + \" where \\\"SD_ID\\\" in (\" + sdIds + \") and \\\"PARAM_KEY\\\" is not null\"\n        + \" order by \\\"SD_ID\\\" asc\";\n    loopJoinOrderedResult(sds, queryText, 0, new ApplyFunc<StorageDescriptor>() {\n      @Override\n      public void apply(StorageDescriptor t, Object[] fields) {\n        t.putToParameters((String)fields[1], (String)fields[2]);\n      }});\n    // Perform conversion of null map values\n    for (StorageDescriptor t : sds.values()) {\n      t.setParameters(MetaStoreUtils.trimMapNulls(t.getParameters(), convertMapNullsToEmptyStrings));\n    }\n\n    queryText = \"select \\\"SD_ID\\\", \\\"COLUMN_NAME\\\", \\\"SORT_COLS\\\".\\\"ORDER\\\" from \\\"SORT_COLS\\\"\"\n        + \" where \\\"SD_ID\\\" in (\" + sdIds + \") and \\\"INTEGER_IDX\\\" >= 0\"\n        + \" order by \\\"SD_ID\\\" asc, \\\"INTEGER_IDX\\\" asc\";\n    loopJoinOrderedResult(sds, queryText, 0, new ApplyFunc<StorageDescriptor>() {\n      @Override\n      public void apply(StorageDescriptor t, Object[] fields) {\n        if (fields[2] == null) return;\n        t.addToSortCols(new Order((String)fields[1], extractSqlInt(fields[2])));\n      }});\n\n    queryText = \"select \\\"SD_ID\\\", \\\"BUCKET_COL_NAME\\\" from \\\"BUCKETING_COLS\\\"\"\n        + \" where \\\"SD_ID\\\" in (\" + sdIds + \") and \\\"INTEGER_IDX\\\" >= 0\"\n        + \" order by \\\"SD_ID\\\" asc, \\\"INTEGER_IDX\\\" asc\";\n    loopJoinOrderedResult(sds, queryText, 0, new ApplyFunc<StorageDescriptor>() {\n      @Override\n      public void apply(StorageDescriptor t, Object[] fields) {\n        t.addToBucketCols((String)fields[1]);\n      }});\n\n    // Skewed columns stuff.\n    queryText = \"select \\\"SD_ID\\\", \\\"SKEWED_COL_NAME\\\" from \\\"SKEWED_COL_NAMES\\\"\"\n        + \" where \\\"SD_ID\\\" in (\" + sdIds + \") and \\\"INTEGER_IDX\\\" >= 0\"\n        + \" order by \\\"SD_ID\\\" asc, \\\"INTEGER_IDX\\\" asc\";\n    boolean hasSkewedColumns =\n      loopJoinOrderedResult(sds, queryText, 0, new ApplyFunc<StorageDescriptor>() {\n        @Override\n        public void apply(StorageDescriptor t, Object[] fields) {\n          if (!t.isSetSkewedInfo()) t.setSkewedInfo(new SkewedInfo());\n          t.getSkewedInfo().addToSkewedColNames((String)fields[1]);\n        }}) > 0;\n\n    // Assume we don't need to fetch the rest of the skewed column data if we have no columns.\n    if (hasSkewedColumns) {\n      // We are skipping the SKEWED_STRING_LIST table here, as it seems to be totally useless.\n      queryText =\n            \"select \\\"SKEWED_VALUES\\\".\\\"SD_ID_OID\\\",\"\n          + \"  \\\"SKEWED_STRING_LIST_VALUES\\\".\\\"STRING_LIST_ID\\\",\"\n          + \"  \\\"SKEWED_STRING_LIST_VALUES\\\".\\\"STRING_LIST_VALUE\\\" \"\n          + \"from \\\"SKEWED_VALUES\\\" \"\n          + \"  left outer join \\\"SKEWED_STRING_LIST_VALUES\\\" on \\\"SKEWED_VALUES\\\".\"\n          + \"\\\"STRING_LIST_ID_EID\\\" = \\\"SKEWED_STRING_LIST_VALUES\\\".\\\"STRING_LIST_ID\\\" \"\n          + \"where \\\"SKEWED_VALUES\\\".\\\"SD_ID_OID\\\" in (\" + sdIds + \") \"\n          + \"  and \\\"SKEWED_VALUES\\\".\\\"STRING_LIST_ID_EID\\\" is not null \"\n          + \"  and \\\"SKEWED_VALUES\\\".\\\"INTEGER_IDX\\\" >= 0 \"\n          + \"order by \\\"SKEWED_VALUES\\\".\\\"SD_ID_OID\\\" asc, \\\"SKEWED_VALUES\\\".\\\"INTEGER_IDX\\\" asc,\"\n          + \"  \\\"SKEWED_STRING_LIST_VALUES\\\".\\\"INTEGER_IDX\\\" asc\";\n      loopJoinOrderedResult(sds, queryText, 0, new ApplyFunc<StorageDescriptor>() {\n        private Long currentListId;\n        private List<String> currentList;\n        @Override\n        public void apply(StorageDescriptor t, Object[] fields) throws MetaException {\n          if (!t.isSetSkewedInfo()) t.setSkewedInfo(new SkewedInfo());\n          // Note that this is not a typical list accumulator - there's no call to finalize\n          // the last list. Instead we add list to SD first, as well as locally to add elements.\n          if (fields[1] == null) {\n            currentList = null; // left outer join produced a list with no values\n            currentListId = null;\n            t.getSkewedInfo().addToSkewedColValues(new ArrayList<String>());\n          } else {\n            long fieldsListId = extractSqlLong(fields[1]);\n            if (currentListId == null || fieldsListId != currentListId) {\n              currentList = new ArrayList<String>();\n              currentListId = fieldsListId;\n              t.getSkewedInfo().addToSkewedColValues(currentList);\n            }\n            currentList.add((String)fields[2]);\n          }\n        }});\n\n      // We are skipping the SKEWED_STRING_LIST table here, as it seems to be totally useless.\n      queryText =\n            \"select \\\"SKEWED_COL_VALUE_LOC_MAP\\\".\\\"SD_ID\\\",\"\n          + \" \\\"SKEWED_STRING_LIST_VALUES\\\".STRING_LIST_ID,\"\n          + \" \\\"SKEWED_COL_VALUE_LOC_MAP\\\".\\\"LOCATION\\\",\"\n          + \" \\\"SKEWED_STRING_LIST_VALUES\\\".\\\"STRING_LIST_VALUE\\\" \"\n          + \"from \\\"SKEWED_COL_VALUE_LOC_MAP\\\"\"\n          + \"  left outer join \\\"SKEWED_STRING_LIST_VALUES\\\" on \\\"SKEWED_COL_VALUE_LOC_MAP\\\".\"\n          + \"\\\"STRING_LIST_ID_KID\\\" = \\\"SKEWED_STRING_LIST_VALUES\\\".\\\"STRING_LIST_ID\\\" \"\n          + \"where \\\"SKEWED_COL_VALUE_LOC_MAP\\\".\\\"SD_ID\\\" in (\" + sdIds + \")\"\n          + \"  and \\\"SKEWED_COL_VALUE_LOC_MAP\\\".\\\"STRING_LIST_ID_KID\\\" is not null \"\n          + \"order by \\\"SKEWED_COL_VALUE_LOC_MAP\\\".\\\"SD_ID\\\" asc,\"\n          + \"  \\\"SKEWED_STRING_LIST_VALUES\\\".\\\"STRING_LIST_ID\\\" asc,\"\n          + \"  \\\"SKEWED_STRING_LIST_VALUES\\\".\\\"INTEGER_IDX\\\" asc\";\n\n      loopJoinOrderedResult(sds, queryText, 0, new ApplyFunc<StorageDescriptor>() {\n        private Long currentListId;\n        private List<String> currentList;\n        @Override\n        public void apply(StorageDescriptor t, Object[] fields) throws MetaException {\n          if (!t.isSetSkewedInfo()) {\n            SkewedInfo skewedInfo = new SkewedInfo();\n            skewedInfo.setSkewedColValueLocationMaps(new HashMap<List<String>, String>());\n            t.setSkewedInfo(skewedInfo);\n          }\n          Map<List<String>, String> skewMap = t.getSkewedInfo().getSkewedColValueLocationMaps();\n          // Note that this is not a typical list accumulator - there's no call to finalize\n          // the last list. Instead we add list to SD first, as well as locally to add elements.\n          if (fields[1] == null) {\n            currentList = new ArrayList<String>(); // left outer join produced a list with no values\n            currentListId = null;\n          } else {\n            long fieldsListId = extractSqlLong(fields[1]);\n            if (currentListId == null || fieldsListId != currentListId) {\n              currentList = new ArrayList<String>();\n              currentListId = fieldsListId;\n            } else {\n              skewMap.remove(currentList); // value based compare.. remove first\n            }\n            currentList.add((String)fields[3]);\n          }\n          skewMap.put(currentList, (String)fields[2]);\n        }});\n    } // if (hasSkewedColumns)\n\n    // Get FieldSchema stuff if any.\n    if (!colss.isEmpty()) {\n      // We are skipping the CDS table here, as it seems to be totally useless.\n      queryText = \"select \\\"CD_ID\\\", \\\"COMMENT\\\", \\\"COLUMN_NAME\\\", \\\"TYPE_NAME\\\"\"\n          + \" from \\\"COLUMNS_V2\\\" where \\\"CD_ID\\\" in (\" + colIds + \") and \\\"INTEGER_IDX\\\" >= 0\"\n          + \" order by \\\"CD_ID\\\" asc, \\\"INTEGER_IDX\\\" asc\";\n      loopJoinOrderedResult(colss, queryText, 0, new ApplyFunc<List<FieldSchema>>() {\n        @Override\n        public void apply(List<FieldSchema> t, Object[] fields) {\n          t.add(new FieldSchema((String)fields[2], (String)fields[3], (String)fields[1]));\n        }});\n    }\n\n    // Finally, get all the stuff for serdes - just the params.\n    queryText = \"select \\\"SERDE_ID\\\", \\\"PARAM_KEY\\\", \\\"PARAM_VALUE\\\" from \\\"SERDE_PARAMS\\\"\"\n        + \" where \\\"SERDE_ID\\\" in (\" + serdeIds + \") and \\\"PARAM_KEY\\\" is not null\"\n        + \" order by \\\"SERDE_ID\\\" asc\";\n    loopJoinOrderedResult(serdes, queryText, 0, new ApplyFunc<SerDeInfo>() {\n      @Override\n      public void apply(SerDeInfo t, Object[] fields) {\n        t.putToParameters((String)fields[1], (String)fields[2]);\n      }});\n    // Perform conversion of null map values\n    for (SerDeInfo t : serdes.values()) {\n      t.setParameters(MetaStoreUtils.trimMapNulls(t.getParameters(), convertMapNullsToEmptyStrings));\n    }\n\n    return orderedResult;\n  }",
                "code_after_change": "  private List<Partition> getPartitionsFromPartitionIds(String dbName, String tblName,\n      Boolean isView, List<Object> partIdList) throws MetaException {\n    boolean doTrace = LOG.isDebugEnabled();\n    int idStringWidth = (int)Math.ceil(Math.log10(partIdList.size())) + 1; // 1 for comma\n    int sbCapacity = partIdList.size() * idStringWidth;\n    // Prepare StringBuilder for \"PART_ID in (...)\" to use in future queries.\n    StringBuilder partSb = new StringBuilder(sbCapacity);\n    for (Object partitionId : partIdList) {\n      partSb.append(extractSqlLong(partitionId)).append(\",\");\n    }\n    String partIds = trimCommaList(partSb);\n\n    // Get most of the fields for the IDs provided.\n    // Assume db and table names are the same for all partition, as provided in arguments.\n    String queryText =\n      \"select \\\"PARTITIONS\\\".\\\"PART_ID\\\", \\\"SDS\\\".\\\"SD_ID\\\", \\\"SDS\\\".\\\"CD_ID\\\",\"\n    + \" \\\"SERDES\\\".\\\"SERDE_ID\\\", \\\"PARTITIONS\\\".\\\"CREATE_TIME\\\",\"\n    + \" \\\"PARTITIONS\\\".\\\"LAST_ACCESS_TIME\\\", \\\"SDS\\\".\\\"INPUT_FORMAT\\\", \\\"SDS\\\".\\\"IS_COMPRESSED\\\",\"\n    + \" \\\"SDS\\\".\\\"IS_STOREDASSUBDIRECTORIES\\\", \\\"SDS\\\".\\\"LOCATION\\\", \\\"SDS\\\".\\\"NUM_BUCKETS\\\",\"\n    + \" \\\"SDS\\\".\\\"OUTPUT_FORMAT\\\", \\\"SERDES\\\".\\\"NAME\\\", \\\"SERDES\\\".\\\"SLIB\\\" \"\n    + \"from \\\"PARTITIONS\\\"\"\n    + \"  left outer join \\\"SDS\\\" on \\\"PARTITIONS\\\".\\\"SD_ID\\\" = \\\"SDS\\\".\\\"SD_ID\\\" \"\n    + \"  left outer join \\\"SERDES\\\" on \\\"SDS\\\".\\\"SERDE_ID\\\" = \\\"SERDES\\\".\\\"SERDE_ID\\\" \"\n    + \"where \\\"PART_ID\\\" in (\" + partIds + \") order by \\\"PART_NAME\\\" asc\";\n    long start = doTrace ? System.nanoTime() : 0;\n    Query query = pm.newQuery(\"javax.jdo.query.SQL\", queryText);\n    @SuppressWarnings(\"unchecked\")\n    List<Object[]> sqlResult = executeWithArray(query, null, queryText);\n    long queryTime = doTrace ? System.nanoTime() : 0;\n    Deadline.checkTimeout();\n\n    // Read all the fields and create partitions, SDs and serdes.\n    TreeMap<Long, Partition> partitions = new TreeMap<Long, Partition>();\n    TreeMap<Long, StorageDescriptor> sds = new TreeMap<Long, StorageDescriptor>();\n    TreeMap<Long, SerDeInfo> serdes = new TreeMap<Long, SerDeInfo>();\n    TreeMap<Long, List<FieldSchema>> colss = new TreeMap<Long, List<FieldSchema>>();\n    // Keep order by name, consistent with JDO.\n    ArrayList<Partition> orderedResult = new ArrayList<Partition>(partIdList.size());\n\n    // Prepare StringBuilder-s for \"in (...)\" lists to use in one-to-many queries.\n    StringBuilder sdSb = new StringBuilder(sbCapacity), serdeSb = new StringBuilder(sbCapacity);\n    StringBuilder colsSb = new StringBuilder(7); // We expect that there's only one field schema.\n    tblName = tblName.toLowerCase();\n    dbName = dbName.toLowerCase();\n    for (Object[] fields : sqlResult) {\n      // Here comes the ugly part...\n      long partitionId = extractSqlLong(fields[0]);\n      Long sdId = extractSqlLong(fields[1]);\n      Long colId = extractSqlLong(fields[2]);\n      Long serdeId = extractSqlLong(fields[3]);\n      // A partition must have at least sdId and serdeId set, or nothing set if it's a view.\n      if (sdId == null || serdeId == null) {\n        if (isView == null) {\n          isView = isViewTable(dbName, tblName);\n        }\n        if ((sdId != null || colId != null || serdeId != null) || !isView) {\n          throw new MetaException(\"Unexpected null for one of the IDs, SD \" + sdId +\n                  \", serde \" + serdeId + \" for a \" + (isView ? \"\" : \"non-\") + \" view\");\n        }\n      }\n\n      Partition part = new Partition();\n      orderedResult.add(part);\n      // Set the collection fields; some code might not check presence before accessing them.\n      part.setParameters(new HashMap<String, String>());\n      part.setValues(new ArrayList<String>());\n      part.setDbName(dbName);\n      part.setTableName(tblName);\n      if (fields[4] != null) part.setCreateTime(extractSqlInt(fields[4]));\n      if (fields[5] != null) part.setLastAccessTime(extractSqlInt(fields[5]));\n      partitions.put(partitionId, part);\n\n      if (sdId == null) continue; // Probably a view.\n      assert serdeId != null;\n\n      // We assume each partition has an unique SD.\n      StorageDescriptor sd = new StorageDescriptor();\n      StorageDescriptor oldSd = sds.put(sdId, sd);\n      if (oldSd != null) {\n        throw new MetaException(\"Partitions reuse SDs; we don't expect that\");\n      }\n      // Set the collection fields; some code might not check presence before accessing them.\n      sd.setSortCols(new ArrayList<Order>());\n      sd.setBucketCols(new ArrayList<String>());\n      sd.setParameters(new HashMap<String, String>());\n      sd.setSkewedInfo(new SkewedInfo(new ArrayList<String>(),\n          new ArrayList<List<String>>(), new HashMap<List<String>, String>()));\n      sd.setInputFormat((String)fields[6]);\n      Boolean tmpBoolean = extractSqlBoolean(fields[7]);\n      if (tmpBoolean != null) sd.setCompressed(tmpBoolean);\n      tmpBoolean = extractSqlBoolean(fields[8]);\n      if (tmpBoolean != null) sd.setStoredAsSubDirectories(tmpBoolean);\n      sd.setLocation((String)fields[9]);\n      if (fields[10] != null) sd.setNumBuckets(extractSqlInt(fields[10]));\n      sd.setOutputFormat((String)fields[11]);\n      sdSb.append(sdId).append(\",\");\n      part.setSd(sd);\n\n      if (colId != null) {\n        List<FieldSchema> cols = colss.get(colId);\n        // We expect that colId will be the same for all (or many) SDs.\n        if (cols == null) {\n          cols = new ArrayList<FieldSchema>();\n          colss.put(colId, cols);\n          colsSb.append(colId).append(\",\");\n        }\n        sd.setCols(cols);\n      }\n\n      // We assume each SD has an unique serde.\n      SerDeInfo serde = new SerDeInfo();\n      SerDeInfo oldSerde = serdes.put(serdeId, serde);\n      if (oldSerde != null) {\n        throw new MetaException(\"SDs reuse serdes; we don't expect that\");\n      }\n      serde.setParameters(new HashMap<String, String>());\n      serde.setName((String)fields[12]);\n      serde.setSerializationLib((String)fields[13]);\n      serdeSb.append(serdeId).append(\",\");\n      sd.setSerdeInfo(serde);\n      Deadline.checkTimeout();\n    }\n    query.closeAll();\n    timingTrace(doTrace, queryText, start, queryTime);\n\n    // Now get all the one-to-many things. Start with partitions.\n    queryText = \"select \\\"PART_ID\\\", \\\"PARAM_KEY\\\", \\\"PARAM_VALUE\\\" from \\\"PARTITION_PARAMS\\\"\"\n        + \" where \\\"PART_ID\\\" in (\" + partIds + \") and \\\"PARAM_KEY\\\" is not null\"\n        + \" order by \\\"PART_ID\\\" asc\";\n    loopJoinOrderedResult(partitions, queryText, 0, new ApplyFunc<Partition>() {\n      @Override\n      public void apply(Partition t, Object[] fields) {\n        t.putToParameters((String)fields[1], (String)fields[2]);\n      }});\n    // Perform conversion of null map values\n    for (Partition t : partitions.values()) {\n      t.setParameters(MetaStoreUtils.trimMapNulls(t.getParameters(), convertMapNullsToEmptyStrings));\n    }\n\n    queryText = \"select \\\"PART_ID\\\", \\\"PART_KEY_VAL\\\" from \\\"PARTITION_KEY_VALS\\\"\"\n        + \" where \\\"PART_ID\\\" in (\" + partIds + \") and \\\"INTEGER_IDX\\\" >= 0\"\n        + \" order by \\\"PART_ID\\\" asc, \\\"INTEGER_IDX\\\" asc\";\n    loopJoinOrderedResult(partitions, queryText, 0, new ApplyFunc<Partition>() {\n      @Override\n      public void apply(Partition t, Object[] fields) {\n        t.addToValues((String)fields[1]);\n      }});\n\n    // Prepare IN (blah) lists for the following queries. Cut off the final ','s.\n    if (sdSb.length() == 0) {\n      assert serdeSb.length() == 0 && colsSb.length() == 0;\n      return orderedResult; // No SDs, probably a view.\n    }\n\n    String sdIds = trimCommaList(sdSb);\n    String serdeIds = trimCommaList(serdeSb);\n    String colIds = trimCommaList(colsSb);\n\n    // Get all the stuff for SD. Don't do empty-list check - we expect partitions do have SDs.\n    queryText = \"select \\\"SD_ID\\\", \\\"PARAM_KEY\\\", \\\"PARAM_VALUE\\\" from \\\"SD_PARAMS\\\"\"\n        + \" where \\\"SD_ID\\\" in (\" + sdIds + \") and \\\"PARAM_KEY\\\" is not null\"\n        + \" order by \\\"SD_ID\\\" asc\";\n    loopJoinOrderedResult(sds, queryText, 0, new ApplyFunc<StorageDescriptor>() {\n      @Override\n      public void apply(StorageDescriptor t, Object[] fields) {\n        t.putToParameters((String)fields[1], (String)fields[2]);\n      }});\n    // Perform conversion of null map values\n    for (StorageDescriptor t : sds.values()) {\n      t.setParameters(MetaStoreUtils.trimMapNulls(t.getParameters(), convertMapNullsToEmptyStrings));\n    }\n\n    queryText = \"select \\\"SD_ID\\\", \\\"COLUMN_NAME\\\", \\\"SORT_COLS\\\".\\\"ORDER\\\" from \\\"SORT_COLS\\\"\"\n        + \" where \\\"SD_ID\\\" in (\" + sdIds + \") and \\\"INTEGER_IDX\\\" >= 0\"\n        + \" order by \\\"SD_ID\\\" asc, \\\"INTEGER_IDX\\\" asc\";\n    loopJoinOrderedResult(sds, queryText, 0, new ApplyFunc<StorageDescriptor>() {\n      @Override\n      public void apply(StorageDescriptor t, Object[] fields) {\n        if (fields[2] == null) return;\n        t.addToSortCols(new Order((String)fields[1], extractSqlInt(fields[2])));\n      }});\n\n    queryText = \"select \\\"SD_ID\\\", \\\"BUCKET_COL_NAME\\\" from \\\"BUCKETING_COLS\\\"\"\n        + \" where \\\"SD_ID\\\" in (\" + sdIds + \") and \\\"INTEGER_IDX\\\" >= 0\"\n        + \" order by \\\"SD_ID\\\" asc, \\\"INTEGER_IDX\\\" asc\";\n    loopJoinOrderedResult(sds, queryText, 0, new ApplyFunc<StorageDescriptor>() {\n      @Override\n      public void apply(StorageDescriptor t, Object[] fields) {\n        t.addToBucketCols((String)fields[1]);\n      }});\n\n    // Skewed columns stuff.\n    queryText = \"select \\\"SD_ID\\\", \\\"SKEWED_COL_NAME\\\" from \\\"SKEWED_COL_NAMES\\\"\"\n        + \" where \\\"SD_ID\\\" in (\" + sdIds + \") and \\\"INTEGER_IDX\\\" >= 0\"\n        + \" order by \\\"SD_ID\\\" asc, \\\"INTEGER_IDX\\\" asc\";\n    boolean hasSkewedColumns =\n      loopJoinOrderedResult(sds, queryText, 0, new ApplyFunc<StorageDescriptor>() {\n        @Override\n        public void apply(StorageDescriptor t, Object[] fields) {\n          if (!t.isSetSkewedInfo()) t.setSkewedInfo(new SkewedInfo());\n          t.getSkewedInfo().addToSkewedColNames((String)fields[1]);\n        }}) > 0;\n\n    // Assume we don't need to fetch the rest of the skewed column data if we have no columns.\n    if (hasSkewedColumns) {\n      // We are skipping the SKEWED_STRING_LIST table here, as it seems to be totally useless.\n      queryText =\n            \"select \\\"SKEWED_VALUES\\\".\\\"SD_ID_OID\\\",\"\n          + \"  \\\"SKEWED_STRING_LIST_VALUES\\\".\\\"STRING_LIST_ID\\\",\"\n          + \"  \\\"SKEWED_STRING_LIST_VALUES\\\".\\\"STRING_LIST_VALUE\\\" \"\n          + \"from \\\"SKEWED_VALUES\\\" \"\n          + \"  left outer join \\\"SKEWED_STRING_LIST_VALUES\\\" on \\\"SKEWED_VALUES\\\".\"\n          + \"\\\"STRING_LIST_ID_EID\\\" = \\\"SKEWED_STRING_LIST_VALUES\\\".\\\"STRING_LIST_ID\\\" \"\n          + \"where \\\"SKEWED_VALUES\\\".\\\"SD_ID_OID\\\" in (\" + sdIds + \") \"\n          + \"  and \\\"SKEWED_VALUES\\\".\\\"STRING_LIST_ID_EID\\\" is not null \"\n          + \"  and \\\"SKEWED_VALUES\\\".\\\"INTEGER_IDX\\\" >= 0 \"\n          + \"order by \\\"SKEWED_VALUES\\\".\\\"SD_ID_OID\\\" asc, \\\"SKEWED_VALUES\\\".\\\"INTEGER_IDX\\\" asc,\"\n          + \"  \\\"SKEWED_STRING_LIST_VALUES\\\".\\\"INTEGER_IDX\\\" asc\";\n      loopJoinOrderedResult(sds, queryText, 0, new ApplyFunc<StorageDescriptor>() {\n        private Long currentListId;\n        private List<String> currentList;\n        @Override\n        public void apply(StorageDescriptor t, Object[] fields) throws MetaException {\n          if (!t.isSetSkewedInfo()) t.setSkewedInfo(new SkewedInfo());\n          // Note that this is not a typical list accumulator - there's no call to finalize\n          // the last list. Instead we add list to SD first, as well as locally to add elements.\n          if (fields[1] == null) {\n            currentList = null; // left outer join produced a list with no values\n            currentListId = null;\n            t.getSkewedInfo().addToSkewedColValues(new ArrayList<String>());\n          } else {\n            long fieldsListId = extractSqlLong(fields[1]);\n            if (currentListId == null || fieldsListId != currentListId) {\n              currentList = new ArrayList<String>();\n              currentListId = fieldsListId;\n              t.getSkewedInfo().addToSkewedColValues(currentList);\n            }\n            currentList.add((String)fields[2]);\n          }\n        }});\n\n      // We are skipping the SKEWED_STRING_LIST table here, as it seems to be totally useless.\n      queryText =\n            \"select \\\"SKEWED_COL_VALUE_LOC_MAP\\\".\\\"SD_ID\\\",\"\n          + \" \\\"SKEWED_STRING_LIST_VALUES\\\".STRING_LIST_ID,\"\n          + \" \\\"SKEWED_COL_VALUE_LOC_MAP\\\".\\\"LOCATION\\\",\"\n          + \" \\\"SKEWED_STRING_LIST_VALUES\\\".\\\"STRING_LIST_VALUE\\\" \"\n          + \"from \\\"SKEWED_COL_VALUE_LOC_MAP\\\"\"\n          + \"  left outer join \\\"SKEWED_STRING_LIST_VALUES\\\" on \\\"SKEWED_COL_VALUE_LOC_MAP\\\".\"\n          + \"\\\"STRING_LIST_ID_KID\\\" = \\\"SKEWED_STRING_LIST_VALUES\\\".\\\"STRING_LIST_ID\\\" \"\n          + \"where \\\"SKEWED_COL_VALUE_LOC_MAP\\\".\\\"SD_ID\\\" in (\" + sdIds + \")\"\n          + \"  and \\\"SKEWED_COL_VALUE_LOC_MAP\\\".\\\"STRING_LIST_ID_KID\\\" is not null \"\n          + \"order by \\\"SKEWED_COL_VALUE_LOC_MAP\\\".\\\"SD_ID\\\" asc,\"\n          + \"  \\\"SKEWED_STRING_LIST_VALUES\\\".\\\"STRING_LIST_ID\\\" asc,\"\n          + \"  \\\"SKEWED_STRING_LIST_VALUES\\\".\\\"INTEGER_IDX\\\" asc\";\n\n      loopJoinOrderedResult(sds, queryText, 0, new ApplyFunc<StorageDescriptor>() {\n        private Long currentListId;\n        private List<String> currentList;\n        @Override\n        public void apply(StorageDescriptor t, Object[] fields) throws MetaException {\n          if (!t.isSetSkewedInfo()) {\n            SkewedInfo skewedInfo = new SkewedInfo();\n            skewedInfo.setSkewedColValueLocationMaps(new HashMap<List<String>, String>());\n            t.setSkewedInfo(skewedInfo);\n          }\n          Map<List<String>, String> skewMap = t.getSkewedInfo().getSkewedColValueLocationMaps();\n          // Note that this is not a typical list accumulator - there's no call to finalize\n          // the last list. Instead we add list to SD first, as well as locally to add elements.\n          if (fields[1] == null) {\n            currentList = new ArrayList<String>(); // left outer join produced a list with no values\n            currentListId = null;\n          } else {\n            long fieldsListId = extractSqlLong(fields[1]);\n            if (currentListId == null || fieldsListId != currentListId) {\n              currentList = new ArrayList<String>();\n              currentListId = fieldsListId;\n            } else {\n              skewMap.remove(currentList); // value based compare.. remove first\n            }\n            currentList.add((String)fields[3]);\n          }\n          skewMap.put(currentList, (String)fields[2]);\n        }});\n    } // if (hasSkewedColumns)\n\n    // Get FieldSchema stuff if any.\n    if (!colss.isEmpty()) {\n      // We are skipping the CDS table here, as it seems to be totally useless.\n      queryText = \"select \\\"CD_ID\\\", \\\"COMMENT\\\", \\\"COLUMN_NAME\\\", \\\"TYPE_NAME\\\"\"\n          + \" from \\\"COLUMNS_V2\\\" where \\\"CD_ID\\\" in (\" + colIds + \") and \\\"INTEGER_IDX\\\" >= 0\"\n          + \" order by \\\"CD_ID\\\" asc, \\\"INTEGER_IDX\\\" asc\";\n      loopJoinOrderedResult(colss, queryText, 0, new ApplyFunc<List<FieldSchema>>() {\n        @Override\n        public void apply(List<FieldSchema> t, Object[] fields) {\n          t.add(new FieldSchema((String)fields[2], (String)fields[3], (String)fields[1]));\n        }});\n    }\n\n    // Finally, get all the stuff for serdes - just the params.\n    queryText = \"select \\\"SERDE_ID\\\", \\\"PARAM_KEY\\\", \\\"PARAM_VALUE\\\" from \\\"SERDE_PARAMS\\\"\"\n        + \" where \\\"SERDE_ID\\\" in (\" + serdeIds + \") and \\\"PARAM_KEY\\\" is not null\"\n        + \" order by \\\"SERDE_ID\\\" asc\";\n    loopJoinOrderedResult(serdes, queryText, 0, new ApplyFunc<SerDeInfo>() {\n      @Override\n      public void apply(SerDeInfo t, Object[] fields) {\n        t.putToParameters((String)fields[1], (String)fields[2]);\n      }});\n    // Perform conversion of null map values\n    for (SerDeInfo t : serdes.values()) {\n      t.setParameters(MetaStoreUtils.trimMapNulls(t.getParameters(), convertMapNullsToEmptyStrings));\n    }\n\n    return orderedResult;\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreDirectSql.apply": {
                "code_before_change": "      public void apply(Partition t, Object[] fields) {\n        t.putToParameters((String)fields[1], (String)fields[2]);\n      }});\n    // Perform conversion of null map values\n    for (Partition t : partitions.values()) {",
                "code_after_change": "      public void apply(Partition t, Object[] fields) {\n        t.putToParameters((String)fields[1], (String)fields[2]);\n      }});\n    // Perform conversion of null map values\n    for (Partition t : partitions.values()) {"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Alternative Fix",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the issue occurring in the 'MetaStoreDirectSql.getPartitions' method, which is in the same stack trace as the ground truth method 'MetaStoreDirectSql.getPartitionsFromPartitionIds'. This places it in the 'Shared Stack Trace Context' sub-category for both root cause and problem location identification. The fix suggestion in the bug report proposes making the Direct SQL code path more consistent with the ORM code path, which is an alternative approach to the actual fix that involved handling null values differently. There is no incorrect information in the bug report, as it accurately describes the problem and provides a reasonable alternative solution."
        }
    },
    {
        "filename": "HIVE-4723.json",
        "code_diff": {
            "ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.getPartitionsByFilter": {
                "code_before_change": "  public List<Partition> getPartitionsByFilter(Table tbl, String filter)\n      throws HiveException, MetaException, NoSuchObjectException, TException {\n\n    if (!tbl.isPartitioned()) {\n      throw new HiveException(\"Partition spec should only be supplied for a \" +\n          \"partitioned table\");\n    }\n\n    List<org.apache.hadoop.hive.metastore.api.Partition> tParts = getMSC().listPartitionsByFilter(\n        tbl.getDbName(), tbl.getTableName(), filter, (short)-1);\n    List<Partition> results = new ArrayList<Partition>(tParts.size());\n\n    for (org.apache.hadoop.hive.metastore.api.Partition tPart: tParts) {\n      Partition part = new Partition(tbl, tPart);\n      results.add(part);\n    }\n    return results;\n  }",
                "code_after_change": "  public List<Partition> getPartitionsByFilter(Table tbl, String filter)\n      throws HiveException, MetaException, NoSuchObjectException, TException {\n\n    if (!tbl.isPartitioned()) {\n      throw new HiveException(ErrorMsg.TABLE_NOT_PARTITIONED, tbl.getTableName());\n    }\n\n    List<org.apache.hadoop.hive.metastore.api.Partition> tParts = getMSC().listPartitionsByFilter(\n        tbl.getDbName(), tbl.getTableName(), filter, (short)-1);\n    return convertFromMetastore(tbl, tParts, null);\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.getPartitionsByNames": {
                "code_before_change": "  public List<Partition> getPartitionsByNames(Table tbl,\n      Map<String, String> partialPartSpec)\n      throws HiveException {\n\n    if (!tbl.isPartitioned()) {\n      throw new HiveException(\"Partition spec should only be supplied for a \" +\n                \"partitioned table\");\n    }\n\n    List<String> names = getPartitionNames(tbl.getDbName(), tbl.getTableName(),\n        partialPartSpec, (short)-1);\n\n    List<Partition> partitions = getPartitionsByNames(tbl, names);\n    return partitions;\n  }",
                "code_after_change": "  public List<Partition> getPartitionsByNames(Table tbl,\n      Map<String, String> partialPartSpec)\n      throws HiveException {\n\n    if (!tbl.isPartitioned()) {\n      throw new HiveException(ErrorMsg.TABLE_NOT_PARTITIONED, tbl.getTableName());\n    }\n\n    List<String> names = getPartitionNames(tbl.getDbName(), tbl.getTableName(),\n        partialPartSpec, (short)-1);\n\n    List<Partition> partitions = getPartitionsByNames(tbl, names);\n    return partitions;\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.getPartitions": {
                "code_before_change": "  public List<Partition> getPartitions(Table tbl) throws HiveException {\n    if (tbl.isPartitioned()) {\n      List<org.apache.hadoop.hive.metastore.api.Partition> tParts;\n      try {\n        tParts = getMSC().listPartitionsWithAuthInfo(tbl.getDbName(), tbl.getTableName(),\n            (short) -1, getUserName(), getGroupNames());\n      } catch (Exception e) {\n        LOG.error(StringUtils.stringifyException(e));\n        throw new HiveException(e);\n      }\n      List<Partition> parts = new ArrayList<Partition>(tParts.size());\n      for (org.apache.hadoop.hive.metastore.api.Partition tpart : tParts) {\n        parts.add(new Partition(tbl, tpart));\n      }\n      return parts;\n    } else {\n      Partition part = new Partition(tbl);\n      ArrayList<Partition> parts = new ArrayList<Partition>(1);\n      parts.add(part);\n      return parts;\n    }\n  }",
                "code_after_change": "  public List<Partition> getPartitions(Table tbl) throws HiveException {\n    if (tbl.isPartitioned()) {\n      List<org.apache.hadoop.hive.metastore.api.Partition> tParts;\n      try {\n        tParts = getMSC().listPartitionsWithAuthInfo(tbl.getDbName(), tbl.getTableName(),\n            (short) -1, getUserName(), getGroupNames());\n      } catch (Exception e) {\n        LOG.error(StringUtils.stringifyException(e));\n        throw new HiveException(e);\n      }\n      List<Partition> parts = new ArrayList<Partition>(tParts.size());\n      for (org.apache.hadoop.hive.metastore.api.Partition tpart : tParts) {\n        parts.add(new Partition(tbl, tpart));\n      }\n      return parts;\n    } else {\n      Partition part = new Partition(tbl);\n      ArrayList<Partition> parts = new ArrayList<Partition>(1);\n      parts.add(part);\n      return parts;\n    }\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.addTablePartsOutputs": {
                "code_before_change": "  private void addTablePartsOutputs(String tblName, List<Map<String, String>> partSpecs)\n      throws SemanticException {\n    addTablePartsOutputs(tblName, partSpecs, false, false, null);\n  }",
                "code_after_change": "  private void addTablePartsOutputs(String tblName, List<Map<String, String>> partSpecs)\n      throws SemanticException {\n    addTablePartsOutputs(tblName, partSpecs, false, false, null);\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report precisely identifies the root cause by mentioning the method 'DDLSemanticAnalyzer.addTablePartsOutputs', which is one of the ground truth methods. However, there is no fix suggestion provided in the bug report, as there is no 'Suggestions' or 'problem_location' field, nor is there any suggestion in the 'Description' field. The problem location is also precisely identified as it directly mentions the method 'DDLSemanticAnalyzer.addTablePartsOutputs', which is in the ground truth list. There is no wrong information in the bug report as all the details provided are relevant to the context of the bug."
        }
    },
    {
        "filename": "HIVE-15686.json",
        "code_diff": {
            "metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.checkTrashPurgeCombination": {
                "code_before_change": "    private void checkTrashPurgeCombination(Path pathToData, String objectName, boolean ifPurge,\n        boolean deleteData) throws MetaException {\n      // There is no need to check TrashPurgeCombination in following cases since Purge/Trash\n      // is not applicable:\n      // a) deleteData is false -- drop an external table\n      // b) pathToData is null -- a view\n      // c) ifPurge is true -- force delete without Trash\n      if (!deleteData || pathToData == null || ifPurge) {\n        return;\n      }\n\n      boolean trashEnabled = false;\n      try {\n        trashEnabled = 0 < hiveConf.getFloat(\"fs.trash.interval\", -1);\n      } catch(NumberFormatException ex) {\n  // nothing to do\n      }\n\n      if (trashEnabled) {\n        try {\n          HadoopShims.HdfsEncryptionShim shim =\n            ShimLoader.getHadoopShims().createHdfsEncryptionShim(FileSystem.get(hiveConf), hiveConf);\n          if (shim.isPathEncrypted(pathToData)) {\n            throw new MetaException(\"Unable to drop \" + objectName + \" because it is in an encryption zone\" +\n              \" and trash is enabled.  Use PURGE option to skip trash.\");\n          }\n        } catch (IOException ex) {\n          MetaException e = new MetaException(ex.getMessage());\n          e.initCause(ex);\n          throw e;\n        }\n      }\n    }",
                "code_after_change": "    private void checkTrashPurgeCombination(Path pathToData, String objectName, boolean ifPurge,\n        boolean deleteData) throws MetaException {\n      // There is no need to check TrashPurgeCombination in following cases since Purge/Trash\n      // is not applicable:\n      // a) deleteData is false -- drop an external table\n      // b) pathToData is null -- a view\n      // c) ifPurge is true -- force delete without Trash\n      if (!deleteData || pathToData == null || ifPurge) {\n        return;\n      }\n\n      boolean trashEnabled = false;\n      try {\n        trashEnabled = 0 < hiveConf.getFloat(\"fs.trash.interval\", -1);\n      } catch(NumberFormatException ex) {\n        // nothing to do\n      }\n\n      if (trashEnabled) {\n        try {\n          HadoopShims.HdfsEncryptionShim shim =\n            ShimLoader.getHadoopShims().createHdfsEncryptionShim(pathToData.getFileSystem(hiveConf), hiveConf);\n          if (shim.isPathEncrypted(pathToData)) {\n            throw new MetaException(\"Unable to drop \" + objectName + \" because it is in an encryption zone\" +\n              \" and trash is enabled.  Use PURGE option to skip trash.\");\n          }\n        } catch (IOException ex) {\n          MetaException e = new MetaException(ex.getMessage());\n          e.initCause(ex);\n          throw e;\n        }\n      }\n    }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the issue with encryption-zone checks for partitions with remote HDFS paths, which is related to the stack trace context but does not precisely identify the root cause in the ground truth method 'checkTrashPurgeCombination'. The fix suggestion is missing as the report only mentions having a 'really simple fix' without providing details. The problem location is partially identified as it mentions methods in the stack trace context but not the exact ground truth method. There is no wrong information as the report accurately describes the error context."
        }
    },
    {
        "filename": "HIVE-4975.json",
        "code_diff": {
            "ql.src.java.org.apache.hadoop.hive.ql.io.orc.OrcStruct.getStructFieldData": {
                "code_before_change": "    public Object getStructFieldData(Object object, StructField field) {\n      return ((OrcStruct) object).fields[((Field) field).offset];\n    }",
                "code_after_change": "    public Object getStructFieldData(Object object, StructField field) {\n      if (object == null) {\n        return null;\n      }\n      int offset = ((Field) field).offset;\n      OrcStruct struct = (OrcStruct) object;\n      if (offset >= struct.fields.length) {\n        return null;\n      }\n\n      return struct.fields[offset];\n    }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report precisely identifies the root cause as an ArrayIndexOutOfBoundsException occurring in the method 'OrcStruct.getStructFieldData', which matches the ground truth method. However, there is no fix suggestion provided in the bug report, as there is no mention of how to resolve the issue. The problem location is also precisely identified as the stack trace and description point to the 'OrcStruct.getStructFieldData' method, which is the ground truth method. There is no wrong information in the bug report as all details are relevant to the context of the bug."
        }
    },
    {
        "filename": "HIVE-10538.json",
        "code_diff": {
            "ql.src.java.org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.makeValueWritable": {
                "code_before_change": "  private BytesWritable makeValueWritable(Object row) throws Exception {\n    int length = valueEval.length;\n\n    // in case of bucketed table, insert the bucket number as the last column in value\n    if (bucketEval != null) {\n      length -= 1;\n      cachedValues[length] = new Text(String.valueOf(bucketNumber));\n    }\n\n    // Evaluate the value\n    for (int i = 0; i < length; i++) {\n      cachedValues[i] = valueEval[i].evaluate(row);\n    }\n\n    // Serialize the value\n    return (BytesWritable) valueSerializer.serialize(cachedValues, valueObjectInspector);\n  }",
                "code_after_change": "  private BytesWritable makeValueWritable(Object row) throws Exception {\n    int length = valueEval.length;\n\n    // in case of bucketed table, insert the bucket number as the last column in value\n    if (bucketEval != null) {\n      length -= 1;\n      assert bucketNumber >= 0;\n      cachedValues[length] = new Text(String.valueOf(bucketNumber));\n    }\n\n    // Evaluate the value\n    for (int i = 0; i < length; i++) {\n      cachedValues[i] = valueEval[i].evaluate(row);\n    }\n\n    // Serialize the value\n    return (BytesWritable) valueSerializer.serialize(cachedValues, valueObjectInspector);\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies a Null Pointer Exception in the FileSinkOperator, which is part of the stack trace but not the ground truth method. Therefore, it is classified as 'Partial' under 'Shared Stack Trace Context' for both root cause and problem location identification. The report does not provide any fix suggestion, so it is marked as 'Missing' for fix suggestion. There is no incorrect information in the bug report, so 'Wrong Information' is marked as 'No'."
        }
    },
    {
        "filename": "HIVE-11902.json",
        "code_diff": {
            "metastore.src.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.performTimeOuts": {
                "code_before_change": "  public void performTimeOuts() {\n    Connection dbConn = null;\n    Statement stmt = null;\n    ResultSet rs = null;\n    try {\n      dbConn = getDbConn(Connection.TRANSACTION_SERIALIZABLE);\n      long now = getDbTime(dbConn);\n      timeOutLocks(dbConn);\n      while(true) {\n        stmt = dbConn.createStatement();\n        String s = \" txn_id from TXNS where txn_state = '\" + TXN_OPEN +\n          \"' and txn_last_heartbeat <  \" + (now - timeout);\n        s = addLimitClause(dbConn, 2500, s);\n        LOG.debug(\"Going to execute query <\" + s + \">\");\n        rs = stmt.executeQuery(s);\n        if(!rs.next()) {\n          return;//no more timedout txns\n        }\n        List<List<Long>> timedOutTxns = new ArrayList<>();\n        List<Long> currentBatch = new ArrayList<>(TIMED_OUT_TXN_ABORT_BATCH_SIZE);\n        timedOutTxns.add(currentBatch);\n        do {\n          currentBatch.add(rs.getLong(1));\n          if(currentBatch.size() == TIMED_OUT_TXN_ABORT_BATCH_SIZE) {\n            currentBatch = new ArrayList<>(TIMED_OUT_TXN_ABORT_BATCH_SIZE);\n            timedOutTxns.add(currentBatch);\n          }\n        } while(rs.next());\n        close(rs, stmt, null);\n        dbConn.commit();\n        for(List<Long> batchToAbort : timedOutTxns) {\n          abortTxns(dbConn, batchToAbort);\n          dbConn.commit();\n          //todo: add TXNS.COMMENT filed and set it to 'aborted by system due to timeout'\n          LOG.info(\"Aborted the following transactions due to timeout: \" + timedOutTxns.toString());\n        }\n        int numTxnsAborted = (timedOutTxns.size() - 1) * TIMED_OUT_TXN_ABORT_BATCH_SIZE +\n          timedOutTxns.get(timedOutTxns.size() - 1).size();\n        LOG.info(\"Aborted \" + numTxnsAborted + \" transactions due to timeout\");\n      }\n    } catch (SQLException ex) {\n      LOG.warn(\"Aborting timedout transactions failed due to \" + getMessage(ex), ex);\n    }\n    catch(MetaException e) {\n      LOG.warn(\"Aborting timedout transactions failed due to \" + e.getMessage(), e);\n    }\n    finally {\n      close(rs, stmt, dbConn);\n    }\n  }",
                "code_after_change": "  public void performTimeOuts() {\n    Connection dbConn = null;\n    Statement stmt = null;\n    ResultSet rs = null;\n    try {\n      dbConn = getDbConn(Connection.TRANSACTION_SERIALIZABLE);\n      long now = getDbTime(dbConn);\n      timeOutLocks(dbConn);\n      while(true) {\n        stmt = dbConn.createStatement();\n        String s = \" txn_id from TXNS where txn_state = '\" + TXN_OPEN +\n          \"' and txn_last_heartbeat <  \" + (now - timeout);\n        s = addLimitClause(dbConn, 2500, s);\n        LOG.debug(\"Going to execute query <\" + s + \">\");\n        rs = stmt.executeQuery(s);\n        if(!rs.next()) {\n          return;//no more timedout txns\n        }\n        List<List<Long>> timedOutTxns = new ArrayList<>();\n        List<Long> currentBatch = new ArrayList<>(TIMED_OUT_TXN_ABORT_BATCH_SIZE);\n        timedOutTxns.add(currentBatch);\n        do {\n          currentBatch.add(rs.getLong(1));\n          if(currentBatch.size() == TIMED_OUT_TXN_ABORT_BATCH_SIZE) {\n            currentBatch = new ArrayList<>(TIMED_OUT_TXN_ABORT_BATCH_SIZE);\n            timedOutTxns.add(currentBatch);\n          }\n        } while(rs.next());\n        close(rs, stmt, null);\n        dbConn.commit();\n        for(List<Long> batchToAbort : timedOutTxns) {\n          abortTxns(dbConn, batchToAbort);\n          dbConn.commit();\n          //todo: add TXNS.COMMENT filed and set it to 'aborted by system due to timeout'\n          LOG.info(\"Aborted the following transactions due to timeout: \" + batchToAbort.toString());\n        }\n        int numTxnsAborted = (timedOutTxns.size() - 1) * TIMED_OUT_TXN_ABORT_BATCH_SIZE +\n          timedOutTxns.get(timedOutTxns.size() - 1).size();\n        LOG.info(\"Aborted \" + numTxnsAborted + \" transactions due to timeout\");\n      }\n    } catch (SQLException ex) {\n      LOG.warn(\"Aborting timedout transactions failed due to \" + getMessage(ex), ex);\n    }\n    catch(MetaException e) {\n      LOG.warn(\"Aborting timedout transactions failed due to \" + e.getMessage(), e);\n    }\n    finally {\n      close(rs, stmt, dbConn);\n    }\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.checkQFileTestHack": {
                "code_before_change": "  private void checkQFileTestHack() {\n    boolean hackOn = HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVE_IN_TEST) ||\n      HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVE_IN_TEZ_TEST);\n    if (hackOn) {\n      LOG.info(\"Hacking in canned values for transaction manager\");\n      // Set up the transaction/locking db in the derby metastore\n      TxnDbUtil.setConfValues(conf);\n      try {\n        TxnDbUtil.prepDb();\n      } catch (Exception e) {\n        // We may have already created the tables and thus don't need to redo it.\n        if (!e.getMessage().contains(\"already exists\")) {\n          throw new RuntimeException(\"Unable to set up transaction database for\" +\n            \" testing: \" + e.getMessage());\n        }\n      }\n    }\n  }",
                "code_after_change": "  private void checkQFileTestHack() {\n    boolean hackOn = HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVE_IN_TEST) ||\n      HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVE_IN_TEZ_TEST);\n    if (hackOn) {\n      LOG.info(\"Hacking in canned values for transaction manager\");\n      // Set up the transaction/locking db in the derby metastore\n      TxnDbUtil.setConfValues(conf);\n      try {\n        TxnDbUtil.prepDb();\n      } catch (Exception e) {\n        // We may have already created the tables and thus don't need to redo it.\n        if (!e.getMessage().contains(\"already exists\")) {\n          throw new RuntimeException(\"Unable to set up transaction database for\" +\n            \" testing: \" + e.getMessage());\n        }\n      }\n    }\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Direct Caller/Callee"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Direct Caller/Callee"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the method `abortTxns` as the location of the issue, which is a direct callee of the ground truth method `performTimeOuts`. This makes the root cause identification and problem location identification partial with a sub-category of Direct Caller/Callee. The report does not provide any fix suggestion, hence it is marked as missing. There is no wrong information in the bug report as it correctly describes the SQL syntax error and the context in which it occurs."
        }
    },
    {
        "filename": "HIVE-18918.json",
        "code_diff": {
            "ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.CompactorMR.launchCompactionJob": {
                "code_before_change": "  private void launchCompactionJob(JobConf job, Path baseDir, CompactionType compactionType,\n                                   StringableList dirsToSearch,\n                                   List<AcidUtils.ParsedDelta> parsedDeltas,\n                                   int curDirNumber, int obsoleteDirNumber, HiveConf hiveConf,\n                                   TxnStore txnHandler, long id, String jobName) throws IOException {\n    job.setBoolean(IS_MAJOR, compactionType == CompactionType.MAJOR);\n    if(dirsToSearch == null) {\n      dirsToSearch = new StringableList();\n    }\n    StringableList deltaDirs = new StringableList();\n    long minTxn = Long.MAX_VALUE;\n    long maxTxn = Long.MIN_VALUE;\n    for (AcidUtils.ParsedDelta delta : parsedDeltas) {\n      LOG.debug(\"Adding delta \" + delta.getPath() + \" to directories to search\");\n      dirsToSearch.add(delta.getPath());\n      deltaDirs.add(delta.getPath());\n      minTxn = Math.min(minTxn, delta.getMinWriteId());\n      maxTxn = Math.max(maxTxn, delta.getMaxWriteId());\n    }\n\n    if (baseDir != null) job.set(BASE_DIR, baseDir.toString());\n    job.set(DELTA_DIRS, deltaDirs.toString());\n    job.set(DIRS_TO_SEARCH, dirsToSearch.toString());\n    job.setLong(MIN_TXN, minTxn);\n    job.setLong(MAX_TXN, maxTxn);\n\n    if (hiveConf.getBoolVar(HiveConf.ConfVars.HIVE_IN_TEST)) {\n      mrJob = job;\n    }\n\n    LOG.info(\"Submitting \" + compactionType + \" compaction job '\" +\n      job.getJobName() + \"' to \" + job.getQueueName() + \" queue.  \" +\n      \"(current delta dirs count=\" + curDirNumber +\n      \", obsolete delta dirs count=\" + obsoleteDirNumber + \". TxnIdRange[\" + minTxn + \",\" + maxTxn + \"]\");\n    JobClient jc = null;\n    try {\n      jc = new JobClient(job);\n      RunningJob rj = jc.submitJob(job);\n      LOG.info(\"Submitted compaction job '\" + job.getJobName() +\n          \"' with jobID=\" + rj.getID() + \" compaction ID=\" + id);\n      txnHandler.setHadoopJobId(rj.getID().toString(), id);\n      rj.waitForCompletion();\n      if (!rj.isSuccessful()) {\n        throw new IOException(compactionType == CompactionType.MAJOR ? \"Major\" : \"Minor\" +\n               \" compactor job failed for \" + jobName + \"! Hadoop JobId: \" + rj.getID());\n      }\n    } finally {\n      if (jc!=null) {\n        jc.close();\n      }\n    }\n  }",
                "code_after_change": "  private void launchCompactionJob(JobConf job, Path baseDir, CompactionType compactionType,\n                                   StringableList dirsToSearch,\n                                   List<AcidUtils.ParsedDelta> parsedDeltas,\n                                   int curDirNumber, int obsoleteDirNumber, HiveConf hiveConf,\n                                   TxnStore txnHandler, long id, String jobName) throws IOException {\n    job.setBoolean(IS_MAJOR, compactionType == CompactionType.MAJOR);\n    if(dirsToSearch == null) {\n      dirsToSearch = new StringableList();\n    }\n    StringableList deltaDirs = new StringableList();\n    long minTxn = Long.MAX_VALUE;\n    long maxTxn = Long.MIN_VALUE;\n    for (AcidUtils.ParsedDelta delta : parsedDeltas) {\n      LOG.debug(\"Adding delta \" + delta.getPath() + \" to directories to search\");\n      dirsToSearch.add(delta.getPath());\n      deltaDirs.add(delta.getPath());\n      minTxn = Math.min(minTxn, delta.getMinWriteId());\n      maxTxn = Math.max(maxTxn, delta.getMaxWriteId());\n    }\n\n    if (baseDir != null) job.set(BASE_DIR, baseDir.toString());\n    job.set(DELTA_DIRS, deltaDirs.toString());\n    job.set(DIRS_TO_SEARCH, dirsToSearch.toString());\n    job.setLong(MIN_TXN, minTxn);\n    job.setLong(MAX_TXN, maxTxn);\n\n    if (hiveConf.getBoolVar(HiveConf.ConfVars.HIVE_IN_TEST)) {\n      mrJob = job;\n    }\n\n    LOG.info(\"Submitting \" + compactionType + \" compaction job '\" +\n      job.getJobName() + \"' to \" + job.getQueueName() + \" queue.  \" +\n      \"(current delta dirs count=\" + curDirNumber +\n      \", obsolete delta dirs count=\" + obsoleteDirNumber + \". TxnIdRange[\" + minTxn + \",\" + maxTxn + \"]\");\n    JobClient jc = null;\n    try {\n      jc = new JobClient(job);\n      RunningJob rj = jc.submitJob(job);\n      LOG.info(\"Submitted compaction job '\" + job.getJobName() +\n          \"' with jobID=\" + rj.getID() + \" compaction ID=\" + id);\n      txnHandler.setHadoopJobId(rj.getID().toString(), id);\n      rj.waitForCompletion();\n      if (!rj.isSuccessful()) {\n        throw new IOException((compactionType == CompactionType.MAJOR ? \"Major\" : \"Minor\") +\n               \" compactor job failed for \" + jobName + \"! Hadoop JobId: \" + rj.getID());\n      }\n    } finally {\n      if (jc!=null) {\n        jc.close();\n      }\n    }\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report precisely identifies the root cause by mentioning the method 'CompactorMR.launchCompactionJob()', which is the ground truth method where the bug occurred. However, there is no fix suggestion provided in the bug report, as there is no 'Suggestions' or 'possible_fix' field, nor is there any suggestion in the 'Description' field. The problem location is also precisely identified as the report mentions 'CompactorMR.launchCompactionJob()' in the 'Title', which matches the ground truth method. There is no wrong information in the bug report as all the details are relevant to the context of the bug."
        }
    },
    {
        "filename": "HIVE-8107.json",
        "code_diff": {
            "ql.src.java.org.apache.hadoop.hive.ql.parse.UpdateDeleteSemanticAnalyzer.reparseAndSuperAnalyze": {
                "code_before_change": "  private void reparseAndSuperAnalyze(ASTNode tree) throws SemanticException {\n    List<? extends Node> children = tree.getChildren();\n    // The first child should be the table we are deleting from\n    ASTNode tabName = (ASTNode)children.get(0);\n    assert tabName.getToken().getType() == HiveParser.TOK_TABNAME :\n        \"Expected tablename as first child of \" + operation() + \" but found \" + tabName.getName();\n    String[] tableName = getQualifiedTableName(tabName);\n\n    // Rewrite the delete or update into an insert.  Crazy, but it works as deletes and update\n    // actually are inserts into the delta file in Hive.  A delete\n    // DELETE FROM _tablename_ [WHERE ...]\n    // will be rewritten as\n    // INSERT INTO TABLE _tablename_ [PARTITION (_partcols_)] SELECT ROW__ID[,\n    // _partcols_] from _tablename_ SORT BY ROW__ID\n    // An update\n    // UPDATE _tablename_ SET x = _expr_ [WHERE...]\n    // will be rewritten as\n    // INSERT INTO TABLE _tablename_ [PARTITION (_partcols_)] SELECT _all_,\n    // _partcols_from _tablename_ SORT BY ROW__ID\n    // where _all_ is all the non-partition columns.  The expressions from the set clause will be\n    // re-attached later.\n    // The where clause will also be re-attached later.\n    // The sort by clause is put in there so that records come out in the right order to enable\n    // merge on read.\n\n    StringBuilder rewrittenQueryStr = new StringBuilder();\n    Table mTable;\n    try {\n      mTable = db.getTable(tableName[0], tableName[1]);\n    } catch (HiveException e) {\n      throw new SemanticException(ErrorMsg.UPDATEDELETE_PARSE_ERROR.getMsg(), e);\n    }\n    List<FieldSchema> partCols = mTable.getPartCols();\n\n    rewrittenQueryStr.append(\"insert into table \");\n    rewrittenQueryStr.append(getDotName(tableName));\n\n    // If the table is partitioned we have to put the partition() clause in\n    if (partCols != null && partCols.size() > 0) {\n      rewrittenQueryStr.append(\" partition (\");\n      boolean first = true;\n      for (FieldSchema fschema : partCols) {\n        if (first) first = false;\n        else rewrittenQueryStr.append(\", \");\n        rewrittenQueryStr.append(fschema.getName());\n      }\n      rewrittenQueryStr.append(\")\");\n    }\n\n    rewrittenQueryStr.append(\" select ROW__ID\");\n    Map<Integer, ASTNode> setColExprs = null;\n    if (updating()) {\n      // An update needs to select all of the columns, as we rewrite the entire row.  Also,\n      // we need to figure out which columns we are going to replace.  We won't write the set\n      // expressions in the rewritten query.  We'll patch that up later.\n      // The set list from update should be the second child (index 1)\n      assert children.size() >= 2 : \"Expected update token to have at least two children\";\n      ASTNode setClause = (ASTNode)children.get(1);\n      assert setClause.getToken().getType() == HiveParser.TOK_SET_COLUMNS_CLAUSE :\n          \"Expected second child of update token to be set token\";\n\n      // Get the children of the set clause, each of which should be a column assignment\n      List<? extends Node> assignments = setClause.getChildren();\n      Map<String, ASTNode> setCols = new HashMap<String, ASTNode>(assignments.size());\n      setColExprs = new HashMap<Integer, ASTNode>(assignments.size());\n      for (Node a : assignments) {\n        ASTNode assignment = (ASTNode)a;\n        assert assignment.getToken().getType() == HiveParser.EQUAL :\n            \"Expected set assignments to use equals operator but found \" + assignment.getName();\n        ASTNode tableOrColTok = (ASTNode)assignment.getChildren().get(0);\n        assert tableOrColTok.getToken().getType() == HiveParser.TOK_TABLE_OR_COL :\n            \"Expected left side of assignment to be table or column\";\n        ASTNode colName = (ASTNode)tableOrColTok.getChildren().get(0);\n        assert colName.getToken().getType() == HiveParser.Identifier :\n            \"Expected column name\";\n\n        String columnName = colName.getText();\n\n        // Make sure this isn't one of the partitioning columns, that's not supported.\n        if (partCols != null) {\n          for (FieldSchema fschema : partCols) {\n            if (fschema.getName().equalsIgnoreCase(columnName)) {\n              throw new SemanticException(ErrorMsg.UPDATE_CANNOT_UPDATE_PART_VALUE.getMsg());\n            }\n          }\n        }\n\n        // This means that in UPDATE T SET x = _something_\n        // _something_ can be whatever is supported in SELECT _something_\n        setCols.put(columnName, (ASTNode)assignment.getChildren().get(1));\n      }\n\n      List<FieldSchema> nonPartCols = mTable.getCols();\n      for (int i = 0; i < nonPartCols.size(); i++) {\n        rewrittenQueryStr.append(',');\n        String name = nonPartCols.get(i).getName();\n        ASTNode setCol = setCols.get(name);\n        rewrittenQueryStr.append(name);\n        if (setCol != null) {\n          // This is one of the columns we're setting, record it's position so we can come back\n          // later and patch it up.\n          // Add one to the index because the select has the ROW__ID as the first column.\n          setColExprs.put(i + 1, setCol);\n        }\n      }\n    }\n\n    // If the table is partitioned, we need to select the partition columns as well.\n    if (partCols != null) {\n      for (FieldSchema fschema : partCols) {\n        rewrittenQueryStr.append(\", \");\n        rewrittenQueryStr.append(fschema.getName());\n      }\n    }\n    rewrittenQueryStr.append(\" from \");\n    rewrittenQueryStr.append(getDotName(tableName));\n\n    ASTNode where = null;\n    int whereIndex = deleting() ? 1 : 2;\n    if (children.size() > whereIndex) {\n      where = (ASTNode)children.get(whereIndex);\n      assert where.getToken().getType() == HiveParser.TOK_WHERE :\n          \"Expected where clause, but found \" + where.getName();\n    }\n\n    // Add a sort by clause so that the row ids come out in the correct order\n    rewrittenQueryStr.append(\" sort by ROW__ID desc \");\n\n    // Parse the rewritten query string\n    Context rewrittenCtx;\n    try {\n      // Set dynamic partitioning to nonstrict so that queries do not need any partition\n      // references.\n      HiveConf.setVar(conf, HiveConf.ConfVars.DYNAMICPARTITIONINGMODE, \"nonstrict\");\n      rewrittenCtx = new Context(conf);\n    } catch (IOException e) {\n      throw new SemanticException(ErrorMsg.UPDATEDELETE_IO_ERROR.getMsg());\n    }\n    rewrittenCtx.setCmd(rewrittenQueryStr.toString());\n    rewrittenCtx.setAcidOperation(ctx.getAcidOperation());\n\n    ParseDriver pd = new ParseDriver();\n    ASTNode rewrittenTree;\n    try {\n      LOG.info(\"Going to reparse \" + operation() + \" as <\" + rewrittenQueryStr.toString() + \">\");\n      rewrittenTree = pd.parse(rewrittenQueryStr.toString(), rewrittenCtx);\n      rewrittenTree = ParseUtils.findRootNonNullToken(rewrittenTree);\n\n    } catch (ParseException e) {\n      throw new SemanticException(ErrorMsg.UPDATEDELETE_PARSE_ERROR.getMsg(), e);\n    }\n\n    ASTNode rewrittenInsert = (ASTNode)rewrittenTree.getChildren().get(1);\n    assert rewrittenInsert.getToken().getType() == HiveParser.TOK_INSERT :\n        \"Expected TOK_INSERT as second child of TOK_QUERY but found \" + rewrittenInsert.getName();\n\n    if (where != null) {\n      // The structure of the AST for the rewritten insert statement is:\n      // TOK_QUERY -> TOK_FROM\n      //          \\-> TOK_INSERT -> TOK_INSERT_INTO\n      //                        \\-> TOK_SELECT\n      //                        \\-> TOK_SORTBY\n      // The following adds the TOK_WHERE and its subtree from the original query as a child of\n      // TOK_INSERT, which is where it would have landed if it had been there originally in the\n      // string.  We do it this way because it's easy then turning the original AST back into a\n      // string and reparsing it.  We have to move the SORT_BY over one,\n      // so grab it and then push it to the second slot, and put the where in the first slot\n      ASTNode sortBy = (ASTNode)rewrittenInsert.getChildren().get(2);\n      assert sortBy.getToken().getType() == HiveParser.TOK_SORTBY :\n          \"Expected TOK_SORTBY to be first child of TOK_SELECT, but found \" + sortBy.getName();\n      rewrittenInsert.addChild(sortBy);\n      rewrittenInsert.setChild(2, where);\n    }\n\n    // Patch up the projection list for updates, putting back the original set expressions.\n    if (updating() && setColExprs != null) {\n      // Walk through the projection list and replace the column names with the\n      // expressions from the original update.  Under the TOK_SELECT (see above) the structure\n      // looks like:\n      // TOK_SELECT -> TOK_SELEXPR -> expr\n      //           \\-> TOK_SELEXPR -> expr ...\n      ASTNode rewrittenSelect = (ASTNode)rewrittenInsert.getChildren().get(1);\n      assert rewrittenSelect.getToken().getType() == HiveParser.TOK_SELECT :\n          \"Expected TOK_SELECT as second child of TOK_INSERT but found \" +\n              rewrittenSelect.getName();\n      for (Map.Entry<Integer, ASTNode> entry : setColExprs.entrySet()) {\n        ASTNode selExpr = (ASTNode)rewrittenSelect.getChildren().get(entry.getKey());\n        assert selExpr.getToken().getType() == HiveParser.TOK_SELEXPR :\n            \"Expected child of TOK_SELECT to be TOK_SELEXPR but was \" + selExpr.getName();\n        // Now, change it's child\n        selExpr.setChild(0, entry.getValue());\n      }\n    }\n\n    try {\n      useSuper = true;\n      super.analyze(rewrittenTree, rewrittenCtx);\n    } finally {\n      useSuper = false;\n    }\n\n    // Walk through all our inputs and set them to note that this read is part of an update or a\n    // delete.\n    for (ReadEntity input : inputs) {\n      input.setUpdateOrDelete(true);\n    }\n\n    if (inputIsPartitioned(inputs)) {\n      // In order to avoid locking the entire write table we need to replace the single WriteEntity\n      // with a WriteEntity for each partition\n      outputs.clear();\n      for (ReadEntity input : inputs) {\n        if (input.getTyp() == Entity.Type.PARTITION) {\n          WriteEntity.WriteType writeType = deleting() ? WriteEntity.WriteType.DELETE :\n              WriteEntity.WriteType.UPDATE;\n          outputs.add(new WriteEntity(input.getPartition(), writeType));\n        }\n      }\n    } else {\n      // We still need to patch up the WriteEntities as they will have an insert type.  Change\n      // them to the appropriate type for our operation.\n      for (WriteEntity output : outputs) {\n        output.setWriteType(deleting() ? WriteEntity.WriteType.DELETE :\n            WriteEntity.WriteType.UPDATE);\n      }\n    }\n  }",
                "code_after_change": "  private void reparseAndSuperAnalyze(ASTNode tree) throws SemanticException {\n    List<? extends Node> children = tree.getChildren();\n    // The first child should be the table we are deleting from\n    ASTNode tabName = (ASTNode)children.get(0);\n    assert tabName.getToken().getType() == HiveParser.TOK_TABNAME :\n        \"Expected tablename as first child of \" + operation() + \" but found \" + tabName.getName();\n    String[] tableName = getQualifiedTableName(tabName);\n\n    // Rewrite the delete or update into an insert.  Crazy, but it works as deletes and update\n    // actually are inserts into the delta file in Hive.  A delete\n    // DELETE FROM _tablename_ [WHERE ...]\n    // will be rewritten as\n    // INSERT INTO TABLE _tablename_ [PARTITION (_partcols_)] SELECT ROW__ID[,\n    // _partcols_] from _tablename_ SORT BY ROW__ID\n    // An update\n    // UPDATE _tablename_ SET x = _expr_ [WHERE...]\n    // will be rewritten as\n    // INSERT INTO TABLE _tablename_ [PARTITION (_partcols_)] SELECT _all_,\n    // _partcols_from _tablename_ SORT BY ROW__ID\n    // where _all_ is all the non-partition columns.  The expressions from the set clause will be\n    // re-attached later.\n    // The where clause will also be re-attached later.\n    // The sort by clause is put in there so that records come out in the right order to enable\n    // merge on read.\n\n    StringBuilder rewrittenQueryStr = new StringBuilder();\n    Table mTable;\n    try {\n      mTable = db.getTable(tableName[0], tableName[1]);\n    } catch (HiveException e) {\n      LOG.error(\"Failed to find table \" + getDotName(tableName) + \" got exception \" +\n          e.getMessage());\n      throw new SemanticException(ErrorMsg.INVALID_TABLE, getDotName(tableName));\n    }\n    List<FieldSchema> partCols = mTable.getPartCols();\n\n    rewrittenQueryStr.append(\"insert into table \");\n    rewrittenQueryStr.append(getDotName(tableName));\n\n    // If the table is partitioned we have to put the partition() clause in\n    if (partCols != null && partCols.size() > 0) {\n      rewrittenQueryStr.append(\" partition (\");\n      boolean first = true;\n      for (FieldSchema fschema : partCols) {\n        if (first) first = false;\n        else rewrittenQueryStr.append(\", \");\n        rewrittenQueryStr.append(fschema.getName());\n      }\n      rewrittenQueryStr.append(\")\");\n    }\n\n    rewrittenQueryStr.append(\" select ROW__ID\");\n    Map<Integer, ASTNode> setColExprs = null;\n    Map<String, ASTNode> setCols = null;\n    Set<String> setRCols = new HashSet<String>();\n    if (updating()) {\n      // An update needs to select all of the columns, as we rewrite the entire row.  Also,\n      // we need to figure out which columns we are going to replace.  We won't write the set\n      // expressions in the rewritten query.  We'll patch that up later.\n      // The set list from update should be the second child (index 1)\n      assert children.size() >= 2 : \"Expected update token to have at least two children\";\n      ASTNode setClause = (ASTNode)children.get(1);\n      assert setClause.getToken().getType() == HiveParser.TOK_SET_COLUMNS_CLAUSE :\n          \"Expected second child of update token to be set token\";\n\n      // Get the children of the set clause, each of which should be a column assignment\n      List<? extends Node> assignments = setClause.getChildren();\n      setCols = new HashMap<String, ASTNode>(assignments.size());\n      setColExprs = new HashMap<Integer, ASTNode>(assignments.size());\n      for (Node a : assignments) {\n        ASTNode assignment = (ASTNode)a;\n        assert assignment.getToken().getType() == HiveParser.EQUAL :\n            \"Expected set assignments to use equals operator but found \" + assignment.getName();\n        ASTNode tableOrColTok = (ASTNode)assignment.getChildren().get(0);\n        assert tableOrColTok.getToken().getType() == HiveParser.TOK_TABLE_OR_COL :\n            \"Expected left side of assignment to be table or column\";\n        ASTNode colName = (ASTNode)tableOrColTok.getChildren().get(0);\n        assert colName.getToken().getType() == HiveParser.Identifier :\n            \"Expected column name\";\n\n        addSetRCols((ASTNode) assignment.getChildren().get(1), setRCols);\n\n        String columnName = colName.getText();\n\n        // Make sure this isn't one of the partitioning columns, that's not supported.\n        if (partCols != null) {\n          for (FieldSchema fschema : partCols) {\n            if (fschema.getName().equalsIgnoreCase(columnName)) {\n              throw new SemanticException(ErrorMsg.UPDATE_CANNOT_UPDATE_PART_VALUE.getMsg());\n            }\n          }\n        }\n\n        // This means that in UPDATE T SET x = _something_\n        // _something_ can be whatever is supported in SELECT _something_\n        setCols.put(columnName, (ASTNode)assignment.getChildren().get(1));\n      }\n\n      List<FieldSchema> nonPartCols = mTable.getCols();\n      for (int i = 0; i < nonPartCols.size(); i++) {\n        rewrittenQueryStr.append(',');\n        String name = nonPartCols.get(i).getName();\n        ASTNode setCol = setCols.get(name);\n        rewrittenQueryStr.append(name);\n        if (setCol != null) {\n          // This is one of the columns we're setting, record it's position so we can come back\n          // later and patch it up.\n          // Add one to the index because the select has the ROW__ID as the first column.\n          setColExprs.put(i + 1, setCol);\n        }\n      }\n    }\n\n    // If the table is partitioned, we need to select the partition columns as well.\n    if (partCols != null) {\n      for (FieldSchema fschema : partCols) {\n        rewrittenQueryStr.append(\", \");\n        rewrittenQueryStr.append(fschema.getName());\n      }\n    }\n    rewrittenQueryStr.append(\" from \");\n    rewrittenQueryStr.append(getDotName(tableName));\n\n    ASTNode where = null;\n    int whereIndex = deleting() ? 1 : 2;\n    if (children.size() > whereIndex) {\n      where = (ASTNode)children.get(whereIndex);\n      assert where.getToken().getType() == HiveParser.TOK_WHERE :\n          \"Expected where clause, but found \" + where.getName();\n    }\n\n    // Add a sort by clause so that the row ids come out in the correct order\n    rewrittenQueryStr.append(\" sort by ROW__ID desc \");\n\n    // Parse the rewritten query string\n    Context rewrittenCtx;\n    try {\n      // Set dynamic partitioning to nonstrict so that queries do not need any partition\n      // references.\n      HiveConf.setVar(conf, HiveConf.ConfVars.DYNAMICPARTITIONINGMODE, \"nonstrict\");\n      rewrittenCtx = new Context(conf);\n    } catch (IOException e) {\n      throw new SemanticException(ErrorMsg.UPDATEDELETE_IO_ERROR.getMsg());\n    }\n    rewrittenCtx.setCmd(rewrittenQueryStr.toString());\n    rewrittenCtx.setAcidOperation(ctx.getAcidOperation());\n\n    ParseDriver pd = new ParseDriver();\n    ASTNode rewrittenTree;\n    try {\n      LOG.info(\"Going to reparse \" + operation() + \" as <\" + rewrittenQueryStr.toString() + \">\");\n      rewrittenTree = pd.parse(rewrittenQueryStr.toString(), rewrittenCtx);\n      rewrittenTree = ParseUtils.findRootNonNullToken(rewrittenTree);\n\n    } catch (ParseException e) {\n      throw new SemanticException(ErrorMsg.UPDATEDELETE_PARSE_ERROR.getMsg(), e);\n    }\n\n    ASTNode rewrittenInsert = (ASTNode)rewrittenTree.getChildren().get(1);\n    assert rewrittenInsert.getToken().getType() == HiveParser.TOK_INSERT :\n        \"Expected TOK_INSERT as second child of TOK_QUERY but found \" + rewrittenInsert.getName();\n\n    if (where != null) {\n      // The structure of the AST for the rewritten insert statement is:\n      // TOK_QUERY -> TOK_FROM\n      //          \\-> TOK_INSERT -> TOK_INSERT_INTO\n      //                        \\-> TOK_SELECT\n      //                        \\-> TOK_SORTBY\n      // The following adds the TOK_WHERE and its subtree from the original query as a child of\n      // TOK_INSERT, which is where it would have landed if it had been there originally in the\n      // string.  We do it this way because it's easy then turning the original AST back into a\n      // string and reparsing it.  We have to move the SORT_BY over one,\n      // so grab it and then push it to the second slot, and put the where in the first slot\n      ASTNode sortBy = (ASTNode)rewrittenInsert.getChildren().get(2);\n      assert sortBy.getToken().getType() == HiveParser.TOK_SORTBY :\n          \"Expected TOK_SORTBY to be first child of TOK_SELECT, but found \" + sortBy.getName();\n      rewrittenInsert.addChild(sortBy);\n      rewrittenInsert.setChild(2, where);\n    }\n\n    // Patch up the projection list for updates, putting back the original set expressions.\n    if (updating() && setColExprs != null) {\n      // Walk through the projection list and replace the column names with the\n      // expressions from the original update.  Under the TOK_SELECT (see above) the structure\n      // looks like:\n      // TOK_SELECT -> TOK_SELEXPR -> expr\n      //           \\-> TOK_SELEXPR -> expr ...\n      ASTNode rewrittenSelect = (ASTNode)rewrittenInsert.getChildren().get(1);\n      assert rewrittenSelect.getToken().getType() == HiveParser.TOK_SELECT :\n          \"Expected TOK_SELECT as second child of TOK_INSERT but found \" +\n              rewrittenSelect.getName();\n      for (Map.Entry<Integer, ASTNode> entry : setColExprs.entrySet()) {\n        ASTNode selExpr = (ASTNode)rewrittenSelect.getChildren().get(entry.getKey());\n        assert selExpr.getToken().getType() == HiveParser.TOK_SELEXPR :\n            \"Expected child of TOK_SELECT to be TOK_SELEXPR but was \" + selExpr.getName();\n        // Now, change it's child\n        selExpr.setChild(0, entry.getValue());\n      }\n    }\n\n    try {\n      useSuper = true;\n      super.analyze(rewrittenTree, rewrittenCtx);\n    } finally {\n      useSuper = false;\n    }\n\n    // Walk through all our inputs and set them to note that this read is part of an update or a\n    // delete.\n    for (ReadEntity input : inputs) {\n      input.setUpdateOrDelete(true);\n    }\n\n    if (inputIsPartitioned(inputs)) {\n      // In order to avoid locking the entire write table we need to replace the single WriteEntity\n      // with a WriteEntity for each partition\n      outputs.clear();\n      for (ReadEntity input : inputs) {\n        if (input.getTyp() == Entity.Type.PARTITION) {\n          WriteEntity.WriteType writeType = deleting() ? WriteEntity.WriteType.DELETE :\n              WriteEntity.WriteType.UPDATE;\n          outputs.add(new WriteEntity(input.getPartition(), writeType));\n        }\n      }\n    } else {\n      // We still need to patch up the WriteEntities as they will have an insert type.  Change\n      // them to the appropriate type for our operation.\n      for (WriteEntity output : outputs) {\n        output.setWriteType(deleting() ? WriteEntity.WriteType.DELETE :\n            WriteEntity.WriteType.UPDATE);\n      }\n    }\n\n    // For updates, we need to set the column access info so that it contains information on\n    // the columns we are updating.\n    if (updating()) {\n      ColumnAccessInfo cai = new ColumnAccessInfo();\n      for (String colName : setCols.keySet()) {\n        cai.add(Table.getCompleteName(mTable.getDbName(), mTable.getTableName()), colName);\n      }\n      setUpdateColumnAccessInfo(cai);\n\n      // Add the setRCols to the input list\n      for (String colName : setRCols) {\n        columnAccessInfo.add(Table.getCompleteName(mTable.getDbName(), mTable.getTableName()),\n            colName);\n      }\n    }\n\n    // We need to weed ROW__ID out of the input column info, as it doesn't make any sense to\n    // require the user to have authorization on that column.\n    if (columnAccessInfo != null) {\n      columnAccessInfo.stripVirtualColumn(VirtualColumn.ROWID);\n    }\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report precisely identifies the root cause by mentioning the method 'reparseAndSuperAnalyze' in the stack trace, which is the ground truth method where the bug occurred. However, the report does not provide any fix suggestion, hence it is marked as 'Missing' for fix suggestion. The problem location is also precisely identified as it directly points to the method in the stack trace. There is no wrong information in the bug report as all the details are relevant to the context of the bug."
        }
    },
    {
        "filename": "HIVE-1326.json",
        "code_diff": {
            "ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.RowContainer.spillBlock": {
                "code_before_change": "  private void spillBlock(Row[] block, int length) throws HiveException {\n    try {\n      if (tmpFile == null) {\n\n        String suffix = \".tmp\";\n        if (this.keyObject != null) {\n          suffix = \".\" + this.keyObject.toString() + suffix;\n        }\n\n        while (true) {\n          String parentId = \"hive-rowcontainer\" + Utilities.randGen.nextInt();\n          parentFile = new File(\"/tmp/\" + parentId);\n          boolean success = parentFile.mkdir();\n          if (success) {\n            break;\n          }\n          LOG.debug(\"retry creating tmp row-container directory...\");\n        }\n\n        tmpFile = File.createTempFile(\"RowContainer\", suffix, parentFile);\n        LOG.info(\"RowContainer created temp file \" + tmpFile.getAbsolutePath());\n        // Delete the temp file if the JVM terminate normally through Hadoop job\n        // kill command.\n        // Caveat: it won't be deleted if JVM is killed by 'kill -9'.\n        parentFile.deleteOnExit();\n        tmpFile.deleteOnExit();\n\n        // rFile = new RandomAccessFile(tmpFile, \"rw\");\n        HiveOutputFormat<?, ?> hiveOutputFormat = tblDesc\n            .getOutputFileFormatClass().newInstance();\n        tempOutPath = new Path(tmpFile.toString());\n        rw = HiveFileFormatUtils.getRecordWriter(this.jobCloneUsingLocalFs,\n            hiveOutputFormat, serde.getSerializedClass(), false, tblDesc\n            .getProperties(), tempOutPath);\n      } else if (rw == null) {\n        throw new HiveException(\n            \"RowContainer has already been closed for writing.\");\n      }\n\n      row.clear();\n      row.add(null);\n      row.add(null);\n\n      if (this.keyObject != null) {\n        row.set(1, this.keyObject);\n        for (int i = 0; i < length; ++i) {\n          Row currentValRow = block[i];\n          row.set(0, currentValRow);\n          Writable outVal = serde.serialize(row, standardOI);\n          rw.write(outVal);\n        }\n      } else {\n        for (int i = 0; i < length; ++i) {\n          Row currentValRow = block[i];\n          Writable outVal = serde.serialize(currentValRow, standardOI);\n          rw.write(outVal);\n        }\n      }\n\n      if (block == this.currentWriteBlock) {\n        this.addCursor = 0;\n      }\n\n      this.numFlushedBlocks++;\n    } catch (Exception e) {\n      clear();\n      LOG.error(e.toString(), e);\n      throw new HiveException(e);\n    }\n  }",
                "code_after_change": "  private void spillBlock(Row[] block, int length) throws HiveException {\n    try {\n      if (tmpFile == null) {\n\n        String suffix = \".tmp\";\n        if (this.keyObject != null) {\n          suffix = \".\" + this.keyObject.toString() + suffix;\n        }\n\n        while (true) {\n          parentFile = File.createTempFile(\"hive-rowcontainer\", \"\");\n          boolean success = parentFile.delete() && parentFile.mkdir();\n          if (success) {\n            break;\n          }\n          LOG.debug(\"retry creating tmp row-container directory...\");\n        }\n\n        tmpFile = File.createTempFile(\"RowContainer\", suffix, parentFile);\n        LOG.info(\"RowContainer created temp file \" + tmpFile.getAbsolutePath());\n        // Delete the temp file if the JVM terminate normally through Hadoop job\n        // kill command.\n        // Caveat: it won't be deleted if JVM is killed by 'kill -9'.\n        parentFile.deleteOnExit();\n        tmpFile.deleteOnExit();\n\n        // rFile = new RandomAccessFile(tmpFile, \"rw\");\n        HiveOutputFormat<?, ?> hiveOutputFormat = tblDesc\n            .getOutputFileFormatClass().newInstance();\n        tempOutPath = new Path(tmpFile.toString());\n        rw = HiveFileFormatUtils.getRecordWriter(this.jobCloneUsingLocalFs,\n            hiveOutputFormat, serde.getSerializedClass(), false, tblDesc\n            .getProperties(), tempOutPath);\n      } else if (rw == null) {\n        throw new HiveException(\n            \"RowContainer has already been closed for writing.\");\n      }\n\n      row.clear();\n      row.add(null);\n      row.add(null);\n\n      if (this.keyObject != null) {\n        row.set(1, this.keyObject);\n        for (int i = 0; i < length; ++i) {\n          Row currentValRow = block[i];\n          row.set(0, currentValRow);\n          Writable outVal = serde.serialize(row, standardOI);\n          rw.write(outVal);\n        }\n      } else {\n        for (int i = 0; i < length; ++i) {\n          Row currentValRow = block[i];\n          Writable outVal = serde.serialize(currentValRow, standardOI);\n          rw.write(outVal);\n        }\n      }\n\n      if (block == this.currentWriteBlock) {\n        this.addCursor = 0;\n      }\n\n      this.numFlushedBlocks++;\n    } catch (Exception e) {\n      clear();\n      LOG.error(e.toString(), e);\n      throw new HiveException(e);\n    }\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Correct",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report precisely identifies the root cause as the RowContainer class using a hard-coded '/tmp/' path for temporary files, which matches the ground truth method 'ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.RowContainer.spillBlock'. The fix suggestion in the report aligns with the developer's fix, which involves changing the method to use a temporary file creation method that does not hard-code the path. The problem location is also precisely identified as the RowContainer class, specifically the method where the fix was applied. There is no wrong information in the bug report as all details are consistent with the context of the bug."
        }
    },
    {
        "filename": "HIVE-11369.json",
        "code_diff": {
            "ql.src.java.org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask.executeInChildVM": {
                "code_before_change": "  public int executeInChildVM(DriverContext driverContext) {\n    // execute in child jvm\n    try {\n      // generate the cmd line to run in the child jvm\n      Context ctx = driverContext.getCtx();\n      String hiveJar = conf.getJar();\n\n      String hadoopExec = conf.getVar(HiveConf.ConfVars.HADOOPBIN);\n      conf.setVar(ConfVars.HIVEADDEDJARS, Utilities.getResourceFiles(conf, SessionState.ResourceType.JAR));\n      // write out the plan to a local file\n      Path planPath = new Path(ctx.getLocalTmpPath(), \"plan.xml\");\n      MapredLocalWork plan = getWork();\n      LOG.info(\"Generating plan file \" + planPath.toString());\n\n      OutputStream out = null;\n      try {\n        out = FileSystem.getLocal(conf).create(planPath);\n        Utilities.serializePlan(plan, out, conf);\n        out.close();\n        out = null;\n      } finally {\n        IOUtils.closeQuietly(out);\n      }\n\n\n      String isSilent = \"true\".equalsIgnoreCase(System.getProperty(\"test.silent\")) ? \"-nolog\" : \"\";\n\n      String jarCmd;\n\n      jarCmd = hiveJar + \" \" + ExecDriver.class.getName();\n      String hiveConfArgs = ExecDriver.generateCmdLine(conf, ctx);\n      String cmdLine = hadoopExec + \" jar \" + jarCmd + \" -localtask -plan \" + planPath.toString()\n          + \" \" + isSilent + \" \" + hiveConfArgs;\n\n      String workDir = (new File(\".\")).getCanonicalPath();\n      String files = Utilities.getResourceFiles(conf, SessionState.ResourceType.FILE);\n\n      if (!files.isEmpty()) {\n        cmdLine = cmdLine + \" -files \" + files;\n\n        workDir = ctx.getLocalTmpPath().toUri().getPath();\n\n        if (!(new File(workDir)).mkdir()) {\n          throw new IOException(\"Cannot create tmp working dir: \" + workDir);\n        }\n\n        for (String f : StringUtils.split(files, ',')) {\n          Path p = new Path(f);\n          String target = p.toUri().getPath();\n          String link = workDir + Path.SEPARATOR + p.getName();\n          if (FileUtil.symLink(target, link) != 0) {\n            throw new IOException(\"Cannot link to added file: \" + target + \" from: \" + link);\n          }\n        }\n      }\n\n      // Inherit Java system variables\n      String hadoopOpts;\n      StringBuilder sb = new StringBuilder();\n      Properties p = System.getProperties();\n      for (String element : HIVE_SYS_PROP) {\n        if (p.containsKey(element)) {\n          sb.append(\" -D\" + element + \"=\" + p.getProperty(element));\n        }\n      }\n      hadoopOpts = sb.toString();\n      // Inherit the environment variables\n      String[] env;\n      Map<String, String> variables = new HashMap(System.getenv());\n      // The user can specify the hadoop memory\n\n      // if (\"local\".equals(conf.getVar(HiveConf.ConfVars.HADOOPJT))) {\n      // if we are running in local mode - then the amount of memory used\n      // by the child jvm can no longer default to the memory used by the\n      // parent jvm\n      // int hadoopMem = conf.getIntVar(HiveConf.ConfVars.HIVEHADOOPMAXMEM);\n      int hadoopMem = conf.getIntVar(HiveConf.ConfVars.HIVEHADOOPMAXMEM);\n      if (hadoopMem == 0) {\n        // remove env var that would default child jvm to use parent's memory\n        // as default. child jvm would use default memory for a hadoop client\n        variables.remove(HADOOP_MEM_KEY);\n      } else {\n        // user specified the memory for local mode hadoop run\n        console.printInfo(\" set heap size\\t\" + hadoopMem + \"MB\");\n        variables.put(HADOOP_MEM_KEY, String.valueOf(hadoopMem));\n      }\n      // } else {\n      // nothing to do - we are not running in local mode - only submitting\n      // the job via a child process. in this case it's appropriate that the\n      // child jvm use the same memory as the parent jvm\n\n      // }\n\n      //Set HADOOP_USER_NAME env variable for child process, so that\n      // it also runs with hadoop permissions for the user the job is running as\n      // This will be used by hadoop only in unsecure(/non kerberos) mode\n      String endUserName = Utils.getUGI().getShortUserName();\n      LOG.debug(\"setting HADOOP_USER_NAME\\t\" + endUserName);\n      variables.put(\"HADOOP_USER_NAME\", endUserName);\n\n      if (variables.containsKey(HADOOP_OPTS_KEY)) {\n        variables.put(HADOOP_OPTS_KEY, variables.get(HADOOP_OPTS_KEY) + hadoopOpts);\n      } else {\n        variables.put(HADOOP_OPTS_KEY, hadoopOpts);\n      }\n\n      //For Windows OS, we need to pass HIVE_HADOOP_CLASSPATH Java parameter while starting\n      //Hiveserver2 using \"-hiveconf hive.hadoop.classpath=%HIVE_LIB%\". This is to combine path(s).\n      if (HiveConf.getVar(conf, HiveConf.ConfVars.HIVE_HADOOP_CLASSPATH)!= null)\n      {\n        if (variables.containsKey(\"HADOOP_CLASSPATH\"))\n        {\n          variables.put(\"HADOOP_CLASSPATH\", variables.get(\"HADOOP_CLASSPATH\") + \";\" + HiveConf.getVar(conf, HiveConf.ConfVars.HIVE_HADOOP_CLASSPATH));\n        } else {\n          variables.put(\"HADOOP_CLASSPATH\", HiveConf.getVar(conf, HiveConf.ConfVars.HIVE_HADOOP_CLASSPATH));\n        }\n      }\n\n      if(variables.containsKey(MapRedTask.HIVE_DEBUG_RECURSIVE)) {\n        MapRedTask.configureDebugVariablesForChildJVM(variables);\n      }\n\n\n      if(UserGroupInformation.isSecurityEnabled() &&\n           UserGroupInformation.isLoginKeytabBased()) {\n        //If kerberos security is enabled, and HS2 doAs is enabled,\n        // then additional params need to be set so that the command is run as\n        // intended user\n        secureDoAs = new SecureCmdDoAs(conf);\n        secureDoAs.addEnv(variables);\n      }\n\n      env = new String[variables.size()];\n      int pos = 0;\n      for (Map.Entry<String, String> entry : variables.entrySet()) {\n        String name = entry.getKey();\n        String value = entry.getValue();\n        env[pos++] = name + \"=\" + value;\n        LOG.debug(\"Setting env: \" + env[pos-1]);\n      }\n\n      LOG.info(\"Executing: \" + cmdLine);\n\n      // Run ExecDriver in another JVM\n      executor = Runtime.getRuntime().exec(cmdLine, env, new File(workDir));\n\n      CachingPrintStream errPrintStream = new CachingPrintStream(System.err);\n\n      StreamPrinter outPrinter = new StreamPrinter(executor.getInputStream(), null, System.out);\n      StreamPrinter errPrinter = new StreamPrinter(executor.getErrorStream(), null, errPrintStream);\n\n      outPrinter.start();\n      errPrinter.start();\n\n      int exitVal = jobExecHelper.progressLocal(executor, getId());\n\n      // wait for stream threads to finish\n      outPrinter.join();\n      errPrinter.join();\n\n      if (exitVal != 0) {\n        LOG.error(\"Execution failed with exit status: \" + exitVal);\n        if (SessionState.get() != null) {\n          SessionState.get().addLocalMapRedErrors(getId(), errPrintStream.getOutput());\n        }\n      } else {\n        LOG.info(\"Execution completed successfully\");\n      }\n\n      return exitVal;\n    } catch (Exception e) {\n      LOG.error(\"Exception: \" + e, e);\n      return (1);\n    } finally {\n      if (secureDoAs != null) {\n        secureDoAs.close();\n      }\n    }\n  }",
                "code_after_change": "  public int executeInChildVM(DriverContext driverContext) {\n    // execute in child jvm\n    try {\n      // generate the cmd line to run in the child jvm\n      Context ctx = driverContext.getCtx();\n      String hiveJar = conf.getJar();\n\n      String hadoopExec = conf.getVar(HiveConf.ConfVars.HADOOPBIN);\n      conf.setVar(ConfVars.HIVEADDEDJARS, Utilities.getResourceFiles(conf, SessionState.ResourceType.JAR));\n      // write out the plan to a local file\n      Path planPath = new Path(ctx.getLocalTmpPath(), \"plan.xml\");\n      MapredLocalWork plan = getWork();\n      LOG.info(\"Generating plan file \" + planPath.toString());\n\n      OutputStream out = null;\n      try {\n        out = FileSystem.getLocal(conf).create(planPath);\n        SerializationUtilities.serializePlan(plan, out);\n        out.close();\n        out = null;\n      } finally {\n        IOUtils.closeQuietly(out);\n      }\n\n\n      String isSilent = \"true\".equalsIgnoreCase(System.getProperty(\"test.silent\")) ? \"-nolog\" : \"\";\n\n      String jarCmd;\n\n      jarCmd = hiveJar + \" \" + ExecDriver.class.getName();\n      String hiveConfArgs = ExecDriver.generateCmdLine(conf, ctx);\n      String cmdLine = hadoopExec + \" jar \" + jarCmd + \" -localtask -plan \" + planPath.toString()\n          + \" \" + isSilent + \" \" + hiveConfArgs;\n\n      String workDir = (new File(\".\")).getCanonicalPath();\n      String files = Utilities.getResourceFiles(conf, SessionState.ResourceType.FILE);\n\n      if (!files.isEmpty()) {\n        cmdLine = cmdLine + \" -files \" + files;\n\n        workDir = ctx.getLocalTmpPath().toUri().getPath();\n\n        if (!(new File(workDir)).mkdir()) {\n          throw new IOException(\"Cannot create tmp working dir: \" + workDir);\n        }\n\n        for (String f : StringUtils.split(files, ',')) {\n          Path p = new Path(f);\n          String target = p.toUri().getPath();\n          String link = workDir + Path.SEPARATOR + p.getName();\n          if (FileUtil.symLink(target, link) != 0) {\n            throw new IOException(\"Cannot link to added file: \" + target + \" from: \" + link);\n          }\n        }\n      }\n\n      // Inherit Java system variables\n      String hadoopOpts;\n      StringBuilder sb = new StringBuilder();\n      Properties p = System.getProperties();\n      for (String element : HIVE_SYS_PROP) {\n        if (p.containsKey(element)) {\n          sb.append(\" -D\" + element + \"=\" + p.getProperty(element));\n        }\n      }\n      hadoopOpts = sb.toString();\n      // Inherit the environment variables\n      String[] env;\n      Map<String, String> variables = new HashMap<String, String>(System.getenv());\n      // The user can specify the hadoop memory\n\n      // if (\"local\".equals(conf.getVar(HiveConf.ConfVars.HADOOPJT))) {\n      // if we are running in local mode - then the amount of memory used\n      // by the child jvm can no longer default to the memory used by the\n      // parent jvm\n      // int hadoopMem = conf.getIntVar(HiveConf.ConfVars.HIVEHADOOPMAXMEM);\n      int hadoopMem = conf.getIntVar(HiveConf.ConfVars.HIVEHADOOPMAXMEM);\n      if (hadoopMem == 0) {\n        // remove env var that would default child jvm to use parent's memory\n        // as default. child jvm would use default memory for a hadoop client\n        variables.remove(HADOOP_MEM_KEY);\n      } else {\n        // user specified the memory for local mode hadoop run\n        console.printInfo(\" set heap size\\t\" + hadoopMem + \"MB\");\n        variables.put(HADOOP_MEM_KEY, String.valueOf(hadoopMem));\n      }\n      // } else {\n      // nothing to do - we are not running in local mode - only submitting\n      // the job via a child process. in this case it's appropriate that the\n      // child jvm use the same memory as the parent jvm\n\n      // }\n\n      //Set HADOOP_USER_NAME env variable for child process, so that\n      // it also runs with hadoop permissions for the user the job is running as\n      // This will be used by hadoop only in unsecure(/non kerberos) mode\n      String endUserName = Utils.getUGI().getShortUserName();\n      LOG.debug(\"setting HADOOP_USER_NAME\\t\" + endUserName);\n      variables.put(\"HADOOP_USER_NAME\", endUserName);\n\n      if (variables.containsKey(HADOOP_OPTS_KEY)) {\n        variables.put(HADOOP_OPTS_KEY, variables.get(HADOOP_OPTS_KEY) + hadoopOpts);\n      } else {\n        variables.put(HADOOP_OPTS_KEY, hadoopOpts);\n      }\n\n      //For Windows OS, we need to pass HIVE_HADOOP_CLASSPATH Java parameter while starting\n      //Hiveserver2 using \"-hiveconf hive.hadoop.classpath=%HIVE_LIB%\". This is to combine path(s).\n      if (HiveConf.getVar(conf, HiveConf.ConfVars.HIVE_HADOOP_CLASSPATH)!= null)\n      {\n        if (variables.containsKey(\"HADOOP_CLASSPATH\"))\n        {\n          variables.put(\"HADOOP_CLASSPATH\", variables.get(\"HADOOP_CLASSPATH\") + \";\" + HiveConf.getVar(conf, HiveConf.ConfVars.HIVE_HADOOP_CLASSPATH));\n        } else {\n          variables.put(\"HADOOP_CLASSPATH\", HiveConf.getVar(conf, HiveConf.ConfVars.HIVE_HADOOP_CLASSPATH));\n        }\n      }\n\n      if(variables.containsKey(MapRedTask.HIVE_DEBUG_RECURSIVE)) {\n        MapRedTask.configureDebugVariablesForChildJVM(variables);\n      }\n\n\n      if(UserGroupInformation.isSecurityEnabled() &&\n           UserGroupInformation.isLoginKeytabBased()) {\n        //If kerberos security is enabled, and HS2 doAs is enabled,\n        // then additional params need to be set so that the command is run as\n        // intended user\n        secureDoAs = new SecureCmdDoAs(conf);\n        secureDoAs.addEnv(variables);\n      }\n\n      // If HIVE_LOCAL_TASK_CHILD_OPTS is set, child VM environment setting\n      // HADOOP_CLIENT_OPTS will be replaced with HIVE_LOCAL_TASK_CHILD_OPTS.\n      // HADOOP_OPTS is updated too since HADOOP_CLIENT_OPTS is appended\n      // to HADOOP_OPTS in most cases. This way, the local task JVM can\n      // have different settings from those of HiveServer2.\n      if (variables.containsKey(HIVE_LOCAL_TASK_CHILD_OPTS_KEY)) {\n        String childOpts = variables.get(HIVE_LOCAL_TASK_CHILD_OPTS_KEY);\n        if (childOpts == null) {\n          childOpts = \"\";\n        }\n        String clientOpts = variables.put(HADOOP_CLIENT_OPTS, childOpts);\n        String tmp = variables.get(HADOOP_OPTS_KEY);\n        if (tmp != null && !StringUtils.isBlank(clientOpts)) {\n          tmp = tmp.replace(clientOpts, childOpts);\n          variables.put(HADOOP_OPTS_KEY, tmp);\n        }\n      }\n\n      env = new String[variables.size()];\n      int pos = 0;\n      for (Map.Entry<String, String> entry : variables.entrySet()) {\n        String name = entry.getKey();\n        String value = entry.getValue();\n        env[pos++] = name + \"=\" + value;\n        LOG.debug(\"Setting env: \" + env[pos-1]);\n      }\n\n      LOG.info(\"Executing: \" + cmdLine);\n\n      // Run ExecDriver in another JVM\n      executor = Runtime.getRuntime().exec(cmdLine, env, new File(workDir));\n\n      CachingPrintStream errPrintStream = new CachingPrintStream(System.err);\n\n      StreamPrinter outPrinter = new StreamPrinter(executor.getInputStream(), null, System.out);\n      StreamPrinter errPrinter = new StreamPrinter(executor.getErrorStream(), null, errPrintStream);\n\n      outPrinter.start();\n      errPrinter.start();\n\n      int exitVal = jobExecHelper.progressLocal(executor, getId());\n\n      // wait for stream threads to finish\n      outPrinter.join();\n      errPrinter.join();\n\n      if (exitVal != 0) {\n        LOG.error(\"Execution failed with exit status: \" + exitVal);\n        if (SessionState.get() != null) {\n          SessionState.get().addLocalMapRedErrors(getId(), errPrintStream.getOutput());\n        }\n      } else {\n        LOG.info(\"Execution completed successfully\");\n      }\n\n      return exitVal;\n    } catch (Exception e) {\n      LOG.error(\"Exception: \" + e, e);\n      return (1);\n    } finally {\n      if (secureDoAs != null) {\n        secureDoAs.close();\n      }\n    }\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.exec.mr.MapRedTask.execute": {
                "code_before_change": "  public int execute(DriverContext driverContext) {\n\n    Context ctx = driverContext.getCtx();\n    boolean ctxCreated = false;\n\n    try {\n      if (ctx == null) {\n        ctx = new Context(conf);\n        ctxCreated = true;\n      }\n\n      // estimate number of reducers\n      setNumberOfReducers();\n\n      // auto-determine local mode if allowed\n      if (!ctx.isLocalOnlyExecutionMode() &&\n          conf.getBoolVar(HiveConf.ConfVars.LOCALMODEAUTO)) {\n\n        if (inputSummary == null) {\n          inputSummary = Utilities.getInputSummary(driverContext.getCtx(), work.getMapWork(), null);\n        }\n\n        // set the values of totalInputFileSize and totalInputNumFiles, estimating them\n        // if percentage block sampling is being used\n        double samplePercentage = Utilities.getHighestSamplePercentage(work.getMapWork());\n        totalInputFileSize = Utilities.getTotalInputFileSize(inputSummary, work.getMapWork(), samplePercentage);\n        totalInputNumFiles = Utilities.getTotalInputNumFiles(inputSummary, work.getMapWork(), samplePercentage);\n\n        // at this point the number of reducers is precisely defined in the plan\n        int numReducers = work.getReduceWork() == null ? 0 : work.getReduceWork().getNumReduceTasks();\n\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Task: \" + getId() + \", Summary: \" +\n                    totalInputFileSize + \",\" + totalInputNumFiles + \",\"\n                    + numReducers);\n        }\n\n        String reason = MapRedTask.isEligibleForLocalMode(conf, numReducers,\n            totalInputFileSize, totalInputNumFiles);\n        if (reason == null) {\n          // clone configuration before modifying it on per-task basis\n          cloneConf();\n          ShimLoader.getHadoopShims().setJobLauncherRpcAddress(conf, \"local\");\n          console.printInfo(\"Selecting local mode for task: \" + getId());\n          this.setLocalMode(true);\n        } else {\n          console.printInfo(\"Cannot run job locally: \" + reason);\n          this.setLocalMode(false);\n        }\n      }\n\n      runningViaChild = conf.getBoolVar(HiveConf.ConfVars.SUBMITVIACHILD);\n\n      if (!runningViaChild) {\n        // we are not running this mapred task via child jvm\n        // so directly invoke ExecDriver\n        return super.execute(driverContext);\n      }\n\n      // we need to edit the configuration to setup cmdline. clone it first\n      cloneConf();\n\n      // propagate input format if necessary\n      super.setInputAttributes(conf);\n\n      // enable assertion\n      String hadoopExec = conf.getVar(HiveConf.ConfVars.HADOOPBIN);\n      String hiveJar = conf.getJar();\n\n      String libJarsOption;\n      String addedJars = Utilities.getResourceFiles(conf, SessionState.ResourceType.JAR);\n      conf.setVar(ConfVars.HIVEADDEDJARS, addedJars);\n      String auxJars = conf.getAuxJars();\n      // Put auxjars and addedjars together into libjars\n      if (StringUtils.isEmpty(addedJars)) {\n        if (StringUtils.isEmpty(auxJars)) {\n          libJarsOption = \" \";\n        } else {\n          libJarsOption = \" -libjars \" + auxJars + \" \";\n        }\n      } else {\n        if (StringUtils.isEmpty(auxJars)) {\n          libJarsOption = \" -libjars \" + addedJars + \" \";\n        } else {\n          libJarsOption = \" -libjars \" + addedJars + \",\" + auxJars + \" \";\n        }\n      }\n\n      // Generate the hiveConfArgs after potentially adding the jars\n      String hiveConfArgs = generateCmdLine(conf, ctx);\n\n      // write out the plan to a local file\n      Path planPath = new Path(ctx.getLocalTmpPath(), \"plan.xml\");\n      MapredWork plan = getWork();\n      LOG.info(\"Generating plan file \" + planPath.toString());\n\n      OutputStream out = null;\n      try {\n        out = FileSystem.getLocal(conf).create(planPath);\n        Utilities.serializePlan(plan, out, conf);\n        out.close();\n        out = null;\n      } finally {\n        IOUtils.closeQuietly(out);\n      }\n\n      String isSilent = \"true\".equalsIgnoreCase(System\n          .getProperty(\"test.silent\")) ? \"-nolog\" : \"\";\n\n      String jarCmd = hiveJar + \" \" + ExecDriver.class.getName() + libJarsOption;\n\n      String cmdLine = hadoopExec + \" jar \" + jarCmd + \" -plan \"\n          + planPath.toString() + \" \" + isSilent + \" \" + hiveConfArgs;\n\n      String workDir = (new File(\".\")).getCanonicalPath();\n      String files = Utilities.getResourceFiles(conf, SessionState.ResourceType.FILE);\n      if (!files.isEmpty()) {\n        cmdLine = cmdLine + \" -files \" + files;\n\n        workDir = ctx.getLocalTmpPath().toUri().getPath();\n\n        if (! (new File(workDir)).mkdir()) {\n          throw new IOException (\"Cannot create tmp working dir: \" + workDir);\n        }\n\n        for (String f: StringUtils.split(files, ',')) {\n          Path p = new Path(f);\n          String target = p.toUri().getPath();\n          String link = workDir + Path.SEPARATOR + p.getName();\n          if (FileUtil.symLink(target, link) != 0) {\n            throw new IOException (\"Cannot link to added file: \" + target + \" from: \" + link);\n          }\n        }\n      }\n\n      LOG.info(\"Executing: \" + cmdLine);\n\n      // Inherit Java system variables\n      String hadoopOpts;\n      StringBuilder sb = new StringBuilder();\n      Properties p = System.getProperties();\n      for (String element : HIVE_SYS_PROP) {\n        if (p.containsKey(element)) {\n          sb.append(\" -D\" + element + \"=\" + p.getProperty(element));\n        }\n      }\n      hadoopOpts = sb.toString();\n      // Inherit the environment variables\n      String[] env;\n      Map<String, String> variables = new HashMap(System.getenv());\n      // The user can specify the hadoop memory\n\n      if (ShimLoader.getHadoopShims().isLocalMode(conf)) {\n        // if we are running in local mode - then the amount of memory used\n        // by the child jvm can no longer default to the memory used by the\n        // parent jvm\n        int hadoopMem = conf.getIntVar(HiveConf.ConfVars.HIVEHADOOPMAXMEM);\n        if (hadoopMem == 0) {\n          // remove env var that would default child jvm to use parent's memory\n          // as default. child jvm would use default memory for a hadoop client\n          variables.remove(HADOOP_MEM_KEY);\n        } else {\n          // user specified the memory for local mode hadoop run\n          variables.put(HADOOP_MEM_KEY, String.valueOf(hadoopMem));\n        }\n      } else {\n        // nothing to do - we are not running in local mode - only submitting\n        // the job via a child process. in this case it's appropriate that the\n        // child jvm use the same memory as the parent jvm\n      }\n\n      if (variables.containsKey(HADOOP_OPTS_KEY)) {\n        variables.put(HADOOP_OPTS_KEY, variables.get(HADOOP_OPTS_KEY)\n            + hadoopOpts);\n      } else {\n        variables.put(HADOOP_OPTS_KEY, hadoopOpts);\n      }\n\n      if(variables.containsKey(HIVE_DEBUG_RECURSIVE)) {\n        configureDebugVariablesForChildJVM(variables);\n      }\n\n      env = new String[variables.size()];\n      int pos = 0;\n      for (Map.Entry<String, String> entry : variables.entrySet()) {\n        String name = entry.getKey();\n        String value = entry.getValue();\n        env[pos++] = name + \"=\" + value;\n      }\n      // Run ExecDriver in another JVM\n      executor = Runtime.getRuntime().exec(cmdLine, env, new File(workDir));\n\n      CachingPrintStream errPrintStream =\n          new CachingPrintStream(SessionState.getConsole().getChildErrStream());\n\n      StreamPrinter outPrinter = new StreamPrinter(\n          executor.getInputStream(), null,\n          SessionState.getConsole().getChildOutStream());\n      StreamPrinter errPrinter = new StreamPrinter(\n          executor.getErrorStream(), null,\n          errPrintStream);\n\n      outPrinter.start();\n      errPrinter.start();\n\n      int exitVal = jobExecHelper.progressLocal(executor, getId());\n\n      // wait for stream threads to finish\n      outPrinter.join();\n      errPrinter.join();\n\n      if (exitVal != 0) {\n        LOG.error(\"Execution failed with exit status: \" + exitVal);\n        if (SessionState.get() != null) {\n          SessionState.get().addLocalMapRedErrors(getId(), errPrintStream.getOutput());\n        }\n      } else {\n        LOG.info(\"Execution completed successfully\");\n      }\n\n      return exitVal;\n    } catch (Exception e) {\n      e.printStackTrace();\n      LOG.error(\"Exception: \" + e.getMessage());\n      return (1);\n    } finally {\n      try {\n        // creating the context can create a bunch of files. So make\n        // sure to clear it out\n        if(ctxCreated) {\n          ctx.clear();\n        }\n\n      } catch (Exception e) {\n        LOG.error(\"Exception: \" + e.getMessage());\n      }\n    }\n  }",
                "code_after_change": "  public int execute(DriverContext driverContext) {\n\n    Context ctx = driverContext.getCtx();\n    boolean ctxCreated = false;\n\n    try {\n      if (ctx == null) {\n        ctx = new Context(conf);\n        ctxCreated = true;\n      }\n\n      // estimate number of reducers\n      setNumberOfReducers();\n\n      // auto-determine local mode if allowed\n      if (!ctx.isLocalOnlyExecutionMode() &&\n          conf.getBoolVar(HiveConf.ConfVars.LOCALMODEAUTO)) {\n\n        if (inputSummary == null) {\n          inputSummary = Utilities.getInputSummary(driverContext.getCtx(), work.getMapWork(), null);\n        }\n\n        // set the values of totalInputFileSize and totalInputNumFiles, estimating them\n        // if percentage block sampling is being used\n        double samplePercentage = Utilities.getHighestSamplePercentage(work.getMapWork());\n        totalInputFileSize = Utilities.getTotalInputFileSize(inputSummary, work.getMapWork(), samplePercentage);\n        totalInputNumFiles = Utilities.getTotalInputNumFiles(inputSummary, work.getMapWork(), samplePercentage);\n\n        // at this point the number of reducers is precisely defined in the plan\n        int numReducers = work.getReduceWork() == null ? 0 : work.getReduceWork().getNumReduceTasks();\n\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Task: \" + getId() + \", Summary: \" +\n                    totalInputFileSize + \",\" + totalInputNumFiles + \",\"\n                    + numReducers);\n        }\n\n        String reason = MapRedTask.isEligibleForLocalMode(conf, numReducers,\n            totalInputFileSize, totalInputNumFiles);\n        if (reason == null) {\n          // clone configuration before modifying it on per-task basis\n          cloneConf();\n          ShimLoader.getHadoopShims().setJobLauncherRpcAddress(conf, \"local\");\n          console.printInfo(\"Selecting local mode for task: \" + getId());\n          this.setLocalMode(true);\n        } else {\n          console.printInfo(\"Cannot run job locally: \" + reason);\n          this.setLocalMode(false);\n        }\n      }\n\n      runningViaChild = conf.getBoolVar(HiveConf.ConfVars.SUBMITVIACHILD);\n\n      if (!runningViaChild) {\n        // we are not running this mapred task via child jvm\n        // so directly invoke ExecDriver\n        return super.execute(driverContext);\n      }\n\n      // we need to edit the configuration to setup cmdline. clone it first\n      cloneConf();\n\n      // propagate input format if necessary\n      super.setInputAttributes(conf);\n\n      // enable assertion\n      String hadoopExec = conf.getVar(HiveConf.ConfVars.HADOOPBIN);\n      String hiveJar = conf.getJar();\n\n      String libJarsOption;\n      String addedJars = Utilities.getResourceFiles(conf, SessionState.ResourceType.JAR);\n      conf.setVar(ConfVars.HIVEADDEDJARS, addedJars);\n      String auxJars = conf.getAuxJars();\n      // Put auxjars and addedjars together into libjars\n      if (StringUtils.isEmpty(addedJars)) {\n        if (StringUtils.isEmpty(auxJars)) {\n          libJarsOption = \" \";\n        } else {\n          libJarsOption = \" -libjars \" + auxJars + \" \";\n        }\n      } else {\n        if (StringUtils.isEmpty(auxJars)) {\n          libJarsOption = \" -libjars \" + addedJars + \" \";\n        } else {\n          libJarsOption = \" -libjars \" + addedJars + \",\" + auxJars + \" \";\n        }\n      }\n\n      // Generate the hiveConfArgs after potentially adding the jars\n      String hiveConfArgs = generateCmdLine(conf, ctx);\n\n      // write out the plan to a local file\n      Path planPath = new Path(ctx.getLocalTmpPath(), \"plan.xml\");\n      MapredWork plan = getWork();\n      LOG.info(\"Generating plan file \" + planPath.toString());\n\n      OutputStream out = null;\n      try {\n        out = FileSystem.getLocal(conf).create(planPath);\n        SerializationUtilities.serializePlan(plan, out);\n        out.close();\n        out = null;\n      } finally {\n        IOUtils.closeQuietly(out);\n      }\n\n      String isSilent = \"true\".equalsIgnoreCase(System\n          .getProperty(\"test.silent\")) ? \"-nolog\" : \"\";\n\n      String jarCmd = hiveJar + \" \" + ExecDriver.class.getName() + libJarsOption;\n\n      String cmdLine = hadoopExec + \" jar \" + jarCmd + \" -plan \"\n          + planPath.toString() + \" \" + isSilent + \" \" + hiveConfArgs;\n\n      String workDir = (new File(\".\")).getCanonicalPath();\n      String files = Utilities.getResourceFiles(conf, SessionState.ResourceType.FILE);\n      if (!files.isEmpty()) {\n        cmdLine = cmdLine + \" -files \" + files;\n\n        workDir = ctx.getLocalTmpPath().toUri().getPath();\n\n        if (! (new File(workDir)).mkdir()) {\n          throw new IOException (\"Cannot create tmp working dir: \" + workDir);\n        }\n\n        for (String f: StringUtils.split(files, ',')) {\n          Path p = new Path(f);\n          String target = p.toUri().getPath();\n          String link = workDir + Path.SEPARATOR + p.getName();\n          if (FileUtil.symLink(target, link) != 0) {\n            throw new IOException (\"Cannot link to added file: \" + target + \" from: \" + link);\n          }\n        }\n      }\n\n      LOG.info(\"Executing: \" + cmdLine);\n\n      // Inherit Java system variables\n      String hadoopOpts;\n      StringBuilder sb = new StringBuilder();\n      Properties p = System.getProperties();\n      for (String element : HIVE_SYS_PROP) {\n        if (p.containsKey(element)) {\n          sb.append(\" -D\" + element + \"=\" + p.getProperty(element));\n        }\n      }\n      hadoopOpts = sb.toString();\n      // Inherit the environment variables\n      String[] env;\n      Map<String, String> variables = new HashMap<String, String>(System.getenv());\n      // The user can specify the hadoop memory\n\n      if (ShimLoader.getHadoopShims().isLocalMode(conf)) {\n        // if we are running in local mode - then the amount of memory used\n        // by the child jvm can no longer default to the memory used by the\n        // parent jvm\n        int hadoopMem = conf.getIntVar(HiveConf.ConfVars.HIVEHADOOPMAXMEM);\n        if (hadoopMem == 0) {\n          // remove env var that would default child jvm to use parent's memory\n          // as default. child jvm would use default memory for a hadoop client\n          variables.remove(HADOOP_MEM_KEY);\n        } else {\n          // user specified the memory for local mode hadoop run\n          variables.put(HADOOP_MEM_KEY, String.valueOf(hadoopMem));\n        }\n      } else {\n        // nothing to do - we are not running in local mode - only submitting\n        // the job via a child process. in this case it's appropriate that the\n        // child jvm use the same memory as the parent jvm\n      }\n\n      if (variables.containsKey(HADOOP_OPTS_KEY)) {\n        variables.put(HADOOP_OPTS_KEY, variables.get(HADOOP_OPTS_KEY)\n            + hadoopOpts);\n      } else {\n        variables.put(HADOOP_OPTS_KEY, hadoopOpts);\n      }\n\n      if(variables.containsKey(HIVE_DEBUG_RECURSIVE)) {\n        configureDebugVariablesForChildJVM(variables);\n      }\n\n      env = new String[variables.size()];\n      int pos = 0;\n      for (Map.Entry<String, String> entry : variables.entrySet()) {\n        String name = entry.getKey();\n        String value = entry.getValue();\n        env[pos++] = name + \"=\" + value;\n      }\n      // Run ExecDriver in another JVM\n      executor = Runtime.getRuntime().exec(cmdLine, env, new File(workDir));\n\n      CachingPrintStream errPrintStream =\n          new CachingPrintStream(SessionState.getConsole().getChildErrStream());\n\n      StreamPrinter outPrinter = new StreamPrinter(\n          executor.getInputStream(), null,\n          SessionState.getConsole().getChildOutStream());\n      StreamPrinter errPrinter = new StreamPrinter(\n          executor.getErrorStream(), null,\n          errPrintStream);\n\n      outPrinter.start();\n      errPrinter.start();\n\n      int exitVal = jobExecHelper.progressLocal(executor, getId());\n\n      // wait for stream threads to finish\n      outPrinter.join();\n      errPrinter.join();\n\n      if (exitVal != 0) {\n        LOG.error(\"Execution failed with exit status: \" + exitVal);\n        if (SessionState.get() != null) {\n          SessionState.get().addLocalMapRedErrors(getId(), errPrintStream.getOutput());\n        }\n      } else {\n        LOG.info(\"Execution completed successfully\");\n      }\n\n      return exitVal;\n    } catch (Exception e) {\n      e.printStackTrace();\n      LOG.error(\"Exception: \" + e.getMessage());\n      return (1);\n    } finally {\n      try {\n        // creating the context can create a bunch of files. So make\n        // sure to clear it out\n        if(ctxCreated) {\n          ctx.clear();\n        }\n\n      } catch (Exception e) {\n        LOG.error(\"Exception: \" + e.getMessage());\n      }\n    }\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report mentions the failure in HiveServer2 when jmx options are used, and the stack trace includes 'org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask', which is part of the ground truth methods. However, it does not precisely identify the root cause or suggest a fix. The problem location is partially identified as it is in the shared stack trace context with the ground truth methods. There is no incorrect information in the bug report."
        }
    },
    {
        "filename": "HIVE-9055.json",
        "code_diff": {
            "ql.src.java.org.apache.hadoop.hive.ql.parse.GenTezWork.process": {
                "code_before_change": "  public Object process(Node nd, Stack<Node> stack,\n      NodeProcessorCtx procContext, Object... nodeOutputs)\n      throws SemanticException {\n\n    GenTezProcContext context = (GenTezProcContext) procContext;\n\n    assert context != null && context.currentTask != null\n        && context.currentRootOperator != null;\n\n    // Operator is a file sink or reduce sink. Something that forces\n    // a new vertex.\n    Operator<?> operator = (Operator<?>) nd;\n\n    // root is the start of the operator pipeline we're currently\n    // packing into a vertex, typically a table scan, union or join\n    Operator<?> root = context.currentRootOperator;\n\n    LOG.debug(\"Root operator: \" + root);\n    LOG.debug(\"Leaf operator: \" + operator);\n\n    if (context.clonedReduceSinks.contains(operator)) {\n      // if we're visiting a terminal we've created ourselves,\n      // just skip and keep going\n      return null;\n    }\n\n    TezWork tezWork = context.currentTask.getWork();\n\n    // Right now the work graph is pretty simple. If there is no\n    // Preceding work we have a root and will generate a map\n    // vertex. If there is a preceding work we will generate\n    // a reduce vertex\n    BaseWork work;\n    if (context.rootToWorkMap.containsKey(root)) {\n      // having seen the root operator before means there was a branch in the\n      // operator graph. There's typically two reasons for that: a) mux/demux\n      // b) multi insert. Mux/Demux will hit the same leaf again, multi insert\n      // will result into a vertex with multiple FS or RS operators.\n      if (context.childToWorkMap.containsKey(operator)) {\n        // if we've seen both root and child, we can bail.\n        \n        // clear out the mapjoin set. we don't need it anymore.\n        context.currentMapJoinOperators.clear();\n\n        // clear out the union set. we don't need it anymore.\n        context.currentUnionOperators.clear();\n\n        return null;\n      } else {\n        // At this point we don't have to do anything special. Just\n        // run through the regular paces w/o creating a new task.\n        work = context.rootToWorkMap.get(root);\n      }\n    } else {\n      // create a new vertex\n      if (context.preceedingWork == null) {\n        work = utils.createMapWork(context, root, tezWork, null);\n      } else {\n        work = utils.createReduceWork(context, root, tezWork);\n      }\n      context.rootToWorkMap.put(root, work);\n    }\n\n    // this is where we set the sort columns that we will be using for KeyValueInputMerge\n    if (operator instanceof DummyStoreOperator) {\n      work.addSortCols(root.getOpTraits().getSortCols().get(0));\n    }\n\n    if (!context.childToWorkMap.containsKey(operator)) {\n      List<BaseWork> workItems = new LinkedList<BaseWork>();\n      workItems.add(work);\n      context.childToWorkMap.put(operator, workItems);\n    } else {\n      context.childToWorkMap.get(operator).add(work);\n    }\n\n    // this transformation needs to be first because it changes the work item itself.\n    // which can affect the working of all downstream transformations.\n    if (context.currentMergeJoinOperator != null) {\n      // we are currently walking the big table side of the merge join. we need to create or hook up\n      // merge join work.\n      MergeJoinWork mergeJoinWork = null;\n      if (context.opMergeJoinWorkMap.containsKey(context.currentMergeJoinOperator)) {\n        // we have found a merge work corresponding to this closing operator. Hook up this work.\n        mergeJoinWork = context.opMergeJoinWorkMap.get(context.currentMergeJoinOperator);\n      } else {\n        // we need to create the merge join work\n        mergeJoinWork = new MergeJoinWork();\n        mergeJoinWork.setMergeJoinOperator(context.currentMergeJoinOperator);\n        tezWork.add(mergeJoinWork);\n        context.opMergeJoinWorkMap.put(context.currentMergeJoinOperator, mergeJoinWork);\n      }\n      // connect the work correctly.\n      work.addSortCols(root.getOpTraits().getSortCols().get(0));\n      mergeJoinWork.addMergedWork(work, null);\n      Operator<? extends OperatorDesc> parentOp =\n          getParentFromStack(context.currentMergeJoinOperator, stack);\n      int pos = context.currentMergeJoinOperator.getTagForOperator(parentOp);\n      work.setTag(pos);\n      tezWork.setVertexType(work, VertexType.MULTI_INPUT_UNINITIALIZED_EDGES);\n      for (BaseWork parentWork : tezWork.getParents(work)) {\n        TezEdgeProperty edgeProp = tezWork.getEdgeProperty(parentWork, work);\n        tezWork.disconnect(parentWork, work);\n        tezWork.connect(parentWork, mergeJoinWork, edgeProp);\n      }\n\n      for (BaseWork childWork : tezWork.getChildren(work)) {\n        TezEdgeProperty edgeProp = tezWork.getEdgeProperty(work, childWork);\n        tezWork.disconnect(work, childWork);\n        tezWork.connect(mergeJoinWork, childWork, edgeProp);\n      }\n      tezWork.remove(work);\n      context.rootToWorkMap.put(root, mergeJoinWork);\n      context.childToWorkMap.get(operator).remove(work);\n      context.childToWorkMap.get(operator).add(mergeJoinWork);\n      work = mergeJoinWork;\n      context.currentMergeJoinOperator = null;\n    }\n\n    // remember which mapjoin operator links with which work\n    if (!context.currentMapJoinOperators.isEmpty()) {\n      for (MapJoinOperator mj: context.currentMapJoinOperators) {\n        LOG.debug(\"Processing map join: \" + mj);\n        // remember the mapping in case we scan another branch of the\n        // mapjoin later\n        if (!context.mapJoinWorkMap.containsKey(mj)) {\n          List<BaseWork> workItems = new LinkedList<BaseWork>();\n          workItems.add(work);\n          context.mapJoinWorkMap.put(mj, workItems);\n        } else {\n          context.mapJoinWorkMap.get(mj).add(work);\n        }\n\n        /*\n         * this happens in case of map join operations.\n         * The tree looks like this:\n         *\n         *        RS <--- we are here perhaps\n         *        |\n         *     MapJoin\n         *     /     \\\n         *   RS       TS\n         *  /\n         * TS\n         *\n         * If we are at the RS pointed above, and we may have already visited the\n         * RS following the TS, we have already generated work for the TS-RS.\n         * We need to hook the current work to this generated work.\n         */\n        if (context.linkOpWithWorkMap.containsKey(mj)) {\n          Map<BaseWork,TezEdgeProperty> linkWorkMap = context.linkOpWithWorkMap.get(mj);\n          if (linkWorkMap != null) {\n            if (context.linkChildOpWithDummyOp.containsKey(mj)) {\n              for (Operator<?> dummy: context.linkChildOpWithDummyOp.get(mj)) {\n                work.addDummyOp((HashTableDummyOperator) dummy);\n              }\n            }\n            for (Entry<BaseWork,TezEdgeProperty> parentWorkMap : linkWorkMap.entrySet()) {\n              BaseWork parentWork = parentWorkMap.getKey();\n              LOG.debug(\"connecting \"+parentWork.getName()+\" with \"+work.getName());\n              TezEdgeProperty edgeProp = parentWorkMap.getValue();\n              tezWork.connect(parentWork, work, edgeProp);\n              if (edgeProp.getEdgeType() == EdgeType.CUSTOM_EDGE) {\n                tezWork.setVertexType(work, VertexType.INITIALIZED_EDGES);\n              }\n\n              // need to set up output name for reduce sink now that we know the name\n              // of the downstream work\n              for (ReduceSinkOperator r:\n                     context.linkWorkWithReduceSinkMap.get(parentWork)) {\n                if (r.getConf().getOutputName() != null) {\n                  LOG.debug(\"Cloning reduce sink for multi-child broadcast edge\");\n                  // we've already set this one up. Need to clone for the next work.\n                  r = (ReduceSinkOperator) OperatorFactory.getAndMakeChild(\n                      (ReduceSinkDesc)r.getConf().clone(), r.getParentOperators());\n                  context.clonedReduceSinks.add(r);\n                }\n                r.getConf().setOutputName(work.getName());\n                context.connectedReduceSinks.add(r);\n              }\n            }\n          }\n        }\n      }\n      // clear out the set. we don't need it anymore.\n      context.currentMapJoinOperators.clear();\n    }\n\n    if (!context.currentUnionOperators.isEmpty()) {\n      // if there are union all operators we need to add the work to the set\n      // of union operators.\n\n      UnionWork unionWork;\n      if (context.unionWorkMap.containsKey(operator)) {\n        // we've seen this terminal before and have created a union work object.\n        // just need to add this work to it. There will be no children of this one\n        // since we've passed this operator before.\n        assert operator.getChildOperators().isEmpty();\n        unionWork = (UnionWork) context.unionWorkMap.get(operator);\n\n      } else {\n        // first time through. we need to create a union work object and add this\n        // work to it. Subsequent work should reference the union and not the actual\n        // work.\n        unionWork = utils.createUnionWork(context, operator, tezWork);\n      }\n\n      // finally hook everything up\n      LOG.debug(\"Connecting union work (\"+unionWork+\") with work (\"+work+\")\");\n      TezEdgeProperty edgeProp = new TezEdgeProperty(EdgeType.CONTAINS);\n      tezWork.connect(unionWork, work, edgeProp);\n      unionWork.addUnionOperators(context.currentUnionOperators);\n      context.currentUnionOperators.clear();\n      context.workWithUnionOperators.add(work);\n      work = unionWork;\n    }\n\n\n    // This is where we cut the tree as described above. We also remember that\n    // we might have to connect parent work with this work later.\n    boolean removeParents = false;\n    for (Operator<?> parent: new ArrayList<Operator<?>>(root.getParentOperators())) {\n      removeParents = true;\n      context.leafOperatorToFollowingWork.put(parent, work);\n      LOG.debug(\"Removing \" + parent + \" as parent from \" + root);\n    }\n    if (removeParents) {\n      for (Operator<?> parent : new ArrayList<Operator<?>>(root.getParentOperators())) {\n        root.removeParent(parent);\n      }\n    }\n\n    // We're scanning a tree from roots to leaf (this is not technically\n    // correct, demux and mux operators might form a diamond shape, but\n    // we will only scan one path and ignore the others, because the\n    // diamond shape is always contained in a single vertex). The scan\n    // is depth first and because we remove parents when we pack a pipeline\n    // into a vertex we will never visit any node twice. But because of that\n    // we might have a situation where we need to connect 'work' that comes after\n    // the 'work' we're currently looking at.\n    //\n    // Also note: the concept of leaf and root is reversed in hive for historical\n    // reasons. Roots are data sources, leaves are data sinks. I know.\n    if (context.leafOperatorToFollowingWork.containsKey(operator)) {\n\n      BaseWork followingWork = context.leafOperatorToFollowingWork.get(operator);\n      long bytesPerReducer = context.conf.getLongVar(HiveConf.ConfVars.BYTESPERREDUCER);\n\n      LOG.debug(\"Second pass. Leaf operator: \"+operator\n        +\" has common downstream work:\"+followingWork);\n\n      if (operator instanceof DummyStoreOperator) {\n        // this is the small table side.\n        assert (followingWork instanceof MergeJoinWork);\n        MergeJoinWork mergeJoinWork = (MergeJoinWork) followingWork;\n        CommonMergeJoinOperator mergeJoinOp = mergeJoinWork.getMergeJoinOperator();\n        work.setTag(mergeJoinOp.getTagForOperator(operator));\n        mergeJoinWork.addMergedWork(null, work);\n        tezWork.setVertexType(mergeJoinWork, VertexType.MULTI_INPUT_UNINITIALIZED_EDGES);\n        for (BaseWork parentWork : tezWork.getParents(work)) {\n          TezEdgeProperty edgeProp = tezWork.getEdgeProperty(parentWork, work);\n          tezWork.disconnect(parentWork, work);\n          tezWork.connect(parentWork, mergeJoinWork, edgeProp);\n        }\n        work = mergeJoinWork;\n      } else {\n        // need to add this branch to the key + value info\n        assert operator instanceof ReduceSinkOperator\n            && ((followingWork instanceof ReduceWork) || (followingWork instanceof MergeJoinWork)\n                || followingWork instanceof UnionWork);\n        ReduceSinkOperator rs = (ReduceSinkOperator) operator;\n        ReduceWork rWork = null;\n        if (followingWork instanceof MergeJoinWork) {\n          MergeJoinWork mergeJoinWork = (MergeJoinWork) followingWork;\n          rWork = (ReduceWork) mergeJoinWork.getMainWork();\n        } else if (followingWork instanceof UnionWork) {\n          // this can only be possible if there is merge work followed by the union\n          UnionWork unionWork = (UnionWork) followingWork;\n          int index = getMergeIndex(tezWork, unionWork, rs);\n          // guaranteed to be instance of MergeJoinWork if index is valid\n          BaseWork baseWork = tezWork.getChildren(unionWork).get(index);\n          if (baseWork instanceof MergeJoinWork) {\n            MergeJoinWork mergeJoinWork = (MergeJoinWork) baseWork;\n            // disconnect the connection to union work and connect to merge work\n            followingWork = mergeJoinWork;\n            rWork = (ReduceWork) mergeJoinWork.getMainWork();\n          } else {\n            throw new SemanticException(\"Unknown work type found: \"\n                + baseWork.getClass().getCanonicalName());\n          }\n        } else {\n          rWork = (ReduceWork) followingWork;\n        }\n        GenMapRedUtils.setKeyAndValueDesc(rWork, rs);\n\n        // remember which parent belongs to which tag\n        int tag = rs.getConf().getTag();\n        rWork.getTagToInput().put(tag == -1 ? 0 : tag, work.getName());\n\n        // remember the output name of the reduce sink\n        rs.getConf().setOutputName(rWork.getName());\n\n        if (!context.connectedReduceSinks.contains(rs)) {\n          // add dependency between the two work items\n          TezEdgeProperty edgeProp;\n          if (rWork.isAutoReduceParallelism()) {\n            edgeProp =\n                new TezEdgeProperty(context.conf, EdgeType.SIMPLE_EDGE, true,\n                    rWork.getMinReduceTasks(), rWork.getMaxReduceTasks(), bytesPerReducer);\n          } else {\n            edgeProp = new TezEdgeProperty(EdgeType.SIMPLE_EDGE);\n          }\n          tezWork.connect(work, followingWork, edgeProp);\n          context.connectedReduceSinks.add(rs);\n        }\n      }\n    } else {\n      LOG.debug(\"First pass. Leaf operator: \"+operator);\n    }\n\n    // No children means we're at the bottom. If there are more operators to scan\n    // the next item will be a new root.\n    if (!operator.getChildOperators().isEmpty()) {\n      assert operator.getChildOperators().size() == 1;\n      context.parentOfRoot = operator;\n      context.currentRootOperator = operator.getChildOperators().get(0);\n      context.preceedingWork = work;\n    }\n\n    return null;\n  }",
                "code_after_change": "  public Object process(Node nd, Stack<Node> stack,\n      NodeProcessorCtx procContext, Object... nodeOutputs)\n      throws SemanticException {\n\n    GenTezProcContext context = (GenTezProcContext) procContext;\n\n    assert context != null && context.currentTask != null\n        && context.currentRootOperator != null;\n\n    // Operator is a file sink or reduce sink. Something that forces\n    // a new vertex.\n    Operator<?> operator = (Operator<?>) nd;\n\n    // root is the start of the operator pipeline we're currently\n    // packing into a vertex, typically a table scan, union or join\n    Operator<?> root = context.currentRootOperator;\n\n    LOG.debug(\"Root operator: \" + root);\n    LOG.debug(\"Leaf operator: \" + operator);\n\n    if (context.clonedReduceSinks.contains(operator)) {\n      // if we're visiting a terminal we've created ourselves,\n      // just skip and keep going\n      return null;\n    }\n\n    TezWork tezWork = context.currentTask.getWork();\n\n    // Right now the work graph is pretty simple. If there is no\n    // Preceding work we have a root and will generate a map\n    // vertex. If there is a preceding work we will generate\n    // a reduce vertex\n    BaseWork work;\n    if (context.rootToWorkMap.containsKey(root)) {\n      // having seen the root operator before means there was a branch in the\n      // operator graph. There's typically two reasons for that: a) mux/demux\n      // b) multi insert. Mux/Demux will hit the same leaf again, multi insert\n      // will result into a vertex with multiple FS or RS operators.\n      if (context.childToWorkMap.containsKey(operator)) {\n        // if we've seen both root and child, we can bail.\n\n        // clear out the mapjoin set. we don't need it anymore.\n        context.currentMapJoinOperators.clear();\n\n        // clear out the union set. we don't need it anymore.\n        context.currentUnionOperators.clear();\n\n        return null;\n      } else {\n        // At this point we don't have to do anything special. Just\n        // run through the regular paces w/o creating a new task.\n        work = context.rootToWorkMap.get(root);\n      }\n    } else {\n      // create a new vertex\n      if (context.preceedingWork == null) {\n        work = utils.createMapWork(context, root, tezWork, null);\n      } else {\n        work = utils.createReduceWork(context, root, tezWork);\n      }\n      context.rootToWorkMap.put(root, work);\n    }\n\n    // this is where we set the sort columns that we will be using for KeyValueInputMerge\n    if (operator instanceof DummyStoreOperator) {\n      work.addSortCols(root.getOpTraits().getSortCols().get(0));\n    }\n\n    if (!context.childToWorkMap.containsKey(operator)) {\n      List<BaseWork> workItems = new LinkedList<BaseWork>();\n      workItems.add(work);\n      context.childToWorkMap.put(operator, workItems);\n    } else {\n      context.childToWorkMap.get(operator).add(work);\n    }\n\n    // this transformation needs to be first because it changes the work item itself.\n    // which can affect the working of all downstream transformations.\n    if (context.currentMergeJoinOperator != null) {\n      // we are currently walking the big table side of the merge join. we need to create or hook up\n      // merge join work.\n      MergeJoinWork mergeJoinWork = null;\n      if (context.opMergeJoinWorkMap.containsKey(context.currentMergeJoinOperator)) {\n        // we have found a merge work corresponding to this closing operator. Hook up this work.\n        mergeJoinWork = context.opMergeJoinWorkMap.get(context.currentMergeJoinOperator);\n      } else {\n        // we need to create the merge join work\n        mergeJoinWork = new MergeJoinWork();\n        mergeJoinWork.setMergeJoinOperator(context.currentMergeJoinOperator);\n        tezWork.add(mergeJoinWork);\n        context.opMergeJoinWorkMap.put(context.currentMergeJoinOperator, mergeJoinWork);\n      }\n      // connect the work correctly.\n      work.addSortCols(root.getOpTraits().getSortCols().get(0));\n      mergeJoinWork.addMergedWork(work, null);\n      Operator<? extends OperatorDesc> parentOp =\n          getParentFromStack(context.currentMergeJoinOperator, stack);\n      int pos = context.currentMergeJoinOperator.getTagForOperator(parentOp);\n      work.setTag(pos);\n      tezWork.setVertexType(work, VertexType.MULTI_INPUT_UNINITIALIZED_EDGES);\n      for (BaseWork parentWork : tezWork.getParents(work)) {\n        TezEdgeProperty edgeProp = tezWork.getEdgeProperty(parentWork, work);\n        tezWork.disconnect(parentWork, work);\n        tezWork.connect(parentWork, mergeJoinWork, edgeProp);\n      }\n\n      for (BaseWork childWork : tezWork.getChildren(work)) {\n        TezEdgeProperty edgeProp = tezWork.getEdgeProperty(work, childWork);\n        tezWork.disconnect(work, childWork);\n        tezWork.connect(mergeJoinWork, childWork, edgeProp);\n      }\n      tezWork.remove(work);\n      context.rootToWorkMap.put(root, mergeJoinWork);\n      context.childToWorkMap.get(operator).remove(work);\n      context.childToWorkMap.get(operator).add(mergeJoinWork);\n      work = mergeJoinWork;\n      context.currentMergeJoinOperator = null;\n    }\n\n    // remember which mapjoin operator links with which work\n    if (!context.currentMapJoinOperators.isEmpty()) {\n      for (MapJoinOperator mj: context.currentMapJoinOperators) {\n        LOG.debug(\"Processing map join: \" + mj);\n        // remember the mapping in case we scan another branch of the\n        // mapjoin later\n        if (!context.mapJoinWorkMap.containsKey(mj)) {\n          List<BaseWork> workItems = new LinkedList<BaseWork>();\n          workItems.add(work);\n          context.mapJoinWorkMap.put(mj, workItems);\n        } else {\n          context.mapJoinWorkMap.get(mj).add(work);\n        }\n\n        /*\n         * this happens in case of map join operations.\n         * The tree looks like this:\n         *\n         *        RS <--- we are here perhaps\n         *        |\n         *     MapJoin\n         *     /     \\\n         *   RS       TS\n         *  /\n         * TS\n         *\n         * If we are at the RS pointed above, and we may have already visited the\n         * RS following the TS, we have already generated work for the TS-RS.\n         * We need to hook the current work to this generated work.\n         */\n        if (context.linkOpWithWorkMap.containsKey(mj)) {\n          Map<BaseWork,TezEdgeProperty> linkWorkMap = context.linkOpWithWorkMap.get(mj);\n          if (linkWorkMap != null) {\n            if (context.linkChildOpWithDummyOp.containsKey(mj)) {\n              for (Operator<?> dummy: context.linkChildOpWithDummyOp.get(mj)) {\n                work.addDummyOp((HashTableDummyOperator) dummy);\n              }\n            }\n            for (Entry<BaseWork,TezEdgeProperty> parentWorkMap : linkWorkMap.entrySet()) {\n              BaseWork parentWork = parentWorkMap.getKey();\n              LOG.debug(\"connecting \"+parentWork.getName()+\" with \"+work.getName());\n              TezEdgeProperty edgeProp = parentWorkMap.getValue();\n              tezWork.connect(parentWork, work, edgeProp);\n              if (edgeProp.getEdgeType() == EdgeType.CUSTOM_EDGE) {\n                tezWork.setVertexType(work, VertexType.INITIALIZED_EDGES);\n              }\n\n              // need to set up output name for reduce sink now that we know the name\n              // of the downstream work\n              for (ReduceSinkOperator r:\n                     context.linkWorkWithReduceSinkMap.get(parentWork)) {\n                if (r.getConf().getOutputName() != null) {\n                  LOG.debug(\"Cloning reduce sink for multi-child broadcast edge\");\n                  // we've already set this one up. Need to clone for the next work.\n                  r = (ReduceSinkOperator) OperatorFactory.getAndMakeChild(\n                      (ReduceSinkDesc)r.getConf().clone(), r.getParentOperators());\n                  context.clonedReduceSinks.add(r);\n                }\n                r.getConf().setOutputName(work.getName());\n                context.connectedReduceSinks.add(r);\n              }\n            }\n          }\n        }\n      }\n      // clear out the set. we don't need it anymore.\n      context.currentMapJoinOperators.clear();\n    }\n\n    if (!context.currentUnionOperators.isEmpty()) {\n      // if there are union all operators we need to add the work to the set\n      // of union operators.\n\n      UnionWork unionWork;\n      if (context.unionWorkMap.containsKey(operator)) {\n        // we've seen this terminal before and have created a union work object.\n        // just need to add this work to it. There will be no children of this one\n        // since we've passed this operator before.\n        assert operator.getChildOperators().isEmpty();\n        unionWork = (UnionWork) context.unionWorkMap.get(operator);\n\n      } else {\n        // first time through. we need to create a union work object and add this\n        // work to it. Subsequent work should reference the union and not the actual\n        // work.\n        unionWork = utils.createUnionWork(context, operator, tezWork);\n      }\n\n      // finally hook everything up\n      LOG.debug(\"Connecting union work (\"+unionWork+\") with work (\"+work+\")\");\n      TezEdgeProperty edgeProp = new TezEdgeProperty(EdgeType.CONTAINS);\n      tezWork.connect(unionWork, work, edgeProp);\n      unionWork.addUnionOperators(context.currentUnionOperators);\n      context.currentUnionOperators.clear();\n      context.workWithUnionOperators.add(work);\n      work = unionWork;\n    }\n\n\n    // This is where we cut the tree as described above. We also remember that\n    // we might have to connect parent work with this work later.\n    boolean removeParents = false;\n    for (Operator<?> parent: new ArrayList<Operator<?>>(root.getParentOperators())) {\n      removeParents = true;\n      context.leafOperatorToFollowingWork.put(parent, work);\n      LOG.debug(\"Removing \" + parent + \" as parent from \" + root);\n    }\n    if (removeParents) {\n      for (Operator<?> parent : new ArrayList<Operator<?>>(root.getParentOperators())) {\n        root.removeParent(parent);\n      }\n    }\n\n    // We're scanning a tree from roots to leaf (this is not technically\n    // correct, demux and mux operators might form a diamond shape, but\n    // we will only scan one path and ignore the others, because the\n    // diamond shape is always contained in a single vertex). The scan\n    // is depth first and because we remove parents when we pack a pipeline\n    // into a vertex we will never visit any node twice. But because of that\n    // we might have a situation where we need to connect 'work' that comes after\n    // the 'work' we're currently looking at.\n    //\n    // Also note: the concept of leaf and root is reversed in hive for historical\n    // reasons. Roots are data sources, leaves are data sinks. I know.\n    if (context.leafOperatorToFollowingWork.containsKey(operator)) {\n\n      BaseWork followingWork = context.leafOperatorToFollowingWork.get(operator);\n      long bytesPerReducer = context.conf.getLongVar(HiveConf.ConfVars.BYTESPERREDUCER);\n\n      LOG.debug(\"Second pass. Leaf operator: \"+operator\n        +\" has common downstream work:\"+followingWork);\n\n      if (operator instanceof DummyStoreOperator) {\n        // this is the small table side.\n        assert (followingWork instanceof MergeJoinWork);\n        MergeJoinWork mergeJoinWork = (MergeJoinWork) followingWork;\n        CommonMergeJoinOperator mergeJoinOp = mergeJoinWork.getMergeJoinOperator();\n        work.setTag(mergeJoinOp.getTagForOperator(operator));\n        mergeJoinWork.addMergedWork(null, work);\n        tezWork.setVertexType(mergeJoinWork, VertexType.MULTI_INPUT_UNINITIALIZED_EDGES);\n        for (BaseWork parentWork : tezWork.getParents(work)) {\n          TezEdgeProperty edgeProp = tezWork.getEdgeProperty(parentWork, work);\n          tezWork.disconnect(parentWork, work);\n          tezWork.connect(parentWork, mergeJoinWork, edgeProp);\n        }\n        work = mergeJoinWork;\n      } else {\n        // need to add this branch to the key + value info\n        assert operator instanceof ReduceSinkOperator\n            && ((followingWork instanceof ReduceWork) || (followingWork instanceof MergeJoinWork)\n                || followingWork instanceof UnionWork);\n        ReduceSinkOperator rs = (ReduceSinkOperator) operator;\n        ReduceWork rWork = null;\n        if (followingWork instanceof MergeJoinWork) {\n          MergeJoinWork mergeJoinWork = (MergeJoinWork) followingWork;\n          rWork = (ReduceWork) mergeJoinWork.getMainWork();\n        } else if (followingWork instanceof UnionWork) {\n          // this can only be possible if there is merge work followed by the union\n          UnionWork unionWork = (UnionWork) followingWork;\n          int index = getFollowingWorkIndex(tezWork, unionWork, rs);\n          if (index != -1) {\n            BaseWork baseWork = tezWork.getChildren(unionWork).get(index);\n            if (baseWork instanceof MergeJoinWork) {\n              MergeJoinWork mergeJoinWork = (MergeJoinWork) baseWork;\n              // disconnect the connection to union work and connect to merge work\n              followingWork = mergeJoinWork;\n              rWork = (ReduceWork) mergeJoinWork.getMainWork();\n            } else {\n              rWork = (ReduceWork) baseWork;\n            }\n          } else {\n            throw new SemanticException(\"Following work not found for the reduce sink: \"\n                + rs.getName());\n          }\n        } else {\n          rWork = (ReduceWork) followingWork;\n        }\n        GenMapRedUtils.setKeyAndValueDesc(rWork, rs);\n\n        // remember which parent belongs to which tag\n        int tag = rs.getConf().getTag();\n        rWork.getTagToInput().put(tag == -1 ? 0 : tag, work.getName());\n\n        // remember the output name of the reduce sink\n        rs.getConf().setOutputName(rWork.getName());\n\n        if (!context.connectedReduceSinks.contains(rs)) {\n          // add dependency between the two work items\n          TezEdgeProperty edgeProp;\n          if (rWork.isAutoReduceParallelism()) {\n            edgeProp =\n                new TezEdgeProperty(context.conf, EdgeType.SIMPLE_EDGE, true,\n                    rWork.getMinReduceTasks(), rWork.getMaxReduceTasks(), bytesPerReducer);\n          } else {\n            edgeProp = new TezEdgeProperty(EdgeType.SIMPLE_EDGE);\n          }\n          tezWork.connect(work, followingWork, edgeProp);\n          context.connectedReduceSinks.add(rs);\n        }\n      }\n    } else {\n      LOG.debug(\"First pass. Leaf operator: \"+operator);\n    }\n\n    // No children means we're at the bottom. If there are more operators to scan\n    // the next item will be a new root.\n    if (!operator.getChildOperators().isEmpty()) {\n      assert operator.getChildOperators().size() == 1;\n      context.parentOfRoot = operator;\n      context.currentRootOperator = operator.getChildOperators().get(0);\n      context.preceedingWork = work;\n    }\n\n    return null;\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report precisely identifies the root cause by mentioning the 'IndexOutOfBoundsException' error and the method 'org.apache.hadoop.hive.ql.parse.GenTezWork.process' where the error occurred, which matches the ground truth method. However, the bug report does not provide any fix suggestion, hence it is marked as 'Missing' for fix suggestion. The problem location is also precisely identified as it directly mentions the method from the ground truth list. There is no wrong information in the bug report as all the details provided are relevant and accurate."
        }
    },
    {
        "filename": "HIVE-13856.json",
        "code_diff": {
            "metastore.src.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.checkLock": {
                "code_before_change": "  public LockResponse checkLock(CheckLockRequest rqst)\n    throws NoSuchTxnException, NoSuchLockException, TxnAbortedException, MetaException {\n    try {\n      Connection dbConn = null;\n      long extLockId = rqst.getLockid();\n      try {\n        lockInternal();\n        dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);\n        // Heartbeat on the lockid first, to assure that our lock is still valid.\n        // Then look up the lock info (hopefully in the cache).  If these locks\n        // are associated with a transaction then heartbeat on that as well.\n        LockInfo info = getTxnIdFromLockId(dbConn, extLockId);\n        if(info == null) {\n          throw new NoSuchLockException(\"No such lock \" + JavaUtils.lockIdToString(extLockId));\n        }\n        if (info.txnId > 0) {\n          heartbeatTxn(dbConn, info.txnId);\n        }\n        else {\n          heartbeatLock(dbConn, extLockId);\n        }\n        //todo: strictly speaking there is a bug here.  heartbeat*() commits but both heartbeat and\n        //checkLock() are in the same retry block, so if checkLock() throws, heartbeat is also retired\n        //extra heartbeat is logically harmless, but ...\n        return checkLock(dbConn, extLockId);\n      } catch (SQLException e) {\n        LOG.debug(\"Going to rollback\");\n        rollbackDBConn(dbConn);\n        checkRetryable(dbConn, e, \"checkLock(\" + rqst + \" )\");\n        throw new MetaException(\"Unable to update transaction database \" +\n          JavaUtils.lockIdToString(extLockId) + \" \" + StringUtils.stringifyException(e));\n      } finally {\n        closeDbConn(dbConn);\n        unlockInternal();\n      }\n    } catch (RetryException e) {\n      return checkLock(rqst);\n    }\n\n  }",
                "code_after_change": "  public LockResponse checkLock(CheckLockRequest rqst)\n    throws NoSuchTxnException, NoSuchLockException, TxnAbortedException, MetaException {\n    try {\n      Connection dbConn = null;\n      long extLockId = rqst.getLockid();\n      try {\n        lockInternal();\n        dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);\n        // Heartbeat on the lockid first, to assure that our lock is still valid.\n        // Then look up the lock info (hopefully in the cache).  If these locks\n        // are associated with a transaction then heartbeat on that as well.\n        LockInfo info = getTxnIdFromLockId(dbConn, extLockId);\n        if(info == null) {\n          throw new NoSuchLockException(\"No such lock \" + JavaUtils.lockIdToString(extLockId));\n        }\n        if (info.txnId > 0) {\n          heartbeatTxn(dbConn, info.txnId);\n        }\n        else {\n          heartbeatLock(dbConn, extLockId);\n        }\n        //todo: strictly speaking there is a bug here.  heartbeat*() commits but both heartbeat and\n        //checkLock() are in the same retry block, so if checkLock() throws, heartbeat is also retired\n        //extra heartbeat is logically harmless, but ...\n        return checkLock(dbConn, extLockId);\n      } catch (SQLException e) {\n        LOG.debug(\"Going to rollback\");\n        rollbackDBConn(dbConn);\n        checkRetryable(dbConn, e, \"checkLock(\" + rqst + \" )\");\n        throw new MetaException(\"Unable to update transaction database \" +\n          JavaUtils.lockIdToString(extLockId) + \" \" + StringUtils.stringifyException(e));\n      } finally {\n        closeDbConn(dbConn);\n        unlockInternal();\n      }\n    } catch (RetryException e) {\n      return checkLock(rqst);\n    }\n\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.openTxns": {
                "code_before_change": "  public OpenTxnsResponse openTxns(OpenTxnRequest rqst) throws MetaException {\n    if (openTxnsCounter == null) {\n      synchronized (TxnHandler.class) {\n        try {\n          if (openTxnsCounter == null) {\n            startHouseKeeperService(conf, Class.forName(\"org.apache.hadoop.hive.ql.txn.AcidOpenTxnsCounterService\"));\n          }\n        } catch (ClassNotFoundException e) {\n          throw new MetaException(e.getMessage());\n        }\n      }\n    }\n\n    if (!tooManyOpenTxns && numOpenTxns >= maxOpenTxns) {\n      tooManyOpenTxns = true;\n    }\n    if (tooManyOpenTxns) {\n      if (numOpenTxns < maxOpenTxns * 0.9) {\n        tooManyOpenTxns = false;\n      } else {\n        LOG.warn(\"Maximum allowed number of open transactions (\" + maxOpenTxns + \") has been \" +\n            \"reached. Current number of open transactions: \" + numOpenTxns);\n        throw new MetaException(\"Maximum allowed number of open transactions has been reached. \" +\n            \"See hive.max.open.txns.\");\n      }\n    }\n\n    int numTxns = rqst.getNum_txns();\n    try {\n      Connection dbConn = null;\n      Statement stmt = null;\n      ResultSet rs = null;\n      try {\n        lockInternal();\n        /**\n         * To make {@link #getOpenTxns()}/{@link #getOpenTxnsInfo()} work correctly, this operation must ensure\n         * that advancing the counter in NEXT_TXN_ID and adding appropriate entries to TXNS is atomic.\n         * Also, advancing the counter must work when multiple metastores are running.\n         * SELECT ... FOR UPDATE is used to prevent\n         * concurrent DB transactions being rolled back due to Write-Write conflict on NEXT_TXN_ID.\n         *\n         * In the current design, there can be several metastore instances running in a given Warehouse.\n         * This makes ideas like reserving a range of IDs to save trips to DB impossible.  For example,\n         * a client may go to MS1 and start a transaction with ID 500 to update a particular row.\n         * Now the same client will start another transaction, except it ends up on MS2 and may get\n         * transaction ID 400 and update the same row.  Now the merge that happens to materialize the snapshot\n         * on read will thing the version of the row from transaction ID 500 is the latest one.\n         *\n         * Longer term we can consider running Active-Passive MS (at least wrt to ACID operations).  This\n         * set could support a write-through cache for added performance.\n         */\n        dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);\n        // Make sure the user has not requested an insane amount of txns.\n        int maxTxns = HiveConf.getIntVar(conf,\n          HiveConf.ConfVars.HIVE_TXN_MAX_OPEN_BATCH);\n        if (numTxns > maxTxns) numTxns = maxTxns;\n\n        stmt = dbConn.createStatement();\n        String s = addForUpdateClause(\"select ntxn_next from NEXT_TXN_ID\");\n        LOG.debug(\"Going to execute query <\" + s + \">\");\n        rs = stmt.executeQuery(s);\n        if (!rs.next()) {\n          throw new MetaException(\"Transaction database not properly \" +\n            \"configured, can't find next transaction id.\");\n        }\n        long first = rs.getLong(1);\n        s = \"update NEXT_TXN_ID set ntxn_next = \" + (first + numTxns);\n        LOG.debug(\"Going to execute update <\" + s + \">\");\n        stmt.executeUpdate(s);\n\n        long now = getDbTime(dbConn);\n        List<Long> txnIds = new ArrayList<Long>(numTxns);\n        ArrayList<String> queries = new ArrayList<String>();\n        String query;\n        String insertClause = \"insert into TXNS (txn_id, txn_state, txn_started, txn_last_heartbeat, txn_user, txn_host) values \";\n        StringBuilder valuesClause = new StringBuilder();\n\n        for (long i = first; i < first + numTxns; i++) {\n          txnIds.add(i);\n\n          if (i > first &&\n              (i - first) % conf.getIntVar(HiveConf.ConfVars.METASTORE_DIRECT_SQL_MAX_ELEMENTS_VALUES_CLAUSE) == 0) {\n            // wrap up the current query, and start a new one\n            query = insertClause + valuesClause.toString();\n            queries.add(query);\n\n            valuesClause.setLength(0);\n            valuesClause.append(\"(\").append(i).append(\", 'o', \").append(now).append(\", \").append(now)\n                .append(\", '\").append(rqst.getUser()).append(\"', '\").append(rqst.getHostname())\n                .append(\"')\");\n\n            continue;\n          }\n\n          if (i > first) {\n            valuesClause.append(\", \");\n          }\n\n          valuesClause.append(\"(\").append(i).append(\", 'o', \").append(now).append(\", \").append(now)\n              .append(\", '\").append(rqst.getUser()).append(\"', '\").append(rqst.getHostname())\n              .append(\"')\");\n        }\n\n        query = insertClause + valuesClause.toString();\n        queries.add(query);\n\n        for (String q : queries) {\n          LOG.debug(\"Going to execute update <\" + q + \">\");\n          stmt.execute(q);\n        }\n        LOG.debug(\"Going to commit\");\n        dbConn.commit();\n        return new OpenTxnsResponse(txnIds);\n      } catch (SQLException e) {\n        LOG.debug(\"Going to rollback\");\n        rollbackDBConn(dbConn);\n        checkRetryable(dbConn, e, \"openTxns(\" + rqst + \")\");\n        throw new MetaException(\"Unable to select from transaction database \"\n          + StringUtils.stringifyException(e));\n      } finally {\n        close(rs, stmt, dbConn);\n        unlockInternal();\n      }\n    } catch (RetryException e) {\n      return openTxns(rqst);\n    }\n  }",
                "code_after_change": "  public OpenTxnsResponse openTxns(OpenTxnRequest rqst) throws MetaException {\n    if (openTxnsCounter == null) {\n      synchronized (TxnHandler.class) {\n        try {\n          if (openTxnsCounter == null) {\n            startHouseKeeperService(conf, Class.forName(\"org.apache.hadoop.hive.ql.txn.AcidOpenTxnsCounterService\"));\n          }\n        } catch (ClassNotFoundException e) {\n          throw new MetaException(e.getMessage());\n        }\n      }\n    }\n    if (!tooManyOpenTxns && numOpenTxns >= maxOpenTxns) {\n      tooManyOpenTxns = true;\n    }\n    if (tooManyOpenTxns) {\n      if (numOpenTxns < maxOpenTxns * 0.9) {\n        tooManyOpenTxns = false;\n      } else {\n        LOG.warn(\"Maximum allowed number of open transactions (\" + maxOpenTxns + \") has been \" +\n            \"reached. Current number of open transactions: \" + numOpenTxns);\n        throw new MetaException(\"Maximum allowed number of open transactions has been reached. \" +\n            \"See hive.max.open.txns.\");\n      }\n    }\n\n    int numTxns = rqst.getNum_txns();\n    try {\n      Connection dbConn = null;\n      Statement stmt = null;\n      ResultSet rs = null;\n      try {\n        lockInternal();\n        /**\n         * To make {@link #getOpenTxns()}/{@link #getOpenTxnsInfo()} work correctly, this operation must ensure\n         * that advancing the counter in NEXT_TXN_ID and adding appropriate entries to TXNS is atomic.\n         * Also, advancing the counter must work when multiple metastores are running.\n         * SELECT ... FOR UPDATE is used to prevent\n         * concurrent DB transactions being rolled back due to Write-Write conflict on NEXT_TXN_ID.\n         *\n         * In the current design, there can be several metastore instances running in a given Warehouse.\n         * This makes ideas like reserving a range of IDs to save trips to DB impossible.  For example,\n         * a client may go to MS1 and start a transaction with ID 500 to update a particular row.\n         * Now the same client will start another transaction, except it ends up on MS2 and may get\n         * transaction ID 400 and update the same row.  Now the merge that happens to materialize the snapshot\n         * on read will thing the version of the row from transaction ID 500 is the latest one.\n         *\n         * Longer term we can consider running Active-Passive MS (at least wrt to ACID operations).  This\n         * set could support a write-through cache for added performance.\n         */\n        dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);\n        // Make sure the user has not requested an insane amount of txns.\n        int maxTxns = HiveConf.getIntVar(conf,\n          HiveConf.ConfVars.HIVE_TXN_MAX_OPEN_BATCH);\n        if (numTxns > maxTxns) numTxns = maxTxns;\n\n        stmt = dbConn.createStatement();\n        String s = sqlGenerator.addForUpdateClause(\"select ntxn_next from NEXT_TXN_ID\");\n        LOG.debug(\"Going to execute query <\" + s + \">\");\n        rs = stmt.executeQuery(s);\n        if (!rs.next()) {\n          throw new MetaException(\"Transaction database not properly \" +\n            \"configured, can't find next transaction id.\");\n        }\n        long first = rs.getLong(1);\n        s = \"update NEXT_TXN_ID set ntxn_next = \" + (first + numTxns);\n        LOG.debug(\"Going to execute update <\" + s + \">\");\n        stmt.executeUpdate(s);\n\n        long now = getDbTime(dbConn);\n        List<Long> txnIds = new ArrayList<Long>(numTxns);\n\n        List<String> rows = new ArrayList<>();\n        for (long i = first; i < first + numTxns; i++) {\n          txnIds.add(i);\n          rows.add(i + \",\" + quoteChar(TXN_OPEN) + \",\" + now + \",\" + now + \",\" + quoteString(rqst.getUser()) + \",\" + quoteString(rqst.getHostname()));\n        }\n        List<String> queries = sqlGenerator.createInsertValuesStmt(\n          \"TXNS (txn_id, txn_state, txn_started, txn_last_heartbeat, txn_user, txn_host)\", rows);\n        for (String q : queries) {\n          LOG.debug(\"Going to execute update <\" + q + \">\");\n          stmt.execute(q);\n        }\n        LOG.debug(\"Going to commit\");\n        dbConn.commit();\n        return new OpenTxnsResponse(txnIds);\n      } catch (SQLException e) {\n        LOG.debug(\"Going to rollback\");\n        rollbackDBConn(dbConn);\n        checkRetryable(dbConn, e, \"openTxns(\" + rqst + \")\");\n        throw new MetaException(\"Unable to select from transaction database \"\n          + StringUtils.stringifyException(e));\n      } finally {\n        close(rs, stmt, dbConn);\n        unlockInternal();\n      }\n    } catch (RetryException e) {\n      return openTxns(rqst);\n    }\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.heartbeatTxnRange": {
                "code_before_change": "  public HeartbeatTxnRangeResponse heartbeatTxnRange(HeartbeatTxnRangeRequest rqst)\n    throws MetaException {\n    try {\n      Connection dbConn = null;\n      HeartbeatTxnRangeResponse rsp = new HeartbeatTxnRangeResponse();\n      Set<Long> nosuch = new HashSet<Long>();\n      Set<Long> aborted = new HashSet<Long>();\n      rsp.setNosuch(nosuch);\n      rsp.setAborted(aborted);\n      try {\n        /**\n         * READ_COMMITTED is sufficient since {@link #heartbeatTxn(java.sql.Connection, long)}\n         * only has 1 update statement in it and\n         * we only update existing txns, i.e. nothing can add additional txns that this operation\n         * would care about (which would have required SERIALIZABLE)\n         */\n        dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);\n        for (long txn = rqst.getMin(); txn <= rqst.getMax(); txn++) {\n          try {\n            //todo: do all updates in 1 SQL statement and check update count\n            //if update count is less than was requested, go into more expensive checks\n            //for each txn\n            heartbeatTxn(dbConn, txn);\n          } catch (NoSuchTxnException e) {\n            nosuch.add(txn);\n          } catch (TxnAbortedException e) {\n            aborted.add(txn);\n          }\n        }\n        return rsp;\n      } catch (SQLException e) {\n        LOG.debug(\"Going to rollback\");\n        rollbackDBConn(dbConn);\n        checkRetryable(dbConn, e, \"heartbeatTxnRange(\" + rqst + \")\");\n        throw new MetaException(\"Unable to select from transaction database \" +\n          StringUtils.stringifyException(e));\n      } finally {\n        closeDbConn(dbConn);\n      }\n    } catch (RetryException e) {\n      return heartbeatTxnRange(rqst);\n    }\n  }",
                "code_after_change": "  public HeartbeatTxnRangeResponse heartbeatTxnRange(HeartbeatTxnRangeRequest rqst)\n    throws MetaException {\n    try {\n      Connection dbConn = null;\n      HeartbeatTxnRangeResponse rsp = new HeartbeatTxnRangeResponse();\n      Set<Long> nosuch = new HashSet<Long>();\n      Set<Long> aborted = new HashSet<Long>();\n      rsp.setNosuch(nosuch);\n      rsp.setAborted(aborted);\n      try {\n        /**\n         * READ_COMMITTED is sufficient since {@link #heartbeatTxn(java.sql.Connection, long)}\n         * only has 1 update statement in it and\n         * we only update existing txns, i.e. nothing can add additional txns that this operation\n         * would care about (which would have required SERIALIZABLE)\n         */\n        dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);\n        for (long txn = rqst.getMin(); txn <= rqst.getMax(); txn++) {\n          try {\n            //todo: do all updates in 1 SQL statement and check update count\n            //if update count is less than was requested, go into more expensive checks\n            //for each txn\n            heartbeatTxn(dbConn, txn);\n          } catch (NoSuchTxnException e) {\n            nosuch.add(txn);\n          } catch (TxnAbortedException e) {\n            aborted.add(txn);\n          }\n        }\n        return rsp;\n      } catch (SQLException e) {\n        LOG.debug(\"Going to rollback\");\n        rollbackDBConn(dbConn);\n        checkRetryable(dbConn, e, \"heartbeatTxnRange(\" + rqst + \")\");\n        throw new MetaException(\"Unable to select from transaction database \" +\n          StringUtils.stringifyException(e));\n      } finally {\n        closeDbConn(dbConn);\n      }\n    } catch (RetryException e) {\n      return heartbeatTxnRange(rqst);\n    }\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.releaseLocks": {
                "code_before_change": "    public void releaseLocks() {\n      rollbackDBConn(dbConn);\n      close(rs, stmt, dbConn);\n      if(derbySemaphore != null) {\n        derbySemaphore.release();\n      }\n      for(String key : keys) {\n        LOG.info(quoteString(key) + \" unlocked by \" + quoteString(TxnHandler.hostname));\n      }\n    }",
                "code_after_change": "    public void releaseLocks() {\n      rollbackDBConn(dbConn);\n      close(rs, stmt, dbConn);\n      if(derbySemaphore != null) {\n        derbySemaphore.release();\n      }\n      for(String key : keys) {\n        LOG.info(quoteString(key) + \" unlocked by \" + quoteString(TxnHandler.hostname));\n      }\n    }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.timeOutLocks": {
                "code_before_change": "  private void timeOutLocks(Connection dbConn, long now) {\n    Statement stmt = null;\n    ResultSet rs = null;\n    try {\n      stmt = dbConn.createStatement();\n      long maxHeartbeatTime = now - timeout;\n      //doing a SELECT first is less efficient but makes it easier to debug things\n      String s = \"select distinct hl_lock_ext_id from HIVE_LOCKS where hl_last_heartbeat < \" +\n        maxHeartbeatTime + \" and hl_txnid = 0\";//when txnid is <> 0, the lock is\n      //associated with a txn and is handled by performTimeOuts()\n      //want to avoid expiring locks for a txn w/o expiring the txn itself\n      List<Long> extLockIDs = new ArrayList<>();\n      rs = stmt.executeQuery(s);\n      while(rs.next()) {\n        extLockIDs.add(rs.getLong(1));\n      }\n      rs.close();\n      dbConn.commit();\n      if(extLockIDs.size() <= 0) {\n        return;\n      }\n\n      List<String> queries = new ArrayList<String>();\n\n      StringBuilder prefix = new StringBuilder();\n      StringBuilder suffix = new StringBuilder();\n\n      //include same hl_last_heartbeat condition in case someone heartbeated since the select\n      prefix.append(\"delete from HIVE_LOCKS where hl_last_heartbeat < \");\n      prefix.append(maxHeartbeatTime);\n      prefix.append(\" and hl_txnid = 0 and \");\n      suffix.append(\"\");\n\n      TxnUtils.buildQueryWithINClause(conf, queries, prefix, suffix, extLockIDs, \"hl_lock_ext_id\", true, false);\n\n      int deletedLocks = 0;\n      for (String query : queries) {\n        LOG.debug(\"Removing expired locks via: \" + query);\n        deletedLocks += stmt.executeUpdate(query);\n      }\n      if(deletedLocks > 0) {\n        Collections.sort(extLockIDs);////easier to read logs\n        LOG.info(\"Deleted \" + deletedLocks + \" ext locks from HIVE_LOCKS due to timeout (vs. \" +\n            extLockIDs.size() + \" found. List: \" + extLockIDs + \") maxHeartbeatTime=\" + maxHeartbeatTime);\n      }\n      LOG.debug(\"Going to commit\");\n      dbConn.commit();\n    }\n    catch(SQLException ex) {\n      LOG.error(\"Failed to purge timedout locks due to: \" + getMessage(ex), ex);\n    }\n    catch(Exception ex) {\n      LOG.error(\"Failed to purge timedout locks due to: \" + ex.getMessage(), ex);\n    } finally {\n      close(rs);\n      closeStmt(stmt);\n    }\n  }",
                "code_after_change": "  private void timeOutLocks(Connection dbConn, long now) {\n    Statement stmt = null;\n    ResultSet rs = null;\n    try {\n      stmt = dbConn.createStatement();\n      long maxHeartbeatTime = now - timeout;\n      //doing a SELECT first is less efficient but makes it easier to debug things\n      String s = \"select distinct hl_lock_ext_id from HIVE_LOCKS where hl_last_heartbeat < \" +\n        maxHeartbeatTime + \" and hl_txnid = 0\";//when txnid is <> 0, the lock is\n      //associated with a txn and is handled by performTimeOuts()\n      //want to avoid expiring locks for a txn w/o expiring the txn itself\n      List<Long> extLockIDs = new ArrayList<>();\n      rs = stmt.executeQuery(s);\n      while(rs.next()) {\n        extLockIDs.add(rs.getLong(1));\n      }\n      rs.close();\n      dbConn.commit();\n      if(extLockIDs.size() <= 0) {\n        return;\n      }\n\n      List<String> queries = new ArrayList<String>();\n\n      StringBuilder prefix = new StringBuilder();\n      StringBuilder suffix = new StringBuilder();\n\n      //include same hl_last_heartbeat condition in case someone heartbeated since the select\n      prefix.append(\"delete from HIVE_LOCKS where hl_last_heartbeat < \");\n      prefix.append(maxHeartbeatTime);\n      prefix.append(\" and hl_txnid = 0 and \");\n      suffix.append(\"\");\n\n      TxnUtils.buildQueryWithINClause(conf, queries, prefix, suffix, extLockIDs, \"hl_lock_ext_id\", true, false);\n\n      int deletedLocks = 0;\n      for (String query : queries) {\n        LOG.debug(\"Removing expired locks via: \" + query);\n        deletedLocks += stmt.executeUpdate(query);\n      }\n      if(deletedLocks > 0) {\n        Collections.sort(extLockIDs);////easier to read logs\n        LOG.info(\"Deleted \" + deletedLocks + \" ext locks from HIVE_LOCKS due to timeout (vs. \" +\n            extLockIDs.size() + \" found. List: \" + extLockIDs + \") maxHeartbeatTime=\" + maxHeartbeatTime);\n      }\n      LOG.debug(\"Going to commit\");\n      dbConn.commit();\n    }\n    catch(SQLException ex) {\n      LOG.error(\"Failed to purge timedout locks due to: \" + getMessage(ex), ex);\n    }\n    catch(Exception ex) {\n      LOG.error(\"Failed to purge timedout locks due to: \" + ex.getMessage(), ex);\n    } finally {\n      close(rs);\n      closeStmt(stmt);\n    }\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.ConnectionLockIdPair": {
                "code_before_change": [],
                "code_after_change": []
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.acquireLock": {
                "code_before_change": "  public LockHandle acquireLock(String key) throws MetaException {\n    /**\n     * The implementation here is a bit kludgey but done so that code exercised by unit tests\n     * (which run against Derby which has no support for select for update) is as similar to\n     * production code as possible.\n     * In particular, with Derby we always run in a single process with a single metastore and\n     * the absence of For Update is handled via a Semaphore.  The later would strictly speaking\n     * make the SQL statments below unnecessary (for Derby), but then they would not be tested.\n     */\n    Connection dbConn = null;\n    Statement stmt = null;\n    ResultSet rs = null;\n    try {\n      try {\n        String sqlStmt = addForUpdateClause(\"select MT_COMMENT from AUX_TABLE where MT_KEY1=\" + quoteString(key) + \" and MT_KEY2=0\");\n        lockInternal();\n        dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);\n        stmt = dbConn.createStatement();\n        if(LOG.isDebugEnabled()) {\n          LOG.debug(\"About to execute SQL: \" + sqlStmt);\n        }\n        rs = stmt.executeQuery(sqlStmt);\n        if (!rs.next()) {\n          close(rs);\n          try {\n            stmt.executeUpdate(\"insert into AUX_TABLE(MT_KEY1,MT_KEY2) values(\" + quoteString(key) + \", 0)\");\n            dbConn.commit();\n          } catch (SQLException ex) {\n            if (!isDuplicateKeyError(ex)) {\n              throw new RuntimeException(\"Unable to lock \" + quoteString(key) + \" due to: \" + getMessage(ex), ex);\n            }\n          }\n          rs = stmt.executeQuery(sqlStmt);\n          if (!rs.next()) {\n            throw new IllegalStateException(\"Unable to lock \" + quoteString(key) + \".  Expected row in AUX_TABLE is missing.\");\n          }\n        }\n        Semaphore derbySemaphore = null;\n        if(dbProduct == DatabaseProduct.DERBY) {\n          derbyKey2Lock.putIfAbsent(key, new Semaphore(1));\n          derbySemaphore =  derbyKey2Lock.get(key);\n          derbySemaphore.acquire();\n        }\n        LOG.info(quoteString(key) + \" locked by \" + quoteString(TxnHandler.hostname));\n        //OK, so now we have a lock\n        return new LockHandleImpl(dbConn, stmt, rs, key, derbySemaphore);\n      } catch (SQLException ex) {\n        rollbackDBConn(dbConn);\n        close(rs, stmt, dbConn);\n        checkRetryable(dbConn, ex, \"acquireLock(\" + key + \")\");\n        throw new MetaException(\"Unable to lock \" + quoteString(key) + \" due to: \" + getMessage(ex) + \"; \" + StringUtils.stringifyException(ex));\n      }\n      catch(InterruptedException ex) {\n        rollbackDBConn(dbConn);\n        close(rs, stmt, dbConn);\n        throw new MetaException(\"Unable to lock \" + quoteString(key) + \" due to: \" + ex.getMessage() + StringUtils.stringifyException(ex));\n      }\n      finally {\n        unlockInternal();\n      }\n    }\n    catch(RetryException ex) {\n      acquireLock(key);\n    }\n    throw new MetaException(\"This can't happen because checkRetryable() has a retry limit\");\n  }",
                "code_after_change": "  public LockHandle acquireLock(String key) throws MetaException {\n    /**\n     * The implementation here is a bit kludgey but done so that code exercised by unit tests\n     * (which run against Derby which has no support for select for update) is as similar to\n     * production code as possible.\n     * In particular, with Derby we always run in a single process with a single metastore and\n     * the absence of For Update is handled via a Semaphore.  The later would strictly speaking\n     * make the SQL statments below unnecessary (for Derby), but then they would not be tested.\n     */\n    Connection dbConn = null;\n    Statement stmt = null;\n    ResultSet rs = null;\n    try {\n      try {\n        String sqlStmt = sqlGenerator.addForUpdateClause(\"select MT_COMMENT from AUX_TABLE where MT_KEY1=\" + quoteString(key) + \" and MT_KEY2=0\");\n        lockInternal();\n        dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);\n        stmt = dbConn.createStatement();\n        if(LOG.isDebugEnabled()) {\n          LOG.debug(\"About to execute SQL: \" + sqlStmt);\n        }\n        rs = stmt.executeQuery(sqlStmt);\n        if (!rs.next()) {\n          close(rs);\n          try {\n            stmt.executeUpdate(\"insert into AUX_TABLE(MT_KEY1,MT_KEY2) values(\" + quoteString(key) + \", 0)\");\n            dbConn.commit();\n          } catch (SQLException ex) {\n            if (!isDuplicateKeyError(ex)) {\n              throw new RuntimeException(\"Unable to lock \" + quoteString(key) + \" due to: \" + getMessage(ex), ex);\n            }\n          }\n          rs = stmt.executeQuery(sqlStmt);\n          if (!rs.next()) {\n            throw new IllegalStateException(\"Unable to lock \" + quoteString(key) + \".  Expected row in AUX_TABLE is missing.\");\n          }\n        }\n        Semaphore derbySemaphore = null;\n        if(dbProduct == DatabaseProduct.DERBY) {\n          derbyKey2Lock.putIfAbsent(key, new Semaphore(1));\n          derbySemaphore =  derbyKey2Lock.get(key);\n          derbySemaphore.acquire();\n        }\n        LOG.info(quoteString(key) + \" locked by \" + quoteString(TxnHandler.hostname));\n        //OK, so now we have a lock\n        return new LockHandleImpl(dbConn, stmt, rs, key, derbySemaphore);\n      } catch (SQLException ex) {\n        rollbackDBConn(dbConn);\n        close(rs, stmt, dbConn);\n        checkRetryable(dbConn, ex, \"acquireLock(\" + key + \")\");\n        throw new MetaException(\"Unable to lock \" + quoteString(key) + \" due to: \" + getMessage(ex) + \"; \" + StringUtils.stringifyException(ex));\n      }\n      catch(InterruptedException ex) {\n        rollbackDBConn(dbConn);\n        close(rs, stmt, dbConn);\n        throw new MetaException(\"Unable to lock \" + quoteString(key) + \" due to: \" + ex.getMessage() + StringUtils.stringifyException(ex));\n      }\n      finally {\n        unlockInternal();\n      }\n    }\n    catch(RetryException ex) {\n      acquireLock(key);\n    }\n    throw new MetaException(\"This can't happen because checkRetryable() has a retry limit\");\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.commitTxn": {
                "code_before_change": "  public void commitTxn(CommitTxnRequest rqst)\n    throws NoSuchTxnException, TxnAbortedException,  MetaException {\n    long txnid = rqst.getTxnid();\n    try {\n      Connection dbConn = null;\n      Statement stmt = null;\n      ResultSet lockHandle = null;\n      ResultSet commitIdRs = null, rs;\n      try {\n        lockInternal();\n        dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);\n        stmt = dbConn.createStatement();\n        /**\n         * This S4U will mutex with other commitTxn() and openTxns(). \n         * -1 below makes txn intervals look like [3,3] [4,4] if all txns are serial\n         * Note: it's possible to have several txns have the same commit id.  Suppose 3 txns start\n         * at the same time and no new txns start until all 3 commit.\n         * We could've incremented the sequence for commitId is well but it doesn't add anything functionally.\n         */\n        commitIdRs = stmt.executeQuery(addForUpdateClause(\"select ntxn_next - 1 from NEXT_TXN_ID\"));\n        if(!commitIdRs.next()) {\n          throw new IllegalStateException(\"No rows found in NEXT_TXN_ID\");\n        }\n        long commitId = commitIdRs.getLong(1);\n        /**\n         * Runs at READ_COMMITTED with S4U on TXNS row for \"txnid\".  S4U ensures that no other\n         * operation can change this txn (such acquiring locks). While lock() and commitTxn()\n         * should not normally run concurrently (for same txn) but could due to bugs in the client\n         * which could then corrupt internal transaction manager state.  Also competes with abortTxn().\n         */\n        lockHandle = lockTransactionRecord(stmt, txnid, TXN_OPEN);\n        if(lockHandle == null) {\n          //this also ensures that txn is still there and in expected state (hasn't been timed out)\n          ensureValidTxn(dbConn, txnid, stmt);\n          shouldNeverHappen(txnid);\n        }\n        Savepoint undoWriteSetForCurrentTxn = dbConn.setSavepoint();\n        int numCompsWritten = stmt.executeUpdate(\"insert into WRITE_SET (ws_database, ws_table, ws_partition, ws_txnid, ws_commit_id, ws_operation_type)\" +\n          \" select tc_database, tc_table, tc_partition, tc_txnid, \" + commitId + \", tc_operation_type \" +\n          \"from TXN_COMPONENTS where tc_txnid=\" + txnid + \" and tc_operation_type IN(\" + quoteChar(OpertaionType.UPDATE.sqlConst) + \",\" + quoteChar(OpertaionType.DELETE.sqlConst) + \")\");\n        if(numCompsWritten == 0) {\n          /**\n           * current txn didn't update/delete anything (may have inserted), so just proceed with commit\n           * \n           * We only care about commit id for write txns, so for RO (when supported) txns we don't\n           * have to mutex on NEXT_TXN_ID.\n           * Consider: if RO txn is after a W txn, then RO's openTxns() will be mutexed with W's\n           * commitTxn() because both do S4U on NEXT_TXN_ID and thus RO will see result of W txn.\n           * If RO < W, then there is no reads-from relationship.\n           */\n        }\n        else {\n          /**\n           * see if there are any overlapping txns wrote the same element, i.e. have a conflict\n           * Since entire commit operation is mutexed wrt other start/commit ops,\n           * committed.ws_commit_id <= current.ws_commit_id for all txns\n           * thus if committed.ws_commit_id < current.ws_txnid, transactions do NOT overlap\n           * For example, [17,20] is committed, [6,80] is being committed right now - these overlap\n           * [17,20] committed and [21,21] committing now - these do not overlap.\n           * [17,18] committed and [18,19] committing now - these overlap  (here 18 started while 17 was still running)\n           */\n          rs = stmt.executeQuery\n            (addLimitClause(1, \"committed.ws_txnid, committed.ws_commit_id, committed.ws_database,\" +\n              \"committed.ws_table, committed.ws_partition, cur.ws_commit_id \" + \n              \"from WRITE_SET committed INNER JOIN WRITE_SET cur \" +\n            \"ON committed.ws_database=cur.ws_database and committed.ws_table=cur.ws_table \" +\n              //For partitioned table we always track writes at partition level (never at table)\n              //and for non partitioned - always at table level, thus the same table should never\n              //have entries with partition key and w/o\n            \"and (committed.ws_partition=cur.ws_partition or (committed.ws_partition is null and cur.ws_partition is null)) \" +\n            \"where cur.ws_txnid <= committed.ws_commit_id\" + //txns overlap; could replace ws_txnid\n              // with txnid, though any decent DB should infer this\n            \" and cur.ws_txnid=\" + txnid + //make sure RHS of join only has rows we just inserted as\n              // part of this commitTxn() op\n            \" and committed.ws_txnid <> \" + txnid + //and LHS only has committed txns\n              //U+U and U+D is a conflict but D+D is not and we don't currently track I in WRITE_SET at all\n              \" and (committed.ws_operation_type=\" + quoteChar(OpertaionType.UPDATE.sqlConst) +  \n                    \" OR cur.ws_operation_type=\" + quoteChar(OpertaionType.UPDATE.sqlConst) + \")\"));\n          if(rs.next()) {\n            //found a conflict\n            String committedTxn = \"[\" + JavaUtils.txnIdToString(rs.getLong(1)) + \",\" + rs.getLong(2) + \"]\";\n            StringBuilder resource = new StringBuilder(rs.getString(3)).append(\"/\").append(rs.getString(4));\n            String partitionName = rs.getString(5);\n            if(partitionName != null) {\n              resource.append('/').append(partitionName);\n            }\n            String msg = \"Aborting [\" + JavaUtils.txnIdToString(txnid) + \",\" + rs.getLong(6) + \"]\" + \" due to a write conflict on \" + resource +\n              \" committed by \" + committedTxn;\n            close(rs);\n            //remove WRITE_SET info for current txn since it's about to abort\n            dbConn.rollback(undoWriteSetForCurrentTxn);\n            LOG.info(msg);\n            //todo: should make abortTxns() write something into TXNS.TXN_META_INFO about this\n            if(abortTxns(dbConn, Collections.singletonList(txnid), true) != 1) {\n              throw new IllegalStateException(msg + \" FAILED!\");\n            }\n            dbConn.commit();\n            close(null, stmt, dbConn);\n            throw new TxnAbortedException(msg);\n          }\n          else {\n            //no conflicting operations, proceed with the rest of commit sequence\n          }\n        }\n        // Move the record from txn_components into completed_txn_components so that the compactor\n        // knows where to look to compact.\n        String s = \"insert into COMPLETED_TXN_COMPONENTS select tc_txnid, tc_database, tc_table, \" +\n          \"tc_partition from TXN_COMPONENTS where tc_txnid = \" + txnid;\n        LOG.debug(\"Going to execute insert <\" + s + \">\");\n        int modCount = 0;\n        if ((modCount = stmt.executeUpdate(s)) < 1) {\n          //this can be reasonable for an empty txn START/COMMIT or read-only txn\n          LOG.info(\"Expected to move at least one record from txn_components to \" +\n            \"completed_txn_components when committing txn! \" + JavaUtils.txnIdToString(txnid));\n        }\n        s = \"delete from TXN_COMPONENTS where tc_txnid = \" + txnid;\n        LOG.debug(\"Going to execute update <\" + s + \">\");\n        modCount = stmt.executeUpdate(s);\n        s = \"delete from HIVE_LOCKS where hl_txnid = \" + txnid;\n        LOG.debug(\"Going to execute update <\" + s + \">\");\n        modCount = stmt.executeUpdate(s);\n        s = \"delete from TXNS where txn_id = \" + txnid;\n        LOG.debug(\"Going to execute update <\" + s + \">\");\n        modCount = stmt.executeUpdate(s);\n        LOG.debug(\"Going to commit\");\n        dbConn.commit();\n      } catch (SQLException e) {\n        LOG.debug(\"Going to rollback\");\n        rollbackDBConn(dbConn);\n        checkRetryable(dbConn, e, \"commitTxn(\" + rqst + \")\");\n        throw new MetaException(\"Unable to update transaction database \"\n          + StringUtils.stringifyException(e));\n      } finally {\n        close(commitIdRs);\n        close(lockHandle, stmt, dbConn);\n        unlockInternal();\n      }\n    } catch (RetryException e) {\n      commitTxn(rqst);\n    }\n  }",
                "code_after_change": "  public void commitTxn(CommitTxnRequest rqst)\n    throws NoSuchTxnException, TxnAbortedException,  MetaException {\n    long txnid = rqst.getTxnid();\n    try {\n      Connection dbConn = null;\n      Statement stmt = null;\n      ResultSet lockHandle = null;\n      ResultSet commitIdRs = null, rs;\n      try {\n        lockInternal();\n        dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);\n        stmt = dbConn.createStatement();\n        /**\n         * Runs at READ_COMMITTED with S4U on TXNS row for \"txnid\".  S4U ensures that no other\n         * operation can change this txn (such acquiring locks). While lock() and commitTxn()\n         * should not normally run concurrently (for same txn) but could due to bugs in the client\n         * which could then corrupt internal transaction manager state.  Also competes with abortTxn().\n         */\n        lockHandle = lockTransactionRecord(stmt, txnid, TXN_OPEN);\n        if (lockHandle == null) {\n          //this also ensures that txn is still there and in expected state (hasn't been timed out)\n          ensureValidTxn(dbConn, txnid, stmt);\n          shouldNeverHappen(txnid);\n        }\n        String conflictSQLSuffix = \"from TXN_COMPONENTS where tc_txnid=\" + txnid + \" and tc_operation_type IN(\" +\n          quoteChar(OpertaionType.UPDATE.sqlConst) + \",\" + quoteChar(OpertaionType.DELETE.sqlConst) + \")\";\n        rs = stmt.executeQuery(sqlGenerator.addLimitClause(1, \"tc_operation_type \" + conflictSQLSuffix));\n        if (rs.next()) {\n          close(rs);\n          //here means currently committing txn performed update/delete and we should check WW conflict\n          /**\n           * This S4U will mutex with other commitTxn() and openTxns(). \n           * -1 below makes txn intervals look like [3,3] [4,4] if all txns are serial\n           * Note: it's possible to have several txns have the same commit id.  Suppose 3 txns start\n           * at the same time and no new txns start until all 3 commit.\n           * We could've incremented the sequence for commitId is well but it doesn't add anything functionally.\n           */\n          commitIdRs = stmt.executeQuery(sqlGenerator.addForUpdateClause(\"select ntxn_next - 1 from NEXT_TXN_ID\"));\n          if (!commitIdRs.next()) {\n            throw new IllegalStateException(\"No rows found in NEXT_TXN_ID\");\n          }\n          long commitId = commitIdRs.getLong(1);\n          Savepoint undoWriteSetForCurrentTxn = dbConn.setSavepoint();\n          int numCompsWritten = stmt.executeUpdate(\n            \"insert into WRITE_SET (ws_database, ws_table, ws_partition, ws_txnid, ws_commit_id, ws_operation_type)\" +\n            \" select tc_database, tc_table, tc_partition, tc_txnid, \" + commitId + \", tc_operation_type \" + conflictSQLSuffix);\n          /**\n           * see if there are any overlapping txns wrote the same element, i.e. have a conflict\n           * Since entire commit operation is mutexed wrt other start/commit ops,\n           * committed.ws_commit_id <= current.ws_commit_id for all txns\n           * thus if committed.ws_commit_id < current.ws_txnid, transactions do NOT overlap\n           * For example, [17,20] is committed, [6,80] is being committed right now - these overlap\n           * [17,20] committed and [21,21] committing now - these do not overlap.\n           * [17,18] committed and [18,19] committing now - these overlap  (here 18 started while 17 was still running)\n           */\n          rs = stmt.executeQuery\n            (sqlGenerator.addLimitClause(1, \"committed.ws_txnid, committed.ws_commit_id, committed.ws_database,\" +\n              \"committed.ws_table, committed.ws_partition, cur.ws_commit_id \" +\n              \"from WRITE_SET committed INNER JOIN WRITE_SET cur \" +\n              \"ON committed.ws_database=cur.ws_database and committed.ws_table=cur.ws_table \" +\n              //For partitioned table we always track writes at partition level (never at table)\n              //and for non partitioned - always at table level, thus the same table should never\n              //have entries with partition key and w/o\n              \"and (committed.ws_partition=cur.ws_partition or (committed.ws_partition is null and cur.ws_partition is null)) \" +\n              \"where cur.ws_txnid <= committed.ws_commit_id\" + //txns overlap; could replace ws_txnid\n              // with txnid, though any decent DB should infer this\n              \" and cur.ws_txnid=\" + txnid + //make sure RHS of join only has rows we just inserted as\n              // part of this commitTxn() op\n              \" and committed.ws_txnid <> \" + txnid + //and LHS only has committed txns\n              //U+U and U+D is a conflict but D+D is not and we don't currently track I in WRITE_SET at all\n              \" and (committed.ws_operation_type=\" + quoteChar(OpertaionType.UPDATE.sqlConst) +\n              \" OR cur.ws_operation_type=\" + quoteChar(OpertaionType.UPDATE.sqlConst) + \")\"));\n          if (rs.next()) {\n            //found a conflict\n            String committedTxn = \"[\" + JavaUtils.txnIdToString(rs.getLong(1)) + \",\" + rs.getLong(2) + \"]\";\n            StringBuilder resource = new StringBuilder(rs.getString(3)).append(\"/\").append(rs.getString(4));\n            String partitionName = rs.getString(5);\n            if (partitionName != null) {\n              resource.append('/').append(partitionName);\n            }\n            String msg = \"Aborting [\" + JavaUtils.txnIdToString(txnid) + \",\" + rs.getLong(6) + \"]\" + \" due to a write conflict on \" + resource +\n              \" committed by \" + committedTxn;\n            close(rs);\n            //remove WRITE_SET info for current txn since it's about to abort\n            dbConn.rollback(undoWriteSetForCurrentTxn);\n            LOG.info(msg);\n            //todo: should make abortTxns() write something into TXNS.TXN_META_INFO about this\n            if (abortTxns(dbConn, Collections.singletonList(txnid), true) != 1) {\n              throw new IllegalStateException(msg + \" FAILED!\");\n            }\n            dbConn.commit();\n            close(null, stmt, dbConn);\n            throw new TxnAbortedException(msg);\n          } else {\n            //no conflicting operations, proceed with the rest of commit sequence\n          }\n        }\n        else {\n          /**\n           * current txn didn't update/delete anything (may have inserted), so just proceed with commit\n           *\n           * We only care about commit id for write txns, so for RO (when supported) txns we don't\n           * have to mutex on NEXT_TXN_ID.\n           * Consider: if RO txn is after a W txn, then RO's openTxns() will be mutexed with W's\n           * commitTxn() because both do S4U on NEXT_TXN_ID and thus RO will see result of W txn.\n           * If RO < W, then there is no reads-from relationship.\n           */\n        }\n        // Move the record from txn_components into completed_txn_components so that the compactor\n        // knows where to look to compact.\n        String s = \"insert into COMPLETED_TXN_COMPONENTS select tc_txnid, tc_database, tc_table, \" +\n          \"tc_partition from TXN_COMPONENTS where tc_txnid = \" + txnid;\n        LOG.debug(\"Going to execute insert <\" + s + \">\");\n        int modCount = 0;\n        if ((modCount = stmt.executeUpdate(s)) < 1) {\n          //this can be reasonable for an empty txn START/COMMIT or read-only txn\n          LOG.info(\"Expected to move at least one record from txn_components to \" +\n            \"completed_txn_components when committing txn! \" + JavaUtils.txnIdToString(txnid));\n        }\n        s = \"delete from TXN_COMPONENTS where tc_txnid = \" + txnid;\n        LOG.debug(\"Going to execute update <\" + s + \">\");\n        modCount = stmt.executeUpdate(s);\n        s = \"delete from HIVE_LOCKS where hl_txnid = \" + txnid;\n        LOG.debug(\"Going to execute update <\" + s + \">\");\n        modCount = stmt.executeUpdate(s);\n        s = \"delete from TXNS where txn_id = \" + txnid;\n        LOG.debug(\"Going to execute update <\" + s + \">\");\n        modCount = stmt.executeUpdate(s);\n        LOG.debug(\"Going to commit\");\n        dbConn.commit();\n      } catch (SQLException e) {\n        LOG.debug(\"Going to rollback\");\n        rollbackDBConn(dbConn);\n        checkRetryable(dbConn, e, \"commitTxn(\" + rqst + \")\");\n        throw new MetaException(\"Unable to update transaction database \"\n          + StringUtils.stringifyException(e));\n      } finally {\n        close(commitIdRs);\n        close(lockHandle, stmt, dbConn);\n        unlockInternal();\n      }\n    } catch (RetryException e) {\n      commitTxn(rqst);\n    }\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.enqueueLockWithRetry": {
                "code_before_change": "  private ConnectionLockIdPair enqueueLockWithRetry(LockRequest rqst) throws NoSuchTxnException, TxnAbortedException, MetaException {\n    boolean success = false;\n    Connection dbConn = null;\n    try {\n      Statement stmt = null;\n      ResultSet rs = null;\n      ResultSet lockHandle = null;\n      try {\n        lockInternal();\n        dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);\n        long txnid = rqst.getTxnid();\n        stmt = dbConn.createStatement();\n        if (isValidTxn(txnid)) {\n          //this also ensures that txn is still there in expected state\n          lockHandle = lockTransactionRecord(stmt, txnid, TXN_OPEN);\n          if(lockHandle == null) {\n            ensureValidTxn(dbConn, txnid, stmt);\n            shouldNeverHappen(txnid);\n          }\n        }\n        /** Get the next lock id.\n         * This has to be atomic with adding entries to HIVE_LOCK entries (1st add in W state) to prevent a race.\n         * Suppose ID gen is a separate txn and 2 concurrent lock() methods are running.  1st one generates nl_next=7,\n         * 2nd nl_next=8.  Then 8 goes first to insert into HIVE_LOCKS and acquires the locks.  Then 7 unblocks,\n         * and add it's W locks but it won't see locks from 8 since to be 'fair' {@link #checkLock(java.sql.Connection, long)}\n         * doesn't block on locks acquired later than one it's checking*/\n        String s = addForUpdateClause(\"select nl_next from NEXT_LOCK_ID\");\n        LOG.debug(\"Going to execute query <\" + s + \">\");\n        rs = stmt.executeQuery(s);\n        if (!rs.next()) {\n          LOG.debug(\"Going to rollback\");\n          dbConn.rollback();\n          throw new MetaException(\"Transaction tables not properly \" +\n            \"initialized, no record found in next_lock_id\");\n        }\n        long extLockId = rs.getLong(1);\n        s = \"update NEXT_LOCK_ID set nl_next = \" + (extLockId + 1);\n        LOG.debug(\"Going to execute update <\" + s + \">\");\n        stmt.executeUpdate(s);\n\n        if (txnid > 0) {\n          /**\n           * todo QueryPlan has BaseSemanticAnalyzer which has acidFileSinks list of FileSinkDesc\n           * FileSinkDesc.table is ql.metadata.Table\n           * Table.tableSpec which is TableSpec, which has specType which is SpecType\n           * So maybe this can work to know that this is part of dynamic partition insert in which case\n           * we'll get addDynamicPartitions() call and should not write TXN_COMPONENTS here.\n           * In any case, that's an optimization for now;  will be required when adding multi-stmt txns\n           */\n          // For each component in this lock request,\n          // add an entry to the txn_components table\n          for (LockComponent lc : rqst.getComponent()) {\n            if(lc.isSetIsAcid() && !lc.isIsAcid()) {\n              //we don't prevent using non-acid resources in a txn but we do lock them\n              continue;\n            }\n            boolean updateTxnComponents;\n            if(!lc.isSetOperationType()) {\n              //request came from old version of the client\n              updateTxnComponents = true;//this matches old behavior\n            }\n            else {\n              switch (lc.getOperationType()) {\n                case INSERT:\n                case UPDATE:\n                case DELETE:\n                  updateTxnComponents = true;\n                  break;\n                case SELECT:\n                  updateTxnComponents = false;\n                  break;\n                default:\n                  //since we have an open transaction, only 4 values above are expected \n                  throw new IllegalStateException(\"Unexpected DataOperationType: \" + lc.getOperationType()\n                    + \" agentInfo=\" + rqst.getAgentInfo() + \" \" + JavaUtils.txnIdToString(txnid));\n              }\n            }\n            if(!updateTxnComponents) {\n              continue;\n            }\n            String dbName = lc.getDbname();\n            String tblName = lc.getTablename();\n            String partName = lc.getPartitionname();\n            s = \"insert into TXN_COMPONENTS \" +\n              \"(tc_txnid, tc_database, tc_table, tc_partition, tc_operation_type) \" +\n              \"values (\" + txnid + \", '\" + dbName + \"', \" +\n              (tblName == null ? \"null\" : \"'\" + tblName + \"'\") + \", \" +\n              (partName == null ? \"null\" : \"'\" + partName + \"'\")+ \",\" +\n              quoteString(OpertaionType.fromDataOperationType(lc.getOperationType()).toString()) + \")\";\n            LOG.debug(\"Going to execute update <\" + s + \">\");\n            int modCount = stmt.executeUpdate(s);\n          }\n        }\n\n        long intLockId = 0;\n        for (LockComponent lc : rqst.getComponent()) {\n          if(lc.isSetOperationType() && lc.getOperationType() == DataOperationType.UNSET) {\n            //old version of thrift client should have (lc.isSetOperationType() == false)\n            throw new IllegalStateException(\"Bug: operationType=\" + lc.getOperationType() + \" for component \"\n              + lc + \" agentInfo=\" + rqst.getAgentInfo());\n          }\n          intLockId++;\n          String dbName = lc.getDbname();\n          String tblName = lc.getTablename();\n          String partName = lc.getPartitionname();\n          LockType lockType = lc.getType();\n          char lockChar = 'z';\n          switch (lockType) {\n            case EXCLUSIVE:\n              lockChar = LOCK_EXCLUSIVE;\n              break;\n            case SHARED_READ:\n              lockChar = LOCK_SHARED;\n              break;\n            case SHARED_WRITE:\n              lockChar = LOCK_SEMI_SHARED;\n              break;\n          }\n          long now = getDbTime(dbConn);\n          s = \"insert into HIVE_LOCKS \" +\n            \" (hl_lock_ext_id, hl_lock_int_id, hl_txnid, hl_db, hl_table, \" +\n            \"hl_partition, hl_lock_state, hl_lock_type, hl_last_heartbeat, hl_user, hl_host)\" +\n            \" values (\" + extLockId + \", \" +\n            +intLockId + \",\" + txnid + \", '\" +\n            dbName + \"', \" + (tblName == null ? \"null\" : \"'\" + tblName + \"'\")\n            + \", \" + (partName == null ? \"null\" : \"'\" + partName + \"'\") +\n            \", '\" + LOCK_WAITING + \"', \" + \"'\" + lockChar + \"', \" +\n            //for locks associated with a txn, we always heartbeat txn and timeout based on that\n            (isValidTxn(txnid) ? 0 : now) + \", '\" +\n            rqst.getUser() + \"', '\" + rqst.getHostname() + \"')\";\n          LOG.debug(\"Going to execute update <\" + s + \">\");\n          stmt.executeUpdate(s);\n        }\n        dbConn.commit();\n        success = true;\n        return new ConnectionLockIdPair(dbConn, extLockId);\n      } catch (SQLException e) {\n        LOG.debug(\"Going to rollback\");\n        rollbackDBConn(dbConn);\n        checkRetryable(dbConn, e, \"enqueueLockWithRetry(\" + rqst + \")\");\n        throw new MetaException(\"Unable to update transaction database \" +\n          StringUtils.stringifyException(e));\n      } finally {\n        close(lockHandle);\n        close(rs, stmt, null);\n        if (!success) {\n          /* This needs to return a \"live\" connection to be used by operation that follows it.\n          Thus it only closes Connection on failure/retry. */\n          closeDbConn(dbConn);\n        }\n        unlockInternal();\n      }\n    }\n    catch(RetryException e) {\n      return enqueueLockWithRetry(rqst);\n    }\n  }",
                "code_after_change": "  private ConnectionLockIdPair enqueueLockWithRetry(LockRequest rqst) throws NoSuchTxnException, TxnAbortedException, MetaException {\n    boolean success = false;\n    Connection dbConn = null;\n    try {\n      Statement stmt = null;\n      ResultSet rs = null;\n      ResultSet lockHandle = null;\n      try {\n        lockInternal();\n        dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);\n        long txnid = rqst.getTxnid();\n        stmt = dbConn.createStatement();\n        if (isValidTxn(txnid)) {\n          //this also ensures that txn is still there in expected state\n          lockHandle = lockTransactionRecord(stmt, txnid, TXN_OPEN);\n          if(lockHandle == null) {\n            ensureValidTxn(dbConn, txnid, stmt);\n            shouldNeverHappen(txnid);\n          }\n        }\n        /** Get the next lock id.\n         * This has to be atomic with adding entries to HIVE_LOCK entries (1st add in W state) to prevent a race.\n         * Suppose ID gen is a separate txn and 2 concurrent lock() methods are running.  1st one generates nl_next=7,\n         * 2nd nl_next=8.  Then 8 goes first to insert into HIVE_LOCKS and acquires the locks.  Then 7 unblocks,\n         * and add it's W locks but it won't see locks from 8 since to be 'fair' {@link #checkLock(java.sql.Connection, long)}\n         * doesn't block on locks acquired later than one it's checking*/\n        String s = sqlGenerator.addForUpdateClause(\"select nl_next from NEXT_LOCK_ID\");\n        LOG.debug(\"Going to execute query <\" + s + \">\");\n        rs = stmt.executeQuery(s);\n        if (!rs.next()) {\n          LOG.debug(\"Going to rollback\");\n          dbConn.rollback();\n          throw new MetaException(\"Transaction tables not properly \" +\n            \"initialized, no record found in next_lock_id\");\n        }\n        long extLockId = rs.getLong(1);\n        s = \"update NEXT_LOCK_ID set nl_next = \" + (extLockId + 1);\n        LOG.debug(\"Going to execute update <\" + s + \">\");\n        stmt.executeUpdate(s);\n\n        if (txnid > 0) {\n          List<String> rows = new ArrayList<>();\n          /**\n           * todo QueryPlan has BaseSemanticAnalyzer which has acidFileSinks list of FileSinkDesc\n           * FileSinkDesc.table is ql.metadata.Table\n           * Table.tableSpec which is TableSpec, which has specType which is SpecType\n           * So maybe this can work to know that this is part of dynamic partition insert in which case\n           * we'll get addDynamicPartitions() call and should not write TXN_COMPONENTS here.\n           * In any case, that's an optimization for now;  will be required when adding multi-stmt txns\n           */\n          // For each component in this lock request,\n          // add an entry to the txn_components table\n          for (LockComponent lc : rqst.getComponent()) {\n            if(lc.isSetIsAcid() && !lc.isIsAcid()) {\n              //we don't prevent using non-acid resources in a txn but we do lock them\n              continue;\n            }\n            boolean updateTxnComponents;\n            if(!lc.isSetOperationType()) {\n              //request came from old version of the client\n              updateTxnComponents = true;//this matches old behavior\n            }\n            else {\n              switch (lc.getOperationType()) {\n                case INSERT:\n                case UPDATE:\n                case DELETE:\n                  updateTxnComponents = true;\n                  break;\n                case SELECT:\n                  updateTxnComponents = false;\n                  break;\n                default:\n                  //since we have an open transaction, only 4 values above are expected \n                  throw new IllegalStateException(\"Unexpected DataOperationType: \" + lc.getOperationType()\n                    + \" agentInfo=\" + rqst.getAgentInfo() + \" \" + JavaUtils.txnIdToString(txnid));\n              }\n            }\n            if(!updateTxnComponents) {\n              continue;\n            }\n            String dbName = lc.getDbname();\n            String tblName = lc.getTablename();\n            String partName = lc.getPartitionname();\n            rows.add(txnid + \", '\" + dbName + \"', \" +\n              (tblName == null ? \"null\" : \"'\" + tblName + \"'\") + \", \" +\n              (partName == null ? \"null\" : \"'\" + partName + \"'\")+ \",\" +\n              quoteString(OpertaionType.fromDataOperationType(lc.getOperationType()).toString()));\n          }\n          List<String> queries = sqlGenerator.createInsertValuesStmt(\n            \"TXN_COMPONENTS (tc_txnid, tc_database, tc_table, tc_partition, tc_operation_type)\", rows);\n          for(String query : queries) {\n            LOG.debug(\"Going to execute update <\" + query + \">\");\n            int modCount = stmt.executeUpdate(query);\n          }\n        }\n\n        List<String> rows = new ArrayList<>();\n        long intLockId = 0;\n        for (LockComponent lc : rqst.getComponent()) {\n          if(lc.isSetOperationType() && lc.getOperationType() == DataOperationType.UNSET) {\n            //old version of thrift client should have (lc.isSetOperationType() == false)\n            throw new IllegalStateException(\"Bug: operationType=\" + lc.getOperationType() + \" for component \"\n              + lc + \" agentInfo=\" + rqst.getAgentInfo());\n          }\n          intLockId++;\n          String dbName = lc.getDbname();\n          String tblName = lc.getTablename();\n          String partName = lc.getPartitionname();\n          LockType lockType = lc.getType();\n          char lockChar = 'z';\n          switch (lockType) {\n            case EXCLUSIVE:\n              lockChar = LOCK_EXCLUSIVE;\n              break;\n            case SHARED_READ:\n              lockChar = LOCK_SHARED;\n              break;\n            case SHARED_WRITE:\n              lockChar = LOCK_SEMI_SHARED;\n              break;\n          }\n          long now = getDbTime(dbConn);\n            rows.add(extLockId + \", \" + intLockId + \",\" + txnid + \", \" +\n            quoteString(dbName) + \", \" +\n            valueOrNullLiteral(tblName) + \", \" +\n            valueOrNullLiteral(partName) + \", \" +\n            quoteChar(LOCK_WAITING) + \", \" + quoteChar(lockChar) + \", \" +\n            //for locks associated with a txn, we always heartbeat txn and timeout based on that\n            (isValidTxn(txnid) ? 0 : now) + \", \" +\n            valueOrNullLiteral(rqst.getUser()) + \", \" +\n            valueOrNullLiteral(rqst.getHostname()) + \", \" +\n            valueOrNullLiteral(rqst.getAgentInfo()));// + \")\";\n        }\n        List<String> queries = sqlGenerator.createInsertValuesStmt(\n          \"HIVE_LOCKS (hl_lock_ext_id, hl_lock_int_id, hl_txnid, hl_db, \" +\n            \"hl_table, hl_partition,hl_lock_state, hl_lock_type, \" +\n            \"hl_last_heartbeat, hl_user, hl_host, hl_agent_info)\", rows);\n        for(String query : queries) {\n          LOG.debug(\"Going to execute update <\" + query + \">\");\n          int modCount = stmt.executeUpdate(query);\n        }\n        dbConn.commit();\n        success = true;\n        return new ConnectionLockIdPair(dbConn, extLockId);\n      } catch (SQLException e) {\n        LOG.debug(\"Going to rollback\");\n        rollbackDBConn(dbConn);\n        checkRetryable(dbConn, e, \"enqueueLockWithRetry(\" + rqst + \")\");\n        throw new MetaException(\"Unable to update transaction database \" +\n          StringUtils.stringifyException(e));\n      } finally {\n        close(lockHandle);\n        close(rs, stmt, null);\n        if (!success) {\n          /* This needs to return a \"live\" connection to be used by operation that follows it.\n          Thus it only closes Connection on failure/retry. */\n          closeDbConn(dbConn);\n        }\n        unlockInternal();\n      }\n    }\n    catch(RetryException e) {\n      return enqueueLockWithRetry(rqst);\n    }\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.setConf": {
                "code_before_change": "  public void setConf(HiveConf conf) {\n    this.conf = conf;\n\n    checkQFileTestHack();\n\n    Connection dbConn = null;\n    // Set up the JDBC connection pool\n    try {\n      setupJdbcConnectionPool(conf);\n      dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);\n      determineDatabaseProduct(dbConn);\n    } catch (SQLException e) {\n      String msg = \"Unable to instantiate JDBC connection pooling, \" + e.getMessage();\n      LOG.error(msg);\n      throw new RuntimeException(e);\n    }\n    finally {\n      closeDbConn(dbConn);\n    }\n\n    timeout = HiveConf.getTimeVar(conf, HiveConf.ConfVars.HIVE_TXN_TIMEOUT, TimeUnit.MILLISECONDS);\n    buildJumpTable();\n    retryInterval = HiveConf.getTimeVar(conf, HiveConf.ConfVars.HMSHANDLERINTERVAL,\n        TimeUnit.MILLISECONDS);\n    retryLimit = HiveConf.getIntVar(conf, HiveConf.ConfVars.HMSHANDLERATTEMPTS);\n    deadlockRetryInterval = retryInterval / 10;\n    maxOpenTxns = HiveConf.getIntVar(conf, HiveConf.ConfVars.HIVE_MAX_OPEN_TXNS);\n  }",
                "code_after_change": "  public void setConf(HiveConf conf) {\n    this.conf = conf;\n\n    checkQFileTestHack();\n    \n    Connection dbConn = null;\n    // Set up the JDBC connection pool\n    try {\n      setupJdbcConnectionPool(conf);\n      dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);\n      determineDatabaseProduct(dbConn);\n      sqlGenerator = new SQLGenerator(dbProduct, conf);\n    } catch (SQLException e) {\n      String msg = \"Unable to instantiate JDBC connection pooling, \" + e.getMessage();\n      LOG.error(msg);\n      throw new RuntimeException(e);\n    }\n    finally {\n      closeDbConn(dbConn);\n    }\n\n    timeout = HiveConf.getTimeVar(conf, HiveConf.ConfVars.HIVE_TXN_TIMEOUT, TimeUnit.MILLISECONDS);\n    buildJumpTable();\n    retryInterval = HiveConf.getTimeVar(conf, HiveConf.ConfVars.HMSHANDLERINTERVAL,\n        TimeUnit.MILLISECONDS);\n    retryLimit = HiveConf.getIntVar(conf, HiveConf.ConfVars.HMSHANDLERATTEMPTS);\n    deadlockRetryInterval = retryInterval / 10;\n    maxOpenTxns = HiveConf.getIntVar(conf, HiveConf.ConfVars.HIVE_MAX_OPEN_TXNS);\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.isDuplicateKeyError": {
                "code_before_change": "  private boolean isDuplicateKeyError(SQLException ex) {\n    switch (dbProduct) {\n      case DERBY:\n        if(\"23505\".equals(ex.getSQLState())) {\n          return true;\n        }\n        break;\n      case MYSQL:\n        if(ex.getErrorCode() == 1022 && \"23000\".equals(ex.getSQLState())) {\n          return true;\n        }\n        break;\n      case SQLSERVER:\n        //2627 is unique constaint violation incl PK, 2601 - unique key\n        if(ex.getErrorCode() == 2627 && \"23000\".equals(ex.getSQLState())) {\n          return true;\n        }\n        break;\n      case ORACLE:\n        if(ex.getErrorCode() == 1 && \"23000\".equals(ex.getSQLState())) {\n          return true;\n        }\n        break;\n      case POSTGRES:\n        //http://www.postgresql.org/docs/8.1/static/errcodes-appendix.html\n        if(\"23505\".equals(ex.getSQLState())) {\n          return true;\n        }\n        break;\n      default:\n        throw new IllegalArgumentException(\"Unexpected DB type: \" + dbProduct + \"; \" + getMessage(ex));\n    }\n    return false;\n  }",
                "code_after_change": "  private boolean isDuplicateKeyError(SQLException ex) {\n    switch (dbProduct) {\n      case DERBY:\n        if(\"23505\".equals(ex.getSQLState())) {\n          return true;\n        }\n        break;\n      case MYSQL:\n        if(ex.getErrorCode() == 1022 && \"23000\".equals(ex.getSQLState())) {\n          return true;\n        }\n        break;\n      case SQLSERVER:\n        //2627 is unique constaint violation incl PK, 2601 - unique key\n        if(ex.getErrorCode() == 2627 && \"23000\".equals(ex.getSQLState())) {\n          return true;\n        }\n        break;\n      case ORACLE:\n        if(ex.getErrorCode() == 1 && \"23000\".equals(ex.getSQLState())) {\n          return true;\n        }\n        break;\n      case POSTGRES:\n        //http://www.postgresql.org/docs/8.1/static/errcodes-appendix.html\n        if(\"23505\".equals(ex.getSQLState())) {\n          return true;\n        }\n        break;\n      default:\n        throw new IllegalArgumentException(\"Unexpected DB type: \" + dbProduct + \"; \" + getMessage(ex));\n    }\n    return false;\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.performTimeOuts": {
                "code_before_change": "  public void performTimeOuts() {\n    Connection dbConn = null;\n    Statement stmt = null;\n    ResultSet rs = null;\n    try {\n      dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);\n      //We currently commit after selecting the TXNS to abort.  So whether SERIALIZABLE\n      //READ_COMMITTED, the effect is the same.  We could use FOR UPDATE on Select from TXNS\n      //and do the whole performTimeOuts() in a single huge transaction, but the only benefit\n      //would be to make sure someone cannot heartbeat one of these txns at the same time.\n      //The attempt to heartbeat would block and fail immediately after it's unblocked.\n      //With current (RC + multiple txns) implementation it is possible for someone to send\n      //heartbeat at the very end of the expire interval, and just after the Select from TXNS\n      //is made, in which case heartbeat will succeed but txn will still be Aborted.\n      //Solving this corner case is not worth the perf penalty.  The client should heartbeat in a\n      //timely way.\n      long now = getDbTime(dbConn);\n      timeOutLocks(dbConn, now);\n      while(true) {\n        stmt = dbConn.createStatement();\n        String s = \" txn_id from TXNS where txn_state = '\" + TXN_OPEN +\n          \"' and txn_last_heartbeat <  \" + (now - timeout);\n        s = addLimitClause(250 * TIMED_OUT_TXN_ABORT_BATCH_SIZE, s);\n        LOG.debug(\"Going to execute query <\" + s + \">\");\n        rs = stmt.executeQuery(s);\n        if(!rs.next()) {\n          return;//no more timedout txns\n        }\n        List<List<Long>> timedOutTxns = new ArrayList<>();\n        List<Long> currentBatch = new ArrayList<>(TIMED_OUT_TXN_ABORT_BATCH_SIZE);\n        timedOutTxns.add(currentBatch);\n        do {\n          currentBatch.add(rs.getLong(1));\n          if(currentBatch.size() == TIMED_OUT_TXN_ABORT_BATCH_SIZE) {\n            currentBatch = new ArrayList<>(TIMED_OUT_TXN_ABORT_BATCH_SIZE);\n            timedOutTxns.add(currentBatch);\n          }\n        } while(rs.next());\n        dbConn.commit();\n        close(rs, stmt, null);\n        int numTxnsAborted = 0;\n        for(List<Long> batchToAbort : timedOutTxns) {\n          if(abortTxns(dbConn, batchToAbort, now - timeout, true) == batchToAbort.size()) {\n            dbConn.commit();\n            numTxnsAborted += batchToAbort.size();\n            //todo: add TXNS.COMMENT filed and set it to 'aborted by system due to timeout'\n            Collections.sort(batchToAbort);//easier to read logs\n            LOG.info(\"Aborted the following transactions due to timeout: \" + batchToAbort.toString());\n          }\n          else {\n            //could not abort all txns in this batch - this may happen because in parallel with this\n            //operation there was activity on one of the txns in this batch (commit/abort/heartbeat)\n            //This is not likely but may happen if client experiences long pause between heartbeats or\n            //unusually long/extreme pauses between heartbeat() calls and other logic in checkLock(),\n            //lock(), etc.\n            dbConn.rollback();\n          }\n        }\n        LOG.info(\"Aborted \" + numTxnsAborted + \" transactions due to timeout\");\n      }\n    } catch (SQLException ex) {\n      LOG.warn(\"Aborting timedout transactions failed due to \" + getMessage(ex), ex);\n    }\n    catch(MetaException e) {\n      LOG.warn(\"Aborting timedout transactions failed due to \" + e.getMessage(), e);\n    }\n    finally {\n      close(rs, stmt, dbConn);\n    }\n  }",
                "code_after_change": "  public void performTimeOuts() {\n    Connection dbConn = null;\n    Statement stmt = null;\n    ResultSet rs = null;\n    try {\n      dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);\n      //We currently commit after selecting the TXNS to abort.  So whether SERIALIZABLE\n      //READ_COMMITTED, the effect is the same.  We could use FOR UPDATE on Select from TXNS\n      //and do the whole performTimeOuts() in a single huge transaction, but the only benefit\n      //would be to make sure someone cannot heartbeat one of these txns at the same time.\n      //The attempt to heartbeat would block and fail immediately after it's unblocked.\n      //With current (RC + multiple txns) implementation it is possible for someone to send\n      //heartbeat at the very end of the expire interval, and just after the Select from TXNS\n      //is made, in which case heartbeat will succeed but txn will still be Aborted.\n      //Solving this corner case is not worth the perf penalty.  The client should heartbeat in a\n      //timely way.\n      long now = getDbTime(dbConn);\n      timeOutLocks(dbConn, now);\n      while(true) {\n        stmt = dbConn.createStatement();\n        String s = \" txn_id from TXNS where txn_state = '\" + TXN_OPEN +\n          \"' and txn_last_heartbeat <  \" + (now - timeout);\n        s = sqlGenerator.addLimitClause(250 * TIMED_OUT_TXN_ABORT_BATCH_SIZE, s);\n        LOG.debug(\"Going to execute query <\" + s + \">\");\n        rs = stmt.executeQuery(s);\n        if(!rs.next()) {\n          return;//no more timedout txns\n        }\n        List<List<Long>> timedOutTxns = new ArrayList<>();\n        List<Long> currentBatch = new ArrayList<>(TIMED_OUT_TXN_ABORT_BATCH_SIZE);\n        timedOutTxns.add(currentBatch);\n        do {\n          currentBatch.add(rs.getLong(1));\n          if(currentBatch.size() == TIMED_OUT_TXN_ABORT_BATCH_SIZE) {\n            currentBatch = new ArrayList<>(TIMED_OUT_TXN_ABORT_BATCH_SIZE);\n            timedOutTxns.add(currentBatch);\n          }\n        } while(rs.next());\n        dbConn.commit();\n        close(rs, stmt, null);\n        int numTxnsAborted = 0;\n        for(List<Long> batchToAbort : timedOutTxns) {\n          if(abortTxns(dbConn, batchToAbort, now - timeout, true) == batchToAbort.size()) {\n            dbConn.commit();\n            numTxnsAborted += batchToAbort.size();\n            //todo: add TXNS.COMMENT filed and set it to 'aborted by system due to timeout'\n            Collections.sort(batchToAbort);//easier to read logs\n            LOG.info(\"Aborted the following transactions due to timeout: \" + batchToAbort.toString());\n          }\n          else {\n            //could not abort all txns in this batch - this may happen because in parallel with this\n            //operation there was activity on one of the txns in this batch (commit/abort/heartbeat)\n            //This is not likely but may happen if client experiences long pause between heartbeats or\n            //unusually long/extreme pauses between heartbeat() calls and other logic in checkLock(),\n            //lock(), etc.\n            dbConn.rollback();\n          }\n        }\n        LOG.info(\"Aborted \" + numTxnsAborted + \" transactions due to timeout\");\n      }\n    } catch (SQLException ex) {\n      LOG.warn(\"Aborting timedout transactions failed due to \" + getMessage(ex), ex);\n    }\n    catch(MetaException e) {\n      LOG.warn(\"Aborting timedout transactions failed due to \" + e.getMessage(), e);\n    }\n    finally {\n      close(rs, stmt, dbConn);\n    }\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.fromDataOperationType": {
                "code_before_change": "    public static OpertaionType fromDataOperationType(DataOperationType dop) {\n      switch (dop) {\n        case SELECT:\n          return OpertaionType.SELECT;\n        case INSERT:\n          return OpertaionType.INSERT;\n        case UPDATE:\n          return OpertaionType.UPDATE;\n        case DELETE:\n          return OpertaionType.DELETE;\n        default:\n          throw new IllegalArgumentException(\"Unexpected value: \" + dop);\n      }\n    }",
                "code_after_change": "    public static OpertaionType fromDataOperationType(DataOperationType dop) {\n      switch (dop) {\n        case SELECT:\n          return OpertaionType.SELECT;\n        case INSERT:\n          return OpertaionType.INSERT;\n        case UPDATE:\n          return OpertaionType.UPDATE;\n        case DELETE:\n          return OpertaionType.DELETE;\n        default:\n          throw new IllegalArgumentException(\"Unexpected value: \" + dop);\n      }\n    }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.addDynamicPartitions": {
                "code_before_change": "  public void addDynamicPartitions(AddDynamicPartitions rqst)\n      throws NoSuchTxnException,  TxnAbortedException, MetaException {\n    Connection dbConn = null;\n    Statement stmt = null;\n    ResultSet lockHandle = null;\n    ResultSet rs = null;\n    try {\n      try {\n        lockInternal();\n        dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);\n        stmt = dbConn.createStatement();\n        lockHandle = lockTransactionRecord(stmt, rqst.getTxnid(), TXN_OPEN);\n        if(lockHandle == null) {\n          //ensures txn is still there and in expected state\n          ensureValidTxn(dbConn, rqst.getTxnid(), stmt);\n          shouldNeverHappen(rqst.getTxnid());\n        }\n        //for RU this may be null so we should default it to 'u' which is most restrictive\n        OpertaionType ot = OpertaionType.UPDATE;\n        if(rqst.isSetOperationType()) {\n          ot = OpertaionType.fromDataOperationType(rqst.getOperationType());\n        }\n        \n        //what if a txn writes the same table > 1 time...(HIVE-9675) let's go with this for now, but really\n        //need to not write this in the first place, i.e. make this delete not needed\n        //see enqueueLockWithRetry() - that's where we write to TXN_COMPONENTS\n        String deleteSql = \"delete from TXN_COMPONENTS where tc_txnid=\" + rqst.getTxnid() + \" and tc_database=\" +\n          quoteString(rqst.getDbname()) + \" and tc_table=\" + quoteString(rqst.getTablename());\n        //we delete the entries made by enqueueLockWithRetry() since those are based on lock information which is\n        //much \"wider\" than necessary in a lot of cases.  Here on the other hand, we know exactly which\n        //partitions have been written to.  w/o this WRITE_SET would contain entries for partitions not actually\n        //written to\n        int modCount = stmt.executeUpdate(deleteSql);\n        for (String partName : rqst.getPartitionnames()) {\n          String s =\n            \"insert into TXN_COMPONENTS (tc_txnid, tc_database, tc_table, tc_partition, tc_operation_type) values (\" +\n              rqst.getTxnid() + \",\" + quoteString(rqst.getDbname()) + \",\" + quoteString(rqst.getTablename()) +\n              \",\" + quoteString(partName) + \",\" + quoteChar(ot.sqlConst) + \")\";\n          LOG.debug(\"Going to execute update <\" + s + \">\");\n          modCount = stmt.executeUpdate(s);\n        }\n        LOG.debug(\"Going to commit\");\n        dbConn.commit();\n      } catch (SQLException e) {\n        LOG.debug(\"Going to rollback\");\n        rollbackDBConn(dbConn);\n        checkRetryable(dbConn, e, \"addDynamicPartitions(\" + rqst + \")\");\n        throw new MetaException(\"Unable to insert into from transaction database \" +\n          StringUtils.stringifyException(e));\n      } finally {\n        close(lockHandle, stmt, dbConn);\n        unlockInternal();\n      }\n    } catch (RetryException e) {\n      addDynamicPartitions(rqst);\n    }\n  }",
                "code_after_change": "  public void addDynamicPartitions(AddDynamicPartitions rqst)\n      throws NoSuchTxnException,  TxnAbortedException, MetaException {\n    Connection dbConn = null;\n    Statement stmt = null;\n    ResultSet lockHandle = null;\n    ResultSet rs = null;\n    try {\n      try {\n        lockInternal();\n        dbConn = getDbConn(Connection.TRANSACTION_READ_COMMITTED);\n        stmt = dbConn.createStatement();\n        lockHandle = lockTransactionRecord(stmt, rqst.getTxnid(), TXN_OPEN);\n        if(lockHandle == null) {\n          //ensures txn is still there and in expected state\n          ensureValidTxn(dbConn, rqst.getTxnid(), stmt);\n          shouldNeverHappen(rqst.getTxnid());\n        }\n        //for RU this may be null so we should default it to 'u' which is most restrictive\n        OpertaionType ot = OpertaionType.UPDATE;\n        if(rqst.isSetOperationType()) {\n          ot = OpertaionType.fromDataOperationType(rqst.getOperationType());\n        }\n        \n        //what if a txn writes the same table > 1 time...(HIVE-9675) let's go with this for now, but really\n        //need to not write this in the first place, i.e. make this delete not needed\n        //see enqueueLockWithRetry() - that's where we write to TXN_COMPONENTS\n        String deleteSql = \"delete from TXN_COMPONENTS where tc_txnid=\" + rqst.getTxnid() + \" and tc_database=\" +\n          quoteString(rqst.getDbname()) + \" and tc_table=\" + quoteString(rqst.getTablename());\n        //we delete the entries made by enqueueLockWithRetry() since those are based on lock information which is\n        //much \"wider\" than necessary in a lot of cases.  Here on the other hand, we know exactly which\n        //partitions have been written to.  w/o this WRITE_SET would contain entries for partitions not actually\n        //written to\n        int modCount = stmt.executeUpdate(deleteSql);\n        List<String> rows = new ArrayList<>();\n        for (String partName : rqst.getPartitionnames()) {\n          rows.add(rqst.getTxnid() + \",\" + quoteString(rqst.getDbname()) + \",\" + quoteString(rqst.getTablename()) +\n            \",\" + quoteString(partName) + \",\" + quoteChar(ot.sqlConst));\n        }\n        List<String> queries = sqlGenerator.createInsertValuesStmt(\n          \"TXN_COMPONENTS (tc_txnid, tc_database, tc_table, tc_partition, tc_operation_type)\", rows);\n        for(String query : queries) {\n          LOG.debug(\"Going to execute update <\" + query + \">\");\n          modCount = stmt.executeUpdate(query);\n        }\n        LOG.debug(\"Going to commit\");\n        dbConn.commit();\n      } catch (SQLException e) {\n        LOG.debug(\"Going to rollback\");\n        rollbackDBConn(dbConn);\n        checkRetryable(dbConn, e, \"addDynamicPartitions(\" + rqst + \")\");\n        throw new MetaException(\"Unable to insert into from transaction database \" +\n          StringUtils.stringifyException(e));\n      } finally {\n        close(lockHandle, stmt, dbConn);\n        unlockInternal();\n      }\n    } catch (RetryException e) {\n      addDynamicPartitions(rqst);\n    }\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Alternative Fix",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report precisely identifies the root cause in the 'TxnHandler.openTxns' method, which is part of the ground truth methods. The report suggests using 'INSERT ALL' semantics or inserting each row individually, which is an alternative fix to the developer's solution of using 'sqlGenerator.createInsertValuesStmt'. The problem location is precisely identified as 'TxnHandler.openTxns', which matches the ground truth. There is no wrong information in the bug report as all details are relevant to the context of the bug."
        }
    },
    {
        "filename": "HIVE-7374.json",
        "code_diff": {
            "metastore.src.java.org.apache.hadoop.hive.metastore.txn.TxnHandler.compact": {
                "code_before_change": "  public void compact(CompactionRequest rqst) throws MetaException {\n    // Put a compaction request in the queue.\n    try {\n      Connection dbConn = getDbConn(Connection.TRANSACTION_SERIALIZABLE);\n      Statement stmt = null;\n      try {\n        stmt = dbConn.createStatement();\n\n        // Get the id for the next entry in the queue\n        String s = \"select ncq_next from NEXT_COMPACTION_QUEUE_ID\";\n        LOG.debug(\"going to execute query <\" + s + \">\");\n        ResultSet rs = stmt.executeQuery(s);\n        if (!rs.next()) {\n          LOG.debug(\"Going to rollback\");\n          dbConn.rollback();\n          throw new MetaException(\"Transaction tables not properly initiated, \" +\n              \"no record found in next_compaction_queue_id\");\n        }\n        long id = rs.getLong(1);\n        s = \"update NEXT_COMPACTION_QUEUE_ID set ncq_next = \" + (id + 1);\n        LOG.debug(\"Going to execute update <\" + s + \">\");\n        stmt.executeUpdate(s);\n\n        StringBuilder buf = new StringBuilder(\"insert into COMPACTION_QUEUE (cq_id, cq_database, \" +\n            \"cq_table, \");\n        String partName = rqst.getPartitionname();\n        if (partName != null) buf.append(\"cq_partition, \");\n        buf.append(\"cq_state, cq_type\");\n        if (rqst.getRunas() != null) buf.append(\", cq_run_as\");\n        buf.append(\") values (\");\n        buf.append(id);\n        buf.append(\", '\");\n        buf.append(rqst.getDbname());\n        buf.append(\"', '\");\n        buf.append(rqst.getTablename());\n        buf.append(\"', '\");\n        if (partName != null) {\n          buf.append(partName);\n          buf.append(\"', '\");\n        }\n        buf.append(INITIATED_STATE);\n        buf.append(\"', '\");\n        switch (rqst.getType()) {\n          case MAJOR:\n            buf.append(MAJOR_TYPE);\n            break;\n\n          case MINOR:\n            buf.append(MINOR_TYPE);\n            break;\n\n          default:\n            LOG.debug(\"Going to rollback\");\n            dbConn.rollback();\n            throw new MetaException(\"Unexpected compaction type \" + rqst.getType().toString());\n        }\n        if (rqst.getRunas() != null) {\n          buf.append(\"', '\");\n          buf.append(rqst.getRunas());\n        }\n        buf.append(\"')\");\n        s = buf.toString();\n        LOG.debug(\"Going to execute update <\" + s + \">\");\n        stmt.executeUpdate(s);\n        LOG.debug(\"Going to commit\");\n        dbConn.commit();\n      } catch (SQLException e) {\n        try {\n          LOG.debug(\"Going to rollback\");\n          dbConn.rollback();\n        } catch (SQLException e1) {\n        }\n        detectDeadlock(e, \"compact\");\n        throw new MetaException(\"Unable to select from transaction database \" +\n            StringUtils.stringifyException(e));\n      } finally {\n        closeStmt(stmt);\n        closeDbConn(dbConn);\n      }\n    } catch (DeadlockException e) {\n      compact(rqst);\n    } finally {\n      deadlockCnt = 0;\n    }\n  }",
                "code_after_change": "  public void compact(CompactionRequest rqst) throws MetaException {\n    // Put a compaction request in the queue.\n    try {\n      Connection dbConn = getDbConn(Connection.TRANSACTION_SERIALIZABLE);\n      Statement stmt = null;\n      try {\n        stmt = dbConn.createStatement();\n\n        // Get the id for the next entry in the queue\n        String s = \"select ncq_next from NEXT_COMPACTION_QUEUE_ID\";\n        LOG.debug(\"going to execute query <\" + s + \">\");\n        ResultSet rs = stmt.executeQuery(s);\n        if (!rs.next()) {\n          LOG.debug(\"Going to rollback\");\n          dbConn.rollback();\n          throw new MetaException(\"Transaction tables not properly initiated, \" +\n              \"no record found in next_compaction_queue_id\");\n        }\n        long id = rs.getLong(1);\n        s = \"update NEXT_COMPACTION_QUEUE_ID set ncq_next = \" + (id + 1);\n        LOG.debug(\"Going to execute update <\" + s + \">\");\n        stmt.executeUpdate(s);\n\n        StringBuilder buf = new StringBuilder(\"insert into COMPACTION_QUEUE (cq_id, cq_database, \" +\n            \"cq_table, \");\n        String partName = rqst.getPartitionname();\n        if (partName != null) buf.append(\"cq_partition, \");\n        buf.append(\"cq_state, cq_type\");\n        if (rqst.getRunas() != null) buf.append(\", cq_run_as\");\n        buf.append(\") values (\");\n        buf.append(id);\n        buf.append(\", '\");\n        buf.append(rqst.getDbname());\n        buf.append(\"', '\");\n        buf.append(rqst.getTablename());\n        buf.append(\"', '\");\n        if (partName != null) {\n          buf.append(partName);\n          buf.append(\"', '\");\n        }\n        buf.append(INITIATED_STATE);\n        buf.append(\"', '\");\n        switch (rqst.getType()) {\n          case MAJOR:\n            buf.append(MAJOR_TYPE);\n            break;\n\n          case MINOR:\n            buf.append(MINOR_TYPE);\n            break;\n\n          default:\n            LOG.debug(\"Going to rollback\");\n            dbConn.rollback();\n            throw new MetaException(\"Unexpected compaction type \" + rqst.getType().toString());\n        }\n        if (rqst.getRunas() != null) {\n          buf.append(\"', '\");\n          buf.append(rqst.getRunas());\n        }\n        buf.append(\"')\");\n        s = buf.toString();\n        LOG.debug(\"Going to execute update <\" + s + \">\");\n        stmt.executeUpdate(s);\n        LOG.debug(\"Going to commit\");\n        dbConn.commit();\n      } catch (SQLException e) {\n        try {\n          LOG.debug(\"Going to rollback\");\n          dbConn.rollback();\n        } catch (SQLException e1) {\n        }\n        detectDeadlock(e, \"compact\");\n        throw new MetaException(\"Unable to select from transaction database \" +\n            StringUtils.stringifyException(e));\n      } finally {\n        closeStmt(stmt);\n        closeDbConn(dbConn);\n      }\n    } catch (DeadlockException e) {\n      compact(rqst);\n    } finally {\n      deadlockCnt = 0;\n    }\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Missing",
                "sub_category": ""
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report mentions a Thrift error related to 'ShowCompactResponse' and 'compacts' being unset, which is in the same stack trace context as the ground truth method 'TxnHandler.compact'. However, it does not precisely identify the root cause in the 'compact' method. There is no fix suggestion provided in the bug report. The problem location is not precisely identified as the report does not mention the 'compact' method or any related methods outside of the stack trace. All information in the bug report is relevant to the context of the bug, so there is no wrong information."
        }
    },
    {
        "filename": "HIVE-12206.json",
        "code_diff": {
            "ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.exceptionThrown": {
                "code_before_change": "      public void exceptionThrown(Exception e) {\n        LOG.warn(org.apache.hadoop.util.StringUtils.stringifyException(e));\n        throw new RuntimeException(\"Cannot serialize object\", e);\n      }",
                "code_after_change": "      public void exceptionThrown(Exception e) {\n        LOG.warn(org.apache.hadoop.util.StringUtils.stringifyException(e));\n        throw new RuntimeException(\"Cannot serialize object\", e);\n      }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.deserializeObjectByJavaXML": {
                "code_before_change": "  private static <T> T deserializeObjectByJavaXML(InputStream in) {\n    XMLDecoder d = null;\n    try {\n      d = new XMLDecoder(in, null, null);\n      return (T) d.readObject();\n    } finally {\n      if (null != d) {\n        d.close();\n      }\n    }\n  }",
                "code_after_change": "  private static <T> T deserializeObjectByJavaXML(InputStream in) {\n    XMLDecoder d = null;\n    try {\n      d = new XMLDecoder(in, null, null);\n      return (T) d.readObject();\n    } finally {\n      if (null != d) {\n        d.close();\n      }\n    }\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies a ClassNotFoundException related to the class 'com.aginity.amp.hive.udf.UniqueNumberGenerator', which is mentioned in the stack trace. However, it does not precisely identify the root cause in the ground truth methods. The stack trace context includes methods like 'org.apache.hadoop.hive.ql.exec.Utilities.deserializeObjectByKryo', which is related but not the exact ground truth method. There is no fix suggestion provided in the bug report. The problem location is partially identified as it mentions methods in the stack trace context but not the exact ground truth methods. There is no wrong information in the bug report as it accurately describes the error encountered."
        }
    },
    {
        "filename": "HIVE-10098.json",
        "code_diff": {
            "ql.src.java.org.apache.hadoop.hive.ql.exec.SecureCmdDoAs.SecureCmdDoAs": {
                "code_before_change": [],
                "code_after_change": []
            },
            "shims.0.20S.src.main.java.org.apache.hadoop.hive.shims.Hadoop20SShims.readByteBuffer": {
                "code_before_change": [],
                "code_after_change": []
            },
            "shims.common.src.main.java.org.apache.hadoop.hive.shims.HadoopShimsSecure.checkFileAccess": {
                "code_before_change": "  public void checkFileAccess(FileSystem fs, FileStatus stat, FsAction action)\n      throws IOException, AccessControlException, Exception {\n    DefaultFileAccess.checkFileAccess(fs, stat, action);\n  }",
                "code_after_change": "  public void checkFileAccess(FileSystem fs, FileStatus stat, FsAction action)\n      throws IOException, AccessControlException, Exception {\n    DefaultFileAccess.checkFileAccess(fs, stat, action);\n  }"
            },
            "shims.0.23.src.main.java.org.apache.hadoop.hive.shims.Hadoop23Shims.readByteBuffer": {
                "code_before_change": [],
                "code_after_change": []
            },
            "shims.common.src.main.java.org.apache.hadoop.hive.shims.HadoopShims.getKeys": {
                "code_before_change": "    public List<String> getKeys() throws IOException;\n  }\n\n  /**\n   * This is a dummy class used when the hadoop version does not support hdfs encryption.\n   */\n  public static class NoopHdfsEncryptionShim implements HdfsEncryptionShim {",
                "code_after_change": "    public List<String> getKeys() throws IOException;\n  }\n\n  /**\n   * This is a dummy class used when the hadoop version does not support hdfs encryption.\n   */\n  public static class NoopHdfsEncryptionShim implements HdfsEncryptionShim {"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the issue occurring in the KMSClientProvider.addDelegationTokens method, which is part of the stack trace context shared with the ground truth methods. However, it does not precisely identify any of the ground truth methods as the root cause. There is no fix suggestion provided in the bug report. The problem location is partially identified as it mentions methods in the shared stack trace context but not the exact ground truth methods. There is no wrong information in the bug report as all details are relevant to the context of the bug."
        }
    },
    {
        "filename": "HIVE-7745.json",
        "code_diff": {
            "ql.src.java.org.apache.hadoop.hive.ql.parse.spark.GenSparkUtils.removeUnionOperators": {
                "code_before_change": "  public void removeUnionOperators(Configuration conf, GenSparkProcContext context,\n      BaseWork work)\n    throws SemanticException {\n\n    Set<Operator<?>> roots = work.getAllRootOperators();\n    if (work.getDummyOps() != null) {\n      roots.addAll(work.getDummyOps());\n    }\n\n    // need to clone the plan.\n    Set<Operator<?>> newRoots = Utilities.cloneOperatorTree(conf, roots);\n\n    // we're cloning the operator plan but we're retaining the original work. That means\n    // that root operators have to be replaced with the cloned ops. The replacement map\n    // tells you what that mapping is.\n    Map<Operator<?>, Operator<?>> replacementMap = new HashMap<Operator<?>, Operator<?>>();\n\n    // there's some special handling for dummyOps required. Mapjoins won't be properly\n    // initialized if their dummy parents aren't initialized. Since we cloned the plan\n    // we need to replace the dummy operators in the work with the cloned ones.\n    List<HashTableDummyOperator> dummyOps = new LinkedList<HashTableDummyOperator>();\n\n    Iterator<Operator<?>> it = newRoots.iterator();\n    for (Operator<?> orig: roots) {\n      Operator<?> newRoot = it.next();\n      if (newRoot instanceof HashTableDummyOperator) {\n        dummyOps.add((HashTableDummyOperator)newRoot);\n        it.remove();\n      } else {\n        replacementMap.put(orig,newRoot);\n      }\n    }\n\n    // now we remove all the unions. we throw away any branch that's not reachable from\n    // the current set of roots. The reason is that those branches will be handled in\n    // different tasks.\n    Deque<Operator<?>> operators = new LinkedList<Operator<?>>();\n    operators.addAll(newRoots);\n\n    Set<Operator<?>> seen = new HashSet<Operator<?>>();\n\n    while(!operators.isEmpty()) {\n      Operator<?> current = operators.pop();\n      seen.add(current);\n\n      if (current instanceof FileSinkOperator) {\n        FileSinkOperator fileSink = (FileSinkOperator)current;\n\n        // remember it for additional processing later\n        context.fileSinkSet.add(fileSink);\n\n        FileSinkDesc desc = fileSink.getConf();\n        Path path = desc.getDirName();\n        List<FileSinkDesc> linked;\n\n        if (!context.linkedFileSinks.containsKey(path)) {\n          linked = new ArrayList<FileSinkDesc>();\n          context.linkedFileSinks.put(path, linked);\n        }\n        linked = context.linkedFileSinks.get(path);\n        linked.add(desc);\n      }\n\n      if (current instanceof UnionOperator) {\n        Operator<?> parent = null;\n        int count = 0;\n\n        for (Operator<?> op: current.getParentOperators()) {\n          if (seen.contains(op)) {\n            ++count;\n            parent = op;\n          }\n        }\n\n        // we should have been able to reach the union from only one side.\n        Preconditions.checkArgument(count <= 1,\n            \"AssertionError: expected count to be <= 1, but was \" + count);\n\n        if (parent == null) {\n          // root operator is union (can happen in reducers)\n          replacementMap.put(current, current.getChildOperators().get(0));\n        } else {\n          parent.removeChildAndAdoptItsChildren(current);\n        }\n      }\n\n      if (current instanceof FileSinkOperator\n          || current instanceof ReduceSinkOperator) {\n        current.setChildOperators(null);\n      } else {\n        operators.addAll(current.getChildOperators());\n      }\n    }\n    work.setDummyOps(dummyOps);\n    work.replaceRoots(replacementMap);\n  }",
                "code_after_change": "  public void removeUnionOperators(Configuration conf, GenSparkProcContext context,\n      BaseWork work)\n    throws SemanticException {\n\n    Set<Operator<?>> roots = work.getAllRootOperators();\n    if (work.getDummyOps() != null) {\n      roots.addAll(work.getDummyOps());\n    }\n\n    // need to clone the plan.\n    Set<Operator<?>> newRoots = Utilities.cloneOperatorTree(conf, roots);\n\n    // we're cloning the operator plan but we're retaining the original work. That means\n    // that root operators have to be replaced with the cloned ops. The replacement map\n    // tells you what that mapping is.\n    Map<Operator<?>, Operator<?>> replacementMap = new HashMap<Operator<?>, Operator<?>>();\n\n    // there's some special handling for dummyOps required. Mapjoins won't be properly\n    // initialized if their dummy parents aren't initialized. Since we cloned the plan\n    // we need to replace the dummy operators in the work with the cloned ones.\n    List<HashTableDummyOperator> dummyOps = new LinkedList<HashTableDummyOperator>();\n\n    Iterator<Operator<?>> it = newRoots.iterator();\n    for (Operator<?> orig: roots) {\n      Operator<?> newRoot = it.next();\n      if (newRoot instanceof HashTableDummyOperator) {\n        dummyOps.add((HashTableDummyOperator)newRoot);\n        it.remove();\n      } else {\n        replacementMap.put(orig,newRoot);\n      }\n    }\n\n    // now we remove all the unions. we throw away any branch that's not reachable from\n    // the current set of roots. The reason is that those branches will be handled in\n    // different tasks.\n    Deque<Operator<?>> operators = new LinkedList<Operator<?>>();\n    operators.addAll(newRoots);\n\n    Set<Operator<?>> seen = new HashSet<Operator<?>>();\n\n    while(!operators.isEmpty()) {\n      Operator<?> current = operators.pop();\n      seen.add(current);\n\n      if (current instanceof FileSinkOperator) {\n        FileSinkOperator fileSink = (FileSinkOperator)current;\n\n        // remember it for additional processing later\n        context.fileSinkSet.add(fileSink);\n\n        FileSinkDesc desc = fileSink.getConf();\n        Path path = desc.getDirName();\n        List<FileSinkDesc> linked;\n\n        if (!context.linkedFileSinks.containsKey(path)) {\n          linked = new ArrayList<FileSinkDesc>();\n          context.linkedFileSinks.put(path, linked);\n        }\n        linked = context.linkedFileSinks.get(path);\n        linked.add(desc);\n        desc.setLinkedFileSinkDesc(linked);\n      }\n\n      if (current instanceof UnionOperator) {\n        Operator<?> parent = null;\n        int count = 0;\n\n        for (Operator<?> op: current.getParentOperators()) {\n          if (seen.contains(op)) {\n            ++count;\n            parent = op;\n          }\n        }\n\n        // we should have been able to reach the union from only one side.\n        Preconditions.checkArgument(count <= 1,\n            \"AssertionError: expected count to be <= 1, but was \" + count);\n\n        if (parent == null) {\n          // root operator is union (can happen in reducers)\n          replacementMap.put(current, current.getChildOperators().get(0));\n        } else {\n          parent.removeChildAndAdoptItsChildren(current);\n        }\n      }\n\n      if (current instanceof FileSinkOperator\n          || current instanceof ReduceSinkOperator) {\n        current.setChildOperators(null);\n      } else {\n        operators.addAll(current.getChildOperators());\n      }\n    }\n    work.setDummyOps(dummyOps);\n    work.replaceRoots(replacementMap);\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report mentions a NullPointerException occurring in the stack trace, specifically pointing to 'GenMapRedUtils.createMoveTask', which is in the same stack trace as the ground truth method 'GenSparkUtils.removeUnionOperators'. However, it does not precisely identify the root cause or the exact method where the fix was applied. There is no fix suggestion provided in the bug report. The problem location is partially identified as it is in the shared stack trace context but not the exact method. There is no wrong information in the bug report as it accurately describes the error and the context in which it occurs."
        }
    },
    {
        "filename": "HIVE-6990.json",
        "code_diff": {
            "metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getDatabase": {
                "code_before_change": [],
                "code_after_change": "  public Database getDatabase(String dbName) throws MetaException{\n    Query queryDbSelector = null;\n    Query queryDbParams = null;\n    try {\n      dbName = dbName.toLowerCase();\n\n      String queryTextDbSelector= \"select \"\n          + \"\\\"DB_ID\\\", \\\"NAME\\\", \\\"DB_LOCATION_URI\\\", \\\"DESC\\\", \"\n          + \"\\\"OWNER_NAME\\\", \\\"OWNER_TYPE\\\" \"\n          + \"FROM \"+ DBS +\" where \\\"NAME\\\" = ? \";\n      Object[] params = new Object[] { dbName };\n      queryDbSelector = pm.newQuery(\"javax.jdo.query.SQL\", queryTextDbSelector);\n\n      if (LOG.isTraceEnabled()) {\n        LOG.trace(\"getDatabase:query instantiated : \" + queryTextDbSelector\n            + \" with param [\" + params[0] + \"]\");\n      }\n\n      List<Object[]> sqlResult = executeWithArray(\n          queryDbSelector, params, queryTextDbSelector);\n      if ((sqlResult == null) || sqlResult.isEmpty()) {\n        return null;\n      }\n\n      assert(sqlResult.size() == 1);\n      if (sqlResult.get(0) == null) {\n        return null;\n      }\n\n      Object[] dbline = sqlResult.get(0);\n      Long dbid = extractSqlLong(dbline[0]);\n\n      String queryTextDbParams = \"select \\\"PARAM_KEY\\\", \\\"PARAM_VALUE\\\" \"\n          + \" from \" + DATABASE_PARAMS + \" \"\n          + \" WHERE \\\"DB_ID\\\" = ? \"\n          + \" AND \\\"PARAM_KEY\\\" IS NOT NULL\";\n      params[0] = dbid;\n      queryDbParams = pm.newQuery(\"javax.jdo.query.SQL\", queryTextDbParams);\n      if (LOG.isTraceEnabled()) {\n        LOG.trace(\"getDatabase:query2 instantiated : \" + queryTextDbParams\n            + \" with param [\" + params[0] + \"]\");\n      }\n\n      Map<String,String> dbParams = new HashMap<String,String>();\n      List<Object[]> sqlResult2 = ensureList(executeWithArray(\n          queryDbParams, params, queryTextDbParams));\n      if (!sqlResult2.isEmpty()) {\n        for (Object[] line : sqlResult2) {\n          dbParams.put(extractSqlString(line[0]), extractSqlString(line[1]));\n        }\n      }\n      Database db = new Database();\n      db.setName(extractSqlString(dbline[1]));\n      db.setLocationUri(extractSqlString(dbline[2]));\n      db.setDescription(extractSqlString(dbline[3]));\n      db.setOwnerName(extractSqlString(dbline[4]));\n      String type = extractSqlString(dbline[5]);\n      db.setOwnerType(\n          (null == type || type.trim().isEmpty()) ? null : PrincipalType.valueOf(type));\n      db.setParameters(MetaStoreUtils.trimMapNulls(dbParams,convertMapNullsToEmptyStrings));\n      if (LOG.isDebugEnabled()){\n        LOG.debug(\"getDatabase: directsql returning db \" + db.getName()\n            + \" locn[\"+db.getLocationUri()  +\"] desc [\" +db.getDescription()\n            + \"] owner [\" + db.getOwnerName() + \"] ownertype [\"+ db.getOwnerType() +\"]\");\n      }\n      return db;\n    } finally {\n      if (queryDbSelector != null){\n        queryDbSelector.closeAll();\n      }\n      if (queryDbParams != null){\n        queryDbParams.closeAll();\n      }\n    }\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getPartitionStats": {
                "code_before_change": "  public List<ColumnStatistics> getPartitionStats(String dbName, String tableName,\n      List<String> partNames, List<String> colNames) throws MetaException {\n    if (colNames.isEmpty() || partNames.isEmpty()) {\n      return Lists.newArrayList();\n    }\n    boolean doTrace = LOG.isDebugEnabled();\n    long start = doTrace ? System.nanoTime() : 0;\n    String queryText = \"select \\\"PARTITION_NAME\\\", \" + STATS_COLLIST + \" from \\\"PART_COL_STATS\\\"\"\n      + \" where \\\"DB_NAME\\\" = ? and \\\"TABLE_NAME\\\" = ? and \\\"COLUMN_NAME\\\" in (\"\n      + makeParams(colNames.size()) + \") AND \\\"PARTITION_NAME\\\" in (\"\n      + makeParams(partNames.size()) + \") order by \\\"PARTITION_NAME\\\"\";\n\n    Query query = pm.newQuery(\"javax.jdo.query.SQL\", queryText);\n    Object[] params = new Object[colNames.size() + partNames.size() + 2];\n    int paramI = 0;\n    params[paramI++] = dbName;\n    params[paramI++] = tableName;\n    for (String colName : colNames) {\n      params[paramI++] = colName;\n    }\n    for (String partName : partNames) {\n      params[paramI++] = partName;\n    }\n    Object qResult = query.executeWithArray(params);\n    long queryTime = doTrace ? System.nanoTime() : 0;\n    if (qResult == null) {\n      query.closeAll();\n      return Lists.newArrayList();\n    }\n    List<Object[]> list = ensureList(qResult);\n    List<ColumnStatistics> result = new ArrayList<ColumnStatistics>(\n        Math.min(list.size(), partNames.size()));\n    String lastPartName = null;\n    int from = 0;\n    for (int i = 0; i <= list.size(); ++i) {\n      boolean isLast = i == list.size();\n      String partName = isLast ? null : (String)list.get(i)[0];\n      if (!isLast && partName.equals(lastPartName)) {\n        continue;\n      } else if (from != i) {\n        ColumnStatisticsDesc csd = new ColumnStatisticsDesc(false, dbName, tableName);\n        csd.setPartName(lastPartName);\n        result.add(makeColumnStats(list.subList(from, i), csd, 1));\n      }\n      lastPartName = partName;\n      from = i;\n    }\n\n    timingTrace(doTrace, queryText, start, queryTime);\n    query.closeAll();\n    return result;\n  }",
                "code_after_change": "  public List<ColumnStatistics> getPartitionStats(final String dbName, final String tableName,\n      final List<String> partNames, List<String> colNames) throws MetaException {\n    if (colNames.isEmpty() || partNames.isEmpty()) {\n      return Collections.emptyList();\n    }\n    final boolean doTrace = LOG.isDebugEnabled();\n    final String queryText0 = \"select \\\"PARTITION_NAME\\\", \" + STATS_COLLIST + \" from \"\n      + \" \" + PART_COL_STATS + \" where \\\"DB_NAME\\\" = ? and \\\"TABLE_NAME\\\" = ? and \\\"COLUMN_NAME\\\"\"\n      + \"  in (%1$s) AND \\\"PARTITION_NAME\\\" in (%2$s) order by \\\"PARTITION_NAME\\\"\";\n    Batchable<String, Object[]> b = new Batchable<String, Object[]>() {\n      public List<Object[]> run(final List<String> inputColNames) throws MetaException {\n        Batchable<String, Object[]> b2 = new Batchable<String, Object[]>() {\n          public List<Object[]> run(List<String> inputPartNames) throws MetaException {\n            String queryText = String.format(queryText0,\n                makeParams(inputColNames.size()), makeParams(inputPartNames.size()));\n            long start = doTrace ? System.nanoTime() : 0;\n            Query query = pm.newQuery(\"javax.jdo.query.SQL\", queryText);\n            Object qResult = executeWithArray(query, prepareParams(\n                dbName, tableName, inputPartNames, inputColNames), queryText);\n            timingTrace(doTrace, queryText0, start, (doTrace ? System.nanoTime() : 0));\n            if (qResult == null) {\n              query.closeAll();\n              return Collections.emptyList();\n            }\n            addQueryAfterUse(query);\n            return ensureList(qResult);\n          }\n        };\n        try {\n          return runBatched(partNames, b2);\n        } finally {\n          addQueryAfterUse(b2);\n        }\n      }\n    };\n    List<Object[]> list = runBatched(colNames, b);\n\n    List<ColumnStatistics> result = new ArrayList<ColumnStatistics>(\n        Math.min(list.size(), partNames.size()));\n    String lastPartName = null;\n    int from = 0;\n    for (int i = 0; i <= list.size(); ++i) {\n      boolean isLast = i == list.size();\n      String partName = isLast ? null : (String)list.get(i)[0];\n      if (!isLast && partName.equals(lastPartName)) {\n        continue;\n      } else if (from != i) {\n        ColumnStatisticsDesc csd = new ColumnStatisticsDesc(false, dbName, tableName);\n        csd.setPartName(lastPartName);\n        result.add(makeColumnStats(list.subList(from, i), csd, 1));\n      }\n      lastPartName = partName;\n      from = i;\n      Deadline.checkTimeout();\n    }\n    b.closeAllQueries();\n    return result;\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreDirectSql.MetaStoreDirectSql": {
                "code_before_change": [],
                "code_after_change": []
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreDirectSql.apply": {
                "code_before_change": "      public void apply(Partition t, Object[] fields) {\n        t.putToParameters((String)fields[1], (String)fields[2]);\n      }});\n\n    queryText = \"select \\\"PART_ID\\\", \\\"PART_KEY_VAL\\\" from \\\"PARTITION_KEY_VALS\\\"\"\n        + \" where \\\"PART_ID\\\" in (\" + partIds + \") and \\\"INTEGER_IDX\\\" >= 0\"\n        + \" order by \\\"PART_ID\\\" asc, \\\"INTEGER_IDX\\\" asc\";\n    loopJoinOrderedResult(partitions, queryText, 0, new ApplyFunc<Partition>() {",
                "code_after_change": "      public void apply(Partition t, Object[] fields) {\n        t.putToParameters((String)fields[1], (String)fields[2]);\n      }});\n    // Perform conversion of null map values\n    for (Partition t : partitions.values()) {"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getForeignKeys": {
                "code_before_change": [],
                "code_after_change": "  public List<SQLForeignKey> getForeignKeys(String parent_db_name, String parent_tbl_name, String foreign_db_name, String foreign_tbl_name) throws MetaException {\n    List<SQLForeignKey> ret = new ArrayList<SQLForeignKey>();\n    String queryText =\n      \"SELECT  \\\"D2\\\".\\\"NAME\\\", \\\"T2\\\".\\\"TBL_NAME\\\", \\\"C2\\\".\\\"COLUMN_NAME\\\",\"\n      + \"\" + DBS + \".\\\"NAME\\\", \" + TBLS + \".\\\"TBL_NAME\\\", \" + COLUMNS_V2 + \".\\\"COLUMN_NAME\\\", \"\n      + \"\" + KEY_CONSTRAINTS + \".\\\"POSITION\\\", \" + KEY_CONSTRAINTS + \".\\\"UPDATE_RULE\\\", \" + KEY_CONSTRAINTS + \".\\\"DELETE_RULE\\\", \"\n      + \"\" + KEY_CONSTRAINTS + \".\\\"CONSTRAINT_NAME\\\" , \\\"KEY_CONSTRAINTS2\\\".\\\"CONSTRAINT_NAME\\\", \" + KEY_CONSTRAINTS + \".\\\"ENABLE_VALIDATE_RELY\\\" \"\n      + \" from \" + TBLS + \" \"\n      + \" INNER join \" + KEY_CONSTRAINTS + \" ON \" + TBLS + \".\\\"TBL_ID\\\" = \" + KEY_CONSTRAINTS + \".\\\"CHILD_TBL_ID\\\" \"\n      + \" INNER join \" + KEY_CONSTRAINTS + \" \\\"KEY_CONSTRAINTS2\\\" ON \\\"KEY_CONSTRAINTS2\\\".\\\"PARENT_TBL_ID\\\"  = \" + KEY_CONSTRAINTS + \".\\\"PARENT_TBL_ID\\\" \"\n      + \" AND \\\"KEY_CONSTRAINTS2\\\".\\\"PARENT_CD_ID\\\"  = \" + KEY_CONSTRAINTS + \".\\\"PARENT_CD_ID\\\" AND \"\n      + \" \\\"KEY_CONSTRAINTS2\\\".\\\"PARENT_INTEGER_IDX\\\"  = \" + KEY_CONSTRAINTS + \".\\\"PARENT_INTEGER_IDX\\\" \"\n      + \" INNER join \" + DBS + \" ON \" + TBLS + \".\\\"DB_ID\\\" = \" + DBS + \".\\\"DB_ID\\\" \"\n      + \" INNER join \" + TBLS + \" \\\"T2\\\" ON  \" + KEY_CONSTRAINTS + \".\\\"PARENT_TBL_ID\\\" = \\\"T2\\\".\\\"TBL_ID\\\" \"\n      + \" INNER join \" + DBS + \" \\\"D2\\\" ON \\\"T2\\\".\\\"DB_ID\\\" = \\\"D2\\\".\\\"DB_ID\\\" \"\n      + \" INNER JOIN \" + COLUMNS_V2 + \"  ON \" + COLUMNS_V2 + \".\\\"CD_ID\\\" = \" + KEY_CONSTRAINTS + \".\\\"CHILD_CD_ID\\\" AND \"\n      + \" \" + COLUMNS_V2 + \".\\\"INTEGER_IDX\\\" = \" + KEY_CONSTRAINTS + \".\\\"CHILD_INTEGER_IDX\\\" \"\n      + \" INNER JOIN \" + COLUMNS_V2 + \" \\\"C2\\\" ON \\\"C2\\\".\\\"CD_ID\\\" = \" + KEY_CONSTRAINTS + \".\\\"PARENT_CD_ID\\\" AND \"\n      + \" \\\"C2\\\".\\\"INTEGER_IDX\\\" = \" + KEY_CONSTRAINTS + \".\\\"PARENT_INTEGER_IDX\\\" \"\n      + \" WHERE \" + KEY_CONSTRAINTS + \".\\\"CONSTRAINT_TYPE\\\" = \"\n      + MConstraint.FOREIGN_KEY_CONSTRAINT\n      + \" AND \\\"KEY_CONSTRAINTS2\\\".\\\"CONSTRAINT_TYPE\\\" = \"\n      + MConstraint.PRIMARY_KEY_CONSTRAINT + \" AND\"\n      + (foreign_db_name == null ? \"\" : \" \" + DBS + \".\\\"NAME\\\" = ? AND\")\n      + (foreign_tbl_name == null ? \"\" : \" \" + TBLS + \".\\\"TBL_NAME\\\" = ? AND\")\n      + (parent_tbl_name == null ? \"\" : \" \\\"T2\\\".\\\"TBL_NAME\\\" = ? AND\")\n      + (parent_db_name == null ? \"\" : \" \\\"D2\\\".\\\"NAME\\\" = ?\") ;\n\n    queryText = queryText.trim();\n    if (queryText.endsWith(\"AND\")) {\n      queryText = queryText.substring(0, queryText.length()-3);\n    }\n    List<String> pms = new ArrayList<String>();\n    if (foreign_db_name != null) {\n      pms.add(foreign_db_name);\n    }\n    if (foreign_tbl_name != null) {\n      pms.add(foreign_tbl_name);\n    }\n    if (parent_tbl_name != null) {\n      pms.add(parent_tbl_name);\n    }\n    if (parent_db_name != null) {\n      pms.add(parent_db_name);\n    }\n\n    Query queryParams = pm.newQuery(\"javax.jdo.query.SQL\", queryText);\n      List<Object[]> sqlResult = ensureList(executeWithArray(\n        queryParams, pms.toArray(), queryText));\n\n    if (!sqlResult.isEmpty()) {\n      for (Object[] line : sqlResult) {\n        int enableValidateRely = extractSqlInt(line[11]);\n        boolean enable = (enableValidateRely & 4) != 0;\n        boolean validate = (enableValidateRely & 2) != 0;\n        boolean rely = (enableValidateRely & 1) != 0;\n        SQLForeignKey currKey = new SQLForeignKey(\n          extractSqlString(line[0]),\n          extractSqlString(line[1]),\n          extractSqlString(line[2]),\n          extractSqlString(line[3]),\n          extractSqlString(line[4]),\n          extractSqlString(line[5]),\n          extractSqlInt(line[6]),\n          extractSqlInt(line[7]),\n          extractSqlInt(line[8]),\n          extractSqlString(line[9]),\n          extractSqlString(line[10]),\n          enable,\n          validate,\n          rely\n          );\n          ret.add(currKey);\n      }\n    }\n    return ret;\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreDirectSql.columnStatisticsObjForPartitionsBatch": {
                "code_before_change": [],
                "code_after_change": "  private List<ColumnStatisticsObj> columnStatisticsObjForPartitionsBatch(String dbName,\n      String tableName, List<String> partNames, List<String> colNames, boolean areAllPartsFound,\n      boolean useDensityFunctionForNDVEstimation, double ndvTuner) throws MetaException {\n    // TODO: all the extrapolation logic should be moved out of this class,\n    // only mechanical data retrieval should remain here.\n    String commonPrefix = \"select \\\"COLUMN_NAME\\\", \\\"COLUMN_TYPE\\\", \"\n        + \"min(\\\"LONG_LOW_VALUE\\\"), max(\\\"LONG_HIGH_VALUE\\\"), min(\\\"DOUBLE_LOW_VALUE\\\"), max(\\\"DOUBLE_HIGH_VALUE\\\"), \"\n        + \"min(cast(\\\"BIG_DECIMAL_LOW_VALUE\\\" as decimal)), max(cast(\\\"BIG_DECIMAL_HIGH_VALUE\\\" as decimal)), \"\n        + \"sum(\\\"NUM_NULLS\\\"), max(\\\"NUM_DISTINCTS\\\"), \"\n        + \"max(\\\"AVG_COL_LEN\\\"), max(\\\"MAX_COL_LEN\\\"), sum(\\\"NUM_TRUES\\\"), sum(\\\"NUM_FALSES\\\"), \"\n        // The following data is used to compute a partitioned table's NDV based\n        // on partitions' NDV when useDensityFunctionForNDVEstimation = true. Global NDVs cannot be\n        // accurately derived from partition NDVs, because the domain of column value two partitions\n        // can overlap. If there is no overlap then global NDV is just the sum\n        // of partition NDVs (UpperBound). But if there is some overlay then\n        // global NDV can be anywhere between sum of partition NDVs (no overlap)\n        // and same as one of the partition NDV (domain of column value in all other\n        // partitions is subset of the domain value in one of the partition)\n        // (LowerBound).But under uniform distribution, we can roughly estimate the global\n        // NDV by leveraging the min/max values.\n        // And, we also guarantee that the estimation makes sense by comparing it to the\n        // UpperBound (calculated by \"sum(\\\"NUM_DISTINCTS\\\")\")\n        // and LowerBound (calculated by \"max(\\\"NUM_DISTINCTS\\\")\")\n        + \"avg((\\\"LONG_HIGH_VALUE\\\"-\\\"LONG_LOW_VALUE\\\")/cast(\\\"NUM_DISTINCTS\\\" as decimal)),\"\n        + \"avg((\\\"DOUBLE_HIGH_VALUE\\\"-\\\"DOUBLE_LOW_VALUE\\\")/\\\"NUM_DISTINCTS\\\"),\"\n        + \"avg((cast(\\\"BIG_DECIMAL_HIGH_VALUE\\\" as decimal)-cast(\\\"BIG_DECIMAL_LOW_VALUE\\\" as decimal))/\\\"NUM_DISTINCTS\\\"),\"\n        + \"sum(\\\"NUM_DISTINCTS\\\")\" + \" from \" + PART_COL_STATS + \"\"\n        + \" where \\\"DB_NAME\\\" = ? and \\\"TABLE_NAME\\\" = ? \";\n    String queryText = null;\n    long start = 0;\n    long end = 0;\n    Query query = null;\n    boolean doTrace = LOG.isDebugEnabled();\n    Object qResult = null;\n    ForwardQueryResult<?> fqr = null;\n    // Check if the status of all the columns of all the partitions exists\n    // Extrapolation is not needed.\n    if (areAllPartsFound) {\n      queryText = commonPrefix + \" and \\\"COLUMN_NAME\\\" in (\" + makeParams(colNames.size()) + \")\"\n          + \" and \\\"PARTITION_NAME\\\" in (\" + makeParams(partNames.size()) + \")\"\n          + \" group by \\\"COLUMN_NAME\\\", \\\"COLUMN_TYPE\\\"\";\n      start = doTrace ? System.nanoTime() : 0;\n      query = pm.newQuery(\"javax.jdo.query.SQL\", queryText);\n      qResult = executeWithArray(query, prepareParams(dbName, tableName, partNames, colNames),\n          queryText);\n      if (qResult == null) {\n        query.closeAll();\n        return Collections.emptyList();\n      }\n      end = doTrace ? System.nanoTime() : 0;\n      timingTrace(doTrace, queryText, start, end);\n      List<Object[]> list = ensureList(qResult);\n      List<ColumnStatisticsObj> colStats = new ArrayList<ColumnStatisticsObj>(list.size());\n      for (Object[] row : list) {\n        colStats.add(prepareCSObjWithAdjustedNDV(row, 0, useDensityFunctionForNDVEstimation, ndvTuner));\n        Deadline.checkTimeout();\n      }\n      query.closeAll();\n      return colStats;\n    } else {\n      // Extrapolation is needed for some columns.\n      // In this case, at least a column status for a partition is missing.\n      // We need to extrapolate this partition based on the other partitions\n      List<ColumnStatisticsObj> colStats = new ArrayList<ColumnStatisticsObj>(colNames.size());\n      queryText = \"select \\\"COLUMN_NAME\\\", \\\"COLUMN_TYPE\\\", count(\\\"PARTITION_NAME\\\") \"\n          + \" from \" + PART_COL_STATS\n          + \" where \\\"DB_NAME\\\" = ? and \\\"TABLE_NAME\\\" = ? \"\n          + \" and \\\"COLUMN_NAME\\\" in (\" + makeParams(colNames.size()) + \")\"\n          + \" and \\\"PARTITION_NAME\\\" in (\" + makeParams(partNames.size()) + \")\"\n          + \" group by \\\"COLUMN_NAME\\\", \\\"COLUMN_TYPE\\\"\";\n      start = doTrace ? System.nanoTime() : 0;\n      query = pm.newQuery(\"javax.jdo.query.SQL\", queryText);\n      qResult = executeWithArray(query, prepareParams(dbName, tableName, partNames, colNames),\n          queryText);\n      end = doTrace ? System.nanoTime() : 0;\n      timingTrace(doTrace, queryText, start, end);\n      if (qResult == null) {\n        query.closeAll();\n        return Collections.emptyList();\n      }\n      List<String> noExtraColumnNames = new ArrayList<String>();\n      Map<String, String[]> extraColumnNameTypeParts = new HashMap<String, String[]>();\n      List<Object[]> list = ensureList(qResult);\n      for (Object[] row : list) {\n        String colName = (String) row[0];\n        String colType = (String) row[1];\n        // Extrapolation is not needed for this column if\n        // count(\\\"PARTITION_NAME\\\")==partNames.size()\n        // Or, extrapolation is not possible for this column if\n        // count(\\\"PARTITION_NAME\\\")<2\n        Long count = extractSqlLong(row[2]);\n        if (count == partNames.size() || count < 2) {\n          noExtraColumnNames.add(colName);\n        } else {\n          extraColumnNameTypeParts.put(colName, new String[] { colType, String.valueOf(count) });\n        }\n        Deadline.checkTimeout();\n      }\n      query.closeAll();\n      // Extrapolation is not needed for columns noExtraColumnNames\n      if (noExtraColumnNames.size() != 0) {\n        queryText = commonPrefix + \" and \\\"COLUMN_NAME\\\" in (\"\n            + makeParams(noExtraColumnNames.size()) + \")\" + \" and \\\"PARTITION_NAME\\\" in (\"\n            + makeParams(partNames.size()) + \")\" + \" group by \\\"COLUMN_NAME\\\", \\\"COLUMN_TYPE\\\"\";\n        start = doTrace ? System.nanoTime() : 0;\n        query = pm.newQuery(\"javax.jdo.query.SQL\", queryText);\n        qResult = executeWithArray(query,\n            prepareParams(dbName, tableName, partNames, noExtraColumnNames), queryText);\n        if (qResult == null) {\n          query.closeAll();\n          return Collections.emptyList();\n        }\n        list = ensureList(qResult);\n        for (Object[] row : list) {\n          colStats.add(prepareCSObjWithAdjustedNDV(row, 0, useDensityFunctionForNDVEstimation, ndvTuner));\n          Deadline.checkTimeout();\n        }\n        end = doTrace ? System.nanoTime() : 0;\n        timingTrace(doTrace, queryText, start, end);\n        query.closeAll();\n      }\n      // Extrapolation is needed for extraColumnNames.\n      // give a sequence number for all the partitions\n      if (extraColumnNameTypeParts.size() != 0) {\n        Map<String, Integer> indexMap = new HashMap<String, Integer>();\n        for (int index = 0; index < partNames.size(); index++) {\n          indexMap.put(partNames.get(index), index);\n        }\n        // get sum for all columns to reduce the number of queries\n        Map<String, Map<Integer, Object>> sumMap = new HashMap<String, Map<Integer, Object>>();\n        queryText = \"select \\\"COLUMN_NAME\\\", sum(\\\"NUM_NULLS\\\"), sum(\\\"NUM_TRUES\\\"), sum(\\\"NUM_FALSES\\\"), sum(\\\"NUM_DISTINCTS\\\")\"\n            + \" from \" + PART_COL_STATS + \" where \\\"DB_NAME\\\" = ? and \\\"TABLE_NAME\\\" = ? \"\n            + \" and \\\"COLUMN_NAME\\\" in (\" + makeParams(extraColumnNameTypeParts.size())\n            + \") and \\\"PARTITION_NAME\\\" in (\" + makeParams(partNames.size())\n            + \") group by \\\"COLUMN_NAME\\\"\";\n        start = doTrace ? System.nanoTime() : 0;\n        query = pm.newQuery(\"javax.jdo.query.SQL\", queryText);\n        List<String> extraColumnNames = new ArrayList<String>();\n        extraColumnNames.addAll(extraColumnNameTypeParts.keySet());\n        qResult = executeWithArray(query,\n            prepareParams(dbName, tableName, partNames, extraColumnNames), queryText);\n        if (qResult == null) {\n          query.closeAll();\n          return Collections.emptyList();\n        }\n        list = ensureList(qResult);\n        // see the indexes for colstats in IExtrapolatePartStatus\n        Integer[] sumIndex = new Integer[] { 6, 10, 11, 15 };\n        for (Object[] row : list) {\n          Map<Integer, Object> indexToObject = new HashMap<Integer, Object>();\n          for (int ind = 1; ind < row.length; ind++) {\n            indexToObject.put(sumIndex[ind - 1], row[ind]);\n          }\n          // row[0] is the column name\n          sumMap.put((String) row[0], indexToObject);\n          Deadline.checkTimeout();\n        }\n        end = doTrace ? System.nanoTime() : 0;\n        timingTrace(doTrace, queryText, start, end);\n        query.closeAll();\n        for (Map.Entry<String, String[]> entry : extraColumnNameTypeParts.entrySet()) {\n          Object[] row = new Object[IExtrapolatePartStatus.colStatNames.length + 2];\n          String colName = entry.getKey();\n          String colType = entry.getValue()[0];\n          Long sumVal = Long.parseLong(entry.getValue()[1]);\n          // fill in colname\n          row[0] = colName;\n          // fill in coltype\n          row[1] = colType;\n          // use linear extrapolation. more complicated one can be added in the\n          // future.\n          IExtrapolatePartStatus extrapolateMethod = new LinearExtrapolatePartStatus();\n          // fill in colstatus\n          Integer[] index = null;\n          boolean decimal = false;\n          if (colType.toLowerCase().startsWith(\"decimal\")) {\n            index = IExtrapolatePartStatus.indexMaps.get(\"decimal\");\n            decimal = true;\n          } else {\n            index = IExtrapolatePartStatus.indexMaps.get(colType.toLowerCase());\n          }\n          // if the colType is not the known type, long, double, etc, then get\n          // all index.\n          if (index == null) {\n            index = IExtrapolatePartStatus.indexMaps.get(\"default\");\n          }\n          for (int colStatIndex : index) {\n            String colStatName = IExtrapolatePartStatus.colStatNames[colStatIndex];\n            // if the aggregation type is sum, we do a scale-up\n            if (IExtrapolatePartStatus.aggrTypes[colStatIndex] == IExtrapolatePartStatus.AggrType.Sum) {\n              Object o = sumMap.get(colName).get(colStatIndex);\n              if (o == null) {\n                row[2 + colStatIndex] = null;\n              } else {\n                Long val = extractSqlLong(o);\n                row[2 + colStatIndex] = (Long) (val / sumVal * (partNames.size()));\n              }\n            } else if (IExtrapolatePartStatus.aggrTypes[colStatIndex] == IExtrapolatePartStatus.AggrType.Min\n                || IExtrapolatePartStatus.aggrTypes[colStatIndex] == IExtrapolatePartStatus.AggrType.Max) {\n              // if the aggregation type is min/max, we extrapolate from the\n              // left/right borders\n              if (!decimal) {\n                queryText = \"select \\\"\" + colStatName\n                    + \"\\\",\\\"PARTITION_NAME\\\" from \" + PART_COL_STATS\n                    + \" where \\\"DB_NAME\\\" = ? and \\\"TABLE_NAME\\\" = ?\" + \" and \\\"COLUMN_NAME\\\" = ?\"\n                    + \" and \\\"PARTITION_NAME\\\" in (\" + makeParams(partNames.size()) + \")\"\n                    + \" order by \\\"\" + colStatName + \"\\\"\";\n              } else {\n                queryText = \"select \\\"\" + colStatName\n                    + \"\\\",\\\"PARTITION_NAME\\\" from \" + PART_COL_STATS\n                    + \" where \\\"DB_NAME\\\" = ? and \\\"TABLE_NAME\\\" = ?\" + \" and \\\"COLUMN_NAME\\\" = ?\"\n                    + \" and \\\"PARTITION_NAME\\\" in (\" + makeParams(partNames.size()) + \")\"\n                    + \" order by cast(\\\"\" + colStatName + \"\\\" as decimal)\";\n              }\n              start = doTrace ? System.nanoTime() : 0;\n              query = pm.newQuery(\"javax.jdo.query.SQL\", queryText);\n              qResult = executeWithArray(query,\n                  prepareParams(dbName, tableName, partNames, Arrays.asList(colName)), queryText);\n              if (qResult == null) {\n                query.closeAll();\n                return Collections.emptyList();\n              }\n              fqr = (ForwardQueryResult<?>) qResult;\n              Object[] min = (Object[]) (fqr.get(0));\n              Object[] max = (Object[]) (fqr.get(fqr.size() - 1));\n              end = doTrace ? System.nanoTime() : 0;\n              timingTrace(doTrace, queryText, start, end);\n              query.closeAll();\n              if (min[0] == null || max[0] == null) {\n                row[2 + colStatIndex] = null;\n              } else {\n                row[2 + colStatIndex] = extrapolateMethod.extrapolate(min, max, colStatIndex,\n                    indexMap);\n              }\n            } else {\n              // if the aggregation type is avg, we use the average on the existing ones.\n              queryText = \"select \"\n                  + \"avg((\\\"LONG_HIGH_VALUE\\\"-\\\"LONG_LOW_VALUE\\\")/cast(\\\"NUM_DISTINCTS\\\" as decimal)),\"\n                  + \"avg((\\\"DOUBLE_HIGH_VALUE\\\"-\\\"DOUBLE_LOW_VALUE\\\")/\\\"NUM_DISTINCTS\\\"),\"\n                  + \"avg((cast(\\\"BIG_DECIMAL_HIGH_VALUE\\\" as decimal)-cast(\\\"BIG_DECIMAL_LOW_VALUE\\\" as decimal))/\\\"NUM_DISTINCTS\\\")\"\n                  + \" from \" + PART_COL_STATS + \"\" + \" where \\\"DB_NAME\\\" = ? and \\\"TABLE_NAME\\\" = ?\"\n                  + \" and \\\"COLUMN_NAME\\\" = ?\" + \" and \\\"PARTITION_NAME\\\" in (\"\n                  + makeParams(partNames.size()) + \")\" + \" group by \\\"COLUMN_NAME\\\"\";\n              start = doTrace ? System.nanoTime() : 0;\n              query = pm.newQuery(\"javax.jdo.query.SQL\", queryText);\n              qResult = executeWithArray(query,\n                  prepareParams(dbName, tableName, partNames, Arrays.asList(colName)), queryText);\n              if (qResult == null) {\n                query.closeAll();\n                return Collections.emptyList();\n              }\n              fqr = (ForwardQueryResult<?>) qResult;\n              Object[] avg = (Object[]) (fqr.get(0));\n              // colStatIndex=12,13,14 respond to \"AVG_LONG\", \"AVG_DOUBLE\",\n              // \"AVG_DECIMAL\"\n              row[2 + colStatIndex] = avg[colStatIndex - 12];\n              end = doTrace ? System.nanoTime() : 0;\n              timingTrace(doTrace, queryText, start, end);\n              query.closeAll();\n            }\n          }\n          colStats.add(prepareCSObjWithAdjustedNDV(row, 0, useDensityFunctionForNDVEstimation, ndvTuner));\n          Deadline.checkTimeout();\n        }\n      }\n      return colStats;\n    }\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreDirectSql.isViewTable": {
                "code_before_change": "  private static Boolean isViewTable(Table t) {\n    return t.isSetTableType() ?\n        t.getTableType().equals(TableType.VIRTUAL_VIEW.toString()) : null;\n  }",
                "code_after_change": "  private static Boolean isViewTable(Table t) {\n    return t.isSetTableType() ?\n        t.getTableType().equals(TableType.VIRTUAL_VIEW.toString()) : null;\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreDirectSql.generateSqlFilter": {
                "code_before_change": "    public static String generateSqlFilter(Table table,\n        ExpressionTree tree, List<Object> params, List<String> joins) throws MetaException {\n      assert table != null;\n      if (tree.getRoot() == null) {\n        return \"\";\n      }\n      PartitionFilterGenerator visitor = new PartitionFilterGenerator(table, params, joins);\n      tree.accept(visitor);\n      if (visitor.filterBuffer.hasError()) {\n        LOG.info(\"Unable to push down SQL filter: \" + visitor.filterBuffer.getErrorMessage());\n        return null;\n      }\n\n      // Some joins might be null (see processNode for LeafNode), clean them up.\n      for (int i = 0; i < joins.size(); ++i) {\n        if (joins.get(i) != null) continue;\n        joins.remove(i--);\n      }\n      return \"(\" + visitor.filterBuffer.getFilter() + \")\";\n    }",
                "code_after_change": "    private static String generateSqlFilter(Table table, ExpressionTree tree, List<Object> params,\n        List<String> joins, boolean dbHasJoinCastBug, String defaultPartName,\n        DatabaseProduct dbType, String schema) throws MetaException {\n      assert table != null;\n      if (tree == null) {\n        // consistent with other APIs like makeExpressionTree, null is returned to indicate that\n        // the filter could not pushed down due to parsing issue etc\n        return null;\n      }\n      if (tree.getRoot() == null) {\n        return \"\";\n      }\n      PartitionFilterGenerator visitor = new PartitionFilterGenerator(\n          table, params, joins, dbHasJoinCastBug, defaultPartName, dbType, schema);\n      tree.accept(visitor);\n      if (visitor.filterBuffer.hasError()) {\n        LOG.info(\"Unable to push down SQL filter: \" + visitor.filterBuffer.getErrorMessage());\n        return null;\n      }\n\n      // Some joins might be null (see processNode for LeafNode), clean them up.\n      for (int i = 0; i < joins.size(); ++i) {\n        if (joins.get(i) != null) continue;\n        joins.remove(i--);\n      }\n      return \"(\" + visitor.filterBuffer.getFilter() + \")\";\n    }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreDirectSql.partsFoundForPartitions": {
                "code_before_change": [],
                "code_after_change": "  private long partsFoundForPartitions(final String dbName, final String tableName,\n      final List<String> partNames, List<String> colNames) throws MetaException {\n    assert !colNames.isEmpty() && !partNames.isEmpty();\n    final boolean doTrace = LOG.isDebugEnabled();\n    final String queryText0  = \"select count(\\\"COLUMN_NAME\\\") from \" + PART_COL_STATS + \"\"\n        + \" where \\\"DB_NAME\\\" = ? and \\\"TABLE_NAME\\\" = ? \"\n        + \" and \\\"COLUMN_NAME\\\" in (%1$s) and \\\"PARTITION_NAME\\\" in (%2$s)\"\n        + \" group by \\\"PARTITION_NAME\\\"\";\n    List<Long> allCounts = runBatched(colNames, new Batchable<String, Long>() {\n      public List<Long> run(final List<String> inputColName) throws MetaException {\n        return runBatched(partNames, new Batchable<String, Long>() {\n          public List<Long> run(List<String> inputPartNames) throws MetaException {\n            long partsFound = 0;\n            String queryText = String.format(queryText0,\n                makeParams(inputColName.size()), makeParams(inputPartNames.size()));\n            long start = doTrace ? System.nanoTime() : 0;\n            Query query = pm.newQuery(\"javax.jdo.query.SQL\", queryText);\n            try {\n              Object qResult = executeWithArray(query, prepareParams(\n                  dbName, tableName, inputPartNames, inputColName), queryText);\n              long end = doTrace ? System.nanoTime() : 0;\n              timingTrace(doTrace, queryText, start, end);\n              ForwardQueryResult<?> fqr = (ForwardQueryResult<?>) qResult;\n              Iterator<?> iter = fqr.iterator();\n              while (iter.hasNext()) {\n                if (extractSqlLong(iter.next()) == inputColName.size()) {\n                  partsFound++;\n                }\n              }\n              return Lists.<Long>newArrayList(partsFound);\n            } finally {\n              query.closeAll();\n            }\n          }\n        });\n      }\n    });\n    long partsFound = 0;\n    for (Long val : allCounts) {\n      partsFound += val;\n    }\n    return partsFound;\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getNotNullConstraints": {
                "code_before_change": [],
                "code_after_change": "  public List<SQLNotNullConstraint> getNotNullConstraints(String db_name, String tbl_name)\n          throws MetaException {\n    List<SQLNotNullConstraint> ret = new ArrayList<SQLNotNullConstraint>();\n    String queryText =\n      \"SELECT \" + DBS + \".\\\"NAME\\\", \" + TBLS + \".\\\"TBL_NAME\\\", \" + COLUMNS_V2 + \".\\\"COLUMN_NAME\\\",\"\n      + \"\" + KEY_CONSTRAINTS + \".\\\"CONSTRAINT_NAME\\\", \" + KEY_CONSTRAINTS + \".\\\"ENABLE_VALIDATE_RELY\\\" \"\n      + \" from \" + TBLS + \" \"\n      + \" INNER join \" + KEY_CONSTRAINTS + \" ON \" + TBLS + \".\\\"TBL_ID\\\" = \" + KEY_CONSTRAINTS + \".\\\"PARENT_TBL_ID\\\" \"\n      + \" INNER join \" + DBS + \" ON \" + TBLS + \".\\\"DB_ID\\\" = \" + DBS + \".\\\"DB_ID\\\" \"\n      + \" INNER JOIN \" + COLUMNS_V2 + \" ON \" + COLUMNS_V2 + \".\\\"CD_ID\\\" = \" + KEY_CONSTRAINTS + \".\\\"PARENT_CD_ID\\\" AND \"\n      + \" \" + COLUMNS_V2 + \".\\\"INTEGER_IDX\\\" = \" + KEY_CONSTRAINTS + \".\\\"PARENT_INTEGER_IDX\\\" \"\n      + \" WHERE \" + KEY_CONSTRAINTS + \".\\\"CONSTRAINT_TYPE\\\" = \"+ MConstraint.NOT_NULL_CONSTRAINT + \" AND\"\n      + (db_name == null ? \"\" : \" \" + DBS + \".\\\"NAME\\\" = ? AND\")\n      + (tbl_name == null ? \"\" : \" \" + TBLS + \".\\\"TBL_NAME\\\" = ? \") ;\n\n    queryText = queryText.trim();\n    if (queryText.endsWith(\"AND\")) {\n      queryText = queryText.substring(0, queryText.length()-3);\n    }\n    List<String> pms = new ArrayList<String>();\n    if (db_name != null) {\n      pms.add(db_name);\n    }\n    if (tbl_name != null) {\n      pms.add(tbl_name);\n    }\n\n    Query queryParams = pm.newQuery(\"javax.jdo.query.SQL\", queryText);\n      List<Object[]> sqlResult = ensureList(executeWithArray(\n        queryParams, pms.toArray(), queryText));\n\n    if (!sqlResult.isEmpty()) {\n      for (Object[] line : sqlResult) {\n          int enableValidateRely = extractSqlInt(line[4]);\n          boolean enable = (enableValidateRely & 4) != 0;\n          boolean validate = (enableValidateRely & 2) != 0;\n          boolean rely = (enableValidateRely & 1) != 0;\n        SQLNotNullConstraint currConstraint = new SQLNotNullConstraint(\n          extractSqlString(line[0]),\n          extractSqlString(line[1]),\n          extractSqlString(line[2]),\n          extractSqlString(line[3]),\n          enable,\n          validate,\n          rely);\n          ret.add(currConstraint);\n      }\n    }\n    return ret;\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreDirectSql.generateSqlFilterForPushdown": {
                "code_before_change": [],
                "code_after_change": "  public boolean generateSqlFilterForPushdown(\n      Table table, ExpressionTree tree, SqlFilterForPushdown result) throws MetaException {\n    // Derby and Oracle do not interpret filters ANSI-properly in some cases and need a workaround.\n    boolean dbHasJoinCastBug = DatabaseProduct.hasJoinOperationOrderBug(dbType);\n    result.table = table;\n    result.filter = PartitionFilterGenerator.generateSqlFilter(table, tree, result.params,\n        result.joins, dbHasJoinCastBug, defaultPartName, dbType, schema);\n    return result.filter != null;\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getUniqueConstraints": {
                "code_before_change": [],
                "code_after_change": "  public List<SQLUniqueConstraint> getUniqueConstraints(String db_name, String tbl_name)\n          throws MetaException {\n    List<SQLUniqueConstraint> ret = new ArrayList<SQLUniqueConstraint>();\n    String queryText =\n      \"SELECT \" + DBS + \".\\\"NAME\\\", \" + TBLS + \".\\\"TBL_NAME\\\", \" + COLUMNS_V2 + \".\\\"COLUMN_NAME\\\",\"\n      + \"\" + KEY_CONSTRAINTS + \".\\\"POSITION\\\", \"\n      + \"\" + KEY_CONSTRAINTS + \".\\\"CONSTRAINT_NAME\\\", \" + KEY_CONSTRAINTS + \".\\\"ENABLE_VALIDATE_RELY\\\" \"\n      + \" from \" + TBLS + \" \"\n      + \" INNER  join \" + KEY_CONSTRAINTS + \" ON \" + TBLS + \".\\\"TBL_ID\\\" = \" + KEY_CONSTRAINTS + \".\\\"PARENT_TBL_ID\\\" \"\n      + \" INNER join \" + DBS + \" ON \" + TBLS + \".\\\"DB_ID\\\" = \" + DBS + \".\\\"DB_ID\\\" \"\n      + \" INNER JOIN \" + COLUMNS_V2 + \" ON \" + COLUMNS_V2 + \".\\\"CD_ID\\\" = \" + KEY_CONSTRAINTS + \".\\\"PARENT_CD_ID\\\" AND \"\n      + \" \" + COLUMNS_V2 + \".\\\"INTEGER_IDX\\\" = \" + KEY_CONSTRAINTS + \".\\\"PARENT_INTEGER_IDX\\\" \"\n      + \" WHERE \" + KEY_CONSTRAINTS + \".\\\"CONSTRAINT_TYPE\\\" = \"+ MConstraint.UNIQUE_CONSTRAINT + \" AND\"\n      + (db_name == null ? \"\" : \" \" + DBS + \".\\\"NAME\\\" = ? AND\")\n      + (tbl_name == null ? \"\" : \" \" + TBLS + \".\\\"TBL_NAME\\\" = ? \") ;\n\n    queryText = queryText.trim();\n    if (queryText.endsWith(\"AND\")) {\n      queryText = queryText.substring(0, queryText.length()-3);\n    }\n    List<String> pms = new ArrayList<String>();\n    if (db_name != null) {\n      pms.add(db_name);\n    }\n    if (tbl_name != null) {\n      pms.add(tbl_name);\n    }\n\n    Query queryParams = pm.newQuery(\"javax.jdo.query.SQL\", queryText);\n      List<Object[]> sqlResult = ensureList(executeWithArray(\n        queryParams, pms.toArray(), queryText));\n\n    if (!sqlResult.isEmpty()) {\n      for (Object[] line : sqlResult) {\n          int enableValidateRely = extractSqlInt(line[5]);\n          boolean enable = (enableValidateRely & 4) != 0;\n          boolean validate = (enableValidateRely & 2) != 0;\n          boolean rely = (enableValidateRely & 1) != 0;\n        SQLUniqueConstraint currConstraint = new SQLUniqueConstraint(\n          extractSqlString(line[0]),\n          extractSqlString(line[1]),\n          extractSqlString(line[2]),\n          extractSqlInt(line[3]), extractSqlString(line[4]),\n          enable,\n          validate,\n          rely);\n          ret.add(currConstraint);\n      }\n    }\n    return ret;\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreDirectSql.PartitionFilterGenerator": {
                "code_before_change": [],
                "code_after_change": []
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getPartitionsFromPartitionIds": {
                "code_before_change": [],
                "code_after_change": "  private List<Partition> getPartitionsFromPartitionIds(String dbName, String tblName,\n      Boolean isView, List<Object> partIdList) throws MetaException {\n    boolean doTrace = LOG.isDebugEnabled();\n    int idStringWidth = (int)Math.ceil(Math.log10(partIdList.size())) + 1; // 1 for comma\n    int sbCapacity = partIdList.size() * idStringWidth;\n    // Prepare StringBuilder for \"PART_ID in (...)\" to use in future queries.\n    StringBuilder partSb = new StringBuilder(sbCapacity);\n    for (Object partitionId : partIdList) {\n      partSb.append(extractSqlLong(partitionId)).append(\",\");\n    }\n    String partIds = trimCommaList(partSb);\n\n    // Get most of the fields for the IDs provided.\n    // Assume db and table names are the same for all partition, as provided in arguments.\n    String queryText =\n      \"select \" + PARTITIONS + \".\\\"PART_ID\\\", \" + SDS + \".\\\"SD_ID\\\", \" + SDS + \".\\\"CD_ID\\\",\"\n    + \" \" + SERDES + \".\\\"SERDE_ID\\\", \" + PARTITIONS + \".\\\"CREATE_TIME\\\",\"\n    + \" \" + PARTITIONS + \".\\\"LAST_ACCESS_TIME\\\", \" + SDS + \".\\\"INPUT_FORMAT\\\", \" + SDS + \".\\\"IS_COMPRESSED\\\",\"\n    + \" \" + SDS + \".\\\"IS_STOREDASSUBDIRECTORIES\\\", \" + SDS + \".\\\"LOCATION\\\", \" + SDS + \".\\\"NUM_BUCKETS\\\",\"\n    + \" \" + SDS + \".\\\"OUTPUT_FORMAT\\\", \" + SERDES + \".\\\"NAME\\\", \" + SERDES + \".\\\"SLIB\\\" \"\n    + \"from \" + PARTITIONS + \"\"\n    + \"  left outer join \" + SDS + \" on \" + PARTITIONS + \".\\\"SD_ID\\\" = \" + SDS + \".\\\"SD_ID\\\" \"\n    + \"  left outer join \" + SERDES + \" on \" + SDS + \".\\\"SERDE_ID\\\" = \" + SERDES + \".\\\"SERDE_ID\\\" \"\n    + \"where \\\"PART_ID\\\" in (\" + partIds + \") order by \\\"PART_NAME\\\" asc\";\n    long start = doTrace ? System.nanoTime() : 0;\n    Query query = pm.newQuery(\"javax.jdo.query.SQL\", queryText);\n    List<Object[]> sqlResult = executeWithArray(query, null, queryText);\n    long queryTime = doTrace ? System.nanoTime() : 0;\n    Deadline.checkTimeout();\n\n    // Read all the fields and create partitions, SDs and serdes.\n    TreeMap<Long, Partition> partitions = new TreeMap<Long, Partition>();\n    TreeMap<Long, StorageDescriptor> sds = new TreeMap<Long, StorageDescriptor>();\n    TreeMap<Long, SerDeInfo> serdes = new TreeMap<Long, SerDeInfo>();\n    TreeMap<Long, List<FieldSchema>> colss = new TreeMap<Long, List<FieldSchema>>();\n    // Keep order by name, consistent with JDO.\n    ArrayList<Partition> orderedResult = new ArrayList<Partition>(partIdList.size());\n\n    // Prepare StringBuilder-s for \"in (...)\" lists to use in one-to-many queries.\n    StringBuilder sdSb = new StringBuilder(sbCapacity), serdeSb = new StringBuilder(sbCapacity);\n    StringBuilder colsSb = new StringBuilder(7); // We expect that there's only one field schema.\n    tblName = tblName.toLowerCase();\n    dbName = dbName.toLowerCase();\n    for (Object[] fields : sqlResult) {\n      // Here comes the ugly part...\n      long partitionId = extractSqlLong(fields[0]);\n      Long sdId = extractSqlLong(fields[1]);\n      Long colId = extractSqlLong(fields[2]);\n      Long serdeId = extractSqlLong(fields[3]);\n      // A partition must have at least sdId and serdeId set, or nothing set if it's a view.\n      if (sdId == null || serdeId == null) {\n        if (isView == null) {\n          isView = isViewTable(dbName, tblName);\n        }\n        if ((sdId != null || colId != null || serdeId != null) || !isView) {\n          throw new MetaException(\"Unexpected null for one of the IDs, SD \" + sdId +\n                  \", serde \" + serdeId + \" for a \" + (isView ? \"\" : \"non-\") + \" view\");\n        }\n      }\n\n      Partition part = new Partition();\n      orderedResult.add(part);\n      // Set the collection fields; some code might not check presence before accessing them.\n      part.setParameters(new HashMap<String, String>());\n      part.setValues(new ArrayList<String>());\n      part.setDbName(dbName);\n      part.setTableName(tblName);\n      if (fields[4] != null) part.setCreateTime(extractSqlInt(fields[4]));\n      if (fields[5] != null) part.setLastAccessTime(extractSqlInt(fields[5]));\n      partitions.put(partitionId, part);\n\n      if (sdId == null) continue; // Probably a view.\n      assert serdeId != null;\n\n      // We assume each partition has an unique SD.\n      StorageDescriptor sd = new StorageDescriptor();\n      StorageDescriptor oldSd = sds.put(sdId, sd);\n      if (oldSd != null) {\n        throw new MetaException(\"Partitions reuse SDs; we don't expect that\");\n      }\n      // Set the collection fields; some code might not check presence before accessing them.\n      sd.setSortCols(new ArrayList<Order>());\n      sd.setBucketCols(new ArrayList<String>());\n      sd.setParameters(new HashMap<String, String>());\n      sd.setSkewedInfo(new SkewedInfo(new ArrayList<String>(),\n          new ArrayList<List<String>>(), new HashMap<List<String>, String>()));\n      sd.setInputFormat((String)fields[6]);\n      Boolean tmpBoolean = extractSqlBoolean(fields[7]);\n      if (tmpBoolean != null) sd.setCompressed(tmpBoolean);\n      tmpBoolean = extractSqlBoolean(fields[8]);\n      if (tmpBoolean != null) sd.setStoredAsSubDirectories(tmpBoolean);\n      sd.setLocation((String)fields[9]);\n      if (fields[10] != null) sd.setNumBuckets(extractSqlInt(fields[10]));\n      sd.setOutputFormat((String)fields[11]);\n      sdSb.append(sdId).append(\",\");\n      part.setSd(sd);\n\n      if (colId != null) {\n        List<FieldSchema> cols = colss.get(colId);\n        // We expect that colId will be the same for all (or many) SDs.\n        if (cols == null) {\n          cols = new ArrayList<FieldSchema>();\n          colss.put(colId, cols);\n          colsSb.append(colId).append(\",\");\n        }\n        sd.setCols(cols);\n      }\n\n      // We assume each SD has an unique serde.\n      SerDeInfo serde = new SerDeInfo();\n      SerDeInfo oldSerde = serdes.put(serdeId, serde);\n      if (oldSerde != null) {\n        throw new MetaException(\"SDs reuse serdes; we don't expect that\");\n      }\n      serde.setParameters(new HashMap<String, String>());\n      serde.setName((String)fields[12]);\n      serde.setSerializationLib((String)fields[13]);\n      serdeSb.append(serdeId).append(\",\");\n      sd.setSerdeInfo(serde);\n      Deadline.checkTimeout();\n    }\n    query.closeAll();\n    timingTrace(doTrace, queryText, start, queryTime);\n\n    // Now get all the one-to-many things. Start with partitions.\n    queryText = \"select \\\"PART_ID\\\", \\\"PARAM_KEY\\\", \\\"PARAM_VALUE\\\" from \" + PARTITION_PARAMS + \"\"\n        + \" where \\\"PART_ID\\\" in (\" + partIds + \") and \\\"PARAM_KEY\\\" is not null\"\n        + \" order by \\\"PART_ID\\\" asc\";\n    loopJoinOrderedResult(partitions, queryText, 0, new ApplyFunc<Partition>() {\n      @Override\n      public void apply(Partition t, Object[] fields) {\n        t.putToParameters((String)fields[1], (String)fields[2]);\n      }});\n    // Perform conversion of null map values\n    for (Partition t : partitions.values()) {\n      t.setParameters(MetaStoreUtils.trimMapNulls(t.getParameters(), convertMapNullsToEmptyStrings));\n    }\n\n    queryText = \"select \\\"PART_ID\\\", \\\"PART_KEY_VAL\\\" from \" + PARTITION_KEY_VALS + \"\"\n        + \" where \\\"PART_ID\\\" in (\" + partIds + \") and \\\"INTEGER_IDX\\\" >= 0\"\n        + \" order by \\\"PART_ID\\\" asc, \\\"INTEGER_IDX\\\" asc\";\n    loopJoinOrderedResult(partitions, queryText, 0, new ApplyFunc<Partition>() {\n      @Override\n      public void apply(Partition t, Object[] fields) {\n        t.addToValues((String)fields[1]);\n      }});\n\n    // Prepare IN (blah) lists for the following queries. Cut off the final ','s.\n    if (sdSb.length() == 0) {\n      assert serdeSb.length() == 0 && colsSb.length() == 0;\n      return orderedResult; // No SDs, probably a view.\n    }\n\n    String sdIds = trimCommaList(sdSb);\n    String serdeIds = trimCommaList(serdeSb);\n    String colIds = trimCommaList(colsSb);\n\n    // Get all the stuff for SD. Don't do empty-list check - we expect partitions do have SDs.\n    queryText = \"select \\\"SD_ID\\\", \\\"PARAM_KEY\\\", \\\"PARAM_VALUE\\\" from \" + SD_PARAMS + \"\"\n        + \" where \\\"SD_ID\\\" in (\" + sdIds + \") and \\\"PARAM_KEY\\\" is not null\"\n        + \" order by \\\"SD_ID\\\" asc\";\n    loopJoinOrderedResult(sds, queryText, 0, new ApplyFunc<StorageDescriptor>() {\n      @Override\n      public void apply(StorageDescriptor t, Object[] fields) {\n        t.putToParameters((String)fields[1], extractSqlClob(fields[2]));\n      }});\n    // Perform conversion of null map values\n    for (StorageDescriptor t : sds.values()) {\n      t.setParameters(MetaStoreUtils.trimMapNulls(t.getParameters(), convertMapNullsToEmptyStrings));\n    }\n\n    queryText = \"select \\\"SD_ID\\\", \\\"COLUMN_NAME\\\", \" + SORT_COLS + \".\\\"ORDER\\\"\"\n        + \" from \" + SORT_COLS + \"\"\n        + \" where \\\"SD_ID\\\" in (\" + sdIds + \") and \\\"INTEGER_IDX\\\" >= 0\"\n        + \" order by \\\"SD_ID\\\" asc, \\\"INTEGER_IDX\\\" asc\";\n    loopJoinOrderedResult(sds, queryText, 0, new ApplyFunc<StorageDescriptor>() {\n      @Override\n      public void apply(StorageDescriptor t, Object[] fields) {\n        if (fields[2] == null) return;\n        t.addToSortCols(new Order((String)fields[1], extractSqlInt(fields[2])));\n      }});\n\n    queryText = \"select \\\"SD_ID\\\", \\\"BUCKET_COL_NAME\\\" from \" + BUCKETING_COLS + \"\"\n        + \" where \\\"SD_ID\\\" in (\" + sdIds + \") and \\\"INTEGER_IDX\\\" >= 0\"\n        + \" order by \\\"SD_ID\\\" asc, \\\"INTEGER_IDX\\\" asc\";\n    loopJoinOrderedResult(sds, queryText, 0, new ApplyFunc<StorageDescriptor>() {\n      @Override\n      public void apply(StorageDescriptor t, Object[] fields) {\n        t.addToBucketCols((String)fields[1]);\n      }});\n\n    // Skewed columns stuff.\n    queryText = \"select \\\"SD_ID\\\", \\\"SKEWED_COL_NAME\\\" from \" + SKEWED_COL_NAMES + \"\"\n        + \" where \\\"SD_ID\\\" in (\" + sdIds + \") and \\\"INTEGER_IDX\\\" >= 0\"\n        + \" order by \\\"SD_ID\\\" asc, \\\"INTEGER_IDX\\\" asc\";\n    boolean hasSkewedColumns =\n      loopJoinOrderedResult(sds, queryText, 0, new ApplyFunc<StorageDescriptor>() {\n        @Override\n        public void apply(StorageDescriptor t, Object[] fields) {\n          if (!t.isSetSkewedInfo()) t.setSkewedInfo(new SkewedInfo());\n          t.getSkewedInfo().addToSkewedColNames((String)fields[1]);\n        }}) > 0;\n\n    // Assume we don't need to fetch the rest of the skewed column data if we have no columns.\n    if (hasSkewedColumns) {\n      // We are skipping the SKEWED_STRING_LIST table here, as it seems to be totally useless.\n      queryText =\n            \"select \" + SKEWED_VALUES + \".\\\"SD_ID_OID\\\",\"\n          + \"  \" + SKEWED_STRING_LIST_VALUES + \".\\\"STRING_LIST_ID\\\",\"\n          + \"  \" + SKEWED_STRING_LIST_VALUES + \".\\\"STRING_LIST_VALUE\\\" \"\n          + \"from \" + SKEWED_VALUES + \" \"\n          + \"  left outer join \" + SKEWED_STRING_LIST_VALUES + \" on \" + SKEWED_VALUES + \".\"\n          + \"\\\"STRING_LIST_ID_EID\\\" = \" + SKEWED_STRING_LIST_VALUES + \".\\\"STRING_LIST_ID\\\" \"\n          + \"where \" + SKEWED_VALUES + \".\\\"SD_ID_OID\\\" in (\" + sdIds + \") \"\n          + \"  and \" + SKEWED_VALUES + \".\\\"STRING_LIST_ID_EID\\\" is not null \"\n          + \"  and \" + SKEWED_VALUES + \".\\\"INTEGER_IDX\\\" >= 0 \"\n          + \"order by \" + SKEWED_VALUES + \".\\\"SD_ID_OID\\\" asc, \" + SKEWED_VALUES + \".\\\"INTEGER_IDX\\\" asc,\"\n          + \"  \" + SKEWED_STRING_LIST_VALUES + \".\\\"INTEGER_IDX\\\" asc\";\n      loopJoinOrderedResult(sds, queryText, 0, new ApplyFunc<StorageDescriptor>() {\n        private Long currentListId;\n        private List<String> currentList;\n        @Override\n        public void apply(StorageDescriptor t, Object[] fields) throws MetaException {\n          if (!t.isSetSkewedInfo()) t.setSkewedInfo(new SkewedInfo());\n          // Note that this is not a typical list accumulator - there's no call to finalize\n          // the last list. Instead we add list to SD first, as well as locally to add elements.\n          if (fields[1] == null) {\n            currentList = null; // left outer join produced a list with no values\n            currentListId = null;\n            t.getSkewedInfo().addToSkewedColValues(Collections.<String>emptyList());\n          } else {\n            long fieldsListId = extractSqlLong(fields[1]);\n            if (currentListId == null || fieldsListId != currentListId) {\n              currentList = new ArrayList<String>();\n              currentListId = fieldsListId;\n              t.getSkewedInfo().addToSkewedColValues(currentList);\n            }\n            currentList.add((String)fields[2]);\n          }\n        }});\n\n      // We are skipping the SKEWED_STRING_LIST table here, as it seems to be totally useless.\n      queryText =\n            \"select \" + SKEWED_COL_VALUE_LOC_MAP + \".\\\"SD_ID\\\",\"\n          + \" \" + SKEWED_STRING_LIST_VALUES + \".STRING_LIST_ID,\"\n          + \" \" + SKEWED_COL_VALUE_LOC_MAP + \".\\\"LOCATION\\\",\"\n          + \" \" + SKEWED_STRING_LIST_VALUES + \".\\\"STRING_LIST_VALUE\\\" \"\n          + \"from \" + SKEWED_COL_VALUE_LOC_MAP + \"\"\n          + \"  left outer join \" + SKEWED_STRING_LIST_VALUES + \" on \" + SKEWED_COL_VALUE_LOC_MAP + \".\"\n          + \"\\\"STRING_LIST_ID_KID\\\" = \" + SKEWED_STRING_LIST_VALUES + \".\\\"STRING_LIST_ID\\\" \"\n          + \"where \" + SKEWED_COL_VALUE_LOC_MAP + \".\\\"SD_ID\\\" in (\" + sdIds + \")\"\n          + \"  and \" + SKEWED_COL_VALUE_LOC_MAP + \".\\\"STRING_LIST_ID_KID\\\" is not null \"\n          + \"order by \" + SKEWED_COL_VALUE_LOC_MAP + \".\\\"SD_ID\\\" asc,\"\n          + \"  \" + SKEWED_STRING_LIST_VALUES + \".\\\"STRING_LIST_ID\\\" asc,\"\n          + \"  \" + SKEWED_STRING_LIST_VALUES + \".\\\"INTEGER_IDX\\\" asc\";\n\n      loopJoinOrderedResult(sds, queryText, 0, new ApplyFunc<StorageDescriptor>() {\n        private Long currentListId;\n        private List<String> currentList;\n        @Override\n        public void apply(StorageDescriptor t, Object[] fields) throws MetaException {\n          if (!t.isSetSkewedInfo()) {\n            SkewedInfo skewedInfo = new SkewedInfo();\n            skewedInfo.setSkewedColValueLocationMaps(new HashMap<List<String>, String>());\n            t.setSkewedInfo(skewedInfo);\n          }\n          Map<List<String>, String> skewMap = t.getSkewedInfo().getSkewedColValueLocationMaps();\n          // Note that this is not a typical list accumulator - there's no call to finalize\n          // the last list. Instead we add list to SD first, as well as locally to add elements.\n          if (fields[1] == null) {\n            currentList = new ArrayList<String>(); // left outer join produced a list with no values\n            currentListId = null;\n          } else {\n            long fieldsListId = extractSqlLong(fields[1]);\n            if (currentListId == null || fieldsListId != currentListId) {\n              currentList = new ArrayList<String>();\n              currentListId = fieldsListId;\n            } else {\n              skewMap.remove(currentList); // value based compare.. remove first\n            }\n            currentList.add((String)fields[3]);\n          }\n          skewMap.put(currentList, (String)fields[2]);\n        }});\n    } // if (hasSkewedColumns)\n\n    // Get FieldSchema stuff if any.\n    if (!colss.isEmpty()) {\n      // We are skipping the CDS table here, as it seems to be totally useless.\n      queryText = \"select \\\"CD_ID\\\", \\\"COMMENT\\\", \\\"COLUMN_NAME\\\", \\\"TYPE_NAME\\\"\"\n          + \" from \" + COLUMNS_V2 + \" where \\\"CD_ID\\\" in (\" + colIds + \") and \\\"INTEGER_IDX\\\" >= 0\"\n          + \" order by \\\"CD_ID\\\" asc, \\\"INTEGER_IDX\\\" asc\";\n      loopJoinOrderedResult(colss, queryText, 0, new ApplyFunc<List<FieldSchema>>() {\n        @Override\n        public void apply(List<FieldSchema> t, Object[] fields) {\n          t.add(new FieldSchema((String)fields[2], extractSqlClob(fields[3]), (String)fields[1]));\n        }});\n    }\n\n    // Finally, get all the stuff for serdes - just the params.\n    queryText = \"select \\\"SERDE_ID\\\", \\\"PARAM_KEY\\\", \\\"PARAM_VALUE\\\" from \" + SERDE_PARAMS + \"\"\n        + \" where \\\"SERDE_ID\\\" in (\" + serdeIds + \") and \\\"PARAM_KEY\\\" is not null\"\n        + \" order by \\\"SERDE_ID\\\" asc\";\n    loopJoinOrderedResult(serdes, queryText, 0, new ApplyFunc<SerDeInfo>() {\n      @Override\n      public void apply(SerDeInfo t, Object[] fields) {\n        t.putToParameters((String)fields[1], extractSqlClob(fields[2]));\n      }});\n    // Perform conversion of null map values\n    for (SerDeInfo t : serdes.values()) {\n      t.setParameters(MetaStoreUtils.trimMapNulls(t.getParameters(), convertMapNullsToEmptyStrings));\n    }\n\n    return orderedResult;\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreDirectSql.run": {
                "code_before_change": [],
                "code_after_change": "      public List<Partition> run(List<String> input) throws MetaException {\n        String filter = \"\" + PARTITIONS + \".\\\"PART_NAME\\\" in (\" + makeParams(input.size()) + \")\";\n        return getPartitionsViaSqlFilterInternal(dbName, tblName, null, filter, input,\n            Collections.<String>emptyList(), null);\n      }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.initializeHelper": {
                "code_before_change": [],
                "code_after_change": "  private void initializeHelper(Properties dsProps) {\n    LOG.info(\"ObjectStore, initialize called\");\n    prop = dsProps;\n    pm = getPersistenceManager();\n    isInitialized = pm != null;\n    if (isInitialized) {\n      expressionProxy = createExpressionProxy(hiveConf);\n      if (HiveConf.getBoolVar(getConf(), ConfVars.METASTORE_TRY_DIRECT_SQL)) {\n        String schema = prop.getProperty(\"javax.jdo.mapping.Schema\");\n        if (schema != null && schema.isEmpty()) {\n          schema = null;\n        }\n        directSql = new MetaStoreDirectSql(pm, hiveConf, schema);\n      }\n    }\n    LOG.debug(\"RawStore: \" + this + \", with PersistenceManager: \" + pm +\n        \" created in the thread with id: \" + Thread.currentThread().getId());\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreDirectSql.visit": {
                "code_before_change": "    public void visit(LeafNode node) throws MetaException {\n      if (node.operator == Operator.LIKE) {\n        filterBuffer.setError(\"LIKE is not supported for SQL filter pushdown\");\n        return;\n      }\n      int partColCount = table.getPartitionKeys().size();\n      int partColIndex = node.getPartColIndexForFilter(table, filterBuffer);\n      if (filterBuffer.hasError()) return;\n\n      // We skipped 'like', other ops should all work as long as the types are right.\n      String colTypeStr = table.getPartitionKeys().get(partColIndex).getType();\n      FilterType colType = FilterType.fromType(colTypeStr);\n      if (colType == FilterType.Invalid) {\n        filterBuffer.setError(\"Filter pushdown not supported for type \" + colTypeStr);\n        return;\n      }\n      FilterType valType = FilterType.fromClass(node.value);\n      Object nodeValue = node.value;\n      if (valType == FilterType.Invalid) {\n        filterBuffer.setError(\"Filter pushdown not supported for value \" + node.value.getClass());\n        return;\n      }\n\n      // TODO: if Filter.g does date parsing for quoted strings, we'd need to verify there's no\n      //       type mismatch when string col is filtered by a string that looks like date.\n      if (colType == FilterType.Date && valType == FilterType.String) {\n        // TODO: Filter.g cannot parse a quoted date; try to parse date here too.\n        try {\n          nodeValue = new java.sql.Date(\n              HiveMetaStore.PARTITION_DATE_FORMAT.parse((String)nodeValue).getTime());\n          valType = FilterType.Date;\n        } catch (ParseException pe) { // do nothing, handled below - types will mismatch\n        }\n      }\n\n      if (colType != valType) {\n        // It's not clear how filtering for e.g. \"stringCol > 5\" should work (which side is\n        // to be coerced?). Let the expression evaluation sort this one out, not metastore.\n        filterBuffer.setError(\"Cannot push down filter for \"\n            + colTypeStr + \" column and value \" + nodeValue.getClass());\n        return;\n      }\n\n      if (joins.isEmpty()) {\n        // There's a fixed number of partition cols that we might have filters on. To avoid\n        // joining multiple times for one column (if there are several filters on it), we will\n        // keep numCols elements in the list, one for each column; we will fill it with nulls,\n        // put each join at a corresponding index when necessary, and remove nulls in the end.\n        for (int i = 0; i < partColCount; ++i) {\n          joins.add(null);\n        }\n      }\n      if (joins.get(partColIndex) == null) {\n        joins.set(partColIndex, \"inner join \\\"PARTITION_KEY_VALS\\\" \\\"FILTER\" + partColIndex\n            + \"\\\" on \\\"FILTER\"  + partColIndex + \"\\\".\\\"PART_ID\\\" = \\\"PARTITIONS\\\".\\\"PART_ID\\\"\"\n            + \" and \\\"FILTER\" + partColIndex + \"\\\".\\\"INTEGER_IDX\\\" = \" + partColIndex);\n      }\n\n      // Build the filter and add parameters linearly; we are traversing leaf nodes LTR.\n      String tableValue = \"\\\"FILTER\" + partColIndex + \"\\\".\\\"PART_KEY_VAL\\\"\";\n      if (node.isReverseOrder) {\n        params.add(nodeValue);\n      }\n      if (colType != FilterType.String) {\n        // The underlying database field is varchar, we need to compare numbers.\n        // Note that this won't work with __HIVE_DEFAULT_PARTITION__. It will fail and fall\n        // back to JDO. That is by design; we could add an ugly workaround here but didn't.\n        if (colType == FilterType.Integral) {\n          tableValue = \"cast(\" + tableValue + \" as decimal(21,0))\";\n        } else if (colType == FilterType.Date) {\n          tableValue = \"cast(\" + tableValue + \" as date)\";\n        }\n\n        // This is a workaround for DERBY-6358; as such, it is pretty horrible.\n        tableValue = \"(case when \\\"TBLS\\\".\\\"TBL_NAME\\\" = ? and \\\"DBS\\\".\\\"NAME\\\" = ? then \"\n          + tableValue + \" else null end)\";\n        params.add(table.getTableName().toLowerCase());\n        params.add(table.getDbName().toLowerCase());\n      }\n      if (!node.isReverseOrder) {\n        params.add(nodeValue);\n      }\n\n      filterBuffer.append(node.isReverseOrder\n          ? \"(? \" + node.operator.getSqlOp() + \" \" + tableValue + \")\"\n          : \"(\" + tableValue + \" \" + node.operator.getSqlOp() + \" ?)\");\n    }",
                "code_after_change": "    public void visit(LeafNode node) throws MetaException {\n      if (node.operator == Operator.LIKE) {\n        filterBuffer.setError(\"LIKE is not supported for SQL filter pushdown\");\n        return;\n      }\n      int partColCount = table.getPartitionKeys().size();\n      int partColIndex = node.getPartColIndexForFilter(table, filterBuffer);\n      if (filterBuffer.hasError()) return;\n\n      // We skipped 'like', other ops should all work as long as the types are right.\n      String colTypeStr = table.getPartitionKeys().get(partColIndex).getType();\n      FilterType colType = FilterType.fromType(colTypeStr);\n      if (colType == FilterType.Invalid) {\n        filterBuffer.setError(\"Filter pushdown not supported for type \" + colTypeStr);\n        return;\n      }\n      FilterType valType = FilterType.fromClass(node.value);\n      Object nodeValue = node.value;\n      if (valType == FilterType.Invalid) {\n        filterBuffer.setError(\"Filter pushdown not supported for value \" + node.value.getClass());\n        return;\n      }\n\n      // if Filter.g does date parsing for quoted strings, we'd need to verify there's no\n      // type mismatch when string col is filtered by a string that looks like date.\n      if (colType == FilterType.Date && valType == FilterType.String) {\n        // Filter.g cannot parse a quoted date; try to parse date here too.\n        try {\n          nodeValue = new java.sql.Date(\n              HiveMetaStore.PARTITION_DATE_FORMAT.get().parse((String)nodeValue).getTime());\n          valType = FilterType.Date;\n        } catch (ParseException pe) { // do nothing, handled below - types will mismatch\n        }\n      }\n\n      if (colType != valType) {\n        // It's not clear how filtering for e.g. \"stringCol > 5\" should work (which side is\n        // to be coerced?). Let the expression evaluation sort this one out, not metastore.\n        filterBuffer.setError(\"Cannot push down filter for \"\n            + colTypeStr + \" column and value \" + nodeValue.getClass());\n        return;\n      }\n\n      if (joins.isEmpty()) {\n        // There's a fixed number of partition cols that we might have filters on. To avoid\n        // joining multiple times for one column (if there are several filters on it), we will\n        // keep numCols elements in the list, one for each column; we will fill it with nulls,\n        // put each join at a corresponding index when necessary, and remove nulls in the end.\n        for (int i = 0; i < partColCount; ++i) {\n          joins.add(null);\n        }\n      }\n      if (joins.get(partColIndex) == null) {\n        joins.set(partColIndex, \"inner join \" + PARTITION_KEY_VALS + \" \\\"FILTER\" + partColIndex\n            + \"\\\" on \\\"FILTER\"  + partColIndex + \"\\\".\\\"PART_ID\\\" = \" + PARTITIONS + \".\\\"PART_ID\\\"\"\n            + \" and \\\"FILTER\" + partColIndex + \"\\\".\\\"INTEGER_IDX\\\" = \" + partColIndex);\n      }\n\n      // Build the filter and add parameters linearly; we are traversing leaf nodes LTR.\n      String tableValue = \"\\\"FILTER\" + partColIndex + \"\\\".\\\"PART_KEY_VAL\\\"\";\n\n      if (node.isReverseOrder) {\n        params.add(nodeValue);\n      }\n      String tableColumn = tableValue;\n      if (colType != FilterType.String) {\n        // The underlying database field is varchar, we need to compare numbers.\n        if (colType == FilterType.Integral) {\n          tableValue = \"cast(\" + tableValue + \" as decimal(21,0))\";\n        } else if (colType == FilterType.Date) {\n          if (dbType == DatabaseProduct.ORACLE) {\n            // Oracle requires special treatment... as usual.\n            tableValue = \"TO_DATE(\" + tableValue + \", 'YYYY-MM-DD')\";\n          } else {\n            tableValue = \"cast(\" + tableValue + \" as date)\";\n          }\n        }\n\n        // Workaround for HIVE_DEFAULT_PARTITION - ignore it like JDO does, for now.\n        String tableValue0 = tableValue;\n        tableValue = \"(case when \" + tableColumn + \" <> ?\";\n        params.add(defaultPartName);\n\n        if (dbHasJoinCastBug) {\n          // This is a workaround for DERBY-6358 and Oracle bug; it is pretty horrible.\n          tableValue += (\" and \" + TBLS + \".\\\"TBL_NAME\\\" = ? and \" + DBS + \".\\\"NAME\\\" = ? and \"\n              + \"\\\"FILTER\" + partColIndex + \"\\\".\\\"PART_ID\\\" = \" + PARTITIONS + \".\\\"PART_ID\\\" and \"\n                + \"\\\"FILTER\" + partColIndex + \"\\\".\\\"INTEGER_IDX\\\" = \" + partColIndex);\n          params.add(table.getTableName().toLowerCase());\n          params.add(table.getDbName().toLowerCase());\n        }\n        tableValue += \" then \" + tableValue0 + \" else null end)\";\n      }\n      if (!node.isReverseOrder) {\n        params.add(nodeValue);\n      }\n\n      filterBuffer.append(node.isReverseOrder\n          ? \"(? \" + node.operator.getSqlOp() + \" \" + tableValue + \")\"\n          : \"(\" + tableValue + \" \" + node.operator.getSqlOp() + \" ?)\");\n    }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getPartitionsViaSqlFilterInternal": {
                "code_before_change": "  private List<Partition> getPartitionsViaSqlFilterInternal(String dbName, String tblName,\n      Boolean isView, String sqlFilter, List<? extends Object> paramsForFilter,\n      List<String> joinsForFilter, Integer max) throws MetaException {\n    boolean doTrace = LOG.isDebugEnabled();\n    dbName = dbName.toLowerCase();\n    tblName = tblName.toLowerCase();\n    // We have to be mindful of order during filtering if we are not returning all partitions.\n    String orderForFilter = (max != null) ? \" order by \\\"PART_NAME\\\" asc\" : \"\";\n    if (isMySql) {\n      assert pm.currentTransaction().isActive();\n      setAnsiQuotesForMysql(); // must be inside tx together with queries\n    }\n\n    // Get all simple fields for partitions and related objects, which we can map one-on-one.\n    // We will do this in 2 queries to use different existing indices for each one.\n    // We do not get table and DB name, assuming they are the same as we are using to filter.\n    // TODO: We might want to tune the indexes instead. With current ones MySQL performs\n    // poorly, esp. with 'order by' w/o index on large tables, even if the number of actual\n    // results is small (query that returns 8 out of 32k partitions can go 4sec. to 0sec. by\n    // just adding a \\\"PART_ID\\\" IN (...) filter that doesn't alter the results to it, probably\n    // causing it to not sort the entire table due to not knowing how selective the filter is.\n    String queryText =\n        \"select \\\"PARTITIONS\\\".\\\"PART_ID\\\" from \\\"PARTITIONS\\\"\"\n      + \"  inner join \\\"TBLS\\\" on \\\"PARTITIONS\\\".\\\"TBL_ID\\\" = \\\"TBLS\\\".\\\"TBL_ID\\\" \"\n      + \"    and \\\"TBLS\\\".\\\"TBL_NAME\\\" = ? \"\n      + \"  inner join \\\"DBS\\\" on \\\"TBLS\\\".\\\"DB_ID\\\" = \\\"DBS\\\".\\\"DB_ID\\\" \"\n      + \"     and \\\"DBS\\\".\\\"NAME\\\" = ? \"\n      + join(joinsForFilter, ' ')\n      + (StringUtils.isBlank(sqlFilter) ? \"\" : (\" where \" + sqlFilter)) + orderForFilter;\n    Object[] params = new Object[paramsForFilter.size() + 2];\n    params[0] = tblName;\n    params[1] = dbName;\n    for (int i = 0; i < paramsForFilter.size(); ++i) {\n      params[i + 2] = paramsForFilter.get(i);\n    }\n\n    long start = doTrace ? System.nanoTime() : 0;\n    Query query = pm.newQuery(\"javax.jdo.query.SQL\", queryText);\n    if (max != null) {\n      query.setRange(0, max.shortValue());\n    }\n    @SuppressWarnings(\"unchecked\")\n    List<Object> sqlResult = (List<Object>)query.executeWithArray(params);\n    long queryTime = doTrace ? System.nanoTime() : 0;\n    if (sqlResult.isEmpty()) {\n      timingTrace(doTrace, queryText, start, queryTime);\n      return new ArrayList<Partition>(); // no partitions, bail early.\n    }\n\n    // Prepare StringBuilder for \"PART_ID in (...)\" to use in future queries.\n    int sbCapacity = sqlResult.size() * 7; // if there are 100k things => 6 chars, plus comma\n    StringBuilder partSb = new StringBuilder(sbCapacity);\n    // Assume db and table names are the same for all partition, that's what we're selecting for.\n    for (Object partitionId : sqlResult) {\n      partSb.append(extractSqlLong(partitionId)).append(\",\");\n    }\n    String partIds = trimCommaList(partSb);\n    timingTrace(doTrace, queryText, start, queryTime);\n\n    // Now get most of the other fields.\n    queryText =\n      \"select \\\"PARTITIONS\\\".\\\"PART_ID\\\", \\\"SDS\\\".\\\"SD_ID\\\", \\\"SDS\\\".\\\"CD_ID\\\",\"\n    + \" \\\"SERDES\\\".\\\"SERDE_ID\\\", \\\"PARTITIONS\\\".\\\"CREATE_TIME\\\",\"\n    + \" \\\"PARTITIONS\\\".\\\"LAST_ACCESS_TIME\\\", \\\"SDS\\\".\\\"INPUT_FORMAT\\\", \\\"SDS\\\".\\\"IS_COMPRESSED\\\",\"\n    + \" \\\"SDS\\\".\\\"IS_STOREDASSUBDIRECTORIES\\\", \\\"SDS\\\".\\\"LOCATION\\\", \\\"SDS\\\".\\\"NUM_BUCKETS\\\",\"\n    + \" \\\"SDS\\\".\\\"OUTPUT_FORMAT\\\", \\\"SERDES\\\".\\\"NAME\\\", \\\"SERDES\\\".\\\"SLIB\\\" \"\n    + \"from \\\"PARTITIONS\\\"\"\n    + \"  left outer join \\\"SDS\\\" on \\\"PARTITIONS\\\".\\\"SD_ID\\\" = \\\"SDS\\\".\\\"SD_ID\\\" \"\n    + \"  left outer join \\\"SERDES\\\" on \\\"SDS\\\".\\\"SERDE_ID\\\" = \\\"SERDES\\\".\\\"SERDE_ID\\\" \"\n    + \"where \\\"PART_ID\\\" in (\" + partIds + \") order by \\\"PART_NAME\\\" asc\";\n    start = doTrace ? System.nanoTime() : 0;\n    query = pm.newQuery(\"javax.jdo.query.SQL\", queryText);\n    @SuppressWarnings(\"unchecked\")\n    List<Object[]> sqlResult2 = (List<Object[]>)query.executeWithArray(params);\n    queryTime = doTrace ? System.nanoTime() : 0;\n\n    // Read all the fields and create partitions, SDs and serdes.\n    TreeMap<Long, Partition> partitions = new TreeMap<Long, Partition>();\n    TreeMap<Long, StorageDescriptor> sds = new TreeMap<Long, StorageDescriptor>();\n    TreeMap<Long, SerDeInfo> serdes = new TreeMap<Long, SerDeInfo>();\n    TreeMap<Long, List<FieldSchema>> colss = new TreeMap<Long, List<FieldSchema>>();\n    // Keep order by name, consistent with JDO.\n    ArrayList<Partition> orderedResult = new ArrayList<Partition>(sqlResult.size());\n\n    // Prepare StringBuilder-s for \"in (...)\" lists to use in one-to-many queries.\n    StringBuilder sdSb = new StringBuilder(sbCapacity), serdeSb = new StringBuilder(sbCapacity);\n    StringBuilder colsSb = new StringBuilder(7); // We expect that there's only one field schema.\n    tblName = tblName.toLowerCase();\n    dbName = dbName.toLowerCase();\n    for (Object[] fields : sqlResult2) {\n      // Here comes the ugly part...\n      long partitionId = extractSqlLong(fields[0]);\n      Long sdId = extractSqlLong(fields[1]);\n      Long colId = extractSqlLong(fields[2]);\n      Long serdeId = extractSqlLong(fields[3]);\n      // A partition must have either everything set, or nothing set if it's a view.\n      if (sdId == null || colId == null || serdeId == null) {\n        if (isView == null) {\n          isView = isViewTable(dbName, tblName);\n        }\n        if ((sdId != null || colId != null || serdeId != null) || !isView) {\n          throw new MetaException(\"Unexpected null for one of the IDs, SD \" + sdId + \", column \"\n              + colId + \", serde \" + serdeId + \" for a \" + (isView ? \"\" : \"non-\") + \" view\");\n        }\n      }\n\n      Partition part = new Partition();\n      orderedResult.add(part);\n      // Set the collection fields; some code might not check presence before accessing them.\n      part.setParameters(new HashMap<String, String>());\n      part.setValues(new ArrayList<String>());\n      part.setDbName(dbName);\n      part.setTableName(tblName);\n      if (fields[4] != null) part.setCreateTime(extractSqlInt(fields[4]));\n      if (fields[5] != null) part.setLastAccessTime(extractSqlInt(fields[5]));\n      partitions.put(partitionId, part);\n\n      if (sdId == null) continue; // Probably a view.\n      assert colId != null && serdeId != null;\n\n      // We assume each partition has an unique SD.\n      StorageDescriptor sd = new StorageDescriptor();\n      StorageDescriptor oldSd = sds.put(sdId, sd);\n      if (oldSd != null) {\n        throw new MetaException(\"Partitions reuse SDs; we don't expect that\");\n      }\n      // Set the collection fields; some code might not check presence before accessing them.\n      sd.setSortCols(new ArrayList<Order>());\n      sd.setBucketCols(new ArrayList<String>());\n      sd.setParameters(new HashMap<String, String>());\n      sd.setSkewedInfo(new SkewedInfo(new ArrayList<String>(),\n          new ArrayList<List<String>>(), new HashMap<List<String>, String>()));\n      sd.setInputFormat((String)fields[6]);\n      Boolean tmpBoolean = extractSqlBoolean(fields[7]);\n      if (tmpBoolean != null) sd.setCompressed(tmpBoolean);\n      tmpBoolean = extractSqlBoolean(fields[8]);\n      if (tmpBoolean != null) sd.setStoredAsSubDirectories(tmpBoolean);\n      sd.setLocation((String)fields[9]);\n      if (fields[10] != null) sd.setNumBuckets(extractSqlInt(fields[10]));\n      sd.setOutputFormat((String)fields[11]);\n      sdSb.append(sdId).append(\",\");\n      part.setSd(sd);\n\n      List<FieldSchema> cols = colss.get(colId);\n      // We expect that colId will be the same for all (or many) SDs.\n      if (cols == null) {\n        cols = new ArrayList<FieldSchema>();\n        colss.put(colId, cols);\n        colsSb.append(colId).append(\",\");\n      }\n      sd.setCols(cols);\n\n      // We assume each SD has an unique serde.\n      SerDeInfo serde = new SerDeInfo();\n      SerDeInfo oldSerde = serdes.put(serdeId, serde);\n      if (oldSerde != null) {\n        throw new MetaException(\"SDs reuse serdes; we don't expect that\");\n      }\n      serde.setParameters(new HashMap<String, String>());\n      serde.setName((String)fields[12]);\n      serde.setSerializationLib((String)fields[13]);\n      serdeSb.append(serdeId).append(\",\");\n      sd.setSerdeInfo(serde);\n    }\n    query.closeAll();\n    timingTrace(doTrace, queryText, start, queryTime);\n\n    // Now get all the one-to-many things. Start with partitions.\n    queryText = \"select \\\"PART_ID\\\", \\\"PARAM_KEY\\\", \\\"PARAM_VALUE\\\" from \\\"PARTITION_PARAMS\\\"\"\n        + \" where \\\"PART_ID\\\" in (\" + partIds + \") and \\\"PARAM_KEY\\\" is not null\"\n        + \" order by \\\"PART_ID\\\" asc\";\n    loopJoinOrderedResult(partitions, queryText, 0, new ApplyFunc<Partition>() {\n      public void apply(Partition t, Object[] fields) {\n        t.putToParameters((String)fields[1], (String)fields[2]);\n      }});\n\n    queryText = \"select \\\"PART_ID\\\", \\\"PART_KEY_VAL\\\" from \\\"PARTITION_KEY_VALS\\\"\"\n        + \" where \\\"PART_ID\\\" in (\" + partIds + \") and \\\"INTEGER_IDX\\\" >= 0\"\n        + \" order by \\\"PART_ID\\\" asc, \\\"INTEGER_IDX\\\" asc\";\n    loopJoinOrderedResult(partitions, queryText, 0, new ApplyFunc<Partition>() {\n      public void apply(Partition t, Object[] fields) {\n        t.addToValues((String)fields[1]);\n      }});\n\n    // Prepare IN (blah) lists for the following queries. Cut off the final ','s.\n    if (sdSb.length() == 0) {\n      assert serdeSb.length() == 0 && colsSb.length() == 0;\n      return orderedResult; // No SDs, probably a view.\n    }\n    String sdIds = trimCommaList(sdSb), serdeIds = trimCommaList(serdeSb),\n        colIds = trimCommaList(colsSb);\n\n    // Get all the stuff for SD. Don't do empty-list check - we expect partitions do have SDs.\n    queryText = \"select \\\"SD_ID\\\", \\\"PARAM_KEY\\\", \\\"PARAM_VALUE\\\" from \\\"SD_PARAMS\\\"\"\n        + \" where \\\"SD_ID\\\" in (\" + sdIds + \") and \\\"PARAM_KEY\\\" is not null\"\n        + \" order by \\\"SD_ID\\\" asc\";\n    loopJoinOrderedResult(sds, queryText, 0, new ApplyFunc<StorageDescriptor>() {\n      public void apply(StorageDescriptor t, Object[] fields) {\n        t.putToParameters((String)fields[1], (String)fields[2]);\n      }});\n\n    queryText = \"select \\\"SD_ID\\\", \\\"COLUMN_NAME\\\", \\\"SORT_COLS\\\".\\\"ORDER\\\" from \\\"SORT_COLS\\\"\"\n        + \" where \\\"SD_ID\\\" in (\" + sdIds + \") and \\\"INTEGER_IDX\\\" >= 0\"\n        + \" order by \\\"SD_ID\\\" asc, \\\"INTEGER_IDX\\\" asc\";\n    loopJoinOrderedResult(sds, queryText, 0, new ApplyFunc<StorageDescriptor>() {\n      public void apply(StorageDescriptor t, Object[] fields) {\n        if (fields[2] == null) return;\n        t.addToSortCols(new Order((String)fields[1], extractSqlInt(fields[2])));\n      }});\n\n    queryText = \"select \\\"SD_ID\\\", \\\"BUCKET_COL_NAME\\\" from \\\"BUCKETING_COLS\\\"\"\n        + \" where \\\"SD_ID\\\" in (\" + sdIds + \") and \\\"INTEGER_IDX\\\" >= 0\"\n        + \" order by \\\"SD_ID\\\" asc, \\\"INTEGER_IDX\\\" asc\";\n    loopJoinOrderedResult(sds, queryText, 0, new ApplyFunc<StorageDescriptor>() {\n      public void apply(StorageDescriptor t, Object[] fields) {\n        t.addToBucketCols((String)fields[1]);\n      }});\n\n    // Skewed columns stuff.\n    queryText = \"select \\\"SD_ID\\\", \\\"SKEWED_COL_NAME\\\" from \\\"SKEWED_COL_NAMES\\\"\"\n        + \" where \\\"SD_ID\\\" in (\" + sdIds + \") and \\\"INTEGER_IDX\\\" >= 0\"\n        + \" order by \\\"SD_ID\\\" asc, \\\"INTEGER_IDX\\\" asc\";\n    boolean hasSkewedColumns =\n      loopJoinOrderedResult(sds, queryText, 0, new ApplyFunc<StorageDescriptor>() {\n        public void apply(StorageDescriptor t, Object[] fields) {\n          if (!t.isSetSkewedInfo()) t.setSkewedInfo(new SkewedInfo());\n          t.getSkewedInfo().addToSkewedColNames((String)fields[1]);\n        }}) > 0;\n\n    // Assume we don't need to fetch the rest of the skewed column data if we have no columns.\n    if (hasSkewedColumns) {\n      // We are skipping the SKEWED_STRING_LIST table here, as it seems to be totally useless.\n      queryText =\n            \"select \\\"SKEWED_VALUES\\\".\\\"SD_ID_OID\\\",\"\n          + \"  \\\"SKEWED_STRING_LIST_VALUES\\\".\\\"STRING_LIST_ID\\\",\"\n          + \"  \\\"SKEWED_STRING_LIST_VALUES\\\".\\\"STRING_LIST_VALUE\\\" \"\n          + \"from \\\"SKEWED_VALUES\\\" \"\n          + \"  left outer join \\\"SKEWED_STRING_LIST_VALUES\\\" on \\\"SKEWED_VALUES\\\".\"\n          + \"\\\"STRING_LIST_ID_EID\\\" = \\\"SKEWED_STRING_LIST_VALUES\\\".\\\"STRING_LIST_ID\\\" \"\n          + \"where \\\"SKEWED_VALUES\\\".\\\"SD_ID_OID\\\" in (\" + sdIds + \") \"\n          + \"  and \\\"SKEWED_VALUES\\\".\\\"STRING_LIST_ID_EID\\\" is not null \"\n          + \"  and \\\"SKEWED_VALUES\\\".\\\"INTEGER_IDX\\\" >= 0 \"\n          + \"order by \\\"SKEWED_VALUES\\\".\\\"SD_ID_OID\\\" asc, \\\"SKEWED_VALUES\\\".\\\"INTEGER_IDX\\\" asc,\"\n          + \"  \\\"SKEWED_STRING_LIST_VALUES\\\".\\\"INTEGER_IDX\\\" asc\";\n      loopJoinOrderedResult(sds, queryText, 0, new ApplyFunc<StorageDescriptor>() {\n        private Long currentListId;\n        private List<String> currentList;\n        public void apply(StorageDescriptor t, Object[] fields) throws MetaException {\n          if (!t.isSetSkewedInfo()) t.setSkewedInfo(new SkewedInfo());\n          // Note that this is not a typical list accumulator - there's no call to finalize\n          // the last list. Instead we add list to SD first, as well as locally to add elements.\n          if (fields[1] == null) {\n            currentList = null; // left outer join produced a list with no values\n            currentListId = null;\n            t.getSkewedInfo().addToSkewedColValues(new ArrayList<String>());\n          } else {\n            long fieldsListId = extractSqlLong(fields[1]);\n            if (currentListId == null || fieldsListId != currentListId) {\n              currentList = new ArrayList<String>();\n              currentListId = fieldsListId;\n              t.getSkewedInfo().addToSkewedColValues(currentList);\n            }\n            currentList.add((String)fields[2]);\n          }\n        }});\n\n      // We are skipping the SKEWED_STRING_LIST table here, as it seems to be totally useless.\n      queryText =\n            \"select \\\"SKEWED_COL_VALUE_LOC_MAP\\\".\\\"SD_ID\\\",\"\n          + \" \\\"SKEWED_STRING_LIST_VALUES\\\".STRING_LIST_ID,\"\n          + \" \\\"SKEWED_COL_VALUE_LOC_MAP\\\".\\\"LOCATION\\\",\"\n          + \" \\\"SKEWED_STRING_LIST_VALUES\\\".\\\"STRING_LIST_VALUE\\\" \"\n          + \"from \\\"SKEWED_COL_VALUE_LOC_MAP\\\"\"\n          + \"  left outer join \\\"SKEWED_STRING_LIST_VALUES\\\" on \\\"SKEWED_COL_VALUE_LOC_MAP\\\".\"\n          + \"\\\"STRING_LIST_ID_KID\\\" = \\\"SKEWED_STRING_LIST_VALUES\\\".\\\"STRING_LIST_ID\\\" \"\n          + \"where \\\"SKEWED_COL_VALUE_LOC_MAP\\\".\\\"SD_ID\\\" in (\" + sdIds + \")\"\n          + \"  and \\\"SKEWED_COL_VALUE_LOC_MAP\\\".\\\"STRING_LIST_ID_KID\\\" is not null \"\n          + \"order by \\\"SKEWED_COL_VALUE_LOC_MAP\\\".\\\"SD_ID\\\" asc,\"\n          + \"  \\\"SKEWED_STRING_LIST_VALUES\\\".\\\"STRING_LIST_ID\\\" asc,\"\n          + \"  \\\"SKEWED_STRING_LIST_VALUES\\\".\\\"INTEGER_IDX\\\" asc\";\n\n      loopJoinOrderedResult(sds, queryText, 0, new ApplyFunc<StorageDescriptor>() {\n        private Long currentListId;\n        private List<String> currentList;\n        public void apply(StorageDescriptor t, Object[] fields) throws MetaException {\n          if (!t.isSetSkewedInfo()) {\n            SkewedInfo skewedInfo = new SkewedInfo();\n            skewedInfo.setSkewedColValueLocationMaps(new HashMap<List<String>, String>());\n            t.setSkewedInfo(skewedInfo);\n          }\n          Map<List<String>, String> skewMap = t.getSkewedInfo().getSkewedColValueLocationMaps();\n          // Note that this is not a typical list accumulator - there's no call to finalize\n          // the last list. Instead we add list to SD first, as well as locally to add elements.\n          if (fields[1] == null) {\n            currentList = new ArrayList<String>(); // left outer join produced a list with no values\n            currentListId = null;\n          } else {\n            long fieldsListId = extractSqlLong(fields[1]);\n            if (currentListId == null || fieldsListId != currentListId) {\n              currentList = new ArrayList<String>();\n              currentListId = fieldsListId;\n            } else {\n              skewMap.remove(currentList); // value based compare.. remove first\n            }\n            currentList.add((String)fields[3]);\n          }\n          skewMap.put(currentList, (String)fields[2]);\n        }});\n    } // if (hasSkewedColumns)\n\n    // Get FieldSchema stuff if any.\n    if (!colss.isEmpty()) {\n      // We are skipping the CDS table here, as it seems to be totally useless.\n      queryText = \"select \\\"CD_ID\\\", \\\"COMMENT\\\", \\\"COLUMN_NAME\\\", \\\"TYPE_NAME\\\"\"\n          + \" from \\\"COLUMNS_V2\\\" where \\\"CD_ID\\\" in (\" + colIds + \") and \\\"INTEGER_IDX\\\" >= 0\"\n          + \" order by \\\"CD_ID\\\" asc, \\\"INTEGER_IDX\\\" asc\";\n      loopJoinOrderedResult(colss, queryText, 0, new ApplyFunc<List<FieldSchema>>() {\n        public void apply(List<FieldSchema> t, Object[] fields) {\n          t.add(new FieldSchema((String)fields[2], (String)fields[3], (String)fields[1]));\n        }});\n    }\n\n    // Finally, get all the stuff for serdes - just the params.\n    queryText = \"select \\\"SERDE_ID\\\", \\\"PARAM_KEY\\\", \\\"PARAM_VALUE\\\" from \\\"SERDE_PARAMS\\\"\"\n        + \" where \\\"SERDE_ID\\\" in (\" + serdeIds + \") and \\\"PARAM_KEY\\\" is not null\"\n        + \" order by \\\"SERDE_ID\\\" asc\";\n    loopJoinOrderedResult(serdes, queryText, 0, new ApplyFunc<SerDeInfo>() {\n      public void apply(SerDeInfo t, Object[] fields) {\n        t.putToParameters((String)fields[1], (String)fields[2]);\n      }});\n\n    return orderedResult;\n  }",
                "code_after_change": "  private List<Partition> getPartitionsViaSqlFilterInternal(String dbName, String tblName,\n      final Boolean isView, String sqlFilter, List<? extends Object> paramsForFilter,\n      List<String> joinsForFilter, Integer max) throws MetaException {\n    boolean doTrace = LOG.isDebugEnabled();\n    final String dbNameLcase = dbName.toLowerCase(), tblNameLcase = tblName.toLowerCase();\n    // We have to be mindful of order during filtering if we are not returning all partitions.\n    String orderForFilter = (max != null) ? \" order by \\\"PART_NAME\\\" asc\" : \"\";\n\n    // Get all simple fields for partitions and related objects, which we can map one-on-one.\n    // We will do this in 2 queries to use different existing indices for each one.\n    // We do not get table and DB name, assuming they are the same as we are using to filter.\n    // TODO: We might want to tune the indexes instead. With current ones MySQL performs\n    // poorly, esp. with 'order by' w/o index on large tables, even if the number of actual\n    // results is small (query that returns 8 out of 32k partitions can go 4sec. to 0sec. by\n    // just adding a \\\"PART_ID\\\" IN (...) filter that doesn't alter the results to it, probably\n    // causing it to not sort the entire table due to not knowing how selective the filter is.\n    String queryText =\n        \"select \" + PARTITIONS + \".\\\"PART_ID\\\" from \" + PARTITIONS + \"\"\n      + \"  inner join \" + TBLS + \" on \" + PARTITIONS + \".\\\"TBL_ID\\\" = \" + TBLS + \".\\\"TBL_ID\\\" \"\n      + \"    and \" + TBLS + \".\\\"TBL_NAME\\\" = ? \"\n      + \"  inner join \" + DBS + \" on \" + TBLS + \".\\\"DB_ID\\\" = \" + DBS + \".\\\"DB_ID\\\" \"\n      + \"     and \" + DBS + \".\\\"NAME\\\" = ? \"\n      + join(joinsForFilter, ' ')\n      + (StringUtils.isBlank(sqlFilter) ? \"\" : (\" where \" + sqlFilter)) + orderForFilter;\n    Object[] params = new Object[paramsForFilter.size() + 2];\n    params[0] = tblNameLcase;\n    params[1] = dbNameLcase;\n    for (int i = 0; i < paramsForFilter.size(); ++i) {\n      params[i + 2] = paramsForFilter.get(i);\n    }\n\n    long start = doTrace ? System.nanoTime() : 0;\n    Query query = pm.newQuery(\"javax.jdo.query.SQL\", queryText);\n    if (max != null) {\n      query.setRange(0, max.shortValue());\n    }\n    List<Object> sqlResult = executeWithArray(query, params, queryText);\n    long queryTime = doTrace ? System.nanoTime() : 0;\n    timingTrace(doTrace, queryText, start, queryTime);\n    if (sqlResult.isEmpty()) {\n      return Collections.emptyList(); // no partitions, bail early.\n    }\n\n    // Get full objects. For Oracle/etc. do it in batches.\n    List<Partition> result = runBatched(sqlResult, new Batchable<Object, Partition>() {\n      public List<Partition> run(List<Object> input) throws MetaException {\n        return getPartitionsFromPartitionIds(dbNameLcase, tblNameLcase, isView, input);\n      }\n    });\n\n    query.closeAll();\n    return result;\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getNumPartitionsViaSqlFilter": {
                "code_before_change": [],
                "code_after_change": "  public int getNumPartitionsViaSqlFilter(SqlFilterForPushdown filter) throws MetaException {\n    boolean doTrace = LOG.isDebugEnabled();\n    String dbName = filter.table.getDbName().toLowerCase();\n    String tblName = filter.table.getTableName().toLowerCase();\n\n    // Get number of partitions by doing count on PART_ID.\n    String queryText = \"select count(\" + PARTITIONS + \".\\\"PART_ID\\\") from \" + PARTITIONS + \"\"\n      + \"  inner join \" + TBLS + \" on \" + PARTITIONS + \".\\\"TBL_ID\\\" = \" + TBLS + \".\\\"TBL_ID\\\" \"\n      + \"    and \" + TBLS + \".\\\"TBL_NAME\\\" = ? \"\n      + \"  inner join \" + DBS + \" on \" + TBLS + \".\\\"DB_ID\\\" = \" + DBS + \".\\\"DB_ID\\\" \"\n      + \"     and \" + DBS + \".\\\"NAME\\\" = ? \"\n      + join(filter.joins, ' ')\n      + (filter.filter == null || filter.filter.trim().isEmpty() ? \"\" : (\" where \" + filter.filter));\n\n    Object[] params = new Object[filter.params.size() + 2];\n    params[0] = tblName;\n    params[1] = dbName;\n    for (int i = 0; i < filter.params.size(); ++i) {\n      params[i + 2] = filter.params.get(i);\n    }\n\n    long start = doTrace ? System.nanoTime() : 0;\n    Query query = pm.newQuery(\"javax.jdo.query.SQL\", queryText);\n    query.setUnique(true);\n    int sqlResult = extractSqlInt(query.executeWithArray(params));\n    long queryTime = doTrace ? System.nanoTime() : 0;\n    timingTrace(doTrace, queryText, start, queryTime);\n    return sqlResult;\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getColStatsForTablePartitions": {
                "code_before_change": [],
                "code_after_change": "  public Map<String, List<ColumnStatisticsObj>> getColStatsForTablePartitions(String dbName,\n      String tblName) throws MetaException {\n    String queryText =\n        \"select \\\"PARTITION_NAME\\\", \\\"COLUMN_NAME\\\", \\\"COLUMN_TYPE\\\", \\\"LONG_LOW_VALUE\\\", \"\n            + \"\\\"LONG_HIGH_VALUE\\\", \\\"DOUBLE_LOW_VALUE\\\", \\\"DOUBLE_HIGH_VALUE\\\",  \"\n            + \"\\\"BIG_DECIMAL_LOW_VALUE\\\", \\\"BIG_DECIMAL_HIGH_VALUE\\\", \\\"NUM_NULLS\\\", \"\n            + \"\\\"NUM_DISTINCTS\\\", \\\"AVG_COL_LEN\\\", \\\"MAX_COL_LEN\\\", \\\"NUM_TRUES\\\", \\\"NUM_FALSES\\\"\"\n            + \" from \" + PART_COL_STATS + \" where \\\"DB_NAME\\\" = ? and \\\"TABLE_NAME\\\" = ?\"\n            + \" order by \\\"PARTITION_NAME\\\"\";\n    long start = 0;\n    long end = 0;\n    Query query = null;\n    boolean doTrace = LOG.isDebugEnabled();\n    Object qResult = null;\n    start = doTrace ? System.nanoTime() : 0;\n    query = pm.newQuery(\"javax.jdo.query.SQL\", queryText);\n    qResult = executeWithArray(query, prepareParams(dbName, tblName,\n        Collections.<String>emptyList(), Collections.<String>emptyList()), queryText);\n    if (qResult == null) {\n      query.closeAll();\n      return Collections.emptyMap();\n    }\n    end = doTrace ? System.nanoTime() : 0;\n    timingTrace(doTrace, queryText, start, end);\n    List<Object[]> list = ensureList(qResult);\n    Map<String, List<ColumnStatisticsObj>> partColStatsMap =\n        new HashMap<String, List<ColumnStatisticsObj>>();\n    String partNameCurrent = null;\n    List<ColumnStatisticsObj> partColStatsList = new ArrayList<ColumnStatisticsObj>();\n    for (Object[] row : list) {\n      String partName = (String) row[0];\n      if (partNameCurrent == null) {\n        // Update the current partition we are working on\n        partNameCurrent = partName;\n        // Create a new list for this new partition\n        partColStatsList = new ArrayList<ColumnStatisticsObj>();\n        // Add the col stat for the current column\n        partColStatsList.add(prepareCSObj(row, 1));\n      } else if (!partNameCurrent.equalsIgnoreCase(partName)) {\n        // Save the previous partition and its col stat list\n        partColStatsMap.put(partNameCurrent, partColStatsList);\n        // Update the current partition we are working on\n        partNameCurrent = partName;\n        // Create a new list for this new partition\n        partColStatsList = new ArrayList<ColumnStatisticsObj>();\n        // Add the col stat for the current column\n        partColStatsList.add(prepareCSObj(row, 1));\n      } else {\n        partColStatsList.add(prepareCSObj(row, 1));\n      }\n      Deadline.checkTimeout();\n    }\n    query.closeAll();\n    return partColStatsMap;\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreDirectSql.runTestQuery": {
                "code_before_change": [],
                "code_after_change": "  private boolean runTestQuery() {\n    Transaction tx = pm.currentTransaction();\n    boolean doCommit = false;\n    if (!tx.isActive()) {\n      tx.begin();\n      doCommit = true;\n    }\n    Query query = null;\n    // Run a self-test query. If it doesn't work, we will self-disable. What a PITA...\n    String selfTestQuery = \"select \\\"DB_ID\\\" from \" + DBS + \"\";\n    try {\n      prepareTxn();\n      query = pm.newQuery(\"javax.jdo.query.SQL\", selfTestQuery);\n      query.execute();\n      return true;\n    } catch (Throwable t) {\n      doCommit = false;\n      LOG.warn(\"Self-test query [\" + selfTestQuery + \"] failed; direct SQL is disabled\", t);\n      tx.rollback();\n      return false;\n    } finally {\n      if (doCommit) {\n        tx.commit();\n      }\n      if (query != null) {\n        query.closeAll();\n      }\n    }\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getTableStats": {
                "code_before_change": "  public ColumnStatistics getTableStats(\n      String dbName, String tableName, List<String> colNames) throws MetaException {\n    if (colNames.isEmpty()) {\n      return null;\n    }\n    boolean doTrace = LOG.isDebugEnabled();\n    long start = doTrace ? System.nanoTime() : 0;\n    String queryText = \"select \" + STATS_COLLIST + \" from \\\"TAB_COL_STATS\\\" \"\n      + \" where \\\"DB_NAME\\\" = ? and \\\"TABLE_NAME\\\" = ? and \\\"COLUMN_NAME\\\" in (\"\n      + makeParams(colNames.size()) + \")\";\n    Query query = pm.newQuery(\"javax.jdo.query.SQL\", queryText);\n    Object[] params = new Object[colNames.size() + 2];\n    params[0] = dbName;\n    params[1] = tableName;\n    for (int i = 0; i < colNames.size(); ++i) {\n      params[i + 2] = colNames.get(i);\n    }\n    Object qResult = query.executeWithArray(params);\n    long queryTime = doTrace ? System.nanoTime() : 0;\n    if (qResult == null) {\n      query.closeAll();\n      return null;\n    }\n    List<Object[]> list = ensureList(qResult);\n    if (list.isEmpty()) return null;\n    ColumnStatisticsDesc csd = new ColumnStatisticsDesc(true, dbName, tableName);\n    ColumnStatistics result = makeColumnStats(list, csd, 0);\n    timingTrace(doTrace, queryText, start, queryTime);\n    query.closeAll();\n    return result;\n  }",
                "code_after_change": "  public ColumnStatistics getTableStats(final String dbName, final String tableName,\n      List<String> colNames) throws MetaException {\n    if (colNames == null || colNames.isEmpty()) {\n      return null;\n    }\n    final boolean doTrace = LOG.isDebugEnabled();\n    final String queryText0 = \"select \" + STATS_COLLIST + \" from \" + TAB_COL_STATS + \" \"\n          + \" where \\\"DB_NAME\\\" = ? and \\\"TABLE_NAME\\\" = ? and \\\"COLUMN_NAME\\\" in (\";\n    Batchable<String, Object[]> b = new Batchable<String, Object[]>() {\n      public List<Object[]> run(List<String> input) throws MetaException {\n        String queryText = queryText0 + makeParams(input.size()) + \")\";\n        Object[] params = new Object[input.size() + 2];\n        params[0] = dbName;\n        params[1] = tableName;\n        for (int i = 0; i < input.size(); ++i) {\n          params[i + 2] = input.get(i);\n        }\n        long start = doTrace ? System.nanoTime() : 0;\n        Query query = pm.newQuery(\"javax.jdo.query.SQL\", queryText);\n        Object qResult = executeWithArray(query, params, queryText);\n        timingTrace(doTrace, queryText0 + \"...)\", start, (doTrace ? System.nanoTime() : 0));\n        if (qResult == null) {\n          query.closeAll();\n          return null;\n        }\n        addQueryAfterUse(query);\n        return ensureList(qResult);\n      }\n    };\n    List<Object[]> list = runBatched(colNames, b);\n    if (list.isEmpty()) {\n      return null;\n    }\n    ColumnStatisticsDesc csd = new ColumnStatisticsDesc(true, dbName, tableName);\n    ColumnStatistics result = makeColumnStats(list, csd, 0);\n    b.closeAllQueries();\n    return result;\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getPartitionsViaSqlFilter": {
                "code_before_change": "  public List<Partition> getPartitionsViaSqlFilter(\n      String dbName, String tblName, List<String> partNames, Integer max) throws MetaException {\n    if (partNames.isEmpty()) {\n      return new ArrayList<Partition>();\n    }\n    return getPartitionsViaSqlFilterInternal(dbName, tblName, null,\n        \"\\\"PARTITIONS\\\".\\\"PART_NAME\\\" in (\" + makeParams(partNames.size()) + \")\",\n        partNames, new ArrayList<String>(), max);\n  }",
                "code_after_change": "  public List<Partition> getPartitionsViaSqlFilter(final String dbName, final String tblName,\n      List<String> partNames) throws MetaException {\n    if (partNames.isEmpty()) {\n      return Collections.emptyList();\n    }\n    return runBatched(partNames, new Batchable<String, Partition>() {\n      public List<Partition> run(List<String> input) throws MetaException {\n        String filter = \"\" + PARTITIONS + \".\\\"PART_NAME\\\" in (\" + makeParams(input.size()) + \")\";\n        return getPartitionsViaSqlFilterInternal(dbName, tblName, null, filter, input,\n            Collections.<String>emptyList(), null);\n      }\n    });\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreDirectSql.aggrColStatsForPartitions": {
                "code_before_change": [],
                "code_after_change": "  public AggrStats aggrColStatsForPartitions(String dbName, String tableName,\n      List<String> partNames, List<String> colNames, boolean useDensityFunctionForNDVEstimation, double  ndvTuner)\n      throws MetaException {\n    if (colNames.isEmpty() || partNames.isEmpty()) {\n      LOG.debug(\"Columns is empty or partNames is empty : Short-circuiting stats eval\");\n      return new AggrStats(Collections.<ColumnStatisticsObj>emptyList(), 0); // Nothing to aggregate\n    }\n    long partsFound = 0;\n    List<ColumnStatisticsObj> colStatsList;\n    // Try to read from the cache first\n    if (isAggregateStatsCacheEnabled\n        && (partNames.size() < aggrStatsCache.getMaxPartsPerCacheNode())) {\n      AggrColStats colStatsAggrCached;\n      List<ColumnStatisticsObj> colStatsAggrFromDB;\n      int maxPartsPerCacheNode = aggrStatsCache.getMaxPartsPerCacheNode();\n      float fpp = aggrStatsCache.getFalsePositiveProbability();\n      colStatsList = new ArrayList<ColumnStatisticsObj>();\n      // Bloom filter for the new node that we will eventually add to the cache\n      BloomFilter bloomFilter = createPartsBloomFilter(maxPartsPerCacheNode, fpp, partNames);\n      boolean computePartsFound = true;\n      for (String colName : colNames) {\n        // Check the cache first\n        colStatsAggrCached = aggrStatsCache.get(dbName, tableName, colName, partNames);\n        if (colStatsAggrCached != null) {\n          colStatsList.add(colStatsAggrCached.getColStats());\n          partsFound = colStatsAggrCached.getNumPartsCached();\n        } else {\n          if (computePartsFound) {\n            partsFound = partsFoundForPartitions(dbName, tableName, partNames, colNames);\n            computePartsFound = false;\n          }\n          List<String> colNamesForDB = new ArrayList<String>();\n          colNamesForDB.add(colName);\n          // Read aggregated stats for one column\n          colStatsAggrFromDB =\n              columnStatisticsObjForPartitions(dbName, tableName, partNames, colNamesForDB,\n                  partsFound, useDensityFunctionForNDVEstimation, ndvTuner);\n          if (!colStatsAggrFromDB.isEmpty()) {\n            ColumnStatisticsObj colStatsAggr = colStatsAggrFromDB.get(0);\n            colStatsList.add(colStatsAggr);\n            // Update the cache to add this new aggregate node\n            aggrStatsCache.add(dbName, tableName, colName, partsFound, colStatsAggr, bloomFilter);\n          }\n        }\n      }\n    } else {\n      partsFound = partsFoundForPartitions(dbName, tableName, partNames, colNames);\n      colStatsList =\n          columnStatisticsObjForPartitions(dbName, tableName, partNames, colNames, partsFound,\n              useDensityFunctionForNDVEstimation, ndvTuner);\n    }\n    LOG.info(\"useDensityFunctionForNDVEstimation = \" + useDensityFunctionForNDVEstimation\n        + \"\\npartsFound = \" + partsFound + \"\\nColumnStatisticsObj = \"\n        + Arrays.toString(colStatsList.toArray()));\n    return new AggrStats(colStatsList, partsFound);\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report mentions the method 'getPartitionsViaSqlFilterInternal' in the stack trace, which is part of the shared stack trace context with the ground truth methods. However, it does not precisely identify any of the ground truth methods as the root cause. There is no fix suggestion provided in the bug report, either in a dedicated field or within the description. The problem location is partially identified as it mentions a method in the shared stack trace context but not the exact ground truth methods. There is no wrong information in the bug report as all the details provided are relevant to the context of the bug."
        }
    },
    {
        "filename": "HIVE-7114.json",
        "code_diff": {
            "service.src.java.org.apache.hive.service.cli.session.SessionManager.init": {
                "code_before_change": "  public synchronized void init(HiveConf hiveConf) {\n    try {\n      applyAuthorizationConfigPolicy(hiveConf);\n    } catch (HiveException e) {\n      throw new RuntimeException(\"Error applying authorization policy on hive configuration\", e);\n    }\n\n    this.hiveConf = hiveConf;\n    int backgroundPoolSize = hiveConf.getIntVar(ConfVars.HIVE_SERVER2_ASYNC_EXEC_THREADS);\n    LOG.info(\"HiveServer2: Async execution thread pool size: \" + backgroundPoolSize);\n    int backgroundPoolQueueSize = hiveConf.getIntVar(ConfVars.HIVE_SERVER2_ASYNC_EXEC_WAIT_QUEUE_SIZE);\n    LOG.info(\"HiveServer2: Async execution wait queue size: \" + backgroundPoolQueueSize);\n    int keepAliveTime = hiveConf.getIntVar(ConfVars.HIVE_SERVER2_ASYNC_EXEC_KEEPALIVE_TIME);\n    LOG.info(\"HiveServer2: Async execution thread keepalive time: \" + keepAliveTime);\n    // Create a thread pool with #backgroundPoolSize threads\n    // Threads terminate when they are idle for more than the keepAliveTime\n    // An bounded blocking queue is used to queue incoming operations, if #operations > backgroundPoolSize\n    backgroundOperationPool = new ThreadPoolExecutor(backgroundPoolSize, backgroundPoolSize,\n        keepAliveTime, TimeUnit.SECONDS, new LinkedBlockingQueue<Runnable>(backgroundPoolQueueSize));\n    backgroundOperationPool.allowCoreThreadTimeOut(true);\n    addService(operationManager);\n    super.init(hiveConf);\n  }",
                "code_after_change": "  public synchronized void init(HiveConf hiveConf) {\n    try {\n      applyAuthorizationConfigPolicy(hiveConf);\n    } catch (HiveException e) {\n      throw new RuntimeException(\"Error applying authorization policy on hive configuration\", e);\n    }\n\n    this.hiveConf = hiveConf;\n    int backgroundPoolSize = hiveConf.getIntVar(ConfVars.HIVE_SERVER2_ASYNC_EXEC_THREADS);\n    LOG.info(\"HiveServer2: Async execution thread pool size: \" + backgroundPoolSize);\n    int backgroundPoolQueueSize = hiveConf.getIntVar(ConfVars.HIVE_SERVER2_ASYNC_EXEC_WAIT_QUEUE_SIZE);\n    LOG.info(\"HiveServer2: Async execution wait queue size: \" + backgroundPoolQueueSize);\n    int keepAliveTime = hiveConf.getIntVar(ConfVars.HIVE_SERVER2_ASYNC_EXEC_KEEPALIVE_TIME);\n    LOG.info(\"HiveServer2: Async execution thread keepalive time: \" + keepAliveTime);\n    // Create a thread pool with #backgroundPoolSize threads\n    // Threads terminate when they are idle for more than the keepAliveTime\n    // An bounded blocking queue is used to queue incoming operations, if #operations > backgroundPoolSize\n    backgroundOperationPool = new ThreadPoolExecutor(backgroundPoolSize, backgroundPoolSize,\n        keepAliveTime, TimeUnit.SECONDS, new LinkedBlockingQueue<Runnable>(backgroundPoolQueueSize));\n    backgroundOperationPool.allowCoreThreadTimeOut(true);\n    addService(operationManager);\n    super.init(hiveConf);\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report mentions the method 'org.apache.hadoop.hive.ql.exec.tez.TezSessionState.open' in the stack trace, which is in the same stack trace context as the ground truth method 'service.src.java.org.apache.hive.service.cli.session.SessionManager.init'. However, it does not precisely identify the root cause or problem location as the ground truth method. There is no fix suggestion provided in the bug report. All information in the bug report is relevant to the context of the bug, so there is no wrong information."
        }
    },
    {
        "filename": "HIVE-11991.json",
        "code_diff": {
            "ql.src.java.org.apache.hadoop.hive.ql.exec.FetchOperator.createPartValue": {
                "code_before_change": "  private Object[] createPartValue(PartitionDesc partDesc, StructObjectInspector partOI) {\n    Map<String, String> partSpec = partDesc.getPartSpec();\n    List<? extends StructField> fields = partOI.getAllStructFieldRefs();\n    Object[] partValues = new Object[fields.size()];\n    for (int i = 0; i < partValues.length; i++) {\n      StructField field = fields.get(i);\n      String value = partSpec.get(field.getFieldName());\n      ObjectInspector oi = field.getFieldObjectInspector();\n      partValues[i] = ObjectInspectorConverters.getConverter(\n          PrimitiveObjectInspectorFactory.javaStringObjectInspector, oi).convert(value);\n    }\n    return partValues;\n  }",
                "code_after_change": "  private Object[] createPartValue(String[] partKeys, Map<String, String> partSpec, String[] partKeyTypes) {\n    Object[] partValues = new Object[partKeys.length];\n    for (int i = 0; i < partKeys.length; i++) {\n      String key = partKeys[i];\n      ObjectInspector oi = PrimitiveObjectInspectorFactory\n          .getPrimitiveWritableObjectInspector(TypeInfoFactory\n              .getPrimitiveTypeInfo(partKeyTypes[i]));\n      partValues[i] = \n          ObjectInspectorConverters.\n          getConverter(PrimitiveObjectInspectorFactory.\n              javaStringObjectInspector, oi).convert(partSpec.get(key));   \n    }\n    return partValues;\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies a ClassCastException occurring in the stack trace, specifically mentioning methods like FetchTask.fetch and ListSinkOperator.processOp, which are in the same stack trace context as the ground truth method FetchOperator.createPartValue. However, it does not precisely identify the root cause or the exact method where the fix was applied. There is no fix suggestion provided in the bug report. The problem location is partially identified as it mentions methods in the shared stack trace context but not the exact ground truth method. There is no wrong information in the bug report as all mentioned methods and errors are relevant to the context of the bug."
        }
    },
    {
        "filename": "HIVE-17900.json",
        "code_diff": {
            "ql.src.java.org.apache.hadoop.hive.ql.txn.compactor.Worker.gatherStats": {
                "code_before_change": "    void gatherStats() throws IOException {\n      if(!ci.isMajorCompaction()) {\n        return;\n      }\n      if(columnList.isEmpty()) {\n        LOG.debug(\"No existing stats for \"\n            + StatsUtils.getFullyQualifiedTableName(ci.dbname, ci.tableName)\n            + \" found.  Will not run analyze.\");\n        return;//nothing to do\n      }\n      //e.g. analyze table page_view partition(dt='10/15/2014',country=\u2019US\u2019)\n      // compute statistics for columns viewtime\n      StringBuilder sb = new StringBuilder(\"analyze table \")\n          .append(StatsUtils.getFullyQualifiedTableName(ci.dbname, ci.tableName));\n      if(ci.partName != null) {\n        try {\n          sb.append(\" partition(\");\n          Map<String, String> partitionColumnValues = Warehouse.makeEscSpecFromName(ci.partName);\n          for(Map.Entry<String, String> ent : partitionColumnValues.entrySet()) {\n            sb.append(ent.getKey()).append(\"='\").append(ent.getValue()).append(\"'\");\n          }\n          sb.append(\")\");\n        }\n        catch(MetaException ex) {\n          throw new IOException(ex);\n        }\n      }\n      sb.append(\" compute statistics for columns \");\n      for(String colName : columnList) {\n        sb.append(colName).append(\",\");\n      }\n      sb.setLength(sb.length() - 1);//remove trailing ,\n      LOG.info(\"running '\" + sb.toString() + \"'\");\n      Driver d = new Driver(conf, userName);\n      SessionState localSession = null;\n      if(SessionState.get() == null) {\n         localSession = SessionState.start(new SessionState(conf));\n      }\n      try {\n        CommandProcessorResponse cpr = d.run(sb.toString());\n        if (cpr.getResponseCode() != 0) {\n          throw new IOException(\"Could not update stats for table \" + ci.getFullTableName() +\n            (ci.partName == null ? \"\" : \"/\" + ci.partName) + \" due to: \" + cpr);\n        }\n      }\n      catch(CommandNeedRetryException cnre) {\n        throw new IOException(\"Could not update stats for table \" + ci.getFullTableName() +\n          (ci.partName == null ? \"\" : \"/\" + ci.partName) + \" due to: \" + cnre.getMessage());\n      }\n      finally {\n        if(localSession != null) {\n          localSession.close();\n        }\n      }\n    }",
                "code_after_change": "    void gatherStats() throws IOException {\n      if(!ci.isMajorCompaction()) {\n        return;\n      }\n      if(columnList.isEmpty()) {\n        LOG.debug(\"No existing stats for \"\n            + StatsUtils.getFullyQualifiedTableName(ci.dbname, ci.tableName)\n            + \" found.  Will not run analyze.\");\n        return;//nothing to do\n      }\n      //e.g. analyze table page_view partition(dt='10/15/2014',country=\u2019US\u2019)\n      // compute statistics for columns viewtime\n      StringBuilder sb = new StringBuilder(\"analyze table \")\n          .append(StatsUtils.getFullyQualifiedTableName(ci.dbname, ci.tableName));\n      if(ci.partName != null) {\n        try {\n          sb.append(\" partition(\");\n          Map<String, String> partitionColumnValues = Warehouse.makeEscSpecFromName(ci.partName);\n          for(Map.Entry<String, String> ent : partitionColumnValues.entrySet()) {\n            sb.append(ent.getKey()).append(\"='\").append(ent.getValue()).append(\"',\");\n          }\n          sb.setLength(sb.length() - 1); //remove trailing ,\n          sb.append(\")\");\n        }\n        catch(MetaException ex) {\n          throw new IOException(ex);\n        }\n      }\n      sb.append(\" compute statistics for columns \");\n      for(String colName : columnList) {\n        sb.append(colName).append(\",\");\n      }\n      sb.setLength(sb.length() - 1); //remove trailing ,\n      LOG.info(\"running '\" + sb.toString() + \"'\");\n      Driver d = new Driver(conf, userName);\n      SessionState localSession = null;\n      if(SessionState.get() == null) {\n         localSession = SessionState.start(new SessionState(conf));\n      }\n      try {\n        CommandProcessorResponse cpr = d.run(sb.toString());\n        if (cpr.getResponseCode() != 0) {\n          throw new IOException(\"Could not update stats for table \" + ci.getFullTableName() +\n            (ci.partName == null ? \"\" : \"/\" + ci.partName) + \" due to: \" + cpr);\n        }\n      }\n      catch(CommandNeedRetryException cnre) {\n        throw new IOException(\"Could not update stats for table \" + ci.getFullTableName() +\n          (ci.partName == null ? \"\" : \"/\" + ci.partName) + \" due to: \" + cnre.getMessage());\n      }\n      finally {\n        if(localSession != null) {\n          localSession.close();\n        }\n      }\n    }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the issue as a ParseException related to SQL generation, which is consistent with the error in the stack trace. However, it does not precisely identify the root cause in the 'gatherStats' method, but it does mention methods in the same stack trace context. There is no fix suggestion provided in the bug report. The problem location is partially identified as it mentions methods in the stack trace but not the exact ground truth method. There is no wrong information in the bug report as it accurately describes the error encountered."
        }
    },
    {
        "filename": "HIVE-10816.json",
        "code_diff": {
            "ql.src.java.org.apache.hadoop.hive.ql.exec.mr.ExecDriver.handleSampling": {
                "code_before_change": "  private void handleSampling(DriverContext context, MapWork mWork, JobConf job, HiveConf conf)\n      throws Exception {\n    assert mWork.getAliasToWork().keySet().size() == 1;\n\n    String alias = mWork.getAliases().get(0);\n    Operator<?> topOp = mWork.getAliasToWork().get(alias);\n    PartitionDesc partDesc = mWork.getAliasToPartnInfo().get(alias);\n\n    ArrayList<String> paths = mWork.getPaths();\n    ArrayList<PartitionDesc> parts = mWork.getPartitionDescs();\n\n    List<Path> inputPaths = new ArrayList<Path>(paths.size());\n    for (String path : paths) {\n      inputPaths.add(new Path(path));\n    }\n\n    Path tmpPath = context.getCtx().getExternalTmpPath(inputPaths.get(0));\n    Path partitionFile = new Path(tmpPath, \".partitions\");\n    ShimLoader.getHadoopShims().setTotalOrderPartitionFile(job, partitionFile);\n    PartitionKeySampler sampler = new PartitionKeySampler();\n\n    if (mWork.getSamplingType() == MapWork.SAMPLING_ON_PREV_MR) {\n      console.printInfo(\"Use sampling data created in previous MR\");\n      // merges sampling data from previous MR and make partition keys for total sort\n      for (Path path : inputPaths) {\n        FileSystem fs = path.getFileSystem(job);\n        for (FileStatus status : fs.globStatus(new Path(path, \".sampling*\"))) {\n          sampler.addSampleFile(status.getPath(), job);\n        }\n      }\n    } else if (mWork.getSamplingType() == MapWork.SAMPLING_ON_START) {\n      console.printInfo(\"Creating sampling data..\");\n      assert topOp instanceof TableScanOperator;\n      TableScanOperator ts = (TableScanOperator) topOp;\n\n      FetchWork fetchWork;\n      if (!partDesc.isPartitioned()) {\n        assert paths.size() == 1;\n        fetchWork = new FetchWork(inputPaths.get(0), partDesc.getTableDesc());\n      } else {\n        fetchWork = new FetchWork(inputPaths, parts, partDesc.getTableDesc());\n      }\n      fetchWork.setSource(ts);\n\n      // random sampling\n      FetchOperator fetcher = PartitionKeySampler.createSampler(fetchWork, conf, job, ts);\n      try {\n        ts.initialize(conf, new ObjectInspector[]{fetcher.getOutputObjectInspector()});\n        OperatorUtils.setChildrenCollector(ts.getChildOperators(), sampler);\n        while (fetcher.pushRow()) { }\n      } finally {\n        fetcher.clearFetchContext();\n      }\n    } else {\n      throw new IllegalArgumentException(\"Invalid sampling type \" + mWork.getSamplingType());\n    }\n    sampler.writePartitionKeys(partitionFile, conf, job);\n  }",
                "code_after_change": "  private void handleSampling(Context context, MapWork mWork, JobConf job)\n      throws Exception {\n    assert mWork.getAliasToWork().keySet().size() == 1;\n\n    String alias = mWork.getAliases().get(0);\n    Operator<?> topOp = mWork.getAliasToWork().get(alias);\n    PartitionDesc partDesc = mWork.getAliasToPartnInfo().get(alias);\n\n    ArrayList<String> paths = mWork.getPaths();\n    ArrayList<PartitionDesc> parts = mWork.getPartitionDescs();\n\n    List<Path> inputPaths = new ArrayList<Path>(paths.size());\n    for (String path : paths) {\n      inputPaths.add(new Path(path));\n    }\n\n    Path tmpPath = context.getExternalTmpPath(inputPaths.get(0));\n    Path partitionFile = new Path(tmpPath, \".partitions\");\n    ShimLoader.getHadoopShims().setTotalOrderPartitionFile(job, partitionFile);\n    PartitionKeySampler sampler = new PartitionKeySampler();\n\n    if (mWork.getSamplingType() == MapWork.SAMPLING_ON_PREV_MR) {\n      console.printInfo(\"Use sampling data created in previous MR\");\n      // merges sampling data from previous MR and make partition keys for total sort\n      for (Path path : inputPaths) {\n        FileSystem fs = path.getFileSystem(job);\n        for (FileStatus status : fs.globStatus(new Path(path, \".sampling*\"))) {\n          sampler.addSampleFile(status.getPath(), job);\n        }\n      }\n    } else if (mWork.getSamplingType() == MapWork.SAMPLING_ON_START) {\n      console.printInfo(\"Creating sampling data..\");\n      assert topOp instanceof TableScanOperator;\n      TableScanOperator ts = (TableScanOperator) topOp;\n\n      FetchWork fetchWork;\n      if (!partDesc.isPartitioned()) {\n        assert paths.size() == 1;\n        fetchWork = new FetchWork(inputPaths.get(0), partDesc.getTableDesc());\n      } else {\n        fetchWork = new FetchWork(inputPaths, parts, partDesc.getTableDesc());\n      }\n      fetchWork.setSource(ts);\n\n      // random sampling\n      FetchOperator fetcher = PartitionKeySampler.createSampler(fetchWork, job, ts);\n      try {\n        ts.initialize(job, new ObjectInspector[]{fetcher.getOutputObjectInspector()});\n        OperatorUtils.setChildrenCollector(ts.getChildOperators(), sampler);\n        while (fetcher.pushRow()) { }\n      } finally {\n        fetcher.clearFetchContext();\n      }\n    } else {\n      throw new IllegalArgumentException(\"Invalid sampling type \" + mWork.getSamplingType());\n    }\n    sampler.writePartitionKeys(partitionFile, job);\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.exec.PartitionKeySampler.toPartitionKeys": {
                "code_before_change": "  static final byte[][] toPartitionKeys(byte[][] sorted, int numPartition) {\n    byte[][] partitionKeys = new byte[numPartition - 1][];\n\n    int last = 0;\n    int current = 0;\n    for(int i = 0; i < numPartition - 1; i++) {\n      current += Math.round((float)(sorted.length - current) / (numPartition - i));\n      while (i > 0 && current < sorted.length && C.compare(sorted[last], sorted[current]) == 0) {\n        current++;\n      }\n      if (current >= sorted.length) {\n        return Arrays.copyOfRange(partitionKeys, 0, i);\n      }\n      if (LOG.isDebugEnabled()) {\n        // print out nth partition key for debugging\n        LOG.debug(\"Partition key \" + current + \"th :\" + new BytesWritable(sorted[current]));\n      }\n      partitionKeys[i] = sorted[current];\n      last = current;\n    }\n    return partitionKeys;\n  }",
                "code_after_change": "  static final byte[][] toPartitionKeys(byte[][] sorted, int numPartition) {\n    byte[][] partitionKeys = new byte[numPartition - 1][];\n\n    int last = 0;\n    int current = 0;\n    for(int i = 0; i < numPartition - 1; i++) {\n      current += Math.round((float)(sorted.length - current) / (numPartition - i));\n      while (i > 0 && current < sorted.length && C.compare(sorted[last], sorted[current]) == 0) {\n        current++;\n      }\n      if (current >= sorted.length) {\n        return Arrays.copyOfRange(partitionKeys, 0, i);\n      }\n      if (LOG.isDebugEnabled()) {\n        // print out nth partition key for debugging\n        LOG.debug(\"Partition key \" + current + \"th :\" + new BytesWritable(sorted[current]));\n      }\n      partitionKeys[i] = sorted[current];\n      last = current;\n    }\n    return partitionKeys;\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.exec.mr.ExecDriver.execute": {
                "code_before_change": "  public int execute(DriverContext driverContext) {\n\n    IOPrepareCache ioPrepareCache = IOPrepareCache.get();\n    ioPrepareCache.clear();\n\n    boolean success = true;\n\n    Context ctx = driverContext.getCtx();\n    boolean ctxCreated = false;\n    Path emptyScratchDir;\n\n    MapWork mWork = work.getMapWork();\n    ReduceWork rWork = work.getReduceWork();\n\n    try {\n      if (ctx == null) {\n        ctx = new Context(job);\n        ctxCreated = true;\n      }\n\n      emptyScratchDir = ctx.getMRTmpPath();\n      FileSystem fs = emptyScratchDir.getFileSystem(job);\n      fs.mkdirs(emptyScratchDir);\n    } catch (IOException e) {\n      e.printStackTrace();\n      console.printError(\"Error launching map-reduce job\", \"\\n\"\n          + org.apache.hadoop.util.StringUtils.stringifyException(e));\n      return 5;\n    }\n\n    HiveFileFormatUtils.prepareJobOutput(job);\n    //See the javadoc on HiveOutputFormatImpl and HadoopShims.prepareJobOutput()\n    job.setOutputFormat(HiveOutputFormatImpl.class);\n\n    job.setMapperClass(ExecMapper.class);\n\n    job.setMapOutputKeyClass(HiveKey.class);\n    job.setMapOutputValueClass(BytesWritable.class);\n\n    try {\n      String partitioner = HiveConf.getVar(job, ConfVars.HIVEPARTITIONER);\n      job.setPartitionerClass((Class<? extends Partitioner>) JavaUtils.loadClass(partitioner));\n    } catch (ClassNotFoundException e) {\n      throw new RuntimeException(e.getMessage(), e);\n    }\n\n    if (mWork.getNumMapTasks() != null) {\n      job.setNumMapTasks(mWork.getNumMapTasks().intValue());\n    }\n\n    if (mWork.getMaxSplitSize() != null) {\n      HiveConf.setLongVar(job, HiveConf.ConfVars.MAPREDMAXSPLITSIZE, mWork.getMaxSplitSize().longValue());\n    }\n\n    if (mWork.getMinSplitSize() != null) {\n      HiveConf.setLongVar(job, HiveConf.ConfVars.MAPREDMINSPLITSIZE, mWork.getMinSplitSize().longValue());\n    }\n\n    if (mWork.getMinSplitSizePerNode() != null) {\n      HiveConf.setLongVar(job, HiveConf.ConfVars.MAPREDMINSPLITSIZEPERNODE, mWork.getMinSplitSizePerNode().longValue());\n    }\n\n    if (mWork.getMinSplitSizePerRack() != null) {\n      HiveConf.setLongVar(job, HiveConf.ConfVars.MAPREDMINSPLITSIZEPERRACK, mWork.getMinSplitSizePerRack().longValue());\n    }\n\n    job.setNumReduceTasks(rWork != null ? rWork.getNumReduceTasks().intValue() : 0);\n    job.setReducerClass(ExecReducer.class);\n\n    // set input format information if necessary\n    setInputAttributes(job);\n\n    // Turn on speculative execution for reducers\n    boolean useSpeculativeExecReducers = HiveConf.getBoolVar(job,\n        HiveConf.ConfVars.HIVESPECULATIVEEXECREDUCERS);\n    HiveConf.setBoolVar(job, HiveConf.ConfVars.HADOOPSPECULATIVEEXECREDUCERS,\n        useSpeculativeExecReducers);\n\n    String inpFormat = HiveConf.getVar(job, HiveConf.ConfVars.HIVEINPUTFORMAT);\n\n    if (mWork.isUseBucketizedHiveInputFormat()) {\n      inpFormat = BucketizedHiveInputFormat.class.getName();\n    }\n\n    LOG.info(\"Using \" + inpFormat);\n\n    try {\n      job.setInputFormat((Class<? extends InputFormat>) JavaUtils.loadClass(inpFormat));\n    } catch (ClassNotFoundException e) {\n      throw new RuntimeException(e.getMessage(), e);\n    }\n\n\n    // No-Op - we don't really write anything here ..\n    job.setOutputKeyClass(Text.class);\n    job.setOutputValueClass(Text.class);\n\n    // Transfer HIVEAUXJARS and HIVEADDEDJARS to \"tmpjars\" so hadoop understands\n    // it\n    String auxJars = HiveConf.getVar(job, HiveConf.ConfVars.HIVEAUXJARS);\n    String addedJars = HiveConf.getVar(job, HiveConf.ConfVars.HIVEADDEDJARS);\n    if (StringUtils.isNotBlank(auxJars) || StringUtils.isNotBlank(addedJars)) {\n      String allJars = StringUtils.isNotBlank(auxJars) ? (StringUtils.isNotBlank(addedJars) ? addedJars\n          + \",\" + auxJars\n          : auxJars)\n          : addedJars;\n      LOG.info(\"adding libjars: \" + allJars);\n      initializeFiles(\"tmpjars\", allJars);\n    }\n\n    // Transfer HIVEADDEDFILES to \"tmpfiles\" so hadoop understands it\n    String addedFiles = HiveConf.getVar(job, HiveConf.ConfVars.HIVEADDEDFILES);\n    if (StringUtils.isNotBlank(addedFiles)) {\n      initializeFiles(\"tmpfiles\", addedFiles);\n    }\n    int returnVal = 0;\n    boolean noName = StringUtils.isEmpty(HiveConf.getVar(job, HiveConf.ConfVars.HADOOPJOBNAME));\n\n    if (noName) {\n      // This is for a special case to ensure unit tests pass\n      HiveConf.setVar(job, HiveConf.ConfVars.HADOOPJOBNAME, \"JOB\" + Utilities.randGen.nextInt());\n    }\n    String addedArchives = HiveConf.getVar(job, HiveConf.ConfVars.HIVEADDEDARCHIVES);\n    // Transfer HIVEADDEDARCHIVES to \"tmparchives\" so hadoop understands it\n    if (StringUtils.isNotBlank(addedArchives)) {\n      initializeFiles(\"tmparchives\", addedArchives);\n    }\n\n    try{\n      MapredLocalWork localwork = mWork.getMapRedLocalWork();\n      if (localwork != null && localwork.hasStagedAlias()) {\n        if (!ShimLoader.getHadoopShims().isLocalMode(job)) {\n          Path localPath = localwork.getTmpPath();\n          Path hdfsPath = mWork.getTmpHDFSPath();\n\n          FileSystem hdfs = hdfsPath.getFileSystem(job);\n          FileSystem localFS = localPath.getFileSystem(job);\n          FileStatus[] hashtableFiles = localFS.listStatus(localPath);\n          int fileNumber = hashtableFiles.length;\n          String[] fileNames = new String[fileNumber];\n\n          for ( int i = 0; i < fileNumber; i++){\n            fileNames[i] = hashtableFiles[i].getPath().getName();\n          }\n\n          //package and compress all the hashtable files to an archive file\n          String stageId = this.getId();\n          String archiveFileName = Utilities.generateTarFileName(stageId);\n          localwork.setStageID(stageId);\n\n          CompressionUtils.tar(localPath.toUri().getPath(), fileNames,archiveFileName);\n          Path archivePath = Utilities.generateTarPath(localPath, stageId);\n          LOG.info(\"Archive \"+ hashtableFiles.length+\" hash table files to \" + archivePath);\n\n          //upload archive file to hdfs\n          Path hdfsFilePath =Utilities.generateTarPath(hdfsPath, stageId);\n          short replication = (short) job.getInt(\"mapred.submit.replication\", 10);\n          hdfs.copyFromLocalFile(archivePath, hdfsFilePath);\n          hdfs.setReplication(hdfsFilePath, replication);\n          LOG.info(\"Upload 1 archive file  from\" + archivePath + \" to: \" + hdfsFilePath);\n\n          //add the archive file to distributed cache\n          DistributedCache.createSymlink(job);\n          DistributedCache.addCacheArchive(hdfsFilePath.toUri(), job);\n          LOG.info(\"Add 1 archive file to distributed cache. Archive file: \" + hdfsFilePath.toUri());\n        }\n      }\n      work.configureJobConf(job);\n      List<Path> inputPaths = Utilities.getInputPaths(job, mWork, emptyScratchDir, ctx, false);\n      Utilities.setInputPaths(job, inputPaths);\n\n      Utilities.setMapRedWork(job, work, ctx.getMRTmpPath());\n\n      if (mWork.getSamplingType() > 0 && rWork != null && job.getNumReduceTasks() > 1) {\n        try {\n          handleSampling(driverContext, mWork, job, conf);\n          job.setPartitionerClass(HiveTotalOrderPartitioner.class);\n        } catch (IllegalStateException e) {\n          console.printInfo(\"Not enough sampling data.. Rolling back to single reducer task\");\n          rWork.setNumReduceTasks(1);\n          job.setNumReduceTasks(1);\n        } catch (Exception e) {\n          LOG.error(\"Sampling error\", e);\n          console.printError(e.toString(),\n              \"\\n\" + org.apache.hadoop.util.StringUtils.stringifyException(e));\n          rWork.setNumReduceTasks(1);\n          job.setNumReduceTasks(1);\n        }\n      }\n\n      // remove the pwd from conf file so that job tracker doesn't show this\n      // logs\n      String pwd = HiveConf.getVar(job, HiveConf.ConfVars.METASTOREPWD);\n      if (pwd != null) {\n        HiveConf.setVar(job, HiveConf.ConfVars.METASTOREPWD, \"HIVE\");\n      }\n      LOG.error(job.get(\"mapreduce.framework.name\"));\n      JobClient jc = new JobClient(job);\n      // make this client wait if job tracker is not behaving well.\n      Throttle.checkJobTracker(job, LOG);\n\n      if (mWork.isGatheringStats() || (rWork != null && rWork.isGatheringStats())) {\n        // initialize stats publishing table\n        StatsPublisher statsPublisher;\n        StatsFactory factory = StatsFactory.newFactory(job);\n        if (factory != null) {\n          statsPublisher = factory.getStatsPublisher();\n          if (!statsPublisher.init(job)) { // creating stats table if not exists\n            if (HiveConf.getBoolVar(job, HiveConf.ConfVars.HIVE_STATS_RELIABLE)) {\n              throw\n                new HiveException(ErrorMsg.STATSPUBLISHER_INITIALIZATION_ERROR.getErrorCodedMsg());\n            }\n          }\n        }\n      }\n\n      Utilities.createTmpDirs(job, mWork);\n      Utilities.createTmpDirs(job, rWork);\n\n      SessionState ss = SessionState.get();\n      if (HiveConf.getVar(job, HiveConf.ConfVars.HIVE_EXECUTION_ENGINE).equals(\"tez\")\n          && ss != null) {\n        TezSessionState session = ss.getTezSession();\n        TezSessionPoolManager.getInstance().close(session, true);\n      }\n\n      // Finally SUBMIT the JOB!\n      rj = jc.submitJob(job);\n      // replace it back\n      if (pwd != null) {\n        HiveConf.setVar(job, HiveConf.ConfVars.METASTOREPWD, pwd);\n      }\n\n      returnVal = jobExecHelper.progress(rj, jc, ctx.getHiveTxnManager());\n      success = (returnVal == 0);\n    } catch (Exception e) {\n      e.printStackTrace();\n      String mesg = \" with exception '\" + Utilities.getNameMessage(e) + \"'\";\n      if (rj != null) {\n        mesg = \"Ended Job = \" + rj.getJobID() + mesg;\n      } else {\n        mesg = \"Job Submission failed\" + mesg;\n      }\n\n      // Has to use full name to make sure it does not conflict with\n      // org.apache.commons.lang.StringUtils\n      console.printError(mesg, \"\\n\" + org.apache.hadoop.util.StringUtils.stringifyException(e));\n\n      success = false;\n      returnVal = 1;\n    } finally {\n      Utilities.clearWork(job);\n      try {\n        if (ctxCreated) {\n          ctx.clear();\n        }\n\n        if (rj != null) {\n          if (returnVal != 0) {\n            rj.killJob();\n          }\n          HadoopJobExecHelper.runningJobs.remove(rj);\n          jobID = rj.getID().toString();\n        }\n      } catch (Exception e) {\n      }\n    }\n\n    // get the list of Dynamic partition paths\n    try {\n      if (rj != null) {\n        if (mWork.getAliasToWork() != null) {\n          for (Operator<? extends OperatorDesc> op : mWork.getAliasToWork().values()) {\n            op.jobClose(job, success);\n          }\n        }\n        if (rWork != null) {\n          rWork.getReducer().jobClose(job, success);\n        }\n      }\n    } catch (Exception e) {\n      // jobClose needs to execute successfully otherwise fail task\n      if (success) {\n        success = false;\n        returnVal = 3;\n        String mesg = \"Job Commit failed with exception '\" + Utilities.getNameMessage(e) + \"'\";\n        console.printError(mesg, \"\\n\" + org.apache.hadoop.util.StringUtils.stringifyException(e));\n      }\n    }\n\n    return (returnVal);\n  }",
                "code_after_change": "  public int execute(DriverContext driverContext) {\n\n    IOPrepareCache ioPrepareCache = IOPrepareCache.get();\n    ioPrepareCache.clear();\n\n    boolean success = true;\n\n    Context ctx = driverContext.getCtx();\n    boolean ctxCreated = false;\n    Path emptyScratchDir;\n\n    MapWork mWork = work.getMapWork();\n    ReduceWork rWork = work.getReduceWork();\n\n    try {\n      if (ctx == null) {\n        ctx = new Context(job);\n        ctxCreated = true;\n      }\n\n      emptyScratchDir = ctx.getMRTmpPath();\n      FileSystem fs = emptyScratchDir.getFileSystem(job);\n      fs.mkdirs(emptyScratchDir);\n    } catch (IOException e) {\n      e.printStackTrace();\n      console.printError(\"Error launching map-reduce job\", \"\\n\"\n          + org.apache.hadoop.util.StringUtils.stringifyException(e));\n      return 5;\n    }\n\n    HiveFileFormatUtils.prepareJobOutput(job);\n    //See the javadoc on HiveOutputFormatImpl and HadoopShims.prepareJobOutput()\n    job.setOutputFormat(HiveOutputFormatImpl.class);\n\n    job.setMapperClass(ExecMapper.class);\n\n    job.setMapOutputKeyClass(HiveKey.class);\n    job.setMapOutputValueClass(BytesWritable.class);\n\n    try {\n      String partitioner = HiveConf.getVar(job, ConfVars.HIVEPARTITIONER);\n      job.setPartitionerClass((Class<? extends Partitioner>) JavaUtils.loadClass(partitioner));\n    } catch (ClassNotFoundException e) {\n      throw new RuntimeException(e.getMessage(), e);\n    }\n\n    if (mWork.getNumMapTasks() != null) {\n      job.setNumMapTasks(mWork.getNumMapTasks().intValue());\n    }\n\n    if (mWork.getMaxSplitSize() != null) {\n      HiveConf.setLongVar(job, HiveConf.ConfVars.MAPREDMAXSPLITSIZE, mWork.getMaxSplitSize().longValue());\n    }\n\n    if (mWork.getMinSplitSize() != null) {\n      HiveConf.setLongVar(job, HiveConf.ConfVars.MAPREDMINSPLITSIZE, mWork.getMinSplitSize().longValue());\n    }\n\n    if (mWork.getMinSplitSizePerNode() != null) {\n      HiveConf.setLongVar(job, HiveConf.ConfVars.MAPREDMINSPLITSIZEPERNODE, mWork.getMinSplitSizePerNode().longValue());\n    }\n\n    if (mWork.getMinSplitSizePerRack() != null) {\n      HiveConf.setLongVar(job, HiveConf.ConfVars.MAPREDMINSPLITSIZEPERRACK, mWork.getMinSplitSizePerRack().longValue());\n    }\n\n    job.setNumReduceTasks(rWork != null ? rWork.getNumReduceTasks().intValue() : 0);\n    job.setReducerClass(ExecReducer.class);\n\n    // set input format information if necessary\n    setInputAttributes(job);\n\n    // Turn on speculative execution for reducers\n    boolean useSpeculativeExecReducers = HiveConf.getBoolVar(job,\n        HiveConf.ConfVars.HIVESPECULATIVEEXECREDUCERS);\n    HiveConf.setBoolVar(job, HiveConf.ConfVars.HADOOPSPECULATIVEEXECREDUCERS,\n        useSpeculativeExecReducers);\n\n    String inpFormat = HiveConf.getVar(job, HiveConf.ConfVars.HIVEINPUTFORMAT);\n\n    if (mWork.isUseBucketizedHiveInputFormat()) {\n      inpFormat = BucketizedHiveInputFormat.class.getName();\n    }\n\n    LOG.info(\"Using \" + inpFormat);\n\n    try {\n      job.setInputFormat((Class<? extends InputFormat>) JavaUtils.loadClass(inpFormat));\n    } catch (ClassNotFoundException e) {\n      throw new RuntimeException(e.getMessage(), e);\n    }\n\n\n    // No-Op - we don't really write anything here ..\n    job.setOutputKeyClass(Text.class);\n    job.setOutputValueClass(Text.class);\n\n    // Transfer HIVEAUXJARS and HIVEADDEDJARS to \"tmpjars\" so hadoop understands\n    // it\n    String auxJars = HiveConf.getVar(job, HiveConf.ConfVars.HIVEAUXJARS);\n    String addedJars = HiveConf.getVar(job, HiveConf.ConfVars.HIVEADDEDJARS);\n    if (StringUtils.isNotBlank(auxJars) || StringUtils.isNotBlank(addedJars)) {\n      String allJars = StringUtils.isNotBlank(auxJars) ? (StringUtils.isNotBlank(addedJars) ? addedJars\n          + \",\" + auxJars\n          : auxJars)\n          : addedJars;\n      LOG.info(\"adding libjars: \" + allJars);\n      initializeFiles(\"tmpjars\", allJars);\n    }\n\n    // Transfer HIVEADDEDFILES to \"tmpfiles\" so hadoop understands it\n    String addedFiles = HiveConf.getVar(job, HiveConf.ConfVars.HIVEADDEDFILES);\n    if (StringUtils.isNotBlank(addedFiles)) {\n      initializeFiles(\"tmpfiles\", addedFiles);\n    }\n    int returnVal = 0;\n    boolean noName = StringUtils.isEmpty(HiveConf.getVar(job, HiveConf.ConfVars.HADOOPJOBNAME));\n\n    if (noName) {\n      // This is for a special case to ensure unit tests pass\n      HiveConf.setVar(job, HiveConf.ConfVars.HADOOPJOBNAME, \"JOB\" + Utilities.randGen.nextInt());\n    }\n    String addedArchives = HiveConf.getVar(job, HiveConf.ConfVars.HIVEADDEDARCHIVES);\n    // Transfer HIVEADDEDARCHIVES to \"tmparchives\" so hadoop understands it\n    if (StringUtils.isNotBlank(addedArchives)) {\n      initializeFiles(\"tmparchives\", addedArchives);\n    }\n\n    try{\n      MapredLocalWork localwork = mWork.getMapRedLocalWork();\n      if (localwork != null && localwork.hasStagedAlias()) {\n        if (!ShimLoader.getHadoopShims().isLocalMode(job)) {\n          Path localPath = localwork.getTmpPath();\n          Path hdfsPath = mWork.getTmpHDFSPath();\n\n          FileSystem hdfs = hdfsPath.getFileSystem(job);\n          FileSystem localFS = localPath.getFileSystem(job);\n          FileStatus[] hashtableFiles = localFS.listStatus(localPath);\n          int fileNumber = hashtableFiles.length;\n          String[] fileNames = new String[fileNumber];\n\n          for ( int i = 0; i < fileNumber; i++){\n            fileNames[i] = hashtableFiles[i].getPath().getName();\n          }\n\n          //package and compress all the hashtable files to an archive file\n          String stageId = this.getId();\n          String archiveFileName = Utilities.generateTarFileName(stageId);\n          localwork.setStageID(stageId);\n\n          CompressionUtils.tar(localPath.toUri().getPath(), fileNames,archiveFileName);\n          Path archivePath = Utilities.generateTarPath(localPath, stageId);\n          LOG.info(\"Archive \"+ hashtableFiles.length+\" hash table files to \" + archivePath);\n\n          //upload archive file to hdfs\n          Path hdfsFilePath =Utilities.generateTarPath(hdfsPath, stageId);\n          short replication = (short) job.getInt(\"mapred.submit.replication\", 10);\n          hdfs.copyFromLocalFile(archivePath, hdfsFilePath);\n          hdfs.setReplication(hdfsFilePath, replication);\n          LOG.info(\"Upload 1 archive file  from\" + archivePath + \" to: \" + hdfsFilePath);\n\n          //add the archive file to distributed cache\n          DistributedCache.createSymlink(job);\n          DistributedCache.addCacheArchive(hdfsFilePath.toUri(), job);\n          LOG.info(\"Add 1 archive file to distributed cache. Archive file: \" + hdfsFilePath.toUri());\n        }\n      }\n      work.configureJobConf(job);\n      List<Path> inputPaths = Utilities.getInputPaths(job, mWork, emptyScratchDir, ctx, false);\n      Utilities.setInputPaths(job, inputPaths);\n\n      Utilities.setMapRedWork(job, work, ctx.getMRTmpPath());\n\n      if (mWork.getSamplingType() > 0 && rWork != null && job.getNumReduceTasks() > 1) {\n        try {\n          handleSampling(ctx, mWork, job);\n          job.setPartitionerClass(HiveTotalOrderPartitioner.class);\n        } catch (IllegalStateException e) {\n          console.printInfo(\"Not enough sampling data.. Rolling back to single reducer task\");\n          rWork.setNumReduceTasks(1);\n          job.setNumReduceTasks(1);\n        } catch (Exception e) {\n          LOG.error(\"Sampling error\", e);\n          console.printError(e.toString(),\n              \"\\n\" + org.apache.hadoop.util.StringUtils.stringifyException(e));\n          rWork.setNumReduceTasks(1);\n          job.setNumReduceTasks(1);\n        }\n      }\n\n      // remove the pwd from conf file so that job tracker doesn't show this\n      // logs\n      String pwd = HiveConf.getVar(job, HiveConf.ConfVars.METASTOREPWD);\n      if (pwd != null) {\n        HiveConf.setVar(job, HiveConf.ConfVars.METASTOREPWD, \"HIVE\");\n      }\n      LOG.error(job.get(\"mapreduce.framework.name\"));\n      JobClient jc = new JobClient(job);\n      // make this client wait if job tracker is not behaving well.\n      Throttle.checkJobTracker(job, LOG);\n\n      if (mWork.isGatheringStats() || (rWork != null && rWork.isGatheringStats())) {\n        // initialize stats publishing table\n        StatsPublisher statsPublisher;\n        StatsFactory factory = StatsFactory.newFactory(job);\n        if (factory != null) {\n          statsPublisher = factory.getStatsPublisher();\n          if (!statsPublisher.init(job)) { // creating stats table if not exists\n            if (HiveConf.getBoolVar(job, HiveConf.ConfVars.HIVE_STATS_RELIABLE)) {\n              throw\n                new HiveException(ErrorMsg.STATSPUBLISHER_INITIALIZATION_ERROR.getErrorCodedMsg());\n            }\n          }\n        }\n      }\n\n      Utilities.createTmpDirs(job, mWork);\n      Utilities.createTmpDirs(job, rWork);\n\n      SessionState ss = SessionState.get();\n      if (HiveConf.getVar(job, HiveConf.ConfVars.HIVE_EXECUTION_ENGINE).equals(\"tez\")\n          && ss != null) {\n        TezSessionState session = ss.getTezSession();\n        TezSessionPoolManager.getInstance().close(session, true);\n      }\n\n      // Finally SUBMIT the JOB!\n      rj = jc.submitJob(job);\n      // replace it back\n      if (pwd != null) {\n        HiveConf.setVar(job, HiveConf.ConfVars.METASTOREPWD, pwd);\n      }\n\n      returnVal = jobExecHelper.progress(rj, jc, ctx.getHiveTxnManager());\n      success = (returnVal == 0);\n    } catch (Exception e) {\n      e.printStackTrace();\n      String mesg = \" with exception '\" + Utilities.getNameMessage(e) + \"'\";\n      if (rj != null) {\n        mesg = \"Ended Job = \" + rj.getJobID() + mesg;\n      } else {\n        mesg = \"Job Submission failed\" + mesg;\n      }\n\n      // Has to use full name to make sure it does not conflict with\n      // org.apache.commons.lang.StringUtils\n      console.printError(mesg, \"\\n\" + org.apache.hadoop.util.StringUtils.stringifyException(e));\n\n      success = false;\n      returnVal = 1;\n    } finally {\n      Utilities.clearWork(job);\n      try {\n        if (ctxCreated) {\n          ctx.clear();\n        }\n\n        if (rj != null) {\n          if (returnVal != 0) {\n            rj.killJob();\n          }\n          jobID = rj.getID().toString();\n        }\n      } catch (Exception e) {\n\tLOG.warn(e);\n      } finally {\n\tHadoopJobExecHelper.runningJobs.remove(rj);\n      }\n    }\n\n    // get the list of Dynamic partition paths\n    try {\n      if (rj != null) {\n        if (mWork.getAliasToWork() != null) {\n          for (Operator<? extends OperatorDesc> op : mWork.getAliasToWork().values()) {\n            op.jobClose(job, success);\n          }\n        }\n        if (rWork != null) {\n          rWork.getReducer().jobClose(job, success);\n        }\n      }\n    } catch (Exception e) {\n      // jobClose needs to execute successfully otherwise fail task\n      if (success) {\n        success = false;\n        returnVal = 3;\n        String mesg = \"Job Commit failed with exception '\" + Utilities.getNameMessage(e) + \"'\";\n        console.printError(mesg, \"\\n\" + org.apache.hadoop.util.StringUtils.stringifyException(e));\n      }\n    }\n\n    return (returnVal);\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.exec.PartitionKeySampler.writePartitionKeys": {
                "code_before_change": "  public void writePartitionKeys(Path path, HiveConf conf, JobConf job) throws IOException {\n    byte[][] partitionKeys = getPartitionKeys(job.getNumReduceTasks());\n    int numPartition = partitionKeys.length + 1;\n    if (numPartition != job.getNumReduceTasks()) {\n      job.setNumReduceTasks(numPartition);\n    }\n\n    FileSystem fs = path.getFileSystem(job);\n    SequenceFile.Writer writer = SequenceFile.createWriter(fs, job, path,\n        BytesWritable.class, NullWritable.class);\n    try {\n      for (byte[] pkey : partitionKeys) {\n        BytesWritable wrapper = new BytesWritable(pkey);\n        writer.append(wrapper, NullWritable.get());\n      }\n    } finally {\n      IOUtils.closeStream(writer);\n    }\n  }",
                "code_after_change": "  public void writePartitionKeys(Path path, JobConf job) throws IOException {\n    byte[][] partitionKeys = getPartitionKeys(job.getNumReduceTasks());\n    int numPartition = partitionKeys.length + 1;\n    if (numPartition != job.getNumReduceTasks()) {\n      job.setNumReduceTasks(numPartition);\n    }\n\n    FileSystem fs = path.getFileSystem(job);\n    SequenceFile.Writer writer = SequenceFile.createWriter(fs, job, path,\n        BytesWritable.class, NullWritable.class);\n    try {\n      for (byte[] pkey : partitionKeys) {\n        BytesWritable wrapper = new BytesWritable(pkey);\n        writer.append(wrapper, NullWritable.get());\n      }\n    } finally {\n      IOUtils.closeStream(writer);\n    }\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.exec.PartitionKeySampler.createSampler": {
                "code_before_change": "  public static FetchOperator createSampler(FetchWork work, HiveConf conf, JobConf job,\n      Operator<?> operator) throws HiveException {\n    int sampleNum = conf.getIntVar(HiveConf.ConfVars.HIVESAMPLINGNUMBERFORORDERBY);\n    float samplePercent = conf.getFloatVar(HiveConf.ConfVars.HIVESAMPLINGPERCENTFORORDERBY);\n    if (samplePercent < 0.0 || samplePercent > 1.0) {\n      throw new IllegalArgumentException(\"Percentile value must be within the range of 0 to 1.\");\n    }\n    RandomSampler sampler = new RandomSampler(work, job, operator);\n    sampler.setSampleNum(sampleNum);\n    sampler.setSamplePercent(samplePercent);\n    return sampler;\n  }",
                "code_after_change": "  public static FetchOperator createSampler(FetchWork work, JobConf job,\n      Operator<?> operator) throws HiveException {\n    int sampleNum = HiveConf.getIntVar(job, HiveConf.ConfVars.HIVESAMPLINGNUMBERFORORDERBY);\n    float samplePercent =\n        HiveConf.getFloatVar(job, HiveConf.ConfVars.HIVESAMPLINGPERCENTFORORDERBY);\n    if (samplePercent < 0.0 || samplePercent > 1.0) {\n      throw new IllegalArgumentException(\"Percentile value must be within the range of 0 to 1.\");\n    }\n    RandomSampler sampler = new RandomSampler(work, job, operator);\n    sampler.setSampleNum(sampleNum);\n    sampler.setSamplePercent(samplePercent);\n    return sampler;\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report precisely identifies the root cause by mentioning the method 'ExecDriver::handleSampling', which is part of the ground truth methods. The report does not provide any fix suggestion, hence it is marked as 'Missing' for fix suggestion. The problem location is also precisely identified as it mentions 'ExecDriver::handleSampling' in the stack trace, which is a ground truth method. There is no wrong information in the bug report as all the details are consistent with the context of the bug."
        }
    },
    {
        "filename": "HIVE-13017.json",
        "code_diff": {
            "ql.src.java.org.apache.hadoop.hive.ql.exec.SecureCmdDoAs.SecureCmdDoAs": {
                "code_before_change": [],
                "code_after_change": []
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Same Class or Module"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Same Class or Module"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report mentions a problem related to HiveServer2 and delegation tokens, which is related to the SecureCmdDoAs class where the actual fix was made. However, it does not precisely identify the SecureCmdDoAs method as the root cause. The report does not provide any specific fix suggestion, hence 'Missing' for fix suggestion. The problem location is partially identified as it relates to the same module but not the exact method. There is no wrong information in the report as it correctly describes the context and symptoms of the issue."
        }
    },
    {
        "filename": "HIVE-11303.json",
        "code_diff": {
            "ql.src.java.org.apache.hadoop.hive.ql.exec.vector.mapjoin.VectorMapJoinCommonOperator.getHashTableLoader": {
                "code_before_change": "  protected HashTableLoader getHashTableLoader(Configuration hconf) {\n\n    VectorMapJoinDesc vectorDesc = conf.getVectorDesc();\n    HashTableImplementationType hashTableImplementationType = vectorDesc.hashTableImplementationType();\n    HashTableLoader hashTableLoader;\n    switch (vectorDesc.hashTableImplementationType()) {\n    case OPTIMIZED:\n      // Use the Tez hash table loader.\n      hashTableLoader = HashTableLoaderFactory.getLoader(hconf);\n      break;\n    case FAST:\n      // Use our specialized hash table loader.\n      hashTableLoader = new VectorMapJoinFastHashTableLoader();\n      break;\n    default:\n      throw new RuntimeException(\"Unknown vector map join hash table implementation type \" + hashTableImplementationType.name());\n    }\n    return hashTableLoader;\n  }",
                "code_after_change": "  protected HashTableLoader getHashTableLoader(Configuration hconf) {\n\n    VectorMapJoinDesc vectorDesc = conf.getVectorDesc();\n    HashTableImplementationType hashTableImplementationType = vectorDesc.hashTableImplementationType();\n    HashTableLoader hashTableLoader;\n    switch (vectorDesc.hashTableImplementationType()) {\n    case OPTIMIZED:\n      // Use the Tez hash table loader.\n      hashTableLoader = HashTableLoaderFactory.getLoader(hconf);\n      break;\n    case FAST:\n      // Use our specialized hash table loader.\n      hashTableLoader = new VectorMapJoinFastHashTableLoader();\n      break;\n    default:\n      throw new RuntimeException(\"Unknown vector map join hash table implementation type \" + hashTableImplementationType.name());\n    }\n    return hashTableLoader;\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.exec.JoinUtil.getObjectInspectorsFromEvaluators": {
                "code_before_change": "  public static List<ObjectInspector>[] getObjectInspectorsFromEvaluators(\n      List<ExprNodeEvaluator>[] exprEntries,\n      ObjectInspector[] inputObjInspector,\n      int posBigTableAlias, int tagLen) throws HiveException {\n    List<ObjectInspector>[] result = new List[tagLen];\n\n    int iterate = Math.min(exprEntries.length, inputObjInspector.length);\n    for (byte alias = 0; alias < iterate; alias++) {\n      if (alias == (byte) posBigTableAlias ||\n          exprEntries[alias] == null || inputObjInspector[alias] == null) {\n        // skip the driver and directly loadable tables\n        continue;\n      }\n\n      List<ExprNodeEvaluator> exprList = exprEntries[alias];\n      List<ObjectInspector> fieldOIList = new ArrayList<ObjectInspector>();\n      for (int i = 0; i < exprList.size(); i++) {\n        fieldOIList.add(exprList.get(i).initialize(inputObjInspector[alias]));\n      }\n      result[alias] = fieldOIList;\n    }\n    return result;\n  }",
                "code_after_change": "  public static List<ObjectInspector>[] getObjectInspectorsFromEvaluators(\n      List<ExprNodeEvaluator>[] exprEntries,\n      ObjectInspector[] inputObjInspector,\n      int posBigTableAlias, int tagLen) throws HiveException {\n    List<ObjectInspector>[] result = new List[tagLen];\n\n    int iterate = Math.min(exprEntries.length, inputObjInspector.length);\n    for (byte alias = 0; alias < iterate; alias++) {\n      ObjectInspector inputOI = inputObjInspector[alias];\n\n      // For vectorized reduce-side operators getting inputs from a reduce sink,\n      // the row object inspector will get a flattened version of the object inspector\n      // where the nested key/value structs are replaced with a single struct:\n      // Example: { key: { reducesinkkey0:int }, value: { _col0:int, _col1:int, .. } }\n      // Would get converted to the following for a vectorized input:\n      //   { 'key.reducesinkkey0':int, 'value._col0':int, 'value._col1':int, .. }\n      // The ExprNodeEvaluator initialzation below gets broken with the flattened\n      // object inpsectors, so convert it back to the a form that contains the\n      // nested key/value structs.\n      inputOI = unflattenObjInspector(inputOI);\n\n      if (alias == (byte) posBigTableAlias ||\n          exprEntries[alias] == null || inputOI == null) {\n        // skip the driver and directly loadable tables\n        continue;\n      }\n\n      List<ExprNodeEvaluator> exprList = exprEntries[alias];\n      List<ObjectInspector> fieldOIList = new ArrayList<ObjectInspector>();\n      for (int i = 0; i < exprList.size(); i++) {\n        fieldOIList.add(exprList.get(i).initialize(inputOI));\n      }\n      result[alias] = fieldOIList;\n    }\n    return result;\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin.convertJoinMapJoin": {
                "code_before_change": "  public MapJoinOperator convertJoinMapJoin(JoinOperator joinOp, OptimizeTezProcContext context,\n      int bigTablePosition) throws SemanticException {\n    // bail on mux operator because currently the mux operator masks the emit keys\n    // of the constituent reduce sinks.\n    for (Operator<? extends OperatorDesc> parentOp : joinOp.getParentOperators()) {\n      if (parentOp instanceof MuxOperator) {\n        return null;\n      }\n    }\n\n    // can safely convert the join to a map join.\n    MapJoinOperator mapJoinOp =\n        MapJoinProcessor.convertJoinOpMapJoinOp(context.conf, joinOp,\n            joinOp.getConf().isLeftInputJoin(), joinOp.getConf().getBaseSrc(),\n            joinOp.getConf().getMapAliases(), bigTablePosition, true);\n    mapJoinOp.getConf().setHybridHashJoin(HiveConf.getBoolVar(context.conf,\n      HiveConf.ConfVars.HIVEUSEHYBRIDGRACEHASHJOIN));\n\n    Operator<? extends OperatorDesc> parentBigTableOp =\n        mapJoinOp.getParentOperators().get(bigTablePosition);\n    if (parentBigTableOp instanceof ReduceSinkOperator) {\n      for (Operator<?> p : parentBigTableOp.getParentOperators()) {\n        // we might have generated a dynamic partition operator chain. Since\n        // we're removing the reduce sink we need do remove that too.\n        Set<Operator<?>> dynamicPartitionOperators = new HashSet<Operator<?>>();\n        Map<Operator<?>, AppMasterEventOperator> opEventPairs = new HashMap<>();\n        for (Operator<?> c : p.getChildOperators()) {\n          AppMasterEventOperator event = findDynamicPartitionBroadcast(c);\n          if (event != null) {\n            dynamicPartitionOperators.add(c);\n            opEventPairs.put(c, event);\n          }\n        }\n        for (Operator<?> c : dynamicPartitionOperators) {\n          if (context.pruningOpsRemovedByPriorOpt.isEmpty() ||\n              !context.pruningOpsRemovedByPriorOpt.contains(opEventPairs.get(c))) {\n            p.removeChild(c);\n            // at this point we've found the fork in the op pipeline that has the pruning as a child plan.\n            LOG.info(\"Disabling dynamic pruning for: \"\n                + ((DynamicPruningEventDesc) opEventPairs.get(c).getConf()).getTableScan().getName()\n                + \". Need to be removed together with reduce sink\");\n          }\n        }\n        for (Operator<?> op : dynamicPartitionOperators) {\n          context.pruningOpsRemovedByPriorOpt.add(opEventPairs.get(op));\n        }\n      }\n      mapJoinOp.getParentOperators().remove(bigTablePosition);\n      if (!(mapJoinOp.getParentOperators().contains(parentBigTableOp.getParentOperators().get(0)))) {\n        mapJoinOp.getParentOperators().add(bigTablePosition,\n            parentBigTableOp.getParentOperators().get(0));\n      }\n      parentBigTableOp.getParentOperators().get(0).removeChild(parentBigTableOp);\n      for (Operator<? extends OperatorDesc>op : mapJoinOp.getParentOperators()) {\n        if (!(op.getChildOperators().contains(mapJoinOp))) {\n          op.getChildOperators().add(mapJoinOp);\n        }\n        op.getChildOperators().remove(joinOp);\n      }\n    }\n\n    return mapJoinOp;\n  }",
                "code_after_change": "  public MapJoinOperator convertJoinMapJoin(JoinOperator joinOp, OptimizeTezProcContext context,\n      int bigTablePosition, boolean removeReduceSink) throws SemanticException {\n    // bail on mux operator because currently the mux operator masks the emit keys\n    // of the constituent reduce sinks.\n    for (Operator<? extends OperatorDesc> parentOp : joinOp.getParentOperators()) {\n      if (parentOp instanceof MuxOperator) {\n        return null;\n      }\n    }\n\n    // can safely convert the join to a map join.\n    MapJoinOperator mapJoinOp =\n        MapJoinProcessor.convertJoinOpMapJoinOp(context.conf, joinOp,\n            joinOp.getConf().isLeftInputJoin(), joinOp.getConf().getBaseSrc(),\n            joinOp.getConf().getMapAliases(), bigTablePosition, true, removeReduceSink);\n    mapJoinOp.getConf().setHybridHashJoin(HiveConf.getBoolVar(context.conf,\n        HiveConf.ConfVars.HIVEUSEHYBRIDGRACEHASHJOIN));\n\n    Operator<? extends OperatorDesc> parentBigTableOp =\n        mapJoinOp.getParentOperators().get(bigTablePosition);\n    if (parentBigTableOp instanceof ReduceSinkOperator) {\n      if (removeReduceSink) {\n        for (Operator<?> p : parentBigTableOp.getParentOperators()) {\n          // we might have generated a dynamic partition operator chain. Since\n          // we're removing the reduce sink we need do remove that too.\n          Set<Operator<?>> dynamicPartitionOperators = new HashSet<Operator<?>>();\n          Map<Operator<?>, AppMasterEventOperator> opEventPairs = new HashMap<>();\n          for (Operator<?> c : p.getChildOperators()) {\n            AppMasterEventOperator event = findDynamicPartitionBroadcast(c);\n            if (event != null) {\n              dynamicPartitionOperators.add(c);\n              opEventPairs.put(c, event);\n            }\n          }\n          for (Operator<?> c : dynamicPartitionOperators) {\n            if (context.pruningOpsRemovedByPriorOpt.isEmpty() ||\n                !context.pruningOpsRemovedByPriorOpt.contains(opEventPairs.get(c))) {\n              p.removeChild(c);\n              // at this point we've found the fork in the op pipeline that has the pruning as a child plan.\n              LOG.info(\"Disabling dynamic pruning for: \"\n                  + ((DynamicPruningEventDesc) opEventPairs.get(c).getConf()).getTableScan().getName()\n                  + \". Need to be removed together with reduce sink\");\n            }\n          }\n          for (Operator<?> op : dynamicPartitionOperators) {\n            context.pruningOpsRemovedByPriorOpt.add(opEventPairs.get(op));\n          }\n        }\n\n        mapJoinOp.getParentOperators().remove(bigTablePosition);\n        if (!(mapJoinOp.getParentOperators().contains(parentBigTableOp.getParentOperators().get(0)))) {\n          mapJoinOp.getParentOperators().add(bigTablePosition,\n              parentBigTableOp.getParentOperators().get(0));\n        }\n        parentBigTableOp.getParentOperators().get(0).removeChild(parentBigTableOp);\n      }\n\n      for (Operator<? extends OperatorDesc>op : mapJoinOp.getParentOperators()) {\n        if (!(op.getChildOperators().contains(mapJoinOp))) {\n          op.getChildOperators().add(mapJoinOp);\n        }\n        op.getChildOperators().remove(joinOp);\n      }\n    }\n\n    return mapJoinOp;\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.getMapJoinDesc": {
                "code_before_change": "  public static MapJoinDesc getMapJoinDesc(HiveConf hconf,\n      JoinOperator op, boolean leftInputJoin, String[] baseSrc, List<String> mapAliases,\n      int mapJoinPos, boolean noCheckOuterJoin) throws SemanticException {\n    JoinDesc desc = op.getConf();\n    JoinCondDesc[] condns = desc.getConds();\n    Byte[] tagOrder = desc.getTagOrder();\n\n    // outer join cannot be performed on a table which is being cached\n    if (!noCheckOuterJoin) {\n      if (checkMapJoin(mapJoinPos, condns) < 0) {\n        throw new SemanticException(ErrorMsg.NO_OUTER_MAPJOIN.getMsg());\n      }\n    }\n\n    Map<String, ExprNodeDesc> colExprMap = op.getColumnExprMap();\n    List<ColumnInfo> schema = new ArrayList<ColumnInfo>(op.getSchema().getSignature());\n    Map<Byte, List<ExprNodeDesc>> valueExprs = op.getConf().getExprs();\n    Map<Byte, List<ExprNodeDesc>> newValueExprs = new HashMap<Byte, List<ExprNodeDesc>>();\n\n    ObjectPair<List<ReduceSinkOperator>, Map<Byte,List<ExprNodeDesc>>> pair =\n            getKeys(leftInputJoin, baseSrc, op);\n    List<ReduceSinkOperator> oldReduceSinkParentOps = pair.getFirst();\n    for (Map.Entry<Byte, List<ExprNodeDesc>> entry : valueExprs.entrySet()) {\n      byte tag = entry.getKey();\n      Operator<?> terminal = oldReduceSinkParentOps.get(tag);\n\n      List<ExprNodeDesc> values = entry.getValue();\n      List<ExprNodeDesc> newValues = ExprNodeDescUtils.backtrack(values, op, terminal);\n      newValueExprs.put(tag, newValues);\n      for (int i = 0; i < schema.size(); i++) {\n        ColumnInfo column = schema.get(i);\n        if (column == null) {\n          continue;\n        }\n        ExprNodeDesc expr = colExprMap.get(column.getInternalName());\n        int index = ExprNodeDescUtils.indexOf(expr, values);\n        if (index >= 0) {\n          colExprMap.put(column.getInternalName(), newValues.get(index));\n          schema.set(i, null);\n        }\n      }\n    }\n\n    // rewrite value index for mapjoin\n    Map<Byte, int[]> valueIndices = new HashMap<Byte, int[]>();\n\n    // get the join keys from old parent ReduceSink operators\n    Map<Byte, List<ExprNodeDesc>> keyExprMap = pair.getSecond();\n\n    // construct valueTableDescs and valueFilteredTableDescs\n    List<TableDesc> valueTableDescs = new ArrayList<TableDesc>();\n    List<TableDesc> valueFilteredTableDescs = new ArrayList<TableDesc>();\n    int[][] filterMap = desc.getFilterMap();\n    for (byte pos = 0; pos < op.getParentOperators().size(); pos++) {\n      List<ExprNodeDesc> valueCols = newValueExprs.get(pos);\n      if (pos != mapJoinPos) {\n        // remove values in key exprs for value table schema\n        // value expression for hashsink will be modified in\n        // LocalMapJoinProcessor\n        int[] valueIndex = new int[valueCols.size()];\n        List<ExprNodeDesc> valueColsInValueExpr = new ArrayList<ExprNodeDesc>();\n        for (int i = 0; i < valueIndex.length; i++) {\n          ExprNodeDesc expr = valueCols.get(i);\n          int kindex = ExprNodeDescUtils.indexOf(expr, keyExprMap.get(pos));\n          if (kindex >= 0) {\n            valueIndex[i] = kindex;\n          } else {\n            valueIndex[i] = -valueColsInValueExpr.size() - 1;\n            valueColsInValueExpr.add(expr);\n          }\n        }\n        if (needValueIndex(valueIndex)) {\n          valueIndices.put(pos, valueIndex);\n        }\n        valueCols = valueColsInValueExpr;\n      }\n      // deep copy expr node desc\n      List<ExprNodeDesc> valueFilteredCols = ExprNodeDescUtils.clone(valueCols);\n      if (filterMap != null && filterMap[pos] != null && pos != mapJoinPos) {\n        ExprNodeColumnDesc isFilterDesc =\n            new ExprNodeColumnDesc(\n                TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.SMALLINT_TYPE_NAME), \"filter\",\n                \"filter\", false);\n        valueFilteredCols.add(isFilterDesc);\n      }\n\n      TableDesc valueTableDesc =\n          PlanUtils.getMapJoinValueTableDesc(PlanUtils.getFieldSchemasFromColumnList(valueCols,\n              \"mapjoinvalue\"));\n      TableDesc valueFilteredTableDesc =\n          PlanUtils.getMapJoinValueTableDesc(PlanUtils.getFieldSchemasFromColumnList(\n              valueFilteredCols, \"mapjoinvalue\"));\n\n      valueTableDescs.add(valueTableDesc);\n      valueFilteredTableDescs.add(valueFilteredTableDesc);\n    }\n\n    Map<Byte, List<ExprNodeDesc>> filters = desc.getFilters();\n    Map<Byte, List<ExprNodeDesc>> newFilters = new HashMap<Byte, List<ExprNodeDesc>>();\n    for (Map.Entry<Byte, List<ExprNodeDesc>> entry : filters.entrySet()) {\n      byte srcTag = entry.getKey();\n      List<ExprNodeDesc> filter = entry.getValue();\n\n      Operator<?> terminal = oldReduceSinkParentOps.get(srcTag);\n      newFilters.put(srcTag, ExprNodeDescUtils.backtrack(filter, op, terminal));\n    }\n    desc.setFilters(filters = newFilters);\n\n    // create dumpfile prefix needed to create descriptor\n    String dumpFilePrefix = \"\";\n    if (mapAliases != null) {\n      for (String mapAlias : mapAliases) {\n        dumpFilePrefix = dumpFilePrefix + mapAlias;\n      }\n      dumpFilePrefix = dumpFilePrefix + \"-\" + PlanUtils.getCountForMapJoinDumpFilePrefix();\n    } else {\n      dumpFilePrefix = \"mapfile\" + PlanUtils.getCountForMapJoinDumpFilePrefix();\n    }\n\n    List<ExprNodeDesc> keyCols = keyExprMap.get((byte) mapJoinPos);\n\n    List<String> outputColumnNames = op.getConf().getOutputColumnNames();\n    TableDesc keyTableDesc =\n        PlanUtils.getMapJoinKeyTableDesc(hconf,\n            PlanUtils.getFieldSchemasFromColumnList(keyCols, MAPJOINKEY_FIELDPREFIX));\n    JoinCondDesc[] joinCondns = op.getConf().getConds();\n    MapJoinDesc mapJoinDescriptor =\n        new MapJoinDesc(keyExprMap, keyTableDesc, newValueExprs, valueTableDescs,\n            valueFilteredTableDescs, outputColumnNames, mapJoinPos, joinCondns, filters, op\n                .getConf().getNoOuterJoin(), dumpFilePrefix);\n    mapJoinDescriptor.setStatistics(op.getConf().getStatistics());\n    mapJoinDescriptor.setTagOrder(tagOrder);\n    mapJoinDescriptor.setNullSafes(desc.getNullSafes());\n    mapJoinDescriptor.setFilterMap(desc.getFilterMap());\n    if (!valueIndices.isEmpty()) {\n      mapJoinDescriptor.setValueIndices(valueIndices);\n    }\n\n    return mapJoinDescriptor;\n  }",
                "code_after_change": "  public static MapJoinDesc getMapJoinDesc(HiveConf hconf,\n      JoinOperator op, boolean leftInputJoin, String[] baseSrc, List<String> mapAliases,\n      int mapJoinPos, boolean noCheckOuterJoin, boolean adjustParentsChildren) throws SemanticException {\n    JoinDesc desc = op.getConf();\n    JoinCondDesc[] condns = desc.getConds();\n    Byte[] tagOrder = desc.getTagOrder();\n\n    // outer join cannot be performed on a table which is being cached\n    if (!noCheckOuterJoin) {\n      if (checkMapJoin(mapJoinPos, condns) < 0) {\n        throw new SemanticException(ErrorMsg.NO_OUTER_MAPJOIN.getMsg());\n      }\n    }\n\n    Map<String, ExprNodeDesc> colExprMap = op.getColumnExprMap();\n    List<ColumnInfo> schema = new ArrayList<ColumnInfo>(op.getSchema().getSignature());\n    Map<Byte, List<ExprNodeDesc>> valueExprs = op.getConf().getExprs();\n    Map<Byte, List<ExprNodeDesc>> newValueExprs = new HashMap<Byte, List<ExprNodeDesc>>();\n\n    ObjectPair<List<ReduceSinkOperator>, Map<Byte,List<ExprNodeDesc>>> pair =\n            getKeys(leftInputJoin, baseSrc, op);\n    List<ReduceSinkOperator> oldReduceSinkParentOps = pair.getFirst();\n    for (Map.Entry<Byte, List<ExprNodeDesc>> entry : valueExprs.entrySet()) {\n      byte tag = entry.getKey();\n      Operator<?> terminal = oldReduceSinkParentOps.get(tag);\n\n      List<ExprNodeDesc> values = entry.getValue();\n      List<ExprNodeDesc> newValues = ExprNodeDescUtils.backtrack(values, op, terminal);\n      newValueExprs.put(tag, newValues);\n      for (int i = 0; i < schema.size(); i++) {\n        ColumnInfo column = schema.get(i);\n        if (column == null) {\n          continue;\n        }\n        ExprNodeDesc expr = colExprMap.get(column.getInternalName());\n        int index = ExprNodeDescUtils.indexOf(expr, values);\n        if (index >= 0) {\n          colExprMap.put(column.getInternalName(), newValues.get(index));\n          schema.set(i, null);\n        }\n      }\n    }\n\n    // rewrite value index for mapjoin\n    Map<Byte, int[]> valueIndices = new HashMap<Byte, int[]>();\n\n    // get the join keys from old parent ReduceSink operators\n    Map<Byte, List<ExprNodeDesc>> keyExprMap = pair.getSecond();\n\n    if (!adjustParentsChildren) {\n      // Since we did not remove reduce sink parents, keep the original value expressions\n      newValueExprs = valueExprs;\n\n      // Join key exprs are represented in terms of the original table columns,\n      // we need to convert these to the generated column names we can see in the Join operator\n      Map<Byte, List<ExprNodeDesc>> newKeyExprMap = new HashMap<Byte, List<ExprNodeDesc>>();\n      for (Map.Entry<Byte, List<ExprNodeDesc>> mapEntry : keyExprMap.entrySet()) {\n        Byte pos = mapEntry.getKey();\n        ReduceSinkOperator rsParent = oldReduceSinkParentOps.get(pos.byteValue());\n        List<ExprNodeDesc> keyExprList =\n            ExprNodeDescUtils.resolveJoinKeysAsRSColumns(mapEntry.getValue(), rsParent);\n        if (keyExprList == null) {\n          throw new SemanticException(\"Error resolving join keys\");\n        }\n        newKeyExprMap.put(pos, keyExprList);\n      }\n      keyExprMap = newKeyExprMap;\n    }\n\n    // construct valueTableDescs and valueFilteredTableDescs\n    List<TableDesc> valueTableDescs = new ArrayList<TableDesc>();\n    List<TableDesc> valueFilteredTableDescs = new ArrayList<TableDesc>();\n    int[][] filterMap = desc.getFilterMap();\n    for (byte pos = 0; pos < op.getParentOperators().size(); pos++) {\n      List<ExprNodeDesc> valueCols = newValueExprs.get(pos);\n      if (pos != mapJoinPos) {\n        // remove values in key exprs for value table schema\n        // value expression for hashsink will be modified in\n        // LocalMapJoinProcessor\n        int[] valueIndex = new int[valueCols.size()];\n        List<ExprNodeDesc> valueColsInValueExpr = new ArrayList<ExprNodeDesc>();\n        for (int i = 0; i < valueIndex.length; i++) {\n          ExprNodeDesc expr = valueCols.get(i);\n          int kindex = ExprNodeDescUtils.indexOf(expr, keyExprMap.get(pos));\n          if (kindex >= 0) {\n            valueIndex[i] = kindex;\n          } else {\n            valueIndex[i] = -valueColsInValueExpr.size() - 1;\n            valueColsInValueExpr.add(expr);\n          }\n        }\n        if (needValueIndex(valueIndex)) {\n          valueIndices.put(pos, valueIndex);\n        }\n        valueCols = valueColsInValueExpr;\n      }\n      // deep copy expr node desc\n      List<ExprNodeDesc> valueFilteredCols = ExprNodeDescUtils.clone(valueCols);\n      if (filterMap != null && filterMap[pos] != null && pos != mapJoinPos) {\n        ExprNodeColumnDesc isFilterDesc =\n            new ExprNodeColumnDesc(\n                TypeInfoFactory.getPrimitiveTypeInfo(serdeConstants.SMALLINT_TYPE_NAME), \"filter\",\n                \"filter\", false);\n        valueFilteredCols.add(isFilterDesc);\n      }\n\n      TableDesc valueTableDesc =\n          PlanUtils.getMapJoinValueTableDesc(PlanUtils.getFieldSchemasFromColumnList(valueCols,\n              \"mapjoinvalue\"));\n      TableDesc valueFilteredTableDesc =\n          PlanUtils.getMapJoinValueTableDesc(PlanUtils.getFieldSchemasFromColumnList(\n              valueFilteredCols, \"mapjoinvalue\"));\n\n      valueTableDescs.add(valueTableDesc);\n      valueFilteredTableDescs.add(valueFilteredTableDesc);\n    }\n\n    Map<Byte, List<ExprNodeDesc>> filters = desc.getFilters();\n    Map<Byte, List<ExprNodeDesc>> newFilters = new HashMap<Byte, List<ExprNodeDesc>>();\n    for (Map.Entry<Byte, List<ExprNodeDesc>> entry : filters.entrySet()) {\n      byte srcTag = entry.getKey();\n      List<ExprNodeDesc> filter = entry.getValue();\n\n      Operator<?> terminal = oldReduceSinkParentOps.get(srcTag);\n      newFilters.put(srcTag, ExprNodeDescUtils.backtrack(filter, op, terminal));\n    }\n    desc.setFilters(filters = newFilters);\n\n    // create dumpfile prefix needed to create descriptor\n    String dumpFilePrefix = \"\";\n    if (mapAliases != null) {\n      for (String mapAlias : mapAliases) {\n        dumpFilePrefix = dumpFilePrefix + mapAlias;\n      }\n      dumpFilePrefix = dumpFilePrefix + \"-\" + PlanUtils.getCountForMapJoinDumpFilePrefix();\n    } else {\n      dumpFilePrefix = \"mapfile\" + PlanUtils.getCountForMapJoinDumpFilePrefix();\n    }\n\n    List<ExprNodeDesc> keyCols = keyExprMap.get((byte) mapJoinPos);\n\n    List<String> outputColumnNames = op.getConf().getOutputColumnNames();\n    TableDesc keyTableDesc =\n        PlanUtils.getMapJoinKeyTableDesc(hconf,\n            PlanUtils.getFieldSchemasFromColumnList(keyCols, MAPJOINKEY_FIELDPREFIX));\n    JoinCondDesc[] joinCondns = op.getConf().getConds();\n    MapJoinDesc mapJoinDescriptor =\n        new MapJoinDesc(keyExprMap, keyTableDesc, newValueExprs, valueTableDescs,\n            valueFilteredTableDescs, outputColumnNames, mapJoinPos, joinCondns, filters, op\n                .getConf().getNoOuterJoin(), dumpFilePrefix);\n    mapJoinDescriptor.setStatistics(op.getConf().getStatistics());\n    mapJoinDescriptor.setTagOrder(tagOrder);\n    mapJoinDescriptor.setNullSafes(desc.getNullSafes());\n    mapJoinDescriptor.setFilterMap(desc.getFilterMap());\n    if (!valueIndices.isEmpty()) {\n      mapJoinDescriptor.setValueIndices(valueIndices);\n    }\n\n    return mapJoinDescriptor;\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.exec.JoinUtil.getRowContainer": {
                "code_before_change": "  public static RowContainer<List<Object>> getRowContainer(Configuration hconf,\n      List<ObjectInspector> structFieldObjectInspectors,\n      Byte alias,int containerSize, TableDesc[] spillTableDesc,\n      JoinDesc conf,boolean noFilter, Reporter reporter) throws HiveException {\n\n    TableDesc tblDesc = JoinUtil.getSpillTableDesc(alias,spillTableDesc,conf, noFilter);\n    SerDe serde = JoinUtil.getSpillSerDe(alias, spillTableDesc, conf, noFilter);\n\n    if (serde == null) {\n      containerSize = -1;\n    }\n\n    RowContainer<List<Object>> rc = new RowContainer<List<Object>>(containerSize, hconf, reporter);\n    StructObjectInspector rcOI = null;\n    if (tblDesc != null) {\n      // arbitrary column names used internally for serializing to spill table\n      List<String> colNames = Utilities.getColumnNames(tblDesc.getProperties());\n      // object inspector for serializing input tuples\n      rcOI = ObjectInspectorFactory.getStandardStructObjectInspector(colNames,\n          structFieldObjectInspectors);\n    }\n\n    rc.setSerDe(serde, rcOI);\n    rc.setTableDesc(tblDesc);\n    return rc;\n  }",
                "code_after_change": "  public static RowContainer<List<Object>> getRowContainer(Configuration hconf,\n      List<ObjectInspector> structFieldObjectInspectors,\n      Byte alias,int containerSize, TableDesc[] spillTableDesc,\n      JoinDesc conf,boolean noFilter, Reporter reporter) throws HiveException {\n\n    TableDesc tblDesc = JoinUtil.getSpillTableDesc(alias,spillTableDesc,conf, noFilter);\n    SerDe serde = JoinUtil.getSpillSerDe(alias, spillTableDesc, conf, noFilter);\n\n    if (serde == null) {\n      containerSize = -1;\n    }\n\n    RowContainer<List<Object>> rc = new RowContainer<List<Object>>(containerSize, hconf, reporter);\n    StructObjectInspector rcOI = null;\n    if (tblDesc != null) {\n      // arbitrary column names used internally for serializing to spill table\n      List<String> colNames = Utilities.getColumnNames(tblDesc.getProperties());\n      // object inspector for serializing input tuples\n      rcOI = ObjectInspectorFactory.getStandardStructObjectInspector(colNames,\n          structFieldObjectInspectors);\n    }\n\n    rc.setSerDe(serde, rcOI);\n    rc.setTableDesc(tblDesc);\n    return rc;\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.parse.GenTezProcContext.GenTezProcContext": {
                "code_before_change": [],
                "code_after_change": []
            },
            "ql.src.java.org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.init": {
                "code_before_change": "  void init(JobConf jconf, Operator<?> reducer, boolean vectorized, TableDesc keyTableDesc,\n      TableDesc valueTableDesc, KeyValuesReader reader, boolean handleGroupKey, byte tag,\n      Map<Integer, String> vectorScratchColumnTypeMap)\n      throws Exception {\n\n    ObjectInspector keyObjectInspector;\n\n    this.reducer = reducer;\n    this.vectorized = vectorized;\n    this.keyTableDesc = keyTableDesc;\n    this.reader = reader;\n    this.handleGroupKey = handleGroupKey;\n    this.tag = tag;\n\n    try {\n      inputKeyDeserializer = ReflectionUtils.newInstance(keyTableDesc\n          .getDeserializerClass(), null);\n      SerDeUtils.initializeSerDe(inputKeyDeserializer, null, keyTableDesc.getProperties(), null);\n      keyObjectInspector = inputKeyDeserializer.getObjectInspector();\n\n      if(vectorized) {\n        keyStructInspector = (StructObjectInspector) keyObjectInspector;\n        firstValueColumnOffset = keyStructInspector.getAllStructFieldRefs().size();\n      }\n\n      // We should initialize the SerDe with the TypeInfo when available.\n      this.valueTableDesc = valueTableDesc;\n      inputValueDeserializer = (SerDe) ReflectionUtils.newInstance(\n          valueTableDesc.getDeserializerClass(), null);\n      SerDeUtils.initializeSerDe(inputValueDeserializer, null,\n          valueTableDesc.getProperties(), null);\n      valueObjectInspector = inputValueDeserializer.getObjectInspector();\n\n      ArrayList<ObjectInspector> ois = new ArrayList<ObjectInspector>();\n\n      if(vectorized) {\n        /* vectorization only works with struct object inspectors */\n        valueStructInspectors = (StructObjectInspector) valueObjectInspector;\n\n        final int totalColumns = firstValueColumnOffset +\n            valueStructInspectors.getAllStructFieldRefs().size();\n        valueStringWriters = new ArrayList<VectorExpressionWriter>(totalColumns);\n        valueStringWriters.addAll(Arrays\n            .asList(VectorExpressionWriterFactory\n                .genVectorStructExpressionWritables(keyStructInspector)));\n        valueStringWriters.addAll(Arrays\n            .asList(VectorExpressionWriterFactory\n                .genVectorStructExpressionWritables(valueStructInspectors)));\n\n        ObjectPair<VectorizedRowBatch, StandardStructObjectInspector> pair =\n            VectorizedBatchUtil.constructVectorizedRowBatch(keyStructInspector, valueStructInspectors, vectorScratchColumnTypeMap);\n        rowObjectInspector = pair.getSecond();\n        batch = pair.getFirst();\n\n        // Setup vectorized deserialization for the key and value.\n        BinarySortableSerDe binarySortableSerDe = (BinarySortableSerDe) inputKeyDeserializer;\n\n        keyBinarySortableDeserializeToRow =\n                  new VectorDeserializeRow(\n                        new BinarySortableDeserializeRead(\n                                  VectorizedBatchUtil.primitiveTypeInfosFromStructObjectInspector(\n                                      keyStructInspector),\n                                  binarySortableSerDe.getSortOrders()));\n        keyBinarySortableDeserializeToRow.init(0);\n\n        final int valuesSize = valueStructInspectors.getAllStructFieldRefs().size();\n        if (valuesSize > 0) {\n          valueLazyBinaryDeserializeToRow =\n                  new VectorDeserializeRow(\n                        new LazyBinaryDeserializeRead(\n                                  VectorizedBatchUtil.primitiveTypeInfosFromStructObjectInspector(\n                                       valueStructInspectors)));\n          valueLazyBinaryDeserializeToRow.init(firstValueColumnOffset);\n\n          // Create data buffers for value bytes column vectors.\n          for (int i = firstValueColumnOffset; i < batch.numCols; i++) {\n            ColumnVector colVector = batch.cols[i];\n            if (colVector instanceof BytesColumnVector) {\n              BytesColumnVector bytesColumnVector = (BytesColumnVector) colVector;\n              bytesColumnVector.initBuffer();\n            }\n          }\n        }\n      } else {\n        ois.add(keyObjectInspector);\n        ois.add(valueObjectInspector);\n        rowObjectInspector =\n            ObjectInspectorFactory.getStandardStructObjectInspector(Utilities.reduceFieldNameList,\n                ois);\n      }\n    } catch (Throwable e) {\n      abort = true;\n      if (e instanceof OutOfMemoryError) {\n        // Don't create a new object if we are already out of memory\n        throw (OutOfMemoryError) e;\n      } else {\n        throw new RuntimeException(\"Reduce operator initialization failed\", e);\n      }\n    }\n    perfLogger.PerfLogEnd(CLASS_NAME, PerfLogger.TEZ_INIT_OPERATORS);\n  }",
                "code_after_change": "  void init(JobConf jconf, Operator<?> reducer, boolean vectorized, TableDesc keyTableDesc,\n      TableDesc valueTableDesc, Reader reader, boolean handleGroupKey, byte tag,\n      Map<Integer, String> vectorScratchColumnTypeMap)\n      throws Exception {\n\n    ObjectInspector keyObjectInspector;\n\n    this.reducer = reducer;\n    this.vectorized = vectorized;\n    this.keyTableDesc = keyTableDesc;\n    if (reader instanceof KeyValueReader) {\n      this.reader = new KeyValuesFromKeyValue((KeyValueReader) reader);\n    } else {\n      this.reader = new KeyValuesFromKeyValues((KeyValuesReader) reader);\n    }\n    this.handleGroupKey = handleGroupKey;\n    this.tag = tag;\n\n    try {\n      inputKeyDeserializer = ReflectionUtils.newInstance(keyTableDesc\n          .getDeserializerClass(), null);\n      SerDeUtils.initializeSerDe(inputKeyDeserializer, null, keyTableDesc.getProperties(), null);\n      keyObjectInspector = inputKeyDeserializer.getObjectInspector();\n\n      if(vectorized) {\n        keyStructInspector = (StructObjectInspector) keyObjectInspector;\n        firstValueColumnOffset = keyStructInspector.getAllStructFieldRefs().size();\n      }\n\n      // We should initialize the SerDe with the TypeInfo when available.\n      this.valueTableDesc = valueTableDesc;\n      inputValueDeserializer = (SerDe) ReflectionUtils.newInstance(\n          valueTableDesc.getDeserializerClass(), null);\n      SerDeUtils.initializeSerDe(inputValueDeserializer, null,\n          valueTableDesc.getProperties(), null);\n      valueObjectInspector = inputValueDeserializer.getObjectInspector();\n\n      ArrayList<ObjectInspector> ois = new ArrayList<ObjectInspector>();\n\n      if(vectorized) {\n        /* vectorization only works with struct object inspectors */\n        valueStructInspectors = (StructObjectInspector) valueObjectInspector;\n\n        final int totalColumns = firstValueColumnOffset +\n            valueStructInspectors.getAllStructFieldRefs().size();\n        valueStringWriters = new ArrayList<VectorExpressionWriter>(totalColumns);\n        valueStringWriters.addAll(Arrays\n            .asList(VectorExpressionWriterFactory\n                .genVectorStructExpressionWritables(keyStructInspector)));\n        valueStringWriters.addAll(Arrays\n            .asList(VectorExpressionWriterFactory\n                .genVectorStructExpressionWritables(valueStructInspectors)));\n\n        ObjectPair<VectorizedRowBatch, StandardStructObjectInspector> pair =\n            VectorizedBatchUtil.constructVectorizedRowBatch(keyStructInspector, valueStructInspectors, vectorScratchColumnTypeMap);\n        rowObjectInspector = pair.getSecond();\n        batch = pair.getFirst();\n\n        // Setup vectorized deserialization for the key and value.\n        BinarySortableSerDe binarySortableSerDe = (BinarySortableSerDe) inputKeyDeserializer;\n\n        keyBinarySortableDeserializeToRow =\n                  new VectorDeserializeRow(\n                        new BinarySortableDeserializeRead(\n                                  VectorizedBatchUtil.primitiveTypeInfosFromStructObjectInspector(\n                                      keyStructInspector),\n                                  binarySortableSerDe.getSortOrders()));\n        keyBinarySortableDeserializeToRow.init(0);\n\n        final int valuesSize = valueStructInspectors.getAllStructFieldRefs().size();\n        if (valuesSize > 0) {\n          valueLazyBinaryDeserializeToRow =\n                  new VectorDeserializeRow(\n                        new LazyBinaryDeserializeRead(\n                                  VectorizedBatchUtil.primitiveTypeInfosFromStructObjectInspector(\n                                       valueStructInspectors)));\n          valueLazyBinaryDeserializeToRow.init(firstValueColumnOffset);\n\n          // Create data buffers for value bytes column vectors.\n          for (int i = firstValueColumnOffset; i < batch.numCols; i++) {\n            ColumnVector colVector = batch.cols[i];\n            if (colVector instanceof BytesColumnVector) {\n              BytesColumnVector bytesColumnVector = (BytesColumnVector) colVector;\n              bytesColumnVector.initBuffer();\n            }\n          }\n        }\n      } else {\n        ois.add(keyObjectInspector);\n        ois.add(valueObjectInspector);\n        rowObjectInspector =\n            ObjectInspectorFactory.getStandardStructObjectInspector(Utilities.reduceFieldNameList,\n                ois);\n      }\n    } catch (Throwable e) {\n      abort = true;\n      if (e instanceof OutOfMemoryError) {\n        // Don't create a new object if we are already out of memory\n        throw (OutOfMemoryError) e;\n      } else {\n        throw new RuntimeException(\"Reduce operator initialization failed\", e);\n      }\n    }\n    perfLogger.PerfLogEnd(CLASS_NAME, PerfLogger.TEZ_INIT_OPERATORS);\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.optimizer.MapJoinProcessor.convertMapJoin": {
                "code_before_change": "  public MapJoinOperator convertMapJoin(HiveConf conf,\n    JoinOperator op, boolean leftInputJoin, String[] baseSrc, List<String> mapAliases,\n    int mapJoinPos, boolean noCheckOuterJoin, boolean validateMapJoinTree) throws SemanticException {\n\n    // outer join cannot be performed on a table which is being cached\n    JoinDesc desc = op.getConf();\n    JoinCondDesc[] condns = desc.getConds();\n\n    if (!noCheckOuterJoin) {\n      if (checkMapJoin(mapJoinPos, condns) < 0) {\n        throw new SemanticException(ErrorMsg.NO_OUTER_MAPJOIN.getMsg());\n      }\n    }\n\n    // Walk over all the sources (which are guaranteed to be reduce sink\n    // operators).\n    // The join outputs a concatenation of all the inputs.\n    List<Operator<? extends OperatorDesc>> parentOps = op.getParentOperators();\n    List<Operator<? extends OperatorDesc>> newParentOps =\n      new ArrayList<Operator<? extends OperatorDesc>>();\n    List<Operator<? extends OperatorDesc>> oldReduceSinkParentOps =\n       new ArrayList<Operator<? extends OperatorDesc>>();\n\n    // found a source which is not to be stored in memory\n    if (leftInputJoin) {\n      // assert mapJoinPos == 0;\n      Operator<? extends OperatorDesc> parentOp = parentOps.get(0);\n      assert parentOp.getParentOperators().size() == 1;\n      Operator<? extends OperatorDesc> grandParentOp =\n        parentOp.getParentOperators().get(0);\n      oldReduceSinkParentOps.add(parentOp);\n      newParentOps.add(grandParentOp);\n    }\n\n    byte pos = 0;\n    // Remove parent reduce-sink operators\n    for (String src : baseSrc) {\n      if (src != null) {\n        Operator<? extends OperatorDesc> parentOp = parentOps.get(pos);\n        assert parentOp.getParentOperators().size() == 1;\n        Operator<? extends OperatorDesc> grandParentOp =\n          parentOp.getParentOperators().get(0);\n\n        oldReduceSinkParentOps.add(parentOp);\n        newParentOps.add(grandParentOp);\n      }\n      pos++;\n    }\n\n    // create the map-join operator\n    MapJoinOperator mapJoinOp = convertJoinOpMapJoinOp(conf,\n        op, leftInputJoin, baseSrc, mapAliases, mapJoinPos, noCheckOuterJoin);\n\n    // remove old parents\n    for (pos = 0; pos < newParentOps.size(); pos++) {\n      newParentOps.get(pos).replaceChild(oldReduceSinkParentOps.get(pos), mapJoinOp);\n    }\n\n    mapJoinOp.getParentOperators().removeAll(oldReduceSinkParentOps);\n    mapJoinOp.setParentOperators(newParentOps);\n\n    // make sure only map-joins can be performed.\n    if (validateMapJoinTree) {\n      validateMapJoinTypes(mapJoinOp);\n    }\n\n    // change the children of the original join operator to point to the map\n    // join operator\n\n    return mapJoinOp;\n  }",
                "code_after_change": "  public MapJoinOperator convertMapJoin(HiveConf conf,\n    JoinOperator op, boolean leftInputJoin, String[] baseSrc, List<String> mapAliases,\n    int mapJoinPos, boolean noCheckOuterJoin, boolean validateMapJoinTree) throws SemanticException {\n\n    // outer join cannot be performed on a table which is being cached\n    JoinDesc desc = op.getConf();\n    JoinCondDesc[] condns = desc.getConds();\n\n    if (!noCheckOuterJoin) {\n      if (checkMapJoin(mapJoinPos, condns) < 0) {\n        throw new SemanticException(ErrorMsg.NO_OUTER_MAPJOIN.getMsg());\n      }\n    }\n\n    // Walk over all the sources (which are guaranteed to be reduce sink\n    // operators).\n    // The join outputs a concatenation of all the inputs.\n    List<Operator<? extends OperatorDesc>> parentOps = op.getParentOperators();\n    List<Operator<? extends OperatorDesc>> newParentOps =\n      new ArrayList<Operator<? extends OperatorDesc>>();\n    List<Operator<? extends OperatorDesc>> oldReduceSinkParentOps =\n       new ArrayList<Operator<? extends OperatorDesc>>();\n\n    // found a source which is not to be stored in memory\n    if (leftInputJoin) {\n      // assert mapJoinPos == 0;\n      Operator<? extends OperatorDesc> parentOp = parentOps.get(0);\n      assert parentOp.getParentOperators().size() == 1;\n      Operator<? extends OperatorDesc> grandParentOp =\n        parentOp.getParentOperators().get(0);\n      oldReduceSinkParentOps.add(parentOp);\n      newParentOps.add(grandParentOp);\n    }\n\n    byte pos = 0;\n    // Remove parent reduce-sink operators\n    for (String src : baseSrc) {\n      if (src != null) {\n        Operator<? extends OperatorDesc> parentOp = parentOps.get(pos);\n        assert parentOp.getParentOperators().size() == 1;\n        Operator<? extends OperatorDesc> grandParentOp =\n          parentOp.getParentOperators().get(0);\n\n        oldReduceSinkParentOps.add(parentOp);\n        newParentOps.add(grandParentOp);\n      }\n      pos++;\n    }\n\n    // create the map-join operator\n    MapJoinOperator mapJoinOp = convertJoinOpMapJoinOp(conf,\n        op, leftInputJoin, baseSrc, mapAliases, mapJoinPos, noCheckOuterJoin);\n\n    // remove old parents\n    for (pos = 0; pos < newParentOps.size(); pos++) {\n      newParentOps.get(pos).replaceChild(oldReduceSinkParentOps.get(pos), mapJoinOp);\n    }\n\n    mapJoinOp.getParentOperators().removeAll(oldReduceSinkParentOps);\n    mapJoinOp.setParentOperators(newParentOps);\n\n    // make sure only map-joins can be performed.\n    if (validateMapJoinTree) {\n      validateMapJoinTypes(mapJoinOp);\n    }\n\n    // change the children of the original join operator to point to the map\n    // join operator\n\n    return mapJoinOp;\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.plan.ExprNodeDescUtils.genExprNodeDesc": {
                "code_before_change": "  public static ArrayList<ExprNodeDesc> genExprNodeDesc(Operator inputOp, int startPos, int endPos,\n      boolean addEmptyTabAlias, boolean setColToNonVirtual) {\n    ArrayList<ExprNodeDesc> exprColLst = new ArrayList<ExprNodeDesc>();\n    List<ColumnInfo> colInfoLst = inputOp.getSchema().getSignature();\n\n    String tabAlias;\n    boolean vc;\n    ColumnInfo ci;\n    for (int i = startPos; i <= endPos; i++) {\n      ci = colInfoLst.get(i);\n      tabAlias = ci.getTabAlias();\n      if (addEmptyTabAlias) {\n        tabAlias = \"\";\n      }\n      vc = ci.getIsVirtualCol();\n      if (setColToNonVirtual) {\n        vc = false;\n      }\n      exprColLst.add(new ExprNodeColumnDesc(ci.getType(), ci.getInternalName(), tabAlias, vc));\n    }\n\n    return exprColLst;\n  }  ",
                "code_after_change": "  public static ArrayList<ExprNodeDesc> genExprNodeDesc(Operator inputOp, int startPos, int endPos,\n      boolean addEmptyTabAlias, boolean setColToNonVirtual) {\n    ArrayList<ExprNodeDesc> exprColLst = new ArrayList<ExprNodeDesc>();\n    List<ColumnInfo> colInfoLst = inputOp.getSchema().getSignature();\n\n    String tabAlias;\n    boolean vc;\n    ColumnInfo ci;\n    for (int i = startPos; i <= endPos; i++) {\n      ci = colInfoLst.get(i);\n      tabAlias = ci.getTabAlias();\n      if (addEmptyTabAlias) {\n        tabAlias = \"\";\n      }\n      vc = ci.getIsVirtualCol();\n      if (setColToNonVirtual) {\n        vc = false;\n      }\n      exprColLst.add(new ExprNodeColumnDesc(ci.getType(), ci.getInternalName(), tabAlias, vc));\n    }\n\n    return exprColLst;\n  }  "
            },
            "ql.src.java.org.apache.hadoop.hive.ql.exec.MapJoinOperator.initializeOp": {
                "code_before_change": "  protected Collection<Future<?>> initializeOp(Configuration hconf) throws HiveException {\n    this.hconf = hconf;\n    unwrapContainer = new UnwrapRowContainer[conf.getTagLength()];\n\n    Collection<Future<?>> result = super.initializeOp(hconf);\n    if (result == null) {\n      result = new HashSet<Future<?>>();\n    }\n\n    int tagLen = conf.getTagLength();\n\n    // On Tez only: The hash map might already be cached in the container we run\n    // the task in. On MR: The cache is a no-op.\n    cacheKey = HiveConf.getVar(hconf, HiveConf.ConfVars.HIVEQUERYID)\n      + \"__HASH_MAP_\"+this.getOperatorId()+\"_container\";\n\n    cache = ObjectCacheFactory.getCache(hconf);\n    loader = getHashTableLoader(hconf);\n\n    hashMapRowGetters = null;\n\n    mapJoinTables = new MapJoinTableContainer[tagLen];\n    mapJoinTableSerdes = new MapJoinTableContainerSerDe[tagLen];\n    hashTblInitedOnce = false;\n\n    generateMapMetaData();\n\n    final ExecMapperContext mapContext = getExecContext();\n    final MapredContext mrContext = MapredContext.get();\n\n    if (!conf.isBucketMapJoin()) {\n      /*\n       * The issue with caching in case of bucket map join is that different tasks\n       * process different buckets and if the container is reused to join a different bucket,\n       * join results can be incorrect. The cache is keyed on operator id and for bucket map join\n       * the operator does not change but data needed is different. For a proper fix, this\n       * requires changes in the Tez API with regard to finding bucket id and\n       * also ability to schedule tasks to re-use containers that have cached the specific bucket.\n       */\n      if (isLogInfoEnabled) {\n        LOG.info(\"This is not bucket map join, so cache\");\n      }\n\n      Future<Pair<MapJoinTableContainer[], MapJoinTableContainerSerDe[]>> future =\n          cache.retrieveAsync(\n              cacheKey,\n              new Callable<Pair<MapJoinTableContainer[], MapJoinTableContainerSerDe[]>>() {\n                @Override\n                public Pair<MapJoinTableContainer[], MapJoinTableContainerSerDe[]> call()\n                    throws HiveException {\n                  return loadHashTable(mapContext, mrContext);\n                }\n              });\n      result.add(future);\n    } else if (mapContext == null || mapContext.getLocalWork() == null\n        || mapContext.getLocalWork().getInputFileChangeSensitive() == false) {\n      loadHashTable(mapContext, mrContext);\n      hashTblInitedOnce = true;\n    }\n    return result;\n  }",
                "code_after_change": "  protected Collection<Future<?>> initializeOp(Configuration hconf) throws HiveException {\n    this.hconf = hconf;\n    unwrapContainer = new UnwrapRowContainer[conf.getTagLength()];\n\n    Collection<Future<?>> result = super.initializeOp(hconf);\n    if (result == null) {\n      result = new HashSet<Future<?>>();\n    }\n\n    int tagLen = conf.getTagLength();\n\n    // On Tez only: The hash map might already be cached in the container we run\n    // the task in. On MR: The cache is a no-op.\n    cacheKey = HiveConf.getVar(hconf, HiveConf.ConfVars.HIVEQUERYID)\n      + \"__HASH_MAP_\"+this.getOperatorId()+\"_container\";\n\n    cache = ObjectCacheFactory.getCache(hconf);\n    loader = getHashTableLoader(hconf);\n\n    hashMapRowGetters = null;\n\n    mapJoinTables = new MapJoinTableContainer[tagLen];\n    mapJoinTableSerdes = new MapJoinTableContainerSerDe[tagLen];\n    hashTblInitedOnce = false;\n\n    generateMapMetaData();\n\n    final ExecMapperContext mapContext = getExecContext();\n    final MapredContext mrContext = MapredContext.get();\n\n    if (!conf.isBucketMapJoin() && !conf.isDynamicPartitionHashJoin()) {\n      /*\n       * The issue with caching in case of bucket map join is that different tasks\n       * process different buckets and if the container is reused to join a different bucket,\n       * join results can be incorrect. The cache is keyed on operator id and for bucket map join\n       * the operator does not change but data needed is different. For a proper fix, this\n       * requires changes in the Tez API with regard to finding bucket id and\n       * also ability to schedule tasks to re-use containers that have cached the specific bucket.\n       */\n      if (isLogInfoEnabled) {\n        LOG.info(\"This is not bucket map join, so cache\");\n      }\n\n      Future<Pair<MapJoinTableContainer[], MapJoinTableContainerSerDe[]>> future =\n          cache.retrieveAsync(\n              cacheKey,\n              new Callable<Pair<MapJoinTableContainer[], MapJoinTableContainerSerDe[]>>() {\n                @Override\n                public Pair<MapJoinTableContainer[], MapJoinTableContainerSerDe[]> call()\n                    throws HiveException {\n                  return loadHashTable(mapContext, mrContext);\n                }\n              });\n      result.add(future);\n    } else if (!isInputFileChangeSensitive(mapContext)) {\n      loadHashTable(mapContext, mrContext);\n      hashTblInitedOnce = true;\n    }\n    return result;\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.plan.ExprNodeDescUtils.getSingleParent": {
                "code_before_change": "  public static Operator<?> getSingleParent(Operator<?> current, Operator<?> terminal)\n      throws SemanticException {\n    if (current == terminal) {\n      return null;\n    }\n    List<Operator<?>> parents = current.getParentOperators();\n    if (parents == null || parents.isEmpty()) {\n      if (terminal != null) {\n        throw new SemanticException(\"Failed to meet terminal operator\");\n      }\n      return null;\n    }\n    if (parents.size() == 1) {\n      return parents.get(0);\n    }\n    if (terminal != null && parents.contains(terminal)) {\n      return terminal;\n    }\n    throw new SemanticException(\"Met multiple parent operators\");\n  }",
                "code_after_change": "  public static Operator<?> getSingleParent(Operator<?> current, Operator<?> terminal)\n      throws SemanticException {\n    if (current == terminal) {\n      return null;\n    }\n    List<Operator<?>> parents = current.getParentOperators();\n    if (parents == null || parents.isEmpty()) {\n      if (terminal != null) {\n        throw new SemanticException(\"Failed to meet terminal operator\");\n      }\n      return null;\n    }\n    if (parents.size() == 1) {\n      return parents.get(0);\n    }\n    if (terminal != null && parents.contains(terminal)) {\n      return terminal;\n    }\n    throw new SemanticException(\"Met multiple parent operators\");\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.exec.Operator.initializeOp": {
                "code_before_change": "  protected Collection<Future<?>> initializeOp(Configuration hconf) throws HiveException {\n    rootInitializeCalled = true;\n    return new ArrayList<Future<?>>();\n  }",
                "code_after_change": "  protected Collection<Future<?>> initializeOp(Configuration hconf) throws HiveException {\n    rootInitializeCalled = true;\n    return new ArrayList<Future<?>>();\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin.findDynamicPartitionBroadcast": {
                "code_before_change": "  private AppMasterEventOperator findDynamicPartitionBroadcast(Operator<?> parent) {\n\n    for (Operator<?> op : parent.getChildOperators()) {\n      while (op != null) {\n        if (op instanceof AppMasterEventOperator && op.getConf() instanceof DynamicPruningEventDesc) {\n          // found dynamic partition pruning operator\n          return (AppMasterEventOperator)op;\n        }\n        if (op instanceof ReduceSinkOperator || op instanceof FileSinkOperator) {\n          // crossing reduce sink or file sink means the pruning isn't for this parent.\n          break;\n        }\n\n        if (op.getChildOperators().size() != 1) {\n          // dynamic partition pruning pipeline doesn't have multiple children\n          break;\n        }\n\n        op = op.getChildOperators().get(0);\n      }\n    }\n\n    return null;\n  }",
                "code_after_change": "  private AppMasterEventOperator findDynamicPartitionBroadcast(Operator<?> parent) {\n\n    for (Operator<?> op : parent.getChildOperators()) {\n      while (op != null) {\n        if (op instanceof AppMasterEventOperator && op.getConf() instanceof DynamicPruningEventDesc) {\n          // found dynamic partition pruning operator\n          return (AppMasterEventOperator)op;\n        }\n        if (op instanceof ReduceSinkOperator || op instanceof FileSinkOperator) {\n          // crossing reduce sink or file sink means the pruning isn't for this parent.\n          break;\n        }\n\n        if (op.getChildOperators().size() != 1) {\n          // dynamic partition pruning pipeline doesn't have multiple children\n          break;\n        }\n\n        op = op.getChildOperators().get(0);\n      }\n    }\n\n    return null;\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.init": {
                "code_before_change": "  void init(\n      MRTaskReporter mrReporter, Map<String, LogicalInput> inputs,\n      Map<String, LogicalOutput> outputs) throws Exception {\n    perfLogger.PerfLogBegin(CLASS_NAME, PerfLogger.TEZ_INIT_OPERATORS);\n    super.init(mrReporter, inputs, outputs);\n\n    MapredContext.init(false, new JobConf(jconf));\n    List<LogicalInput> shuffleInputs = getShuffleInputs(inputs);\n    if (shuffleInputs != null) {\n      l4j.info(\"Waiting for ShuffleInputs to become ready\");\n      processorContext.waitForAllInputsReady(new ArrayList<Input>(shuffleInputs));\n    }\n\n    connectOps.clear();\n    ReduceWork redWork = reduceWork;\n    tagToReducerMap.put(redWork.getTag(), redWork);\n    if (mergeWorkList != null) {\n      for (BaseWork mergeWork : mergeWorkList) {\n        ReduceWork mergeReduceWork = (ReduceWork) mergeWork;\n        reducer = mergeReduceWork.getReducer();\n        DummyStoreOperator dummyStoreOp = getJoinParentOp(reducer);\n        connectOps.put(mergeReduceWork.getTag(), dummyStoreOp);\n        tagToReducerMap.put(mergeReduceWork.getTag(), mergeReduceWork);\n      }\n\n      bigTablePosition = (byte) reduceWork.getTag();\n      ((TezContext) MapredContext.get()).setDummyOpsMap(connectOps);\n    }\n\n    ObjectInspector[] mainWorkOIs = null;\n    ((TezContext) MapredContext.get()).setInputs(inputs);\n    ((TezContext) MapredContext.get()).setTezProcessorContext(processorContext);\n    int numTags = reduceWork.getTagToValueDesc().size();\n    reducer = reduceWork.getReducer();\n    if (numTags > 1) {\n      sources = new ReduceRecordSource[numTags];\n      mainWorkOIs = new ObjectInspector[numTags];\n      initializeMultipleSources(reduceWork, numTags, mainWorkOIs, sources);\n      ((TezContext) MapredContext.get()).setRecordSources(sources);\n      reducer.initialize(jconf, mainWorkOIs);\n    } else {\n      numTags = tagToReducerMap.keySet().size();\n      sources = new ReduceRecordSource[numTags];\n      mainWorkOIs = new ObjectInspector[numTags];\n      for (int i : tagToReducerMap.keySet()) {\n        redWork = tagToReducerMap.get(i);\n        reducer = redWork.getReducer();\n        initializeSourceForTag(redWork, i, mainWorkOIs, sources,\n            redWork.getTagToValueDesc().get(0), redWork.getTagToInput().get(0));\n        reducer.initializeLocalWork(jconf);\n      }\n      reducer = reduceWork.getReducer();\n      ((TezContext) MapredContext.get()).setRecordSources(sources);\n      reducer.initialize(jconf, new ObjectInspector[] { mainWorkOIs[bigTablePosition] });\n      for (int i : tagToReducerMap.keySet()) {\n        if (i == bigTablePosition) {\n          continue;\n        }\n        redWork = tagToReducerMap.get(i);\n        reducer = redWork.getReducer();\n        reducer.initialize(jconf, new ObjectInspector[] { mainWorkOIs[i] });\n      }\n    }\n\n    reducer = reduceWork.getReducer();\n    // initialize reduce operator tree\n    try {\n      l4j.info(reducer.dump(0));\n\n      // Initialization isn't finished until all parents of all operators\n      // are initialized. For broadcast joins that means initializing the\n      // dummy parent operators as well.\n      List<HashTableDummyOperator> dummyOps = redWork.getDummyOps();\n      if (dummyOps != null) {\n        for (HashTableDummyOperator dummyOp : dummyOps) {\n          dummyOp.initialize(jconf, null);\n        }\n      }\n\n      // set output collector for any reduce sink operators in the pipeline.\n      List<Operator<?>> children = new LinkedList<Operator<?>>();\n      children.add(reducer);\n      if (dummyOps != null) {\n        children.addAll(dummyOps);\n      }\n      createOutputMap();\n      OperatorUtils.setChildrenCollector(children, outMap);\n\n      reducer.setReporter(reporter);\n      MapredContext.get().setReporter(reporter);\n\n    } catch (Throwable e) {\n      abort = true;\n      if (e instanceof OutOfMemoryError) {\n        // Don't create a new object if we are already out of memory\n        throw (OutOfMemoryError) e;\n      } else {\n        throw new RuntimeException(\"Reduce operator initialization failed\", e);\n      }\n    }\n\n    perfLogger.PerfLogEnd(CLASS_NAME, PerfLogger.TEZ_INIT_OPERATORS);\n  }",
                "code_after_change": "  void init(\n      MRTaskReporter mrReporter, Map<String, LogicalInput> inputs,\n      Map<String, LogicalOutput> outputs) throws Exception {\n    perfLogger.PerfLogBegin(CLASS_NAME, PerfLogger.TEZ_INIT_OPERATORS);\n    super.init(mrReporter, inputs, outputs);\n\n    MapredContext.init(false, new JobConf(jconf));\n    List<LogicalInput> shuffleInputs = getShuffleInputs(inputs);\n    if (shuffleInputs != null) {\n      l4j.info(\"Waiting for ShuffleInputs to become ready\");\n      processorContext.waitForAllInputsReady(new ArrayList<Input>(shuffleInputs));\n    }\n\n    connectOps.clear();\n    ReduceWork redWork = reduceWork;\n    tagToReducerMap.put(redWork.getTag(), redWork);\n    if (mergeWorkList != null) {\n      for (BaseWork mergeWork : mergeWorkList) {\n        ReduceWork mergeReduceWork = (ReduceWork) mergeWork;\n        reducer = mergeReduceWork.getReducer();\n        DummyStoreOperator dummyStoreOp = getJoinParentOp(reducer);\n        connectOps.put(mergeReduceWork.getTag(), dummyStoreOp);\n        tagToReducerMap.put(mergeReduceWork.getTag(), mergeReduceWork);\n      }\n\n      ((TezContext) MapredContext.get()).setDummyOpsMap(connectOps);\n    }\n\n    bigTablePosition = (byte) reduceWork.getTag();\n\n    ObjectInspector[] mainWorkOIs = null;\n    ((TezContext) MapredContext.get()).setInputs(inputs);\n    ((TezContext) MapredContext.get()).setTezProcessorContext(processorContext);\n    int numTags = reduceWork.getTagToValueDesc().size();\n    reducer = reduceWork.getReducer();\n    if (numTags > 1) {\n      sources = new ReduceRecordSource[numTags];\n      mainWorkOIs = new ObjectInspector[numTags];\n      initializeMultipleSources(reduceWork, numTags, mainWorkOIs, sources);\n      ((TezContext) MapredContext.get()).setRecordSources(sources);\n      reducer.initialize(jconf, mainWorkOIs);\n    } else {\n      numTags = tagToReducerMap.keySet().size();\n      sources = new ReduceRecordSource[numTags];\n      mainWorkOIs = new ObjectInspector[numTags];\n      for (int i : tagToReducerMap.keySet()) {\n        redWork = tagToReducerMap.get(i);\n        reducer = redWork.getReducer();\n        initializeSourceForTag(redWork, i, mainWorkOIs, sources,\n            redWork.getTagToValueDesc().get(0), redWork.getTagToInput().get(0));\n        reducer.initializeLocalWork(jconf);\n      }\n      reducer = reduceWork.getReducer();\n      ((TezContext) MapredContext.get()).setRecordSources(sources);\n      reducer.initialize(jconf, new ObjectInspector[] { mainWorkOIs[bigTablePosition] });\n      for (int i : tagToReducerMap.keySet()) {\n        if (i == bigTablePosition) {\n          continue;\n        }\n        redWork = tagToReducerMap.get(i);\n        reducer = redWork.getReducer();\n        reducer.initialize(jconf, new ObjectInspector[] { mainWorkOIs[i] });\n      }\n    }\n\n    reducer = reduceWork.getReducer();\n    // initialize reduce operator tree\n    try {\n      l4j.info(reducer.dump(0));\n\n      // Initialization isn't finished until all parents of all operators\n      // are initialized. For broadcast joins that means initializing the\n      // dummy parent operators as well.\n      List<HashTableDummyOperator> dummyOps = redWork.getDummyOps();\n      if (dummyOps != null) {\n        for (HashTableDummyOperator dummyOp : dummyOps) {\n          dummyOp.initialize(jconf, null);\n        }\n      }\n\n      // set output collector for any reduce sink operators in the pipeline.\n      List<Operator<?>> children = new LinkedList<Operator<?>>();\n      children.add(reducer);\n      if (dummyOps != null) {\n        children.addAll(dummyOps);\n      }\n      createOutputMap();\n      OperatorUtils.setChildrenCollector(children, outMap);\n\n      reducer.setReporter(reporter);\n      MapredContext.get().setReporter(reporter);\n\n    } catch (Throwable e) {\n      abort = true;\n      if (e instanceof OutOfMemoryError) {\n        // Don't create a new object if we are already out of memory\n        throw (OutOfMemoryError) e;\n      } else {\n        throw new RuntimeException(\"Reduce operator initialization failed\", e);\n      }\n    }\n\n    perfLogger.PerfLogEnd(CLASS_NAME, PerfLogger.TEZ_INIT_OPERATORS);\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.parse.GenTezUtils.createReduceWork": {
                "code_before_change": "  public static ReduceWork createReduceWork(\n      GenTezProcContext context, Operator<?> root, TezWork tezWork) {\n    assert !root.getParentOperators().isEmpty();\n\n    boolean isAutoReduceParallelism =\n        context.conf.getBoolVar(HiveConf.ConfVars.TEZ_AUTO_REDUCER_PARALLELISM);\n\n    float maxPartitionFactor =\n        context.conf.getFloatVar(HiveConf.ConfVars.TEZ_MAX_PARTITION_FACTOR);\n    float minPartitionFactor = context.conf.getFloatVar(HiveConf.ConfVars.TEZ_MIN_PARTITION_FACTOR);\n    long bytesPerReducer = context.conf.getLongVar(HiveConf.ConfVars.BYTESPERREDUCER);\n\n    ReduceWork reduceWork = new ReduceWork(Utilities.REDUCENAME + context.nextSequenceNumber());\n    LOG.debug(\"Adding reduce work (\" + reduceWork.getName() + \") for \" + root);\n    reduceWork.setReducer(root);\n    reduceWork.setNeedsTagging(GenMapRedUtils.needsTagging(reduceWork));\n\n    // All parents should be reduce sinks. We pick the one we just walked\n    // to choose the number of reducers. In the join/union case they will\n    // all be -1. In sort/order case where it matters there will be only\n    // one parent.\n    assert context.parentOfRoot instanceof ReduceSinkOperator;\n    ReduceSinkOperator reduceSink = (ReduceSinkOperator) context.parentOfRoot;\n\n    reduceWork.setNumReduceTasks(reduceSink.getConf().getNumReducers());\n\n    if (isAutoReduceParallelism && reduceSink.getConf().getReducerTraits().contains(AUTOPARALLEL)) {\n      reduceWork.setAutoReduceParallelism(true);\n\n      // configured limit for reducers\n      int maxReducers = context.conf.getIntVar(HiveConf.ConfVars.MAXREDUCERS);\n\n      // min we allow tez to pick\n      int minPartition = Math.max(1, (int) (reduceSink.getConf().getNumReducers()\n        * minPartitionFactor));\n      minPartition = (minPartition > maxReducers) ? maxReducers : minPartition;\n\n      // max we allow tez to pick\n      int maxPartition = (int) (reduceSink.getConf().getNumReducers() * maxPartitionFactor);\n      maxPartition = (maxPartition > maxReducers) ? maxReducers : maxPartition;\n\n      reduceWork.setMinReduceTasks(minPartition);\n      reduceWork.setMaxReduceTasks(maxPartition);\n    }\n\n    setupReduceSink(context, reduceWork, reduceSink);\n\n    tezWork.add(reduceWork);\n\n    TezEdgeProperty edgeProp;\n    if (reduceWork.isAutoReduceParallelism()) {\n      edgeProp =\n          new TezEdgeProperty(context.conf, EdgeType.SIMPLE_EDGE, true,\n              reduceWork.getMinReduceTasks(), reduceWork.getMaxReduceTasks(), bytesPerReducer);\n    } else {\n      edgeProp = new TezEdgeProperty(EdgeType.SIMPLE_EDGE);\n    }\n\n    tezWork.connect(\n        context.preceedingWork,\n        reduceWork, edgeProp);\n    context.connectedReduceSinks.add(reduceSink);\n\n    return reduceWork;\n  }",
                "code_after_change": "  public static ReduceWork createReduceWork(\n      GenTezProcContext context, Operator<?> root, TezWork tezWork) {\n    assert !root.getParentOperators().isEmpty();\n\n    boolean isAutoReduceParallelism =\n        context.conf.getBoolVar(HiveConf.ConfVars.TEZ_AUTO_REDUCER_PARALLELISM);\n\n    float maxPartitionFactor =\n        context.conf.getFloatVar(HiveConf.ConfVars.TEZ_MAX_PARTITION_FACTOR);\n    float minPartitionFactor = context.conf.getFloatVar(HiveConf.ConfVars.TEZ_MIN_PARTITION_FACTOR);\n    long bytesPerReducer = context.conf.getLongVar(HiveConf.ConfVars.BYTESPERREDUCER);\n\n    ReduceWork reduceWork = new ReduceWork(Utilities.REDUCENAME + context.nextSequenceNumber());\n    LOG.debug(\"Adding reduce work (\" + reduceWork.getName() + \") for \" + root);\n    reduceWork.setReducer(root);\n    reduceWork.setNeedsTagging(GenMapRedUtils.needsTagging(reduceWork));\n\n    // All parents should be reduce sinks. We pick the one we just walked\n    // to choose the number of reducers. In the join/union case they will\n    // all be -1. In sort/order case where it matters there will be only\n    // one parent.\n    assert context.parentOfRoot instanceof ReduceSinkOperator;\n    ReduceSinkOperator reduceSink = (ReduceSinkOperator) context.parentOfRoot;\n\n    reduceWork.setNumReduceTasks(reduceSink.getConf().getNumReducers());\n\n    if (isAutoReduceParallelism && reduceSink.getConf().getReducerTraits().contains(AUTOPARALLEL)) {\n      reduceWork.setAutoReduceParallelism(true);\n\n      // configured limit for reducers\n      int maxReducers = context.conf.getIntVar(HiveConf.ConfVars.MAXREDUCERS);\n\n      // min we allow tez to pick\n      int minPartition = Math.max(1, (int) (reduceSink.getConf().getNumReducers()\n        * minPartitionFactor));\n      minPartition = (minPartition > maxReducers) ? maxReducers : minPartition;\n\n      // max we allow tez to pick\n      int maxPartition = (int) (reduceSink.getConf().getNumReducers() * maxPartitionFactor);\n      maxPartition = (maxPartition > maxReducers) ? maxReducers : maxPartition;\n\n      reduceWork.setMinReduceTasks(minPartition);\n      reduceWork.setMaxReduceTasks(maxPartition);\n    }\n\n    setupReduceSink(context, reduceWork, reduceSink);\n\n    tezWork.add(reduceWork);\n\n    TezEdgeProperty edgeProp;\n    EdgeType edgeType = determineEdgeType(context.preceedingWork, reduceWork);\n    if (reduceWork.isAutoReduceParallelism()) {\n      edgeProp =\n          new TezEdgeProperty(context.conf, edgeType, true,\n              reduceWork.getMinReduceTasks(), reduceWork.getMaxReduceTasks(), bytesPerReducer);\n    } else {\n      edgeProp = new TezEdgeProperty(edgeType);\n    }\n\n    tezWork.connect(\n        context.preceedingWork,\n        reduceWork, edgeProp);\n    context.connectedReduceSinks.add(reduceSink);\n\n    return reduceWork;\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin.getMapJoinConversionPos": {
                "code_before_change": "  public int getMapJoinConversionPos(JoinOperator joinOp, OptimizeTezProcContext context,\n      int buckets) throws SemanticException {\n    /*\n     * HIVE-9038: Join tests fail in tez when we have more than 1 join on the same key and there is\n     * an outer join down the join tree that requires filterTag. We disable this conversion to map\n     * join here now. We need to emulate the behavior of HashTableSinkOperator as in MR or create a\n     * new operation to be able to support this. This seems like a corner case enough to special\n     * case this for now.\n     */\n    if (joinOp.getConf().getConds().length > 1) {\n      boolean hasOuter = false;\n      for (JoinCondDesc joinCondDesc : joinOp.getConf().getConds()) {\n        switch (joinCondDesc.getType()) {\n        case JoinDesc.INNER_JOIN:\n        case JoinDesc.LEFT_SEMI_JOIN:\n        case JoinDesc.UNIQUE_JOIN:\n          hasOuter = false;\n          break;\n\n        case JoinDesc.FULL_OUTER_JOIN:\n        case JoinDesc.LEFT_OUTER_JOIN:\n        case JoinDesc.RIGHT_OUTER_JOIN:\n          hasOuter = true;\n          break;\n\n        default:\n          throw new SemanticException(\"Unknown join type \" + joinCondDesc.getType());\n        }\n      }\n      if (hasOuter) {\n        return -1;\n      }\n    }\n    Set<Integer> bigTableCandidateSet =\n        MapJoinProcessor.getBigTableCandidates(joinOp.getConf().getConds());\n\n    long maxSize = context.conf.getLongVar(\n        HiveConf.ConfVars.HIVECONVERTJOINNOCONDITIONALTASKTHRESHOLD);\n\n    int bigTablePosition = -1;\n\n    Statistics bigInputStat = null;\n    long totalSize = 0;\n    int pos = 0;\n\n    // bigTableFound means we've encountered a table that's bigger than the\n    // max. This table is either the the big table or we cannot convert.\n    boolean bigTableFound = false;\n\n    for (Operator<? extends OperatorDesc> parentOp : joinOp.getParentOperators()) {\n\n      Statistics currInputStat = parentOp.getStatistics();\n      if (currInputStat == null) {\n        LOG.warn(\"Couldn't get statistics from: \" + parentOp);\n        return -1;\n      }\n\n      long inputSize = currInputStat.getDataSize();\n      if ((bigInputStat == null)\n          || ((bigInputStat != null) && (inputSize > bigInputStat.getDataSize()))) {\n\n        if (bigTableFound) {\n          // cannot convert to map join; we've already chosen a big table\n          // on size and there's another one that's bigger.\n          return -1;\n        }\n\n        if (inputSize/buckets > maxSize) {\n          if (!bigTableCandidateSet.contains(pos)) {\n            // can't use the current table as the big table, but it's too\n            // big for the map side.\n            return -1;\n          }\n\n          bigTableFound = true;\n        }\n\n        if (bigInputStat != null) {\n          // we're replacing the current big table with a new one. Need\n          // to count the current one as a map table then.\n          totalSize += bigInputStat.getDataSize();\n        }\n\n        if (totalSize/buckets > maxSize) {\n          // sum of small tables size in this join exceeds configured limit\n          // hence cannot convert.\n          return -1;\n        }\n\n        if (bigTableCandidateSet.contains(pos)) {\n          bigTablePosition = pos;\n          bigInputStat = currInputStat;\n        }\n      } else {\n        totalSize += currInputStat.getDataSize();\n        if (totalSize/buckets > maxSize) {\n          // cannot hold all map tables in memory. Cannot convert.\n          return -1;\n        }\n      }\n      pos++;\n    }\n\n    return bigTablePosition;\n  }",
                "code_after_change": "  public int getMapJoinConversionPos(JoinOperator joinOp, OptimizeTezProcContext context,\n      int buckets) throws SemanticException {\n    /*\n     * HIVE-9038: Join tests fail in tez when we have more than 1 join on the same key and there is\n     * an outer join down the join tree that requires filterTag. We disable this conversion to map\n     * join here now. We need to emulate the behavior of HashTableSinkOperator as in MR or create a\n     * new operation to be able to support this. This seems like a corner case enough to special\n     * case this for now.\n     */\n    if (joinOp.getConf().getConds().length > 1) {\n      boolean hasOuter = false;\n      for (JoinCondDesc joinCondDesc : joinOp.getConf().getConds()) {\n        switch (joinCondDesc.getType()) {\n        case JoinDesc.INNER_JOIN:\n        case JoinDesc.LEFT_SEMI_JOIN:\n        case JoinDesc.UNIQUE_JOIN:\n          hasOuter = false;\n          break;\n\n        case JoinDesc.FULL_OUTER_JOIN:\n        case JoinDesc.LEFT_OUTER_JOIN:\n        case JoinDesc.RIGHT_OUTER_JOIN:\n          hasOuter = true;\n          break;\n\n        default:\n          throw new SemanticException(\"Unknown join type \" + joinCondDesc.getType());\n        }\n      }\n      if (hasOuter) {\n        return -1;\n      }\n    }\n    Set<Integer> bigTableCandidateSet =\n        MapJoinProcessor.getBigTableCandidates(joinOp.getConf().getConds());\n\n    long maxSize = context.conf.getLongVar(\n        HiveConf.ConfVars.HIVECONVERTJOINNOCONDITIONALTASKTHRESHOLD);\n\n    int bigTablePosition = -1;\n\n    Statistics bigInputStat = null;\n    long totalSize = 0;\n    int pos = 0;\n\n    // bigTableFound means we've encountered a table that's bigger than the\n    // max. This table is either the the big table or we cannot convert.\n    boolean bigTableFound = false;\n\n    for (Operator<? extends OperatorDesc> parentOp : joinOp.getParentOperators()) {\n\n      Statistics currInputStat = parentOp.getStatistics();\n      if (currInputStat == null) {\n        LOG.warn(\"Couldn't get statistics from: \" + parentOp);\n        return -1;\n      }\n\n      long inputSize = currInputStat.getDataSize();\n      if ((bigInputStat == null)\n          || ((bigInputStat != null) && (inputSize > bigInputStat.getDataSize()))) {\n\n        if (bigTableFound) {\n          // cannot convert to map join; we've already chosen a big table\n          // on size and there's another one that's bigger.\n          return -1;\n        }\n\n        if (inputSize/buckets > maxSize) {\n          if (!bigTableCandidateSet.contains(pos)) {\n            // can't use the current table as the big table, but it's too\n            // big for the map side.\n            return -1;\n          }\n\n          bigTableFound = true;\n        }\n\n        if (bigInputStat != null) {\n          // we're replacing the current big table with a new one. Need\n          // to count the current one as a map table then.\n          totalSize += bigInputStat.getDataSize();\n        }\n\n        if (totalSize/buckets > maxSize) {\n          // sum of small tables size in this join exceeds configured limit\n          // hence cannot convert.\n          return -1;\n        }\n\n        if (bigTableCandidateSet.contains(pos)) {\n          bigTablePosition = pos;\n          bigInputStat = currInputStat;\n        }\n      } else {\n        totalSize += currInputStat.getDataSize();\n        if (totalSize/buckets > maxSize) {\n          // cannot hold all map tables in memory. Cannot convert.\n          return -1;\n        }\n      }\n      pos++;\n    }\n\n    return bigTablePosition;\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.plan.MapJoinDesc.setGenJoinKeys": {
                "code_before_change": "  public void setGenJoinKeys(boolean genJoinKeys) {\n    this.genJoinKeys = genJoinKeys;\n  }",
                "code_after_change": "  public void setGenJoinKeys(boolean genJoinKeys) {\n    this.genJoinKeys = genJoinKeys;\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.plan.CommonMergeJoinDesc.getNumBuckets": {
                "code_before_change": "  public int getNumBuckets() {\n    return numBuckets;\n  }",
                "code_after_change": "  public int getNumBuckets() {\n    return numBuckets;\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.optimizer.ReduceSinkMapJoinProc.processReduceSinkToHashJoin": {
                "code_before_change": [],
                "code_after_change": "  public static Object processReduceSinkToHashJoin(ReduceSinkOperator parentRS, MapJoinOperator mapJoinOp,\n      GenTezProcContext context) throws SemanticException {\n    // remove the tag for in-memory side of mapjoin\n    parentRS.getConf().setSkipTag(true);\n    parentRS.setSkipTag(true);\n\n    // Mark this small table as being processed\n    if (mapJoinOp.getConf().isDynamicPartitionHashJoin()) {\n      context.mapJoinToUnprocessedSmallTableReduceSinks.get(mapJoinOp).remove(parentRS);\n    }\n\n    List<BaseWork> mapJoinWork = null;\n\n    /*\n     *  if there was a pre-existing work generated for the big-table mapjoin side,\n     *  we need to hook the work generated for the RS (associated with the RS-MJ pattern)\n     *  with the pre-existing work.\n     *\n     *  Otherwise, we need to associate that the mapjoin op\n     *  to be linked to the RS work (associated with the RS-MJ pattern).\n     *\n     */\n    mapJoinWork = context.mapJoinWorkMap.get(mapJoinOp);\n    BaseWork parentWork = getMapJoinParentWork(context, parentRS);\n\n    // set the link between mapjoin and parent vertex\n    int pos = context.mapJoinParentMap.get(mapJoinOp).indexOf(parentRS);\n    if (pos == -1) {\n      throw new SemanticException(\"Cannot find position of parent in mapjoin\");\n    }\n    MapJoinDesc joinConf = mapJoinOp.getConf();\n    long keyCount = Long.MAX_VALUE, rowCount = Long.MAX_VALUE, bucketCount = 1;\n    long tableSize = Long.MAX_VALUE;\n    Statistics stats = parentRS.getStatistics();\n    if (stats != null) {\n      keyCount = rowCount = stats.getNumRows();\n      if (keyCount <= 0) {\n        keyCount = rowCount = Long.MAX_VALUE;\n      }\n      tableSize = stats.getDataSize();\n      ArrayList<String> keyCols = parentRS.getConf().getOutputKeyColumnNames();\n      if (keyCols != null && !keyCols.isEmpty()) {\n        // See if we can arrive at a smaller number using distinct stats from key columns.\n        long maxKeyCount = 1;\n        String prefix = Utilities.ReduceField.KEY.toString();\n        for (String keyCol : keyCols) {\n          ExprNodeDesc realCol = parentRS.getColumnExprMap().get(prefix + \".\" + keyCol);\n          ColStatistics cs =\n              StatsUtils.getColStatisticsFromExpression(context.conf, stats, realCol);\n          if (cs == null || cs.getCountDistint() <= 0) {\n            maxKeyCount = Long.MAX_VALUE;\n            break;\n          }\n          maxKeyCount *= cs.getCountDistint();\n          if (maxKeyCount >= keyCount) {\n            break;\n          }\n        }\n        keyCount = Math.min(maxKeyCount, keyCount);\n      }\n      if (joinConf.isBucketMapJoin()) {\n        OpTraits opTraits = mapJoinOp.getOpTraits();\n        bucketCount = (opTraits == null) ? -1 : opTraits.getNumBuckets();\n        if (bucketCount > 0) {\n          // We cannot obtain a better estimate without CustomPartitionVertex providing it\n          // to us somehow; in which case using statistics would be completely unnecessary.\n          keyCount /= bucketCount;\n          tableSize /= bucketCount;\n        }\n      } else if (joinConf.isDynamicPartitionHashJoin()) {\n        // For dynamic partitioned hash join, assuming table is split evenly among the reduce tasks.\n        bucketCount = parentRS.getConf().getNumReducers();\n        keyCount /= bucketCount;\n        tableSize /= bucketCount;\n      }\n    }\n    LOG.info(\"Mapjoin \" + mapJoinOp + \", pos: \" + pos + \" --> \" + parentWork.getName() + \" (\"\n      + keyCount + \" keys estimated from \" + rowCount + \" rows, \" + bucketCount + \" buckets)\");\n    joinConf.getParentToInput().put(pos, parentWork.getName());\n    if (keyCount != Long.MAX_VALUE) {\n      joinConf.getParentKeyCounts().put(pos, keyCount);\n    }\n    joinConf.getParentDataSizes().put(pos, tableSize);\n\n    int numBuckets = -1;\n    EdgeType edgeType = EdgeType.BROADCAST_EDGE;\n    if (joinConf.isBucketMapJoin()) {\n\n      // disable auto parallelism for bucket map joins\n      parentRS.getConf().setReducerTraits(EnumSet.of(FIXED));\n\n      numBuckets = (Integer) joinConf.getBigTableBucketNumMapping().values().toArray()[0];\n      /*\n       * Here, we can be in one of 4 states.\n       *\n       * 1. If map join work is null implies that we have not yet traversed the big table side. We\n       * just need to see if we can find a reduce sink operator in the big table side. This would\n       * imply a reduce side operation.\n       *\n       * 2. If we don't find a reducesink in 1 it has to be the case that it is a map side operation.\n       *\n       * 3. If we have already created a work item for the big table side, we need to see if we can\n       * find a table scan operator in the big table side. This would imply a map side operation.\n       *\n       * 4. If we don't find a table scan operator, it has to be a reduce side operation.\n       */\n      if (mapJoinWork == null) {\n        Operator<?> rootOp =\n          OperatorUtils.findSingleOperatorUpstream(\n              mapJoinOp.getParentOperators().get(joinConf.getPosBigTable()),\n              ReduceSinkOperator.class);\n        if (rootOp == null) {\n          // likely we found a table scan operator\n          edgeType = EdgeType.CUSTOM_EDGE;\n        } else {\n          // we have found a reduce sink\n          edgeType = EdgeType.CUSTOM_SIMPLE_EDGE;\n        }\n      } else {\n        Operator<?> rootOp =\n            OperatorUtils.findSingleOperatorUpstream(\n                mapJoinOp.getParentOperators().get(joinConf.getPosBigTable()),\n                TableScanOperator.class);\n        if (rootOp != null) {\n          // likely we found a table scan operator\n          edgeType = EdgeType.CUSTOM_EDGE;\n        } else {\n          // we have found a reduce sink\n          edgeType = EdgeType.CUSTOM_SIMPLE_EDGE;\n        }\n      }\n    } else if (mapJoinOp.getConf().isDynamicPartitionHashJoin()) {\n      edgeType = EdgeType.CUSTOM_SIMPLE_EDGE;\n    }\n    TezEdgeProperty edgeProp = new TezEdgeProperty(null, edgeType, numBuckets);\n\n    if (mapJoinWork != null) {\n      for (BaseWork myWork: mapJoinWork) {\n        // link the work with the work associated with the reduce sink that triggered this rule\n        TezWork tezWork = context.currentTask.getWork();\n        LOG.debug(\"connecting \"+parentWork.getName()+\" with \"+myWork.getName());\n        tezWork.connect(parentWork, myWork, edgeProp);\n        if (edgeType == EdgeType.CUSTOM_EDGE) {\n          tezWork.setVertexType(myWork, VertexType.INITIALIZED_EDGES);\n        }\n\n        ReduceSinkOperator r = null;\n        if (context.connectedReduceSinks.contains(parentRS)) {\n          LOG.debug(\"Cloning reduce sink for multi-child broadcast edge\");\n          // we've already set this one up. Need to clone for the next work.\n          r = (ReduceSinkOperator) OperatorFactory.getAndMakeChild(\n              (ReduceSinkDesc) parentRS.getConf().clone(),\n              new RowSchema(parentRS.getSchema()),\n              parentRS.getParentOperators());\n          context.clonedReduceSinks.add(r);\n        } else {\n          r = parentRS;\n        }\n        // remember the output name of the reduce sink\n        r.getConf().setOutputName(myWork.getName());\n        context.connectedReduceSinks.add(r);\n      }\n    }\n\n    // remember in case we need to connect additional work later\n    Map<BaseWork, TezEdgeProperty> linkWorkMap = null;\n    if (context.linkOpWithWorkMap.containsKey(mapJoinOp)) {\n      linkWorkMap = context.linkOpWithWorkMap.get(mapJoinOp);\n    } else {\n      linkWorkMap = new HashMap<BaseWork, TezEdgeProperty>();\n    }\n    linkWorkMap.put(parentWork, edgeProp);\n    context.linkOpWithWorkMap.put(mapJoinOp, linkWorkMap);\n\n    List<ReduceSinkOperator> reduceSinks\n      = context.linkWorkWithReduceSinkMap.get(parentWork);\n    if (reduceSinks == null) {\n      reduceSinks = new ArrayList<ReduceSinkOperator>();\n    }\n    reduceSinks.add(parentRS);\n    context.linkWorkWithReduceSinkMap.put(parentWork, reduceSinks);\n\n    // create the dummy operators\n    List<Operator<?>> dummyOperators = new ArrayList<Operator<?>>();\n\n    // create an new operator: HashTableDummyOperator, which share the table desc\n    HashTableDummyDesc desc = new HashTableDummyDesc();\n    @SuppressWarnings(\"unchecked\")\n    HashTableDummyOperator dummyOp = (HashTableDummyOperator) OperatorFactory.get(desc);\n    TableDesc tbl;\n\n    // need to create the correct table descriptor for key/value\n    RowSchema rowSchema = parentRS.getParentOperators().get(0).getSchema();\n    tbl = PlanUtils.getReduceValueTableDesc(PlanUtils.getFieldSchemasFromRowSchema(rowSchema, \"\"));\n    dummyOp.getConf().setTbl(tbl);\n\n    Map<Byte, List<ExprNodeDesc>> keyExprMap = mapJoinOp.getConf().getKeys();\n    List<ExprNodeDesc> keyCols = keyExprMap.get(Byte.valueOf((byte) 0));\n    StringBuilder keyOrder = new StringBuilder();\n    for (ExprNodeDesc k: keyCols) {\n      keyOrder.append(\"+\");\n    }\n    TableDesc keyTableDesc = PlanUtils.getReduceKeyTableDesc(PlanUtils\n        .getFieldSchemasFromColumnList(keyCols, \"mapjoinkey\"), keyOrder.toString());\n    mapJoinOp.getConf().setKeyTableDesc(keyTableDesc);\n\n    // let the dummy op be the parent of mapjoin op\n    mapJoinOp.replaceParent(parentRS, dummyOp);\n    List<Operator<? extends OperatorDesc>> dummyChildren =\n      new ArrayList<Operator<? extends OperatorDesc>>();\n    dummyChildren.add(mapJoinOp);\n    dummyOp.setChildOperators(dummyChildren);\n    dummyOperators.add(dummyOp);\n\n    // cut the operator tree so as to not retain connections from the parent RS downstream\n    List<Operator<? extends OperatorDesc>> childOperators = parentRS.getChildOperators();\n    int childIndex = childOperators.indexOf(mapJoinOp);\n    childOperators.remove(childIndex);\n\n    // the \"work\" needs to know about the dummy operators. They have to be separately initialized\n    // at task startup\n    if (mapJoinWork != null) {\n      for (BaseWork myWork: mapJoinWork) {\n        myWork.addDummyOp(dummyOp);\n      }\n    }\n    if (context.linkChildOpWithDummyOp.containsKey(mapJoinOp)) {\n      for (Operator<?> op: context.linkChildOpWithDummyOp.get(mapJoinOp)) {\n        dummyOperators.add(op);\n      }\n    }\n    context.linkChildOpWithDummyOp.put(mapJoinOp, dummyOperators);\n\n    return true;\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin.convertJoinBucketMapJoin": {
                "code_before_change": "  private boolean convertJoinBucketMapJoin(JoinOperator joinOp, OptimizeTezProcContext context,\n      int bigTablePosition, TezBucketJoinProcCtx tezBucketJoinProcCtx) throws SemanticException {\n\n    if (!checkConvertJoinBucketMapJoin(joinOp, context, bigTablePosition, tezBucketJoinProcCtx)) {\n      LOG.info(\"Check conversion to bucket map join failed.\");\n      return false;\n    }\n\n    MapJoinOperator mapJoinOp = convertJoinMapJoin(joinOp, context, bigTablePosition);\n    MapJoinDesc joinDesc = mapJoinOp.getConf();\n    joinDesc.setBucketMapJoin(true);\n\n    // we can set the traits for this join operator\n    OpTraits opTraits = new OpTraits(joinOp.getOpTraits().getBucketColNames(),\n            tezBucketJoinProcCtx.getNumBuckets(), null);\n    mapJoinOp.setOpTraits(opTraits);\n    mapJoinOp.setStatistics(joinOp.getStatistics());\n    setNumberOfBucketsOnChildren(mapJoinOp);\n\n    // Once the conversion is done, we can set the partitioner to bucket cols on the small table\n    Map<String, Integer> bigTableBucketNumMapping = new HashMap<String, Integer>();\n    bigTableBucketNumMapping.put(joinDesc.getBigTableAlias(), tezBucketJoinProcCtx.getNumBuckets());\n    joinDesc.setBigTableBucketNumMapping(bigTableBucketNumMapping);\n\n    return true;\n  }",
                "code_after_change": "  private boolean convertJoinBucketMapJoin(JoinOperator joinOp, OptimizeTezProcContext context,\n      int bigTablePosition, TezBucketJoinProcCtx tezBucketJoinProcCtx) throws SemanticException {\n\n    if (!checkConvertJoinBucketMapJoin(joinOp, context, bigTablePosition, tezBucketJoinProcCtx)) {\n      LOG.info(\"Check conversion to bucket map join failed.\");\n      return false;\n    }\n\n    MapJoinOperator mapJoinOp = convertJoinMapJoin(joinOp, context, bigTablePosition, true);\n    MapJoinDesc joinDesc = mapJoinOp.getConf();\n    joinDesc.setBucketMapJoin(true);\n\n    // we can set the traits for this join operator\n    OpTraits opTraits = new OpTraits(joinOp.getOpTraits().getBucketColNames(),\n            tezBucketJoinProcCtx.getNumBuckets(), null);\n    mapJoinOp.setOpTraits(opTraits);\n    mapJoinOp.setStatistics(joinOp.getStatistics());\n    setNumberOfBucketsOnChildren(mapJoinOp);\n\n    // Once the conversion is done, we can set the partitioner to bucket cols on the small table\n    Map<String, Integer> bigTableBucketNumMapping = new HashMap<String, Integer>();\n    bigTableBucketNumMapping.put(joinDesc.getBigTableAlias(), tezBucketJoinProcCtx.getNumBuckets());\n    joinDesc.setBigTableBucketNumMapping(bigTableBucketNumMapping);\n\n    return true;\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.parse.GenTezUtils.removeBranch": {
                "code_before_change": "  public static void removeBranch(AppMasterEventOperator event) {\n    Operator<?> child = event;\n    Operator<?> curr = event;\n\n    while (curr.getChildOperators().size() <= 1) {\n      child = curr;\n      curr = curr.getParentOperators().get(0);\n    }\n\n    curr.removeChild(child);\n  }",
                "code_after_change": "  public static void removeBranch(AppMasterEventOperator event) {\n    Operator<?> child = event;\n    Operator<?> curr = event;\n\n    while (curr.getChildOperators().size() <= 1) {\n      child = curr;\n      curr = curr.getParentOperators().get(0);\n    }\n\n    curr.removeChild(child);\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.optimizer.ReduceSinkMapJoinProc.process": {
                "code_before_change": "  public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procContext, Object... nodeOutputs)\n      throws SemanticException {\n    GenTezProcContext context = (GenTezProcContext) procContext;\n    MapJoinOperator mapJoinOp = (MapJoinOperator)nd;\n\n    if (stack.size() < 2 || !(stack.get(stack.size() - 2) instanceof ReduceSinkOperator)) {\n      context.currentMapJoinOperators.add(mapJoinOp);\n      return null;\n    }\n\n    context.preceedingWork = null;\n    context.currentRootOperator = null;\n\n    ReduceSinkOperator parentRS = (ReduceSinkOperator)stack.get(stack.size() - 2);\n    // remove the tag for in-memory side of mapjoin\n    parentRS.getConf().setSkipTag(true);\n    parentRS.setSkipTag(true);\n    // remember the original parent list before we start modifying it.\n    if (!context.mapJoinParentMap.containsKey(mapJoinOp)) {\n      List<Operator<?>> parents = new ArrayList<Operator<?>>(mapJoinOp.getParentOperators());\n      context.mapJoinParentMap.put(mapJoinOp, parents);\n    }\n\n    List<BaseWork> mapJoinWork = null;\n\n    /*\n     *  if there was a pre-existing work generated for the big-table mapjoin side,\n     *  we need to hook the work generated for the RS (associated with the RS-MJ pattern)\n     *  with the pre-existing work.\n     *\n     *  Otherwise, we need to associate that the mapjoin op\n     *  to be linked to the RS work (associated with the RS-MJ pattern).\n     *\n     */\n    mapJoinWork = context.mapJoinWorkMap.get(mapJoinOp);\n    BaseWork parentWork;\n    if (context.unionWorkMap.containsKey(parentRS)) {\n      parentWork = context.unionWorkMap.get(parentRS);\n    } else {\n      assert context.childToWorkMap.get(parentRS).size() == 1;\n      parentWork = context.childToWorkMap.get(parentRS).get(0);\n    }\n\n    // set the link between mapjoin and parent vertex\n    int pos = context.mapJoinParentMap.get(mapJoinOp).indexOf(parentRS);\n    if (pos == -1) {\n      throw new SemanticException(\"Cannot find position of parent in mapjoin\");\n    }\n    MapJoinDesc joinConf = mapJoinOp.getConf();\n    long keyCount = Long.MAX_VALUE, rowCount = Long.MAX_VALUE, bucketCount = 1;\n    long tableSize = Long.MAX_VALUE;\n    Statistics stats = parentRS.getStatistics();\n    if (stats != null) {\n      keyCount = rowCount = stats.getNumRows();\n      if (keyCount <= 0) {\n        keyCount = rowCount = Long.MAX_VALUE;\n      }\n      tableSize = stats.getDataSize();\n      ArrayList<String> keyCols = parentRS.getConf().getOutputKeyColumnNames();\n      if (keyCols != null && !keyCols.isEmpty()) {\n        // See if we can arrive at a smaller number using distinct stats from key columns.\n        long maxKeyCount = 1;\n        String prefix = Utilities.ReduceField.KEY.toString();\n        for (String keyCol : keyCols) {\n          ExprNodeDesc realCol = parentRS.getColumnExprMap().get(prefix + \".\" + keyCol);\n          ColStatistics cs =\n              StatsUtils.getColStatisticsFromExpression(context.conf, stats, realCol);\n          if (cs == null || cs.getCountDistint() <= 0) {\n            maxKeyCount = Long.MAX_VALUE;\n            break;\n          }\n          maxKeyCount *= cs.getCountDistint();\n          if (maxKeyCount >= keyCount) {\n            break;\n          }\n        }\n        keyCount = Math.min(maxKeyCount, keyCount);\n      }\n      if (joinConf.isBucketMapJoin()) {\n        OpTraits opTraits = mapJoinOp.getOpTraits();\n        bucketCount = (opTraits == null) ? -1 : opTraits.getNumBuckets();\n        if (bucketCount > 0) {\n          // We cannot obtain a better estimate without CustomPartitionVertex providing it\n          // to us somehow; in which case using statistics would be completely unnecessary.\n          keyCount /= bucketCount;\n          tableSize /= bucketCount;\n        }\n      }\n    }\n    LOG.info(\"Mapjoin \" + mapJoinOp + \", pos: \" + pos + \" --> \" + parentWork.getName() + \" (\"\n      + keyCount + \" keys estimated from \" + rowCount + \" rows, \" + bucketCount + \" buckets)\");\n    joinConf.getParentToInput().put(pos, parentWork.getName());\n    if (keyCount != Long.MAX_VALUE) {\n      joinConf.getParentKeyCounts().put(pos, keyCount);\n    }\n    joinConf.getParentDataSizes().put(pos, tableSize);\n\n    int numBuckets = -1;\n    EdgeType edgeType = EdgeType.BROADCAST_EDGE;\n    if (joinConf.isBucketMapJoin()) {\n\n      // disable auto parallelism for bucket map joins\n      parentRS.getConf().setReducerTraits(EnumSet.of(FIXED));\n\n      numBuckets = (Integer) joinConf.getBigTableBucketNumMapping().values().toArray()[0];\n      /*\n       * Here, we can be in one of 4 states.\n       *\n       * 1. If map join work is null implies that we have not yet traversed the big table side. We\n       * just need to see if we can find a reduce sink operator in the big table side. This would\n       * imply a reduce side operation.\n       *\n       * 2. If we don't find a reducesink in 1 it has to be the case that it is a map side operation.\n       *\n       * 3. If we have already created a work item for the big table side, we need to see if we can\n       * find a table scan operator in the big table side. This would imply a map side operation.\n       *\n       * 4. If we don't find a table scan operator, it has to be a reduce side operation.\n       */\n      if (mapJoinWork == null) {\n        Operator<?> rootOp =\n          OperatorUtils.findSingleOperatorUpstream(\n              mapJoinOp.getParentOperators().get(joinConf.getPosBigTable()),\n              ReduceSinkOperator.class);\n        if (rootOp == null) {\n          // likely we found a table scan operator\n          edgeType = EdgeType.CUSTOM_EDGE;\n        } else {\n          // we have found a reduce sink\n          edgeType = EdgeType.CUSTOM_SIMPLE_EDGE;\n        }\n      } else {\n        Operator<?> rootOp =\n            OperatorUtils.findSingleOperatorUpstream(\n                mapJoinOp.getParentOperators().get(joinConf.getPosBigTable()),\n                TableScanOperator.class);\n        if (rootOp != null) {\n          // likely we found a table scan operator\n          edgeType = EdgeType.CUSTOM_EDGE;\n        } else {\n          // we have found a reduce sink\n          edgeType = EdgeType.CUSTOM_SIMPLE_EDGE;\n        }\n      }\n    }\n    TezEdgeProperty edgeProp = new TezEdgeProperty(null, edgeType, numBuckets);\n\n    if (mapJoinWork != null) {\n      for (BaseWork myWork: mapJoinWork) {\n        // link the work with the work associated with the reduce sink that triggered this rule\n        TezWork tezWork = context.currentTask.getWork();\n        LOG.debug(\"connecting \"+parentWork.getName()+\" with \"+myWork.getName());\n        tezWork.connect(parentWork, myWork, edgeProp);\n        if (edgeType == EdgeType.CUSTOM_EDGE) {\n          tezWork.setVertexType(myWork, VertexType.INITIALIZED_EDGES);\n        }\n\n        ReduceSinkOperator r = null;\n        if (parentRS.getConf().getOutputName() != null) {\n          LOG.debug(\"Cloning reduce sink for multi-child broadcast edge\");\n          // we've already set this one up. Need to clone for the next work.\n          r = (ReduceSinkOperator) OperatorFactory.getAndMakeChild(\n              (ReduceSinkDesc) parentRS.getConf().clone(),\n              new RowSchema(parentRS.getSchema()),\n              parentRS.getParentOperators());\n          context.clonedReduceSinks.add(r);\n        } else {\n          r = parentRS;\n        }\n        // remember the output name of the reduce sink\n        r.getConf().setOutputName(myWork.getName());\n        context.connectedReduceSinks.add(r);\n      }\n    }\n\n    // remember in case we need to connect additional work later\n    Map<BaseWork, TezEdgeProperty> linkWorkMap = null;\n    if (context.linkOpWithWorkMap.containsKey(mapJoinOp)) {\n      linkWorkMap = context.linkOpWithWorkMap.get(mapJoinOp);\n    } else {\n      linkWorkMap = new HashMap<BaseWork, TezEdgeProperty>();\n    }\n    linkWorkMap.put(parentWork, edgeProp);\n    context.linkOpWithWorkMap.put(mapJoinOp, linkWorkMap);\n\n    List<ReduceSinkOperator> reduceSinks\n      = context.linkWorkWithReduceSinkMap.get(parentWork);\n    if (reduceSinks == null) {\n      reduceSinks = new ArrayList<ReduceSinkOperator>();\n    }\n    reduceSinks.add(parentRS);\n    context.linkWorkWithReduceSinkMap.put(parentWork, reduceSinks);\n\n    // create the dummy operators\n    List<Operator<?>> dummyOperators = new ArrayList<Operator<?>>();\n\n    // create an new operator: HashTableDummyOperator, which share the table desc\n    HashTableDummyDesc desc = new HashTableDummyDesc();\n    @SuppressWarnings(\"unchecked\")\n    HashTableDummyOperator dummyOp = (HashTableDummyOperator) OperatorFactory.get(desc);\n    TableDesc tbl;\n\n    // need to create the correct table descriptor for key/value\n    RowSchema rowSchema = parentRS.getParentOperators().get(0).getSchema();\n    tbl = PlanUtils.getReduceValueTableDesc(PlanUtils.getFieldSchemasFromRowSchema(rowSchema, \"\"));\n    dummyOp.getConf().setTbl(tbl);\n\n    Map<Byte, List<ExprNodeDesc>> keyExprMap = mapJoinOp.getConf().getKeys();\n    List<ExprNodeDesc> keyCols = keyExprMap.get(Byte.valueOf((byte) 0));\n    StringBuilder keyOrder = new StringBuilder();\n    for (ExprNodeDesc k: keyCols) {\n      keyOrder.append(\"+\");\n    }\n    TableDesc keyTableDesc = PlanUtils.getReduceKeyTableDesc(PlanUtils\n        .getFieldSchemasFromColumnList(keyCols, \"mapjoinkey\"), keyOrder.toString());\n    mapJoinOp.getConf().setKeyTableDesc(keyTableDesc);\n\n    // let the dummy op be the parent of mapjoin op\n    mapJoinOp.replaceParent(parentRS, dummyOp);\n    List<Operator<? extends OperatorDesc>> dummyChildren =\n      new ArrayList<Operator<? extends OperatorDesc>>();\n    dummyChildren.add(mapJoinOp);\n    dummyOp.setChildOperators(dummyChildren);\n    dummyOperators.add(dummyOp);\n\n    // cut the operator tree so as to not retain connections from the parent RS downstream\n    List<Operator<? extends OperatorDesc>> childOperators = parentRS.getChildOperators();\n    int childIndex = childOperators.indexOf(mapJoinOp);\n    childOperators.remove(childIndex);\n\n    // the \"work\" needs to know about the dummy operators. They have to be separately initialized\n    // at task startup\n    if (mapJoinWork != null) {\n      for (BaseWork myWork: mapJoinWork) {\n        myWork.addDummyOp(dummyOp);\n      }\n    }\n    if (context.linkChildOpWithDummyOp.containsKey(mapJoinOp)) {\n      for (Operator<?> op: context.linkChildOpWithDummyOp.get(mapJoinOp)) {\n        dummyOperators.add(op);\n      }\n    }\n    context.linkChildOpWithDummyOp.put(mapJoinOp, dummyOperators);\n\n    return true;\n  }",
                "code_after_change": "  public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procContext, Object... nodeOutputs)\n      throws SemanticException {\n    GenTezProcContext context = (GenTezProcContext) procContext;\n    MapJoinOperator mapJoinOp = (MapJoinOperator)nd;\n\n    // remember the original parent list before we start modifying it.\n    if (!context.mapJoinParentMap.containsKey(mapJoinOp)) {\n      List<Operator<?>> parents = new ArrayList<Operator<?>>(mapJoinOp.getParentOperators());\n      context.mapJoinParentMap.put(mapJoinOp, parents);\n    }\n\n    boolean isBigTable = stack.size() < 2\n        || !(stack.get(stack.size() - 2) instanceof ReduceSinkOperator);\n\n    ReduceSinkOperator parentRS = null;\n    if (!isBigTable) {\n      parentRS = (ReduceSinkOperator)stack.get(stack.size() - 2);\n\n      // For dynamic partitioned hash join, the big table will also be coming from a ReduceSinkOperator\n      // Check for this condition.\n      // TODO: use indexOf(), or parentRS.getTag()?\n      isBigTable =\n          (mapJoinOp.getParentOperators().indexOf(parentRS) == mapJoinOp.getConf().getPosBigTable());\n    }\n\n    if (mapJoinOp.getConf().isDynamicPartitionHashJoin() &&\n        !context.mapJoinToUnprocessedSmallTableReduceSinks.containsKey(mapJoinOp)) {\n      // Initialize set of unprocessed small tables\n      Set<ReduceSinkOperator> rsSet = Sets.newIdentityHashSet();\n      for (int pos = 0; pos < mapJoinOp.getParentOperators().size(); ++pos) {\n        if (pos == mapJoinOp.getConf().getPosBigTable()) {\n          continue;\n        }\n        rsSet.add((ReduceSinkOperator) mapJoinOp.getParentOperators().get(pos));\n      }\n      context.mapJoinToUnprocessedSmallTableReduceSinks.put(mapJoinOp, rsSet);\n    }\n\n    if (isBigTable) {\n      context.currentMapJoinOperators.add(mapJoinOp);\n      return null;\n    }\n\n    context.preceedingWork = null;\n    context.currentRootOperator = null;\n\n    return processReduceSinkToHashJoin(parentRS, mapJoinOp, context);\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.plan.ReduceWork.getValueObjectInspector": {
                "code_before_change": "  public ObjectInspector getValueObjectInspector() {\n    if (needsTagging) {\n      return null;\n    }\n    if (valueObjectInspector == null) {\n      valueObjectInspector = getObjectInspector(tagToValueDesc.get(0));\n    }\n    return valueObjectInspector;\n  }",
                "code_after_change": "  public ObjectInspector getValueObjectInspector() {\n    if (needsTagging) {\n      return null;\n    }\n    if (valueObjectInspector == null) {\n      valueObjectInspector = getObjectInspector(tagToValueDesc.get(tag));\n    }\n    return valueObjectInspector;\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin.checkAndConvertSMBJoin": {
                "code_before_change": "  private Object checkAndConvertSMBJoin(OptimizeTezProcContext context, JoinOperator joinOp,\n      TezBucketJoinProcCtx tezBucketJoinProcCtx) throws SemanticException {\n    // we cannot convert to bucket map join, we cannot convert to\n    // map join either based on the size. Check if we can convert to SMB join.\n    if (context.conf.getBoolVar(HiveConf.ConfVars.HIVE_AUTO_SORTMERGE_JOIN) == false) {\n      convertJoinSMBJoin(joinOp, context, 0, 0, false);\n      return null;\n    }\n    Class<? extends BigTableSelectorForAutoSMJ> bigTableMatcherClass = null;\n    try {\n      String selector = HiveConf.getVar(context.parseContext.getConf(),\n          HiveConf.ConfVars.HIVE_AUTO_SORTMERGE_JOIN_BIGTABLE_SELECTOR);\n      bigTableMatcherClass =\n          JavaUtils.loadClass(selector);\n    } catch (ClassNotFoundException e) {\n      throw new SemanticException(e.getMessage());\n    }\n\n    BigTableSelectorForAutoSMJ bigTableMatcher =\n        ReflectionUtils.newInstance(bigTableMatcherClass, null);\n    JoinDesc joinDesc = joinOp.getConf();\n    JoinCondDesc[] joinCondns = joinDesc.getConds();\n    Set<Integer> joinCandidates = MapJoinProcessor.getBigTableCandidates(joinCondns);\n    if (joinCandidates.isEmpty()) {\n      // This is a full outer join. This can never be a map-join\n      // of any type. So return false.\n      return false;\n    }\n    int mapJoinConversionPos =\n        bigTableMatcher.getBigTablePosition(context.parseContext, joinOp, joinCandidates);\n    if (mapJoinConversionPos < 0) {\n      // contains aliases from sub-query\n      // we are just converting to a common merge join operator. The shuffle\n      // join in map-reduce case.\n      int pos = 0; // it doesn't matter which position we use in this case.\n      convertJoinSMBJoin(joinOp, context, pos, 0, false);\n      return null;\n    }\n\n    if (checkConvertJoinSMBJoin(joinOp, context, mapJoinConversionPos, tezBucketJoinProcCtx)) {\n      convertJoinSMBJoin(joinOp, context, mapJoinConversionPos,\n          tezBucketJoinProcCtx.getNumBuckets(), true);\n    } else {\n      // we are just converting to a common merge join operator. The shuffle\n      // join in map-reduce case.\n      int pos = 0; // it doesn't matter which position we use in this case.\n      convertJoinSMBJoin(joinOp, context, pos, 0, false);\n    }\n    return null;\n  }",
                "code_after_change": "  private Object checkAndConvertSMBJoin(OptimizeTezProcContext context, JoinOperator joinOp,\n      TezBucketJoinProcCtx tezBucketJoinProcCtx) throws SemanticException {\n    // we cannot convert to bucket map join, we cannot convert to\n    // map join either based on the size. Check if we can convert to SMB join.\n    if (context.conf.getBoolVar(HiveConf.ConfVars.HIVE_AUTO_SORTMERGE_JOIN) == false) {\n      fallbackToReduceSideJoin(joinOp, context);\n      return null;\n    }\n    Class<? extends BigTableSelectorForAutoSMJ> bigTableMatcherClass = null;\n    try {\n      String selector = HiveConf.getVar(context.parseContext.getConf(),\n          HiveConf.ConfVars.HIVE_AUTO_SORTMERGE_JOIN_BIGTABLE_SELECTOR);\n      bigTableMatcherClass =\n          JavaUtils.loadClass(selector);\n    } catch (ClassNotFoundException e) {\n      throw new SemanticException(e.getMessage());\n    }\n\n    BigTableSelectorForAutoSMJ bigTableMatcher =\n        ReflectionUtils.newInstance(bigTableMatcherClass, null);\n    JoinDesc joinDesc = joinOp.getConf();\n    JoinCondDesc[] joinCondns = joinDesc.getConds();\n    Set<Integer> joinCandidates = MapJoinProcessor.getBigTableCandidates(joinCondns);\n    if (joinCandidates.isEmpty()) {\n      // This is a full outer join. This can never be a map-join\n      // of any type. So return false.\n      return false;\n    }\n    int mapJoinConversionPos =\n        bigTableMatcher.getBigTablePosition(context.parseContext, joinOp, joinCandidates);\n    if (mapJoinConversionPos < 0) {\n      // contains aliases from sub-query\n      // we are just converting to a common merge join operator. The shuffle\n      // join in map-reduce case.\n      fallbackToReduceSideJoin(joinOp, context);\n      return null;\n    }\n\n    if (checkConvertJoinSMBJoin(joinOp, context, mapJoinConversionPos, tezBucketJoinProcCtx)) {\n      convertJoinSMBJoin(joinOp, context, mapJoinConversionPos,\n          tezBucketJoinProcCtx.getNumBuckets(), true);\n    } else {\n      // we are just converting to a common merge join operator. The shuffle\n      // join in map-reduce case.\n      fallbackToReduceSideJoin(joinOp, context);\n    }\n    return null;\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.parse.GenTezWork.process": {
                "code_before_change": "  public Object process(Node nd, Stack<Node> stack,\n      NodeProcessorCtx procContext, Object... nodeOutputs)\n      throws SemanticException {\n\n    GenTezProcContext context = (GenTezProcContext) procContext;\n\n    assert context != null && context.currentTask != null\n        && context.currentRootOperator != null;\n\n    // Operator is a file sink or reduce sink. Something that forces\n    // a new vertex.\n    Operator<?> operator = (Operator<?>) nd;\n\n    // root is the start of the operator pipeline we're currently\n    // packing into a vertex, typically a table scan, union or join\n    Operator<?> root = context.currentRootOperator;\n\n    LOG.debug(\"Root operator: \" + root);\n    LOG.debug(\"Leaf operator: \" + operator);\n\n    if (context.clonedReduceSinks.contains(operator)) {\n      // if we're visiting a terminal we've created ourselves,\n      // just skip and keep going\n      return null;\n    }\n\n    TezWork tezWork = context.currentTask.getWork();\n\n    // Right now the work graph is pretty simple. If there is no\n    // Preceding work we have a root and will generate a map\n    // vertex. If there is a preceding work we will generate\n    // a reduce vertex\n    BaseWork work;\n    if (context.rootToWorkMap.containsKey(root)) {\n      // having seen the root operator before means there was a branch in the\n      // operator graph. There's typically two reasons for that: a) mux/demux\n      // b) multi insert. Mux/Demux will hit the same leaf again, multi insert\n      // will result into a vertex with multiple FS or RS operators.\n      if (context.childToWorkMap.containsKey(operator)) {\n        // if we've seen both root and child, we can bail.\n\n        // clear out the mapjoin set. we don't need it anymore.\n        context.currentMapJoinOperators.clear();\n\n        // clear out the union set. we don't need it anymore.\n        context.currentUnionOperators.clear();\n\n        return null;\n      } else {\n        // At this point we don't have to do anything special. Just\n        // run through the regular paces w/o creating a new task.\n        work = context.rootToWorkMap.get(root);\n      }\n    } else {\n      // create a new vertex\n      if (context.preceedingWork == null) {\n        work = utils.createMapWork(context, root, tezWork, null);\n      } else {\n        work = GenTezUtils.createReduceWork(context, root, tezWork);\n      }\n      context.rootToWorkMap.put(root, work);\n    }\n\n    // this is where we set the sort columns that we will be using for KeyValueInputMerge\n    if (operator instanceof DummyStoreOperator) {\n      work.addSortCols(root.getOpTraits().getSortCols().get(0));\n    }\n\n    if (!context.childToWorkMap.containsKey(operator)) {\n      List<BaseWork> workItems = new LinkedList<BaseWork>();\n      workItems.add(work);\n      context.childToWorkMap.put(operator, workItems);\n    } else {\n      context.childToWorkMap.get(operator).add(work);\n    }\n\n    // this transformation needs to be first because it changes the work item itself.\n    // which can affect the working of all downstream transformations.\n    if (context.currentMergeJoinOperator != null) {\n      // we are currently walking the big table side of the merge join. we need to create or hook up\n      // merge join work.\n      MergeJoinWork mergeJoinWork = null;\n      if (context.opMergeJoinWorkMap.containsKey(context.currentMergeJoinOperator)) {\n        // we have found a merge work corresponding to this closing operator. Hook up this work.\n        mergeJoinWork = context.opMergeJoinWorkMap.get(context.currentMergeJoinOperator);\n      } else {\n        // we need to create the merge join work\n        mergeJoinWork = new MergeJoinWork();\n        mergeJoinWork.setMergeJoinOperator(context.currentMergeJoinOperator);\n        tezWork.add(mergeJoinWork);\n        context.opMergeJoinWorkMap.put(context.currentMergeJoinOperator, mergeJoinWork);\n      }\n      // connect the work correctly.\n      work.addSortCols(root.getOpTraits().getSortCols().get(0));\n      mergeJoinWork.addMergedWork(work, null, context.leafOperatorToFollowingWork);\n      Operator<? extends OperatorDesc> parentOp =\n          getParentFromStack(context.currentMergeJoinOperator, stack);\n      int pos = context.currentMergeJoinOperator.getTagForOperator(parentOp);\n      work.setTag(pos);\n      tezWork.setVertexType(work, VertexType.MULTI_INPUT_UNINITIALIZED_EDGES);\n      for (BaseWork parentWork : tezWork.getParents(work)) {\n        TezEdgeProperty edgeProp = tezWork.getEdgeProperty(parentWork, work);\n        tezWork.disconnect(parentWork, work);\n        tezWork.connect(parentWork, mergeJoinWork, edgeProp);\n      }\n\n      for (BaseWork childWork : tezWork.getChildren(work)) {\n        TezEdgeProperty edgeProp = tezWork.getEdgeProperty(work, childWork);\n        tezWork.disconnect(work, childWork);\n        tezWork.connect(mergeJoinWork, childWork, edgeProp);\n      }\n      tezWork.remove(work);\n      context.rootToWorkMap.put(root, mergeJoinWork);\n      context.childToWorkMap.get(operator).remove(work);\n      context.childToWorkMap.get(operator).add(mergeJoinWork);\n      work = mergeJoinWork;\n      context.currentMergeJoinOperator = null;\n    }\n\n    // remember which mapjoin operator links with which work\n    if (!context.currentMapJoinOperators.isEmpty()) {\n      for (MapJoinOperator mj: context.currentMapJoinOperators) {\n        LOG.debug(\"Processing map join: \" + mj);\n        // remember the mapping in case we scan another branch of the\n        // mapjoin later\n        if (!context.mapJoinWorkMap.containsKey(mj)) {\n          List<BaseWork> workItems = new LinkedList<BaseWork>();\n          workItems.add(work);\n          context.mapJoinWorkMap.put(mj, workItems);\n        } else {\n          context.mapJoinWorkMap.get(mj).add(work);\n        }\n\n        /*\n         * this happens in case of map join operations.\n         * The tree looks like this:\n         *\n         *        RS <--- we are here perhaps\n         *        |\n         *     MapJoin\n         *     /     \\\n         *   RS       TS\n         *  /\n         * TS\n         *\n         * If we are at the RS pointed above, and we may have already visited the\n         * RS following the TS, we have already generated work for the TS-RS.\n         * We need to hook the current work to this generated work.\n         */\n        if (context.linkOpWithWorkMap.containsKey(mj)) {\n          Map<BaseWork,TezEdgeProperty> linkWorkMap = context.linkOpWithWorkMap.get(mj);\n          if (linkWorkMap != null) {\n            if (context.linkChildOpWithDummyOp.containsKey(mj)) {\n              for (Operator<?> dummy: context.linkChildOpWithDummyOp.get(mj)) {\n                work.addDummyOp((HashTableDummyOperator) dummy);\n              }\n            }\n            for (Entry<BaseWork,TezEdgeProperty> parentWorkMap : linkWorkMap.entrySet()) {\n              BaseWork parentWork = parentWorkMap.getKey();\n              LOG.debug(\"connecting \"+parentWork.getName()+\" with \"+work.getName());\n              TezEdgeProperty edgeProp = parentWorkMap.getValue();\n              tezWork.connect(parentWork, work, edgeProp);\n              if (edgeProp.getEdgeType() == EdgeType.CUSTOM_EDGE) {\n                tezWork.setVertexType(work, VertexType.INITIALIZED_EDGES);\n              }\n\n              // need to set up output name for reduce sink now that we know the name\n              // of the downstream work\n              for (ReduceSinkOperator r:\n                     context.linkWorkWithReduceSinkMap.get(parentWork)) {\n                if (r.getConf().getOutputName() != null) {\n                  LOG.debug(\"Cloning reduce sink for multi-child broadcast edge\");\n                  // we've already set this one up. Need to clone for the next work.\n                  r = (ReduceSinkOperator) OperatorFactory.getAndMakeChild(\n                      (ReduceSinkDesc)r.getConf().clone(),\n                      new RowSchema(r.getSchema()),\n                      r.getParentOperators());\n                  context.clonedReduceSinks.add(r);\n                }\n                r.getConf().setOutputName(work.getName());\n                context.connectedReduceSinks.add(r);\n              }\n            }\n          }\n        }\n      }\n      // clear out the set. we don't need it anymore.\n      context.currentMapJoinOperators.clear();\n    }\n\n    // This is where we cut the tree as described above. We also remember that\n    // we might have to connect parent work with this work later.\n    for (Operator<?> parent : new ArrayList<Operator<?>>(root.getParentOperators())) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Removing \" + parent + \" as parent from \" + root);\n      }\n      context.leafOperatorToFollowingWork.remove(parent);\n      context.leafOperatorToFollowingWork.put(parent, work);\n      root.removeParent(parent);\n    }\n\n    if (!context.currentUnionOperators.isEmpty()) {\n      // if there are union all operators, it means that the walking context contains union all operators.\n      // please see more details of context.currentUnionOperator in GenTezWorkWalker\n\n      UnionWork unionWork;\n      if (context.unionWorkMap.containsKey(operator)) {\n        // we've seen this terminal before and have created a union work object.\n        // just need to add this work to it. There will be no children of this one\n        // since we've passed this operator before.\n        assert operator.getChildOperators().isEmpty();\n        unionWork = (UnionWork) context.unionWorkMap.get(operator);\n        // finally connect the union work with work\n        connectUnionWorkWithWork(unionWork, work, tezWork, context);\n      } else {\n        // we've not seen this terminal before. we need to check\n        // rootUnionWorkMap which contains the information of mapping the root\n        // operator of a union work to a union work\n        unionWork = context.rootUnionWorkMap.get(root);\n        if (unionWork == null) {\n          // if unionWork is null, it means it is the first time. we need to\n          // create a union work object and add this work to it. Subsequent \n          // work should reference the union and not the actual work.\n          unionWork = GenTezUtils.createUnionWork(context, root, operator, tezWork);\n          // finally connect the union work with work\n          connectUnionWorkWithWork(unionWork, work, tezWork, context);\n        }\n      }\n      context.currentUnionOperators.clear();\n      work = unionWork;\n\n    }\n\n    // We're scanning a tree from roots to leaf (this is not technically\n    // correct, demux and mux operators might form a diamond shape, but\n    // we will only scan one path and ignore the others, because the\n    // diamond shape is always contained in a single vertex). The scan\n    // is depth first and because we remove parents when we pack a pipeline\n    // into a vertex we will never visit any node twice. But because of that\n    // we might have a situation where we need to connect 'work' that comes after\n    // the 'work' we're currently looking at.\n    //\n    // Also note: the concept of leaf and root is reversed in hive for historical\n    // reasons. Roots are data sources, leaves are data sinks. I know.\n    if (context.leafOperatorToFollowingWork.containsKey(operator)) {\n\n      BaseWork followingWork = context.leafOperatorToFollowingWork.get(operator);\n      long bytesPerReducer = context.conf.getLongVar(HiveConf.ConfVars.BYTESPERREDUCER);\n\n      LOG.debug(\"Second pass. Leaf operator: \"+operator\n        +\" has common downstream work:\"+followingWork);\n\n      if (operator instanceof DummyStoreOperator) {\n        // this is the small table side.\n        assert (followingWork instanceof MergeJoinWork);\n        MergeJoinWork mergeJoinWork = (MergeJoinWork) followingWork;\n        CommonMergeJoinOperator mergeJoinOp = mergeJoinWork.getMergeJoinOperator();\n        work.setTag(mergeJoinOp.getTagForOperator(operator));\n        mergeJoinWork.addMergedWork(null, work, context.leafOperatorToFollowingWork);\n        tezWork.setVertexType(mergeJoinWork, VertexType.MULTI_INPUT_UNINITIALIZED_EDGES);\n        for (BaseWork parentWork : tezWork.getParents(work)) {\n          TezEdgeProperty edgeProp = tezWork.getEdgeProperty(parentWork, work);\n          tezWork.disconnect(parentWork, work);\n          tezWork.connect(parentWork, mergeJoinWork, edgeProp);\n        }\n        work = mergeJoinWork;\n      } else {\n        // need to add this branch to the key + value info\n        assert operator instanceof ReduceSinkOperator\n            && ((followingWork instanceof ReduceWork) || (followingWork instanceof MergeJoinWork)\n                || followingWork instanceof UnionWork);\n        ReduceSinkOperator rs = (ReduceSinkOperator) operator;\n        ReduceWork rWork = null;\n        if (followingWork instanceof MergeJoinWork) {\n          MergeJoinWork mergeJoinWork = (MergeJoinWork) followingWork;\n          rWork = (ReduceWork) mergeJoinWork.getMainWork();\n        } else if (followingWork instanceof UnionWork) {\n          // this can only be possible if there is merge work followed by the union\n          UnionWork unionWork = (UnionWork) followingWork;\n          int index = getFollowingWorkIndex(tezWork, unionWork, rs);\n          BaseWork baseWork = tezWork.getChildren(unionWork).get(index);\n          if (baseWork instanceof MergeJoinWork) {\n            MergeJoinWork mergeJoinWork = (MergeJoinWork) baseWork;\n            // disconnect the connection to union work and connect to merge work\n            followingWork = mergeJoinWork;\n            rWork = (ReduceWork) mergeJoinWork.getMainWork();\n          } else {\n            rWork = (ReduceWork) baseWork;\n          }\n        } else {\n          rWork = (ReduceWork) followingWork;\n        }\n        GenMapRedUtils.setKeyAndValueDesc(rWork, rs);\n\n        // remember which parent belongs to which tag\n        int tag = rs.getConf().getTag();\n        rWork.getTagToInput().put(tag == -1 ? 0 : tag, work.getName());\n\n        // remember the output name of the reduce sink\n        rs.getConf().setOutputName(rWork.getName());\n\n        if (!context.connectedReduceSinks.contains(rs)) {\n          // add dependency between the two work items\n          TezEdgeProperty edgeProp;\n          if (rWork.isAutoReduceParallelism()) {\n            edgeProp =\n                new TezEdgeProperty(context.conf, EdgeType.SIMPLE_EDGE, true,\n                    rWork.getMinReduceTasks(), rWork.getMaxReduceTasks(), bytesPerReducer);\n          } else {\n            edgeProp = new TezEdgeProperty(EdgeType.SIMPLE_EDGE);\n          }\n          tezWork.connect(work, followingWork, edgeProp);\n          context.connectedReduceSinks.add(rs);\n        }\n      }\n    } else {\n      LOG.debug(\"First pass. Leaf operator: \"+operator);\n    }\n\n    // No children means we're at the bottom. If there are more operators to scan\n    // the next item will be a new root.\n    if (!operator.getChildOperators().isEmpty()) {\n      assert operator.getChildOperators().size() == 1;\n      context.parentOfRoot = operator;\n      context.currentRootOperator = operator.getChildOperators().get(0);\n      context.preceedingWork = work;\n    }\n\n    return null;\n  }",
                "code_after_change": "  public Object process(Node nd, Stack<Node> stack,\n      NodeProcessorCtx procContext, Object... nodeOutputs)\n      throws SemanticException {\n\n    GenTezProcContext context = (GenTezProcContext) procContext;\n\n    assert context != null && context.currentTask != null\n        && context.currentRootOperator != null;\n\n    // Operator is a file sink or reduce sink. Something that forces\n    // a new vertex.\n    Operator<?> operator = (Operator<?>) nd;\n\n    // root is the start of the operator pipeline we're currently\n    // packing into a vertex, typically a table scan, union or join\n    Operator<?> root = context.currentRootOperator;\n\n    LOG.debug(\"Root operator: \" + root);\n    LOG.debug(\"Leaf operator: \" + operator);\n\n    if (context.clonedReduceSinks.contains(operator)) {\n      // if we're visiting a terminal we've created ourselves,\n      // just skip and keep going\n      return null;\n    }\n\n    TezWork tezWork = context.currentTask.getWork();\n\n    // Right now the work graph is pretty simple. If there is no\n    // Preceding work we have a root and will generate a map\n    // vertex. If there is a preceding work we will generate\n    // a reduce vertex\n    BaseWork work;\n    if (context.rootToWorkMap.containsKey(root)) {\n      // having seen the root operator before means there was a branch in the\n      // operator graph. There's typically two reasons for that: a) mux/demux\n      // b) multi insert. Mux/Demux will hit the same leaf again, multi insert\n      // will result into a vertex with multiple FS or RS operators.\n      if (context.childToWorkMap.containsKey(operator)) {\n        // if we've seen both root and child, we can bail.\n\n        // clear out the mapjoin set. we don't need it anymore.\n        context.currentMapJoinOperators.clear();\n\n        // clear out the union set. we don't need it anymore.\n        context.currentUnionOperators.clear();\n\n        return null;\n      } else {\n        // At this point we don't have to do anything special. Just\n        // run through the regular paces w/o creating a new task.\n        work = context.rootToWorkMap.get(root);\n      }\n    } else {\n      // create a new vertex\n      if (context.preceedingWork == null) {\n        work = utils.createMapWork(context, root, tezWork, null);\n      } else {\n        work = GenTezUtils.createReduceWork(context, root, tezWork);\n      }\n      context.rootToWorkMap.put(root, work);\n    }\n\n    // this is where we set the sort columns that we will be using for KeyValueInputMerge\n    if (operator instanceof DummyStoreOperator) {\n      work.addSortCols(root.getOpTraits().getSortCols().get(0));\n    }\n\n    if (!context.childToWorkMap.containsKey(operator)) {\n      List<BaseWork> workItems = new LinkedList<BaseWork>();\n      workItems.add(work);\n      context.childToWorkMap.put(operator, workItems);\n    } else {\n      context.childToWorkMap.get(operator).add(work);\n    }\n\n    // this transformation needs to be first because it changes the work item itself.\n    // which can affect the working of all downstream transformations.\n    if (context.currentMergeJoinOperator != null) {\n      // we are currently walking the big table side of the merge join. we need to create or hook up\n      // merge join work.\n      MergeJoinWork mergeJoinWork = null;\n      if (context.opMergeJoinWorkMap.containsKey(context.currentMergeJoinOperator)) {\n        // we have found a merge work corresponding to this closing operator. Hook up this work.\n        mergeJoinWork = context.opMergeJoinWorkMap.get(context.currentMergeJoinOperator);\n      } else {\n        // we need to create the merge join work\n        mergeJoinWork = new MergeJoinWork();\n        mergeJoinWork.setMergeJoinOperator(context.currentMergeJoinOperator);\n        tezWork.add(mergeJoinWork);\n        context.opMergeJoinWorkMap.put(context.currentMergeJoinOperator, mergeJoinWork);\n      }\n      // connect the work correctly.\n      work.addSortCols(root.getOpTraits().getSortCols().get(0));\n      mergeJoinWork.addMergedWork(work, null, context.leafOperatorToFollowingWork);\n      Operator<? extends OperatorDesc> parentOp =\n          getParentFromStack(context.currentMergeJoinOperator, stack);\n      // Set the big table position. Both the reduce work and merge join operator\n      // should be set with the same value.\n      int pos = context.currentMergeJoinOperator.getTagForOperator(parentOp);\n      work.setTag(pos);\n      context.currentMergeJoinOperator.getConf().setBigTablePosition(pos);\n      tezWork.setVertexType(work, VertexType.MULTI_INPUT_UNINITIALIZED_EDGES);\n      for (BaseWork parentWork : tezWork.getParents(work)) {\n        TezEdgeProperty edgeProp = tezWork.getEdgeProperty(parentWork, work);\n        tezWork.disconnect(parentWork, work);\n        tezWork.connect(parentWork, mergeJoinWork, edgeProp);\n      }\n\n      for (BaseWork childWork : tezWork.getChildren(work)) {\n        TezEdgeProperty edgeProp = tezWork.getEdgeProperty(work, childWork);\n        tezWork.disconnect(work, childWork);\n        tezWork.connect(mergeJoinWork, childWork, edgeProp);\n      }\n      tezWork.remove(work);\n      context.rootToWorkMap.put(root, mergeJoinWork);\n      context.childToWorkMap.get(operator).remove(work);\n      context.childToWorkMap.get(operator).add(mergeJoinWork);\n      work = mergeJoinWork;\n      context.currentMergeJoinOperator = null;\n    }\n\n    // remember which mapjoin operator links with which work\n    if (!context.currentMapJoinOperators.isEmpty()) {\n      for (MapJoinOperator mj: context.currentMapJoinOperators) {\n        // For dynamic partitioned hash join, ReduceSinkMapJoinProc rule may not get run for all\n        // of the ReduceSink parents, because the parents of the MapJoin operator get\n        // removed later on in this method. Keep track of the parent to mapjoin mapping\n        // so we can later run the same logic that is run in ReduceSinkMapJoinProc.\n        if (mj.getConf().isDynamicPartitionHashJoin()) {\n          // Since this is a dynamic partitioned hash join, the work for this join should be a ReduceWork\n          ReduceWork reduceWork = (ReduceWork) work;\n          int bigTablePosition = mj.getConf().getPosBigTable();\n          reduceWork.setTag(bigTablePosition);\n\n          // Use context.mapJoinParentMap to get the original RS parents, because\n          // the MapJoin's parents may have been replaced by dummy operator.\n          List<Operator<?>> mapJoinOriginalParents = context.mapJoinParentMap.get(mj);\n          if (mapJoinOriginalParents == null) {\n            throw new SemanticException(\"Unexpected error - context.mapJoinParentMap did not have an entry for \" + mj);\n          }\n          for (int pos = 0; pos < mapJoinOriginalParents.size(); ++pos) {\n            // This processing only needs to happen for the small tables\n            if (pos == bigTablePosition) {\n              continue;\n            }\n            Operator<?> parentOp = mapJoinOriginalParents.get(pos);\n            context.smallTableParentToMapJoinMap.put(parentOp, mj);\n\n            ReduceSinkOperator parentRS = (ReduceSinkOperator) parentOp;\n\n            // TableDesc needed for dynamic partitioned hash join\n            GenMapRedUtils.setKeyAndValueDesc(reduceWork, parentRS);\n\n            // For small table RS parents that have already been processed, we need to\n            // add the tag to the RS work to the reduce work that contains this map join.\n            // This was not being done for normal mapjoins, where the small table typically\n            // has its ReduceSink parent removed.\n            if (!context.mapJoinToUnprocessedSmallTableReduceSinks.get(mj).contains(parentRS)) {\n              // This reduce sink has been processed already, so the work for the parentRS exists\n              BaseWork parentWork = ReduceSinkMapJoinProc.getMapJoinParentWork(context, parentRS);\n              int tag = parentRS.getConf().getTag();\n              tag = (tag == -1 ? 0 : tag);\n              reduceWork.getTagToInput().put(tag, parentWork.getName());\n            }\n\n          }\n        }\n\n        LOG.debug(\"Processing map join: \" + mj);\n        // remember the mapping in case we scan another branch of the\n        // mapjoin later\n        if (!context.mapJoinWorkMap.containsKey(mj)) {\n          List<BaseWork> workItems = new LinkedList<BaseWork>();\n          workItems.add(work);\n          context.mapJoinWorkMap.put(mj, workItems);\n        } else {\n          context.mapJoinWorkMap.get(mj).add(work);\n        }\n\n        /*\n         * this happens in case of map join operations.\n         * The tree looks like this:\n         *\n         *        RS <--- we are here perhaps\n         *        |\n         *     MapJoin\n         *     /     \\\n         *   RS       TS\n         *  /\n         * TS\n         *\n         * If we are at the RS pointed above, and we may have already visited the\n         * RS following the TS, we have already generated work for the TS-RS.\n         * We need to hook the current work to this generated work.\n         */\n        if (context.linkOpWithWorkMap.containsKey(mj)) {\n          Map<BaseWork,TezEdgeProperty> linkWorkMap = context.linkOpWithWorkMap.get(mj);\n          if (linkWorkMap != null) {\n            if (context.linkChildOpWithDummyOp.containsKey(mj)) {\n              for (Operator<?> dummy: context.linkChildOpWithDummyOp.get(mj)) {\n                work.addDummyOp((HashTableDummyOperator) dummy);\n              }\n            }\n            for (Entry<BaseWork,TezEdgeProperty> parentWorkMap : linkWorkMap.entrySet()) {\n              BaseWork parentWork = parentWorkMap.getKey();\n              LOG.debug(\"connecting \"+parentWork.getName()+\" with \"+work.getName());\n              TezEdgeProperty edgeProp = parentWorkMap.getValue();\n              tezWork.connect(parentWork, work, edgeProp);\n              if (edgeProp.getEdgeType() == EdgeType.CUSTOM_EDGE) {\n                tezWork.setVertexType(work, VertexType.INITIALIZED_EDGES);\n              }\n\n              // need to set up output name for reduce sink now that we know the name\n              // of the downstream work\n              for (ReduceSinkOperator r:\n                     context.linkWorkWithReduceSinkMap.get(parentWork)) {\n                if (r.getConf().getOutputName() != null) {\n                  LOG.debug(\"Cloning reduce sink for multi-child broadcast edge\");\n                  // we've already set this one up. Need to clone for the next work.\n                  r = (ReduceSinkOperator) OperatorFactory.getAndMakeChild(\n                      (ReduceSinkDesc)r.getConf().clone(),\n                      new RowSchema(r.getSchema()),\n                      r.getParentOperators());\n                  context.clonedReduceSinks.add(r);\n                }\n                r.getConf().setOutputName(work.getName());\n                context.connectedReduceSinks.add(r);\n              }\n            }\n          }\n        }\n      }\n      // clear out the set. we don't need it anymore.\n      context.currentMapJoinOperators.clear();\n    }\n\n    // This is where we cut the tree as described above. We also remember that\n    // we might have to connect parent work with this work later.\n    for (Operator<?> parent : new ArrayList<Operator<?>>(root.getParentOperators())) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Removing \" + parent + \" as parent from \" + root);\n      }\n      context.leafOperatorToFollowingWork.remove(parent);\n      context.leafOperatorToFollowingWork.put(parent, work);\n      root.removeParent(parent);\n    }\n\n    if (!context.currentUnionOperators.isEmpty()) {\n      // if there are union all operators, it means that the walking context contains union all operators.\n      // please see more details of context.currentUnionOperator in GenTezWorkWalker\n\n      UnionWork unionWork;\n      if (context.unionWorkMap.containsKey(operator)) {\n        // we've seen this terminal before and have created a union work object.\n        // just need to add this work to it. There will be no children of this one\n        // since we've passed this operator before.\n        assert operator.getChildOperators().isEmpty();\n        unionWork = (UnionWork) context.unionWorkMap.get(operator);\n        // finally connect the union work with work\n        connectUnionWorkWithWork(unionWork, work, tezWork, context);\n      } else {\n        // we've not seen this terminal before. we need to check\n        // rootUnionWorkMap which contains the information of mapping the root\n        // operator of a union work to a union work\n        unionWork = context.rootUnionWorkMap.get(root);\n        if (unionWork == null) {\n          // if unionWork is null, it means it is the first time. we need to\n          // create a union work object and add this work to it. Subsequent \n          // work should reference the union and not the actual work.\n          unionWork = GenTezUtils.createUnionWork(context, root, operator, tezWork);\n          // finally connect the union work with work\n          connectUnionWorkWithWork(unionWork, work, tezWork, context);\n        }\n      }\n      context.currentUnionOperators.clear();\n      work = unionWork;\n\n    }\n\n    // We're scanning a tree from roots to leaf (this is not technically\n    // correct, demux and mux operators might form a diamond shape, but\n    // we will only scan one path and ignore the others, because the\n    // diamond shape is always contained in a single vertex). The scan\n    // is depth first and because we remove parents when we pack a pipeline\n    // into a vertex we will never visit any node twice. But because of that\n    // we might have a situation where we need to connect 'work' that comes after\n    // the 'work' we're currently looking at.\n    //\n    // Also note: the concept of leaf and root is reversed in hive for historical\n    // reasons. Roots are data sources, leaves are data sinks. I know.\n    if (context.leafOperatorToFollowingWork.containsKey(operator)) {\n\n      BaseWork followingWork = context.leafOperatorToFollowingWork.get(operator);\n      long bytesPerReducer = context.conf.getLongVar(HiveConf.ConfVars.BYTESPERREDUCER);\n\n      LOG.debug(\"Second pass. Leaf operator: \"+operator\n        +\" has common downstream work:\"+followingWork);\n\n      if (operator instanceof DummyStoreOperator) {\n        // this is the small table side.\n        assert (followingWork instanceof MergeJoinWork);\n        MergeJoinWork mergeJoinWork = (MergeJoinWork) followingWork;\n        CommonMergeJoinOperator mergeJoinOp = mergeJoinWork.getMergeJoinOperator();\n        work.setTag(mergeJoinOp.getTagForOperator(operator));\n        mergeJoinWork.addMergedWork(null, work, context.leafOperatorToFollowingWork);\n        tezWork.setVertexType(mergeJoinWork, VertexType.MULTI_INPUT_UNINITIALIZED_EDGES);\n        for (BaseWork parentWork : tezWork.getParents(work)) {\n          TezEdgeProperty edgeProp = tezWork.getEdgeProperty(parentWork, work);\n          tezWork.disconnect(parentWork, work);\n          tezWork.connect(parentWork, mergeJoinWork, edgeProp);\n        }\n        work = mergeJoinWork;\n      } else {\n        // need to add this branch to the key + value info\n        assert operator instanceof ReduceSinkOperator\n            && ((followingWork instanceof ReduceWork) || (followingWork instanceof MergeJoinWork)\n                || followingWork instanceof UnionWork);\n        ReduceSinkOperator rs = (ReduceSinkOperator) operator;\n        ReduceWork rWork = null;\n        if (followingWork instanceof MergeJoinWork) {\n          MergeJoinWork mergeJoinWork = (MergeJoinWork) followingWork;\n          rWork = (ReduceWork) mergeJoinWork.getMainWork();\n        } else if (followingWork instanceof UnionWork) {\n          // this can only be possible if there is merge work followed by the union\n          UnionWork unionWork = (UnionWork) followingWork;\n          int index = getFollowingWorkIndex(tezWork, unionWork, rs);\n          BaseWork baseWork = tezWork.getChildren(unionWork).get(index);\n          if (baseWork instanceof MergeJoinWork) {\n            MergeJoinWork mergeJoinWork = (MergeJoinWork) baseWork;\n            // disconnect the connection to union work and connect to merge work\n            followingWork = mergeJoinWork;\n            rWork = (ReduceWork) mergeJoinWork.getMainWork();\n          } else {\n            rWork = (ReduceWork) baseWork;\n          }\n        } else {\n          rWork = (ReduceWork) followingWork;\n        }\n        GenMapRedUtils.setKeyAndValueDesc(rWork, rs);\n\n        // remember which parent belongs to which tag\n        int tag = rs.getConf().getTag();\n        rWork.getTagToInput().put(tag == -1 ? 0 : tag, work.getName());\n\n        // remember the output name of the reduce sink\n        rs.getConf().setOutputName(rWork.getName());\n\n        // For dynamic partitioned hash join, run the ReduceSinkMapJoinProc logic for any\n        // ReduceSink parents that we missed.\n        MapJoinOperator mj = context.smallTableParentToMapJoinMap.get(rs);\n        if (mj != null) {\n          // Only need to run the logic for tables we missed\n          if (context.mapJoinToUnprocessedSmallTableReduceSinks.get(mj).contains(rs)) {\n            // ReduceSinkMapJoinProc logic does not work unless the ReduceSink is connected as\n            // a parent of the MapJoin, but at this point we have already removed all of the\n            // parents from the MapJoin.\n            // Try temporarily adding the RS as a parent\n            ArrayList<Operator<?>> tempMJParents = new ArrayList<Operator<?>>();\n            tempMJParents.add(rs);\n            mj.setParentOperators(tempMJParents);\n            // ReduceSink also needs MapJoin as child\n            List<Operator<?>> rsChildren = rs.getChildOperators();\n            rsChildren.add(mj);\n\n            // Since the MapJoin has had all of its other parents removed at this point,\n            // it would be bad here if processReduceSinkToHashJoin() tries to do anything\n            // with the RS parent based on its position in the list of parents.\n            ReduceSinkMapJoinProc.processReduceSinkToHashJoin(rs, mj, context);\n\n            // Remove any parents from MapJoin again\n            mj.removeParents();\n            // TODO: do we also need to remove the MapJoin from the list of RS's children?\n          }\n        }\n\n        if (!context.connectedReduceSinks.contains(rs)) {\n          // add dependency between the two work items\n          TezEdgeProperty edgeProp;\n          EdgeType edgeType = utils.determineEdgeType(work, followingWork);\n          if (rWork.isAutoReduceParallelism()) {\n            edgeProp =\n                new TezEdgeProperty(context.conf, edgeType, true,\n                    rWork.getMinReduceTasks(), rWork.getMaxReduceTasks(), bytesPerReducer);\n          } else {\n            edgeProp = new TezEdgeProperty(edgeType);\n          }\n          tezWork.connect(work, followingWork, edgeProp);\n          context.connectedReduceSinks.add(rs);\n        }\n      }\n    } else {\n      LOG.debug(\"First pass. Leaf operator: \"+operator);\n    }\n\n    // No children means we're at the bottom. If there are more operators to scan\n    // the next item will be a new root.\n    if (!operator.getChildOperators().isEmpty()) {\n      assert operator.getChildOperators().size() == 1;\n      context.parentOfRoot = operator;\n      context.currentRootOperator = operator.getChildOperators().get(0);\n      context.preceedingWork = work;\n    }\n\n    return null;\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.optimizer.ConvertJoinMapJoin.convertJoinSMBJoin": {
                "code_before_change": "  private void convertJoinSMBJoin(JoinOperator joinOp, OptimizeTezProcContext context,\n      int mapJoinConversionPos, int numBuckets, boolean adjustParentsChildren)\n      throws SemanticException {\n    MapJoinDesc mapJoinDesc = null;\n    if (adjustParentsChildren) {\n      mapJoinDesc = MapJoinProcessor.getMapJoinDesc(context.conf,\n            joinOp, joinOp.getConf().isLeftInputJoin(), joinOp.getConf().getBaseSrc(),\n            joinOp.getConf().getMapAliases(), mapJoinConversionPos, true);\n    } else {\n      JoinDesc joinDesc = joinOp.getConf();\n      // retain the original join desc in the map join.\n      mapJoinDesc =\n          new MapJoinDesc(\n                  MapJoinProcessor.getKeys(joinOp.getConf().isLeftInputJoin(),\n                  joinOp.getConf().getBaseSrc(), joinOp).getSecond(),\n                  null, joinDesc.getExprs(), null, null,\n                  joinDesc.getOutputColumnNames(), mapJoinConversionPos, joinDesc.getConds(),\n                  joinDesc.getFilters(), joinDesc.getNoOuterJoin(), null);\n      mapJoinDesc.setNullSafes(joinDesc.getNullSafes());\n      mapJoinDesc.setFilterMap(joinDesc.getFilterMap());\n      mapJoinDesc.resetOrder();\n    }\n\n    CommonMergeJoinOperator mergeJoinOp =\n        (CommonMergeJoinOperator) OperatorFactory.get(new CommonMergeJoinDesc(numBuckets,\n            mapJoinConversionPos, mapJoinDesc), joinOp.getSchema());\n    OpTraits opTraits =\n        new OpTraits(joinOp.getOpTraits().getBucketColNames(), numBuckets, joinOp.getOpTraits()\n            .getSortCols());\n    mergeJoinOp.setOpTraits(opTraits);\n    mergeJoinOp.setStatistics(joinOp.getStatistics());\n\n    for (Operator<? extends OperatorDesc> parentOp : joinOp.getParentOperators()) {\n      int pos = parentOp.getChildOperators().indexOf(joinOp);\n      parentOp.getChildOperators().remove(pos);\n      parentOp.getChildOperators().add(pos, mergeJoinOp);\n    }\n\n    for (Operator<? extends OperatorDesc> childOp : joinOp.getChildOperators()) {\n      int pos = childOp.getParentOperators().indexOf(joinOp);\n      childOp.getParentOperators().remove(pos);\n      childOp.getParentOperators().add(pos, mergeJoinOp);\n    }\n\n    List<Operator<? extends OperatorDesc>> childOperators = mergeJoinOp.getChildOperators();\n    List<Operator<? extends OperatorDesc>> parentOperators = mergeJoinOp.getParentOperators();\n\n    childOperators.clear();\n    parentOperators.clear();\n    childOperators.addAll(joinOp.getChildOperators());\n    parentOperators.addAll(joinOp.getParentOperators());\n    mergeJoinOp.getConf().setGenJoinKeys(false);\n\n    if (adjustParentsChildren) {\n      mergeJoinOp.getConf().setGenJoinKeys(true);\n      List<Operator<? extends OperatorDesc>> newParentOpList = new ArrayList<Operator<? extends OperatorDesc>>();\n      for (Operator<? extends OperatorDesc> parentOp : mergeJoinOp.getParentOperators()) {\n        for (Operator<? extends OperatorDesc> grandParentOp : parentOp.getParentOperators()) {\n          grandParentOp.getChildOperators().remove(parentOp);\n          grandParentOp.getChildOperators().add(mergeJoinOp);\n          newParentOpList.add(grandParentOp);\n        }\n      }\n      mergeJoinOp.getParentOperators().clear();\n      mergeJoinOp.getParentOperators().addAll(newParentOpList);\n      List<Operator<? extends OperatorDesc>> parentOps =\n          new ArrayList<Operator<? extends OperatorDesc>>(mergeJoinOp.getParentOperators());\n      for (Operator<? extends OperatorDesc> parentOp : parentOps) {\n        int parentIndex = mergeJoinOp.getParentOperators().indexOf(parentOp);\n        if (parentIndex == mapJoinConversionPos) {\n          continue;\n        }\n\n        // insert the dummy store operator here\n        DummyStoreOperator dummyStoreOp = new TezDummyStoreOperator();\n        dummyStoreOp.setParentOperators(new ArrayList<Operator<? extends OperatorDesc>>());\n        dummyStoreOp.setChildOperators(new ArrayList<Operator<? extends OperatorDesc>>());\n        dummyStoreOp.getChildOperators().add(mergeJoinOp);\n        int index = parentOp.getChildOperators().indexOf(mergeJoinOp);\n        parentOp.getChildOperators().remove(index);\n        parentOp.getChildOperators().add(index, dummyStoreOp);\n        dummyStoreOp.getParentOperators().add(parentOp);\n        mergeJoinOp.getParentOperators().remove(parentIndex);\n        mergeJoinOp.getParentOperators().add(parentIndex, dummyStoreOp);\n      }\n    }\n    mergeJoinOp.cloneOriginalParentsList(mergeJoinOp.getParentOperators());\n  }",
                "code_after_change": "  private void convertJoinSMBJoin(JoinOperator joinOp, OptimizeTezProcContext context,\n      int mapJoinConversionPos, int numBuckets, boolean adjustParentsChildren)\n      throws SemanticException {\n    MapJoinDesc mapJoinDesc = null;\n    if (adjustParentsChildren) {\n      mapJoinDesc = MapJoinProcessor.getMapJoinDesc(context.conf,\n            joinOp, joinOp.getConf().isLeftInputJoin(), joinOp.getConf().getBaseSrc(),\n            joinOp.getConf().getMapAliases(), mapJoinConversionPos, true);\n    } else {\n      JoinDesc joinDesc = joinOp.getConf();\n      // retain the original join desc in the map join.\n      mapJoinDesc =\n          new MapJoinDesc(\n                  MapJoinProcessor.getKeys(joinOp.getConf().isLeftInputJoin(),\n                  joinOp.getConf().getBaseSrc(), joinOp).getSecond(),\n                  null, joinDesc.getExprs(), null, null,\n                  joinDesc.getOutputColumnNames(), mapJoinConversionPos, joinDesc.getConds(),\n                  joinDesc.getFilters(), joinDesc.getNoOuterJoin(), null);\n      mapJoinDesc.setNullSafes(joinDesc.getNullSafes());\n      mapJoinDesc.setFilterMap(joinDesc.getFilterMap());\n      mapJoinDesc.resetOrder();\n    }\n\n    CommonMergeJoinOperator mergeJoinOp =\n        (CommonMergeJoinOperator) OperatorFactory.get(new CommonMergeJoinDesc(numBuckets,\n            mapJoinConversionPos, mapJoinDesc), joinOp.getSchema());\n    OpTraits opTraits =\n        new OpTraits(joinOp.getOpTraits().getBucketColNames(), numBuckets, joinOp.getOpTraits()\n            .getSortCols());\n    mergeJoinOp.setOpTraits(opTraits);\n    mergeJoinOp.setStatistics(joinOp.getStatistics());\n\n    for (Operator<? extends OperatorDesc> parentOp : joinOp.getParentOperators()) {\n      int pos = parentOp.getChildOperators().indexOf(joinOp);\n      parentOp.getChildOperators().remove(pos);\n      parentOp.getChildOperators().add(pos, mergeJoinOp);\n    }\n\n    for (Operator<? extends OperatorDesc> childOp : joinOp.getChildOperators()) {\n      int pos = childOp.getParentOperators().indexOf(joinOp);\n      childOp.getParentOperators().remove(pos);\n      childOp.getParentOperators().add(pos, mergeJoinOp);\n    }\n\n    List<Operator<? extends OperatorDesc>> childOperators = mergeJoinOp.getChildOperators();\n    List<Operator<? extends OperatorDesc>> parentOperators = mergeJoinOp.getParentOperators();\n\n    childOperators.clear();\n    parentOperators.clear();\n    childOperators.addAll(joinOp.getChildOperators());\n    parentOperators.addAll(joinOp.getParentOperators());\n    mergeJoinOp.getConf().setGenJoinKeys(false);\n\n    if (adjustParentsChildren) {\n      mergeJoinOp.getConf().setGenJoinKeys(true);\n      List<Operator<? extends OperatorDesc>> newParentOpList = new ArrayList<Operator<? extends OperatorDesc>>();\n      for (Operator<? extends OperatorDesc> parentOp : mergeJoinOp.getParentOperators()) {\n        for (Operator<? extends OperatorDesc> grandParentOp : parentOp.getParentOperators()) {\n          grandParentOp.getChildOperators().remove(parentOp);\n          grandParentOp.getChildOperators().add(mergeJoinOp);\n          newParentOpList.add(grandParentOp);\n        }\n      }\n      mergeJoinOp.getParentOperators().clear();\n      mergeJoinOp.getParentOperators().addAll(newParentOpList);\n      List<Operator<? extends OperatorDesc>> parentOps =\n          new ArrayList<Operator<? extends OperatorDesc>>(mergeJoinOp.getParentOperators());\n      for (Operator<? extends OperatorDesc> parentOp : parentOps) {\n        int parentIndex = mergeJoinOp.getParentOperators().indexOf(parentOp);\n        if (parentIndex == mapJoinConversionPos) {\n          continue;\n        }\n\n        // insert the dummy store operator here\n        DummyStoreOperator dummyStoreOp = new TezDummyStoreOperator();\n        dummyStoreOp.setParentOperators(new ArrayList<Operator<? extends OperatorDesc>>());\n        dummyStoreOp.setChildOperators(new ArrayList<Operator<? extends OperatorDesc>>());\n        dummyStoreOp.getChildOperators().add(mergeJoinOp);\n        int index = parentOp.getChildOperators().indexOf(mergeJoinOp);\n        parentOp.getChildOperators().remove(index);\n        parentOp.getChildOperators().add(index, dummyStoreOp);\n        dummyStoreOp.getParentOperators().add(parentOp);\n        mergeJoinOp.getParentOperators().remove(parentIndex);\n        mergeJoinOp.getParentOperators().add(parentIndex, dummyStoreOp);\n      }\n    }\n    mergeJoinOp.cloneOriginalParentsList(mergeJoinOp.getParentOperators());\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.initializeSourceForTag": {
                "code_before_change": "  private void initializeSourceForTag(ReduceWork redWork, int tag, ObjectInspector[] ois,\n      ReduceRecordSource[] sources, TableDesc valueTableDesc, String inputName)\n      throws Exception {\n    reducer = redWork.getReducer();\n    reducer.getParentOperators().clear();\n    reducer.setParentOperators(null); // clear out any parents as reducer is the root\n\n    TableDesc keyTableDesc = redWork.getKeyDesc();\n    KeyValuesReader reader = (KeyValuesReader) inputs.get(inputName).getReader();\n\n    sources[tag] = new ReduceRecordSource();\n    sources[tag].init(jconf, redWork.getReducer(), redWork.getVectorMode(), keyTableDesc,\n        valueTableDesc, reader, tag == bigTablePosition, (byte) tag,\n        redWork.getVectorScratchColumnTypeMap());\n    ois[tag] = sources[tag].getObjectInspector();\n  }",
                "code_after_change": "  private void initializeSourceForTag(ReduceWork redWork, int tag, ObjectInspector[] ois,\n      ReduceRecordSource[] sources, TableDesc valueTableDesc, String inputName)\n      throws Exception {\n    reducer = redWork.getReducer();\n    reducer.getParentOperators().clear();\n    reducer.setParentOperators(null); // clear out any parents as reducer is the root\n\n    TableDesc keyTableDesc = redWork.getKeyDesc();\n    Reader reader = inputs.get(inputName).getReader();\n\n    sources[tag] = new ReduceRecordSource();\n    // Only the big table input source should be vectorized (if applicable)\n    // Note this behavior may have to change if we ever implement a vectorized merge join\n    boolean vectorizedRecordSource = (tag == bigTablePosition) && redWork.getVectorMode();\n    sources[tag].init(jconf, redWork.getReducer(), vectorizedRecordSource, keyTableDesc,\n        valueTableDesc, reader, tag == bigTablePosition, (byte) tag,\n        redWork.getVectorScratchColumnTypeMap());\n    ois[tag] = sources[tag].getObjectInspector();\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.exec.vector.VectorMapJoinOperator.VectorMapJoinOperator": {
                "code_before_change": [],
                "code_after_change": []
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the error as a 'LimitExceededException' due to too many counters, which is related to the stack trace context but does not precisely identify the root cause in the ground truth methods. The stack trace includes 'org.apache.hadoop.hive.ql.exec.tez.TezTask.execute', which is in the same stack trace context as the ground truth methods, but it is not the exact location of the bug. There is no fix suggestion provided in the bug report, as it only describes the error without suggesting a solution. The problem location is partially identified through the stack trace context, but it does not point to the exact ground truth methods. There is no wrong information in the bug report; it accurately describes the error encountered."
        }
    },
    {
        "filename": "HIVE-5899.json",
        "code_diff": {
            "metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.getPartitionColumnStatisticsObj": {
                "code_before_change": "  private ColumnStatisticsObj getPartitionColumnStatisticsObj(MPartitionColumnStatistics mStatsObj)\n  {\n    ColumnStatisticsObj statsObj = new ColumnStatisticsObj();\n    statsObj.setColType(mStatsObj.getColType());\n    statsObj.setColName(mStatsObj.getColName());\n    String colType = mStatsObj.getColType();\n    ColumnStatisticsData colStatsData = new ColumnStatisticsData();\n\n    if (colType.equalsIgnoreCase(\"boolean\")) {\n      BooleanColumnStatsData boolStats = new BooleanColumnStatsData();\n      boolStats.setNumFalses(mStatsObj.getNumFalses());\n      boolStats.setNumTrues(mStatsObj.getNumTrues());\n      boolStats.setNumNulls(mStatsObj.getNumNulls());\n      colStatsData.setBooleanStats(boolStats);\n    } else if (colType.equalsIgnoreCase(\"string\")) {\n      StringColumnStatsData stringStats = new StringColumnStatsData();\n      stringStats.setNumNulls(mStatsObj.getNumNulls());\n      stringStats.setAvgColLen(mStatsObj.getAvgColLen());\n      stringStats.setMaxColLen(mStatsObj.getMaxColLen());\n      stringStats.setNumDVs(mStatsObj.getNumDVs());\n      colStatsData.setStringStats(stringStats);\n    } else if (colType.equalsIgnoreCase(\"binary\")) {\n      BinaryColumnStatsData binaryStats = new BinaryColumnStatsData();\n      binaryStats.setNumNulls(mStatsObj.getNumNulls());\n      binaryStats.setAvgColLen(mStatsObj.getAvgColLen());\n      binaryStats.setMaxColLen(mStatsObj.getMaxColLen());\n      colStatsData.setBinaryStats(binaryStats);\n    } else if (colType.equalsIgnoreCase(\"tinyint\") || colType.equalsIgnoreCase(\"smallint\") ||\n        colType.equalsIgnoreCase(\"int\") || colType.equalsIgnoreCase(\"bigint\") ||\n        colType.equalsIgnoreCase(\"timestamp\")) {\n      LongColumnStatsData longStats = new LongColumnStatsData();\n      longStats.setNumNulls(mStatsObj.getNumNulls());\n      longStats.setHighValue(mStatsObj.getLongHighValue());\n      longStats.setLowValue(mStatsObj.getLongLowValue());\n      longStats.setNumDVs(mStatsObj.getNumDVs());\n      colStatsData.setLongStats(longStats);\n   } else if (colType.equalsIgnoreCase(\"double\") || colType.equalsIgnoreCase(\"float\")) {\n     DoubleColumnStatsData doubleStats = new DoubleColumnStatsData();\n     doubleStats.setNumNulls(mStatsObj.getNumNulls());\n     doubleStats.setHighValue(mStatsObj.getDoubleHighValue());\n     doubleStats.setLowValue(mStatsObj.getDoubleLowValue());\n     doubleStats.setNumDVs(mStatsObj.getNumDVs());\n     colStatsData.setDoubleStats(doubleStats);\n   }\n   statsObj.setStatsData(colStatsData);\n   return statsObj;\n  }",
                "code_after_change": "  private ColumnStatisticsObj getPartitionColumnStatisticsObj(MPartitionColumnStatistics mStatsObj)\n  {\n    ColumnStatisticsObj statsObj = new ColumnStatisticsObj();\n    statsObj.setColType(mStatsObj.getColType());\n    statsObj.setColName(mStatsObj.getColName());\n    String colType = mStatsObj.getColType().toLowerCase();\n    ColumnStatisticsData colStatsData = new ColumnStatisticsData();\n\n    if (colType.equals(\"boolean\")) {\n      BooleanColumnStatsData boolStats = new BooleanColumnStatsData();\n      boolStats.setNumFalses(mStatsObj.getNumFalses());\n      boolStats.setNumTrues(mStatsObj.getNumTrues());\n      boolStats.setNumNulls(mStatsObj.getNumNulls());\n      colStatsData.setBooleanStats(boolStats);\n    } else if (colType.equals(\"string\") ||\n        colType.startsWith(\"varchar\") || colType.startsWith(\"char\")) {\n      StringColumnStatsData stringStats = new StringColumnStatsData();\n      stringStats.setNumNulls(mStatsObj.getNumNulls());\n      stringStats.setAvgColLen(mStatsObj.getAvgColLen());\n      stringStats.setMaxColLen(mStatsObj.getMaxColLen());\n      stringStats.setNumDVs(mStatsObj.getNumDVs());\n      colStatsData.setStringStats(stringStats);\n    } else if (colType.equals(\"binary\")) {\n      BinaryColumnStatsData binaryStats = new BinaryColumnStatsData();\n      binaryStats.setNumNulls(mStatsObj.getNumNulls());\n      binaryStats.setAvgColLen(mStatsObj.getAvgColLen());\n      binaryStats.setMaxColLen(mStatsObj.getMaxColLen());\n      colStatsData.setBinaryStats(binaryStats);\n    } else if (colType.equals(\"tinyint\") || colType.equals(\"smallint\") ||\n        colType.equals(\"int\") || colType.equals(\"bigint\") ||\n        colType.equals(\"timestamp\")) {\n      LongColumnStatsData longStats = new LongColumnStatsData();\n      longStats.setNumNulls(mStatsObj.getNumNulls());\n      longStats.setHighValue(mStatsObj.getLongHighValue());\n      longStats.setLowValue(mStatsObj.getLongLowValue());\n      longStats.setNumDVs(mStatsObj.getNumDVs());\n      colStatsData.setLongStats(longStats);\n   } else if (colType.equals(\"double\") || colType.equals(\"float\")) {\n     DoubleColumnStatsData doubleStats = new DoubleColumnStatsData();\n     doubleStats.setNumNulls(mStatsObj.getNumNulls());\n     doubleStats.setHighValue(mStatsObj.getDoubleHighValue());\n     doubleStats.setLowValue(mStatsObj.getDoubleLowValue());\n     doubleStats.setNumDVs(mStatsObj.getNumDVs());\n     colStatsData.setDoubleStats(doubleStats);\n   }\n   statsObj.setStatsData(colStatsData);\n   return statsObj;\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.io.orc.WriterImpl.getRawDataSizeFromPrimitives": {
                "code_before_change": "  private long getRawDataSizeFromPrimitives(TreeWriter child, ObjectInspector oi) {\n    long result = 0;\n    long numVals = child.fileStatistics.getNumberOfValues();\n    switch (((PrimitiveObjectInspector) oi).getPrimitiveCategory()) {\n    case BOOLEAN:\n    case BYTE:\n    case SHORT:\n    case INT:\n    case FLOAT:\n      return numVals * JavaDataModel.get().primitive1();\n    case LONG:\n    case DOUBLE:\n      return numVals * JavaDataModel.get().primitive2();\n    case STRING:\n      // ORC strings are converted to java Strings. so use JavaDataModel to\n      // compute the overall size of strings\n      child = (StringTreeWriter) child;\n      StringColumnStatistics scs = (StringColumnStatistics) child.fileStatistics;\n      numVals = numVals == 0 ? 1 : numVals;\n      int avgStringLen = (int) (scs.getSum() / numVals);\n      return numVals * JavaDataModel.get().lengthForStringOfLength(avgStringLen);\n    case DECIMAL:\n      return numVals * JavaDataModel.get().lengthOfDecimal();\n    case DATE:\n      return numVals * JavaDataModel.get().lengthOfDate();\n    case BINARY:\n      // get total length of binary blob\n      BinaryColumnStatistics bcs = (BinaryColumnStatistics) child.fileStatistics;\n      return bcs.getSum();\n    case TIMESTAMP:\n      return numVals * JavaDataModel.get().lengthOfTimestamp();\n    default:\n      LOG.debug(\"Unknown primitive category.\");\n      break;\n    }\n\n    return result;\n  }",
                "code_after_change": "  private long getRawDataSizeFromPrimitives(TreeWriter child, ObjectInspector oi) {\n    long result = 0;\n    long numVals = child.fileStatistics.getNumberOfValues();\n    switch (((PrimitiveObjectInspector) oi).getPrimitiveCategory()) {\n    case BOOLEAN:\n    case BYTE:\n    case SHORT:\n    case INT:\n    case FLOAT:\n      return numVals * JavaDataModel.get().primitive1();\n    case LONG:\n    case DOUBLE:\n      return numVals * JavaDataModel.get().primitive2();\n    case STRING:\n    case VARCHAR:\n    case CHAR:\n      // ORC strings are converted to java Strings. so use JavaDataModel to\n      // compute the overall size of strings\n      child = (StringTreeWriter) child;\n      StringColumnStatistics scs = (StringColumnStatistics) child.fileStatistics;\n      numVals = numVals == 0 ? 1 : numVals;\n      int avgStringLen = (int) (scs.getSum() / numVals);\n      return numVals * JavaDataModel.get().lengthForStringOfLength(avgStringLen);\n    case DECIMAL:\n      return numVals * JavaDataModel.get().lengthOfDecimal();\n    case DATE:\n      return numVals * JavaDataModel.get().lengthOfDate();\n    case BINARY:\n      // get total length of binary blob\n      BinaryColumnStatistics bcs = (BinaryColumnStatistics) child.fileStatistics;\n      return bcs.getSum();\n    case TIMESTAMP:\n      return numVals * JavaDataModel.get().lengthOfTimestamp();\n    default:\n      LOG.debug(\"Unknown primitive category.\");\n      break;\n    }\n\n    return result;\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.getTableColumnStatisticsObj": {
                "code_before_change": "  private ColumnStatisticsObj getTableColumnStatisticsObj(MTableColumnStatistics mStatsObj) {\n    ColumnStatisticsObj statsObj = new ColumnStatisticsObj();\n    statsObj.setColType(mStatsObj.getColType());\n    statsObj.setColName(mStatsObj.getColName());\n    String colType = mStatsObj.getColType();\n    ColumnStatisticsData colStatsData = new ColumnStatisticsData();\n\n    if (colType.equalsIgnoreCase(\"boolean\")) {\n      BooleanColumnStatsData boolStats = new BooleanColumnStatsData();\n      boolStats.setNumFalses(mStatsObj.getNumFalses());\n      boolStats.setNumTrues(mStatsObj.getNumTrues());\n      boolStats.setNumNulls(mStatsObj.getNumNulls());\n      colStatsData.setBooleanStats(boolStats);\n    } else if (colType.equalsIgnoreCase(\"string\")) {\n      StringColumnStatsData stringStats = new StringColumnStatsData();\n      stringStats.setNumNulls(mStatsObj.getNumNulls());\n      stringStats.setAvgColLen(mStatsObj.getAvgColLen());\n      stringStats.setMaxColLen(mStatsObj.getMaxColLen());\n      stringStats.setNumDVs(mStatsObj.getNumDVs());\n      colStatsData.setStringStats(stringStats);\n    } else if (colType.equalsIgnoreCase(\"binary\")) {\n      BinaryColumnStatsData binaryStats = new BinaryColumnStatsData();\n      binaryStats.setNumNulls(mStatsObj.getNumNulls());\n      binaryStats.setAvgColLen(mStatsObj.getAvgColLen());\n      binaryStats.setMaxColLen(mStatsObj.getMaxColLen());\n      colStatsData.setBinaryStats(binaryStats);\n    } else if (colType.equalsIgnoreCase(\"bigint\") || colType.equalsIgnoreCase(\"int\") ||\n        colType.equalsIgnoreCase(\"smallint\") || colType.equalsIgnoreCase(\"tinyint\") ||\n        colType.equalsIgnoreCase(\"timestamp\")) {\n      LongColumnStatsData longStats = new LongColumnStatsData();\n      longStats.setNumNulls(mStatsObj.getNumNulls());\n      longStats.setHighValue(mStatsObj.getLongHighValue());\n      longStats.setLowValue(mStatsObj.getLongLowValue());\n      longStats.setNumDVs(mStatsObj.getNumDVs());\n      colStatsData.setLongStats(longStats);\n   } else if (colType.equalsIgnoreCase(\"double\") || colType.equalsIgnoreCase(\"float\")) {\n     DoubleColumnStatsData doubleStats = new DoubleColumnStatsData();\n     doubleStats.setNumNulls(mStatsObj.getNumNulls());\n     doubleStats.setHighValue(mStatsObj.getDoubleHighValue());\n     doubleStats.setLowValue(mStatsObj.getDoubleLowValue());\n     doubleStats.setNumDVs(mStatsObj.getNumDVs());\n     colStatsData.setDoubleStats(doubleStats);\n   }\n   statsObj.setStatsData(colStatsData);\n   return statsObj;\n  }",
                "code_after_change": "  private ColumnStatisticsObj getTableColumnStatisticsObj(MTableColumnStatistics mStatsObj) {\n    ColumnStatisticsObj statsObj = new ColumnStatisticsObj();\n    statsObj.setColType(mStatsObj.getColType());\n    statsObj.setColName(mStatsObj.getColName());\n    String colType = mStatsObj.getColType().toLowerCase();\n    ColumnStatisticsData colStatsData = new ColumnStatisticsData();\n\n    if (colType.equals(\"boolean\")) {\n      BooleanColumnStatsData boolStats = new BooleanColumnStatsData();\n      boolStats.setNumFalses(mStatsObj.getNumFalses());\n      boolStats.setNumTrues(mStatsObj.getNumTrues());\n      boolStats.setNumNulls(mStatsObj.getNumNulls());\n      colStatsData.setBooleanStats(boolStats);\n    } else if (colType.equals(\"string\") ||\n        colType.startsWith(\"varchar\") || colType.startsWith(\"char\")) {\n      StringColumnStatsData stringStats = new StringColumnStatsData();\n      stringStats.setNumNulls(mStatsObj.getNumNulls());\n      stringStats.setAvgColLen(mStatsObj.getAvgColLen());\n      stringStats.setMaxColLen(mStatsObj.getMaxColLen());\n      stringStats.setNumDVs(mStatsObj.getNumDVs());\n      colStatsData.setStringStats(stringStats);\n    } else if (colType.equals(\"binary\")) {\n      BinaryColumnStatsData binaryStats = new BinaryColumnStatsData();\n      binaryStats.setNumNulls(mStatsObj.getNumNulls());\n      binaryStats.setAvgColLen(mStatsObj.getAvgColLen());\n      binaryStats.setMaxColLen(mStatsObj.getMaxColLen());\n      colStatsData.setBinaryStats(binaryStats);\n    } else if (colType.equals(\"bigint\") || colType.equals(\"int\") ||\n        colType.equals(\"smallint\") || colType.equals(\"tinyint\") ||\n        colType.equals(\"timestamp\")) {\n      LongColumnStatsData longStats = new LongColumnStatsData();\n      longStats.setNumNulls(mStatsObj.getNumNulls());\n      longStats.setHighValue(mStatsObj.getLongHighValue());\n      longStats.setLowValue(mStatsObj.getLongLowValue());\n      longStats.setNumDVs(mStatsObj.getNumDVs());\n      colStatsData.setLongStats(longStats);\n   } else if (colType.equals(\"double\") || colType.equals(\"float\")) {\n     DoubleColumnStatsData doubleStats = new DoubleColumnStatsData();\n     doubleStats.setNumNulls(mStatsObj.getNumNulls());\n     doubleStats.setHighValue(mStatsObj.getDoubleHighValue());\n     doubleStats.setLowValue(mStatsObj.getDoubleLowValue());\n     doubleStats.setNumDVs(mStatsObj.getNumDVs());\n     colStatsData.setDoubleStats(doubleStats);\n   }\n   statsObj.setStatsData(colStatsData);\n   return statsObj;\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies a NullPointerException occurring in methods within the stack trace, such as 'org.apache.hadoop.hive.metastore.api.ColumnStatisticsData.getFieldDesc', which are related to the ground truth methods through shared stack trace context. However, it does not pinpoint the exact root cause in the ground truth methods. There is no fix suggestion provided in the bug report. The problem location is partially identified as it mentions methods in the stack trace but not the exact ground truth methods. There is no wrong information as the report accurately describes the error context."
        }
    },
    {
        "filename": "HIVE-11102.json",
        "code_diff": {
            "ql.src.java.org.apache.hadoop.hive.ql.io.orc.ReaderImpl.getColumnIndicesFromNames": {
                "code_before_change": "  private List<Integer> getColumnIndicesFromNames(List<String> colNames) {\n    // top level struct\n    Type type = footer.getTypesList().get(0);\n    List<Integer> colIndices = Lists.newArrayList();\n    List<String> fieldNames = type.getFieldNamesList();\n    int fieldIdx = 0;\n    for (String colName : colNames) {\n      if (fieldNames.contains(colName)) {\n        fieldIdx = fieldNames.indexOf(colName);\n      }\n\n      // a single field may span multiple columns. find start and end column\n      // index for the requested field\n      int idxStart = type.getSubtypes(fieldIdx);\n\n      int idxEnd;\n\n      // if the specified is the last field and then end index will be last\n      // column index\n      if (fieldIdx + 1 > fieldNames.size() - 1) {\n        idxEnd = getLastIdx() + 1;\n      } else {\n        idxEnd = type.getSubtypes(fieldIdx + 1);\n      }\n\n      // if start index and end index are same then the field is a primitive\n      // field else complex field (like map, list, struct, union)\n      if (idxStart == idxEnd) {\n        // simple field\n        colIndices.add(idxStart);\n      } else {\n        // complex fields spans multiple columns\n        for (int i = idxStart; i < idxEnd; i++) {\n          colIndices.add(i);\n        }\n      }\n    }\n    return colIndices;\n  }",
                "code_after_change": "  private List<Integer> getColumnIndicesFromNames(List<String> colNames) {\n    // top level struct\n    Type type = footer.getTypesList().get(0);\n    List<Integer> colIndices = Lists.newArrayList();\n    List<String> fieldNames = type.getFieldNamesList();\n    int fieldIdx = 0;\n    for (String colName : colNames) {\n      if (fieldNames.contains(colName)) {\n        fieldIdx = fieldNames.indexOf(colName);\n      } else {\n        String s = \"Cannot find field for: \" + colName + \" in \";\n        for (String fn : fieldNames) {\n          s += fn + \", \";\n        }\n        LOG.warn(s);\n        continue;\n      }\n\n      // a single field may span multiple columns. find start and end column\n      // index for the requested field\n      int idxStart = type.getSubtypes(fieldIdx);\n\n      int idxEnd;\n\n      // if the specified is the last field and then end index will be last\n      // column index\n      if (fieldIdx + 1 > fieldNames.size() - 1) {\n        idxEnd = getLastIdx() + 1;\n      } else {\n        idxEnd = type.getSubtypes(fieldIdx + 1);\n      }\n\n      // if start index and end index are same then the field is a primitive\n      // field else complex field (like map, list, struct, union)\n      if (idxStart == idxEnd) {\n        // simple field\n        colIndices.add(idxStart);\n      } else {\n        // complex fields spans multiple columns\n        for (int i = idxStart; i < idxEnd; i++) {\n          colIndices.add(i);\n        }\n      }\n    }\n    return colIndices;\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report precisely identifies the root cause by mentioning the method 'ReaderImpl.getColumnIndicesFromNames', which is the ground truth method where the bug occurred. However, there is no fix suggestion provided in the bug report, as there is no 'Suggestions' or 'possible_fix' field, nor is there any suggestion in the 'Description'. The problem location is also precisely identified as the method 'ReaderImpl.getColumnIndicesFromNames' is mentioned in the stack trace. There is no wrong information in the bug report as all the information is relevant to the context of the bug."
        }
    },
    {
        "filename": "HIVE-9195.json",
        "code_diff": {
            "serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableVoidObjectInspector.getWritableConstantValue": {
                "code_before_change": "  public Object getWritableConstantValue() {\n    return null;\n  }",
                "code_after_change": "  public Object getWritableConstantValue() {\n    return null;\n  }"
            },
            "serde.src.java.org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyVoidObjectInspector.copyObject": {
                "code_before_change": "  public Object copyObject(Object o) {\n    return o;\n  }",
                "code_after_change": "  public Object copyObject(Object o) {\n    return o;\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.isDescendant": {
                "code_before_change": "    private boolean isDescendant(Node ans, Node des) {\n      if (ans.getChildren() == null) {\n        return false;\n      }\n      for (Node c : ans.getChildren()) {\n        if (c == des) {\n          return true;\n        }\n        if (isDescendant(c, des)) {\n          return true;\n        }\n      }\n      return false;\n    }",
                "code_after_change": "    private boolean isDescendant(Node ans, Node des) {\n      if (ans.getChildren() == null) {\n        return false;\n      }\n      for (Node c : ans.getChildren()) {\n        if (c == des) {\n          return true;\n        }\n        if (isDescendant(c, des)) {\n          return true;\n        }\n      }\n      return false;\n    }"
            },
            "serde.src.java.org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.compareTypes": {
                "code_before_change": "  public static boolean compareTypes(ObjectInspector o1, ObjectInspector o2) {\n    Category c1 = o1.getCategory();\n    Category c2 = o2.getCategory();\n\n    // Return false if categories are not equal\n    if (!c1.equals(c2)) {\n      return false;\n    }\n\n    // If both categories are primitive return the comparison of type names.\n    if (c1.equals(Category.PRIMITIVE)) {\n      return o1.getTypeName().equals(o2.getTypeName());\n    }\n\n    // If lists, recursively compare the list element types\n    if (c1.equals(Category.LIST)) {\n      ObjectInspector child1 =\n        ((ListObjectInspector) o1).getListElementObjectInspector();\n      ObjectInspector child2 =\n        ((ListObjectInspector) o2).getListElementObjectInspector();\n      return compareTypes(child1, child2);\n    }\n\n    // If maps, recursively compare the key and value types\n    if (c1.equals(Category.MAP)) {\n      MapObjectInspector mapOI1 = (MapObjectInspector) o1;\n      MapObjectInspector mapOI2 = (MapObjectInspector) o2;\n\n      ObjectInspector childKey1 = mapOI1.getMapKeyObjectInspector();\n      ObjectInspector childKey2 = mapOI2.getMapKeyObjectInspector();\n\n      if (compareTypes(childKey1, childKey2)) {\n        ObjectInspector childVal1 = mapOI1.getMapValueObjectInspector();\n        ObjectInspector childVal2 = mapOI2.getMapValueObjectInspector();\n\n        if (compareTypes(childVal1, childVal2)) {\n          return true;\n        }\n      }\n\n      return false;\n    }\n\n    // If structs, recursively compare the fields\n    if (c1.equals(Category.STRUCT)) {\n      StructObjectInspector structOI1 = (StructObjectInspector) o1;\n      StructObjectInspector structOI2 = (StructObjectInspector) o2;\n\n      List<? extends StructField> childFieldsList1\n        = structOI1.getAllStructFieldRefs();\n      List<? extends StructField> childFieldsList2\n        = structOI2.getAllStructFieldRefs();\n\n      if (childFieldsList1 == null && childFieldsList2 == null) {\n        return true;\n      } else if (childFieldsList1 == null || childFieldsList2 == null) {\n        return false;\n      } else if (childFieldsList1.size() != childFieldsList2.size()) {\n        return false;\n      }\n\n      Iterator<? extends StructField> it1 = childFieldsList1.iterator();\n      Iterator<? extends StructField> it2 = childFieldsList2.iterator();\n      while (it1.hasNext()) {\n        StructField field1 = it1.next();\n        StructField field2 = it2.next();\n\n        if (!compareTypes(field1.getFieldObjectInspector(),\n              field2.getFieldObjectInspector())) {\n          return false;\n        }\n      }\n\n      return true;\n    }\n\n    if (c1.equals(Category.UNION)) {\n      UnionObjectInspector uoi1 = (UnionObjectInspector) o1;\n      UnionObjectInspector uoi2 = (UnionObjectInspector) o2;\n      List<ObjectInspector> ois1 = uoi1.getObjectInspectors();\n      List<ObjectInspector> ois2 = uoi2.getObjectInspectors();\n\n      if (ois1 == null && ois2 == null) {\n        return true;\n      } else if (ois1 == null || ois2 == null) {\n        return false;\n      } else if (ois1.size() != ois2.size()) {\n        return false;\n      }\n      Iterator<? extends ObjectInspector> it1 = ois1.iterator();\n      Iterator<? extends ObjectInspector> it2 = ois2.iterator();\n      while (it1.hasNext()) {\n        if (!compareTypes(it1.next(), it2.next())) {\n          return false;\n        }\n      }\n      return true;\n    }\n\n    // Unknown category\n    throw new RuntimeException(\"Unknown category encountered: \" + c1);\n  }",
                "code_after_change": "  public static boolean compareTypes(ObjectInspector o1, ObjectInspector o2) {\n    Category c1 = o1.getCategory();\n    Category c2 = o2.getCategory();\n\n    // Return false if categories are not equal\n    if (!c1.equals(c2)) {\n      return false;\n    }\n\n    // If both categories are primitive return the comparison of type names.\n    if (c1.equals(Category.PRIMITIVE)) {\n      return o1.getTypeName().equals(o2.getTypeName());\n    }\n\n    // If lists, recursively compare the list element types\n    if (c1.equals(Category.LIST)) {\n      ObjectInspector child1 =\n        ((ListObjectInspector) o1).getListElementObjectInspector();\n      ObjectInspector child2 =\n        ((ListObjectInspector) o2).getListElementObjectInspector();\n      return compareTypes(child1, child2);\n    }\n\n    // If maps, recursively compare the key and value types\n    if (c1.equals(Category.MAP)) {\n      MapObjectInspector mapOI1 = (MapObjectInspector) o1;\n      MapObjectInspector mapOI2 = (MapObjectInspector) o2;\n\n      ObjectInspector childKey1 = mapOI1.getMapKeyObjectInspector();\n      ObjectInspector childKey2 = mapOI2.getMapKeyObjectInspector();\n\n      if (compareTypes(childKey1, childKey2)) {\n        ObjectInspector childVal1 = mapOI1.getMapValueObjectInspector();\n        ObjectInspector childVal2 = mapOI2.getMapValueObjectInspector();\n\n        if (compareTypes(childVal1, childVal2)) {\n          return true;\n        }\n      }\n\n      return false;\n    }\n\n    // If structs, recursively compare the fields\n    if (c1.equals(Category.STRUCT)) {\n      StructObjectInspector structOI1 = (StructObjectInspector) o1;\n      StructObjectInspector structOI2 = (StructObjectInspector) o2;\n\n      List<? extends StructField> childFieldsList1\n        = structOI1.getAllStructFieldRefs();\n      List<? extends StructField> childFieldsList2\n        = structOI2.getAllStructFieldRefs();\n\n      if (childFieldsList1 == null && childFieldsList2 == null) {\n        return true;\n      } else if (childFieldsList1 == null || childFieldsList2 == null) {\n        return false;\n      } else if (childFieldsList1.size() != childFieldsList2.size()) {\n        return false;\n      }\n\n      Iterator<? extends StructField> it1 = childFieldsList1.iterator();\n      Iterator<? extends StructField> it2 = childFieldsList2.iterator();\n      while (it1.hasNext()) {\n        StructField field1 = it1.next();\n        StructField field2 = it2.next();\n\n        if (!compareTypes(field1.getFieldObjectInspector(),\n              field2.getFieldObjectInspector())) {\n          return false;\n        }\n      }\n\n      return true;\n    }\n\n    if (c1.equals(Category.UNION)) {\n      UnionObjectInspector uoi1 = (UnionObjectInspector) o1;\n      UnionObjectInspector uoi2 = (UnionObjectInspector) o2;\n      List<ObjectInspector> ois1 = uoi1.getObjectInspectors();\n      List<ObjectInspector> ois2 = uoi2.getObjectInspectors();\n\n      if (ois1 == null && ois2 == null) {\n        return true;\n      } else if (ois1 == null || ois2 == null) {\n        return false;\n      } else if (ois1.size() != ois2.size()) {\n        return false;\n      }\n      Iterator<? extends ObjectInspector> it1 = ois1.iterator();\n      Iterator<? extends ObjectInspector> it2 = ois2.iterator();\n      while (it1.hasNext()) {\n        if (!compareTypes(it1.next(), it2.next())) {\n          return false;\n        }\n      }\n      return true;\n    }\n\n    // Unknown category\n    throw new RuntimeException(\"Unknown category encountered: \" + c1);\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.processQualifiedColRef": {
                "code_before_change": "    protected ExprNodeColumnDesc processQualifiedColRef(TypeCheckCtx ctx, ASTNode expr,\n        Object... nodeOutputs) throws SemanticException {\n      RowResolver input = ctx.getInputRR();\n      String tableAlias = BaseSemanticAnalyzer.unescapeIdentifier(expr.getChild(0).getChild(0)\n          .getText());\n      // NOTE: tableAlias must be a valid non-ambiguous table alias,\n      // because we've checked that in TOK_TABLE_OR_COL's process method.\n      ColumnInfo colInfo = input.get(tableAlias, ((ExprNodeConstantDesc) nodeOutputs[1]).getValue()\n          .toString());\n\n      if (colInfo == null) {\n        ctx.setError(ErrorMsg.INVALID_COLUMN.getMsg(expr.getChild(1)), expr);\n        return null;\n      }\n      return new ExprNodeColumnDesc(colInfo.getType(), colInfo.getInternalName(),\n          colInfo.getTabAlias(), colInfo.getIsVirtualCol());\n    }",
                "code_after_change": "    protected ExprNodeDesc processQualifiedColRef(TypeCheckCtx ctx, ASTNode expr,\n        Object... nodeOutputs) throws SemanticException {\n      RowResolver input = ctx.getInputRR();\n      String tableAlias = BaseSemanticAnalyzer.unescapeIdentifier(expr.getChild(0).getChild(0)\n          .getText());\n      // NOTE: tableAlias must be a valid non-ambiguous table alias,\n      // because we've checked that in TOK_TABLE_OR_COL's process method.\n      ColumnInfo colInfo = input.get(tableAlias, ((ExprNodeConstantDesc) nodeOutputs[1]).getValue()\n          .toString());\n\n      if (colInfo == null) {\n        ctx.setError(ErrorMsg.INVALID_COLUMN.getMsg(expr.getChild(1)), expr);\n        return null;\n      }\n      return toExprNodeDesc(colInfo);\n    }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.optimizer.calcite.translator.RexNodeConverter.convert": {
                "code_before_change": [],
                "code_after_change": "  public RexNode convert(ExprNodeDesc expr) throws SemanticException {\n    if (expr instanceof ExprNodeNullDesc) {\n      return createNullLiteral(expr);\n    } else if (expr instanceof ExprNodeGenericFuncDesc) {\n      return convert((ExprNodeGenericFuncDesc) expr);\n    } else if (expr instanceof ExprNodeConstantDesc) {\n      return convert((ExprNodeConstantDesc) expr);\n    } else if (expr instanceof ExprNodeColumnDesc) {\n      return convert((ExprNodeColumnDesc) expr);\n    } else if (expr instanceof ExprNodeFieldDesc) {\n      return convert((ExprNodeFieldDesc) expr);\n    } else {\n      throw new RuntimeException(\"Unsupported Expression\");\n    }\n    // TODO: handle ExprNodeColumnListDesc\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.plan.ExprNodeColumnListDesc.getTypeString": {
                "code_before_change": "  public String getTypeString() {\n    throw new IllegalStateException();\n  }",
                "code_after_change": "  public String getTypeString() {\n    throw new IllegalStateException();\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.process": {
                "code_before_change": "    public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,\n        Object... nodeOutputs) throws SemanticException {\n\n      TypeCheckCtx ctx = (TypeCheckCtx) procCtx;\n      if (ctx.getError() != null) {\n        return null;\n      }\n\n      ExprNodeDesc desc = TypeCheckProcFactory.processGByExpr(nd, procCtx);\n      if (desc != null) {\n        return desc;\n      }\n\n      return new ExprNodeNullDesc();\n    }",
                "code_after_change": "    public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,\n        Object... nodeOutputs) throws SemanticException {\n\n      TypeCheckCtx ctx = (TypeCheckCtx) procCtx;\n      if (ctx.getError() != null) {\n        return null;\n      }\n\n      ExprNodeDesc desc = TypeCheckProcFactory.processGByExpr(nd, procCtx);\n      if (desc != null) {\n        return desc;\n      }\n\n      return new ExprNodeNullDesc();\n    }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the issue related to the CBO changing a constant expression to a column expression, which is related to the stack trace context but does not precisely identify the root cause in the ground truth methods. The report does not provide any fix suggestion, hence 'Missing' for fix suggestion. The problem location is identified in the context of the stack trace but not precisely in the ground truth methods, so it is 'Partial' with 'Shared Stack Trace Context' as the sub-category. There is no incorrect information in the bug report, so 'No' for wrong information."
        }
    },
    {
        "filename": "HIVE-11285.json",
        "code_diff": {
            "ql.src.java.org.apache.hadoop.hive.ql.exec.FetchOperator.getPartitionKeyOI": {
                "code_before_change": "  private StructObjectInspector getPartitionKeyOI(TableDesc tableDesc) throws Exception {\n    String pcols = tableDesc.getProperties().getProperty(\n        org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.META_TABLE_PARTITION_COLUMNS);\n    String pcolTypes = tableDesc.getProperties().getProperty(\n        org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.META_TABLE_PARTITION_COLUMN_TYPES);\n\n    String[] partKeys = pcols.trim().split(\"/\");\n    String[] partKeyTypes = pcolTypes.trim().split(\":\");\n    ObjectInspector[] inspectors = new ObjectInspector[partKeys.length];\n    for (int i = 0; i < partKeys.length; i++) {\n      inspectors[i] = TypeInfoUtils.getStandardJavaObjectInspectorFromTypeInfo(\n          TypeInfoFactory.getPrimitiveTypeInfo(partKeyTypes[i]));\n    }\n    return ObjectInspectorFactory.getStandardStructObjectInspector(\n        Arrays.asList(partKeys), Arrays.asList(inspectors));\n  }",
                "code_after_change": "  private StructObjectInspector getPartitionKeyOI(TableDesc tableDesc) throws Exception {\n    String pcols = tableDesc.getProperties().getProperty(\n        org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.META_TABLE_PARTITION_COLUMNS);\n    String pcolTypes = tableDesc.getProperties().getProperty(\n        org.apache.hadoop.hive.metastore.api.hive_metastoreConstants.META_TABLE_PARTITION_COLUMN_TYPES);\n\n    String[] partKeys = pcols.trim().split(\"/\");\n    String[] partKeyTypes = pcolTypes.trim().split(\":\");\n    ObjectInspector[] inspectors = new ObjectInspector[partKeys.length];\n    for (int i = 0; i < partKeys.length; i++) {\n      inspectors[i] = PrimitiveObjectInspectorFactory\n          .getPrimitiveWritableObjectInspector(TypeInfoFactory\n              .getPrimitiveTypeInfo(partKeyTypes[i]));\n    }\n    return ObjectInspectorFactory.getStandardStructObjectInspector(\n        Arrays.asList(partKeys), Arrays.asList(inspectors));\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Missing",
                "sub_category": ""
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report mentions an exception related to the FetchOperator in the title, which is in the same stack trace context as the ground truth method 'getPartitionKeyOI'. However, it does not precisely identify the root cause at the method level. There is no fix suggestion provided in the bug report. The problem location is not precisely identified as the report does not mention the ground truth method or any related methods outside of the stack trace. All information in the report is relevant to the context of the bug, so there is no wrong information."
        }
    },
    {
        "filename": "HIVE-10288.json",
        "code_diff": {
            "ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.reloadFunctions": {
                "code_before_change": "  public static void reloadFunctions() throws HiveException {\n    Hive db = Hive.get();\n    for (String dbName : db.getAllDatabases()) {\n      for (String functionName : db.getFunctions(dbName, \"*\")) {\n        Function function = db.getFunction(dbName, functionName);\n        try {\n          FunctionRegistry.registerPermanentFunction(functionName, function.getClassName(), false,\n              FunctionTask.toFunctionResource(function.getResourceUris()));\n        } catch (Exception e) {\n          LOG.warn(\"Failed to register persistent function \" +\n              functionName + \":\" + function.getClassName() + \". Ignore and continue.\");\n        }\n      }\n    }\n  }",
                "code_after_change": "  public static void reloadFunctions() throws HiveException {\n    Hive db = Hive.get();\n    for (String dbName : db.getAllDatabases()) {\n      for (String functionName : db.getFunctions(dbName, \"*\")) {\n        Function function = db.getFunction(dbName, functionName);\n        try {\n\t  FunctionRegistry.registerPermanentFunction(\n\t      FunctionUtils.qualifyFunctionName(functionName, dbName), function.getClassName(),\n\t      false, FunctionTask.toFunctionResource(function.getResourceUris()));\n        } catch (Exception e) {\n          LOG.warn(\"Failed to register persistent function \" +\n              functionName + \":\" + function.getClassName() + \". Ignore and continue.\");\n        }\n      }\n    }\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Missing",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report mentions a NullPointerException occurring in the stack trace, specifically in methods like 'org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc.newInstance' and 'org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.getXpathOrFuncExprNodeDesc'. These methods are in the same stack trace context as the ground truth method 'ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.reloadFunctions', but they are not the exact root cause. The report does not provide any fix suggestion, nor does it identify the problem location precisely. There is no wrong information in the report as it accurately describes the observed behavior and the stack trace."
        }
    },
    {
        "filename": "HIVE-8771.json",
        "code_diff": {
            "ql.src.java.org.apache.hadoop.hive.ql.exec.AbstractFileMergeOperator.closeOp": {
                "code_before_change": "  public void closeOp(boolean abort) throws HiveException {\n    try {\n      if (!exception) {\n        FileStatus fss = fs.getFileStatus(outPath);\n        if (!fs.rename(outPath, finalPath)) {\n          throw new IOException(\n              \"Unable to rename \" + outPath + \" to \" + finalPath);\n        }\n        LOG.info(\"renamed path \" + outPath + \" to \" + finalPath + \" . File\" +\n            \" size is \"\n            + fss.getLen());\n\n        // move any incompatible files to final path\n        if (!incompatFileSet.isEmpty()) {\n          for (Path incompatFile : incompatFileSet) {\n            String fileName = incompatFile.getName();\n            Path destFile = new Path(finalPath.getParent(), fileName);\n            try {\n              Utilities.renameOrMoveFiles(fs, incompatFile, destFile);\n              LOG.info(\"Moved incompatible file \" + incompatFile + \" to \" +\n                  destFile);\n            } catch (HiveException e) {\n              LOG.error(\"Unable to move \" + incompatFile + \" to \" + destFile);\n              throw new IOException(e);\n            }\n          }\n        }\n      } else {\n        if (!autoDelete) {\n          fs.delete(outPath, true);\n        }\n      }\n    } catch (IOException e) {\n      throw new HiveException(\"Failed to close AbstractFileMergeOperator\", e);\n    }\n  }",
                "code_after_change": "  public void closeOp(boolean abort) throws HiveException {\n    try {\n      if (!exception) {\n        FileStatus fss = fs.getFileStatus(outPath);\n        if (!fs.rename(outPath, finalPath)) {\n          throw new IOException(\n              \"Unable to rename \" + outPath + \" to \" + finalPath);\n        }\n        LOG.info(\"renamed path \" + outPath + \" to \" + finalPath + \" . File\" +\n            \" size is \"\n            + fss.getLen());\n\n        // move any incompatible files to final path\n        if (!incompatFileSet.isEmpty()) {\n          for (Path incompatFile : incompatFileSet) {\n            Path destDir = finalPath.getParent();\n            try {\n              Utilities.renameOrMoveFiles(fs, incompatFile, destDir);\n              LOG.info(\"Moved incompatible file \" + incompatFile + \" to \" +\n                  destDir);\n            } catch (HiveException e) {\n              LOG.error(\"Unable to move \" + incompatFile + \" to \" + destDir);\n              throw new IOException(e);\n            }\n          }\n        }\n      } else {\n        if (!autoDelete) {\n          fs.delete(outPath, true);\n        }\n      }\n    } catch (IOException e) {\n      throw new HiveException(\"Failed to close AbstractFileMergeOperator\", e);\n    }\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report precisely identifies the root cause related to the ground truth method 'ql.src.java.org.apache.hadoop.hive.ql.exec.AbstractFileMergeOperator.closeOp', as it mentions the issue with moving/renaming files in the 'AbstractFileMergeOperator'. However, there is no explicit fix suggestion provided in the bug report, hence it is marked as 'Missing'. The problem location is also precisely identified as it directly relates to the method mentioned in the ground truth. There is no wrong information in the bug report as all details are consistent with the context of the bug."
        }
    },
    {
        "filename": "HIVE-8008.json",
        "code_diff": {
            "common.src.java.org.apache.hadoop.hive.common.type.HiveDecimal.enforcePrecisionScale": {
                "code_before_change": "  public static BigDecimal enforcePrecisionScale(BigDecimal bd, int maxPrecision, int maxScale) {\n    if (bd == null) {\n      return null;\n    }\n\n    int maxIntDigits = maxPrecision - maxScale;\n    int intDigits = bd.precision() - bd.scale();\n    if (intDigits > maxIntDigits) {\n      return null;\n    }\n\n    if (bd.scale() > maxScale) {\n      bd = bd.setScale(maxScale, RoundingMode.HALF_UP);\n    }\n\n    return bd;\n  }",
                "code_after_change": "  public static BigDecimal enforcePrecisionScale(BigDecimal bd, int maxPrecision, int maxScale) {\n    if (bd == null) {\n      return null;\n    }\n\n    if (bd.scale() > maxScale) {\n      bd = bd.setScale(maxScale, RoundingMode.HALF_UP);\n    }\n\n    int maxIntDigits = maxPrecision - maxScale;\n    int intDigits = bd.precision() - bd.scale();\n    if (intDigits > maxIntDigits) {\n      return null;\n    }\n\n    return bd;\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies a NullPointerException occurring in the stack trace, specifically mentioning methods like 'LazyUtils.writePrimitiveUTF8' and 'LazySimpleSerDe.serialize', which are in the same stack trace context as the ground truth method 'HiveDecimal.enforcePrecisionScale'. However, it does not precisely identify the root cause in the ground truth method. There is no fix suggestion provided in the bug report. The problem location is partially identified as it mentions methods in the shared stack trace context but not the exact ground truth method. There is no wrong information in the bug report as it accurately describes the error and its context."
        }
    },
    {
        "filename": "HIVE-6915.json",
        "code_diff": {
            "hbase-handler.src.java.org.apache.hadoop.hive.hbase.HBaseStorageHandler.configureJobConf": {
                "code_before_change": "  public void configureJobConf(TableDesc tableDesc, JobConf jobConf) {\n    try {\n      /*\n       * HIVE-6356\n       * The following code change is only needed for hbase-0.96.0 due to HBASE-9165, and\n       * will not be required once Hive bumps up its hbase version). At that time , we will\n       * only need TableMapReduceUtil.addDependencyJars(jobConf) here.\n       */\n      TableMapReduceUtil.addDependencyJars(\n          jobConf, HBaseStorageHandler.class, TableInputFormatBase.class);\n      Set<String> merged = new LinkedHashSet<String>(jobConf.getStringCollection(\"tmpjars\"));\n\n      Job copy = new Job(jobConf);\n      TableMapReduceUtil.addDependencyJars(copy);\n      merged.addAll(copy.getConfiguration().getStringCollection(\"tmpjars\"));\n      jobConf.set(\"tmpjars\", StringUtils.arrayToString(merged.toArray(new String[0])));\n    } catch (IOException e) {\n      throw new RuntimeException(e);\n    }\n  }",
                "code_after_change": "  public void configureJobConf(TableDesc tableDesc, JobConf jobConf) {\n    try {\n      HBaseSerDe.configureJobConf(tableDesc, jobConf);\n      /*\n       * HIVE-6356\n       * The following code change is only needed for hbase-0.96.0 due to HBASE-9165, and\n       * will not be required once Hive bumps up its hbase version). At that time , we will\n       * only need TableMapReduceUtil.addDependencyJars(jobConf) here.\n       */\n      TableMapReduceUtil.addDependencyJars(\n          jobConf, HBaseStorageHandler.class, TableInputFormatBase.class,\n          org.cliffc.high_scale_lib.Counter.class); // this will be removed for HBase 1.0\n      Set<String> merged = new LinkedHashSet<String>(jobConf.getStringCollection(\"tmpjars\"));\n\n      Job copy = new Job(jobConf);\n      TableMapReduceUtil.addDependencyJars(copy);\n      merged.addAll(copy.getConfiguration().getStringCollection(\"tmpjars\"));\n      jobConf.set(\"tmpjars\", StringUtils.arrayToString(merged.toArray(new String[0])));\n\n      // Get credentials using the configuration instance which has HBase properties\n      JobConf hbaseJobConf = new JobConf(getConf());\n      org.apache.hadoop.hbase.mapred.TableMapReduceUtil.initCredentials(hbaseJobConf);\n      jobConf.getCredentials().mergeAll(hbaseJobConf.getCredentials());\n    } catch (Exception e) {\n      throw new RuntimeException(e);\n    }\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.exec.tez.TezTask.build": {
                "code_before_change": "  DAG build(JobConf conf, TezWork work, Path scratchDir,\n      LocalResource appJarLr, List<LocalResource> additionalLr, Context ctx)\n      throws Exception {\n\n    perfLogger.PerfLogBegin(CLASS_NAME, PerfLogger.TEZ_BUILD_DAG);\n    Map<BaseWork, Vertex> workToVertex = new HashMap<BaseWork, Vertex>();\n    Map<BaseWork, JobConf> workToConf = new HashMap<BaseWork, JobConf>();\n\n    // getAllWork returns a topologically sorted list, which we use to make\n    // sure that vertices are created before they are used in edges.\n    List<BaseWork> ws = work.getAllWork();\n    Collections.reverse(ws);\n\n    FileSystem fs = scratchDir.getFileSystem(conf);\n\n    // the name of the dag is what is displayed in the AM/Job UI\n    DAG dag = new DAG(work.getName());\n\n    for (BaseWork w: ws) {\n\n      boolean isFinal = work.getLeaves().contains(w);\n\n      // translate work to vertex\n      perfLogger.PerfLogBegin(CLASS_NAME, PerfLogger.TEZ_CREATE_VERTEX + w.getName());\n\n      if (w instanceof UnionWork) {\n        // Special case for unions. These items translate to VertexGroups\n\n        List<BaseWork> unionWorkItems = new LinkedList<BaseWork>();\n        List<BaseWork> children = new LinkedList<BaseWork>();\n\n        // split the children into vertices that make up the union and vertices that are\n        // proper children of the union\n        for (BaseWork v: work.getChildren(w)) {\n          EdgeType type = work.getEdgeProperty(w, v).getEdgeType();\n          if (type == EdgeType.CONTAINS) {\n            unionWorkItems.add(v);\n          } else {\n            children.add(v);\n          }\n        }\n\n        // create VertexGroup\n        Vertex[] vertexArray = new Vertex[unionWorkItems.size()];\n\n        int i = 0;\n        for (BaseWork v: unionWorkItems) {\n          vertexArray[i++] = workToVertex.get(v);\n        }\n        VertexGroup group = dag.createVertexGroup(w.getName(), vertexArray);\n        \n        // now hook up the children\n        for (BaseWork v: children) {\n          // need to pairwise patch up the configuration of the vertices\n          for (BaseWork part: unionWorkItems) {\n            utils.updateConfigurationForEdge(workToConf.get(part), workToVertex.get(part), \n                 workToConf.get(v), workToVertex.get(v));\n          }\n          \n          // finally we can create the grouped edge\n          GroupInputEdge e = utils.createEdge(group, workToConf.get(v),\n               workToVertex.get(v), work.getEdgeProperty(w, v));\n\n          dag.addEdge(e);\n        }\n      } else {\n        // Regular vertices\n        JobConf wxConf = utils.initializeVertexConf(conf, w);\n        Vertex wx = utils.createVertex(wxConf, w, scratchDir, appJarLr, \n          additionalLr, fs, ctx, !isFinal, work);\n        dag.addVertex(wx);\n        utils.addCredentials(w, dag);\n        perfLogger.PerfLogEnd(CLASS_NAME, PerfLogger.TEZ_CREATE_VERTEX + w.getName());\n        workToVertex.put(w, wx);\n        workToConf.put(w, wxConf);\n        \n        // add all dependencies (i.e.: edges) to the graph\n        for (BaseWork v: work.getChildren(w)) {\n          assert workToVertex.containsKey(v);\n          Edge e = null;\n\n          TezEdgeProperty edgeProp = work.getEdgeProperty(w, v);\n\n          e = utils.createEdge(wxConf, wx, workToConf.get(v), workToVertex.get(v), edgeProp);\n          dag.addEdge(e);\n        }\n      }\n    }\n    perfLogger.PerfLogEnd(CLASS_NAME, PerfLogger.TEZ_BUILD_DAG);\n    return dag;\n  }",
                "code_after_change": "  DAG build(JobConf conf, TezWork work, Path scratchDir,\n      LocalResource appJarLr, List<LocalResource> additionalLr, Context ctx)\n      throws Exception {\n\n    perfLogger.PerfLogBegin(CLASS_NAME, PerfLogger.TEZ_BUILD_DAG);\n    Map<BaseWork, Vertex> workToVertex = new HashMap<BaseWork, Vertex>();\n    Map<BaseWork, JobConf> workToConf = new HashMap<BaseWork, JobConf>();\n\n    // getAllWork returns a topologically sorted list, which we use to make\n    // sure that vertices are created before they are used in edges.\n    List<BaseWork> ws = work.getAllWork();\n    Collections.reverse(ws);\n\n    FileSystem fs = scratchDir.getFileSystem(conf);\n\n    // the name of the dag is what is displayed in the AM/Job UI\n    DAG dag = DAG.create(work.getName());\n    dag.setCredentials(conf.getCredentials());\n\n    for (BaseWork w: ws) {\n\n      boolean isFinal = work.getLeaves().contains(w);\n\n      // translate work to vertex\n      perfLogger.PerfLogBegin(CLASS_NAME, PerfLogger.TEZ_CREATE_VERTEX + w.getName());\n\n      if (w instanceof UnionWork) {\n        // Special case for unions. These items translate to VertexGroups\n\n        List<BaseWork> unionWorkItems = new LinkedList<BaseWork>();\n        List<BaseWork> children = new LinkedList<BaseWork>();\n\n        // split the children into vertices that make up the union and vertices that are\n        // proper children of the union\n        for (BaseWork v: work.getChildren(w)) {\n          EdgeType type = work.getEdgeProperty(w, v).getEdgeType();\n          if (type == EdgeType.CONTAINS) {\n            unionWorkItems.add(v);\n          } else {\n            children.add(v);\n          }\n        }\n\n        // create VertexGroup\n        Vertex[] vertexArray = new Vertex[unionWorkItems.size()];\n\n        int i = 0;\n        for (BaseWork v: unionWorkItems) {\n          vertexArray[i++] = workToVertex.get(v);\n        }\n        VertexGroup group = dag.createVertexGroup(w.getName(), vertexArray);\n\n        // For a vertex group, all Outputs use the same Key-class, Val-class and partitioner.\n        // Pick any one source vertex to figure out the Edge configuration.\n        JobConf parentConf = workToConf.get(unionWorkItems.get(0));\n\n        // now hook up the children\n        for (BaseWork v: children) {\n          // finally we can create the grouped edge\n          GroupInputEdge e = utils.createEdge(group, parentConf,\n               workToVertex.get(v), work.getEdgeProperty(w, v), work.getVertexType(v));\n\n          dag.addEdge(e);\n        }\n      } else {\n        // Regular vertices\n        JobConf wxConf = utils.initializeVertexConf(conf, ctx, w);\n        Vertex wx =\n            utils.createVertex(wxConf, w, scratchDir, appJarLr, additionalLr, fs, ctx, !isFinal,\n                work, work.getVertexType(w));\n        dag.addVertex(wx);\n        utils.addCredentials(w, dag);\n        perfLogger.PerfLogEnd(CLASS_NAME, PerfLogger.TEZ_CREATE_VERTEX + w.getName());\n        workToVertex.put(w, wx);\n        workToConf.put(w, wxConf);\n\n        // add all dependencies (i.e.: edges) to the graph\n        for (BaseWork v: work.getChildren(w)) {\n          assert workToVertex.containsKey(v);\n          Edge e = null;\n\n          TezEdgeProperty edgeProp = work.getEdgeProperty(w, v);\n\n          e = utils.createEdge(wxConf, wx, workToVertex.get(v), edgeProp, work.getVertexType(v));\n          dag.addEdge(e);\n        }\n      }\n    }\n    perfLogger.PerfLogEnd(CLASS_NAME, PerfLogger.TEZ_BUILD_DAG);\n    return dag;\n  }"
            },
            "hbase-handler.src.java.org.apache.hadoop.hive.hbase.HiveHBaseTableInputFormat.newIndexPredicateAnalyzer": {
                "code_before_change": "  static IndexPredicateAnalyzer newIndexPredicateAnalyzer(\n    String keyColumnName, String keyColType, boolean isKeyBinary) {\n\n    IndexPredicateAnalyzer analyzer = new IndexPredicateAnalyzer();\n\n    // We can always do equality predicate. Just need to make sure we get appropriate\n    // BA representation of constant of filter condition.\n    analyzer.addComparisonOp(\"org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPEqual\");\n    // We can do other comparisons only if storage format in hbase is either binary\n    // or we are dealing with string types since there lexographic ordering will suffice.\n    if(isKeyBinary || (keyColType.equalsIgnoreCase(\"string\"))){\n      analyzer.addComparisonOp(\"org.apache.hadoop.hive.ql.udf.generic.\" +\n        \"GenericUDFOPEqualOrGreaterThan\");\n      analyzer.addComparisonOp(\"org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPEqualOrLessThan\");\n      analyzer.addComparisonOp(\"org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPLessThan\");\n      analyzer.addComparisonOp(\"org.apache.hadoop.hive.ql.udf.generic.GenericUDFOPGreaterThan\");\n    }\n\n    // and only on the key column\n    analyzer.clearAllowedColumnNames();\n    analyzer.allowColumnName(keyColumnName);\n\n    return analyzer;\n  }",
                "code_after_change": "  static IndexPredicateAnalyzer newIndexPredicateAnalyzer(\n      String keyColumnName, TypeInfo keyColType, boolean isKeyBinary) {\n    return newIndexPredicateAnalyzer(keyColumnName, keyColType.getTypeName(), isKeyBinary);\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the issue as related to SASL authentication failure due to missing or invalid credentials, which is mentioned in the stack trace context. However, it does not precisely identify the root cause in the ground truth methods. The fix suggestion is missing as there is no suggestion provided in the bug report. The problem location is partially identified as it mentions methods in the stack trace context but not the exact ground truth methods. There is no wrong information in the bug report as all the information is relevant to the context of the bug."
        }
    },
    {
        "filename": "HIVE-12364.json",
        "code_diff": {
            "shims.0.23.src.main.java.org.apache.hadoop.hive.shims.Hadoop23Shims.runDistCp": {
                "code_before_change": [],
                "code_after_change": []
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the issue occurring during a distcp operation, which is related to the ground truth method 'Hadoop23Shims.runDistCp'. However, it does not precisely identify this method as the root cause, instead mentioning methods in the same stack trace context, such as 'Hive.moveFile'. There is no fix suggestion provided in the bug report. The problem location is partially identified as it mentions methods in the shared stack trace context but not the exact ground truth method. There is no wrong information in the bug report as it accurately describes the failure context."
        }
    },
    {
        "filename": "HIVE-8766.json",
        "code_diff": {
            "metastore.src.java.org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke": {
                "code_before_change": "  public Object invoke(final Object proxy, final Method method, final Object[] args) throws Throwable {\n\n    boolean gotNewConnectUrl = false;\n    boolean reloadConf = HiveConf.getBoolVar(origConf,\n        HiveConf.ConfVars.HMSHANDLERFORCERELOADCONF);\n    long retryInterval = HiveConf.getTimeVar(origConf,\n        HiveConf.ConfVars.HMSHANDLERINTERVAL, TimeUnit.MILLISECONDS);\n    int retryLimit = HiveConf.getIntVar(origConf,\n        HiveConf.ConfVars.HMSHANDLERATTEMPTS);\n\n    if (reloadConf) {\n      MetaStoreInit.updateConnectionURL(origConf, getActiveConf(),\n        null, metaStoreInitData);\n    }\n\n    int retryCount = 0;\n    Throwable caughtException = null;\n    while (true) {\n      try {\n        if (reloadConf || gotNewConnectUrl) {\n          baseHandler.setConf(getActiveConf());\n        }\n        return method.invoke(baseHandler, args);\n\n      } catch (javax.jdo.JDOException e) {\n        caughtException = e;\n      } catch (UndeclaredThrowableException e) {\n        if (e.getCause() != null) {\n          if (e.getCause() instanceof javax.jdo.JDOException) {\n            // Due to reflection, the jdo exception is wrapped in\n            // invocationTargetException\n            caughtException = e.getCause();\n          } else if (e.getCause() instanceof MetaException && e.getCause().getCause() != null\n              && e.getCause().getCause() instanceof javax.jdo.JDOException) {\n            // The JDOException may be wrapped further in a MetaException\n            caughtException = e.getCause().getCause();\n          } else {\n            LOG.error(ExceptionUtils.getStackTrace(e.getCause()));\n            throw e.getCause();\n          }\n        } else {\n          LOG.error(ExceptionUtils.getStackTrace(e));\n          throw e;\n        }\n      } catch (InvocationTargetException e) {\n        if (e.getCause() instanceof javax.jdo.JDOException) {\n          // Due to reflection, the jdo exception is wrapped in\n          // invocationTargetException\n          caughtException = e.getCause();\n        } else if (e.getCause() instanceof NoSuchObjectException || e.getTargetException().getCause() instanceof NoSuchObjectException) {\n          String methodName = method.getName();\n          if (!methodName.startsWith(\"get_table\") && !methodName.startsWith(\"get_partition\") && !methodName.startsWith(\"get_function\")) {\n            LOG.error(ExceptionUtils.getStackTrace(e.getCause()));\n          }\n          throw e.getCause();\n        } else if (e.getCause() instanceof MetaException && e.getCause().getCause() != null\n            && e.getCause().getCause() instanceof javax.jdo.JDOException) {\n          // The JDOException may be wrapped further in a MetaException\n          caughtException = e.getCause().getCause();\n        } else {\n          LOG.error(ExceptionUtils.getStackTrace(e.getCause()));\n          throw e.getCause();\n        }\n      }\n\n      if (retryCount >= retryLimit) {\n        LOG.error(\"HMSHandler Fatal error: \" + ExceptionUtils.getStackTrace(caughtException));\n        // Since returning exceptions with a nested \"cause\" can be a problem in\n        // Thrift, we are stuffing the stack trace into the message itself.\n        throw new MetaException(ExceptionUtils.getStackTrace(caughtException));\n      }\n\n      assert (retryInterval >= 0);\n      retryCount++;\n      LOG.error(\n        String.format(\n          \"Retrying HMSHandler after %d ms (attempt %d of %d)\", retryInterval, retryCount, retryLimit) +\n          \" with error: \" + ExceptionUtils.getStackTrace(caughtException));\n\n      Thread.sleep(retryInterval);\n      // If we have a connection error, the JDO connection URL hook might\n      // provide us with a new URL to access the datastore.\n      String lastUrl = MetaStoreInit.getConnectionURL(getActiveConf());\n      gotNewConnectUrl = MetaStoreInit.updateConnectionURL(origConf, getActiveConf(),\n        lastUrl, metaStoreInitData);\n    }\n  }",
                "code_after_change": "  public Object invoke(final Object proxy, final Method method, final Object[] args) throws Throwable {\n\n    boolean gotNewConnectUrl = false;\n    boolean reloadConf = HiveConf.getBoolVar(origConf,\n        HiveConf.ConfVars.HMSHANDLERFORCERELOADCONF);\n    long retryInterval = HiveConf.getTimeVar(origConf,\n        HiveConf.ConfVars.HMSHANDLERINTERVAL, TimeUnit.MILLISECONDS);\n    int retryLimit = HiveConf.getIntVar(origConf,\n        HiveConf.ConfVars.HMSHANDLERATTEMPTS);\n\n    if (reloadConf) {\n      MetaStoreInit.updateConnectionURL(origConf, getActiveConf(),\n        null, metaStoreInitData);\n    }\n\n    int retryCount = 0;\n    Throwable caughtException = null;\n    while (true) {\n      try {\n        if (reloadConf || gotNewConnectUrl) {\n          baseHandler.setConf(getActiveConf());\n        }\n        return method.invoke(baseHandler, args);\n\n      } catch (javax.jdo.JDOException e) {\n        caughtException = e;\n      } catch (UndeclaredThrowableException e) {\n        if (e.getCause() != null) {\n          if (e.getCause() instanceof javax.jdo.JDOException) {\n            // Due to reflection, the jdo exception is wrapped in\n            // invocationTargetException\n            caughtException = e.getCause();\n          } else if (e.getCause() instanceof MetaException && e.getCause().getCause() != null\n              && e.getCause().getCause() instanceof javax.jdo.JDOException) {\n            // The JDOException may be wrapped further in a MetaException\n            caughtException = e.getCause().getCause();\n          } else {\n            LOG.error(ExceptionUtils.getStackTrace(e.getCause()));\n            throw e.getCause();\n          }\n        } else {\n          LOG.error(ExceptionUtils.getStackTrace(e));\n          throw e;\n        }\n      } catch (InvocationTargetException e) {\n        if (e.getCause() instanceof javax.jdo.JDOException) {\n          // Due to reflection, the jdo exception is wrapped in\n          // invocationTargetException\n          caughtException = e.getCause();\n        } else if (e.getCause() instanceof NoSuchObjectException || e.getTargetException().getCause() instanceof NoSuchObjectException) {\n          String methodName = method.getName();\n          if (!methodName.startsWith(\"get_table\") && !methodName.startsWith(\"get_partition\") && !methodName.startsWith(\"get_function\")) {\n            LOG.error(ExceptionUtils.getStackTrace(e.getCause()));\n          }\n          throw e.getCause();\n        } else if (e.getCause() instanceof MetaException && e.getCause().getCause() != null\n            && (e.getCause().getCause() instanceof javax.jdo.JDOException || \n            \te.getCause().getCause() instanceof NucleusException)) {\n          // The JDOException or the Nucleus Exception may be wrapped further in a MetaException\n          caughtException = e.getCause().getCause();\n        } else {\n          LOG.error(ExceptionUtils.getStackTrace(e.getCause()));\n          throw e.getCause();\n        }\n      }\n\n      if (retryCount >= retryLimit) {\n        LOG.error(\"HMSHandler Fatal error: \" + ExceptionUtils.getStackTrace(caughtException));\n        // Since returning exceptions with a nested \"cause\" can be a problem in\n        // Thrift, we are stuffing the stack trace into the message itself.\n        throw new MetaException(ExceptionUtils.getStackTrace(caughtException));\n      }\n\n      assert (retryInterval >= 0);\n      retryCount++;\n      LOG.error(\n        String.format(\n          \"Retrying HMSHandler after %d ms (attempt %d of %d)\", retryInterval, retryCount, retryLimit) +\n          \" with error: \" + ExceptionUtils.getStackTrace(caughtException));\n\n      Thread.sleep(retryInterval);\n      // If we have a connection error, the JDO connection URL hook might\n      // provide us with a new URL to access the datastore.\n      String lastUrl = MetaStoreInit.getConnectionURL(getActiveConf());\n      gotNewConnectUrl = MetaStoreInit.updateConnectionURL(origConf, getActiveConf(),\n        lastUrl, metaStoreInitData);\n    }\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Correct",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report precisely identifies the root cause by mentioning the need for retries in the 'RetryingHMSHandler' when a 'NucleusException' occurs, which is directly related to the ground truth method 'metastore.src.java.org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke'. The fix suggestion in the report aligns with the developer's fix, which involves handling 'NucleusException' in the retry mechanism. The problem location is also precisely identified as it directly refers to the 'RetryingHMSHandler'. There is no wrong information in the bug report as all details are relevant and accurate."
        }
    },
    {
        "filename": "HIVE-5857.json",
        "code_diff": {
            "ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.getBaseWork": {
                "code_before_change": "  private static BaseWork getBaseWork(Configuration conf, String name) {\n    BaseWork gWork = null;\n    Path path = null;\n    InputStream in = null;\n    try {\n      path = getPlanPath(conf, name);\n      assert path != null;\n      gWork = gWorkMap.get(path);\n      if (gWork == null) {\n        Path localPath;\n        if (ShimLoader.getHadoopShims().isLocalMode(conf)) {\n          localPath = path;\n        } else {\n          localPath = new Path(name);\n        }\n        in = new FileInputStream(localPath.toUri().getPath());\n        if(MAP_PLAN_NAME.equals(name)){\n          if (ExecMapper.class.getName().equals(conf.get(MAPRED_MAPPER_CLASS))){\n            gWork = deserializePlan(in, MapWork.class, conf);\n          } else if(RCFileMergeMapper.class.getName().equals(conf.get(MAPRED_MAPPER_CLASS))) {\n            gWork = deserializePlan(in, MergeWork.class, conf);\n          } else if(ColumnTruncateMapper.class.getName().equals(conf.get(MAPRED_MAPPER_CLASS))) {\n            gWork = deserializePlan(in, ColumnTruncateWork.class, conf);\n          } else if(PartialScanMapper.class.getName().equals(conf.get(MAPRED_MAPPER_CLASS))) {\n            gWork = deserializePlan(in, PartialScanWork.class,conf);\n          } else {\n            throw new RuntimeException(\"unable to determine work from configuration .\"\n                + MAPRED_MAPPER_CLASS + \" was \"+ conf.get(MAPRED_MAPPER_CLASS)) ;\n          }\n        } else if (REDUCE_PLAN_NAME.equals(name)) {\n          if(ExecReducer.class.getName().equals(conf.get(MAPRED_REDUCER_CLASS))) {\n            gWork = deserializePlan(in, ReduceWork.class, conf);\n          } else {\n            throw new RuntimeException(\"unable to determine work from configuration .\"\n                + MAPRED_REDUCER_CLASS +\" was \"+ conf.get(MAPRED_REDUCER_CLASS)) ;\n          }\n        }\n        gWorkMap.put(path, gWork);\n      }\n      return gWork;\n    } catch (FileNotFoundException fnf) {\n      // happens. e.g.: no reduce work.\n      LOG.debug(\"No plan file found: \"+path);\n      return null;\n    } catch (Exception e) {\n      LOG.error(\"Failed to load plan: \"+path, e);\n      throw new RuntimeException(e);\n    } finally {\n      if (in != null) {\n        try {\n          in.close();\n        } catch (IOException cantBlameMeForTrying) { }\n      }\n    }\n  }",
                "code_after_change": "  private static BaseWork getBaseWork(Configuration conf, String name) {\n    BaseWork gWork = null;\n    Path path = null;\n    InputStream in = null;\n    try {\n      path = getPlanPath(conf, name);\n      assert path != null;\n      if (!gWorkMap.containsKey(path)) {\n        Path localPath;\n        if (conf.getBoolean(\"mapreduce.task.uberized\", false) && name.equals(REDUCE_PLAN_NAME)) {\n          localPath = new Path(name);\n        } else if (ShimLoader.getHadoopShims().isLocalMode(conf)) {\n          localPath = path;\n        } else {\n          localPath = new Path(name);\n        }\n\n        if (HiveConf.getBoolVar(conf, ConfVars.HIVE_RPC_QUERY_PLAN)) {\n          LOG.debug(\"Loading plan from string: \"+path.toUri().getPath());\n          String planString = conf.get(path.toUri().getPath());\n          if (planString == null) {\n            LOG.info(\"Could not find plan string in conf\");\n            return null;\n          }\n          byte[] planBytes = Base64.decodeBase64(planString);\n          in = new ByteArrayInputStream(planBytes);\n          in = new InflaterInputStream(in);\n        } else {\n          in = new FileInputStream(localPath.toUri().getPath());\n        }\n\n        if(MAP_PLAN_NAME.equals(name)){\n          if (ExecMapper.class.getName().equals(conf.get(MAPRED_MAPPER_CLASS))){\n            gWork = deserializePlan(in, MapWork.class, conf);\n          } else if(RCFileMergeMapper.class.getName().equals(conf.get(MAPRED_MAPPER_CLASS)) ||\n              OrcFileMergeMapper.class.getName().equals(conf.get(MAPRED_MAPPER_CLASS))) {\n            gWork = deserializePlan(in, MergeWork.class, conf);\n          } else if(ColumnTruncateMapper.class.getName().equals(conf.get(MAPRED_MAPPER_CLASS))) {\n            gWork = deserializePlan(in, ColumnTruncateWork.class, conf);\n          } else if(PartialScanMapper.class.getName().equals(conf.get(MAPRED_MAPPER_CLASS))) {\n            gWork = deserializePlan(in, PartialScanWork.class,conf);\n          } else {\n            throw new RuntimeException(\"unable to determine work from configuration .\"\n                + MAPRED_MAPPER_CLASS + \" was \"+ conf.get(MAPRED_MAPPER_CLASS)) ;\n          }\n        } else if (REDUCE_PLAN_NAME.equals(name)) {\n          if(ExecReducer.class.getName().equals(conf.get(MAPRED_REDUCER_CLASS))) {\n            gWork = deserializePlan(in, ReduceWork.class, conf);\n          } else {\n            throw new RuntimeException(\"unable to determine work from configuration .\"\n                + MAPRED_REDUCER_CLASS +\" was \"+ conf.get(MAPRED_REDUCER_CLASS)) ;\n          }\n        }\n        gWorkMap.put(path, gWork);\n      } else {\n        LOG.debug(\"Found plan in cache.\");\n        gWork = gWorkMap.get(path);\n      }\n      return gWork;\n    } catch (FileNotFoundException fnf) {\n      // happens. e.g.: no reduce work.\n      LOG.info(\"No plan file found: \"+path);\n      return null;\n    } catch (Exception e) {\n      LOG.error(\"Failed to load plan: \"+path, e);\n      throw new RuntimeException(e);\n    } finally {\n      if (in != null) {\n        try {\n          in.close();\n        } catch (IOException cantBlameMeForTrying) { }\n      }\n    }\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Alternative Fix",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report precisely identifies the root cause as the `Utilities.getBaseWork` method returning null due to a `FileNotFoundException`, which aligns with the ground truth method. The fix suggestion in the report suggests adding an if-branch to check for uber mode and look for a plan file in a different location, which is an alternative to the developer's fix that checks for 'mapreduce.task.uberized' and adjusts the path accordingly. The problem location is precisely identified as the `Utilities.getBaseWork` method, which is the ground truth method. There is no wrong information in the bug report as all details are relevant and accurate."
        }
    },
    {
        "filename": "HIVE-1547.json",
        "code_diff": {
            "ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.unarchive": {
                "code_before_change": "  private int unarchive(Hive db, AlterTableSimpleDesc simpleDesc)\n      throws HiveException {\n    String dbName = simpleDesc.getDbName();\n    String tblName = simpleDesc.getTableName();\n\n    Table tbl = db.getTable(dbName, tblName);\n    validateAlterTableType(tbl, AlterTableDesc.AlterTableTypes.UNARCHIVE);\n\n    // Means user specified a table, not a partition\n    if (simpleDesc.getPartSpec() == null) {\n      throw new HiveException(\"ARCHIVE is for partitions only\");\n    }\n\n    Map<String, String> partSpec = simpleDesc.getPartSpec();\n    Partition p = db.getPartition(tbl, partSpec, false);\n\n    if (tbl.getTableType() != TableType.MANAGED_TABLE) {\n      throw new HiveException(\"UNARCHIVE can only be performed on managed tables\");\n    }\n\n    if (p == null) {\n      throw new HiveException(\"Specified partition does not exist\");\n    }\n\n    if (!isArchived(p)) {\n      Path location = new Path(p.getLocation());\n      Path leftOverArchiveDir = new Path(location.getParent(),\n          location.getName() + INTERMEDIATE_ARCHIVED_DIR_SUFFIX);\n\n      if (pathExists(leftOverArchiveDir)) {\n        console.printInfo(\"Deleting \" + leftOverArchiveDir + \" left over \" +\n        \"from a previous unarchiving operation\");\n        deleteDir(leftOverArchiveDir);\n      }\n\n      throw new HiveException(\"Specified partition is not archived\");\n    }\n\n    Path originalLocation = new Path(getOriginalLocation(p));\n    Path sourceDir = new Path(p.getLocation());\n    Path intermediateArchiveDir = new Path(originalLocation.getParent(),\n        originalLocation.getName() + INTERMEDIATE_ARCHIVED_DIR_SUFFIX);\n    Path intermediateExtractedDir = new Path(originalLocation.getParent(),\n        originalLocation.getName() + INTERMEDIATE_EXTRACTED_DIR_SUFFIX);\n\n    Path tmpDir = new Path(driverContext\n          .getCtx()\n          .getExternalTmpFileURI(originalLocation.toUri()));\n\n    FileSystem fs = null;\n    try {\n      fs = tmpDir.getFileSystem(conf);\n      // Verify that there are no files in the tmp dir, because if there are, it\n      // would be copied to the partition\n      FileStatus [] filesInTmpDir = fs.listStatus(tmpDir);\n      if (filesInTmpDir.length != 0) {\n        for (FileStatus file : filesInTmpDir) {\n          console.printInfo(file.getPath().toString());\n        }\n        throw new HiveException(\"Temporary directory \" + tmpDir + \" is not empty\");\n      }\n\n    } catch (IOException e) {\n      throw new HiveException(e);\n    }\n\n    // Some sanity checks\n    if (originalLocation == null) {\n      throw new HiveException(\"Missing archive data in the partition\");\n    }\n    if (!\"har\".equals(sourceDir.toUri().getScheme())) {\n      throw new HiveException(\"Location should refer to a HAR\");\n    }\n\n    // Clarification of terms:\n    // - The originalLocation directory represents the original directory of the\n    //   partition's files. They now contain an archived version of those files\n    //   eg. hdfs:/warehouse/myTable/ds=1/\n    // - The source directory is the directory containing all the files that\n    //   should be in the partition. e.g. har:/warehouse/myTable/ds=1/myTable.har/\n    //   Note the har:/ scheme\n\n    // Steps:\n    // 1. Extract the archive in a temporary folder\n    // 2. Move the archive dir to an intermediate dir that is in at the same\n    //    dir as originalLocation. Call the new dir intermediate-extracted.\n    // 3. Rename the original partition dir to an intermediate dir. Call the\n    //    renamed dir intermediate-archive\n    // 4. Rename intermediate-extracted to the original partition dir\n    // 5. Change the metadata\n    // 6. Delete the archived partition files in intermediate-archive\n\n    if (!pathExists(intermediateExtractedDir) &&\n        !pathExists(intermediateArchiveDir)) {\n      try {\n\n        // Copy the files out of the archive into the temporary directory\n        String copySource = (new Path(sourceDir, \"*\")).toString();\n        String copyDest = tmpDir.toString();\n        List<String> args = new ArrayList<String>();\n        args.add(\"-cp\");\n        args.add(copySource);\n        args.add(copyDest);\n\n        console.printInfo(\"Copying \" + copySource + \" to \" + copyDest);\n        FsShell fss = new FsShell(conf);\n        int ret = 0;\n        try {\n          ret = ToolRunner.run(fss, args.toArray(new String[0]));\n        } catch (Exception e) {\n          throw new HiveException(e);\n        }\n        if (ret != 0) {\n          throw new HiveException(\"Error while copying files from archive\");\n        }\n\n        console.printInfo(\"Moving \" + tmpDir + \" to \" + intermediateExtractedDir);\n        if (fs.exists(intermediateExtractedDir)) {\n          throw new HiveException(\"Invalid state: the intermediate extracted \" +\n              \"directory already exists.\");\n        }\n        fs.rename(tmpDir, intermediateExtractedDir);\n      } catch (Exception e) {\n        throw new HiveException(e);\n      }\n    }\n\n    // At this point, we know that the extracted files are in the intermediate\n    // extracted dir, or in the the original directory.\n\n    if (!pathExists(intermediateArchiveDir)) {\n      try {\n        console.printInfo(\"Moving \" + originalLocation + \" to \" + intermediateArchiveDir);\n        fs.rename(originalLocation, intermediateArchiveDir);\n      } catch (IOException e) {\n        throw new HiveException(e);\n      }\n    } else {\n      console.printInfo(intermediateArchiveDir + \" already exists. \" +\n      \"Assuming it contains the archived version of the partition\");\n    }\n\n    // If there is a failure from here to until when the metadata is changed,\n    // the partition will be empty or throw errors on read.\n\n    // If the original location exists here, then it must be the extracted files\n    // because in the previous step, we moved the previous original location\n    // (containing the archived version of the files) to intermediateArchiveDir\n    if (!pathExists(originalLocation)) {\n      try {\n        console.printInfo(\"Moving \" + intermediateExtractedDir + \" to \" + originalLocation);\n        fs.rename(intermediateExtractedDir, originalLocation);\n      } catch (IOException e) {\n        throw new HiveException(e);\n      }\n    } else {\n      console.printInfo(originalLocation + \" already exists. \" +\n      \"Assuming it contains the extracted files in the partition\");\n    }\n\n    setUnArchived(p);\n    try {\n      db.alterPartition(tblName, p);\n    } catch (InvalidOperationException e) {\n      throw new HiveException(e);\n    }\n    // If a failure happens here, the intermediate archive files won't be\n    // deleted. The user will need to call unarchive again to clear those up.\n    deleteDir(intermediateArchiveDir);\n\n    return 0;\n  }",
                "code_after_change": "  private int unarchive(Hive db, AlterTableSimpleDesc simpleDesc)\n      throws HiveException {\n    String dbName = simpleDesc.getDbName();\n    String tblName = simpleDesc.getTableName();\n\n    Table tbl = db.getTable(dbName, tblName);\n    validateAlterTableType(tbl, AlterTableDesc.AlterTableTypes.UNARCHIVE);\n\n    // Means user specified a table, not a partition\n    if (simpleDesc.getPartSpec() == null) {\n      throw new HiveException(\"ARCHIVE is for partitions only\");\n    }\n\n    Map<String, String> partSpec = simpleDesc.getPartSpec();\n    Partition p = db.getPartition(tbl, partSpec, false);\n\n    if (tbl.getTableType() != TableType.MANAGED_TABLE) {\n      throw new HiveException(\"UNARCHIVE can only be performed on managed tables\");\n    }\n\n    if (p == null) {\n      throw new HiveException(\"Specified partition does not exist\");\n    }\n\n    if (!isArchived(p)) {\n      Path location = new Path(p.getLocation());\n      Path leftOverArchiveDir = new Path(location.getParent(),\n          location.getName() + INTERMEDIATE_ARCHIVED_DIR_SUFFIX);\n\n      if (pathExists(leftOverArchiveDir)) {\n        console.printInfo(\"Deleting \" + leftOverArchiveDir + \" left over \" +\n        \"from a previous unarchiving operation\");\n        deleteDir(leftOverArchiveDir);\n      }\n\n      throw new HiveException(\"Specified partition is not archived\");\n    }\n\n    Path originalLocation = new Path(getOriginalLocation(p));\n    Path sourceDir = new Path(p.getLocation());\n    Path intermediateArchiveDir = new Path(originalLocation.getParent(),\n        originalLocation.getName() + INTERMEDIATE_ARCHIVED_DIR_SUFFIX);\n    Path intermediateExtractedDir = new Path(originalLocation.getParent(),\n        originalLocation.getName() + INTERMEDIATE_EXTRACTED_DIR_SUFFIX);\n\n    Path tmpDir = new Path(driverContext\n          .getCtx()\n          .getExternalTmpFileURI(originalLocation.toUri()));\n\n    FileSystem fs = null;\n    try {\n      fs = tmpDir.getFileSystem(conf);\n      // Verify that there are no files in the tmp dir, because if there are, it\n      // would be copied to the partition\n      FileStatus [] filesInTmpDir = fs.listStatus(tmpDir);\n      if (filesInTmpDir != null && filesInTmpDir.length != 0) {\n        for (FileStatus file : filesInTmpDir) {\n          console.printInfo(file.getPath().toString());\n        }\n        throw new HiveException(\"Temporary directory \" + tmpDir + \" is not empty\");\n      }\n\n    } catch (IOException e) {\n      throw new HiveException(e);\n    }\n\n    // Some sanity checks\n    if (originalLocation == null) {\n      throw new HiveException(\"Missing archive data in the partition\");\n    }\n    if (!\"har\".equals(sourceDir.toUri().getScheme())) {\n      throw new HiveException(\"Location should refer to a HAR\");\n    }\n\n    // Clarification of terms:\n    // - The originalLocation directory represents the original directory of the\n    //   partition's files. They now contain an archived version of those files\n    //   eg. hdfs:/warehouse/myTable/ds=1/\n    // - The source directory is the directory containing all the files that\n    //   should be in the partition. e.g. har:/warehouse/myTable/ds=1/myTable.har/\n    //   Note the har:/ scheme\n\n    // Steps:\n    // 1. Extract the archive in a temporary folder\n    // 2. Move the archive dir to an intermediate dir that is in at the same\n    //    dir as originalLocation. Call the new dir intermediate-extracted.\n    // 3. Rename the original partition dir to an intermediate dir. Call the\n    //    renamed dir intermediate-archive\n    // 4. Rename intermediate-extracted to the original partition dir\n    // 5. Change the metadata\n    // 6. Delete the archived partition files in intermediate-archive\n\n    if (!pathExists(intermediateExtractedDir) &&\n        !pathExists(intermediateArchiveDir)) {\n      try {\n\n        // Copy the files out of the archive into the temporary directory\n        String copySource = (new Path(sourceDir, \"*\")).toString();\n        String copyDest = tmpDir.toString();\n        List<String> args = new ArrayList<String>();\n        args.add(\"-cp\");\n        args.add(copySource);\n        args.add(copyDest);\n\n        console.printInfo(\"Copying \" + copySource + \" to \" + copyDest);\n        FsShell fss = new FsShell(conf);\n        int ret = 0;\n        try {\n          ret = ToolRunner.run(fss, args.toArray(new String[0]));\n        } catch (Exception e) {\n          throw new HiveException(e);\n        }\n        if (ret != 0) {\n          throw new HiveException(\"Error while copying files from archive\");\n        }\n\n        console.printInfo(\"Moving \" + tmpDir + \" to \" + intermediateExtractedDir);\n        if (fs.exists(intermediateExtractedDir)) {\n          throw new HiveException(\"Invalid state: the intermediate extracted \" +\n              \"directory already exists.\");\n        }\n        fs.rename(tmpDir, intermediateExtractedDir);\n      } catch (Exception e) {\n        throw new HiveException(e);\n      }\n    }\n\n    // At this point, we know that the extracted files are in the intermediate\n    // extracted dir, or in the the original directory.\n\n    if (!pathExists(intermediateArchiveDir)) {\n      try {\n        console.printInfo(\"Moving \" + originalLocation + \" to \" + intermediateArchiveDir);\n        fs.rename(originalLocation, intermediateArchiveDir);\n      } catch (IOException e) {\n        throw new HiveException(e);\n      }\n    } else {\n      console.printInfo(intermediateArchiveDir + \" already exists. \" +\n      \"Assuming it contains the archived version of the partition\");\n    }\n\n    // If there is a failure from here to until when the metadata is changed,\n    // the partition will be empty or throw errors on read.\n\n    // If the original location exists here, then it must be the extracted files\n    // because in the previous step, we moved the previous original location\n    // (containing the archived version of the files) to intermediateArchiveDir\n    if (!pathExists(originalLocation)) {\n      try {\n        console.printInfo(\"Moving \" + intermediateExtractedDir + \" to \" + originalLocation);\n        fs.rename(intermediateExtractedDir, originalLocation);\n      } catch (IOException e) {\n        throw new HiveException(e);\n      }\n    } else {\n      console.printInfo(originalLocation + \" already exists. \" +\n      \"Assuming it contains the extracted files in the partition\");\n    }\n\n    setUnArchived(p);\n    try {\n      db.alterPartition(tblName, p);\n    } catch (InvalidOperationException e) {\n      throw new HiveException(e);\n    }\n    // If a failure happens here, the intermediate archive files won't be\n    // deleted. The user will need to call unarchive again to clear those up.\n    deleteDir(intermediateArchiveDir);\n\n    return 0;\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.exec.DDLTask.alterTable": {
                "code_before_change": "  private int alterTable(Hive db, AlterTableDesc alterTbl) throws HiveException {\n    // alter the table\n    Table tbl = db.getTable(MetaStoreUtils.DEFAULT_DATABASE_NAME, alterTbl\n        .getOldName());\n    \n    Partition part = null;\n    if(alterTbl.getPartSpec() != null) {\n      part = db.getPartition(tbl, alterTbl.getPartSpec(), false);\n      if(part == null) {\n        console.printError(\"Partition : \" + alterTbl.getPartSpec().toString()\n            + \" does not exist.\");\n        return 1;\n      }\n    }\n\n    validateAlterTableType(tbl, alterTbl.getOp());\n\n    if (tbl.isView()) {\n      if (!alterTbl.getExpectView()) {\n        throw new HiveException(\"Cannot alter a view with ALTER TABLE\");\n      }\n    } else {\n      if (alterTbl.getExpectView()) {\n        throw new HiveException(\"Cannot alter a base table with ALTER VIEW\");\n      }\n    }\n\n    Table oldTbl = tbl.copy();\n\n    if (alterTbl.getOp() == AlterTableDesc.AlterTableTypes.RENAME) {\n      tbl.setTableName(alterTbl.getNewName());\n    } else if (alterTbl.getOp() == AlterTableDesc.AlterTableTypes.ADDCOLS) {\n      List<FieldSchema> newCols = alterTbl.getNewCols();\n      List<FieldSchema> oldCols = tbl.getCols();\n      if (tbl.getSerializationLib().equals(\n          \"org.apache.hadoop.hive.serde.thrift.columnsetSerDe\")) {\n        console\n            .printInfo(\"Replacing columns for columnsetSerDe and changing to LazySimpleSerDe\");\n        tbl.setSerializationLib(LazySimpleSerDe.class.getName());\n        tbl.getTTable().getSd().setCols(newCols);\n      } else {\n        // make sure the columns does not already exist\n        Iterator<FieldSchema> iterNewCols = newCols.iterator();\n        while (iterNewCols.hasNext()) {\n          FieldSchema newCol = iterNewCols.next();\n          String newColName = newCol.getName();\n          Iterator<FieldSchema> iterOldCols = oldCols.iterator();\n          while (iterOldCols.hasNext()) {\n            String oldColName = iterOldCols.next().getName();\n            if (oldColName.equalsIgnoreCase(newColName)) {\n              console.printError(\"Column '\" + newColName + \"' exists\");\n              return 1;\n            }\n          }\n          oldCols.add(newCol);\n        }\n        tbl.getTTable().getSd().setCols(oldCols);\n      }\n    } else if (alterTbl.getOp() == AlterTableDesc.AlterTableTypes.RENAMECOLUMN) {\n      List<FieldSchema> oldCols = tbl.getCols();\n      List<FieldSchema> newCols = new ArrayList<FieldSchema>();\n      Iterator<FieldSchema> iterOldCols = oldCols.iterator();\n      String oldName = alterTbl.getOldColName();\n      String newName = alterTbl.getNewColName();\n      String type = alterTbl.getNewColType();\n      String comment = alterTbl.getNewColComment();\n      boolean first = alterTbl.getFirst();\n      String afterCol = alterTbl.getAfterCol();\n      FieldSchema column = null;\n\n      boolean found = false;\n      int position = -1;\n      if (first) {\n        position = 0;\n      }\n\n      int i = 1;\n      while (iterOldCols.hasNext()) {\n        FieldSchema col = iterOldCols.next();\n        String oldColName = col.getName();\n        if (oldColName.equalsIgnoreCase(newName)\n            && !oldColName.equalsIgnoreCase(oldName)) {\n          console.printError(\"Column '\" + newName + \"' exists\");\n          return 1;\n        } else if (oldColName.equalsIgnoreCase(oldName)) {\n          col.setName(newName);\n          if (type != null && !type.trim().equals(\"\")) {\n            col.setType(type);\n          }\n          if (comment != null) {\n            col.setComment(comment);\n          }\n          found = true;\n          if (first || (afterCol != null && !afterCol.trim().equals(\"\"))) {\n            column = col;\n            continue;\n          }\n        }\n\n        if (afterCol != null && !afterCol.trim().equals(\"\")\n            && oldColName.equalsIgnoreCase(afterCol)) {\n          position = i;\n        }\n\n        i++;\n        newCols.add(col);\n      }\n\n      // did not find the column\n      if (!found) {\n        console.printError(\"Column '\" + oldName + \"' does not exist\");\n        return 1;\n      }\n      // after column is not null, but we did not find it.\n      if ((afterCol != null && !afterCol.trim().equals(\"\")) && position < 0) {\n        console.printError(\"Column '\" + afterCol + \"' does not exist\");\n        return 1;\n      }\n\n      if (position >= 0) {\n        newCols.add(position, column);\n      }\n\n      tbl.getTTable().getSd().setCols(newCols);\n    } else if (alterTbl.getOp() == AlterTableDesc.AlterTableTypes.REPLACECOLS) {\n      // change SerDe to LazySimpleSerDe if it is columnsetSerDe\n      if (tbl.getSerializationLib().equals(\n          \"org.apache.hadoop.hive.serde.thrift.columnsetSerDe\")) {\n        console\n            .printInfo(\"Replacing columns for columnsetSerDe and changing to LazySimpleSerDe\");\n        tbl.setSerializationLib(LazySimpleSerDe.class.getName());\n      } else if (!tbl.getSerializationLib().equals(\n          MetadataTypedColumnsetSerDe.class.getName())\n          && !tbl.getSerializationLib().equals(LazySimpleSerDe.class.getName())\n          && !tbl.getSerializationLib().equals(ColumnarSerDe.class.getName())\n          && !tbl.getSerializationLib().equals(DynamicSerDe.class.getName())) {\n        console.printError(\"Replace columns is not supported for this table. \"\n            + \"SerDe may be incompatible.\");\n        return 1;\n      }\n      tbl.getTTable().getSd().setCols(alterTbl.getNewCols());\n    } else if (alterTbl.getOp() == AlterTableDesc.AlterTableTypes.ADDPROPS) {\n      tbl.getTTable().getParameters().putAll(alterTbl.getProps());\n    } else if (alterTbl.getOp() == AlterTableDesc.AlterTableTypes.ADDSERDEPROPS) {\n      tbl.getTTable().getSd().getSerdeInfo().getParameters().putAll(\n          alterTbl.getProps());\n    } else if (alterTbl.getOp() == AlterTableDesc.AlterTableTypes.ADDSERDE) {\n      tbl.setSerializationLib(alterTbl.getSerdeName());\n      if ((alterTbl.getProps() != null) && (alterTbl.getProps().size() > 0)) {\n        tbl.getTTable().getSd().getSerdeInfo().getParameters().putAll(\n            alterTbl.getProps());\n      }\n      tbl.setFields(Hive.getFieldsFromDeserializer(tbl.getTableName(), tbl\n          .getDeserializer()));\n    } else if (alterTbl.getOp() == AlterTableDesc.AlterTableTypes.ADDFILEFORMAT) {\n      if(part != null) {\n        part.getTPartition().getSd().setInputFormat(alterTbl.getInputFormat());\n        part.getTPartition().getSd().setOutputFormat(alterTbl.getOutputFormat());\n        if (alterTbl.getSerdeName() != null) {\n          part.getTPartition().getSd().getSerdeInfo().setSerializationLib(\n              alterTbl.getSerdeName());\n        }\n      } else {\n        tbl.getTTable().getSd().setInputFormat(alterTbl.getInputFormat());\n        tbl.getTTable().getSd().setOutputFormat(alterTbl.getOutputFormat());\n        if (alterTbl.getSerdeName() != null) {\n          tbl.setSerializationLib(alterTbl.getSerdeName());\n        }\n      }\n    } else if (alterTbl.getOp() == AlterTableDesc.AlterTableTypes.ALTERPROTECTMODE) {\n      boolean protectModeEnable = alterTbl.isProtectModeEnable();\n      AlterTableDesc.ProtectModeType protectMode = alterTbl.getProtectModeType();\n\n      ProtectMode mode = null;\n      if(part != null) {\n        mode = part.getProtectMode();\n      } else {\n        mode = tbl.getProtectMode();\n      }\n\n      if (protectModeEnable\n          && protectMode == AlterTableDesc.ProtectModeType.OFFLINE) {\n        mode.offline = true;\n      } else if (protectModeEnable\n          && protectMode == AlterTableDesc.ProtectModeType.NO_DROP) {\n        mode.noDrop = true;\n      } else if (!protectModeEnable\n          && protectMode == AlterTableDesc.ProtectModeType.OFFLINE) {\n        mode.offline = false;\n      } else if (!protectModeEnable\n          && protectMode == AlterTableDesc.ProtectModeType.NO_DROP) {\n        mode.noDrop = false;\n      }\n\n      if (part != null) {\n        part.setProtectMode(mode);\n      } else {\n        tbl.setProtectMode(mode);        \n      }\n\n    } else if (alterTbl.getOp() == AlterTableDesc.AlterTableTypes.ADDCLUSTERSORTCOLUMN) {\n      // validate sort columns and bucket columns\n      List<String> columns = Utilities.getColumnNamesFromFieldSchema(tbl\n          .getCols());\n      Utilities.validateColumnNames(columns, alterTbl.getBucketColumns());\n      if (alterTbl.getSortColumns() != null) {\n        Utilities.validateColumnNames(columns, Utilities\n            .getColumnNamesFromSortCols(alterTbl.getSortColumns()));\n      }\n\n      int numBuckets = -1;\n      ArrayList<String> bucketCols = null;\n      ArrayList<Order> sortCols = null;\n\n      // -1 buckets means to turn off bucketing\n      if (alterTbl.getNumberBuckets() == -1) {\n        bucketCols = new ArrayList<String>();\n        sortCols = new ArrayList<Order>();\n        numBuckets = -1;\n      } else {\n        bucketCols = alterTbl.getBucketColumns();\n        sortCols = alterTbl.getSortColumns();\n        numBuckets = alterTbl.getNumberBuckets();\n      }\n      tbl.getTTable().getSd().setBucketCols(bucketCols);\n      tbl.getTTable().getSd().setNumBuckets(numBuckets);\n      tbl.getTTable().getSd().setSortCols(sortCols);\n    } else if (alterTbl.getOp() == AlterTableDesc.AlterTableTypes.ALTERLOCATION) {\n      String newLocation = alterTbl.getNewLocation();\n      try {\n        URI locURI = new URI(newLocation);\n        if (!locURI.isAbsolute() || locURI.getScheme() == null\n            || locURI.getScheme().trim().equals(\"\")) {\n          throw new HiveException(\n              newLocation\n                  + \" is not absolute or has no scheme information. \"\n                  + \"Please specify a complete absolute uri with scheme information.\");\n        }\n        if (part != null) {\n          part.setLocation(newLocation);\n        } else {\n          tbl.setDataLocation(locURI);\n        }\n      } catch (URISyntaxException e) {\n        throw new HiveException(e);\n      }\n    } else {\n      console.printError(\"Unsupported Alter commnad\");\n      return 1;\n    }\n\n    // set last modified by properties\n    String user = null;\n    try {\n      user = conf.getUser();\n    } catch (IOException e) {\n      console.printError(\"Unable to get current user: \" + e.getMessage(),\n          stringifyException(e));\n      return 1;\n    }\n\n    if(part == null) {\n      tbl.setProperty(\"last_modified_by\", user);\n      tbl.setProperty(\"last_modified_time\", Long.toString(System\n          .currentTimeMillis() / 1000));\n      try {\n        tbl.checkValidity();\n      } catch (HiveException e) {\n        console.printError(\"Invalid table columns : \" + e.getMessage(),\n            stringifyException(e));\n        return 1;\n      }\n    } else {\n      part.getParameters().put(\"last_modified_by\", user);\n      part.getParameters().put(\"last_modified_time\", Long.toString(System\n          .currentTimeMillis() / 1000));\n    }\n    \n    try {\n      if (part == null) {\n        db.alterTable(alterTbl.getOldName(), tbl);\n      } else {\n        db.alterPartition(tbl.getTableName(), part);        \n      }\n    } catch (InvalidOperationException e) {\n      console.printError(\"Invalid alter operation: \" + e.getMessage());\n      LOG.info(\"alter table: \" + stringifyException(e));\n      return 1;\n    } catch (HiveException e) {\n      return 1;\n    }\n\n    // This is kind of hacky - the read entity contains the old table, whereas\n    // the write entity\n    // contains the new table. This is needed for rename - both the old and the\n    // new table names are\n    // passed\n    if(part != null) {\n      work.getInputs().add(new ReadEntity(part));\n      work.getOutputs().add(new WriteEntity(part));\n    } else {\n      work.getInputs().add(new ReadEntity(oldTbl));\n      work.getOutputs().add(new WriteEntity(tbl));\n    }\n    return 0;\n  }",
                "code_after_change": "  private int alterTable(Hive db, AlterTableDesc alterTbl) throws HiveException {\n    // alter the table\n    Table tbl = db.getTable(MetaStoreUtils.DEFAULT_DATABASE_NAME, alterTbl\n        .getOldName());\n\n    Partition part = null;\n    if(alterTbl.getPartSpec() != null) {\n      part = db.getPartition(tbl, alterTbl.getPartSpec(), false);\n      if(part == null) {\n        console.printError(\"Partition : \" + alterTbl.getPartSpec().toString()\n            + \" does not exist.\");\n        return 1;\n      }\n    }\n\n    validateAlterTableType(tbl, alterTbl.getOp());\n\n    if (tbl.isView()) {\n      if (!alterTbl.getExpectView()) {\n        throw new HiveException(\"Cannot alter a view with ALTER TABLE\");\n      }\n    } else {\n      if (alterTbl.getExpectView()) {\n        throw new HiveException(\"Cannot alter a base table with ALTER VIEW\");\n      }\n    }\n\n    Table oldTbl = tbl.copy();\n\n    if (alterTbl.getOp() == AlterTableDesc.AlterTableTypes.RENAME) {\n      tbl.setTableName(alterTbl.getNewName());\n    } else if (alterTbl.getOp() == AlterTableDesc.AlterTableTypes.ADDCOLS) {\n      List<FieldSchema> newCols = alterTbl.getNewCols();\n      List<FieldSchema> oldCols = tbl.getCols();\n      if (tbl.getSerializationLib().equals(\n          \"org.apache.hadoop.hive.serde.thrift.columnsetSerDe\")) {\n        console\n            .printInfo(\"Replacing columns for columnsetSerDe and changing to LazySimpleSerDe\");\n        tbl.setSerializationLib(LazySimpleSerDe.class.getName());\n        tbl.getTTable().getSd().setCols(newCols);\n      } else {\n        // make sure the columns does not already exist\n        Iterator<FieldSchema> iterNewCols = newCols.iterator();\n        while (iterNewCols.hasNext()) {\n          FieldSchema newCol = iterNewCols.next();\n          String newColName = newCol.getName();\n          Iterator<FieldSchema> iterOldCols = oldCols.iterator();\n          while (iterOldCols.hasNext()) {\n            String oldColName = iterOldCols.next().getName();\n            if (oldColName.equalsIgnoreCase(newColName)) {\n              console.printError(\"Column '\" + newColName + \"' exists\");\n              return 1;\n            }\n          }\n          oldCols.add(newCol);\n        }\n        tbl.getTTable().getSd().setCols(oldCols);\n      }\n    } else if (alterTbl.getOp() == AlterTableDesc.AlterTableTypes.RENAMECOLUMN) {\n      List<FieldSchema> oldCols = tbl.getCols();\n      List<FieldSchema> newCols = new ArrayList<FieldSchema>();\n      Iterator<FieldSchema> iterOldCols = oldCols.iterator();\n      String oldName = alterTbl.getOldColName();\n      String newName = alterTbl.getNewColName();\n      String type = alterTbl.getNewColType();\n      String comment = alterTbl.getNewColComment();\n      boolean first = alterTbl.getFirst();\n      String afterCol = alterTbl.getAfterCol();\n      FieldSchema column = null;\n\n      boolean found = false;\n      int position = -1;\n      if (first) {\n        position = 0;\n      }\n\n      int i = 1;\n      while (iterOldCols.hasNext()) {\n        FieldSchema col = iterOldCols.next();\n        String oldColName = col.getName();\n        if (oldColName.equalsIgnoreCase(newName)\n            && !oldColName.equalsIgnoreCase(oldName)) {\n          console.printError(\"Column '\" + newName + \"' exists\");\n          return 1;\n        } else if (oldColName.equalsIgnoreCase(oldName)) {\n          col.setName(newName);\n          if (type != null && !type.trim().equals(\"\")) {\n            col.setType(type);\n          }\n          if (comment != null) {\n            col.setComment(comment);\n          }\n          found = true;\n          if (first || (afterCol != null && !afterCol.trim().equals(\"\"))) {\n            column = col;\n            continue;\n          }\n        }\n\n        if (afterCol != null && !afterCol.trim().equals(\"\")\n            && oldColName.equalsIgnoreCase(afterCol)) {\n          position = i;\n        }\n\n        i++;\n        newCols.add(col);\n      }\n\n      // did not find the column\n      if (!found) {\n        console.printError(\"Column '\" + oldName + \"' does not exist\");\n        return 1;\n      }\n      // after column is not null, but we did not find it.\n      if ((afterCol != null && !afterCol.trim().equals(\"\")) && position < 0) {\n        console.printError(\"Column '\" + afterCol + \"' does not exist\");\n        return 1;\n      }\n\n      if (position >= 0) {\n        newCols.add(position, column);\n      }\n\n      tbl.getTTable().getSd().setCols(newCols);\n    } else if (alterTbl.getOp() == AlterTableDesc.AlterTableTypes.REPLACECOLS) {\n      // change SerDe to LazySimpleSerDe if it is columnsetSerDe\n      if (tbl.getSerializationLib().equals(\n          \"org.apache.hadoop.hive.serde.thrift.columnsetSerDe\")) {\n        console\n            .printInfo(\"Replacing columns for columnsetSerDe and changing to LazySimpleSerDe\");\n        tbl.setSerializationLib(LazySimpleSerDe.class.getName());\n      } else if (!tbl.getSerializationLib().equals(\n          MetadataTypedColumnsetSerDe.class.getName())\n          && !tbl.getSerializationLib().equals(LazySimpleSerDe.class.getName())\n          && !tbl.getSerializationLib().equals(ColumnarSerDe.class.getName())\n          && !tbl.getSerializationLib().equals(DynamicSerDe.class.getName())) {\n        console.printError(\"Replace columns is not supported for this table. \"\n            + \"SerDe may be incompatible.\");\n        return 1;\n      }\n      tbl.getTTable().getSd().setCols(alterTbl.getNewCols());\n    } else if (alterTbl.getOp() == AlterTableDesc.AlterTableTypes.ADDPROPS) {\n      tbl.getTTable().getParameters().putAll(alterTbl.getProps());\n    } else if (alterTbl.getOp() == AlterTableDesc.AlterTableTypes.ADDSERDEPROPS) {\n      tbl.getTTable().getSd().getSerdeInfo().getParameters().putAll(\n          alterTbl.getProps());\n    } else if (alterTbl.getOp() == AlterTableDesc.AlterTableTypes.ADDSERDE) {\n      tbl.setSerializationLib(alterTbl.getSerdeName());\n      if ((alterTbl.getProps() != null) && (alterTbl.getProps().size() > 0)) {\n        tbl.getTTable().getSd().getSerdeInfo().getParameters().putAll(\n            alterTbl.getProps());\n      }\n      tbl.setFields(Hive.getFieldsFromDeserializer(tbl.getTableName(), tbl\n          .getDeserializer()));\n    } else if (alterTbl.getOp() == AlterTableDesc.AlterTableTypes.ADDFILEFORMAT) {\n      if(part != null) {\n        part.getTPartition().getSd().setInputFormat(alterTbl.getInputFormat());\n        part.getTPartition().getSd().setOutputFormat(alterTbl.getOutputFormat());\n        if (alterTbl.getSerdeName() != null) {\n          part.getTPartition().getSd().getSerdeInfo().setSerializationLib(\n              alterTbl.getSerdeName());\n        }\n      } else {\n        tbl.getTTable().getSd().setInputFormat(alterTbl.getInputFormat());\n        tbl.getTTable().getSd().setOutputFormat(alterTbl.getOutputFormat());\n        if (alterTbl.getSerdeName() != null) {\n          tbl.setSerializationLib(alterTbl.getSerdeName());\n        }\n      }\n    } else if (alterTbl.getOp() == AlterTableDesc.AlterTableTypes.ALTERPROTECTMODE) {\n      boolean protectModeEnable = alterTbl.isProtectModeEnable();\n      AlterTableDesc.ProtectModeType protectMode = alterTbl.getProtectModeType();\n\n      ProtectMode mode = null;\n      if(part != null) {\n        mode = part.getProtectMode();\n      } else {\n        mode = tbl.getProtectMode();\n      }\n\n      if (protectModeEnable\n          && protectMode == AlterTableDesc.ProtectModeType.OFFLINE) {\n        mode.offline = true;\n      } else if (protectModeEnable\n          && protectMode == AlterTableDesc.ProtectModeType.NO_DROP) {\n        mode.noDrop = true;\n      } else if (!protectModeEnable\n          && protectMode == AlterTableDesc.ProtectModeType.OFFLINE) {\n        mode.offline = false;\n      } else if (!protectModeEnable\n          && protectMode == AlterTableDesc.ProtectModeType.NO_DROP) {\n        mode.noDrop = false;\n      }\n\n      if (part != null) {\n        part.setProtectMode(mode);\n      } else {\n        tbl.setProtectMode(mode);\n      }\n\n    } else if (alterTbl.getOp() == AlterTableDesc.AlterTableTypes.ADDCLUSTERSORTCOLUMN) {\n      // validate sort columns and bucket columns\n      List<String> columns = Utilities.getColumnNamesFromFieldSchema(tbl\n          .getCols());\n      Utilities.validateColumnNames(columns, alterTbl.getBucketColumns());\n      if (alterTbl.getSortColumns() != null) {\n        Utilities.validateColumnNames(columns, Utilities\n            .getColumnNamesFromSortCols(alterTbl.getSortColumns()));\n      }\n\n      int numBuckets = -1;\n      ArrayList<String> bucketCols = null;\n      ArrayList<Order> sortCols = null;\n\n      // -1 buckets means to turn off bucketing\n      if (alterTbl.getNumberBuckets() == -1) {\n        bucketCols = new ArrayList<String>();\n        sortCols = new ArrayList<Order>();\n        numBuckets = -1;\n      } else {\n        bucketCols = alterTbl.getBucketColumns();\n        sortCols = alterTbl.getSortColumns();\n        numBuckets = alterTbl.getNumberBuckets();\n      }\n      tbl.getTTable().getSd().setBucketCols(bucketCols);\n      tbl.getTTable().getSd().setNumBuckets(numBuckets);\n      tbl.getTTable().getSd().setSortCols(sortCols);\n    } else if (alterTbl.getOp() == AlterTableDesc.AlterTableTypes.ALTERLOCATION) {\n      String newLocation = alterTbl.getNewLocation();\n      try {\n        URI locURI = new URI(newLocation);\n        if (!locURI.isAbsolute() || locURI.getScheme() == null\n            || locURI.getScheme().trim().equals(\"\")) {\n          throw new HiveException(\n              newLocation\n                  + \" is not absolute or has no scheme information. \"\n                  + \"Please specify a complete absolute uri with scheme information.\");\n        }\n        if (part != null) {\n          part.setLocation(newLocation);\n        } else {\n          tbl.setDataLocation(locURI);\n        }\n      } catch (URISyntaxException e) {\n        throw new HiveException(e);\n      }\n    } else {\n      console.printError(\"Unsupported Alter commnad\");\n      return 1;\n    }\n\n    // set last modified by properties\n    String user = null;\n    try {\n      user = conf.getUser();\n    } catch (IOException e) {\n      console.printError(\"Unable to get current user: \" + e.getMessage(),\n          stringifyException(e));\n      return 1;\n    }\n\n    if(part == null) {\n      tbl.setProperty(\"last_modified_by\", user);\n      tbl.setProperty(\"last_modified_time\", Long.toString(System\n          .currentTimeMillis() / 1000));\n      try {\n        tbl.checkValidity();\n      } catch (HiveException e) {\n        console.printError(\"Invalid table columns : \" + e.getMessage(),\n            stringifyException(e));\n        return 1;\n      }\n    } else {\n      part.getParameters().put(\"last_modified_by\", user);\n      part.getParameters().put(\"last_modified_time\", Long.toString(System\n          .currentTimeMillis() / 1000));\n    }\n\n    try {\n      if (part == null) {\n        db.alterTable(alterTbl.getOldName(), tbl);\n      } else {\n        db.alterPartition(tbl.getTableName(), part);\n      }\n    } catch (InvalidOperationException e) {\n      console.printError(\"Invalid alter operation: \" + e.getMessage());\n      LOG.info(\"alter table: \" + stringifyException(e));\n      return 1;\n    } catch (HiveException e) {\n      return 1;\n    }\n\n    // This is kind of hacky - the read entity contains the old table, whereas\n    // the write entity\n    // contains the new table. This is needed for rename - both the old and the\n    // new table names are\n    // passed\n    if(part != null) {\n      work.getInputs().add(new ReadEntity(part));\n      work.getOutputs().add(new WriteEntity(part));\n    } else {\n      work.getInputs().add(new ReadEntity(oldTbl));\n      work.getOutputs().add(new WriteEntity(tbl));\n    }\n    return 0;\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report precisely identifies the root cause by mentioning the 'unarchive' method in the stack trace, which is one of the ground truth methods. However, there is no fix suggestion provided in the bug report, as there is no 'Suggestions' or 'possible_fix' field, nor any suggestion in the 'Description'. The problem location is also precisely identified as the 'unarchive' method is mentioned in the stack trace, which matches the ground truth method. There is no wrong information in the bug report as all the details are consistent with the context of the bug."
        }
    },
    {
        "filename": "HIVE-6113.json",
        "code_diff": {
            "common.src.java.org.apache.hadoop.hive.conf.HiveConf.initialize": {
                "code_before_change": "  private void initialize(Class<?> cls) {\n    hiveJar = (new JobConf(cls)).getJar();\n\n    // preserve the original configuration\n    origProp = getAllProperties();\n\n    // Overlay the ConfVars. Note that this ignores ConfVars with null values\n    addResource(getConfVarInputStream());\n\n    // Overlay hive-site.xml if it exists\n    if (hiveSiteURL != null) {\n      addResource(hiveSiteURL);\n    }\n\n    // Overlay the values of any system properties whose names appear in the list of ConfVars\n    applySystemProperties();\n\n    if(this.get(\"hive.metastore.local\", null) != null) {\n      l4j.warn(\"DEPRECATED: Configuration property hive.metastore.local no longer has any \" +\n      \t\t\"effect. Make sure to provide a valid value for hive.metastore.uris if you are \" +\n      \t\t\"connecting to a remote metastore.\");\n    }\n\n    // if the running class was loaded directly (through eclipse) rather than through a\n    // jar then this would be needed\n    if (hiveJar == null) {\n      hiveJar = this.get(ConfVars.HIVEJAR.varname);\n    }\n\n    if (auxJars == null) {\n      auxJars = this.get(ConfVars.HIVEAUXJARS.varname);\n    }\n\n    if (getBoolVar(ConfVars.METASTORE_SCHEMA_VERIFICATION)) {\n      setBoolVar(ConfVars.METASTORE_AUTO_CREATE_SCHEMA, false);\n      setBoolVar(ConfVars.METASTORE_FIXED_DATASTORE, true);\n    }\n\n    // setup list of conf vars that are not allowed to change runtime\n    setupRestrictList();\n  }",
                "code_after_change": "  private void initialize(Class<?> cls) {\n    hiveJar = (new JobConf(cls)).getJar();\n\n    // preserve the original configuration\n    origProp = getAllProperties();\n\n    // Overlay the ConfVars. Note that this ignores ConfVars with null values\n    addResource(getConfVarInputStream());\n\n    // Overlay hive-site.xml if it exists\n    if (hiveSiteURL != null) {\n      addResource(hiveSiteURL);\n    }\n\n    // if embedded metastore is to be used as per config so far\n    // then this is considered like the metastore server case\n    String msUri = this.getVar(HiveConf.ConfVars.METASTOREURIS);\n    if(HiveConfUtil.isEmbeddedMetaStore(msUri)){\n      setLoadMetastoreConfig(true);\n    }\n\n    // load hivemetastore-site.xml if this is metastore and file exists\n    if (isLoadMetastoreConfig() && hivemetastoreSiteUrl != null) {\n      addResource(hivemetastoreSiteUrl);\n    }\n\n    // load hiveserver2-site.xml if this is hiveserver2 and file exists\n    // metastore can be embedded within hiveserver2, in such cases\n    // the conf params in hiveserver2-site.xml will override whats defined\n    // in hivemetastore-site.xml\n    if (isLoadHiveServer2Config() && hiveServer2SiteUrl != null) {\n      addResource(hiveServer2SiteUrl);\n    }\n\n    // Overlay the values of any system properties whose names appear in the list of ConfVars\n    applySystemProperties();\n\n    if ((this.get(\"hive.metastore.ds.retry.attempts\") != null) ||\n      this.get(\"hive.metastore.ds.retry.interval\") != null) {\n        l4j.warn(\"DEPRECATED: hive.metastore.ds.retry.* no longer has any effect.  \" +\n        \"Use hive.hmshandler.retry.* instead\");\n    }\n\n    // if the running class was loaded directly (through eclipse) rather than through a\n    // jar then this would be needed\n    if (hiveJar == null) {\n      hiveJar = this.get(ConfVars.HIVEJAR.varname);\n    }\n\n    if (auxJars == null) {\n      auxJars = this.get(ConfVars.HIVEAUXJARS.varname);\n    }\n\n    if (getBoolVar(ConfVars.METASTORE_SCHEMA_VERIFICATION)) {\n      setBoolVar(ConfVars.METASTORE_AUTO_CREATE_ALL, false);\n    }\n\n    if (getBoolVar(HiveConf.ConfVars.HIVECONFVALIDATION)) {\n      List<String> trimmed = new ArrayList<String>();\n      for (Map.Entry<String,String> entry : this) {\n        String key = entry.getKey();\n        if (key == null || !key.startsWith(\"hive.\")) {\n          continue;\n        }\n        ConfVars var = HiveConf.getConfVars(key);\n        if (var == null) {\n          var = HiveConf.getConfVars(key.trim());\n          if (var != null) {\n            trimmed.add(key);\n          }\n        }\n        if (var == null) {\n          l4j.warn(\"HiveConf of name \" + key + \" does not exist\");\n        } else if (!var.isType(entry.getValue())) {\n          l4j.warn(\"HiveConf \" + var.varname + \" expects \" + var.typeString() + \" type value\");\n        }\n      }\n      for (String key : trimmed) {\n        set(key.trim(), getRaw(key));\n        unset(key);\n      }\n    }\n\n    setupSQLStdAuthWhiteList();\n\n    // setup list of conf vars that are not allowed to change runtime\n    setupRestrictList();\n    setupHiddenSet();\n\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.exec.Utilities.constructVectorizedReduceRowOI": {
                "code_before_change": [],
                "code_after_change": "  public static StandardStructObjectInspector constructVectorizedReduceRowOI(\n      StructObjectInspector keyInspector, StructObjectInspector valueInspector)\n          throws HiveException {\n\n    ArrayList<String> colNames = new ArrayList<String>();\n    ArrayList<ObjectInspector> ois = new ArrayList<ObjectInspector>();\n    List<? extends StructField> fields = keyInspector.getAllStructFieldRefs();\n    for (StructField field: fields) {\n      colNames.add(Utilities.ReduceField.KEY.toString() + \".\" + field.getFieldName());\n      ois.add(field.getFieldObjectInspector());\n    }\n    fields = valueInspector.getAllStructFieldRefs();\n    for (StructField field: fields) {\n      colNames.add(Utilities.ReduceField.VALUE.toString() + \".\" + field.getFieldName());\n      ois.add(field.getFieldObjectInspector());\n    }\n    StandardStructObjectInspector rowObjectInspector = ObjectInspectorFactory.getStandardStructObjectInspector(colNames, ois);\n\n    return rowObjectInspector;\n  }"
            },
            "common.src.java.org.apache.hadoop.hive.conf.HiveConf.setSparkConfigUpdated": {
                "code_before_change": [],
                "code_after_change": "  public void setSparkConfigUpdated(boolean isSparkConfigUpdated) {\n    this.isSparkConfigUpdated = isSparkConfigUpdated;\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.unCacheDataNucleusClassLoaders": {
                "code_before_change": [],
                "code_after_change": "  public static void unCacheDataNucleusClassLoaders() {\n    PersistenceManagerFactory pmf = ObjectStore.getPMF();\n    if ((pmf != null) && (pmf instanceof JDOPersistenceManagerFactory)) {\n      JDOPersistenceManagerFactory jdoPmf = (JDOPersistenceManagerFactory) pmf;\n      NucleusContext nc = jdoPmf.getNucleusContext();\n      try {\n        Field classLoaderResolverMap = AbstractNucleusContext.class.getDeclaredField(\n            \"classLoaderResolverMap\");\n        classLoaderResolverMap.setAccessible(true);\n        classLoaderResolverMap.set(nc, new HashMap<String, ClassLoaderResolver>());\n        LOG.debug(\"Removed cached classloaders from DataNucleus NucleusContext\");\n      } catch (Exception e) {\n        LOG.warn(\"Failed to remove cached classloaders from DataNucleus NucleusContext \", e);\n      }\n    }\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report mentions the error 'Unable to instantiate org.apache.hadoop.hive.metastore.HiveMetaStoreClient' and provides a stack trace that includes methods like 'org.apache.hadoop.hive.ql.metadata.Hive.getDatabase' and 'org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance'. These methods are in the same stack trace context as the ground truth method 'metastore.src.java.org.apache.hadoop.hive.metastore.ObjectStore.unCacheDataNucleusClassLoaders', but do not directly identify the root cause or problem location. There is no fix suggestion provided in the bug report. All information in the bug report is relevant to the context of the bug, so there is no wrong information."
        }
    },
    {
        "filename": "HIVE-9570.json",
        "code_diff": {
            "ql.src.java.org.apache.hadoop.hive.ql.parse.spark.GenSparkUtils.removeUnionOperators": {
                "code_before_change": "  public void removeUnionOperators(Configuration conf, GenSparkProcContext context,\n      BaseWork work)\n    throws SemanticException {\n\n    List<Operator<?>> roots = new ArrayList<Operator<?>>();\n    roots.addAll(work.getAllRootOperators());\n    if (work.getDummyOps() != null) {\n      roots.addAll(work.getDummyOps());\n    }\n\n    // need to clone the plan.\n    List<Operator<?>> newRoots = Utilities.cloneOperatorTree(conf, roots);\n\n    // Build a map to map the original FileSinkOperator and the cloned FileSinkOperators\n    // This map is used for set the stats flag for the cloned FileSinkOperators in later process\n    Iterator<Operator<?>> newRootsIt = newRoots.iterator();\n    for (Operator<?> root : roots) {\n      Operator<?> newRoot = newRootsIt.next();\n      List<Operator<?>> newOpQueue = new LinkedList<Operator<?>>();\n      collectOperators(newRoot, newOpQueue);\n      List<Operator<?>> opQueue = new LinkedList<Operator<?>>();\n      collectOperators(root, opQueue);\n      Iterator<Operator<?>> newOpQueueIt = newOpQueue.iterator();\n      for (Operator<?> op : opQueue) {\n        Operator<?> newOp = newOpQueueIt.next();\n        if (op instanceof FileSinkOperator) {\n          List<FileSinkOperator> fileSinkList = context.fileSinkMap.get(op);\n          if (fileSinkList == null) {\n            fileSinkList = new LinkedList<FileSinkOperator>();\n          }\n          fileSinkList.add((FileSinkOperator) newOp);\n          context.fileSinkMap.put((FileSinkOperator) op, fileSinkList);\n        }\n      }\n    }\n\n    // we're cloning the operator plan but we're retaining the original work. That means\n    // that root operators have to be replaced with the cloned ops. The replacement map\n    // tells you what that mapping is.\n    Map<Operator<?>, Operator<?>> replacementMap = new HashMap<Operator<?>, Operator<?>>();\n\n    // there's some special handling for dummyOps required. Mapjoins won't be properly\n    // initialized if their dummy parents aren't initialized. Since we cloned the plan\n    // we need to replace the dummy operators in the work with the cloned ones.\n    List<HashTableDummyOperator> dummyOps = new LinkedList<HashTableDummyOperator>();\n\n    Iterator<Operator<?>> it = newRoots.iterator();\n    for (Operator<?> orig: roots) {\n      Operator<?> newRoot = it.next();\n      if (newRoot instanceof HashTableDummyOperator) {\n        dummyOps.add((HashTableDummyOperator) newRoot);\n        it.remove();\n      } else {\n        replacementMap.put(orig, newRoot);\n      }\n    }\n\n    // now we remove all the unions. we throw away any branch that's not reachable from\n    // the current set of roots. The reason is that those branches will be handled in\n    // different tasks.\n    Deque<Operator<?>> operators = new LinkedList<Operator<?>>();\n    operators.addAll(newRoots);\n\n    Set<Operator<?>> seen = new HashSet<Operator<?>>();\n\n    while (!operators.isEmpty()) {\n      Operator<?> current = operators.pop();\n      seen.add(current);\n\n      if (current instanceof UnionOperator) {\n        Operator<?> parent = null;\n        int count = 0;\n\n        for (Operator<?> op: current.getParentOperators()) {\n          if (seen.contains(op)) {\n            ++count;\n            parent = op;\n          }\n        }\n\n        // we should have been able to reach the union from only one side.\n        Preconditions.checkArgument(count <= 1,\n            \"AssertionError: expected count to be <= 1, but was \" + count);\n\n        if (parent == null) {\n          // root operator is union (can happen in reducers)\n          replacementMap.put(current, current.getChildOperators().get(0));\n        } else {\n          parent.removeChildAndAdoptItsChildren(current);\n        }\n      }\n\n      if (current instanceof FileSinkOperator\n          || current instanceof ReduceSinkOperator) {\n        current.setChildOperators(null);\n      } else {\n        operators.addAll(current.getChildOperators());\n      }\n    }\n    work.setDummyOps(dummyOps);\n    work.replaceRoots(replacementMap);\n  }",
                "code_after_change": "  public void removeUnionOperators(Configuration conf, GenSparkProcContext context,\n      BaseWork work)\n    throws SemanticException {\n\n    List<Operator<?>> roots = new ArrayList<Operator<?>>();\n\n    // For MapWork, getAllRootOperators is not suitable, since it checks\n    // getPathToAliases, and will return null if this is empty. Here we are\n    // replacing getAliasToWork, so should use that information instead.\n    if (work instanceof MapWork) {\n      roots.addAll(((MapWork) work).getAliasToWork().values());\n    } else {\n      roots.addAll(work.getAllRootOperators());\n    }\n    if (work.getDummyOps() != null) {\n      roots.addAll(work.getDummyOps());\n    }\n\n    // need to clone the plan.\n    List<Operator<?>> newRoots = Utilities.cloneOperatorTree(conf, roots);\n\n    // Build a map to map the original FileSinkOperator and the cloned FileSinkOperators\n    // This map is used for set the stats flag for the cloned FileSinkOperators in later process\n    Iterator<Operator<?>> newRootsIt = newRoots.iterator();\n    for (Operator<?> root : roots) {\n      Operator<?> newRoot = newRootsIt.next();\n      List<Operator<?>> newOpQueue = new LinkedList<Operator<?>>();\n      collectOperators(newRoot, newOpQueue);\n      List<Operator<?>> opQueue = new LinkedList<Operator<?>>();\n      collectOperators(root, opQueue);\n      Iterator<Operator<?>> newOpQueueIt = newOpQueue.iterator();\n      for (Operator<?> op : opQueue) {\n        Operator<?> newOp = newOpQueueIt.next();\n        if (op instanceof FileSinkOperator) {\n          List<FileSinkOperator> fileSinkList = context.fileSinkMap.get(op);\n          if (fileSinkList == null) {\n            fileSinkList = new LinkedList<FileSinkOperator>();\n          }\n          fileSinkList.add((FileSinkOperator) newOp);\n          context.fileSinkMap.put((FileSinkOperator) op, fileSinkList);\n        }\n      }\n    }\n\n    // we're cloning the operator plan but we're retaining the original work. That means\n    // that root operators have to be replaced with the cloned ops. The replacement map\n    // tells you what that mapping is.\n    Map<Operator<?>, Operator<?>> replacementMap = new HashMap<Operator<?>, Operator<?>>();\n\n    // there's some special handling for dummyOps required. Mapjoins won't be properly\n    // initialized if their dummy parents aren't initialized. Since we cloned the plan\n    // we need to replace the dummy operators in the work with the cloned ones.\n    List<HashTableDummyOperator> dummyOps = new LinkedList<HashTableDummyOperator>();\n\n    Iterator<Operator<?>> it = newRoots.iterator();\n    for (Operator<?> orig: roots) {\n      Operator<?> newRoot = it.next();\n      if (newRoot instanceof HashTableDummyOperator) {\n        dummyOps.add((HashTableDummyOperator) newRoot);\n        it.remove();\n      } else {\n        replacementMap.put(orig, newRoot);\n      }\n    }\n\n    // now we remove all the unions. we throw away any branch that's not reachable from\n    // the current set of roots. The reason is that those branches will be handled in\n    // different tasks.\n    Deque<Operator<?>> operators = new LinkedList<Operator<?>>();\n    operators.addAll(newRoots);\n\n    Set<Operator<?>> seen = new HashSet<Operator<?>>();\n\n    while (!operators.isEmpty()) {\n      Operator<?> current = operators.pop();\n      seen.add(current);\n\n      if (current instanceof UnionOperator) {\n        Operator<?> parent = null;\n        int count = 0;\n\n        for (Operator<?> op: current.getParentOperators()) {\n          if (seen.contains(op)) {\n            ++count;\n            parent = op;\n          }\n        }\n\n        // we should have been able to reach the union from only one side.\n        Preconditions.checkArgument(count <= 1,\n            \"AssertionError: expected count to be <= 1, but was \" + count);\n\n        if (parent == null) {\n          // root operator is union (can happen in reducers)\n          replacementMap.put(current, current.getChildOperators().get(0));\n        } else {\n          parent.removeChildAndAdoptItsChildren(current);\n        }\n      }\n\n      if (current instanceof FileSinkOperator\n          || current instanceof ReduceSinkOperator) {\n        current.setChildOperators(null);\n      } else {\n        operators.addAll(current.getChildOperators());\n      }\n    }\n    work.setDummyOps(dummyOps);\n    work.replaceRoots(replacementMap);\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies a NullPointerException occurring in the method 'setInputFormat' of the 'SparkCompiler' class, which is part of the stack trace. However, the actual root cause and fix were in the 'removeUnionOperators' method, which is not mentioned in the report. Therefore, the root cause identification is partial, with a sub-category of 'Shared Stack Trace Context'. There is no fix suggestion provided in the bug report, so it is marked as 'Missing'. The problem location identification is also partial, as it points to a method in the stack trace but not the actual method where the fix was applied. There is no wrong information in the bug report; it accurately describes the error as it appears in the stack trace."
        }
    },
    {
        "filename": "HIVE-1678.json",
        "code_diff": {
            "ql.src.java.org.apache.hadoop.hive.ql.optimizer.GenMapRedUtils.initMapJoinPlan": {
                "code_before_change": "  public static void initMapJoinPlan(\n      Operator<? extends Serializable> op, GenMRProcContext ctx,\n      boolean readInputMapJoin, boolean readInputUnion, boolean setReducer, int pos) throws SemanticException {\n    initMapJoinPlan(op, ctx, readInputMapJoin, readInputUnion, setReducer, pos, false);\n  }",
                "code_after_change": "  public static void initMapJoinPlan(\n      Operator<? extends Serializable> op, GenMRProcContext ctx,\n      boolean readInputMapJoin, boolean readInputUnion, boolean setReducer, int pos) throws SemanticException {\n    initMapJoinPlan(op, ctx, readInputMapJoin, readInputUnion, setReducer, pos, false);\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies a NullPointerException occurring in the MapJoinOperator.processOp method, which is part of the stack trace context shared with the ground truth method initMapJoinPlan. However, it does not precisely identify the root cause in the ground truth method. There is no fix suggestion provided in the bug report. The problem location is partially identified as it mentions methods in the stack trace context but not the exact ground truth method. There is no wrong information in the bug report as all mentioned methods are relevant to the context of the bug."
        }
    },
    {
        "filename": "HIVE-11820.json",
        "code_diff": {
            "shims.0.23.src.main.java.org.apache.hadoop.hive.shims.Hadoop23Shims.getStoragePolicyShim": {
                "code_before_change": [],
                "code_after_change": []
            },
            "shims.0.20S.src.main.java.org.apache.hadoop.hive.shims.Hadoop20SShims.getStoragePolicyShim": {
                "code_before_change": [],
                "code_after_change": []
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Alternative Fix",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the issue occurring in the method 'org.apache.hadoop.tools.DistCpOptions.validate', which is part of the stack trace and shares context with the ground truth methods. However, it does not precisely identify the root cause in the ground truth methods. The fix suggestion to reverse the order of two lines is an alternative fix that could resolve the issue, as it addresses the configuration of options that likely led to the exception. The problem location is identified in the stack trace context, but not precisely in the ground truth methods. There is no wrong information in the bug report as all details are relevant to the context of the bug."
        }
    },
    {
        "filename": "HIVE-17274.json",
        "code_diff": {
            "ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.RowContainer.setupWriter": {
                "code_before_change": "  protected void setupWriter() throws HiveException {\n    try {\n\n      if ( tmpFile != null ) {\n        return;\n      }\n\n      String suffix = \".tmp\";\n      if (this.keyObject != null) {\n        suffix = \".\" + this.keyObject.toString() + suffix;\n      }\n\n      parentDir = FileUtils.createLocalDirsTempFile(spillFileDirs, \"hive-rowcontainer\", \"\", true);\n\n      tmpFile = File.createTempFile(\"RowContainer\", suffix, parentDir);\n      LOG.info(\"RowContainer created temp file \" + tmpFile.getAbsolutePath());\n      // Delete the temp file if the JVM terminate normally through Hadoop job\n      // kill command.\n      // Caveat: it won't be deleted if JVM is killed by 'kill -9'.\n      parentDir.deleteOnExit();\n      tmpFile.deleteOnExit();\n\n      // rFile = new RandomAccessFile(tmpFile, \"rw\");\n      HiveOutputFormat<?, ?> hiveOutputFormat = HiveFileFormatUtils.getHiveOutputFormat(jc, tblDesc);\n      tempOutPath = new Path(tmpFile.toString());\n      JobConf localJc = getLocalFSJobConfClone(jc);\n      rw = HiveFileFormatUtils.getRecordWriter(this.jobCloneUsingLocalFs,\n          hiveOutputFormat, serde.getSerializedClass(), false,\n          tblDesc.getProperties(), tempOutPath, reporter);\n    } catch (Exception e) {\n      clearRows();\n      LOG.error(e.toString(), e);\n      throw new HiveException(e);\n    }\n\n  }",
                "code_after_change": "  protected void setupWriter() throws HiveException {\n    try {\n\n      if ( tmpFile != null ) {\n        return;\n      }\n\n      String suffix = \".tmp\";\n      if (this.keyObject != null) {\n        String keyObjectStr = this.keyObject.toString();\n        String md5Str = DigestUtils.md5Hex(keyObjectStr.toString());\n        LOG.info(\"Using md5Str: \" + md5Str + \" for keyObject: \" + keyObjectStr);\n        suffix = \".\" + md5Str + suffix;\n      }\n\n      parentDir = FileUtils.createLocalDirsTempFile(spillFileDirs, \"hive-rowcontainer\", \"\", true);\n\n      tmpFile = File.createTempFile(\"RowContainer\", suffix, parentDir);\n      LOG.info(\"RowContainer created temp file \" + tmpFile.getAbsolutePath());\n      // Delete the temp file if the JVM terminate normally through Hadoop job\n      // kill command.\n      // Caveat: it won't be deleted if JVM is killed by 'kill -9'.\n      parentDir.deleteOnExit();\n      tmpFile.deleteOnExit();\n\n      // rFile = new RandomAccessFile(tmpFile, \"rw\");\n      HiveOutputFormat<?, ?> hiveOutputFormat = HiveFileFormatUtils.getHiveOutputFormat(jc, tblDesc);\n      tempOutPath = new Path(tmpFile.toString());\n      JobConf localJc = getLocalFSJobConfClone(jc);\n      rw = HiveFileFormatUtils.getRecordWriter(this.jobCloneUsingLocalFs,\n          hiveOutputFormat, serde.getSerializedClass(), false,\n          tblDesc.getProperties(), tempOutPath, reporter);\n    } catch (Exception e) {\n      clearRows();\n      LOG.error(e.toString(), e);\n      throw new HiveException(e);\n    }\n\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.exec.persistence.RowContainer.close": {
                "code_before_change": "  protected void close() throws HiveException {\n    clearRows();\n    currentReadBlock = firstReadBlockPointer = currentWriteBlock = null;\n  }",
                "code_after_change": "  protected void close() throws HiveException {\n    clearRows();\n    currentReadBlock = firstReadBlockPointer = currentWriteBlock = null;\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Buggy Method"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Buggy Method"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the issue occurring in the 'RowContainer' class, specifically mentioning the 'setupWriter' method in the stack trace, which is the method where the error occurs but not where the actual fix was made. Therefore, it is classified as 'Partial' under 'Buggy Method' for both root cause and problem location identification. The report does not provide any fix suggestion, hence 'Missing' for fix suggestion. There is no incorrect information in the bug report, so 'No' for wrong information."
        }
    },
    {
        "filename": "HIVE-12522.json",
        "code_diff": {
            "ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DagUtils.createVertex": {
                "code_before_change": "  private Vertex createVertex(JobConf conf, MergeJoinWork mergeJoinWork, LocalResource appJarLr,\n      List<LocalResource> additionalLr, FileSystem fs, Path mrScratchDir, Context ctx,\n      VertexType vertexType)\n      throws Exception {\n    Utilities.setMergeWork(conf, mergeJoinWork, mrScratchDir, false);\n    if (mergeJoinWork.getMainWork() instanceof MapWork) {\n      List<BaseWork> mapWorkList = mergeJoinWork.getBaseWorkList();\n      MapWork mapWork = (MapWork) (mergeJoinWork.getMainWork());\n      Vertex mergeVx =\n          createVertex(conf, mapWork, appJarLr, additionalLr, fs, mrScratchDir, ctx, vertexType);\n\n      conf.setClass(\"mapred.input.format.class\", HiveInputFormat.class, InputFormat.class);\n      // mapreduce.tez.input.initializer.serialize.event.payload should be set\n      // to false when using this plug-in to avoid getting a serialized event at run-time.\n      conf.setBoolean(\"mapreduce.tez.input.initializer.serialize.event.payload\", false);\n      for (int i = 0; i < mapWorkList.size(); i++) {\n\n        mapWork = (MapWork) (mapWorkList.get(i));\n        conf.set(TEZ_MERGE_CURRENT_MERGE_FILE_PREFIX, mapWork.getName());\n        conf.set(Utilities.INPUT_NAME, mapWork.getName());\n        LOG.info(\"Going through each work and adding MultiMRInput\");\n        mergeVx.addDataSource(mapWork.getName(),\n            MultiMRInput.createConfigBuilder(conf, HiveInputFormat.class).build());\n      }\n\n      VertexManagerPluginDescriptor desc =\n        VertexManagerPluginDescriptor.create(CustomPartitionVertex.class.getName());\n      // the +1 to the size is because of the main work.\n      CustomVertexConfiguration vertexConf =\n          new CustomVertexConfiguration(mergeJoinWork.getMergeJoinOperator().getConf()\n              .getNumBuckets(), vertexType, mergeJoinWork.getBigTableAlias(),\n              mapWorkList.size() + 1);\n      DataOutputBuffer dob = new DataOutputBuffer();\n      vertexConf.write(dob);\n      byte[] userPayload = dob.getData();\n      desc.setUserPayload(UserPayload.create(ByteBuffer.wrap(userPayload)));\n      mergeVx.setVertexManagerPlugin(desc);\n      return mergeVx;\n    } else {\n      Vertex mergeVx =\n          createVertex(conf, (ReduceWork) mergeJoinWork.getMainWork(), appJarLr, additionalLr, fs,\n              mrScratchDir, ctx);\n      return mergeVx;\n    }\n  }",
                "code_after_change": "  private Vertex createVertex(JobConf conf, MergeJoinWork mergeJoinWork, LocalResource appJarLr,\n      List<LocalResource> additionalLr, FileSystem fs, Path mrScratchDir, Context ctx,\n      VertexType vertexType)\n      throws Exception {\n    Utilities.setMergeWork(conf, mergeJoinWork, mrScratchDir, false);\n    if (mergeJoinWork.getMainWork() instanceof MapWork) {\n      List<BaseWork> mapWorkList = mergeJoinWork.getBaseWorkList();\n      MapWork mapWork = (MapWork) (mergeJoinWork.getMainWork());\n      Vertex mergeVx =\n          createVertex(conf, mapWork, appJarLr, additionalLr, fs, mrScratchDir, ctx, vertexType);\n\n      conf.setClass(\"mapred.input.format.class\", HiveInputFormat.class, InputFormat.class);\n      // mapreduce.tez.input.initializer.serialize.event.payload should be set\n      // to false when using this plug-in to avoid getting a serialized event at run-time.\n      conf.setBoolean(\"mapreduce.tez.input.initializer.serialize.event.payload\", false);\n      for (int i = 0; i < mapWorkList.size(); i++) {\n\n        mapWork = (MapWork) (mapWorkList.get(i));\n        conf.set(TEZ_MERGE_CURRENT_MERGE_FILE_PREFIX, mapWork.getName());\n        conf.set(Utilities.INPUT_NAME, mapWork.getName());\n        LOG.info(\"Going through each work and adding MultiMRInput\");\n        mergeVx.addDataSource(mapWork.getName(),\n            MultiMRInput.createConfigBuilder(conf, HiveInputFormat.class).build());\n      }\n\n      VertexManagerPluginDescriptor desc =\n        VertexManagerPluginDescriptor.create(CustomPartitionVertex.class.getName());\n      // the +1 to the size is because of the main work.\n      CustomVertexConfiguration vertexConf =\n          new CustomVertexConfiguration(mergeJoinWork.getMergeJoinOperator().getConf()\n              .getNumBuckets(), vertexType, mergeJoinWork.getBigTableAlias(),\n              mapWorkList.size() + 1);\n      DataOutputBuffer dob = new DataOutputBuffer();\n      vertexConf.write(dob);\n      byte[] userPayload = dob.getData();\n      desc.setUserPayload(UserPayload.create(ByteBuffer.wrap(userPayload)));\n      mergeVx.setVertexManagerPlugin(desc);\n      return mergeVx;\n    } else {\n      Vertex mergeVx =\n          createVertex(conf, (ReduceWork) mergeJoinWork.getMainWork(), appJarLr, additionalLr, fs,\n              mrScratchDir, ctx);\n      return mergeVx;\n    }\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report mentions the error occurring during the execution of a Tez graph, which is related to the stack trace context where the ground truth method 'ql.src.java.org.apache.hadoop.hive.ql.exec.tez.DagUtils.createVertex' is involved. However, it does not precisely identify this method as the root cause. There is no fix suggestion provided in the bug report. The problem location is partially identified as it is related to the shared stack trace context but does not precisely mention the ground truth method. There is no wrong information in the bug report as it accurately describes the error context."
        }
    },
    {
        "filename": "HIVE-16845.json",
        "code_diff": {
            "ql.src.java.org.apache.hadoop.hive.ql.plan.ConditionalResolverMergeFiles.generateActualTasks": {
                "code_before_change": "  private void generateActualTasks(HiveConf conf, List<Task<? extends Serializable>> resTsks,\n      long trgtSize, long avgConditionSize, Task<? extends Serializable> mvTask,\n      Task<? extends Serializable> mrTask, Task<? extends Serializable> mrAndMvTask, Path dirPath,\n      FileSystem inpFs, ConditionalResolverMergeFilesCtx ctx, MapWork work, int dpLbLevel)\n      throws IOException {\n    DynamicPartitionCtx dpCtx = ctx.getDPCtx();\n    // get list of dynamic partitions\n    FileStatus[] status = HiveStatsUtils.getFileStatusRecurse(dirPath, dpLbLevel, inpFs);\n\n    // cleanup pathToPartitionInfo\n    Map<Path, PartitionDesc> ptpi = work.getPathToPartitionInfo();\n    assert ptpi.size() == 1;\n    Path path = ptpi.keySet().iterator().next();\n    PartitionDesc partDesc = ptpi.get(path);\n    TableDesc tblDesc = partDesc.getTableDesc();\n    work.removePathToPartitionInfo(path); // the root path is not useful anymore\n\n    // cleanup pathToAliases\n    LinkedHashMap<Path, ArrayList<String>> pta = work.getPathToAliases();\n    assert pta.size() == 1;\n    path = pta.keySet().iterator().next();\n    ArrayList<String> aliases = pta.get(path);\n    work.removePathToAlias(path); // the root path is not useful anymore\n\n    // populate pathToPartitionInfo and pathToAliases w/ DP paths\n    long totalSz = 0;\n    boolean doMerge = false;\n    // list of paths that don't need to merge but need to move to the dest location\n    List<Path> toMove = new ArrayList<Path>();\n    for (int i = 0; i < status.length; ++i) {\n      long len = getMergeSize(inpFs, status[i].getPath(), avgConditionSize);\n      if (len >= 0) {\n        doMerge = true;\n        totalSz += len;\n        PartitionDesc pDesc = (dpCtx != null) ? generateDPFullPartSpec(dpCtx, status, tblDesc, i)\n            : partDesc;\n        work.resolveDynamicPartitionStoredAsSubDirsMerge(conf, status[i].getPath(), tblDesc,\n            aliases, pDesc);\n      } else {\n        toMove.add(status[i].getPath());\n      }\n    }\n    if (doMerge) {\n      // add the merge MR job\n      setupMapRedWork(conf, work, trgtSize, totalSz);\n\n      // add the move task for those partitions that do not need merging\n      if (toMove.size() > 0) {\n        // modify the existing move task as it is already in the candidate running tasks\n\n        // running the MoveTask and MR task in parallel may\n        // cause the mvTask write to /ds=1 and MR task write\n        // to /ds=1_1 for the same partition.\n        // make the MoveTask as the child of the MR Task\n        resTsks.add(mrAndMvTask);\n\n        MoveWork mvWork = (MoveWork) mvTask.getWork();\n        LoadFileDesc lfd = mvWork.getLoadFileWork();\n\n        Path targetDir = lfd.getTargetDir();\n        List<Path> targetDirs = new ArrayList<Path>(toMove.size());\n\n        for (int i = 0; i < toMove.size(); i++) {\n          String[] moveStrSplits = toMove.get(i).toUri().toString().split(Path.SEPARATOR);\n          int dpIndex = moveStrSplits.length - dpLbLevel;\n          Path target = targetDir;\n          while (dpIndex < moveStrSplits.length) {\n            target = new Path(target, moveStrSplits[dpIndex]);\n            dpIndex++;\n          }\n\n          targetDirs.add(target);\n        }\n\n        LoadMultiFilesDesc lmfd = new LoadMultiFilesDesc(toMove,\n            targetDirs, lfd.getIsDfsDir(), lfd.getColumns(), lfd.getColumnTypes());\n        mvWork.setLoadFileWork(null);\n        mvWork.setLoadTableWork(null);\n        mvWork.setMultiFilesDesc(lmfd);\n      } else {\n        resTsks.add(mrTask);\n      }\n    } else { // add the move task\n      resTsks.add(mvTask);\n    }\n  }",
                "code_after_change": "  private void generateActualTasks(HiveConf conf, List<Task<? extends Serializable>> resTsks,\n      long trgtSize, long avgConditionSize, Task<? extends Serializable> mvTask,\n      Task<? extends Serializable> mrTask, Task<? extends Serializable> mrAndMvTask, Path dirPath,\n      FileSystem inpFs, ConditionalResolverMergeFilesCtx ctx, MapWork work, int dpLbLevel)\n      throws IOException {\n    DynamicPartitionCtx dpCtx = ctx.getDPCtx();\n    // get list of dynamic partitions\n    FileStatus[] status = HiveStatsUtils.getFileStatusRecurse(dirPath, dpLbLevel, inpFs);\n\n    // cleanup pathToPartitionInfo\n    Map<Path, PartitionDesc> ptpi = work.getPathToPartitionInfo();\n    assert ptpi.size() == 1;\n    Path path = ptpi.keySet().iterator().next();\n    PartitionDesc partDesc = ptpi.get(path);\n    TableDesc tblDesc = partDesc.getTableDesc();\n    work.removePathToPartitionInfo(path); // the root path is not useful anymore\n\n    // cleanup pathToAliases\n    LinkedHashMap<Path, ArrayList<String>> pta = work.getPathToAliases();\n    assert pta.size() == 1;\n    path = pta.keySet().iterator().next();\n    ArrayList<String> aliases = pta.get(path);\n    work.removePathToAlias(path); // the root path is not useful anymore\n\n    // populate pathToPartitionInfo and pathToAliases w/ DP paths\n    long totalSz = 0;\n    boolean doMerge = false;\n    // list of paths that don't need to merge but need to move to the dest location\n    List<Path> toMove = new ArrayList<Path>();\n    for (int i = 0; i < status.length; ++i) {\n      long len = getMergeSize(inpFs, status[i].getPath(), avgConditionSize);\n      if (len >= 0) {\n        doMerge = true;\n        totalSz += len;\n        PartitionDesc pDesc = (dpCtx != null) ? generateDPFullPartSpec(dpCtx, status, tblDesc, i)\n            : partDesc;\n        work.resolveDynamicPartitionStoredAsSubDirsMerge(conf, status[i].getPath(), tblDesc,\n            aliases, pDesc);\n      } else {\n        toMove.add(status[i].getPath());\n      }\n    }\n    if (doMerge) {\n      // add the merge MR job\n      setupMapRedWork(conf, work, trgtSize, totalSz);\n\n      // add the move task for those partitions that do not need merging\n      if (toMove.size() > 0) {\n        // modify the existing move task as it is already in the candidate running tasks\n\n        // running the MoveTask and MR task in parallel may\n        // cause the mvTask write to /ds=1 and MR task write\n        // to /ds=1_1 for the same partition.\n        // make the MoveTask as the child of the MR Task\n        resTsks.add(mrAndMvTask);\n\n        // Originally the mvTask and the child move task of the mrAndMvTask contain the same\n        // MoveWork object.\n        // If the blobstore optimizations are on and the input/output paths are merged\n        // in the move only MoveWork, the mvTask and the child move task of the mrAndMvTask\n        // will contain different MoveWork objects, which causes problems.\n        // Not just in this case, but also in general the child move task of the mrAndMvTask should\n        // be used, because that is the correct move task for the \"merge and move\" use case.\n        Task<? extends Serializable> mergeAndMoveMoveTask = mrAndMvTask.getChildTasks().get(0);\n        MoveWork mvWork = (MoveWork) mergeAndMoveMoveTask.getWork();\n\n        LoadFileDesc lfd = mvWork.getLoadFileWork();\n\n        Path targetDir = lfd.getTargetDir();\n        List<Path> targetDirs = new ArrayList<Path>(toMove.size());\n\n        for (int i = 0; i < toMove.size(); i++) {\n          String[] moveStrSplits = toMove.get(i).toUri().toString().split(Path.SEPARATOR);\n          int dpIndex = moveStrSplits.length - dpLbLevel;\n          Path target = targetDir;\n          while (dpIndex < moveStrSplits.length) {\n            target = new Path(target, moveStrSplits[dpIndex]);\n            dpIndex++;\n          }\n\n          targetDirs.add(target);\n        }\n\n        LoadMultiFilesDesc lmfd = new LoadMultiFilesDesc(toMove,\n            targetDirs, lfd.getIsDfsDir(), lfd.getColumns(), lfd.getColumnTypes());\n        mvWork.setLoadFileWork(null);\n        mvWork.setLoadTableWork(null);\n        mvWork.setMultiFilesDesc(lmfd);\n      } else {\n        resTsks.add(mrTask);\n      }\n    } else { // add the move task\n      resTsks.add(mvTask);\n    }\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report precisely identifies the root cause by mentioning the NullPointerException occurring in the method 'org.apache.hadoop.hive.ql.plan.ConditionalResolverMergeFiles.generateActualTasks', which is the ground truth method. However, there is no fix suggestion provided in the bug report, as there is no mention of how to resolve the issue. The problem location is also precisely identified as the stack trace in the description points directly to the ground truth method. There is no wrong information in the bug report as all the details are consistent with the context of the bug."
        }
    },
    {
        "filename": "HIVE-9655.json",
        "code_diff": {
            "ql.src.java.org.apache.hadoop.hive.ql.optimizer.ColumnPrunerProcFactory.process": {
                "code_before_change": "    public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx ctx,\n        Object... nodeOutputs) throws SemanticException {\n      FilterOperator op = (FilterOperator) nd;\n      ColumnPrunerProcCtx cppCtx = (ColumnPrunerProcCtx) ctx;\n      ExprNodeDesc condn = op.getConf().getPredicate();\n      // get list of columns used in the filter\n      List<String> cl = condn.getCols();\n      // merge it with the downstream col list\n      List<String> filterOpPrunedColLists = Utilities.mergeUniqElems(cppCtx.genColLists(op), cl);\n      List<String> filterOpPrunedColListsOrderPreserved = preserveColumnOrder(op,\n          filterOpPrunedColLists);\n      cppCtx.getPrunedColLists().put(op,\n          filterOpPrunedColListsOrderPreserved);\n\n      pruneOperator(cppCtx, op, cppCtx.getPrunedColLists().get(op));\n\n      return null;\n    }",
                "code_after_change": "    public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx ctx,\n        Object... nodeOutputs) throws SemanticException {\n      FilterOperator op = (FilterOperator) nd;\n      ColumnPrunerProcCtx cppCtx = (ColumnPrunerProcCtx) ctx;\n      ExprNodeDesc condn = op.getConf().getPredicate();\n      // get list of columns used in the filter\n      List<String> cl = condn.getCols();\n      // merge it with the downstream col list\n      List<String> filterOpPrunedColLists = Utilities.mergeUniqElems(cppCtx.genColLists(op), cl);\n      List<String> filterOpPrunedColListsOrderPreserved = preserveColumnOrder(op,\n          filterOpPrunedColLists);\n      cppCtx.getPrunedColLists().put(op,\n          filterOpPrunedColListsOrderPreserved);\n\n      pruneOperator(cppCtx, op, cppCtx.getPrunedColLists().get(op));\n\n      return null;\n    }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Missing",
                "sub_category": ""
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report mentions an error related to dynamic partitioning and provides a stack trace that includes methods like `ReduceSinkOperator.processOp` and `MapOperator.process`, which are in the shared stack trace context with the ground truth method `ColumnPrunerProcFactory.process`. However, it does not precisely identify the root cause at the ground truth method. There is no fix suggestion provided in the bug report, as it lacks any `Suggestions` or `possible_fix` fields, and the `Description` does not offer any fix. The problem location is not precisely identified, as the report does not mention any specific methods or locations related to the ground truth. There is no wrong information in the report; it accurately describes the error and the context in which it occurs."
        }
    },
    {
        "filename": "HIVE-11441.json",
        "code_diff": {
            "ql.src.java.org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer.analyzeAlterTableLocation": {
                "code_before_change": "  private void analyzeAlterTableLocation(ASTNode ast, String tableName,\n      HashMap<String, String> partSpec) throws SemanticException {\n\n    String newLocation = unescapeSQLString(ast.getChild(0).getText());\n    addLocationToOutputs(newLocation);\n    AlterTableDesc alterTblDesc = new AlterTableDesc(tableName, newLocation, partSpec);\n\n    addInputsOutputsAlterTable(tableName, partSpec, alterTblDesc);\n    rootTasks.add(TaskFactory.get(new DDLWork(getInputs(), getOutputs(),\n        alterTblDesc), conf));\n\n  }",
                "code_after_change": "  private void analyzeAlterTableLocation(ASTNode ast, String tableName,\n      HashMap<String, String> partSpec) throws SemanticException {\n\n    String newLocation = unescapeSQLString(ast.getChild(0).getText());\n    try {\n      // To make sure host/port pair is valid, the status of the location\n      // does not matter\n      FileSystem.get(new URI(newLocation), conf).getFileStatus(new Path(newLocation));\n    } catch (FileNotFoundException e) {\n      // Only check host/port pair is valid, wheter the file exist or not does not matter\n    } catch (Exception e) {\n      throw new SemanticException(\"Cannot connect to namenode, please check if host/port pair for \" + newLocation + \" is valid\", e);\n    }\n    addLocationToOutputs(newLocation);\n    AlterTableDesc alterTblDesc = new AlterTableDesc(tableName, newLocation, partSpec);\n\n    addInputsOutputsAlterTable(tableName, partSpec, alterTblDesc);\n    rootTasks.add(TaskFactory.get(new DDLWork(getInputs(), getOutputs(),\n        alterTblDesc), conf));\n\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report mentions the error occurring when setting a table location, which is related to the ground truth method 'analyzeAlterTableLocation'. However, it does not precisely identify this method as the root cause, but it does mention methods in the same stack trace context, such as 'BaseSemanticAnalyzer.getTable'. There is no fix suggestion provided in the bug report. The problem location is partially identified as it mentions the context of the error but not the exact method. There is no wrong information in the bug report as it accurately describes the issue and the context in which it occurs."
        }
    },
    {
        "filename": "HIVE-10801.json",
        "code_diff": {
            "metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStore.drop_table_core": {
                "code_before_change": "    private boolean drop_table_core(final RawStore ms, final String dbname, final String name,\n        final boolean deleteData, final EnvironmentContext envContext,\n        final String indexName) throws NoSuchObjectException,\n        MetaException, IOException, InvalidObjectException, InvalidInputException {\n      boolean success = false;\n      boolean isExternal = false;\n      Path tblPath = null;\n      List<Path> partPaths = null;\n      Table tbl = null;\n      try {\n        ms.openTransaction();\n        // drop any partitions\n        tbl = get_table_core(dbname, name);\n        if (tbl == null) {\n          throw new NoSuchObjectException(name + \" doesn't exist\");\n        }\n        if (tbl.getSd() == null) {\n          throw new MetaException(\"Table metadata is corrupted\");\n        }\n\n        firePreEvent(new PreDropTableEvent(tbl, deleteData, this));\n\n        boolean isIndexTable = isIndexTable(tbl);\n        if (indexName == null && isIndexTable) {\n          throw new RuntimeException(\n              \"The table \" + name + \" is an index table. Please do drop index instead.\");\n        }\n\n        if (!isIndexTable) {\n          try {\n            List<Index> indexes = ms.getIndexes(dbname, name, Short.MAX_VALUE);\n            while (indexes != null && indexes.size() > 0) {\n              for (Index idx : indexes) {\n                this.drop_index_by_name(dbname, name, idx.getIndexName(), true);\n              }\n              indexes = ms.getIndexes(dbname, name, Short.MAX_VALUE);\n            }\n          } catch (TException e) {\n            throw new MetaException(e.getMessage());\n          }\n        }\n        isExternal = isExternal(tbl);\n        if (tbl.getSd().getLocation() != null) {\n          tblPath = new Path(tbl.getSd().getLocation());\n          if (!wh.isWritable(tblPath.getParent())) {\n            String target = indexName == null ? \"Table\" : \"Index table\";\n            throw new MetaException(target + \" metadata not deleted since \" +\n                tblPath.getParent() + \" is not writable by \" +\n                hiveConf.getUser());\n          }\n        }\n\n        // Drop the partitions and get a list of locations which need to be deleted\n        partPaths = dropPartitionsAndGetLocations(ms, dbname, name, tblPath,\n            tbl.getPartitionKeys(), deleteData && !isExternal);\n\n        if (!ms.dropTable(dbname, name)) {\n          String tableName = dbname + \".\" + name;\n          throw new MetaException(indexName == null ? \"Unable to drop table \" + tableName:\n              \"Unable to drop index table \" + tableName + \" for index \" + indexName);\n        }\n        success = ms.commitTransaction();\n      } finally {\n        if (!success) {\n          ms.rollbackTransaction();\n        } else if (deleteData && !isExternal) {\n          // Data needs deletion. Check if trash may be skipped.\n          // Trash may be skipped iff:\n          //  1. deleteData == true, obviously.\n          //  2. tbl is external.\n          //  3. Either\n          //    3.1. User has specified PURGE from the commandline, and if not,\n          //    3.2. User has set the table to auto-purge.\n          boolean ifPurge = ((envContext != null) && Boolean.parseBoolean(envContext.getProperties().get(\"ifPurge\")))\n                            ||\n                             (tbl.isSetParameters() && \"true\".equalsIgnoreCase(tbl.getParameters().get(\"auto.purge\")));\n          // Delete the data in the partitions which have other locations\n          deletePartitionData(partPaths, ifPurge);\n          // Delete the data in the table\n          deleteTableData(tblPath, ifPurge);\n          // ok even if the data is not deleted\n        }\n        for (MetaStoreEventListener listener : listeners) {\n          DropTableEvent dropTableEvent = new DropTableEvent(tbl, success, deleteData, this);\n          dropTableEvent.setEnvironmentContext(envContext);\n          listener.onDropTable(dropTableEvent);\n        }\n      }\n      return success;\n    }",
                "code_after_change": "    private boolean drop_table_core(final RawStore ms, final String dbname, final String name,\n        final boolean deleteData, final EnvironmentContext envContext,\n        final String indexName) throws NoSuchObjectException,\n        MetaException, IOException, InvalidObjectException, InvalidInputException {\n      boolean success = false;\n      boolean isExternal = false;\n      Path tblPath = null;\n      List<Path> partPaths = null;\n      Table tbl = null;\n      boolean ifPurge = false;\n      try {\n        ms.openTransaction();\n        // drop any partitions\n        tbl = get_table_core(dbname, name);\n        if (tbl == null) {\n          throw new NoSuchObjectException(name + \" doesn't exist\");\n        }\n        if (tbl.getSd() == null) {\n          throw new MetaException(\"Table metadata is corrupted\");\n        }\n\n        /**\n         * Trash may be skipped iff:\n         * 1. deleteData == true, obviously.\n         * 2. tbl is external.\n         * 3. Either\n         *  3.1. User has specified PURGE from the commandline, and if not,\n         *  3.2. User has set the table to auto-purge.\n         */\n        ifPurge = ((envContext != null) && Boolean.parseBoolean(envContext.getProperties().get(\"ifPurge\")))\n          || (tbl.isSetParameters() && \"true\".equalsIgnoreCase(tbl.getParameters().get(\"auto.purge\")));\n\n        firePreEvent(new PreDropTableEvent(tbl, deleteData, this));\n\n        boolean isIndexTable = isIndexTable(tbl);\n        if (indexName == null && isIndexTable) {\n          throw new RuntimeException(\n              \"The table \" + name + \" is an index table. Please do drop index instead.\");\n        }\n\n        if (!isIndexTable) {\n          try {\n            List<Index> indexes = ms.getIndexes(dbname, name, Short.MAX_VALUE);\n            while (indexes != null && indexes.size() > 0) {\n              for (Index idx : indexes) {\n                this.drop_index_by_name(dbname, name, idx.getIndexName(), true);\n              }\n              indexes = ms.getIndexes(dbname, name, Short.MAX_VALUE);\n            }\n          } catch (TException e) {\n            throw new MetaException(e.getMessage());\n          }\n        }\n        isExternal = isExternal(tbl);\n        if (tbl.getSd().getLocation() != null) {\n          tblPath = new Path(tbl.getSd().getLocation());\n          if (!wh.isWritable(tblPath.getParent())) {\n            String target = indexName == null ? \"Table\" : \"Index table\";\n            throw new MetaException(target + \" metadata not deleted since \" +\n                tblPath.getParent() + \" is not writable by \" +\n                hiveConf.getUser());\n          }\n        }\n\n        // tblPath will be null when tbl is a view. We skip the following if block in that case.\n        if(tblPath != null && !ifPurge) {\n          String trashInterval = hiveConf.get(\"fs.trash.interval\");\n          boolean trashEnabled = trashInterval != null && trashInterval.length() > 0\n            && Float.parseFloat(trashInterval) > 0;\n          if (trashEnabled) {\n            HadoopShims.HdfsEncryptionShim shim =\n              ShimLoader.getHadoopShims().createHdfsEncryptionShim(FileSystem.get(hiveConf), hiveConf);\n            if (shim.isPathEncrypted(tblPath)) {\n              throw new MetaException(\"Unable to drop table because it is in an encryption zone\" +\n                \" and trash is enabled.  Use PURGE option to skip trash.\");\n            }\n          }\n        }\n        // Drop the partitions and get a list of locations which need to be deleted\n        partPaths = dropPartitionsAndGetLocations(ms, dbname, name, tblPath,\n            tbl.getPartitionKeys(), deleteData && !isExternal);\n\n        if (!ms.dropTable(dbname, name)) {\n          String tableName = dbname + \".\" + name;\n          throw new MetaException(indexName == null ? \"Unable to drop table \" + tableName:\n              \"Unable to drop index table \" + tableName + \" for index \" + indexName);\n        }\n        success = ms.commitTransaction();\n      } finally {\n        if (!success) {\n          ms.rollbackTransaction();\n        } else if (deleteData && !isExternal) {\n          // Data needs deletion. Check if trash may be skipped.\n          // Delete the data in the partitions which have other locations\n          deletePartitionData(partPaths, ifPurge);\n          // Delete the data in the table\n          deleteTableData(tblPath, ifPurge);\n          // ok even if the data is not deleted\n        }\n        for (MetaStoreEventListener listener : listeners) {\n          DropTableEvent dropTableEvent = new DropTableEvent(tbl, success, deleteData, this);\n          dropTableEvent.setEnvironmentContext(envContext);\n          listener.onDropTable(dropTableEvent);\n        }\n      }\n      return success;\n    }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report precisely identifies the root cause of the issue as a NullPointerException occurring in the method 'drop_table_core' of the 'HiveMetaStore' class, which matches the ground truth method. However, the report does not provide any fix suggestion, hence it is marked as 'Missing' for fix suggestion. The problem location is also precisely identified as 'drop_table_core', which is the ground truth method. There is no wrong information in the bug report as all the details provided are relevant and accurate."
        }
    },
    {
        "filename": "HIVE-9141.json",
        "code_diff": {
            "ql.src.java.org.apache.hadoop.hive.ql.parse.GenTezWork.process": {
                "code_before_change": "  public Object process(Node nd, Stack<Node> stack,\n      NodeProcessorCtx procContext, Object... nodeOutputs)\n      throws SemanticException {\n\n    GenTezProcContext context = (GenTezProcContext) procContext;\n\n    assert context != null && context.currentTask != null\n        && context.currentRootOperator != null;\n\n    // Operator is a file sink or reduce sink. Something that forces\n    // a new vertex.\n    Operator<?> operator = (Operator<?>) nd;\n\n    // root is the start of the operator pipeline we're currently\n    // packing into a vertex, typically a table scan, union or join\n    Operator<?> root = context.currentRootOperator;\n\n    LOG.debug(\"Root operator: \" + root);\n    LOG.debug(\"Leaf operator: \" + operator);\n\n    if (context.clonedReduceSinks.contains(operator)) {\n      // if we're visiting a terminal we've created ourselves,\n      // just skip and keep going\n      return null;\n    }\n\n    TezWork tezWork = context.currentTask.getWork();\n\n    // Right now the work graph is pretty simple. If there is no\n    // Preceding work we have a root and will generate a map\n    // vertex. If there is a preceding work we will generate\n    // a reduce vertex\n    BaseWork work;\n    if (context.rootToWorkMap.containsKey(root)) {\n      // having seen the root operator before means there was a branch in the\n      // operator graph. There's typically two reasons for that: a) mux/demux\n      // b) multi insert. Mux/Demux will hit the same leaf again, multi insert\n      // will result into a vertex with multiple FS or RS operators.\n      if (context.childToWorkMap.containsKey(operator)) {\n        // if we've seen both root and child, we can bail.\n\n        // clear out the mapjoin set. we don't need it anymore.\n        context.currentMapJoinOperators.clear();\n\n        // clear out the union set. we don't need it anymore.\n        context.currentUnionOperators.clear();\n\n        return null;\n      } else {\n        // At this point we don't have to do anything special. Just\n        // run through the regular paces w/o creating a new task.\n        work = context.rootToWorkMap.get(root);\n      }\n    } else {\n      // create a new vertex\n      if (context.preceedingWork == null) {\n        work = utils.createMapWork(context, root, tezWork, null);\n      } else {\n        work = utils.createReduceWork(context, root, tezWork);\n      }\n      context.rootToWorkMap.put(root, work);\n    }\n\n    // this is where we set the sort columns that we will be using for KeyValueInputMerge\n    if (operator instanceof DummyStoreOperator) {\n      work.addSortCols(root.getOpTraits().getSortCols().get(0));\n    }\n\n    if (!context.childToWorkMap.containsKey(operator)) {\n      List<BaseWork> workItems = new LinkedList<BaseWork>();\n      workItems.add(work);\n      context.childToWorkMap.put(operator, workItems);\n    } else {\n      context.childToWorkMap.get(operator).add(work);\n    }\n\n    // this transformation needs to be first because it changes the work item itself.\n    // which can affect the working of all downstream transformations.\n    if (context.currentMergeJoinOperator != null) {\n      // we are currently walking the big table side of the merge join. we need to create or hook up\n      // merge join work.\n      MergeJoinWork mergeJoinWork = null;\n      if (context.opMergeJoinWorkMap.containsKey(context.currentMergeJoinOperator)) {\n        // we have found a merge work corresponding to this closing operator. Hook up this work.\n        mergeJoinWork = context.opMergeJoinWorkMap.get(context.currentMergeJoinOperator);\n      } else {\n        // we need to create the merge join work\n        mergeJoinWork = new MergeJoinWork();\n        mergeJoinWork.setMergeJoinOperator(context.currentMergeJoinOperator);\n        tezWork.add(mergeJoinWork);\n        context.opMergeJoinWorkMap.put(context.currentMergeJoinOperator, mergeJoinWork);\n      }\n      // connect the work correctly.\n      work.addSortCols(root.getOpTraits().getSortCols().get(0));\n      mergeJoinWork.addMergedWork(work, null);\n      Operator<? extends OperatorDesc> parentOp =\n          getParentFromStack(context.currentMergeJoinOperator, stack);\n      int pos = context.currentMergeJoinOperator.getTagForOperator(parentOp);\n      work.setTag(pos);\n      tezWork.setVertexType(work, VertexType.MULTI_INPUT_UNINITIALIZED_EDGES);\n      for (BaseWork parentWork : tezWork.getParents(work)) {\n        TezEdgeProperty edgeProp = tezWork.getEdgeProperty(parentWork, work);\n        tezWork.disconnect(parentWork, work);\n        tezWork.connect(parentWork, mergeJoinWork, edgeProp);\n      }\n\n      for (BaseWork childWork : tezWork.getChildren(work)) {\n        TezEdgeProperty edgeProp = tezWork.getEdgeProperty(work, childWork);\n        tezWork.disconnect(work, childWork);\n        tezWork.connect(mergeJoinWork, childWork, edgeProp);\n      }\n      tezWork.remove(work);\n      context.rootToWorkMap.put(root, mergeJoinWork);\n      context.childToWorkMap.get(operator).remove(work);\n      context.childToWorkMap.get(operator).add(mergeJoinWork);\n      work = mergeJoinWork;\n      context.currentMergeJoinOperator = null;\n    }\n\n    // remember which mapjoin operator links with which work\n    if (!context.currentMapJoinOperators.isEmpty()) {\n      for (MapJoinOperator mj: context.currentMapJoinOperators) {\n        LOG.debug(\"Processing map join: \" + mj);\n        // remember the mapping in case we scan another branch of the\n        // mapjoin later\n        if (!context.mapJoinWorkMap.containsKey(mj)) {\n          List<BaseWork> workItems = new LinkedList<BaseWork>();\n          workItems.add(work);\n          context.mapJoinWorkMap.put(mj, workItems);\n        } else {\n          context.mapJoinWorkMap.get(mj).add(work);\n        }\n\n        /*\n         * this happens in case of map join operations.\n         * The tree looks like this:\n         *\n         *        RS <--- we are here perhaps\n         *        |\n         *     MapJoin\n         *     /     \\\n         *   RS       TS\n         *  /\n         * TS\n         *\n         * If we are at the RS pointed above, and we may have already visited the\n         * RS following the TS, we have already generated work for the TS-RS.\n         * We need to hook the current work to this generated work.\n         */\n        if (context.linkOpWithWorkMap.containsKey(mj)) {\n          Map<BaseWork,TezEdgeProperty> linkWorkMap = context.linkOpWithWorkMap.get(mj);\n          if (linkWorkMap != null) {\n            if (context.linkChildOpWithDummyOp.containsKey(mj)) {\n              for (Operator<?> dummy: context.linkChildOpWithDummyOp.get(mj)) {\n                work.addDummyOp((HashTableDummyOperator) dummy);\n              }\n            }\n            for (Entry<BaseWork,TezEdgeProperty> parentWorkMap : linkWorkMap.entrySet()) {\n              BaseWork parentWork = parentWorkMap.getKey();\n              LOG.debug(\"connecting \"+parentWork.getName()+\" with \"+work.getName());\n              TezEdgeProperty edgeProp = parentWorkMap.getValue();\n              tezWork.connect(parentWork, work, edgeProp);\n              if (edgeProp.getEdgeType() == EdgeType.CUSTOM_EDGE) {\n                tezWork.setVertexType(work, VertexType.INITIALIZED_EDGES);\n              }\n\n              // need to set up output name for reduce sink now that we know the name\n              // of the downstream work\n              for (ReduceSinkOperator r:\n                     context.linkWorkWithReduceSinkMap.get(parentWork)) {\n                if (r.getConf().getOutputName() != null) {\n                  LOG.debug(\"Cloning reduce sink for multi-child broadcast edge\");\n                  // we've already set this one up. Need to clone for the next work.\n                  r = (ReduceSinkOperator) OperatorFactory.getAndMakeChild(\n                      (ReduceSinkDesc)r.getConf().clone(), r.getParentOperators());\n                  context.clonedReduceSinks.add(r);\n                }\n                r.getConf().setOutputName(work.getName());\n                context.connectedReduceSinks.add(r);\n              }\n            }\n          }\n        }\n      }\n      // clear out the set. we don't need it anymore.\n      context.currentMapJoinOperators.clear();\n    }\n\n    if (!context.currentUnionOperators.isEmpty()) {\n      // if there are union all operators we need to add the work to the set\n      // of union operators.\n\n      UnionWork unionWork;\n      if (context.unionWorkMap.containsKey(operator)) {\n        // we've seen this terminal before and have created a union work object.\n        // just need to add this work to it. There will be no children of this one\n        // since we've passed this operator before.\n        assert operator.getChildOperators().isEmpty();\n        unionWork = (UnionWork) context.unionWorkMap.get(operator);\n\n      } else {\n        // first time through. we need to create a union work object and add this\n        // work to it. Subsequent work should reference the union and not the actual\n        // work.\n        unionWork = utils.createUnionWork(context, operator, tezWork);\n      }\n\n      // finally hook everything up\n      LOG.debug(\"Connecting union work (\"+unionWork+\") with work (\"+work+\")\");\n      TezEdgeProperty edgeProp = new TezEdgeProperty(EdgeType.CONTAINS);\n      tezWork.connect(unionWork, work, edgeProp);\n      unionWork.addUnionOperators(context.currentUnionOperators);\n      context.currentUnionOperators.clear();\n      context.workWithUnionOperators.add(work);\n      work = unionWork;\n    }\n\n\n    // This is where we cut the tree as described above. We also remember that\n    // we might have to connect parent work with this work later.\n    boolean removeParents = false;\n    for (Operator<?> parent: new ArrayList<Operator<?>>(root.getParentOperators())) {\n      removeParents = true;\n      context.leafOperatorToFollowingWork.put(parent, work);\n      LOG.debug(\"Removing \" + parent + \" as parent from \" + root);\n    }\n    if (removeParents) {\n      for (Operator<?> parent : new ArrayList<Operator<?>>(root.getParentOperators())) {\n        root.removeParent(parent);\n      }\n    }\n\n    // We're scanning a tree from roots to leaf (this is not technically\n    // correct, demux and mux operators might form a diamond shape, but\n    // we will only scan one path and ignore the others, because the\n    // diamond shape is always contained in a single vertex). The scan\n    // is depth first and because we remove parents when we pack a pipeline\n    // into a vertex we will never visit any node twice. But because of that\n    // we might have a situation where we need to connect 'work' that comes after\n    // the 'work' we're currently looking at.\n    //\n    // Also note: the concept of leaf and root is reversed in hive for historical\n    // reasons. Roots are data sources, leaves are data sinks. I know.\n    if (context.leafOperatorToFollowingWork.containsKey(operator)) {\n\n      BaseWork followingWork = context.leafOperatorToFollowingWork.get(operator);\n      long bytesPerReducer = context.conf.getLongVar(HiveConf.ConfVars.BYTESPERREDUCER);\n\n      LOG.debug(\"Second pass. Leaf operator: \"+operator\n        +\" has common downstream work:\"+followingWork);\n\n      if (operator instanceof DummyStoreOperator) {\n        // this is the small table side.\n        assert (followingWork instanceof MergeJoinWork);\n        MergeJoinWork mergeJoinWork = (MergeJoinWork) followingWork;\n        CommonMergeJoinOperator mergeJoinOp = mergeJoinWork.getMergeJoinOperator();\n        work.setTag(mergeJoinOp.getTagForOperator(operator));\n        mergeJoinWork.addMergedWork(null, work);\n        tezWork.setVertexType(mergeJoinWork, VertexType.MULTI_INPUT_UNINITIALIZED_EDGES);\n        for (BaseWork parentWork : tezWork.getParents(work)) {\n          TezEdgeProperty edgeProp = tezWork.getEdgeProperty(parentWork, work);\n          tezWork.disconnect(parentWork, work);\n          tezWork.connect(parentWork, mergeJoinWork, edgeProp);\n        }\n        work = mergeJoinWork;\n      } else {\n        // need to add this branch to the key + value info\n        assert operator instanceof ReduceSinkOperator\n            && ((followingWork instanceof ReduceWork) || (followingWork instanceof MergeJoinWork)\n                || followingWork instanceof UnionWork);\n        ReduceSinkOperator rs = (ReduceSinkOperator) operator;\n        ReduceWork rWork = null;\n        if (followingWork instanceof MergeJoinWork) {\n          MergeJoinWork mergeJoinWork = (MergeJoinWork) followingWork;\n          rWork = (ReduceWork) mergeJoinWork.getMainWork();\n        } else if (followingWork instanceof UnionWork) {\n          // this can only be possible if there is merge work followed by the union\n          UnionWork unionWork = (UnionWork) followingWork;\n          int index = getFollowingWorkIndex(tezWork, unionWork, rs);\n          if (index != -1) {\n            BaseWork baseWork = tezWork.getChildren(unionWork).get(index);\n            if (baseWork instanceof MergeJoinWork) {\n              MergeJoinWork mergeJoinWork = (MergeJoinWork) baseWork;\n              // disconnect the connection to union work and connect to merge work\n              followingWork = mergeJoinWork;\n              rWork = (ReduceWork) mergeJoinWork.getMainWork();\n            } else {\n              rWork = (ReduceWork) baseWork;\n            }\n          } else {\n            throw new SemanticException(\"Following work not found for the reduce sink: \"\n                + rs.getName());\n          }\n        } else {\n          rWork = (ReduceWork) followingWork;\n        }\n        GenMapRedUtils.setKeyAndValueDesc(rWork, rs);\n\n        // remember which parent belongs to which tag\n        int tag = rs.getConf().getTag();\n        rWork.getTagToInput().put(tag == -1 ? 0 : tag, work.getName());\n\n        // remember the output name of the reduce sink\n        rs.getConf().setOutputName(rWork.getName());\n\n        if (!context.connectedReduceSinks.contains(rs)) {\n          // add dependency between the two work items\n          TezEdgeProperty edgeProp;\n          if (rWork.isAutoReduceParallelism()) {\n            edgeProp =\n                new TezEdgeProperty(context.conf, EdgeType.SIMPLE_EDGE, true,\n                    rWork.getMinReduceTasks(), rWork.getMaxReduceTasks(), bytesPerReducer);\n          } else {\n            edgeProp = new TezEdgeProperty(EdgeType.SIMPLE_EDGE);\n          }\n          tezWork.connect(work, followingWork, edgeProp);\n          context.connectedReduceSinks.add(rs);\n        }\n      }\n    } else {\n      LOG.debug(\"First pass. Leaf operator: \"+operator);\n    }\n\n    // No children means we're at the bottom. If there are more operators to scan\n    // the next item will be a new root.\n    if (!operator.getChildOperators().isEmpty()) {\n      assert operator.getChildOperators().size() == 1;\n      context.parentOfRoot = operator;\n      context.currentRootOperator = operator.getChildOperators().get(0);\n      context.preceedingWork = work;\n    }\n\n    return null;\n  }",
                "code_after_change": "  public Object process(Node nd, Stack<Node> stack,\n      NodeProcessorCtx procContext, Object... nodeOutputs)\n      throws SemanticException {\n\n    GenTezProcContext context = (GenTezProcContext) procContext;\n\n    assert context != null && context.currentTask != null\n        && context.currentRootOperator != null;\n\n    // Operator is a file sink or reduce sink. Something that forces\n    // a new vertex.\n    Operator<?> operator = (Operator<?>) nd;\n\n    // root is the start of the operator pipeline we're currently\n    // packing into a vertex, typically a table scan, union or join\n    Operator<?> root = context.currentRootOperator;\n\n    LOG.debug(\"Root operator: \" + root);\n    LOG.debug(\"Leaf operator: \" + operator);\n\n    if (context.clonedReduceSinks.contains(operator)) {\n      // if we're visiting a terminal we've created ourselves,\n      // just skip and keep going\n      return null;\n    }\n\n    TezWork tezWork = context.currentTask.getWork();\n\n    // Right now the work graph is pretty simple. If there is no\n    // Preceding work we have a root and will generate a map\n    // vertex. If there is a preceding work we will generate\n    // a reduce vertex\n    BaseWork work;\n    if (context.rootToWorkMap.containsKey(root)) {\n      // having seen the root operator before means there was a branch in the\n      // operator graph. There's typically two reasons for that: a) mux/demux\n      // b) multi insert. Mux/Demux will hit the same leaf again, multi insert\n      // will result into a vertex with multiple FS or RS operators.\n      if (context.childToWorkMap.containsKey(operator)) {\n        // if we've seen both root and child, we can bail.\n\n        // clear out the mapjoin set. we don't need it anymore.\n        context.currentMapJoinOperators.clear();\n\n        // clear out the union set. we don't need it anymore.\n        context.currentUnionOperators.clear();\n\n        return null;\n      } else {\n        // At this point we don't have to do anything special. Just\n        // run through the regular paces w/o creating a new task.\n        work = context.rootToWorkMap.get(root);\n      }\n    } else {\n      // create a new vertex\n      if (context.preceedingWork == null) {\n        work = utils.createMapWork(context, root, tezWork, null);\n      } else {\n        work = utils.createReduceWork(context, root, tezWork);\n      }\n      context.rootToWorkMap.put(root, work);\n    }\n\n    // this is where we set the sort columns that we will be using for KeyValueInputMerge\n    if (operator instanceof DummyStoreOperator) {\n      work.addSortCols(root.getOpTraits().getSortCols().get(0));\n    }\n\n    if (!context.childToWorkMap.containsKey(operator)) {\n      List<BaseWork> workItems = new LinkedList<BaseWork>();\n      workItems.add(work);\n      context.childToWorkMap.put(operator, workItems);\n    } else {\n      context.childToWorkMap.get(operator).add(work);\n    }\n\n    // this transformation needs to be first because it changes the work item itself.\n    // which can affect the working of all downstream transformations.\n    if (context.currentMergeJoinOperator != null) {\n      // we are currently walking the big table side of the merge join. we need to create or hook up\n      // merge join work.\n      MergeJoinWork mergeJoinWork = null;\n      if (context.opMergeJoinWorkMap.containsKey(context.currentMergeJoinOperator)) {\n        // we have found a merge work corresponding to this closing operator. Hook up this work.\n        mergeJoinWork = context.opMergeJoinWorkMap.get(context.currentMergeJoinOperator);\n      } else {\n        // we need to create the merge join work\n        mergeJoinWork = new MergeJoinWork();\n        mergeJoinWork.setMergeJoinOperator(context.currentMergeJoinOperator);\n        tezWork.add(mergeJoinWork);\n        context.opMergeJoinWorkMap.put(context.currentMergeJoinOperator, mergeJoinWork);\n      }\n      // connect the work correctly.\n      work.addSortCols(root.getOpTraits().getSortCols().get(0));\n      mergeJoinWork.addMergedWork(work, null);\n      Operator<? extends OperatorDesc> parentOp =\n          getParentFromStack(context.currentMergeJoinOperator, stack);\n      int pos = context.currentMergeJoinOperator.getTagForOperator(parentOp);\n      work.setTag(pos);\n      tezWork.setVertexType(work, VertexType.MULTI_INPUT_UNINITIALIZED_EDGES);\n      for (BaseWork parentWork : tezWork.getParents(work)) {\n        TezEdgeProperty edgeProp = tezWork.getEdgeProperty(parentWork, work);\n        tezWork.disconnect(parentWork, work);\n        tezWork.connect(parentWork, mergeJoinWork, edgeProp);\n      }\n\n      for (BaseWork childWork : tezWork.getChildren(work)) {\n        TezEdgeProperty edgeProp = tezWork.getEdgeProperty(work, childWork);\n        tezWork.disconnect(work, childWork);\n        tezWork.connect(mergeJoinWork, childWork, edgeProp);\n      }\n      tezWork.remove(work);\n      context.rootToWorkMap.put(root, mergeJoinWork);\n      context.childToWorkMap.get(operator).remove(work);\n      context.childToWorkMap.get(operator).add(mergeJoinWork);\n      work = mergeJoinWork;\n      context.currentMergeJoinOperator = null;\n    }\n\n    // remember which mapjoin operator links with which work\n    if (!context.currentMapJoinOperators.isEmpty()) {\n      for (MapJoinOperator mj: context.currentMapJoinOperators) {\n        LOG.debug(\"Processing map join: \" + mj);\n        // remember the mapping in case we scan another branch of the\n        // mapjoin later\n        if (!context.mapJoinWorkMap.containsKey(mj)) {\n          List<BaseWork> workItems = new LinkedList<BaseWork>();\n          workItems.add(work);\n          context.mapJoinWorkMap.put(mj, workItems);\n        } else {\n          context.mapJoinWorkMap.get(mj).add(work);\n        }\n\n        /*\n         * this happens in case of map join operations.\n         * The tree looks like this:\n         *\n         *        RS <--- we are here perhaps\n         *        |\n         *     MapJoin\n         *     /     \\\n         *   RS       TS\n         *  /\n         * TS\n         *\n         * If we are at the RS pointed above, and we may have already visited the\n         * RS following the TS, we have already generated work for the TS-RS.\n         * We need to hook the current work to this generated work.\n         */\n        if (context.linkOpWithWorkMap.containsKey(mj)) {\n          Map<BaseWork,TezEdgeProperty> linkWorkMap = context.linkOpWithWorkMap.get(mj);\n          if (linkWorkMap != null) {\n            if (context.linkChildOpWithDummyOp.containsKey(mj)) {\n              for (Operator<?> dummy: context.linkChildOpWithDummyOp.get(mj)) {\n                work.addDummyOp((HashTableDummyOperator) dummy);\n              }\n            }\n            for (Entry<BaseWork,TezEdgeProperty> parentWorkMap : linkWorkMap.entrySet()) {\n              BaseWork parentWork = parentWorkMap.getKey();\n              LOG.debug(\"connecting \"+parentWork.getName()+\" with \"+work.getName());\n              TezEdgeProperty edgeProp = parentWorkMap.getValue();\n              tezWork.connect(parentWork, work, edgeProp);\n              if (edgeProp.getEdgeType() == EdgeType.CUSTOM_EDGE) {\n                tezWork.setVertexType(work, VertexType.INITIALIZED_EDGES);\n              }\n\n              // need to set up output name for reduce sink now that we know the name\n              // of the downstream work\n              for (ReduceSinkOperator r:\n                     context.linkWorkWithReduceSinkMap.get(parentWork)) {\n                if (r.getConf().getOutputName() != null) {\n                  LOG.debug(\"Cloning reduce sink for multi-child broadcast edge\");\n                  // we've already set this one up. Need to clone for the next work.\n                  r = (ReduceSinkOperator) OperatorFactory.getAndMakeChild(\n                      (ReduceSinkDesc)r.getConf().clone(), r.getParentOperators());\n                  context.clonedReduceSinks.add(r);\n                }\n                r.getConf().setOutputName(work.getName());\n                context.connectedReduceSinks.add(r);\n              }\n            }\n          }\n        }\n      }\n      // clear out the set. we don't need it anymore.\n      context.currentMapJoinOperators.clear();\n    }\n\n    // This is where we cut the tree as described above. We also remember that\n    // we might have to connect parent work with this work later.\n    for (Operator<?> parent : new ArrayList<Operator<?>>(root.getParentOperators())) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Removing \" + parent + \" as parent from \" + root);\n      }\n      context.leafOperatorToFollowingWork.put(parent, work);\n      root.removeParent(parent);\n    }\n\n    if (!context.currentUnionOperators.isEmpty()) {\n      // if there are union all operators we need to add the work to the set\n      // of union operators.\n\n      UnionWork unionWork;\n      if (context.unionWorkMap.containsKey(operator)) {\n        // we've seen this terminal before and have created a union work object.\n        // just need to add this work to it. There will be no children of this one\n        // since we've passed this operator before.\n        assert operator.getChildOperators().isEmpty();\n        unionWork = (UnionWork) context.unionWorkMap.get(operator);\n\n      } else {\n        // first time through. we need to create a union work object and add this\n        // work to it. Subsequent work should reference the union and not the actual\n        // work.\n        unionWork = utils.createUnionWork(context, operator, tezWork);\n      }\n\n      // finally hook everything up\n      LOG.debug(\"Connecting union work (\"+unionWork+\") with work (\"+work+\")\");\n      TezEdgeProperty edgeProp = new TezEdgeProperty(EdgeType.CONTAINS);\n      tezWork.connect(unionWork, work, edgeProp);\n      unionWork.addUnionOperators(context.currentUnionOperators);\n      context.currentUnionOperators.clear();\n      context.workWithUnionOperators.add(work);\n      work = unionWork;\n    }\n\n    // We're scanning a tree from roots to leaf (this is not technically\n    // correct, demux and mux operators might form a diamond shape, but\n    // we will only scan one path and ignore the others, because the\n    // diamond shape is always contained in a single vertex). The scan\n    // is depth first and because we remove parents when we pack a pipeline\n    // into a vertex we will never visit any node twice. But because of that\n    // we might have a situation where we need to connect 'work' that comes after\n    // the 'work' we're currently looking at.\n    //\n    // Also note: the concept of leaf and root is reversed in hive for historical\n    // reasons. Roots are data sources, leaves are data sinks. I know.\n    if (context.leafOperatorToFollowingWork.containsKey(operator)) {\n\n      BaseWork followingWork = context.leafOperatorToFollowingWork.get(operator);\n      long bytesPerReducer = context.conf.getLongVar(HiveConf.ConfVars.BYTESPERREDUCER);\n\n      LOG.debug(\"Second pass. Leaf operator: \"+operator\n        +\" has common downstream work:\"+followingWork);\n\n      if (operator instanceof DummyStoreOperator) {\n        // this is the small table side.\n        assert (followingWork instanceof MergeJoinWork);\n        MergeJoinWork mergeJoinWork = (MergeJoinWork) followingWork;\n        CommonMergeJoinOperator mergeJoinOp = mergeJoinWork.getMergeJoinOperator();\n        work.setTag(mergeJoinOp.getTagForOperator(operator));\n        mergeJoinWork.addMergedWork(null, work);\n        tezWork.setVertexType(mergeJoinWork, VertexType.MULTI_INPUT_UNINITIALIZED_EDGES);\n        for (BaseWork parentWork : tezWork.getParents(work)) {\n          TezEdgeProperty edgeProp = tezWork.getEdgeProperty(parentWork, work);\n          tezWork.disconnect(parentWork, work);\n          tezWork.connect(parentWork, mergeJoinWork, edgeProp);\n        }\n        work = mergeJoinWork;\n      } else {\n        // need to add this branch to the key + value info\n        assert operator instanceof ReduceSinkOperator\n            && ((followingWork instanceof ReduceWork) || (followingWork instanceof MergeJoinWork)\n                || followingWork instanceof UnionWork);\n        ReduceSinkOperator rs = (ReduceSinkOperator) operator;\n        ReduceWork rWork = null;\n        if (followingWork instanceof MergeJoinWork) {\n          MergeJoinWork mergeJoinWork = (MergeJoinWork) followingWork;\n          rWork = (ReduceWork) mergeJoinWork.getMainWork();\n        } else if (followingWork instanceof UnionWork) {\n          // this can only be possible if there is merge work followed by the union\n          UnionWork unionWork = (UnionWork) followingWork;\n          int index = getFollowingWorkIndex(tezWork, unionWork, rs);\n          BaseWork baseWork = tezWork.getChildren(unionWork).get(index);\n          if (baseWork instanceof MergeJoinWork) {\n            MergeJoinWork mergeJoinWork = (MergeJoinWork) baseWork;\n            // disconnect the connection to union work and connect to merge work\n            followingWork = mergeJoinWork;\n            rWork = (ReduceWork) mergeJoinWork.getMainWork();\n          } else {\n            rWork = (ReduceWork) baseWork;\n          }\n        } else {\n          rWork = (ReduceWork) followingWork;\n        }\n        GenMapRedUtils.setKeyAndValueDesc(rWork, rs);\n\n        // remember which parent belongs to which tag\n        int tag = rs.getConf().getTag();\n        rWork.getTagToInput().put(tag == -1 ? 0 : tag, work.getName());\n\n        // remember the output name of the reduce sink\n        rs.getConf().setOutputName(rWork.getName());\n\n        if (!context.connectedReduceSinks.contains(rs)) {\n          // add dependency between the two work items\n          TezEdgeProperty edgeProp;\n          if (rWork.isAutoReduceParallelism()) {\n            edgeProp =\n                new TezEdgeProperty(context.conf, EdgeType.SIMPLE_EDGE, true,\n                    rWork.getMinReduceTasks(), rWork.getMaxReduceTasks(), bytesPerReducer);\n          } else {\n            edgeProp = new TezEdgeProperty(EdgeType.SIMPLE_EDGE);\n          }\n          tezWork.connect(work, followingWork, edgeProp);\n          context.connectedReduceSinks.add(rs);\n        }\n      }\n    } else {\n      LOG.debug(\"First pass. Leaf operator: \"+operator);\n    }\n\n    // No children means we're at the bottom. If there are more operators to scan\n    // the next item will be a new root.\n    if (!operator.getChildOperators().isEmpty()) {\n      assert operator.getChildOperators().size() == 1;\n      context.parentOfRoot = operator;\n      context.currentRootOperator = operator.getChildOperators().get(0);\n      context.preceedingWork = work;\n    }\n\n    return null;\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report mentions a ClassCastException occurring in the 'org.apache.hadoop.hive.ql.parse.GenTezWork.process' method, which is part of the stack trace. This method is the ground truth method where the bug occurred, but the report does not explicitly identify it as the root cause, hence 'Partial' with 'Shared Stack Trace Context'. There is no fix suggestion provided in the bug report, so it is marked as 'Missing'. The problem location is also 'Partial' with 'Shared Stack Trace Context' because the method is mentioned in the stack trace but not directly identified as the problem location. There is no wrong information in the bug report as it accurately describes the error and the context in which it occurs."
        }
    },
    {
        "filename": "HIVE-10010.json",
        "code_diff": {
            "metastore.src.java.org.apache.hadoop.hive.metastore.hbase.SharedStorageDescriptor.setShared": {
                "code_before_change": "  void setShared(StorageDescriptor sd) {\n    shared = sd;\n  }",
                "code_after_change": "  void setShared(StorageDescriptor shared) {\n    if (shared.getCols() != null) super.setCols(shared.getCols());\n    // Skip location\n    if (shared.getInputFormat() != null) super.setInputFormat(shared.getInputFormat());\n    if (shared.getOutputFormat() != null) super.setOutputFormat(shared.getOutputFormat());\n    super.setCompressed(shared.isCompressed());\n    super.setNumBuckets(shared.getNumBuckets());\n    if (shared.getSerdeInfo() != null) super.setSerdeInfo(shared.getSerdeInfo());\n    if (shared.getBucketCols() != null) super.setBucketCols(shared.getBucketCols());\n    if (shared.getSortCols() != null) super.setSortCols(shared.getSortCols());\n    // skip parameters\n    if (shared.getSkewedInfo() != null) super.setSkewedInfo(shared.getSkewedInfo());\n    super.setStoredAsSubDirectories(shared.isStoredAsSubDirectories());\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.hbase.SharedStorageDescriptor.setReadOnly": {
                "code_before_change": [],
                "code_after_change": "  public void setReadOnly() {\n    colsCopied = serdeCopied = bucketsCopied = sortCopied = skewedCopied = true;\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.hbase.SharedStorageDescriptor.addToCols": {
                "code_before_change": "  public void addToCols(FieldSchema fs) {\n    copyOnWrite();\n    shared.addToCols(fs);\n  }",
                "code_after_change": "  public void addToCols(FieldSchema fs) {\n    copyCols();\n    super.addToCols(fs);\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.hbase.SharedStorageDescriptor.getCols": {
                "code_before_change": "  public List<FieldSchema> getCols() {\n    return copied ? shared.getCols() : (\n        shared.getCols() == null ? null : copyCols(shared.getCols()));\n  }",
                "code_after_change": "  public List<FieldSchema> getCols() {\n    copyCols();\n    return super.getCols();\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.hbase.SharedStorageDescriptor.setCols": {
                "code_before_change": "  public void setCols(List<FieldSchema> cols) {\n    copyOnWrite();\n    shared.setCols(cols);\n  }",
                "code_after_change": "  public void setCols(List<FieldSchema> cols) {\n    colsCopied = true;\n    super.setCols(cols);\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.hbase.SharedStorageDescriptor.unsetCols": {
                "code_before_change": "  public void unsetCols() {\n    copyOnWrite();\n    shared.unsetCols();\n  }",
                "code_after_change": "  public void unsetCols() {\n    colsCopied = true;\n    super.unsetCols();\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.hbase.SharedStorageDescriptor.getSerdeInfo": {
                "code_before_change": "  public SerDeInfo getSerdeInfo() {\n    return copied ? shared.getSerdeInfo() : (\n        shared.getSerdeInfo() == null ? null : new SerDeInfoWrapper(shared.getSerdeInfo()));\n  }",
                "code_after_change": "  public SerDeInfo getSerdeInfo() {\n    copySerde();\n    return super.getSerdeInfo();\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.hbase.SharedStorageDescriptor.setSerdeInfo": {
                "code_before_change": "  public void setSerdeInfo(SerDeInfo serdeInfo) {\n    copyOnWrite();\n    shared.setSerdeInfo(serdeInfo);\n  }",
                "code_after_change": "  public void setSerdeInfo(SerDeInfo serdeInfo) {\n    serdeCopied = true;\n    super.setSerdeInfo(serdeInfo);\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.hbase.SharedStorageDescriptor.addToBucketCols": {
                "code_before_change": "  public void addToBucketCols(String bucketCol) {\n    copyOnWrite();\n    shared.addToBucketCols(bucketCol);\n  }",
                "code_after_change": "  public void addToBucketCols(String bucket) {\n    copyBucketCols();\n    super.addToBucketCols(bucket);\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.hbase.SharedStorageDescriptor.getBucketCols": {
                "code_before_change": "  public List<String> getBucketCols() {\n    return copied ? shared.getBucketCols() : (\n        shared.getBucketCols() == null ? null : copyBucketCols(shared.getBucketCols()));\n  }",
                "code_after_change": "  public List<String> getBucketCols() {\n    copyBucketCols();\n    return super.getBucketCols();\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.hbase.SharedStorageDescriptor.setBucketCols": {
                "code_before_change": "  public void setBucketCols(List<String> bucketCols) {\n    copyOnWrite();\n    shared.setBucketCols(bucketCols);\n  }",
                "code_after_change": "  public void setBucketCols(List<String> buckets) {\n    bucketsCopied = true;\n    super.setBucketCols(buckets);\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.hbase.SharedStorageDescriptor.unsetBucketCols": {
                "code_before_change": "  public void unsetBucketCols() {\n    copyOnWrite();\n    shared.unsetBucketCols();\n  }",
                "code_after_change": "  public void unsetBucketCols() {\n    bucketsCopied = true;\n    super.unsetBucketCols();\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.hbase.SharedStorageDescriptor.getBucketColsIterator": {
                "code_before_change": "  public Iterator<String> getBucketColsIterator() {\n    return shared.getBucketColsIterator();\n  }",
                "code_after_change": "  public Iterator<String> getBucketColsIterator() {\n    copyBucketCols();\n    return super.getBucketColsIterator();\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.hbase.SharedStorageDescriptor.addToSortCols": {
                "code_before_change": "  public void addToSortCols(Order sortCol) {\n    copyOnWrite();\n    shared.addToSortCols(sortCol);\n  }",
                "code_after_change": "  public void addToSortCols(Order sort) {\n    copySort();\n    super.addToSortCols(sort);\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.hbase.SharedStorageDescriptor.getSortCols": {
                "code_before_change": "  public List<Order> getSortCols() {\n    return copied ? shared.getSortCols() : (\n        shared.getSortCols() == null ? null : copySort(shared.getSortCols()));\n  }",
                "code_after_change": "  public List<Order> getSortCols() {\n    copySort();\n    return super.getSortCols();\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.hbase.SharedStorageDescriptor.setSortCols": {
                "code_before_change": "  public void setSortCols(List<Order> sortCols) {\n    copyOnWrite();\n    shared.setSortCols(sortCols);\n  }",
                "code_after_change": "  public void setSortCols(List<Order> sorts) {\n    sortCopied = true;\n    super.setSortCols(sorts);\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.hbase.SharedStorageDescriptor.unsetSortCols": {
                "code_before_change": "  public void unsetSortCols() {\n    copyOnWrite();\n    shared.unsetSortCols();\n  }",
                "code_after_change": "  public void unsetSortCols() {\n    sortCopied = true;\n    super.unsetSortCols();\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.hbase.SharedStorageDescriptor.getSortColsIterator": {
                "code_before_change": "  public Iterator<Order> getSortColsIterator() {\n    return shared.getSortColsIterator();\n  }",
                "code_after_change": "  public Iterator<Order> getSortColsIterator() {\n    copySort();\n    return super.getSortColsIterator();\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.hbase.SharedStorageDescriptor.getSkewedInfo": {
                "code_before_change": "  public SkewedInfo getSkewedInfo() {\n    return copied ? shared.getSkewedInfo() : (\n        shared.getSkewedInfo() == null ? null : new SkewWrapper(shared.getSkewedInfo()));\n  }",
                "code_after_change": "  public SkewedInfo getSkewedInfo() {\n    copySkewed();\n    return super.getSkewedInfo();\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.hbase.SharedStorageDescriptor.setSkewedInfo": {
                "code_before_change": "  public void setSkewedInfo(SkewedInfo skewedInfo) {\n    copyOnWrite();\n    shared.setSkewedInfo(skewedInfo);\n  }",
                "code_after_change": "  public void setSkewedInfo(SkewedInfo skewedInfo) {\n    skewedCopied = true;\n    super.setSkewedInfo(skewedInfo);\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.hbase.SharedStorageDescriptor.unsetSkewedInfo": {
                "code_before_change": "  public void unsetSkewedInfo() {\n    copyOnWrite();\n    shared.unsetSkewedInfo();\n  }",
                "code_after_change": "  public void unsetSkewedInfo() {\n    skewedCopied = true;\n    super.unsetSkewedInfo();\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies a NullPointerException occurring during an alter table operation, with the stack trace pointing to methods like 'DDLTask.alterTable' and 'StorageDescriptor.<init>'. However, it does not precisely identify the root cause in the 'SharedStorageDescriptor' methods where the fix was applied. The report lacks any fix suggestion, as there is no 'Suggestions' or 'problem_location' field, nor any mention of a fix in the 'Description'. The problem location is partially identified as it mentions methods in the stack trace that are related to the ground truth methods. There is no wrong information in the report; it accurately describes the error and its context."
        }
    },
    {
        "filename": "HIVE-7763.json",
        "code_diff": {
            "ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkPlanGenerator.getInputFormat": {
                "code_before_change": [],
                "code_after_change": "  private Class getInputFormat(MapWork mWork) throws HiveException {\n    if (mWork.getInputformat() != null) {\n      HiveConf.setVar(jobConf, HiveConf.ConfVars.HIVEINPUTFORMAT,\n          mWork.getInputformat());\n    }\n    String inpFormat = HiveConf.getVar(jobConf,\n        HiveConf.ConfVars.HIVEINPUTFORMAT);\n    if ((inpFormat == null) || (StringUtils.isBlank(inpFormat))) {\n      inpFormat = ShimLoader.getHadoopShims().getInputFormatClassName();\n    }\n\n    if (mWork.isUseBucketizedHiveInputFormat()) {\n      inpFormat = BucketizedHiveInputFormat.class.getName();\n    }\n\n    Class inputFormatClass;\n    try {\n      inputFormatClass = Class.forName(inpFormat);\n    } catch (ClassNotFoundException e) {\n      String message = \"Failed to load specified input format class:\"\n          + inpFormat;\n      LOG.error(message, e);\n      throw new HiveException(message, e);\n    }\n\n    return inputFormatClass;\n  }"
            },
            "ql.src.java.org.apache.hadoop.hive.ql.exec.spark.SparkPlanGenerator.generate": {
                "code_before_change": [],
                "code_after_change": "  public SparkPlan generate(SparkWork sparkWork) throws Exception {\n    SparkPlan plan = new SparkPlan();\n    GraphTran trans = new GraphTran();\n    Set<BaseWork> roots = sparkWork.getRoots();\n    for (BaseWork w : roots) {\n      if (!(w instanceof MapWork)) {\n        throw new Exception(\n            \"The roots in the SparkWork must be MapWork instances!\");\n      }\n      MapWork mapWork = (MapWork) w;\n      SparkTran tran = generate(w);\n      JavaPairRDD<BytesWritable, BytesWritable> input = generateRDD(mapWork);\n      trans.addTranWithInput(tran, input);\n\n      while (sparkWork.getChildren(w).size() > 0) {\n        BaseWork child = sparkWork.getChildren(w).get(0);\n        if (child instanceof ReduceWork) {\n          SparkEdgeProperty edge = sparkWork.getEdgeProperty(w, child);\n          SparkShuffler st = generate(edge);\n          ReduceTran rt = generate((ReduceWork) child);\n          rt.setShuffler(st);\n          rt.setNumPartitions(edge.getNumPartitions());\n          trans.addTran(rt);\n          trans.connect(tran, rt);\n          w = child;\n          tran = rt;\n        } else if (child instanceof UnionWork) {\n          if (unionWorkTrans.get(child) != null) {\n            trans.connect(tran, unionWorkTrans.get(child));\n            break;\n          } else {\n            SparkTran ut = generate((UnionWork) child);\n            unionWorkTrans.put(child, ut);\n            trans.addTran(ut);\n            trans.connect(tran, ut);\n            w = child;\n            tran = ut;\n          }\n        }\n      }\n    }\n    unionWorkTrans.clear();\n    plan.setTran(trans);\n    return plan;\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report mentions the method 'org.apache.hadoop.hive.ql.exec.spark.SparkMapRecordHandler.init' in the stack trace, which is in the same stack trace context as the ground truth methods. However, it does not precisely identify the root cause or problem location as the ground truth methods 'SparkPlanGenerator.getInputFormat' and 'SparkPlanGenerator.generate'. There is no fix suggestion provided in the bug report. All information in the bug report is relevant to the context of the bug, so there is no wrong information."
        }
    },
    {
        "filename": "HIVE-12083.json",
        "code_diff": {
            "ql.src.java.org.apache.hadoop.hive.ql.metadata.Hive.getAggrColStatsFor": {
                "code_before_change": "  public AggrStats getAggrColStatsFor(String dbName, String tblName,\n    List<String> colNames, List<String> partName) {\n    try {\n      return getMSC().getAggrColStatsFor(dbName, tblName, colNames, partName);\n    } catch (Exception e) {\n      LOG.debug(StringUtils.stringifyException(e));\n      return null;\n    }\n  }",
                "code_after_change": "  public AggrStats getAggrColStatsFor(String dbName, String tblName,\n    List<String> colNames, List<String> partName) {\n    try {\n      return getMSC().getAggrColStatsFor(dbName, tblName, colNames, partName);\n    } catch (Exception e) {\n      LOG.debug(StringUtils.stringifyException(e));\n      return new AggrStats(new ArrayList<ColumnStatisticsObj>(),0);\n    }\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getTableStats": {
                "code_before_change": "  public ColumnStatistics getTableStats(\n      String dbName, String tableName, List<String> colNames) throws MetaException {\n    if (colNames.isEmpty()) {\n      return null;\n    }\n    doDbSpecificInitializationsBeforeQuery();\n    boolean doTrace = LOG.isDebugEnabled();\n    long start = doTrace ? System.nanoTime() : 0;\n    String queryText = \"select \" + STATS_COLLIST + \" from \" + STATS_TABLE_JOINED_TBLS\n        + \"where \" + STATS_DB_NAME + \" = ? and \" + STATS_TABLE_NAME + \" = ? \"\n        + \"and \\\"COLUMN_NAME\\\" in (\" +  makeParams(colNames.size()) + \")\";\n    Query query = pm.newQuery(\"javax.jdo.query.SQL\", queryText);\n    Object[] params = new Object[colNames.size() + 2];\n    params[0] = dbName;\n    params[1] = tableName;\n    for (int i = 0; i < colNames.size(); ++i) {\n      params[i + 2] = colNames.get(i);\n    }\n    Object qResult = executeWithArray(query, params, queryText);\n    long queryTime = doTrace ? System.nanoTime() : 0;\n    if (qResult == null) {\n      query.closeAll();\n      return null;\n    }\n    List<Object[]> list = ensureList(qResult);\n    if (list.isEmpty()) return null;\n    ColumnStatisticsDesc csd = new ColumnStatisticsDesc(true, dbName, tableName);\n    ColumnStatistics result = makeColumnStats(list, csd, 0);\n    timingTrace(doTrace, queryText, start, queryTime);\n    query.closeAll();\n    return result;\n  }",
                "code_after_change": "  public ColumnStatistics getTableStats(\n      String dbName, String tableName, List<String> colNames) throws MetaException {\n    if (colNames.isEmpty()) {\n      return null;\n    }\n    doDbSpecificInitializationsBeforeQuery();\n    boolean doTrace = LOG.isDebugEnabled();\n    long start = doTrace ? System.nanoTime() : 0;\n    String queryText = \"select \" + STATS_COLLIST + \" from \\\"TAB_COL_STATS\\\" \"\n      + \" where \\\"DB_NAME\\\" = ? and \\\"TABLE_NAME\\\" = ? and \\\"COLUMN_NAME\\\" in (\"\n      + makeParams(colNames.size()) + \")\";\n    Query query = pm.newQuery(\"javax.jdo.query.SQL\", queryText);\n    Object[] params = new Object[colNames.size() + 2];\n    params[0] = dbName;\n    params[1] = tableName;\n    for (int i = 0; i < colNames.size(); ++i) {\n      params[i + 2] = colNames.get(i);\n    }\n    Object qResult = executeWithArray(query, params, queryText);\n    long queryTime = doTrace ? System.nanoTime() : 0;\n    if (qResult == null) {\n      query.closeAll();\n      return null;\n    }\n    List<Object[]> list = ensureList(qResult);\n    if (list.isEmpty()) return null;\n    ColumnStatisticsDesc csd = new ColumnStatisticsDesc(true, dbName, tableName);\n    ColumnStatistics result = makeColumnStats(list, csd, 0);\n    timingTrace(doTrace, queryText, start, queryTime);\n    query.closeAll();\n    return result;\n  }"
            },
            "metastore.src.java.org.apache.hadoop.hive.metastore.HiveMetaStoreClient.drop_table_with_environment_context": {
                "code_before_change": "  protected void drop_table_with_environment_context(String dbname, String name,\n      boolean deleteData, EnvironmentContext envContext) throws MetaException, TException,\n      NoSuchObjectException, UnsupportedOperationException {\n    client.drop_table_with_environment_context(dbname, name, deleteData, envContext);\n  }",
                "code_after_change": "  protected void drop_table_with_environment_context(String dbname, String name,\n      boolean deleteData, EnvironmentContext envContext) throws MetaException, TException,\n      NoSuchObjectException, UnsupportedOperationException {\n    client.drop_table_with_environment_context(dbname, name, deleteData, envContext);\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Same Class or Module"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Same Class or Module"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the issue in the method 'aggrColStatsForPartitions' within the MetaStoreDirectSql class, which is in the same module as the ground truth method 'getTableStats'. However, it does not precisely identify the ground truth methods. The report does not provide a fix suggestion, as it only describes the problem and its consequences. The problem location is partially identified as it points to a method in the same class as the ground truth method. There is no wrong information in the report; it accurately describes the issue and its context."
        }
    },
    {
        "filename": "HIVE-14784.json",
        "code_diff": {
            "service.src.java.org.apache.hive.service.cli.operation.Operation.createOperationLog": {
                "code_before_change": "  protected void createOperationLog() {\n    if (parentSession.isOperationLogEnabled()) {\n      File operationLogFile = new File(parentSession.getOperationLogSessionDir(),\n          opHandle.getHandleIdentifier().toString());\n      isOperationLogEnabled = true;\n\n      // create log file\n      try {\n        if (operationLogFile.exists()) {\n          LOG.warn(\"The operation log file should not exist, but it is already there: \" +\n              operationLogFile.getAbsolutePath());\n          operationLogFile.delete();\n        }\n        if (!operationLogFile.createNewFile()) {\n          // the log file already exists and cannot be deleted.\n          // If it can be read/written, keep its contents and use it.\n          if (!operationLogFile.canRead() || !operationLogFile.canWrite()) {\n            LOG.warn(\"The already existed operation log file cannot be recreated, \" +\n                \"and it cannot be read or written: \" + operationLogFile.getAbsolutePath());\n            isOperationLogEnabled = false;\n            return;\n          }\n        }\n      } catch (Exception e) {\n        LOG.warn(\"Unable to create operation log file: \" + operationLogFile.getAbsolutePath(), e);\n        isOperationLogEnabled = false;\n        return;\n      }\n\n      // create OperationLog object with above log file\n      try {\n        operationLog = new OperationLog(opHandle.toString(), operationLogFile, parentSession.getHiveConf());\n      } catch (FileNotFoundException e) {\n        LOG.warn(\"Unable to instantiate OperationLog object for operation: \" +\n            opHandle, e);\n        isOperationLogEnabled = false;\n        return;\n      }\n\n      // register this operationLog to current thread\n      OperationLog.setCurrentOperationLog(operationLog);\n    }\n  }",
                "code_after_change": "  protected void createOperationLog() {\n    if (parentSession.isOperationLogEnabled()) {\n      File operationLogFile = new File(parentSession.getOperationLogSessionDir(),\n          opHandle.getHandleIdentifier().toString());\n      isOperationLogEnabled = true;\n\n      // create log file\n      try {\n        if (operationLogFile.exists()) {\n          LOG.warn(\"The operation log file should not exist, but it is already there: \" +\n              operationLogFile.getAbsolutePath());\n          operationLogFile.delete();\n        }\n        if (!operationLogFile.getParentFile().exists()) {\n          LOG.warn(\"Operations log directory for this session does not exist, it could have been deleted \" +\n              \"externally. Recreating the directory for future queries in this session but the older operation \" +\n              \"logs for this session are no longer available\");\n          if (!operationLogFile.getParentFile().mkdir()) {\n            LOG.warn(\"Log directory for this session could not be created, disabling \" +\n                \"operation logs: \" + operationLogFile.getParentFile().getAbsolutePath());\n            isOperationLogEnabled = false;\n            return;\n          }\n        }\n        if (!operationLogFile.createNewFile()) {\n          // the log file already exists and cannot be deleted.\n          // If it can be read/written, keep its contents and use it.\n          if (!operationLogFile.canRead() || !operationLogFile.canWrite()) {\n            LOG.warn(\"The already existed operation log file cannot be recreated, \" +\n                \"and it cannot be read or written: \" + operationLogFile.getAbsolutePath());\n            isOperationLogEnabled = false;\n            return;\n          }\n        }\n      } catch (Exception e) {\n        LOG.warn(\"Unable to create operation log file: \" + operationLogFile.getAbsolutePath(), e);\n        isOperationLogEnabled = false;\n        return;\n      }\n\n      // create OperationLog object with above log file\n      try {\n        operationLog = new OperationLog(opHandle.toString(), operationLogFile, parentSession.getHiveConf());\n      } catch (FileNotFoundException e) {\n        LOG.warn(\"Unable to instantiate OperationLog object for operation: \" +\n            opHandle, e);\n        isOperationLogEnabled = false;\n        return;\n      }\n\n      // register this operationLog to current thread\n      OperationLog.setCurrentOperationLog(operationLog);\n    }\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report precisely identifies the root cause related to the ground truth method 'Operation.createOperationLog', as it describes the issue with the operation log file creation when the parent directory is missing. The report does not provide any fix suggestion, hence it is marked as 'Missing' for fix suggestion. The problem location is also precise as the stack trace and description point directly to the 'createOperationLog' method. There is no wrong information in the bug report as all details are consistent with the context of the bug."
        }
    }
]