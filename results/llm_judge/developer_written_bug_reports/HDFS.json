[
    {
        "filename": "HDFS-4558.json",
        "code_diff": {
            "src.hdfs.org.apache.hadoop.hdfs.server.balancer.Balancer.checkReplicationPolicyCompatibility": {
                "code_before_change": [],
                "code_after_change": "  private void checkReplicationPolicyCompatibility(Configuration conf)\n      throws UnsupportedActionException {\n    if (!(BlockPlacementPolicy.getInstance(conf, null, null) \n        instanceof BlockPlacementPolicyDefault)) {\n      throw new UnsupportedActionException(\n          \"Balancer without BlockPlacementPolicyDefault\");\n    }\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report precisely identifies the root cause by pointing to the method 'checkReplicationPolicyCompatibility' in the Balancer class, which matches the ground truth method. The report does not provide any fix suggestion, hence it is marked as 'Missing' for fix suggestion. The problem location is also precisely identified as it directly mentions the method 'checkReplicationPolicyCompatibility', which is the ground truth method. There is no wrong information in the bug report as all the details are relevant and correctly related to the bug."
        }
    },
    {
        "filename": "HDFS-13023.json",
        "code_diff": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.qjournal.server.JournalNodeRpcServer.getJournalCTime": {
                "code_before_change": "  public Long getJournalCTime(String journalId,\n                              String nameServiceId) throws IOException {\n    return jn.getJournalCTime(journalId, nameServiceId);\n  }",
                "code_after_change": "  public Long getJournalCTime(String journalId,\n                              String nameServiceId) throws IOException {\n    return jn.getJournalCTime(journalId, nameServiceId);\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.qjournal.server.JournalNodeSyncer.getThrottler": {
                "code_before_change": "  private static DataTransferThrottler getThrottler(Configuration conf) {\n    long transferBandwidth =\n        conf.getLong(DFSConfigKeys.DFS_EDIT_LOG_TRANSFER_RATE_KEY,\n            DFSConfigKeys.DFS_EDIT_LOG_TRANSFER_RATE_DEFAULT);\n    DataTransferThrottler throttler = null;\n    if (transferBandwidth > 0) {\n      throttler = new DataTransferThrottler(transferBandwidth);\n    }\n    return throttler;\n  }",
                "code_after_change": "  private static DataTransferThrottler getThrottler(Configuration conf) {\n    long transferBandwidth =\n        conf.getLong(DFSConfigKeys.DFS_EDIT_LOG_TRANSFER_RATE_KEY,\n            DFSConfigKeys.DFS_EDIT_LOG_TRANSFER_RATE_DEFAULT);\n    DataTransferThrottler throttler = null;\n    if (transferBandwidth > 0) {\n      throttler = new DataTransferThrottler(transferBandwidth);\n    }\n    return throttler;\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.qjournal.server.JournalNodeSyncer.syncWithJournalAtIndex": {
                "code_before_change": "  private void syncWithJournalAtIndex(int index) {\n    LOG.info(\"Syncing Journal \" + jn.getBoundIpcAddress().getAddress() + \":\"\n        + jn.getBoundIpcAddress().getPort() + \" with \"\n        + otherJNProxies.get(index) + \", journal id: \" + jid);\n    final QJournalProtocolPB jnProxy = otherJNProxies.get(index).jnProxy;\n    if (jnProxy == null) {\n      LOG.error(\"JournalNode Proxy not found.\");\n      return;\n    }\n\n    List<RemoteEditLog> thisJournalEditLogs;\n    try {\n      thisJournalEditLogs = journal.getEditLogManifest(0, false).getLogs();\n    } catch (IOException e) {\n      LOG.error(\"Exception in getting local edit log manifest\", e);\n      return;\n    }\n\n    GetEditLogManifestResponseProto editLogManifest;\n    try {\n      editLogManifest = jnProxy.getEditLogManifest(null,\n          GetEditLogManifestRequestProto.newBuilder().setJid(jidProto)\n              .setSinceTxId(0)\n              .setInProgressOk(false).build());\n    } catch (ServiceException e) {\n      LOG.error(\"Could not sync with Journal at \" +\n          otherJNProxies.get(journalNodeIndexForSync), e);\n      return;\n    }\n\n    getMissingLogSegments(thisJournalEditLogs, editLogManifest,\n        otherJNProxies.get(index));\n  }",
                "code_after_change": "  private void syncWithJournalAtIndex(int index) {\n    LOG.info(\"Syncing Journal \" + jn.getBoundIpcAddress().getAddress() + \":\"\n        + jn.getBoundIpcAddress().getPort() + \" with \"\n        + otherJNProxies.get(index) + \", journal id: \" + jid);\n    final InterQJournalProtocol jnProxy = otherJNProxies.get(index).jnProxy;\n    if (jnProxy == null) {\n      LOG.error(\"JournalNode Proxy not found.\");\n      return;\n    }\n\n    List<RemoteEditLog> thisJournalEditLogs;\n    try {\n      thisJournalEditLogs = journal.getEditLogManifest(0, false).getLogs();\n    } catch (IOException e) {\n      LOG.error(\"Exception in getting local edit log manifest\", e);\n      return;\n    }\n\n    GetEditLogManifestFromJournalResponseProto editLogManifest;\n    try {\n      editLogManifest = jnProxy.getEditLogManifestFromJournal(jid,\n          nameServiceId, 0, false);\n    } catch (IOException e) {\n      LOG.error(\"Could not sync with Journal at \" +\n          otherJNProxies.get(journalNodeIndexForSync), e);\n      return;\n    }\n\n    getMissingLogSegments(thisJournalEditLogs, editLogManifest,\n        otherJNProxies.get(index));\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.qjournal.server.JournalNodeSyncer.getJournalAddrList": {
                "code_before_change": "  private List<InetSocketAddress> getJournalAddrList(String uriStr) throws\n      URISyntaxException,\n      IOException {\n    URI uri = new URI(uriStr);\n    return Util.getLoggerAddresses(uri,\n        Sets.newHashSet(jn.getBoundIpcAddress()));\n  }",
                "code_after_change": "  private List<InetSocketAddress> getJournalAddrList(String uriStr) throws\n      URISyntaxException,\n      IOException {\n    URI uri = new URI(uriStr);\n    return Util.getLoggerAddresses(uri,\n        Sets.newHashSet(jn.getBoundIpcAddress()));\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report mentions the method 'syncWithJournalAtIndex' in the stack trace, which is a ground truth method, but it does not explicitly identify it as the root cause. Therefore, it is classified as 'Partial' with 'Shared Stack Trace Context' as the sub-category. There is no fix suggestion provided in the bug report, so it is marked as 'Missing'. The problem location is also identified as 'Partial' with 'Shared Stack Trace Context' because the method is mentioned in the stack trace but not directly in the problem location field. There is no wrong information in the bug report as all the details are relevant to the context of the bug."
        }
    },
    {
        "filename": "HDFS-3157.json",
        "code_diff": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.checkReplicaCorrupt": {
                "code_before_change": "  private BlockToMarkCorrupt checkReplicaCorrupt(\n      Block iblk, ReplicaState reportedState, \n      BlockInfo storedBlock, BlockUCState ucState, \n      DatanodeDescriptor dn) {\n    switch(reportedState) {\n    case FINALIZED:\n      switch(ucState) {\n      case COMPLETE:\n      case COMMITTED:\n        if (storedBlock.getGenerationStamp() != iblk.getGenerationStamp()) {\n          return new BlockToMarkCorrupt(storedBlock,\n              \"block is \" + ucState + \" and reported genstamp \" +\n              iblk.getGenerationStamp() + \" does not match \" +\n              \"genstamp in block map \" + storedBlock.getGenerationStamp());\n        } else if (storedBlock.getNumBytes() != iblk.getNumBytes()) {\n          return new BlockToMarkCorrupt(storedBlock,\n              \"block is \" + ucState + \" and reported length \" +\n              iblk.getNumBytes() + \" does not match \" +\n              \"length in block map \" + storedBlock.getNumBytes());\n        } else {\n          return null; // not corrupt\n        }\n      default:\n        return null;\n      }\n    case RBW:\n    case RWR:\n      if (!storedBlock.isComplete()) {\n        return null; // not corrupt\n      } else if (storedBlock.getGenerationStamp() != iblk.getGenerationStamp()) {\n        return new BlockToMarkCorrupt(storedBlock,\n            \"reported \" + reportedState + \" replica with genstamp \" +\n            iblk.getGenerationStamp() + \" does not match COMPLETE block's \" +\n            \"genstamp in block map \" + storedBlock.getGenerationStamp());\n      } else { // COMPLETE block, same genstamp\n        if (reportedState == ReplicaState.RBW) {\n          // If it's a RBW report for a COMPLETE block, it may just be that\n          // the block report got a little bit delayed after the pipeline\n          // closed. So, ignore this report, assuming we will get a\n          // FINALIZED replica later. See HDFS-2791\n          LOG.info(\"Received an RBW replica for block \" + storedBlock +\n              \" on \" + dn.getName() + \": ignoring it, since the block is \" +\n              \"complete with the same generation stamp.\");\n          return null;\n        } else {\n          return new BlockToMarkCorrupt(storedBlock,\n              \"reported replica has invalid state \" + reportedState);\n        }\n      }\n    case RUR:       // should not be reported\n    case TEMPORARY: // should not be reported\n    default:\n      String msg = \"Unexpected replica state \" + reportedState\n      + \" for block: \" + storedBlock + \n      \" on \" + dn.getName() + \" size \" + storedBlock.getNumBytes();\n      // log here at WARN level since this is really a broken HDFS\n      // invariant\n      LOG.warn(msg);\n      return new BlockToMarkCorrupt(storedBlock, msg);\n    }\n  }",
                "code_after_change": "  private BlockToMarkCorrupt checkReplicaCorrupt(\n      Block iblk, ReplicaState reportedState, \n      BlockInfo storedBlock, BlockUCState ucState, \n      DatanodeDescriptor dn) {\n    switch(reportedState) {\n    case FINALIZED:\n      switch(ucState) {\n      case COMPLETE:\n      case COMMITTED:\n        if (storedBlock.getGenerationStamp() != iblk.getGenerationStamp()) {\n          return new BlockToMarkCorrupt(new BlockInfo(iblk, storedBlock\n              .getINode().getReplication()),\n              \"block is \" + ucState + \" and reported genstamp \" +\n              iblk.getGenerationStamp() + \" does not match \" +\n              \"genstamp in block map \" + storedBlock.getGenerationStamp());\n        } else if (storedBlock.getNumBytes() != iblk.getNumBytes()) {\n          return new BlockToMarkCorrupt(storedBlock,\n              \"block is \" + ucState + \" and reported length \" +\n              iblk.getNumBytes() + \" does not match \" +\n              \"length in block map \" + storedBlock.getNumBytes());\n        } else {\n          return null; // not corrupt\n        }\n      default:\n        return null;\n      }\n    case RBW:\n    case RWR:\n      if (!storedBlock.isComplete()) {\n        return null; // not corrupt\n      } else if (storedBlock.getGenerationStamp() != iblk.getGenerationStamp()) {\n        return new BlockToMarkCorrupt(new BlockInfo(iblk, storedBlock\n            .getINode().getReplication()),\n            \"reported \" + reportedState + \" replica with genstamp \" +\n            iblk.getGenerationStamp() + \" does not match COMPLETE block's \" +\n            \"genstamp in block map \" + storedBlock.getGenerationStamp());\n      } else { // COMPLETE block, same genstamp\n        if (reportedState == ReplicaState.RBW) {\n          // If it's a RBW report for a COMPLETE block, it may just be that\n          // the block report got a little bit delayed after the pipeline\n          // closed. So, ignore this report, assuming we will get a\n          // FINALIZED replica later. See HDFS-2791\n          LOG.info(\"Received an RBW replica for block \" + storedBlock +\n              \" on \" + dn + \": ignoring it, since the block is \" +\n              \"complete with the same generation stamp.\");\n          return null;\n        } else {\n          return new BlockToMarkCorrupt(storedBlock,\n              \"reported replica has invalid state \" + reportedState);\n        }\n      }\n    case RUR:       // should not be reported\n    case TEMPORARY: // should not be reported\n    default:\n      String msg = \"Unexpected replica state \" + reportedState\n      + \" for block: \" + storedBlock + \n      \" on \" + dn + \" size \" + storedBlock.getNumBytes();\n      // log here at WARN level since this is really a broken HDFS\n      // invariant\n      LOG.warn(msg);\n      return new BlockToMarkCorrupt(storedBlock, msg);\n    }\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report describes an error related to block deletion and mentions stack traces involving methods like `FSDataset.invalidate` and `BPOfferService.processCommandFromActive`, which are in the shared stack trace context with the ground truth method `BlockManager.checkReplicaCorrupt`. However, it does not precisely identify the root cause or the exact method where the fix was applied. There is no fix suggestion provided in the bug report. The problem location is partially identified as it mentions methods in the shared stack trace context but not the exact ground truth method. There is no wrong information in the bug report as all the details are relevant to the context of the bug."
        }
    },
    {
        "filename": "HDFS-4850.json",
        "code_diff": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.processDirectoryDiffList": {
                "code_before_change": "  private void processDirectoryDiffList(DataInputStream in, ImageVisitor v,\n      String currentINodeName) throws IOException {\n    final int numDirDiff = in.readInt();\n    if (numDirDiff >= 0) {\n      v.visitEnclosingElement(ImageElement.SNAPSHOT_DIR_DIFFS,\n          ImageElement.NUM_SNAPSHOT_DIR_DIFF, numDirDiff);\n      for (int i = 0; i < numDirDiff; i++) {\n        // process directory diffs in reverse chronological oder\n        processDirectoryDiff(in, v, currentINodeName); \n      }\n      v.leaveEnclosingElement();\n    }\n  }",
                "code_after_change": "  private void processDirectoryDiffList(DataInputStream in, ImageVisitor v,\n      String currentINodeName) throws IOException {\n    final int numDirDiff = in.readInt();\n    if (numDirDiff >= 0) {\n      v.visitEnclosingElement(ImageElement.SNAPSHOT_DIR_DIFFS,\n          ImageElement.NUM_SNAPSHOT_DIR_DIFF, numDirDiff);\n      for (int i = 0; i < numDirDiff; i++) {\n        // process directory diffs in reverse chronological oder\n        processDirectoryDiff(in, v, currentINodeName); \n      }\n      v.leaveEnclosingElement();\n    }\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.INodesInPath.resolve": {
                "code_before_change": "  static INodesInPath resolve(final INodeDirectory startingDir,\n      final byte[][] components) throws UnresolvedLinkException {\n    return resolve(startingDir, components, components.length, false);\n  }",
                "code_after_change": "  static INodesInPath resolve(final INodeDirectory startingDir,\n      final byte[][] components) throws UnresolvedLinkException {\n    return resolve(startingDir, components, components.length, false);\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.processFileDiffList": {
                "code_before_change": "  private void processFileDiffList(DataInputStream in, ImageVisitor v,\n      String currentINodeName) throws IOException {\n    final int size = in.readInt();\n    if (size >= 0) {\n      v.visitEnclosingElement(ImageElement.SNAPSHOT_FILE_DIFFS,\n          ImageElement.NUM_SNAPSHOT_FILE_DIFF, size);\n      String snapshot = FSImageSerialization.readString(in);\n      v.visit(ImageElement.SNAPSHOT_DIFF_SNAPSHOTROOT, snapshot);\n      v.visit(ImageElement.SNAPSHOT_FILE_SIZE, in.readLong());\n      if (in.readBoolean()) {\n        v.visitEnclosingElement(ImageElement.SNAPSHOT_DIFF_SNAPSHOTINODE);\n        processINode(in, v, true, currentINodeName, true);\n        v.leaveEnclosingElement();\n      }\n      v.leaveEnclosingElement();\n    }\n  }",
                "code_after_change": "  private void processFileDiffList(DataInputStream in, ImageVisitor v,\n      String currentINodeName) throws IOException {\n    final int size = in.readInt();\n    if (size >= 0) {\n      v.visitEnclosingElement(ImageElement.SNAPSHOT_FILE_DIFFS,\n          ImageElement.NUM_SNAPSHOT_FILE_DIFF, size);\n      for (int i = 0; i < size; i++) {\n        processFileDiff(in, v, currentINodeName);\n      }\n      v.leaveEnclosingElement();\n    }\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.loadImage": {
                "code_before_change": "  public void loadImage(DataInputStream in, ImageVisitor v,\n      boolean skipBlocks) throws IOException {\n    boolean done = false;\n    try {\n      v.start();\n      v.visitEnclosingElement(ImageElement.FS_IMAGE);\n\n      imageVersion = in.readInt();\n      if( !canLoadVersion(imageVersion))\n        throw new IOException(\"Cannot process fslayout version \" + imageVersion);\n\n      v.visit(ImageElement.IMAGE_VERSION, imageVersion);\n      v.visit(ImageElement.NAMESPACE_ID, in.readInt());\n\n      long numInodes = in.readLong();\n\n      v.visit(ImageElement.GENERATION_STAMP, in.readLong());\n\n      if (LayoutVersion.supports(Feature.STORED_TXIDS, imageVersion)) {\n        v.visit(ImageElement.TRANSACTION_ID, in.readLong());\n      }\n      \n      if (LayoutVersion.supports(Feature.ADD_INODE_ID, imageVersion)) {\n        v.visit(ImageElement.LAST_INODE_ID, in.readLong());\n      }\n      \n      boolean supportSnapshot = LayoutVersion.supports(Feature.SNAPSHOT,\n          imageVersion);\n      if (supportSnapshot) {\n        v.visit(ImageElement.SNAPSHOT_COUNTER, in.readInt());\n        v.visit(ImageElement.NUM_SNAPSHOTS_TOTAL, in.readInt());\n      }\n      \n      if (LayoutVersion.supports(Feature.FSIMAGE_COMPRESSION, imageVersion)) {\n        boolean isCompressed = in.readBoolean();\n        v.visit(ImageElement.IS_COMPRESSED, String.valueOf(isCompressed));\n        if (isCompressed) {\n          String codecClassName = Text.readString(in);\n          v.visit(ImageElement.COMPRESS_CODEC, codecClassName);\n          CompressionCodecFactory codecFac = new CompressionCodecFactory(\n              new Configuration());\n          CompressionCodec codec = codecFac.getCodecByClassName(codecClassName);\n          if (codec == null) {\n            throw new IOException(\"Image compression codec not supported: \"\n                + codecClassName);\n          }\n          in = new DataInputStream(codec.createInputStream(in));\n        }\n      }\n      processINodes(in, v, numInodes, skipBlocks, supportSnapshot);\n      subtreeMap.clear();\n      dirNodeMap.clear();\n\n      processINodesUC(in, v, skipBlocks);\n\n      if (LayoutVersion.supports(Feature.DELEGATION_TOKEN, imageVersion)) {\n        processDelegationTokens(in, v);\n      }\n      \n      v.leaveEnclosingElement(); // FSImage\n      done = true;\n    } finally {\n      if (done) {\n        v.finish();\n      } else {\n        v.finishAbnormally();\n      }\n    }\n  }",
                "code_after_change": "  public void loadImage(DataInputStream in, ImageVisitor v,\n      boolean skipBlocks) throws IOException {\n    boolean done = false;\n    try {\n      v.start();\n      v.visitEnclosingElement(ImageElement.FS_IMAGE);\n\n      imageVersion = in.readInt();\n      if( !canLoadVersion(imageVersion))\n        throw new IOException(\"Cannot process fslayout version \" + imageVersion);\n\n      v.visit(ImageElement.IMAGE_VERSION, imageVersion);\n      v.visit(ImageElement.NAMESPACE_ID, in.readInt());\n\n      long numInodes = in.readLong();\n\n      v.visit(ImageElement.GENERATION_STAMP, in.readLong());\n\n      if (LayoutVersion.supports(Feature.STORED_TXIDS, imageVersion)) {\n        v.visit(ImageElement.TRANSACTION_ID, in.readLong());\n      }\n      \n      if (LayoutVersion.supports(Feature.ADD_INODE_ID, imageVersion)) {\n        v.visit(ImageElement.LAST_INODE_ID, in.readLong());\n      }\n      \n      boolean supportSnapshot = LayoutVersion.supports(Feature.SNAPSHOT,\n          imageVersion);\n      if (supportSnapshot) {\n        v.visit(ImageElement.SNAPSHOT_COUNTER, in.readInt());\n        int numSnapshots = in.readInt();\n        v.visit(ImageElement.NUM_SNAPSHOTS_TOTAL, numSnapshots);\n        for (int i = 0; i < numSnapshots; i++) {\n          processSnapshot(in, v);\n        }\n      }\n      \n      if (LayoutVersion.supports(Feature.FSIMAGE_COMPRESSION, imageVersion)) {\n        boolean isCompressed = in.readBoolean();\n        v.visit(ImageElement.IS_COMPRESSED, String.valueOf(isCompressed));\n        if (isCompressed) {\n          String codecClassName = Text.readString(in);\n          v.visit(ImageElement.COMPRESS_CODEC, codecClassName);\n          CompressionCodecFactory codecFac = new CompressionCodecFactory(\n              new Configuration());\n          CompressionCodec codec = codecFac.getCodecByClassName(codecClassName);\n          if (codec == null) {\n            throw new IOException(\"Image compression codec not supported: \"\n                + codecClassName);\n          }\n          in = new DataInputStream(codec.createInputStream(in));\n        }\n      }\n      processINodes(in, v, numInodes, skipBlocks, supportSnapshot);\n      subtreeMap.clear();\n      dirNodeMap.clear();\n\n      processINodesUC(in, v, skipBlocks);\n\n      if (LayoutVersion.supports(Feature.DELEGATION_TOKEN, imageVersion)) {\n        processDelegationTokens(in, v);\n      }\n      \n      v.leaveEnclosingElement(); // FSImage\n      done = true;\n    } finally {\n      if (done) {\n        v.finish();\n      } else {\n        v.finishAbnormally();\n      }\n    }\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.processBlocks": {
                "code_before_change": "  private void processBlocks(DataInputStream in, ImageVisitor v,\n      int numBlocks, boolean skipBlocks) throws IOException {\n    v.visitEnclosingElement(ImageElement.BLOCKS,\n                            ImageElement.NUM_BLOCKS, numBlocks);\n    \n    // directory or symlink, no blocks to process    \n    if(numBlocks == -1 || numBlocks == -2) { \n      v.leaveEnclosingElement(); // Blocks\n      return;\n    }\n    \n    if(skipBlocks) {\n      int bytesToSkip = ((Long.SIZE * 3 /* fields */) / 8 /*bits*/) * numBlocks;\n      if(in.skipBytes(bytesToSkip) != bytesToSkip)\n        throw new IOException(\"Error skipping over blocks\");\n      \n    } else {\n      for(int j = 0; j < numBlocks; j++) {\n        v.visitEnclosingElement(ImageElement.BLOCK);\n        v.visit(ImageElement.BLOCK_ID, in.readLong());\n        v.visit(ImageElement.NUM_BYTES, in.readLong());\n        v.visit(ImageElement.GENERATION_STAMP, in.readLong());\n        v.leaveEnclosingElement(); // Block\n      }\n    }\n    v.leaveEnclosingElement(); // Blocks\n  }",
                "code_after_change": "  private void processBlocks(DataInputStream in, ImageVisitor v,\n      int numBlocks, boolean skipBlocks) throws IOException {\n    v.visitEnclosingElement(ImageElement.BLOCKS,\n                            ImageElement.NUM_BLOCKS, numBlocks);\n    \n    // directory or symlink or reference node, no blocks to process    \n    if(numBlocks < 0) { \n      v.leaveEnclosingElement(); // Blocks\n      return;\n    }\n    \n    if(skipBlocks) {\n      int bytesToSkip = ((Long.SIZE * 3 /* fields */) / 8 /*bits*/) * numBlocks;\n      if(in.skipBytes(bytesToSkip) != bytesToSkip)\n        throw new IOException(\"Error skipping over blocks\");\n      \n    } else {\n      for(int j = 0; j < numBlocks; j++) {\n        v.visitEnclosingElement(ImageElement.BLOCK);\n        v.visit(ImageElement.BLOCK_ID, in.readLong());\n        v.visit(ImageElement.NUM_BYTES, in.readLong());\n        v.visit(ImageElement.GENERATION_STAMP, in.readLong());\n        v.leaveEnclosingElement(); // Block\n      }\n    }\n    v.leaveEnclosingElement(); // Blocks\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.processSnapshots": {
                "code_before_change": "  private void processSnapshots(DataInputStream in, ImageVisitor v,\n      String rootName) throws IOException {\n    final int numSnapshots = in.readInt();\n    if (numSnapshots >= 0) {\n      v.visitEnclosingElement(ImageElement.SNAPSHOTS,\n          ImageElement.NUM_SNAPSHOTS, numSnapshots);\n      for (int i = 0; i < numSnapshots; i++) {\n        // process snapshot\n        v.visitEnclosingElement(ImageElement.SNAPSHOT);\n        v.visit(ImageElement.SNAPSHOT_ID, in.readInt());\n        // process root of snapshot\n        v.visitEnclosingElement(ImageElement.SNAPSHOT_ROOT);\n        processINode(in, v, true, rootName, false);\n        v.leaveEnclosingElement();\n        v.leaveEnclosingElement();\n      }\n      v.visit(ImageElement.SNAPSHOT_QUOTA, in.readInt());\n      v.leaveEnclosingElement();\n    }\n  }",
                "code_after_change": "  private void processSnapshots(DataInputStream in, ImageVisitor v,\n      String rootName) throws IOException {\n    final int numSnapshots = in.readInt();\n    if (numSnapshots >= 0) {\n      v.visitEnclosingElement(ImageElement.SNAPSHOTS,\n          ImageElement.NUM_SNAPSHOTS, numSnapshots);\n      for (int i = 0; i < numSnapshots; i++) {\n        // process snapshot\n        v.visitEnclosingElement(ImageElement.SNAPSHOT);\n        v.visit(ImageElement.SNAPSHOT_ID, in.readInt());\n        v.leaveEnclosingElement();\n      }\n      v.visit(ImageElement.SNAPSHOT_QUOTA, in.readInt());\n      v.leaveEnclosingElement();\n    }\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.tools.offlineImageViewer.ImageLoaderCurrent.processINode": {
                "code_before_change": "  private void processINode(DataInputStream in, ImageVisitor v,\n      boolean skipBlocks, String parentName, boolean isSnapshotCopy)\n      throws IOException {\n    boolean supportSnapshot = \n        LayoutVersion.supports(Feature.SNAPSHOT, imageVersion);\n    boolean supportInodeId = \n        LayoutVersion.supports(Feature.ADD_INODE_ID, imageVersion);\n    \n    v.visitEnclosingElement(ImageElement.INODE);\n    String pathName = FSImageSerialization.readString(in);\n    if (parentName != null) {  // local name\n      pathName = \"/\" + pathName;\n      if (!\"/\".equals(parentName)) { // children of non-root directory\n        pathName = parentName + pathName;\n      }\n    }\n\n    long inodeId = INodeId.GRANDFATHER_INODE_ID;\n    v.visit(ImageElement.INODE_PATH, pathName);\n    if (supportInodeId) {\n      inodeId = in.readLong();\n      v.visit(ImageElement.INODE_ID, inodeId);\n    }\n    v.visit(ImageElement.REPLICATION, in.readShort());\n    v.visit(ImageElement.MODIFICATION_TIME, formatDate(in.readLong()));\n    if(LayoutVersion.supports(Feature.FILE_ACCESS_TIME, imageVersion))\n      v.visit(ImageElement.ACCESS_TIME, formatDate(in.readLong()));\n    v.visit(ImageElement.BLOCK_SIZE, in.readLong());\n    int numBlocks = in.readInt();\n\n    processBlocks(in, v, numBlocks, skipBlocks);\n    \n    if (numBlocks > 0) { // File\n      if (supportSnapshot) {\n        // process file diffs\n        processFileDiffList(in, v, parentName);\n        if (isSnapshotCopy) {\n          boolean underConstruction = in.readBoolean();\n          if (underConstruction) {\n            v.visit(ImageElement.CLIENT_NAME,\n                FSImageSerialization.readString(in));\n            v.visit(ImageElement.CLIENT_MACHINE,\n                FSImageSerialization.readString(in));\n          }\n        }\n      }\n    } else if (numBlocks == -1) { // Directory\n      if (supportSnapshot && supportInodeId) {\n        dirNodeMap.put(inodeId, pathName);\n      }\n      v.visit(ImageElement.NS_QUOTA, numBlocks == -1 ? in.readLong() : -1);\n      if (LayoutVersion.supports(Feature.DISKSPACE_QUOTA, imageVersion))\n        v.visit(ImageElement.DS_QUOTA, numBlocks == -1 ? in.readLong() : -1);\n      if (supportSnapshot) {\n        boolean snapshottable = in.readBoolean();\n        if (!snapshottable) {\n          boolean withSnapshot = in.readBoolean();\n          v.visit(ImageElement.IS_WITHSNAPSHOT_DIR, Boolean.toString(withSnapshot));\n        } else {\n          v.visit(ImageElement.IS_SNAPSHOTTABLE_DIR, Boolean.toString(snapshottable));\n        }\n      }\n    } else if (numBlocks == -2) {\n      v.visit(ImageElement.SYMLINK, Text.readString(in));\n    } else if (numBlocks == -3) { // reference node\n      final boolean isWithName = in.readBoolean();\n      int snapshotId = in.readInt();\n      if (isWithName) {\n        v.visit(ImageElement.SNAPSHOT_LAST_SNAPSHOT_ID, snapshotId);\n      } else {\n        v.visit(ImageElement.SNAPSHOT_DST_SNAPSHOT_ID, snapshotId);\n      }\n      \n      final boolean firstReferred = in.readBoolean();\n      if (firstReferred) {\n        v.visitEnclosingElement(ImageElement.SNAPSHOT_REF_INODE);\n        processINode(in, v, skipBlocks, parentName, isSnapshotCopy);\n        v.leaveEnclosingElement();  // referred inode    \n      } else {\n        v.visit(ImageElement.SNAPSHOT_REF_INODE_ID, in.readLong());\n      }\n    }\n\n    processPermission(in, v);\n    v.leaveEnclosingElement(); // INode\n  }",
                "code_after_change": "  private void processINode(DataInputStream in, ImageVisitor v,\n      boolean skipBlocks, String parentName, boolean isSnapshotCopy)\n      throws IOException {\n    boolean supportSnapshot = \n        LayoutVersion.supports(Feature.SNAPSHOT, imageVersion);\n    boolean supportInodeId = \n        LayoutVersion.supports(Feature.ADD_INODE_ID, imageVersion);\n    \n    v.visitEnclosingElement(ImageElement.INODE);\n    String pathName = FSImageSerialization.readString(in);\n    if (parentName != null) {  // local name\n      pathName = \"/\" + pathName;\n      if (!\"/\".equals(parentName)) { // children of non-root directory\n        pathName = parentName + pathName;\n      }\n    }\n\n    long inodeId = INodeId.GRANDFATHER_INODE_ID;\n    v.visit(ImageElement.INODE_PATH, pathName);\n    if (supportInodeId) {\n      inodeId = in.readLong();\n      v.visit(ImageElement.INODE_ID, inodeId);\n    }\n    v.visit(ImageElement.REPLICATION, in.readShort());\n    v.visit(ImageElement.MODIFICATION_TIME, formatDate(in.readLong()));\n    if(LayoutVersion.supports(Feature.FILE_ACCESS_TIME, imageVersion))\n      v.visit(ImageElement.ACCESS_TIME, formatDate(in.readLong()));\n    v.visit(ImageElement.BLOCK_SIZE, in.readLong());\n    int numBlocks = in.readInt();\n\n    processBlocks(in, v, numBlocks, skipBlocks);\n    \n    if (numBlocks >= 0) { // File\n      if (supportSnapshot) {\n        // process file diffs\n        processFileDiffList(in, v, parentName);\n        if (isSnapshotCopy) {\n          boolean underConstruction = in.readBoolean();\n          if (underConstruction) {\n            v.visit(ImageElement.CLIENT_NAME,\n                FSImageSerialization.readString(in));\n            v.visit(ImageElement.CLIENT_MACHINE,\n                FSImageSerialization.readString(in));\n          }\n        }\n      }\n      processPermission(in, v);\n    } else if (numBlocks == -1) { // Directory\n      if (supportSnapshot && supportInodeId) {\n        dirNodeMap.put(inodeId, pathName);\n      }\n      v.visit(ImageElement.NS_QUOTA, numBlocks == -1 ? in.readLong() : -1);\n      if (LayoutVersion.supports(Feature.DISKSPACE_QUOTA, imageVersion))\n        v.visit(ImageElement.DS_QUOTA, numBlocks == -1 ? in.readLong() : -1);\n      if (supportSnapshot) {\n        boolean snapshottable = in.readBoolean();\n        if (!snapshottable) {\n          boolean withSnapshot = in.readBoolean();\n          v.visit(ImageElement.IS_WITHSNAPSHOT_DIR, Boolean.toString(withSnapshot));\n        } else {\n          v.visit(ImageElement.IS_SNAPSHOTTABLE_DIR, Boolean.toString(snapshottable));\n        }\n      }\n      processPermission(in, v);\n    } else if (numBlocks == -2) {\n      v.visit(ImageElement.SYMLINK, Text.readString(in));\n    } else if (numBlocks == -3) { // reference node\n      final boolean isWithName = in.readBoolean();\n      int snapshotId = in.readInt();\n      if (isWithName) {\n        v.visit(ImageElement.SNAPSHOT_LAST_SNAPSHOT_ID, snapshotId);\n      } else {\n        v.visit(ImageElement.SNAPSHOT_DST_SNAPSHOT_ID, snapshotId);\n      }\n      \n      final boolean firstReferred = in.readBoolean();\n      if (firstReferred) {\n        v.visitEnclosingElement(ImageElement.SNAPSHOT_REF_INODE);\n        processINode(in, v, skipBlocks, parentName, isSnapshotCopy);\n        v.leaveEnclosingElement();  // referred inode    \n      } else {\n        v.visit(ImageElement.SNAPSHOT_REF_INODE_ID, in.readLong());\n      }\n    }\n\n    v.leaveEnclosingElement(); // INode\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies a NegativeArraySizeException occurring in the method 'org.apache.hadoop.io.Text.readString', which is part of the stack trace but not the exact root cause. The actual root cause involves methods like 'processINode' and 'loadImage' in the 'ImageLoaderCurrent' class, which are also part of the stack trace, hence 'Shared Stack Trace Context'. There is no explicit fix suggestion provided in the bug report, leading to a 'Missing' evaluation for fix suggestion. The problem location is identified in the stack trace context but not precisely, hence 'Partial' with 'Shared Stack Trace Context'. There is no incorrect information in the bug report, so 'No' for wrong information."
        }
    },
    {
        "filename": "HDFS-3415.json",
        "code_diff": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NNStorage.readAndInspectDirs": {
                "code_before_change": "  FSImageStorageInspector readAndInspectDirs()\n      throws IOException {\n    int minLayoutVersion = Integer.MAX_VALUE; // the newest\n    int maxLayoutVersion = Integer.MIN_VALUE; // the oldest\n    \n    // First determine what range of layout versions we're going to inspect\n    for (Iterator<StorageDirectory> it = dirIterator();\n         it.hasNext();) {\n      StorageDirectory sd = it.next();\n      if (!sd.getVersionFile().exists()) {\n        FSImage.LOG.warn(\"Storage directory \" + sd + \" contains no VERSION file. Skipping...\");\n        continue;\n      }\n      readProperties(sd); // sets layoutVersion\n      minLayoutVersion = Math.min(minLayoutVersion, getLayoutVersion());\n      maxLayoutVersion = Math.max(maxLayoutVersion, getLayoutVersion());\n    }\n    \n    if (minLayoutVersion > maxLayoutVersion) {\n      throw new IOException(\"No storage directories contained VERSION information\");\n    }\n    assert minLayoutVersion <= maxLayoutVersion;\n    \n    // If we have any storage directories with the new layout version\n    // (ie edits_<txnid>) then use the new inspector, which will ignore\n    // the old format dirs.\n    FSImageStorageInspector inspector;\n    if (LayoutVersion.supports(Feature.TXID_BASED_LAYOUT, minLayoutVersion)) {\n      inspector = new FSImageTransactionalStorageInspector();\n      if (!LayoutVersion.supports(Feature.TXID_BASED_LAYOUT, maxLayoutVersion)) {\n        FSImage.LOG.warn(\"Ignoring one or more storage directories with old layouts\");\n      }\n    } else {\n      inspector = new FSImagePreTransactionalStorageInspector();\n    }\n    \n    inspectStorageDirs(inspector);\n    return inspector;\n  }",
                "code_after_change": "  FSImageStorageInspector readAndInspectDirs()\n      throws IOException {\n    Integer layoutVersion = null;\n    boolean multipleLV = false;\n    StringBuilder layoutVersions = new StringBuilder();\n\n    // First determine what range of layout versions we're going to inspect\n    for (Iterator<StorageDirectory> it = dirIterator();\n         it.hasNext();) {\n      StorageDirectory sd = it.next();\n      if (!sd.getVersionFile().exists()) {\n        FSImage.LOG.warn(\"Storage directory \" + sd + \" contains no VERSION file. Skipping...\");\n        continue;\n      }\n      readProperties(sd); // sets layoutVersion\n      int lv = getLayoutVersion();\n      if (layoutVersion == null) {\n        layoutVersion = Integer.valueOf(lv);\n      } else if (!layoutVersion.equals(lv)) {\n        multipleLV = true;\n      }\n      layoutVersions.append(\"(\").append(sd.getRoot()).append(\", \").append(lv).append(\") \");\n    }\n    \n    if (layoutVersion == null) {\n      throw new IOException(\"No storage directories contained VERSION information\");\n    }\n    if (multipleLV) {            \n      throw new IOException(\n          \"Storage directories containe multiple layout versions: \"\n              + layoutVersions);\n    }\n    // If the storage directories are with the new layout version\n    // (ie edits_<txnid>) then use the new inspector, which will ignore\n    // the old format dirs.\n    FSImageStorageInspector inspector;\n    if (LayoutVersion.supports(Feature.TXID_BASED_LAYOUT, getLayoutVersion())) {\n      inspector = new FSImageTransactionalStorageInspector();\n    } else {\n      inspector = new FSImagePreTransactionalStorageInspector();\n    }\n    \n    inspectStorageDirs(inspector);\n    return inspector;\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NNStorage.inspectStorageDirs": {
                "code_before_change": "  void inspectStorageDirs(FSImageStorageInspector inspector)\n      throws IOException {\n\n    // Process each of the storage directories to find the pair of\n    // newest image file and edit file\n    for (Iterator<StorageDirectory> it = dirIterator(); it.hasNext();) {\n      StorageDirectory sd = it.next();\n      inspector.inspectDirectory(sd);\n    }\n  }",
                "code_after_change": "  void inspectStorageDirs(FSImageStorageInspector inspector)\n      throws IOException {\n\n    // Process each of the storage directories to find the pair of\n    // newest image file and edit file\n    for (Iterator<StorageDirectory> it = dirIterator(); it.hasNext();) {\n      StorageDirectory sd = it.next();\n      inspector.inspectDirectory(sd);\n    }\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report mentions a NullPointerException occurring in the method 'NNStorage.getStorageFile', which is part of the stack trace and shares context with the ground truth methods 'NNStorage.readAndInspectDirs' and 'NNStorage.inspectStorageDirs'. However, it does not precisely identify the root cause or problem location as being in the ground truth methods. There is no fix suggestion provided in the bug report. All information in the bug report is relevant and related to the context of the bug, so there is no wrong information."
        }
    },
    {
        "filename": "HDFS-2245.json",
        "code_diff": {
            "hdfs.src.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget": {
                "code_before_change": "  public DatanodeDescriptor[] chooseTarget(final String src,\n      final int numOfReplicas, final DatanodeDescriptor client,\n      final HashMap<Node, Node> excludedNodes,\n      final long blocksize) throws IOException {\n    // choose targets for the new block to be allocated.\n    final DatanodeDescriptor targets[] = blockplacement.chooseTarget(\n        src, numOfReplicas, client, excludedNodes, blocksize);\n    if (targets.length < minReplication) {\n      throw new IOException(\"File \" + src + \" could only be replicated to \" +\n                            targets.length + \" nodes, instead of \" +\n                            minReplication + \". There are \"\n                            + getDatanodeManager().getNetworkTopology().getNumOfLeaves()\n                            + \" datanode(s) running but \"+excludedNodes.size() +\n                            \" node(s) are excluded in this operation.\");\n    }\n    return targets;\n  }",
                "code_after_change": "  public DatanodeDescriptor[] chooseTarget(final String src,\n      final int numOfReplicas, final DatanodeDescriptor client,\n      final HashMap<Node, Node> excludedNodes,\n      final long blocksize) throws IOException {\n    // choose targets for the new block to be allocated.\n    final DatanodeDescriptor targets[] = blockplacement.chooseTarget(\n        src, numOfReplicas, client, excludedNodes, blocksize);\n    if (targets.length < minReplication) {\n      throw new IOException(\"File \" + src + \" could only be replicated to \"\n          + targets.length + \" nodes instead of minReplication (=\"\n          + minReplication + \").  There are \"\n          + getDatanodeManager().getNetworkTopology().getNumOfLeaves()\n          + \" datanode(s) running and \"\n          + (excludedNodes == null? \"no\": excludedNodes.size())\n          + \" node(s) are excluded in this operation.\");\n    }\n    return targets;\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report precisely identifies the root cause by mentioning the method 'BlockManager.chooseTarget' where the NullPointerException occurs, which matches the ground truth method. However, there is no fix suggestion provided in the bug report, as there is no 'Suggestions' or 'possible_fix' field, nor is there any suggestion in the 'Description' field. The problem location is also precisely identified as 'BlockManager.chooseTarget', which is the ground truth method. There is no wrong information in the bug report as all the details are relevant and correctly identify the issue."
        }
    },
    {
        "filename": "HDFS-10320.json",
        "code_diff": {
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.net.NetworkTopology.remove": {
                "code_before_change": "    boolean remove(Node n) {\n      if (!isAncestor(n)) {\n        throw new IllegalArgumentException(n.getName()\n            + \", which is located at \" + n.getNetworkLocation()\n            + \", is not a descendent of \" + getPath(this));\n      }\n      if (isParent(n)) {\n        // this node is the parent of n; remove n directly\n        if (childrenMap.containsKey(n.getName())) {\n          for (int i=0; i<children.size(); i++) {\n            if (children.get(i).getName().equals(n.getName())) {\n              children.remove(i);\n              childrenMap.remove(n.getName());\n              numOfLeaves--;\n              n.setParent(null);\n              return true;\n            }\n          }\n        }\n        return false;\n      } else {\n        // find the next ancestor node: the parent node\n        String parentName = getNextAncestorName(n);\n        InnerNode parentNode = (InnerNode)childrenMap.get(parentName);\n        if (parentNode == null) {\n          return false;\n        }\n        // remove n from the parent node\n        boolean isRemoved = parentNode.remove(n);\n        // if the parent node has no children, remove the parent node too\n        if (isRemoved) {\n          if (parentNode.getNumOfChildren() == 0) {\n            for(int i=0; i < children.size(); i++) {\n              if (children.get(i).getName().equals(parentName)) {\n                children.remove(i);\n                childrenMap.remove(parentName);\n                break;\n              }\n            }\n          }\n          numOfLeaves--;\n        }\n        return isRemoved;\n      }\n    } // end of remove",
                "code_after_change": "    boolean remove(Node n) {\n      if (!isAncestor(n)) {\n        throw new IllegalArgumentException(n.getName()\n            + \", which is located at \" + n.getNetworkLocation()\n            + \", is not a descendent of \" + getPath(this));\n      }\n      if (isParent(n)) {\n        // this node is the parent of n; remove n directly\n        if (childrenMap.containsKey(n.getName())) {\n          for (int i=0; i<children.size(); i++) {\n            if (children.get(i).getName().equals(n.getName())) {\n              children.remove(i);\n              childrenMap.remove(n.getName());\n              numOfLeaves--;\n              n.setParent(null);\n              return true;\n            }\n          }\n        }\n        return false;\n      } else {\n        // find the next ancestor node: the parent node\n        String parentName = getNextAncestorName(n);\n        InnerNode parentNode = (InnerNode)childrenMap.get(parentName);\n        if (parentNode == null) {\n          return false;\n        }\n        // remove n from the parent node\n        boolean isRemoved = parentNode.remove(n);\n        // if the parent node has no children, remove the parent node too\n        if (isRemoved) {\n          if (parentNode.getNumOfChildren() == 0) {\n            for(int i=0; i < children.size(); i++) {\n              if (children.get(i).getName().equals(parentName)) {\n                children.remove(i);\n                childrenMap.remove(parentName);\n                break;\n              }\n            }\n          }\n          numOfLeaves--;\n        }\n        return isRemoved;\n      }\n    } // end of remove"
            },
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.net.NetworkTopology.countNumOfAvailableNodes": {
                "code_before_change": "  public int countNumOfAvailableNodes(String scope,\n                                      Collection<Node> excludedNodes) {\n    boolean isExcluded=false;\n    if (scope.startsWith(\"~\")) {\n      isExcluded=true;\n      scope=scope.substring(1);\n    }\n    scope = NodeBase.normalize(scope);\n    int excludedCountInScope = 0; // the number of nodes in both scope & excludedNodes\n    int excludedCountOffScope = 0; // the number of nodes outside scope & excludedNodes\n    netlock.readLock().lock();\n    try {\n      for (Node node : excludedNodes) {\n        node = getNode(NodeBase.getPath(node));\n        if (node == null) {\n          continue;\n        }\n        if ((NodeBase.getPath(node) + NodeBase.PATH_SEPARATOR_STR)\n            .startsWith(scope + NodeBase.PATH_SEPARATOR_STR)) {\n          excludedCountInScope++;\n        } else {\n          excludedCountOffScope++;\n        }\n      }\n      Node n = getNode(scope);\n      int scopeNodeCount = 0;\n      if (n != null) {\n        scopeNodeCount++;\n      }\n      if (n instanceof InnerNode) {\n        scopeNodeCount=((InnerNode)n).getNumOfLeaves();\n      }\n      if (isExcluded) {\n        return clusterMap.getNumOfLeaves() - scopeNodeCount\n            - excludedCountOffScope;\n      } else {\n        return scopeNodeCount - excludedCountInScope;\n      }\n    } finally {\n      netlock.readLock().unlock();\n    }\n  }",
                "code_after_change": "  public int countNumOfAvailableNodes(String scope,\n                                      Collection<Node> excludedNodes) {\n    boolean isExcluded=false;\n    if (scope.startsWith(\"~\")) {\n      isExcluded=true;\n      scope=scope.substring(1);\n    }\n    scope = NodeBase.normalize(scope);\n    int excludedCountInScope = 0; // the number of nodes in both scope & excludedNodes\n    int excludedCountOffScope = 0; // the number of nodes outside scope & excludedNodes\n    netlock.readLock().lock();\n    try {\n      if (excludedNodes != null) {\n        for (Node node : excludedNodes) {\n          node = getNode(NodeBase.getPath(node));\n          if (node == null) {\n            continue;\n          }\n          if ((NodeBase.getPath(node) + NodeBase.PATH_SEPARATOR_STR)\n              .startsWith(scope + NodeBase.PATH_SEPARATOR_STR)) {\n            excludedCountInScope++;\n          } else {\n            excludedCountOffScope++;\n          }\n        }\n      }\n      Node n = getNode(scope);\n      int scopeNodeCount = 0;\n      if (n != null) {\n        scopeNodeCount++;\n      }\n      if (n instanceof InnerNode) {\n        scopeNodeCount=((InnerNode)n).getNumOfLeaves();\n      }\n      if (isExcluded) {\n        return clusterMap.getNumOfLeaves() - scopeNodeCount\n            - excludedCountOffScope;\n      } else {\n        return scopeNodeCount - excludedCountInScope;\n      }\n    } finally {\n      netlock.readLock().unlock();\n    }\n  }"
            },
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.net.NetworkTopology.getLeaves": {
                "code_before_change": "  public List<Node> getLeaves(String scope) {\n    Node node = getNode(scope);\n    List<Node> leafNodes = new ArrayList<Node>();\n    if (!(node instanceof InnerNode)) {\n      leafNodes.add(node);\n    } else {\n      InnerNode innerNode = (InnerNode) node;\n      for (int i=0;i<innerNode.getNumOfLeaves();i++) {\n        leafNodes.add(innerNode.getLeaf(i, null));\n      }\n    }\n    return leafNodes;\n  }",
                "code_after_change": "  public List<Node> getLeaves(String scope) {\n    Node node = getNode(scope);\n    List<Node> leafNodes = new ArrayList<Node>();\n    if (!(node instanceof InnerNode)) {\n      leafNodes.add(node);\n    } else {\n      InnerNode innerNode = (InnerNode) node;\n      for (int i=0;i<innerNode.getNumOfLeaves();i++) {\n        leafNodes.add(innerNode.getLeaf(i, null));\n      }\n    }\n    return leafNodes;\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods.redirectURI": {
                "code_before_change": "  private URI redirectURI(final NameNode namenode,\n      final UserGroupInformation ugi, final DelegationParam delegation,\n      final UserParam username, final DoAsParam doAsUser,\n      final String path, final HttpOpParam.Op op, final long openOffset,\n      final long blocksize, final String excludeDatanodes,\n      final Param<?, ?>... parameters) throws URISyntaxException, IOException {\n    final DatanodeInfo dn;\n    try {\n      dn = chooseDatanode(namenode, path, op, openOffset, blocksize,\n          excludeDatanodes);\n    } catch (InvalidTopologyException ite) {\n      throw new IOException(\"Failed to find datanode, suggest to check cluster health.\", ite);\n    }\n\n    final String delegationQuery;\n    if (!UserGroupInformation.isSecurityEnabled()) {\n      //security disabled\n      delegationQuery = Param.toSortedString(\"&\", doAsUser, username);\n    } else if (delegation.getValue() != null) {\n      //client has provided a token\n      delegationQuery = \"&\" + delegation;\n    } else {\n      //generate a token\n      final Token<? extends TokenIdentifier> t = generateDelegationToken(\n          namenode, ugi, request.getUserPrincipal().getName());\n      delegationQuery = \"&\" + new DelegationParam(t.encodeToUrlString());\n    }\n    final String query = op.toQueryString() + delegationQuery\n        + \"&\" + new NamenodeAddressParam(namenode)\n        + Param.toSortedString(\"&\", parameters);\n    final String uripath = WebHdfsFileSystem.PATH_PREFIX + path;\n\n    final String scheme = request.getScheme();\n    int port = \"http\".equals(scheme) ? dn.getInfoPort() : dn\n        .getInfoSecurePort();\n    final URI uri = new URI(scheme, null, dn.getHostName(), port, uripath,\n        query, null);\n\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"redirectURI=\" + uri);\n    }\n    return uri;\n  }",
                "code_after_change": "  private URI redirectURI(final NameNode namenode,\n      final UserGroupInformation ugi, final DelegationParam delegation,\n      final UserParam username, final DoAsParam doAsUser,\n      final String path, final HttpOpParam.Op op, final long openOffset,\n      final long blocksize, final String excludeDatanodes,\n      final Param<?, ?>... parameters) throws URISyntaxException, IOException {\n    final DatanodeInfo dn;\n    dn = chooseDatanode(namenode, path, op, openOffset, blocksize,\n        excludeDatanodes);\n    if (dn == null) {\n      throw new IOException(\"Failed to find datanode, suggest to check cluster\"\n          + \" health. excludeDatanodes=\" + excludeDatanodes);\n    }\n\n    final String delegationQuery;\n    if (!UserGroupInformation.isSecurityEnabled()) {\n      //security disabled\n      delegationQuery = Param.toSortedString(\"&\", doAsUser, username);\n    } else if (delegation.getValue() != null) {\n      //client has provided a token\n      delegationQuery = \"&\" + delegation;\n    } else {\n      //generate a token\n      final Token<? extends TokenIdentifier> t = generateDelegationToken(\n          namenode, ugi, request.getUserPrincipal().getName());\n      delegationQuery = \"&\" + new DelegationParam(t.encodeToUrlString());\n    }\n    final String query = op.toQueryString() + delegationQuery\n        + \"&\" + new NamenodeAddressParam(namenode)\n        + Param.toSortedString(\"&\", parameters);\n    final String uripath = WebHdfsFileSystem.PATH_PREFIX + path;\n\n    final String scheme = request.getScheme();\n    int port = \"http\".equals(scheme) ? dn.getInfoPort() : dn\n        .getInfoSecurePort();\n    final URI uri = new URI(scheme, null, dn.getHostName(), port, uripath,\n        query, null);\n\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"redirectURI=\" + uri);\n    }\n    return uri;\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.AvailableSpaceBlockPlacementPolicy.initialize": {
                "code_before_change": "  public void initialize(Configuration conf, FSClusterStats stats,\n      NetworkTopology clusterMap, Host2NodesMap host2datanodeMap) {\n    super.initialize(conf, stats, clusterMap, host2datanodeMap);\n    float balancedPreferencePercent =\n        conf.getFloat(\n          DFS_NAMENODE_AVAILABLE_SPACE_BLOCK_PLACEMENT_POLICY_BALANCED_SPACE_PREFERENCE_FRACTION_KEY,\n          DFS_NAMENODE_AVAILABLE_SPACE_BLOCK_PLACEMENT_POLICY_BALANCED_SPACE_PREFERENCE_FRACTION_DEFAULT);\n\n    LOG.info(\"Available space block placement policy initialized: \"\n        + DFSConfigKeys.DFS_NAMENODE_AVAILABLE_SPACE_BLOCK_PLACEMENT_POLICY_BALANCED_SPACE_PREFERENCE_FRACTION_KEY\n        + \" = \" + balancedPreferencePercent);\n\n    if (balancedPreferencePercent > 1.0) {\n      LOG.warn(\"The value of \"\n          + DFS_NAMENODE_AVAILABLE_SPACE_BLOCK_PLACEMENT_POLICY_BALANCED_SPACE_PREFERENCE_FRACTION_KEY\n          + \" is greater than 1.0 but should be in the range 0.0 - 1.0\");\n    }\n    if (balancedPreferencePercent < 0.5) {\n      LOG.warn(\"The value of \"\n          + DFS_NAMENODE_AVAILABLE_SPACE_BLOCK_PLACEMENT_POLICY_BALANCED_SPACE_PREFERENCE_FRACTION_KEY\n          + \" is less than 0.5 so datanodes with more used percent will\"\n          + \" receive  more block allocations.\");\n    }\n    balancedPreference = (int) (100 * balancedPreferencePercent);\n  }",
                "code_after_change": "  public void initialize(Configuration conf, FSClusterStats stats,\n      NetworkTopology clusterMap, Host2NodesMap host2datanodeMap) {\n    super.initialize(conf, stats, clusterMap, host2datanodeMap);\n    float balancedPreferencePercent =\n        conf.getFloat(\n          DFS_NAMENODE_AVAILABLE_SPACE_BLOCK_PLACEMENT_POLICY_BALANCED_SPACE_PREFERENCE_FRACTION_KEY,\n          DFS_NAMENODE_AVAILABLE_SPACE_BLOCK_PLACEMENT_POLICY_BALANCED_SPACE_PREFERENCE_FRACTION_DEFAULT);\n\n    LOG.info(\"Available space block placement policy initialized: \"\n        + DFSConfigKeys.DFS_NAMENODE_AVAILABLE_SPACE_BLOCK_PLACEMENT_POLICY_BALANCED_SPACE_PREFERENCE_FRACTION_KEY\n        + \" = \" + balancedPreferencePercent);\n\n    if (balancedPreferencePercent > 1.0) {\n      LOG.warn(\"The value of \"\n          + DFS_NAMENODE_AVAILABLE_SPACE_BLOCK_PLACEMENT_POLICY_BALANCED_SPACE_PREFERENCE_FRACTION_KEY\n          + \" is greater than 1.0 but should be in the range 0.0 - 1.0\");\n    }\n    if (balancedPreferencePercent < 0.5) {\n      LOG.warn(\"The value of \"\n          + DFS_NAMENODE_AVAILABLE_SPACE_BLOCK_PLACEMENT_POLICY_BALANCED_SPACE_PREFERENCE_FRACTION_KEY\n          + \" is less than 0.5 so datanodes with more used percent will\"\n          + \" receive  more block allocations.\");\n    }\n    balancedPreference = (int) (100 * balancedPreferencePercent);\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRandom": {
                "code_before_change": "  protected DatanodeStorageInfo chooseRandom(String scope,\n      Set<Node> excludedNodes,\n      long blocksize,\n      int maxNodesPerRack,\n      List<DatanodeStorageInfo> results,\n      boolean avoidStaleNodes,\n      EnumMap<StorageType, Integer> storageTypes)\n          throws NotEnoughReplicasException {\n    return chooseRandom(1, scope, excludedNodes, blocksize, maxNodesPerRack,\n        results, avoidStaleNodes, storageTypes);\n  }",
                "code_after_change": "  protected DatanodeStorageInfo chooseRandom(String scope,\n      Set<Node> excludedNodes,\n      long blocksize,\n      int maxNodesPerRack,\n      List<DatanodeStorageInfo> results,\n      boolean avoidStaleNodes,\n      EnumMap<StorageType, Integer> storageTypes)\n          throws NotEnoughReplicasException {\n    return chooseRandom(1, scope, excludedNodes, blocksize, maxNodesPerRack,\n        results, avoidStaleNodes, storageTypes);\n  }"
            },
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.net.NetworkTopology.add": {
                "code_before_change": "    boolean add(Node n) {\n      if (!isAncestor(n)) {\n        throw new IllegalArgumentException(n.getName()\n            + \", which is located at \" + n.getNetworkLocation()\n            + \", is not a descendent of \" + getPath(this));\n      }\n      if (isParent(n)) {\n        // this node is the parent of n; add n directly\n        n.setParent(this);\n        n.setLevel(this.level+1);\n        Node prev = childrenMap.put(n.getName(), n);\n        if (prev != null) {\n          for(int i=0; i<children.size(); i++) {\n            if (children.get(i).getName().equals(n.getName())) {\n              children.set(i, n);\n              return false;\n            }\n          }\n        }\n        children.add(n);\n        numOfLeaves++;\n        return true;\n      } else {\n        // find the next ancestor node\n        String parentName = getNextAncestorName(n);\n        InnerNode parentNode = (InnerNode)childrenMap.get(parentName);\n        if (parentNode == null) {\n          // create a new InnerNode\n          parentNode = createParentNode(parentName);\n          children.add(parentNode);\n          childrenMap.put(parentNode.getName(), parentNode);\n        }\n        // add n to the subtree of the next ancestor node\n        if (parentNode.add(n)) {\n          numOfLeaves++;\n          return true;\n        } else {\n          return false;\n        }\n      }\n    }",
                "code_after_change": "    boolean add(Node n) {\n      if (!isAncestor(n)) {\n        throw new IllegalArgumentException(n.getName()\n            + \", which is located at \" + n.getNetworkLocation()\n            + \", is not a descendent of \" + getPath(this));\n      }\n      if (isParent(n)) {\n        // this node is the parent of n; add n directly\n        n.setParent(this);\n        n.setLevel(this.level+1);\n        Node prev = childrenMap.put(n.getName(), n);\n        if (prev != null) {\n          for(int i=0; i<children.size(); i++) {\n            if (children.get(i).getName().equals(n.getName())) {\n              children.set(i, n);\n              return false;\n            }\n          }\n        }\n        children.add(n);\n        numOfLeaves++;\n        return true;\n      } else {\n        // find the next ancestor node\n        String parentName = getNextAncestorName(n);\n        InnerNode parentNode = (InnerNode)childrenMap.get(parentName);\n        if (parentNode == null) {\n          // create a new InnerNode\n          parentNode = createParentNode(parentName);\n          children.add(parentNode);\n          childrenMap.put(parentNode.getName(), parentNode);\n        }\n        // add n to the subtree of the next ancestor node\n        if (parentNode.add(n)) {\n          numOfLeaves++;\n          return true;\n        } else {\n          return false;\n        }\n      }\n    }"
            },
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.net.NetworkTopology.chooseRandom": {
                "code_before_change": "  public Node chooseRandom(String scope) {\n    netlock.readLock().lock();\n    try {\n      if (scope.startsWith(\"~\")) {\n        return chooseRandom(NodeBase.ROOT, scope.substring(1));\n      } else {\n        return chooseRandom(scope, null);\n      }\n    } finally {\n      netlock.readLock().unlock();\n    }\n  }",
                "code_after_change": "  public Node chooseRandom(final String scope) {\n    return chooseRandom(scope, null);\n  }"
            },
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.net.NetworkTopology.setRandomSeed": {
                "code_before_change": "  void setRandomSeed(long seed) {\n    r.setSeed(seed);\n  }",
                "code_after_change": "  void setRandomSeed(long seed) {\n    r.setSeed(seed);\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.web.resources.NamenodeWebHdfsMethods.chooseDatanode": {
                "code_before_change": "  static DatanodeInfo chooseDatanode(final NameNode namenode,\n      final String path, final HttpOpParam.Op op, final long openOffset,\n      final long blocksize, final String excludeDatanodes) throws IOException {\n    FSNamesystem fsn = namenode.getNamesystem();\n    if (fsn == null) {\n      throw new IOException(\"Namesystem has not been intialized yet.\");\n    }\n    final BlockManager bm = fsn.getBlockManager();\n    \n    HashSet<Node> excludes = new HashSet<Node>();\n    if (excludeDatanodes != null) {\n      for (String host : StringUtils\n          .getTrimmedStringCollection(excludeDatanodes)) {\n        int idx = host.indexOf(\":\");\n        if (idx != -1) {          \n          excludes.add(bm.getDatanodeManager().getDatanodeByXferAddr(\n              host.substring(0, idx), Integer.parseInt(host.substring(idx + 1))));\n        } else {\n          excludes.add(bm.getDatanodeManager().getDatanodeByHost(host));\n        }\n      }\n    }\n\n    if (op == PutOpParam.Op.CREATE) {\n      //choose a datanode near to client \n      final DatanodeDescriptor clientNode = bm.getDatanodeManager(\n          ).getDatanodeByHost(getRemoteAddress());\n      if (clientNode != null) {\n        final DatanodeStorageInfo[] storages = bm.chooseTarget4WebHDFS(\n            path, clientNode, excludes, blocksize);\n        if (storages.length > 0) {\n          return storages[0].getDatanodeDescriptor();\n        }\n      }\n    } else if (op == GetOpParam.Op.OPEN\n        || op == GetOpParam.Op.GETFILECHECKSUM\n        || op == PostOpParam.Op.APPEND) {\n      //choose a datanode containing a replica \n      final NamenodeProtocols np = getRPCServer(namenode);\n      final HdfsFileStatus status = np.getFileInfo(path);\n      if (status == null) {\n        throw new FileNotFoundException(\"File \" + path + \" not found.\");\n      }\n      final long len = status.getLen();\n      if (op == GetOpParam.Op.OPEN) {\n        if (openOffset < 0L || (openOffset >= len && len > 0)) {\n          throw new IOException(\"Offset=\" + openOffset\n              + \" out of the range [0, \" + len + \"); \" + op + \", path=\" + path);\n        }\n      }\n\n      if (len > 0) {\n        final long offset = op == GetOpParam.Op.OPEN? openOffset: len - 1;\n        final LocatedBlocks locations = np.getBlockLocations(path, offset, 1);\n        final int count = locations.locatedBlockCount();\n        if (count > 0) {\n          return bestNode(locations.get(0).getLocations(), excludes);\n        }\n      }\n    } \n\n    return (DatanodeDescriptor)bm.getDatanodeManager().getNetworkTopology(\n        ).chooseRandom(NodeBase.ROOT);\n  }",
                "code_after_change": "  static DatanodeInfo chooseDatanode(final NameNode namenode,\n      final String path, final HttpOpParam.Op op, final long openOffset,\n      final long blocksize, final String excludeDatanodes) throws IOException {\n    FSNamesystem fsn = namenode.getNamesystem();\n    if (fsn == null) {\n      throw new IOException(\"Namesystem has not been intialized yet.\");\n    }\n    final BlockManager bm = fsn.getBlockManager();\n    \n    HashSet<Node> excludes = new HashSet<Node>();\n    if (excludeDatanodes != null) {\n      for (String host : StringUtils\n          .getTrimmedStringCollection(excludeDatanodes)) {\n        int idx = host.indexOf(\":\");\n        if (idx != -1) {          \n          excludes.add(bm.getDatanodeManager().getDatanodeByXferAddr(\n              host.substring(0, idx), Integer.parseInt(host.substring(idx + 1))));\n        } else {\n          excludes.add(bm.getDatanodeManager().getDatanodeByHost(host));\n        }\n      }\n    }\n\n    if (op == PutOpParam.Op.CREATE) {\n      //choose a datanode near to client \n      final DatanodeDescriptor clientNode = bm.getDatanodeManager(\n          ).getDatanodeByHost(getRemoteAddress());\n      if (clientNode != null) {\n        final DatanodeStorageInfo[] storages = bm.chooseTarget4WebHDFS(\n            path, clientNode, excludes, blocksize);\n        if (storages.length > 0) {\n          return storages[0].getDatanodeDescriptor();\n        }\n      }\n    } else if (op == GetOpParam.Op.OPEN\n        || op == GetOpParam.Op.GETFILECHECKSUM\n        || op == PostOpParam.Op.APPEND) {\n      //choose a datanode containing a replica \n      final NamenodeProtocols np = getRPCServer(namenode);\n      final HdfsFileStatus status = np.getFileInfo(path);\n      if (status == null) {\n        throw new FileNotFoundException(\"File \" + path + \" not found.\");\n      }\n      final long len = status.getLen();\n      if (op == GetOpParam.Op.OPEN) {\n        if (openOffset < 0L || (openOffset >= len && len > 0)) {\n          throw new IOException(\"Offset=\" + openOffset\n              + \" out of the range [0, \" + len + \"); \" + op + \", path=\" + path);\n        }\n      }\n\n      if (len > 0) {\n        final long offset = op == GetOpParam.Op.OPEN? openOffset: len - 1;\n        final LocatedBlocks locations = np.getBlockLocations(path, offset, 1);\n        final int count = locations.locatedBlockCount();\n        if (count > 0) {\n          return bestNode(locations.get(0).getLocations(), excludes);\n        }\n      }\n    } \n\n    return (DatanodeDescriptor)bm.getDatanodeManager().getNetworkTopology(\n        ).chooseRandom(NodeBase.ROOT, excludes);\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseFromNextRack": {
                "code_before_change": "  private DatanodeStorageInfo chooseFromNextRack(Node next,\n      Set<Node> excludedNodes,\n      long blocksize,\n      int maxNodesPerRack,\n      List<DatanodeStorageInfo> results,\n      boolean avoidStaleNodes,\n      EnumMap<StorageType, Integer> storageTypes) throws NotEnoughReplicasException {\n    final String nextRack = next.getNetworkLocation();\n    try {\n      return chooseRandom(nextRack, excludedNodes, blocksize, maxNodesPerRack,\n          results, avoidStaleNodes, storageTypes);\n    } catch(NotEnoughReplicasException e) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Failed to choose from the next rack (location = \" + nextRack\n            + \"), retry choosing ramdomly\", e);\n      }\n      //otherwise randomly choose one from the network\n      return chooseRandom(NodeBase.ROOT, excludedNodes, blocksize,\n          maxNodesPerRack, results, avoidStaleNodes, storageTypes);\n    }\n  }",
                "code_after_change": "  private DatanodeStorageInfo chooseFromNextRack(Node next,\n      Set<Node> excludedNodes,\n      long blocksize,\n      int maxNodesPerRack,\n      List<DatanodeStorageInfo> results,\n      boolean avoidStaleNodes,\n      EnumMap<StorageType, Integer> storageTypes) throws NotEnoughReplicasException {\n    final String nextRack = next.getNetworkLocation();\n    try {\n      return chooseRandom(nextRack, excludedNodes, blocksize, maxNodesPerRack,\n          results, avoidStaleNodes, storageTypes);\n    } catch(NotEnoughReplicasException e) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Failed to choose from the next rack (location = \" + nextRack\n            + \"), retry choosing ramdomly\", e);\n      }\n      //otherwise randomly choose one from the network\n      return chooseRandom(NodeBase.ROOT, excludedNodes, blocksize,\n          maxNodesPerRack, results, avoidStaleNodes, storageTypes);\n    }\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report precisely identifies the root cause by mentioning the method 'BlockPlacementPolicyDefault#chooseRandom' and 'NetworkTopology#chooseRandom', which are part of the ground truth methods. However, there is no fix suggestion provided in the bug report, as there is no 'Suggestions' or 'problem_location' field, nor any suggestion in the 'Description'. The problem location is also precisely identified as it mentions 'BlockPlacementPolicyDefault#chooseRandom' and 'NetworkTopology#chooseRandom', which are ground truth methods. There is no wrong information in the bug report as all the information is relevant to the context of the bug."
        }
    },
    {
        "filename": "HDFS-4201.json",
        "code_diff": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BPOfferService.getDataNode": {
                "code_before_change": "  DataNode getDataNode() {\n    return dn;\n  }",
                "code_after_change": "  DataNode getDataNode() {\n    return dn;\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the method 'BPServiceActor#sendHeartBeat' as the location of the NullPointerException, which is in the same stack trace as the ground truth method 'BPOfferService.getDataNode'. However, it does not identify the exact root cause related to the ground truth method, hence it is classified as 'Partial' with 'Shared Stack Trace Context'. There is no fix suggestion provided in the bug report, so it is marked as 'Missing' for fix suggestion. The problem location is also identified as 'Partial' with 'Shared Stack Trace Context' because the report points to a method in the same stack trace but not the exact ground truth method. There is no wrong information in the bug report as all the information is relevant to the context of the bug."
        }
    },
    {
        "filename": "HDFS-13721.json",
        "code_diff": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataNode.submitDiskBalancerPlan": {
                "code_before_change": "  public void submitDiskBalancerPlan(String planID, long planVersion,\n      String planFile, String planData, boolean skipDateCheck)\n      throws IOException {\n    checkSuperuserPrivilege();\n    if (getStartupOption(getConf()) != StartupOption.REGULAR) {\n      throw new DiskBalancerException(\n          \"Datanode is in special state, e.g. Upgrade/Rollback etc.\"\n              + \" Disk balancing not permitted.\",\n          DiskBalancerException.Result.DATANODE_STATUS_NOT_REGULAR);\n    }\n\n    this.diskBalancer.submitPlan(planID, planVersion, planFile, planData,\n            skipDateCheck);\n  }",
                "code_after_change": "  public void submitDiskBalancerPlan(String planID, long planVersion,\n      String planFile, String planData, boolean skipDateCheck)\n      throws IOException {\n    checkSuperuserPrivilege();\n    if (getStartupOption(getConf()) != StartupOption.REGULAR) {\n      throw new DiskBalancerException(\n          \"Datanode is in special state, e.g. Upgrade/Rollback etc.\"\n              + \" Disk balancing not permitted.\",\n          DiskBalancerException.Result.DATANODE_STATUS_NOT_REGULAR);\n    }\n\n    getDiskBalancer().submitPlan(planID, planVersion, planFile, planData,\n            skipDateCheck);\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataNode.getDiskBalancerSetting": {
                "code_before_change": "  public String getDiskBalancerSetting(String key) throws IOException {\n    checkSuperuserPrivilege();\n    Preconditions.checkNotNull(key);\n    switch (key) {\n    case DiskBalancerConstants.DISKBALANCER_VOLUME_NAME:\n      return this.diskBalancer.getVolumeNames();\n    case DiskBalancerConstants.DISKBALANCER_BANDWIDTH :\n      return Long.toString(this.diskBalancer.getBandwidth());\n    default:\n      LOG.error(\"Disk Balancer - Unknown key in get balancer setting. Key: {}\",\n          key);\n      throw new DiskBalancerException(\"Unknown key\",\n          DiskBalancerException.Result.UNKNOWN_KEY);\n    }\n  }",
                "code_after_change": "  public String getDiskBalancerSetting(String key) throws IOException {\n    checkSuperuserPrivilege();\n    Preconditions.checkNotNull(key);\n    switch (key) {\n    case DiskBalancerConstants.DISKBALANCER_VOLUME_NAME:\n      return getDiskBalancer().getVolumeNames();\n    case DiskBalancerConstants.DISKBALANCER_BANDWIDTH :\n      return Long.toString(getDiskBalancer().getBandwidth());\n    default:\n      LOG.error(\"Disk Balancer - Unknown key in get balancer setting. Key: {}\",\n          key);\n      throw new DiskBalancerException(\"Unknown key\",\n          DiskBalancerException.Result.UNKNOWN_KEY);\n    }\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataNode.cancelDiskBalancePlan": {
                "code_before_change": "  public void cancelDiskBalancePlan(String planID) throws\n      IOException {\n    checkSuperuserPrivilege();\n    this.diskBalancer.cancelPlan(planID);\n  }",
                "code_after_change": "  public void cancelDiskBalancePlan(String planID) throws\n      IOException {\n    checkSuperuserPrivilege();\n    getDiskBalancer().cancelPlan(planID);\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataNode.getClusterId": {
                "code_before_change": "  public synchronized String getClusterId() {\n    return clusterId;\n  }",
                "code_after_change": "  public synchronized String getClusterId() {\n    return clusterId;\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataNode.getVolumeReport": {
                "code_before_change": "  public List<DatanodeVolumeInfo> getVolumeReport() throws IOException {\n    checkSuperuserPrivilege();\n    Map<String, Object> volumeInfoMap = data.getVolumeInfoMap();\n    if (volumeInfoMap == null) {\n      LOG.warn(\"DataNode volume info not available.\");\n      return new ArrayList<>(0);\n    }\n    List<DatanodeVolumeInfo> volumeInfoList = new ArrayList<>();\n    for (Entry<String, Object> volume : volumeInfoMap.entrySet()) {\n      @SuppressWarnings(\"unchecked\")\n      Map<String, Object> volumeInfo = (Map<String, Object>) volume.getValue();\n      DatanodeVolumeInfo dnStorageInfo = new DatanodeVolumeInfo(\n          volume.getKey(), (Long) volumeInfo.get(\"usedSpace\"),\n          (Long) volumeInfo.get(\"freeSpace\"),\n          (Long) volumeInfo.get(\"reservedSpace\"),\n          (Long) volumeInfo.get(\"reservedSpaceForReplicas\"),\n          (Long) volumeInfo.get(\"numBlocks\"),\n          (StorageType) volumeInfo.get(\"storageType\"));\n      volumeInfoList.add(dnStorageInfo);\n    }\n    return volumeInfoList;\n  }",
                "code_after_change": "  public List<DatanodeVolumeInfo> getVolumeReport() throws IOException {\n    checkSuperuserPrivilege();\n    Map<String, Object> volumeInfoMap = data.getVolumeInfoMap();\n    if (volumeInfoMap == null) {\n      LOG.warn(\"DataNode volume info not available.\");\n      return new ArrayList<>(0);\n    }\n    List<DatanodeVolumeInfo> volumeInfoList = new ArrayList<>();\n    for (Entry<String, Object> volume : volumeInfoMap.entrySet()) {\n      @SuppressWarnings(\"unchecked\")\n      Map<String, Object> volumeInfo = (Map<String, Object>) volume.getValue();\n      DatanodeVolumeInfo dnStorageInfo = new DatanodeVolumeInfo(\n          volume.getKey(), (Long) volumeInfo.get(\"usedSpace\"),\n          (Long) volumeInfo.get(\"freeSpace\"),\n          (Long) volumeInfo.get(\"reservedSpace\"),\n          (Long) volumeInfo.get(\"reservedSpaceForReplicas\"),\n          (Long) volumeInfo.get(\"numBlocks\"),\n          (StorageType) volumeInfo.get(\"storageType\"));\n      volumeInfoList.add(dnStorageInfo);\n    }\n    return volumeInfoList;\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataNode.getDiskBalancerStatus": {
                "code_before_change": "  public String getDiskBalancerStatus() {\n    try {\n      return this.diskBalancer.queryWorkStatus().toJsonString();\n    } catch (IOException ex) {\n      LOG.debug(\"Reading diskbalancer Status failed. ex:{}\", ex);\n      return \"\";\n    }\n  }",
                "code_after_change": "  public String getDiskBalancerStatus() {\n    try {\n      return getDiskBalancer().queryWorkStatus().toJsonString();\n    } catch (IOException ex) {\n      LOG.debug(\"Reading diskbalancer Status failed. ex:{}\", ex);\n      return \"\";\n    }\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Preventive",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report precisely identifies the root cause as a NullPointerException in the method 'getDiskBalancerStatus', which is one of the ground truth methods. The fix suggestion is preventive as it suggests changing the NPE to an IOE, which would prevent the error from occurring. The problem location is also precise as it directly mentions 'getDiskBalancerStatus', which is a ground truth method. There is no wrong information in the bug report as all details are relevant and accurate."
        }
    },
    {
        "filename": "HDFS-7180.json",
        "code_diff": {
            "hadoop-hdfs-project.hadoop-hdfs-nfs.src.main.java.org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3.readdirplus": {
                "code_before_change": "  public READDIRPLUS3Response readdirplus(XDR xdr, RpcInfo info) {\n    return readdirplus(xdr, getSecurityHandler(info), info.remoteAddress());\n  }",
                "code_after_change": "  public READDIRPLUS3Response readdirplus(XDR xdr, RpcInfo info) {\n    return readdirplus(xdr, getSecurityHandler(info), info.remoteAddress());\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs-nfs.src.main.java.org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx.checkCommitInternal": {
                "code_before_change": "  synchronized COMMIT_STATUS checkCommitInternal(long commitOffset,\n      Channel channel, int xid, Nfs3FileAttributes preOpAttr, boolean fromRead) {\n    if (!activeState) {\n      if (pendingWrites.isEmpty()) {\n        return COMMIT_STATUS.COMMIT_INACTIVE_CTX;\n      } else {\n        // TODO: return success if already committed\n        return COMMIT_STATUS.COMMIT_INACTIVE_WITH_PENDING_WRITE;\n      }\n    }\n\n    long flushed = getFlushedOffset();\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"getFlushedOffset=\" + flushed + \" commitOffset=\" + commitOffset);\n    }\n\n    if (commitOffset > 0) {\n      if (aixCompatMode) {\n        // The AIX NFS client misinterprets RFC-1813 and will always send 4096\n        // for the commitOffset even if fewer bytes than that have ever (or will\n        // ever) be sent by the client. So, if in AIX compatibility mode, we\n        // will always DO_SYNC if the number of bytes to commit have already all\n        // been flushed, else we will fall through to the logic below which\n        // checks for pending writes in the case that we're being asked to\n        // commit more bytes than have so far been flushed. See HDFS-6549 for\n        // more info.\n        if (commitOffset <= flushed) {\n          return COMMIT_STATUS.COMMIT_DO_SYNC;\n        }\n      } else {\n        if (commitOffset > flushed) {\n          if (!fromRead) {\n            CommitCtx commitCtx = new CommitCtx(commitOffset, channel, xid,\n                preOpAttr);\n            pendingCommits.put(commitOffset, commitCtx);\n          }\n          return COMMIT_STATUS.COMMIT_WAIT;\n        } else {\n          return COMMIT_STATUS.COMMIT_DO_SYNC;\n        } \n      }\n    }\n\n    Entry<OffsetRange, WriteCtx> key = pendingWrites.firstEntry();\n\n    // Commit whole file, commitOffset == 0\n    if (pendingWrites.isEmpty()) {\n      // Note that, there is no guarantee data is synced. TODO: We could still\n      // do a sync here though the output stream might be closed.\n      return COMMIT_STATUS.COMMIT_FINISHED;\n    } else {\n      if (!fromRead) {\n        // Insert commit\n        long maxOffset = key.getKey().getMax() - 1;\n        Preconditions.checkState(maxOffset > 0);\n        CommitCtx commitCtx = new CommitCtx(maxOffset, channel, xid, preOpAttr);\n        pendingCommits.put(maxOffset, commitCtx);\n      }\n      return COMMIT_STATUS.COMMIT_WAIT;\n    }\n  }",
                "code_after_change": "  synchronized COMMIT_STATUS checkCommitInternal(long commitOffset,\n      Channel channel, int xid, Nfs3FileAttributes preOpAttr, boolean fromRead) {\n    if (!activeState) {\n      if (pendingWrites.isEmpty()) {\n        return COMMIT_STATUS.COMMIT_INACTIVE_CTX;\n      } else {\n        // TODO: return success if already committed\n        return COMMIT_STATUS.COMMIT_INACTIVE_WITH_PENDING_WRITE;\n      }\n    }\n    if (pendingWrites.isEmpty()) {\n      // Note that, there is no guarantee data is synced. Caller should still\n      // do a sync here though the output stream might be closed.\n      return COMMIT_STATUS.COMMIT_FINISHED;\n    }\n\n    long flushed = 0;\n    try {\n      flushed = getFlushedOffset();\n    } catch (IOException e) {\n      LOG.error(\"Can't get flushed offset, error:\" + e);\n      return COMMIT_STATUS.COMMIT_ERROR;\n    }\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"getFlushedOffset=\" + flushed + \" commitOffset=\" + commitOffset);\n    }\n\n    // Handle large file upload\n    if (uploadLargeFile && !aixCompatMode) {\n      long co = (commitOffset > 0) ? commitOffset : pendingWrites.firstEntry()\n          .getKey().getMax() - 1;\n\n      if (co <= flushed) {\n        return COMMIT_STATUS.COMMIT_DO_SYNC;\n      } else if (co < nextOffset.get()) {\n        if (!fromRead) {\n          // let client retry the same request, add pending commit to sync later\n          CommitCtx commitCtx = new CommitCtx(commitOffset, channel, xid,\n              preOpAttr);\n          pendingCommits.put(commitOffset, commitCtx);\n        }\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"return COMMIT_SPECIAL_WAIT\");\n        }\n        return COMMIT_STATUS.COMMIT_SPECIAL_WAIT;\n      } else {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"return COMMIT_SPECIAL_SUCCESS\");\n        }\n        return COMMIT_STATUS.COMMIT_SPECIAL_SUCCESS;\n      }\n    }\n    \n    if (commitOffset > 0) {\n      if (aixCompatMode) {\n        // The AIX NFS client misinterprets RFC-1813 and will always send 4096\n        // for the commitOffset even if fewer bytes than that have ever (or will\n        // ever) be sent by the client. So, if in AIX compatibility mode, we\n        // will always DO_SYNC if the number of bytes to commit have already all\n        // been flushed, else we will fall through to the logic below which\n        // checks for pending writes in the case that we're being asked to\n        // commit more bytes than have so far been flushed. See HDFS-6549 for\n        // more info.\n        if (commitOffset <= flushed) {\n          return COMMIT_STATUS.COMMIT_DO_SYNC;\n        }\n      } else {\n        if (commitOffset > flushed) {\n          if (!fromRead) {\n            CommitCtx commitCtx = new CommitCtx(commitOffset, channel, xid,\n                preOpAttr);\n            pendingCommits.put(commitOffset, commitCtx);\n          }\n          return COMMIT_STATUS.COMMIT_WAIT;\n        } else {\n          return COMMIT_STATUS.COMMIT_DO_SYNC;\n        } \n      }\n    }\n\n    Entry<OffsetRange, WriteCtx> key = pendingWrites.firstEntry();\n\n    // Commit whole file, commitOffset == 0\n    if (!fromRead) {\n      // Insert commit\n      long maxOffset = key.getKey().getMax() - 1;\n      Preconditions.checkState(maxOffset > 0);\n      CommitCtx commitCtx = new CommitCtx(maxOffset, channel, xid, preOpAttr);\n      pendingCommits.put(maxOffset, commitCtx);\n    }\n    return COMMIT_STATUS.COMMIT_WAIT;\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs-nfs.src.main.java.org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx.hasPendingWork": {
                "code_before_change": "  boolean hasPendingWork() {\n    return (pendingWrites.size() != 0 || pendingCommits.size() != 0);\n  }",
                "code_after_change": "  boolean hasPendingWork() {\n    return (pendingWrites.size() != 0 || pendingCommits.size() != 0);\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs-nfs.src.main.java.org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3.create": {
                "code_before_change": "  public CREATE3Response create(XDR xdr, RpcInfo info) {\n    return create(xdr, getSecurityHandler(info), info.remoteAddress());\n  }",
                "code_after_change": "  public CREATE3Response create(XDR xdr, RpcInfo info) {\n    return create(xdr, getSecurityHandler(info), info.remoteAddress());\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs-nfs.src.main.java.org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3.fsstat": {
                "code_before_change": "  public FSSTAT3Response fsstat(XDR xdr, RpcInfo info) {\n    return fsstat(xdr, getSecurityHandler(info), info.remoteAddress());\n  }",
                "code_after_change": "  public FSSTAT3Response fsstat(XDR xdr, RpcInfo info) {\n    return fsstat(xdr, getSecurityHandler(info), info.remoteAddress());\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs-nfs.src.main.java.org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3.link": {
                "code_before_change": "  public READDIR3Response link(XDR xdr, RpcInfo info) {\n    return new READDIR3Response(Nfs3Status.NFS3ERR_NOTSUPP);\n  }",
                "code_after_change": "  public READDIR3Response link(XDR xdr, RpcInfo info) {\n    return new READDIR3Response(Nfs3Status.NFS3ERR_NOTSUPP);\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs-nfs.src.main.java.org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3.symlink": {
                "code_before_change": "  public SYMLINK3Response symlink(XDR xdr, RpcInfo info) {\n    return symlink(xdr, getSecurityHandler(info), info.remoteAddress());\n  }",
                "code_after_change": "  public SYMLINK3Response symlink(XDR xdr, RpcInfo info) {\n    return symlink(xdr, getSecurityHandler(info), info.remoteAddress());\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs-nfs.src.main.java.org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx.receivedNewWriteInternal": {
                "code_before_change": "  private void receivedNewWriteInternal(DFSClient dfsClient,\n      WRITE3Request request, Channel channel, int xid,\n      AsyncDataService asyncDataService, IdUserGroup iug) {\n    WriteStableHow stableHow = request.getStableHow();\n    WccAttr preOpAttr = latestAttr.getWccAttr();\n    int count = request.getCount();\n\n    WriteCtx writeCtx = addWritesToCache(request, channel, xid);\n    if (writeCtx == null) {\n      // offset < nextOffset\n      processOverWrite(dfsClient, request, channel, xid, iug);\n    } else {\n      // The writes is added to pendingWrites.\n      // Check and start writing back if necessary\n      boolean startWriting = checkAndStartWrite(asyncDataService, writeCtx);\n      if (!startWriting) {\n        // offset > nextOffset. check if we need to dump data\n        checkDump();\n        \n        // In test, noticed some Linux client sends a batch (e.g., 1MB)\n        // of reordered writes and won't send more writes until it gets\n        // responses of the previous batch. So here send response immediately\n        // for unstable non-sequential write\n        if (stableHow != WriteStableHow.UNSTABLE) {\n          LOG.info(\"Have to change stable write to unstable write:\"\n              + request.getStableHow());\n          stableHow = WriteStableHow.UNSTABLE;\n        }\n\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"UNSTABLE write request, send response for offset: \"\n              + writeCtx.getOffset());\n        }\n        WccData fileWcc = new WccData(preOpAttr, latestAttr);\n        WRITE3Response response = new WRITE3Response(Nfs3Status.NFS3_OK,\n            fileWcc, count, stableHow, Nfs3Constant.WRITE_COMMIT_VERF);\n        Nfs3Utils\n            .writeChannel(channel, response.serialize(new XDR(),\n                xid, new VerifierNone()), xid);\n        writeCtx.setReplied(true);\n      }\n    }\n  }",
                "code_after_change": "  private void receivedNewWriteInternal(DFSClient dfsClient,\n      WRITE3Request request, Channel channel, int xid,\n      AsyncDataService asyncDataService, IdUserGroup iug) {\n    WriteStableHow stableHow = request.getStableHow();\n    WccAttr preOpAttr = latestAttr.getWccAttr();\n    int count = request.getCount();\n\n    WriteCtx writeCtx = addWritesToCache(request, channel, xid);\n    if (writeCtx == null) {\n      // offset < nextOffset\n      processOverWrite(dfsClient, request, channel, xid, iug);\n    } else {\n      // The writes is added to pendingWrites.\n      // Check and start writing back if necessary\n      boolean startWriting = checkAndStartWrite(asyncDataService, writeCtx);\n      if (!startWriting) {\n        // offset > nextOffset. check if we need to dump data\n        waitForDump();\n        \n        // In test, noticed some Linux client sends a batch (e.g., 1MB)\n        // of reordered writes and won't send more writes until it gets\n        // responses of the previous batch. So here send response immediately\n        // for unstable non-sequential write\n        if (stableHow != WriteStableHow.UNSTABLE) {\n          LOG.info(\"Have to change stable write to unstable write:\"\n              + request.getStableHow());\n          stableHow = WriteStableHow.UNSTABLE;\n        }\n\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"UNSTABLE write request, send response for offset: \"\n              + writeCtx.getOffset());\n        }\n        WccData fileWcc = new WccData(preOpAttr, latestAttr);\n        WRITE3Response response = new WRITE3Response(Nfs3Status.NFS3_OK,\n            fileWcc, count, stableHow, Nfs3Constant.WRITE_COMMIT_VERF);\n        Nfs3Utils\n            .writeChannel(channel, response.serialize(new XDR(),\n                xid, new VerifierNone()), xid);\n        writeCtx.setReplied(true);\n      }\n    }\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs-nfs.src.main.java.org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3.getattr": {
                "code_before_change": "  public GETATTR3Response getattr(XDR xdr, RpcInfo info) {\n    return getattr(xdr, getSecurityHandler(info), info.remoteAddress());\n  }",
                "code_after_change": "  public GETATTR3Response getattr(XDR xdr, RpcInfo info) {\n    return getattr(xdr, getSecurityHandler(info), info.remoteAddress());\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs-nfs.src.main.java.org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx.toString": {
                "code_before_change": "    public String toString() {\n      return String.format(\"offset: %d xid: %d startTime: %d\", offset, xid,\n          startTime);\n    }",
                "code_after_change": "    public String toString() {\n      return String.format(\"offset: %d xid: %d startTime: %d\", offset, xid,\n          startTime);\n    }"
            },
            "hadoop-hdfs-project.hadoop-hdfs-nfs.src.main.java.org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3.access": {
                "code_before_change": "  public ACCESS3Response access(XDR xdr, RpcInfo info) {\n    return access(xdr, getSecurityHandler(info), info.remoteAddress());\n  }",
                "code_after_change": "  public ACCESS3Response access(XDR xdr, RpcInfo info) {\n    return access(xdr, getSecurityHandler(info), info.remoteAddress());\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs-nfs.src.main.java.org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx.waitForDump": {
                "code_before_change": [],
                "code_after_change": "  private void waitForDump() {\n    if (!enabledDump) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Do nothing, dump is disabled.\");\n      }\n      return;\n    }\n\n    if (nonSequentialWriteInMemory.get() < DUMP_WRITE_WATER_MARK) {\n      return;\n    }\n\n    // wake up the dumper thread to dump the data\n    synchronized (this) {\n      if (nonSequentialWriteInMemory.get() >= DUMP_WRITE_WATER_MARK) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Asking dumper to dump...\");\n        }\n        if (dumpThread == null) {\n          dumpThread = new Daemon(new Dumper());\n          dumpThread.start();\n        } else {\n          this.notifyAll();          \n        }\n      }\n      \n      while (nonSequentialWriteInMemory.get() >= DUMP_WRITE_WATER_MARK) {\n        try {\n          this.wait();\n        } catch (InterruptedException ignored) {\n        }\n      }\n\n    }\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs-nfs.src.main.java.org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3.pathconf": {
                "code_before_change": "  public PATHCONF3Response pathconf(XDR xdr, RpcInfo info) {\n    return pathconf(xdr, getSecurityHandler(info), info.remoteAddress());\n  }",
                "code_after_change": "  public PATHCONF3Response pathconf(XDR xdr, RpcInfo info) {\n    return pathconf(xdr, getSecurityHandler(info), info.remoteAddress());\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs-nfs.src.main.java.org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3.setattr": {
                "code_before_change": "  public SETATTR3Response setattr(XDR xdr, RpcInfo info) {\n    return setattr(xdr, getSecurityHandler(info), info.remoteAddress());\n  }",
                "code_after_change": "  public SETATTR3Response setattr(XDR xdr, RpcInfo info) {\n    return setattr(xdr, getSecurityHandler(info), info.remoteAddress());\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs-nfs.src.main.java.org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3.commit": {
                "code_before_change": "  public COMMIT3Response commit(XDR xdr, RpcInfo info) {\n    SecurityHandler securityHandler = getSecurityHandler(info);\n    RpcCall rpcCall = (RpcCall) info.header();\n    int xid = rpcCall.getXid();\n    SocketAddress remoteAddress = info.remoteAddress();\n    return commit(xdr, info.channel(), xid, securityHandler, remoteAddress);\n  }",
                "code_after_change": "  public COMMIT3Response commit(XDR xdr, RpcInfo info) {\n    SecurityHandler securityHandler = getSecurityHandler(info);\n    RpcCall rpcCall = (RpcCall) info.header();\n    int xid = rpcCall.getXid();\n    SocketAddress remoteAddress = info.remoteAddress();\n    return commit(xdr, info.channel(), xid, securityHandler, remoteAddress);\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs-nfs.src.main.java.org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx.addWritesToCache": {
                "code_before_change": "  private synchronized WriteCtx addWritesToCache(WRITE3Request request,\n      Channel channel, int xid) {\n    long offset = request.getOffset();\n    int count = request.getCount();\n    long cachedOffset = nextOffset.get();\n    int originalCount = WriteCtx.INVALID_ORIGINAL_COUNT;\n    \n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"requesed offset=\" + offset + \" and current offset=\"\n          + cachedOffset);\n    }\n\n    // Handle a special case first\n    if ((offset < cachedOffset) && (offset + count > cachedOffset)) {\n      // One Linux client behavior: after a file is closed and reopened to\n      // write, the client sometimes combines previous written data(could still\n      // be in kernel buffer) with newly appended data in one write. This is\n      // usually the first write after file reopened. In this\n      // case, we log the event and drop the overlapped section.\n      LOG.warn(String.format(\"Got overwrite with appended data (%d-%d),\"\n          + \" current offset %d,\" + \" drop the overlapped section (%d-%d)\"\n          + \" and append new data (%d-%d).\", offset, (offset + count - 1),\n          cachedOffset, offset, (cachedOffset - 1), cachedOffset, (offset\n              + count - 1)));\n\n      if (!pendingWrites.isEmpty()) {\n        LOG.warn(\"There are other pending writes, fail this jumbo write\");\n        return null;\n      }\n      \n      LOG.warn(\"Modify this write to write only the appended data\");\n      alterWriteRequest(request, cachedOffset);\n\n      // Update local variable\n      originalCount = count;\n      offset = request.getOffset();\n      count = request.getCount();\n    }\n    \n    // Fail non-append call\n    if (offset < cachedOffset) {\n      LOG.warn(\"(offset,count,nextOffset):\" + \"(\" + offset + \",\" + count + \",\"\n          + nextOffset + \")\");\n      return null;\n    } else {\n      DataState dataState = offset == cachedOffset ? WriteCtx.DataState.NO_DUMP\n          : WriteCtx.DataState.ALLOW_DUMP;\n      WriteCtx writeCtx = new WriteCtx(request.getHandle(),\n          request.getOffset(), request.getCount(), originalCount,\n          request.getStableHow(), request.getData(), channel, xid, false,\n          dataState);\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Add new write to the list with nextOffset \" + cachedOffset\n            + \" and requesed offset=\" + offset);\n      }\n      if (writeCtx.getDataState() == WriteCtx.DataState.ALLOW_DUMP) {\n        // update the memory size\n        updateNonSequentialWriteInMemory(count);\n      }\n      // check if there is a WriteCtx with the same range in pendingWrites\n      WriteCtx oldWriteCtx = checkRepeatedWriteRequest(request, channel, xid);\n      if (oldWriteCtx == null) {\n        addWrite(writeCtx);\n      } else {\n        LOG.warn(\"Got a repeated request, same range, with xid:\"\n            + writeCtx.getXid());\n      }\n      return writeCtx;\n    }\n  }",
                "code_after_change": "  private synchronized WriteCtx addWritesToCache(WRITE3Request request,\n      Channel channel, int xid) {\n    long offset = request.getOffset();\n    int count = request.getCount();\n    long cachedOffset = nextOffset.get();\n    int originalCount = WriteCtx.INVALID_ORIGINAL_COUNT;\n    \n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"requesed offset=\" + offset + \" and current offset=\"\n          + cachedOffset);\n    }\n\n    // Handle a special case first\n    if ((offset < cachedOffset) && (offset + count > cachedOffset)) {\n      // One Linux client behavior: after a file is closed and reopened to\n      // write, the client sometimes combines previous written data(could still\n      // be in kernel buffer) with newly appended data in one write. This is\n      // usually the first write after file reopened. In this\n      // case, we log the event and drop the overlapped section.\n      LOG.warn(String.format(\"Got overwrite with appended data (%d-%d),\"\n          + \" current offset %d,\" + \" drop the overlapped section (%d-%d)\"\n          + \" and append new data (%d-%d).\", offset, (offset + count - 1),\n          cachedOffset, offset, (cachedOffset - 1), cachedOffset, (offset\n              + count - 1)));\n\n      if (!pendingWrites.isEmpty()) {\n        LOG.warn(\"There are other pending writes, fail this jumbo write\");\n        return null;\n      }\n      \n      LOG.warn(\"Modify this write to write only the appended data\");\n      alterWriteRequest(request, cachedOffset);\n\n      // Update local variable\n      originalCount = count;\n      offset = request.getOffset();\n      count = request.getCount();\n    }\n    \n    // Fail non-append call\n    if (offset < cachedOffset) {\n      LOG.warn(\"(offset,count,nextOffset):\" + \"(\" + offset + \",\" + count + \",\"\n          + nextOffset + \")\");\n      return null;\n    } else {\n      DataState dataState = offset == cachedOffset ? WriteCtx.DataState.NO_DUMP\n          : WriteCtx.DataState.ALLOW_DUMP;\n      WriteCtx writeCtx = new WriteCtx(request.getHandle(),\n          request.getOffset(), request.getCount(), originalCount,\n          request.getStableHow(), request.getData(), channel, xid, false,\n          dataState);\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"Add new write to the list with nextOffset \" + cachedOffset\n            + \" and requesed offset=\" + offset);\n      }\n      if (writeCtx.getDataState() == WriteCtx.DataState.ALLOW_DUMP) {\n        // update the memory size\n        updateNonSequentialWriteInMemory(count);\n      }\n      // check if there is a WriteCtx with the same range in pendingWrites\n      WriteCtx oldWriteCtx = checkRepeatedWriteRequest(request, channel, xid);\n      if (oldWriteCtx == null) {\n        pendingWrites.put(new OffsetRange(offset, offset + count), writeCtx);\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"New write buffered with xid \" + xid + \" nextOffset \"\n              + cachedOffset + \" req offset=\" + offset + \" mapsize=\"\n              + pendingWrites.size());\n        }\n      } else {\n        LOG.warn(\"Got a repeated request, same range, with xid:\" + xid\n            + \" nextOffset \" + +cachedOffset + \" req offset=\" + offset);\n      }\n      return writeCtx;\n    }\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs-nfs.src.main.java.org.apache.hadoop.hdfs.nfs.nfs3.DFSClientCache.onRemoval": {
                "code_before_change": "      public void onRemoval(RemovalNotification<String, DFSClient> notification) {\n        DFSClient client = notification.getValue();\n        try {\n          client.close();\n        } catch (IOException e) {\n          LOG.warn(String.format(\n              \"IOException when closing the DFSClient(%s), cause: %s\", client,\n              e));\n        }\n      }",
                "code_after_change": "      public void onRemoval(RemovalNotification<String, DFSClient> notification) {\n        DFSClient client = notification.getValue();\n        try {\n          client.close();\n        } catch (IOException e) {\n          LOG.warn(String.format(\n              \"IOException when closing the DFSClient(%s), cause: %s\", client,\n              e));\n        }\n      }"
            },
            "hadoop-hdfs-project.hadoop-hdfs-nfs.src.main.java.org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3.readdir": {
                "code_before_change": "  public READDIR3Response readdir(XDR xdr, RpcInfo info) {\n    return readdir(xdr, getSecurityHandler(info), info.remoteAddress());\n  }",
                "code_after_change": "  public READDIR3Response readdir(XDR xdr, RpcInfo info) {\n    return readdir(xdr, getSecurityHandler(info), info.remoteAddress());\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs-nfs.src.main.java.org.apache.hadoop.hdfs.nfs.nfs3.WriteCtx.loadData": {
                "code_before_change": "  private void loadData() throws IOException {\n    Preconditions.checkState(data == null);\n    byte[] rawData = new byte[count];\n    raf.seek(dumpFileOffset);\n    int size = raf.read(rawData, 0, count);\n    if (size != count) {\n      throw new IOException(\"Data count is \" + count + \", but read back \"\n          + size + \"bytes\");\n    }\n    data = ByteBuffer.wrap(rawData);\n  }",
                "code_after_change": "  private void loadData() throws IOException {\n    Preconditions.checkState(data == null);\n    byte[] rawData = new byte[count];\n    raf.seek(dumpFileOffset);\n    int size = raf.read(rawData, 0, count);\n    if (size != count) {\n      throw new IOException(\"Data count is \" + count + \", but read back \"\n          + size + \"bytes\");\n    }\n    data = ByteBuffer.wrap(rawData);\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs-nfs.src.main.java.org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3.remove": {
                "code_before_change": "  public REMOVE3Response remove(XDR xdr, RpcInfo info) {\n    return remove(xdr, getSecurityHandler(info), info.remoteAddress());\n  }",
                "code_after_change": "  public REMOVE3Response remove(XDR xdr, RpcInfo info) {\n    return remove(xdr, getSecurityHandler(info), info.remoteAddress());\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs-nfs.src.main.java.org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx.getFlushedOffset": {
                "code_before_change": "  private long getFlushedOffset() {\n    return fos.getPos();\n  }",
                "code_after_change": "  private long getFlushedOffset() throws IOException {\n    return fos.getPos();\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs-nfs.src.main.java.org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3.readlink": {
                "code_before_change": "  public READLINK3Response readlink(XDR xdr, RpcInfo info) {\n    return readlink(xdr, getSecurityHandler(info), info.remoteAddress());\n  }",
                "code_after_change": "  public READLINK3Response readlink(XDR xdr, RpcInfo info) {\n    return readlink(xdr, getSecurityHandler(info), info.remoteAddress());\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs-nfs.src.main.java.org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3.read": {
                "code_before_change": "  public READ3Response read(XDR xdr, RpcInfo info) {\n    return read(xdr, getSecurityHandler(info), info.remoteAddress());\n  }",
                "code_after_change": "  public READ3Response read(XDR xdr, RpcInfo info) {\n    return read(xdr, getSecurityHandler(info), info.remoteAddress());\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs-nfs.src.main.java.org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3.fsinfo": {
                "code_before_change": "  public FSINFO3Response fsinfo(XDR xdr, RpcInfo info) {\n    return fsinfo(xdr, getSecurityHandler(info), info.remoteAddress());\n  }",
                "code_after_change": "  public FSINFO3Response fsinfo(XDR xdr, RpcInfo info) {\n    return fsinfo(xdr, getSecurityHandler(info), info.remoteAddress());\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs-nfs.src.main.java.org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3.write": {
                "code_before_change": "  public WRITE3Response write(XDR xdr, RpcInfo info) {\n    SecurityHandler securityHandler = getSecurityHandler(info);\n    RpcCall rpcCall = (RpcCall) info.header();\n    int xid = rpcCall.getXid();\n    SocketAddress remoteAddress = info.remoteAddress();\n    return write(xdr, info.channel(), xid, securityHandler, remoteAddress);\n  }",
                "code_after_change": "  public WRITE3Response write(XDR xdr, RpcInfo info) {\n    SecurityHandler securityHandler = getSecurityHandler(info);\n    RpcCall rpcCall = (RpcCall) info.header();\n    int xid = rpcCall.getXid();\n    SocketAddress remoteAddress = info.remoteAddress();\n    return write(xdr, info.channel(), xid, securityHandler, remoteAddress);\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs-nfs.src.main.java.org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx.updateNonSequentialWriteInMemory": {
                "code_before_change": "  private long updateNonSequentialWriteInMemory(long count) {\n    long newValue = nonSequentialWriteInMemory.addAndGet(count);\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Update nonSequentialWriteInMemory by \" + count + \" new value:\"\n          + newValue);\n    }\n\n    Preconditions.checkState(newValue >= 0,\n        \"nonSequentialWriteInMemory is negative after update with count \"\n            + count);\n    return newValue;\n  }",
                "code_after_change": "  private long updateNonSequentialWriteInMemory(long count) {\n    long newValue = nonSequentialWriteInMemory.addAndGet(count);\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Update nonSequentialWriteInMemory by \" + count + \" new value:\"\n          + newValue);\n    }\n\n    Preconditions.checkState(newValue >= 0,\n        \"nonSequentialWriteInMemory is negative \" + newValue\n            + \" after update with count \" + count);\n    return newValue;\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs-nfs.src.main.java.org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3.mkdir": {
                "code_before_change": "  public MKDIR3Response mkdir(XDR xdr, RpcInfo info) {\n    return mkdir(xdr, getSecurityHandler(info), info.remoteAddress());\n  }",
                "code_after_change": "  public MKDIR3Response mkdir(XDR xdr, RpcInfo info) {\n    return mkdir(xdr, getSecurityHandler(info), info.remoteAddress());\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs-nfs.src.main.java.org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx.cleanup": {
                "code_before_change": "  synchronized void cleanup() {\n    if (!activeState) {\n      LOG.info(\"Current OpenFileCtx is already inactive, no need to cleanup.\");\n      return;\n    }\n    activeState = false;\n\n    // stop the dump thread\n    if (dumpThread != null && dumpThread.isAlive()) {\n      dumpThread.interrupt();\n      try {\n        dumpThread.join(3000);\n      } catch (InterruptedException e) {\n      }\n    }\n    \n    // Close stream\n    try {\n      if (fos != null) {\n        fos.close();\n      }\n    } catch (IOException e) {\n      LOG.info(\"Can't close stream for fileId:\" + latestAttr.getFileId()\n          + \", error:\" + e);\n    }\n    \n    // Reply error for pending writes\n    LOG.info(\"There are \" + pendingWrites.size() + \" pending writes.\");\n    WccAttr preOpAttr = latestAttr.getWccAttr();\n    while (!pendingWrites.isEmpty()) {\n      OffsetRange key = pendingWrites.firstKey();\n      LOG.info(\"Fail pending write: (\" + key.getMin() + \",\" + key.getMax()\n          + \"), nextOffset=\" + nextOffset.get());\n      \n      WriteCtx writeCtx = pendingWrites.remove(key);\n      if (!writeCtx.getReplied()) {\n        WccData fileWcc = new WccData(preOpAttr, latestAttr);\n        WRITE3Response response = new WRITE3Response(Nfs3Status.NFS3ERR_IO,\n            fileWcc, 0, writeCtx.getStableHow(), Nfs3Constant.WRITE_COMMIT_VERF);\n        Nfs3Utils.writeChannel(writeCtx.getChannel(), response\n            .serialize(new XDR(), writeCtx.getXid(),\n                new VerifierNone()), writeCtx.getXid());\n      }\n    }\n    \n    // Cleanup dump file\n    if (dumpOut != null) {\n      try {\n        dumpOut.close();\n      } catch (IOException e) {\n        LOG.error(\"Failed to close outputstream of dump file\" + dumpFilePath, e);\n      }\n      File dumpFile = new File(dumpFilePath);\n      if (dumpFile.exists() && !dumpFile.delete()) {\n        LOG.error(\"Failed to delete dumpfile: \" + dumpFile);\n      }\n    }\n    if (raf != null) {\n      try {\n        raf.close();\n      } catch (IOException e) {\n        LOG.error(\"Got exception when closing input stream of dump file.\", e);\n      }\n    }\n  }",
                "code_after_change": "  synchronized void cleanup() {\n    if (!activeState) {\n      LOG.info(\"Current OpenFileCtx is already inactive, no need to cleanup.\");\n      return;\n    }\n    activeState = false;\n\n    // stop the dump thread\n    if (dumpThread != null && dumpThread.isAlive()) {\n      dumpThread.interrupt();\n      try {\n        dumpThread.join(3000);\n      } catch (InterruptedException ignored) {\n      }\n    }\n    \n    // Close stream\n    try {\n      if (fos != null) {\n        fos.close();\n      }\n    } catch (IOException e) {\n      LOG.info(\"Can't close stream for fileId:\" + latestAttr.getFileId()\n          + \", error:\" + e);\n    }\n    \n    // Reply error for pending writes\n    LOG.info(\"There are \" + pendingWrites.size() + \" pending writes.\");\n    WccAttr preOpAttr = latestAttr.getWccAttr();\n    while (!pendingWrites.isEmpty()) {\n      OffsetRange key = pendingWrites.firstKey();\n      LOG.info(\"Fail pending write: (\" + key.getMin() + \",\" + key.getMax()\n          + \"), nextOffset=\" + nextOffset.get());\n      \n      WriteCtx writeCtx = pendingWrites.remove(key);\n      if (!writeCtx.getReplied()) {\n        WccData fileWcc = new WccData(preOpAttr, latestAttr);\n        WRITE3Response response = new WRITE3Response(Nfs3Status.NFS3ERR_IO,\n            fileWcc, 0, writeCtx.getStableHow(), Nfs3Constant.WRITE_COMMIT_VERF);\n        Nfs3Utils.writeChannel(writeCtx.getChannel(), response\n            .serialize(new XDR(), writeCtx.getXid(),\n                new VerifierNone()), writeCtx.getXid());\n      }\n    }\n    \n    // Cleanup dump file\n    if (dumpOut != null) {\n      try {\n        dumpOut.close();\n      } catch (IOException e) {\n        LOG.error(\"Failed to close outputstream of dump file\" + dumpFilePath, e);\n      }\n      File dumpFile = new File(dumpFilePath);\n      if (dumpFile.exists() && !dumpFile.delete()) {\n        LOG.error(\"Failed to delete dumpfile: \" + dumpFile);\n      }\n    }\n    if (raf != null) {\n      try {\n        raf.close();\n      } catch (IOException e) {\n        LOG.error(\"Got exception when closing input stream of dump file.\", e);\n      }\n    }\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs-nfs.src.main.java.org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtxCache.cleanAll": {
                "code_before_change": "  void cleanAll() {\n    ArrayList<OpenFileCtx> cleanedContext = new ArrayList<OpenFileCtx>();\n    synchronized (this) {\n      Iterator<Entry<FileHandle, OpenFileCtx>> it = openFileMap.entrySet()\n          .iterator();\n      if (LOG.isTraceEnabled()) {\n        LOG.trace(\"openFileMap size:\" + openFileMap.size());\n      }\n\n      while (it.hasNext()) {\n        Entry<FileHandle, OpenFileCtx> pairs = it.next();\n        OpenFileCtx ctx = pairs.getValue();\n        it.remove();\n        cleanedContext.add(ctx);\n      }\n    }\n\n    // Invoke the cleanup outside the lock\n    for (OpenFileCtx ofc : cleanedContext) {\n      ofc.cleanup();\n    }\n  }",
                "code_after_change": "  void cleanAll() {\n    ArrayList<OpenFileCtx> cleanedContext = new ArrayList<OpenFileCtx>();\n    synchronized (this) {\n      Iterator<Entry<FileHandle, OpenFileCtx>> it = openFileMap.entrySet()\n          .iterator();\n      if (LOG.isTraceEnabled()) {\n        LOG.trace(\"openFileMap size:\" + openFileMap.size());\n      }\n\n      while (it.hasNext()) {\n        Entry<FileHandle, OpenFileCtx> pairs = it.next();\n        OpenFileCtx ctx = pairs.getValue();\n        it.remove();\n        cleanedContext.add(ctx);\n      }\n    }\n\n    // Invoke the cleanup outside the lock\n    for (OpenFileCtx ofc : cleanedContext) {\n      ofc.cleanup();\n    }\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs-nfs.src.main.java.org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3.lookup": {
                "code_before_change": "  public LOOKUP3Response lookup(XDR xdr, RpcInfo info) {\n    return lookup(xdr, getSecurityHandler(info), info.remoteAddress());\n  }",
                "code_after_change": "  public LOOKUP3Response lookup(XDR xdr, RpcInfo info) {\n    return lookup(xdr, getSecurityHandler(info), info.remoteAddress());\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs-nfs.src.main.java.org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx.run": {
                "code_before_change": "    public void run() {\n      while (activeState && enabledDump) {\n        try {\n          if (nonSequentialWriteInMemory.get() >= DUMP_WRITE_WATER_MARK) {\n            dump();\n          }\n          synchronized (OpenFileCtx.this) {\n            if (nonSequentialWriteInMemory.get() < DUMP_WRITE_WATER_MARK) {\n              try {\n                OpenFileCtx.this.wait();\n                if (LOG.isDebugEnabled()) {\n                  LOG.debug(\"Dumper woke up\");\n                }\n              } catch (InterruptedException e) {\n                LOG.info(\"Dumper is interrupted, dumpFilePath= \"\n                    + OpenFileCtx.this.dumpFilePath);\n              }\n            }\n          }\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Dumper checking OpenFileCtx activeState: \" + activeState\n                + \" enabledDump: \" + enabledDump);\n          }\n        } catch (Throwable t) {\n          LOG.info(\"Dumper get Throwable: \" + t + \". dumpFilePath: \"\n              + OpenFileCtx.this.dumpFilePath, t);\n        }\n      }\n    }",
                "code_after_change": "    public void run() {\n      while (activeState && enabledDump) {\n        try {\n          if (nonSequentialWriteInMemory.get() >= DUMP_WRITE_WATER_MARK) {\n            dump();\n          }\n          synchronized (OpenFileCtx.this) {\n            if (nonSequentialWriteInMemory.get() < DUMP_WRITE_WATER_MARK) {\n              OpenFileCtx.this.notifyAll();\n              try {\n                OpenFileCtx.this.wait();\n                if (LOG.isDebugEnabled()) {\n                  LOG.debug(\"Dumper woke up\");\n                }\n              } catch (InterruptedException e) {\n                LOG.info(\"Dumper is interrupted, dumpFilePath= \"\n                    + OpenFileCtx.this.dumpFilePath);\n              }\n            }\n          }\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Dumper checking OpenFileCtx activeState: \" + activeState\n                + \" enabledDump: \" + enabledDump);\n          }\n        } catch (Throwable t) {\n          // unblock threads with new request\n          synchronized (OpenFileCtx.this) {\n            OpenFileCtx.this.notifyAll();\n          }\n          LOG.info(\"Dumper get Throwable: \" + t + \". dumpFilePath: \"\n              + OpenFileCtx.this.dumpFilePath, t);\n          activeState = false;\n        }\n      }\n    }"
            },
            "hadoop-hdfs-project.hadoop-hdfs-nfs.src.main.java.org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3.rmdir": {
                "code_before_change": "  public RMDIR3Response rmdir(XDR xdr, RpcInfo info) {\n    return rmdir(xdr, getSecurityHandler(info), info.remoteAddress());\n  }",
                "code_after_change": "  public RMDIR3Response rmdir(XDR xdr, RpcInfo info) {\n    return rmdir(xdr, getSecurityHandler(info), info.remoteAddress());\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Missing",
                "sub_category": ""
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report mentions issues with the NFSv3 gateway and provides logs indicating problems with the DFSClient, which is part of the shared stack trace context with the ground truth methods. However, it does not precisely identify the root cause in the ground truth methods. There is no fix suggestion provided in the bug report. The problem location is not precisely identified as no specific methods are mentioned outside of stack traces. All information in the report is relevant to the context of the bug, so there is no wrong information."
        }
    },
    {
        "filename": "HDFS-11377.json",
        "code_diff": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.balancer.Dispatcher.executePendingMove": {
                "code_before_change": "  public void executePendingMove(final PendingMove p) {\n    // move the reportedBlock\n    final DDatanode targetDn = p.target.getDDatanode();\n    ExecutorService moveExecutor = targetDn.getMoveExecutor();\n    if (moveExecutor == null) {\n      final int nThreads = moverThreadAllocator.allocate(maxConcurrentMovesPerNode);\n      if (nThreads > 0) {\n        moveExecutor = targetDn.initMoveExecutor(nThreads);\n      }\n    }\n    if (moveExecutor == null) {\n      LOG.warn(\"No mover threads available: skip moving \" + p);\n      return;\n    }\n    moveExecutor.execute(new Runnable() {\n      @Override\n      public void run() {\n        p.dispatch();\n      }\n    });\n  }",
                "code_after_change": "  public void executePendingMove(final PendingMove p) {\n    // move the block\n    final DDatanode targetDn = p.target.getDDatanode();\n    ExecutorService moveExecutor = targetDn.getMoveExecutor();\n    if (moveExecutor == null) {\n      final int nThreads = moverThreadAllocator.allocate(maxConcurrentMovesPerNode);\n      if (nThreads > 0) {\n        moveExecutor = targetDn.initMoveExecutor(nThreads);\n      }\n    }\n    if (moveExecutor == null) {\n      LOG.warn(\"No mover threads available: skip moving \" + p);\n      targetDn.removePendingBlock(p);\n      p.proxySource.removePendingBlock(p);\n      return;\n    }\n\n    moveExecutor.execute(new Runnable() {\n      @Override\n      public void run() {\n        p.dispatch();\n      }\n    });\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the issue as being related to 'No mover threads available' and mentions the method 'Dispatcher.waitForMoveCompletion' in the stack trace, which is in the same context as the ground truth method 'Dispatcher.executePendingMove'. However, it does not precisely identify the root cause or suggest a fix related to the ground truth method. The report lacks a specific fix suggestion, and while it mentions related methods, it does not pinpoint the exact method where the fix was applied. There is no incorrect information in the report."
        }
    },
    {
        "filename": "HDFS-6753.json",
        "code_diff": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DirectoryScanner.getGenStamp": {
                "code_before_change": "    public long getGenStamp() {\n      return metaSuffix != null ? Block.getGenerationStamp(\n          getMetaFile().getName()) : \n            GenerationStamp.GRANDFATHER_GENERATION_STAMP;\n    }",
                "code_after_change": "    public long getGenStamp() {\n      return metaSuffix != null ? Block.getGenerationStamp(\n          getMetaFile().getName()) : \n            GenerationStamp.GRANDFATHER_GENERATION_STAMP;\n    }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DirectoryScanner.isBlockMetaFile": {
                "code_before_change": "  private static boolean isBlockMetaFile(String blockId, String metaFile) {\n    return metaFile.startsWith(blockId)\n        && metaFile.endsWith(Block.METADATA_EXTENSION);\n  }",
                "code_after_change": "  private static boolean isBlockMetaFile(String blockId, String metaFile) {\n    return metaFile.startsWith(blockId)\n        && metaFile.endsWith(Block.METADATA_EXTENSION);\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DirectoryScanner.compileReport": {
                "code_before_change": "    private LinkedList<ScanInfo> compileReport(FsVolumeSpi vol, File dir,\n        LinkedList<ScanInfo> report) {\n      File[] files;\n      try {\n        files = FileUtil.listFiles(dir);\n      } catch (IOException ioe) {\n        LOG.warn(\"Exception occured while compiling report: \", ioe);\n        // Ignore this directory and proceed.\n        return report;\n      }\n      Arrays.sort(files);\n      /*\n       * Assumption: In the sorted list of files block file appears immediately\n       * before block metadata file. This is true for the current naming\n       * convention for block file blk_<blockid> and meta file\n       * blk_<blockid>_<genstamp>.meta\n       */\n      for (int i = 0; i < files.length; i++) {\n        if (files[i].isDirectory()) {\n          compileReport(vol, files[i], report);\n          continue;\n        }\n        if (!Block.isBlockFilename(files[i])) {\n          if (isBlockMetaFile(\"blk_\", files[i].getName())) {\n            long blockId = Block.getBlockId(files[i].getName());\n            report.add(new ScanInfo(blockId, null, files[i], vol));\n          }\n          continue;\n        }\n        File blockFile = files[i];\n        long blockId = Block.filename2id(blockFile.getName());\n        File metaFile = null;\n\n        // Skip all the files that start with block name until\n        // getting to the metafile for the block\n        while (i + 1 < files.length && files[i + 1].isFile()\n            && files[i + 1].getName().startsWith(blockFile.getName())) {\n          i++;\n          if (isBlockMetaFile(blockFile.getName(), files[i].getName())) {\n            metaFile = files[i];\n            break;\n          }\n        }\n        report.add(new ScanInfo(blockId, blockFile, metaFile, vol));\n      }\n      return report;\n    }",
                "code_after_change": "    private LinkedList<ScanInfo> compileReport(FsVolumeSpi vol,\n        File bpFinalizedDir, File dir, LinkedList<ScanInfo> report) {\n      File[] files;\n      try {\n        files = FileUtil.listFiles(dir);\n      } catch (IOException ioe) {\n        LOG.warn(\"Exception occured while compiling report: \", ioe);\n        // Initiate a check on disk failure.\n        datanode.checkDiskErrorAsync();\n        // Ignore this directory and proceed.\n        return report;\n      }\n      Arrays.sort(files);\n      /*\n       * Assumption: In the sorted list of files block file appears immediately\n       * before block metadata file. This is true for the current naming\n       * convention for block file blk_<blockid> and meta file\n       * blk_<blockid>_<genstamp>.meta\n       */\n      for (int i = 0; i < files.length; i++) {\n        if (files[i].isDirectory()) {\n          compileReport(vol, bpFinalizedDir, files[i], report);\n          continue;\n        }\n        if (!Block.isBlockFilename(files[i])) {\n          if (isBlockMetaFile(\"blk_\", files[i].getName())) {\n            long blockId = Block.getBlockId(files[i].getName());\n            verifyFileLocation(files[i].getParentFile(), bpFinalizedDir,\n                blockId);\n            report.add(new ScanInfo(blockId, null, files[i], vol));\n          }\n          continue;\n        }\n        File blockFile = files[i];\n        long blockId = Block.filename2id(blockFile.getName());\n        File metaFile = null;\n\n        // Skip all the files that start with block name until\n        // getting to the metafile for the block\n        while (i + 1 < files.length && files[i + 1].isFile()\n            && files[i + 1].getName().startsWith(blockFile.getName())) {\n          i++;\n          if (isBlockMetaFile(blockFile.getName(), files[i].getName())) {\n            metaFile = files[i];\n            break;\n          }\n        }\n        verifyFileLocation(blockFile.getParentFile(), bpFinalizedDir,\n            blockId);\n        report.add(new ScanInfo(blockId, blockFile, metaFile, vol));\n      }\n      return report;\n    }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DirectoryScanner.getDiskReport": {
                "code_before_change": "  private Map<String, ScanInfo[]> getDiskReport() {\n    // First get list of data directories\n    final List<? extends FsVolumeSpi> volumes = dataset.getVolumes();\n\n    // Use an array since the threads may return out of order and\n    // compilersInProgress#keySet may return out of order as well.\n    ScanInfoPerBlockPool[] dirReports = new ScanInfoPerBlockPool[volumes.size()];\n\n    Map<Integer, Future<ScanInfoPerBlockPool>> compilersInProgress =\n      new HashMap<Integer, Future<ScanInfoPerBlockPool>>();\n\n    for (int i = 0; i < volumes.size(); i++) {\n      if (isValid(dataset, volumes.get(i))) {\n        ReportCompiler reportCompiler =\n          new ReportCompiler(volumes.get(i));\n        Future<ScanInfoPerBlockPool> result = \n          reportCompileThreadPool.submit(reportCompiler);\n        compilersInProgress.put(i, result);\n      }\n    }\n    \n    for (Entry<Integer, Future<ScanInfoPerBlockPool>> report :\n        compilersInProgress.entrySet()) {\n      try {\n        dirReports[report.getKey()] = report.getValue().get();\n      } catch (Exception ex) {\n        LOG.error(\"Error compiling report\", ex);\n        // Propagate ex to DataBlockScanner to deal with\n        throw new RuntimeException(ex);\n      }\n    }\n\n    // Compile consolidated report for all the volumes\n    ScanInfoPerBlockPool list = new ScanInfoPerBlockPool();\n    for (int i = 0; i < volumes.size(); i++) {\n      if (isValid(dataset, volumes.get(i))) {\n        // volume is still valid\n        list.addAll(dirReports[i]);\n      }\n    }\n\n    return list.toSortedArrays();\n  }",
                "code_after_change": "  private Map<String, ScanInfo[]> getDiskReport() {\n    // First get list of data directories\n    final List<? extends FsVolumeSpi> volumes = dataset.getVolumes();\n\n    // Use an array since the threads may return out of order and\n    // compilersInProgress#keySet may return out of order as well.\n    ScanInfoPerBlockPool[] dirReports = new ScanInfoPerBlockPool[volumes.size()];\n\n    Map<Integer, Future<ScanInfoPerBlockPool>> compilersInProgress =\n      new HashMap<Integer, Future<ScanInfoPerBlockPool>>();\n\n    for (int i = 0; i < volumes.size(); i++) {\n      if (isValid(dataset, volumes.get(i))) {\n        ReportCompiler reportCompiler =\n          new ReportCompiler(datanode,volumes.get(i));\n        Future<ScanInfoPerBlockPool> result = \n          reportCompileThreadPool.submit(reportCompiler);\n        compilersInProgress.put(i, result);\n      }\n    }\n    \n    for (Entry<Integer, Future<ScanInfoPerBlockPool>> report :\n        compilersInProgress.entrySet()) {\n      try {\n        dirReports[report.getKey()] = report.getValue().get();\n      } catch (Exception ex) {\n        LOG.error(\"Error compiling report\", ex);\n        // Propagate ex to DataBlockScanner to deal with\n        throw new RuntimeException(ex);\n      }\n    }\n\n    // Compile consolidated report for all the volumes\n    ScanInfoPerBlockPool list = new ScanInfoPerBlockPool();\n    for (int i = 0; i < volumes.size(); i++) {\n      if (isValid(dataset, volumes.get(i))) {\n        // volume is still valid\n        list.addAll(dirReports[i]);\n      }\n    }\n\n    return list.toSortedArrays();\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataNode.initDirectoryScanner": {
                "code_before_change": "  private synchronized void initDirectoryScanner(Configuration conf) {\n    if (directoryScanner != null) {\n      return;\n    }\n    String reason = null;\n    if (conf.getInt(DFS_DATANODE_DIRECTORYSCAN_INTERVAL_KEY, \n                    DFS_DATANODE_DIRECTORYSCAN_INTERVAL_DEFAULT) < 0) {\n      reason = \"verification is turned off by configuration\";\n    } else if (\"SimulatedFSDataset\".equals(data.getClass().getSimpleName())) {\n      reason = \"verifcation is not supported by SimulatedFSDataset\";\n    } \n    if (reason == null) {\n      directoryScanner = new DirectoryScanner(data, conf);\n      directoryScanner.start();\n    } else {\n      LOG.info(\"Periodic Directory Tree Verification scan is disabled because \" +\n                   reason);\n    }\n  }",
                "code_after_change": "  private synchronized void initDirectoryScanner(Configuration conf) {\n    if (directoryScanner != null) {\n      return;\n    }\n    String reason = null;\n    if (conf.getInt(DFS_DATANODE_DIRECTORYSCAN_INTERVAL_KEY, \n                    DFS_DATANODE_DIRECTORYSCAN_INTERVAL_DEFAULT) < 0) {\n      reason = \"verification is turned off by configuration\";\n    } else if (\"SimulatedFSDataset\".equals(data.getClass().getSimpleName())) {\n      reason = \"verifcation is not supported by SimulatedFSDataset\";\n    } \n    if (reason == null) {\n      directoryScanner = new DirectoryScanner(this, data, conf);\n      directoryScanner.start();\n    } else {\n      LOG.info(\"Periodic Directory Tree Verification scan is disabled because \" +\n                   reason);\n    }\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report mentions the method 'DirectoryScanner.compileReport' in the stack trace, which is part of the ground truth methods, but it does not explicitly identify it as the root cause. Therefore, the root cause identification is 'Partial' with 'Shared Stack Trace Context' as the sub-category. The report does not provide any fix suggestion, so it is marked as 'Missing' for fix suggestion. The problem location identification is also 'Partial' with 'Shared Stack Trace Context' because the method is mentioned in the stack trace but not directly identified as the problem location. There is no wrong information in the bug report, so it is marked as 'No' for wrong information."
        }
    },
    {
        "filename": "HDFS-3443.json",
        "code_diff": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.complete": {
                "code_before_change": "  public boolean complete(String src, String clientName, ExtendedBlock last)\n      throws IOException {\n    if(stateChangeLog.isDebugEnabled()) {\n      stateChangeLog.debug(\"*DIR* NameNode.complete: \"\n          + src + \" for \" + clientName);\n    }\n    return namesystem.completeFile(src, clientName, last);\n  }",
                "code_after_change": "  public boolean complete(String src, String clientName,\n                          ExtendedBlock last,  long fileId)\n      throws IOException {\n    checkNNStartup();\n    if(stateChangeLog.isDebugEnabled()) {\n      stateChangeLog.debug(\"*DIR* NameNode.complete: \"\n          + src + \" fileId=\" + fileId +\" for \" + clientName);\n    }\n    return namesystem.completeFile(src, clientName, last, fileId);\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.setXAttr": {
                "code_before_change": [],
                "code_after_change": "  public void setXAttr(String src, XAttr xAttr, EnumSet<XAttrSetFlag> flag)\n      throws IOException {\n    checkNNStartup();\n    CacheEntry cacheEntry = RetryCache.waitForCompletion(retryCache);\n    if (cacheEntry != null && cacheEntry.isSuccess()) {\n      return; // Return previous response\n    }\n    boolean success = false;\n    try {\n      namesystem.setXAttr(src, xAttr, flag, cacheEntry != null);\n      success = true;\n    } finally {\n      RetryCache.setState(cacheEntry, success);\n    }\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.errorReport": {
                "code_before_change": "  public void errorReport(NamenodeRegistration registration,\n                          int errorCode, \n                          String msg) throws IOException {\n    namesystem.checkOperation(OperationCategory.UNCHECKED);\n    verifyRequest(registration);\n    LOG.info(\"Error report from \" + registration + \": \" + msg);\n    if(errorCode == FATAL)\n      namesystem.releaseBackupNode(registration);\n  }",
                "code_after_change": "  public void errorReport(NamenodeRegistration registration,\n                          int errorCode, \n                          String msg) throws IOException {\n    checkNNStartup();\n    namesystem.checkOperation(OperationCategory.UNCHECKED);\n    namesystem.checkSuperuserPrivilege();\n    verifyRequest(registration);\n    LOG.info(\"Error report from \" + registration + \": \" + msg);\n    if (errorCode == FATAL) {\n      namesystem.releaseBackupNode(registration);\n    }\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getLinkTarget": {
                "code_before_change": "  public String getLinkTarget(String path) throws IOException {\n    metrics.incrGetLinkTargetOps();\n    try {\n      HdfsFileStatus stat = namesystem.getFileInfo(path, false);\n      if (stat != null) {\n        // NB: getSymlink throws IOException if !stat.isSymlink() \n        return stat.getSymlink();\n      }\n    } catch (UnresolvedPathException e) {\n      return e.getResolvedPath().toString();\n    } catch (UnresolvedLinkException e) {\n      // The NameNode should only throw an UnresolvedPathException\n      throw new AssertionError(\"UnresolvedLinkException thrown\");\n    }\n    return null;\n  }",
                "code_after_change": "  public String getLinkTarget(String path) throws IOException {\n    checkNNStartup();\n    metrics.incrGetLinkTargetOps();\n    HdfsFileStatus stat = null;\n    try {\n      stat = namesystem.getFileInfo(path, false);\n    } catch (UnresolvedPathException e) {\n      return e.getResolvedPath().toString();\n    } catch (UnresolvedLinkException e) {\n      // The NameNode should only throw an UnresolvedPathException\n      throw new AssertionError(\"UnresolvedLinkException thrown\");\n    }\n    if (stat == null) {\n      throw new FileNotFoundException(\"File does not exist: \" + path);\n    } else if (!stat.isSymlink()) {\n      throw new IOException(\"Path \" + path + \" is not a symbolic link\");\n    }\n    return stat.getSymlink();\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.cacheReport": {
                "code_before_change": [],
                "code_after_change": "  public DatanodeCommand cacheReport(DatanodeRegistration nodeReg,\n      String poolId, List<Long> blockIds) throws IOException {\n    checkNNStartup();\n    verifyRequest(nodeReg);\n    if (blockStateChangeLog.isDebugEnabled()) {\n      blockStateChangeLog.debug(\"*BLOCK* NameNode.cacheReport: \"\n           + \"from \" + nodeReg + \" \" + blockIds.size() + \" blocks\");\n    }\n    namesystem.getCacheManager().processCacheReport(nodeReg, blockIds);\n    return null;\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getClientMachine": {
                "code_before_change": "  private static String getClientMachine() {\n    String clientMachine = NamenodeWebHdfsMethods.getRemoteAddress();\n    if (clientMachine == null) { //not a web client\n      clientMachine = Server.getRemoteAddress();\n    }\n    if (clientMachine == null) { //not a RPC client\n      clientMachine = \"\";\n    }\n    return clientMachine;\n  }",
                "code_after_change": "  private static String getClientMachine() {\n    String clientMachine = NamenodeWebHdfsMethods.getRemoteAddress();\n    if (clientMachine == null) { //not a web client\n      clientMachine = Server.getRemoteAddress();\n    }\n    if (clientMachine == null) { //not a RPC client\n      clientMachine = \"\";\n    }\n    return clientMachine;\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getMostRecentCheckpointTxId": {
                "code_before_change": [],
                "code_after_change": "  public long getMostRecentCheckpointTxId() throws IOException {\n    checkNNStartup();\n    namesystem.checkOperation(OperationCategory.UNCHECKED);\n    namesystem.checkSuperuserPrivilege();\n    return namesystem.getFSImage().getMostRecentCheckpointTxId();\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getListing": {
                "code_before_change": "  public DirectoryListing getListing(String src, byte[] startAfter,\n      boolean needLocation) throws IOException {\n    DirectoryListing files = namesystem.getListing(\n        src, startAfter, needLocation);\n    if (files != null) {\n      metrics.incrGetListingOps();\n      metrics.incrFilesInGetListingOps(files.getPartialListing().length);\n    }\n    return files;\n  }",
                "code_after_change": "  public DirectoryListing getListing(String src, byte[] startAfter,\n      boolean needLocation) throws IOException {\n    checkNNStartup();\n    DirectoryListing files = namesystem.getListing(\n        src, startAfter, needLocation);\n    if (files != null) {\n      metrics.incrGetListingOps();\n      metrics.incrFilesInGetListingOps(files.getPartialListing().length);\n    }\n    return files;\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rename2": {
                "code_before_change": "  public void rename2(String src, String dst, Options.Rename... options)\n      throws IOException {\n    if(stateChangeLog.isDebugEnabled()) {\n      stateChangeLog.debug(\"*DIR* NameNode.rename: \" + src + \" to \" + dst);\n    }\n    if (!checkPathLength(dst)) {\n      throw new IOException(\"rename: Pathname too long.  Limit \"\n          + MAX_PATH_LENGTH + \" characters, \" + MAX_PATH_DEPTH + \" levels.\");\n    }\n    namesystem.renameTo(src, dst, options);\n    metrics.incrFilesRenamed();\n  }",
                "code_after_change": "  public void rename2(String src, String dst, Options.Rename... options)\n      throws IOException {\n    checkNNStartup();\n    if(stateChangeLog.isDebugEnabled()) {\n      stateChangeLog.debug(\"*DIR* NameNode.rename: \" + src + \" to \" + dst);\n    }\n    if (!checkPathLength(dst)) {\n      throw new IOException(\"rename: Pathname too long.  Limit \"\n          + MAX_PATH_LENGTH + \" characters, \" + MAX_PATH_DEPTH + \" levels.\");\n    }\n    CacheEntry cacheEntry = RetryCache.waitForCompletion(retryCache);\n    if (cacheEntry != null && cacheEntry.isSuccess()) {\n      return; // Return previous response\n    }\n    boolean success = false;\n    try {\n      namesystem.renameTo(src, dst, cacheEntry != null, options);\n      success = true;\n    } finally {\n      RetryCache.setState(cacheEntry, success);\n    }\n    metrics.incrFilesRenamed();\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getStats": {
                "code_before_change": "  public long[] getStats() throws IOException {\n    namesystem.checkOperation(OperationCategory.READ);\n    return namesystem.getStats();\n  }",
                "code_after_change": "  public long[] getStats() throws IOException {\n    checkNNStartup();\n    namesystem.checkOperation(OperationCategory.READ);\n    return namesystem.getStats();\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.create": {
                "code_before_change": "  public void create(String src, \n                     FsPermission masked,\n                     String clientName, \n                     EnumSetWritable<CreateFlag> flag,\n                     boolean createParent,\n                     short replication,\n                     long blockSize) throws IOException {\n    String clientMachine = getClientMachine();\n    if (stateChangeLog.isDebugEnabled()) {\n      stateChangeLog.debug(\"*DIR* NameNode.create: file \"\n                         +src+\" for \"+clientName+\" at \"+clientMachine);\n    }\n    if (!checkPathLength(src)) {\n      throw new IOException(\"create: Pathname too long.  Limit \"\n          + MAX_PATH_LENGTH + \" characters, \" + MAX_PATH_DEPTH + \" levels.\");\n    }\n    namesystem.startFile(src,\n        new PermissionStatus(UserGroupInformation.getCurrentUser().getShortUserName(),\n            null, masked),\n        clientName, clientMachine, flag.get(), createParent, replication, blockSize);\n    metrics.incrFilesCreated();\n    metrics.incrCreateFileOps();\n  }",
                "code_after_change": "  public HdfsFileStatus create(String src, FsPermission masked,\n      String clientName, EnumSetWritable<CreateFlag> flag,\n      boolean createParent, short replication, long blockSize, \n      CryptoProtocolVersion[] supportedVersions)\n      throws IOException {\n    checkNNStartup();\n    String clientMachine = getClientMachine();\n    if (stateChangeLog.isDebugEnabled()) {\n      stateChangeLog.debug(\"*DIR* NameNode.create: file \"\n          +src+\" for \"+clientName+\" at \"+clientMachine);\n    }\n    if (!checkPathLength(src)) {\n      throw new IOException(\"create: Pathname too long.  Limit \"\n          + MAX_PATH_LENGTH + \" characters, \" + MAX_PATH_DEPTH + \" levels.\");\n    }\n\n    CacheEntryWithPayload cacheEntry = RetryCache.waitForCompletion(retryCache, null);\n    if (cacheEntry != null && cacheEntry.isSuccess()) {\n      return (HdfsFileStatus) cacheEntry.getPayload();\n    }\n\n    HdfsFileStatus status = null;\n    try {\n      PermissionStatus perm = new PermissionStatus(getRemoteUser()\n          .getShortUserName(), null, masked);\n      status = namesystem.startFile(src, perm, clientName, clientMachine,\n          flag.get(), createParent, replication, blockSize, supportedVersions,\n          cacheEntry != null);\n    } finally {\n      RetryCache.setState(cacheEntry, status != null, status);\n    }\n\n    metrics.incrFilesCreated();\n    metrics.incrCreateFileOps();\n    return status;\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.append": {
                "code_before_change": "  public LocatedBlock append(String src, String clientName) \n      throws IOException {\n    String clientMachine = getClientMachine();\n    if (stateChangeLog.isDebugEnabled()) {\n      stateChangeLog.debug(\"*DIR* NameNode.append: file \"\n          +src+\" for \"+clientName+\" at \"+clientMachine);\n    }\n    LocatedBlock info = namesystem.appendFile(src, clientName, clientMachine);\n    metrics.incrFilesAppended();\n    return info;\n  }",
                "code_after_change": "  public LastBlockWithStatus append(String src, String clientName) \n      throws IOException {\n    checkNNStartup();\n    String clientMachine = getClientMachine();\n    if (stateChangeLog.isDebugEnabled()) {\n      stateChangeLog.debug(\"*DIR* NameNode.append: file \"\n          +src+\" for \"+clientName+\" at \"+clientMachine);\n    }\n    CacheEntryWithPayload cacheEntry = RetryCache.waitForCompletion(retryCache, null);\n    if (cacheEntry != null && cacheEntry.isSuccess()) {\n      return (LastBlockWithStatus) cacheEntry.getPayload();\n    }\n\n    LastBlockWithStatus info = null;\n    boolean success = false;\n    try {\n      info = namesystem.appendFile(src, clientName, clientMachine,\n          cacheEntry != null);\n      success = true;\n    } finally {\n      RetryCache.setState(cacheEntry, success, info);\n    }\n    metrics.incrFilesAppended();\n    return info;\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.registerSubordinateNamenode": {
                "code_before_change": [],
                "code_after_change": "  public NamenodeRegistration registerSubordinateNamenode(\n      NamenodeRegistration registration) throws IOException {\n    checkNNStartup();\n    namesystem.checkSuperuserPrivilege();\n    verifyLayoutVersion(registration.getVersion());\n    NamenodeRegistration myRegistration = nn.setRegistration();\n    namesystem.registerBackupNode(registration, myRegistration);\n    return myRegistration;\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.renameSnapshot": {
                "code_before_change": [],
                "code_after_change": "  public void renameSnapshot(String snapshotRoot, String snapshotOldName,\n      String snapshotNewName) throws IOException {\n    checkNNStartup();\n    if (snapshotNewName == null || snapshotNewName.isEmpty()) {\n      throw new IOException(\"The new snapshot name is null or empty.\");\n    }\n    metrics.incrRenameSnapshotOps();\n    CacheEntry cacheEntry = RetryCache.waitForCompletion(retryCache);\n    if (cacheEntry != null && cacheEntry.isSuccess()) {\n      return; // Return previous response\n    }\n    boolean success = false;\n    try {\n      namesystem.renameSnapshot(snapshotRoot, snapshotOldName,\n          snapshotNewName, cacheEntry != null);\n      success = true;\n    } finally {\n      RetryCache.setState(cacheEntry, success);\n    }\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.updateBlockForPipeline": {
                "code_before_change": "  public LocatedBlock updateBlockForPipeline(ExtendedBlock block, String clientName)\n      throws IOException {\n    return namesystem.updateBlockForPipeline(block, clientName);\n  }",
                "code_after_change": "  public LocatedBlock updateBlockForPipeline(ExtendedBlock block, String clientName)\n      throws IOException {\n    checkNNStartup();\n    return namesystem.updateBlockForPipeline(block, clientName);\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.concat": {
                "code_before_change": "  public void concat(String trg, String[] src) throws IOException {\n    namesystem.concat(trg, src);\n  }",
                "code_after_change": "  public void concat(String trg, String[] src) throws IOException {\n    checkNNStartup();\n    CacheEntry cacheEntry = RetryCache.waitForCompletion(retryCache);\n    if (cacheEntry != null && cacheEntry.isSuccess()) {\n      return; // Return previous response\n    }\n    boolean success = false;\n\n    try {\n      namesystem.concat(trg, src, cacheEntry != null);\n      success = true;\n    } finally {\n      RetryCache.setState(cacheEntry, success);\n    }\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getTransactionID": {
                "code_before_change": "  public long getTransactionID() throws IOException {\n    namesystem.checkOperation(OperationCategory.CHECKPOINT);\n    return namesystem.getEditLog().getSyncTxId();\n  }",
                "code_after_change": "  public long getTransactionID() throws IOException {\n    checkNNStartup();\n    namesystem.checkOperation(OperationCategory.UNCHECKED);\n    namesystem.checkSuperuserPrivilege();\n    return namesystem.getFSImage().getLastAppliedOrWrittenTxId();\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getDatanodeReport": {
                "code_before_change": "  public DatanodeInfo[] getDatanodeReport(DatanodeReportType type)\n  throws IOException {\n    namesystem.checkOperation(OperationCategory.UNCHECKED);\n    DatanodeInfo results[] = namesystem.datanodeReport(type);\n    if (results == null ) {\n      throw new IOException(\"Cannot find datanode report\");\n    }\n    return results;\n  }",
                "code_after_change": "  public DatanodeInfo[] getDatanodeReport(DatanodeReportType type)\n  throws IOException {\n    checkNNStartup();\n    DatanodeInfo results[] = namesystem.datanodeReport(type);\n    return results;\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.setSafeMode": {
                "code_before_change": "  public boolean setSafeMode(SafeModeAction action) throws IOException {\n    namesystem.checkOperation(OperationCategory.UNCHECKED);\n    return namesystem.setSafeMode(action);\n  }",
                "code_after_change": "  public boolean setSafeMode(SafeModeAction action, boolean isChecked)\n      throws IOException {\n    checkNNStartup();\n    OperationCategory opCategory = OperationCategory.UNCHECKED;\n    if (isChecked) {\n      if (action == SafeModeAction.SAFEMODE_GET) {\n        opCategory = OperationCategory.READ;\n      } else {\n        opCategory = OperationCategory.WRITE;\n      }\n    }\n    namesystem.checkOperation(opCategory);\n    return namesystem.setSafeMode(action);\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.allowSnapshot": {
                "code_before_change": [],
                "code_after_change": "  public void allowSnapshot(String snapshotRoot) throws IOException {\n    checkNNStartup();\n    metrics.incrAllowSnapshotOps();\n    namesystem.allowSnapshot(snapshotRoot);\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.getProtocolVersion": {
                "code_before_change": "  public long getProtocolVersion(String protocol, \n                                 long clientVersion) throws IOException {\n    if (protocol.equals(ClientProtocol.class.getName())) {\n      return ClientProtocol.versionID; \n    } else if (protocol.equals(DatanodeProtocol.class.getName())){\n      return DatanodeProtocol.versionID;\n    } else if (protocol.equals(NamenodeProtocol.class.getName())){\n      return NamenodeProtocol.versionID;\n    } else if (protocol.equals(RefreshAuthorizationPolicyProtocol.class.getName())){\n      return RefreshAuthorizationPolicyProtocol.versionID;\n    } else if (protocol.equals(RefreshUserMappingsProtocol.class.getName())){\n      return RefreshUserMappingsProtocol.versionID;\n    } else if (protocol.equals(GetUserMappingsProtocol.class.getName())){\n      return GetUserMappingsProtocol.versionID;\n    } else {\n      throw new IOException(\"Unknown protocol to name node: \" + protocol);\n    }\n  }",
                "code_after_change": "  public long getProtocolVersion(String protocol, \n                                 long clientVersion) throws IOException {\n    if (protocol.equals(ClientProtocol.class.getName())) {\n      return ClientProtocol.versionID; \n    } else if (protocol.equals(DatanodeProtocol.class.getName())){\n      return DatanodeProtocol.versionID;\n    } else if (protocol.equals(NamenodeProtocol.class.getName())){\n      return NamenodeProtocol.versionID;\n    } else if (protocol.equals(RefreshAuthorizationPolicyProtocol.class.getName())){\n      return RefreshAuthorizationPolicyProtocol.versionID;\n    } else if (protocol.equals(RefreshUserMappingsProtocol.class.getName())){\n      return RefreshUserMappingsProtocol.versionID;\n    } else if (protocol.equals(RefreshCallQueueProtocol.class.getName())) {\n      return RefreshCallQueueProtocol.versionID;\n    } else if (protocol.equals(GetUserMappingsProtocol.class.getName())){\n      return GetUserMappingsProtocol.versionID;\n    } else if (protocol.equals(TraceAdminProtocol.class.getName())){\n      return TraceAdminProtocol.versionID;\n    } else {\n      throw new IOException(\"Unknown protocol to name node: \" + protocol);\n    }\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getEditLogManifest": {
                "code_before_change": "  public RemoteEditLogManifest getEditLogManifest(long sinceTxId)\n  throws IOException {\n    namesystem.checkOperation(OperationCategory.READ);\n    return namesystem.getEditLog().getEditLogManifest(sinceTxId);\n  }",
                "code_after_change": "  public RemoteEditLogManifest getEditLogManifest(long sinceTxId)\n      throws IOException {\n    checkNNStartup();\n    namesystem.checkOperation(OperationCategory.READ);\n    namesystem.checkSuperuserPrivilege();\n    return namesystem.getEditLog().getEditLogManifest(sinceTxId);\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.listCorruptFileBlocks": {
                "code_before_change": "  public CorruptFileBlocks listCorruptFileBlocks(String path, String cookie)\n      throws IOException {\n    String[] cookieTab = new String[] { cookie };\n    Collection<FSNamesystem.CorruptFileBlockInfo> fbs =\n      namesystem.listCorruptFileBlocks(path, cookieTab);\n\n    String[] files = new String[fbs.size()];\n    int i = 0;\n    for(FSNamesystem.CorruptFileBlockInfo fb: fbs) {\n      files[i++] = fb.path;\n    }\n    return new CorruptFileBlocks(files, cookieTab[0]);\n  }",
                "code_after_change": "  public CorruptFileBlocks listCorruptFileBlocks(String path, String cookie)\n      throws IOException {\n    checkNNStartup();\n    String[] cookieTab = new String[] { cookie };\n    Collection<FSNamesystem.CorruptFileBlockInfo> fbs =\n      namesystem.listCorruptFileBlocks(path, cookieTab);\n\n    String[] files = new String[fbs.size()];\n    int i = 0;\n    for(FSNamesystem.CorruptFileBlockInfo fb: fbs) {\n      files[i++] = fb.path;\n    }\n    return new CorruptFileBlocks(files, cookieTab[0]);\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getEditsFromTxid": {
                "code_before_change": [],
                "code_after_change": "  public EventBatchList getEditsFromTxid(long txid) throws IOException {\n    checkNNStartup();\n    namesystem.checkOperation(OperationCategory.READ); // only active\n    namesystem.checkSuperuserPrivilege();\n    int maxEventsPerRPC = nn.conf.getInt(\n        DFSConfigKeys.DFS_NAMENODE_INOTIFY_MAX_EVENTS_PER_RPC_KEY,\n        DFSConfigKeys.DFS_NAMENODE_INOTIFY_MAX_EVENTS_PER_RPC_DEFAULT);\n    FSEditLog log = namesystem.getFSImage().getEditLog();\n    long syncTxid = log.getSyncTxId();\n    // If we haven't synced anything yet, we can only read finalized\n    // segments since we can't reliably determine which txns in in-progress\n    // segments have actually been committed (e.g. written to a quorum of JNs).\n    // If we have synced txns, we can definitely read up to syncTxid since\n    // syncTxid is only updated after a transaction is committed to all\n    // journals. (In-progress segments written by old writers are already\n    // discarded for us, so if we read any in-progress segments they are\n    // guaranteed to have been written by this NameNode.)\n    boolean readInProgress = syncTxid > 0;\n\n    List<EventBatch> batches = Lists.newArrayList();\n    int totalEvents = 0;\n    long maxSeenTxid = -1;\n    long firstSeenTxid = -1;\n\n    if (syncTxid > 0 && txid > syncTxid) {\n      // we can't read past syncTxid, so there's no point in going any further\n      return new EventBatchList(batches, firstSeenTxid, maxSeenTxid, syncTxid);\n    }\n\n    Collection<EditLogInputStream> streams = null;\n    try {\n      streams = log.selectInputStreams(txid, 0, null, readInProgress);\n    } catch (IllegalStateException e) { // can happen if we have\n      // transitioned out of active and haven't yet transitioned to standby\n      // and are using QJM -- the edit log will be closed and this exception\n      // will result\n      LOG.info(\"NN is transitioning from active to standby and FSEditLog \" +\n      \"is closed -- could not read edits\");\n      return new EventBatchList(batches, firstSeenTxid, maxSeenTxid, syncTxid);\n    }\n\n    boolean breakOuter = false;\n    for (EditLogInputStream elis : streams) {\n      // our assumption in this code is the EditLogInputStreams are ordered by\n      // starting txid\n      try {\n        FSEditLogOp op = null;\n        while ((op = readOp(elis)) != null) {\n          // break out of here in the unlikely event that syncTxid is so\n          // out of date that its segment has already been deleted, so the first\n          // txid we get is greater than syncTxid\n          if (syncTxid > 0 && op.getTransactionId() > syncTxid) {\n            breakOuter = true;\n            break;\n          }\n\n          EventBatch eventBatch = InotifyFSEditLogOpTranslator.translate(op);\n          if (eventBatch != null) {\n            batches.add(eventBatch);\n            totalEvents += eventBatch.getEvents().length;\n          }\n          if (op.getTransactionId() > maxSeenTxid) {\n            maxSeenTxid = op.getTransactionId();\n          }\n          if (firstSeenTxid == -1) {\n            firstSeenTxid = op.getTransactionId();\n          }\n          if (totalEvents >= maxEventsPerRPC || (syncTxid > 0 &&\n              op.getTransactionId() == syncTxid)) {\n            // we're done\n            breakOuter = true;\n            break;\n          }\n        }\n      } finally {\n        elis.close();\n      }\n      if (breakOuter) {\n        break;\n      }\n    }\n\n    return new EventBatchList(batches, firstSeenTxid, maxSeenTxid, syncTxid);\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.removeCachePool": {
                "code_before_change": [],
                "code_after_change": "  public void removeCachePool(String cachePoolName) throws IOException {\n    checkNNStartup();\n    CacheEntry cacheEntry = RetryCache.waitForCompletion(retryCache);\n    if (cacheEntry != null && cacheEntry.isSuccess()) {\n      return;\n    }\n    boolean success = false;\n    try {\n      namesystem.removeCachePool(cachePoolName, cacheEntry != null);\n      success = true;\n    } finally {\n      RetryCache.setState(cacheEntry, success);\n    }\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rename": {
                "code_before_change": "  public boolean rename(String src, String dst) throws IOException {\n    if(stateChangeLog.isDebugEnabled()) {\n      stateChangeLog.debug(\"*DIR* NameNode.rename: \" + src + \" to \" + dst);\n    }\n    if (!checkPathLength(dst)) {\n      throw new IOException(\"rename: Pathname too long.  Limit \"\n          + MAX_PATH_LENGTH + \" characters, \" + MAX_PATH_DEPTH + \" levels.\");\n    }\n    boolean ret = namesystem.renameTo(src, dst);\n    if (ret) {\n      metrics.incrFilesRenamed();\n    }\n    return ret;\n  }",
                "code_after_change": "  public boolean rename(String src, String dst) throws IOException {\n    checkNNStartup();\n    if(stateChangeLog.isDebugEnabled()) {\n      stateChangeLog.debug(\"*DIR* NameNode.rename: \" + src + \" to \" + dst);\n    }\n    if (!checkPathLength(dst)) {\n      throw new IOException(\"rename: Pathname too long.  Limit \"\n          + MAX_PATH_LENGTH + \" characters, \" + MAX_PATH_DEPTH + \" levels.\");\n    }\n\n    CacheEntry cacheEntry = RetryCache.waitForCompletion(retryCache);\n    if (cacheEntry != null && cacheEntry.isSuccess()) {\n      return true; // Return previous response\n    }\n\n    boolean ret = false;\n    try {\n      ret = namesystem.renameTo(src, dst, cacheEntry != null);\n    } finally {\n      RetryCache.setState(cacheEntry, ret);\n    }\n    if (ret) {\n      metrics.incrFilesRenamed();\n    }\n    return ret;\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations": {
                "code_before_change": "  public LocatedBlocks getBlockLocations(String src, \n                                          long offset, \n                                          long length) \n      throws IOException {\n    metrics.incrGetBlockLocations();\n    return namesystem.getBlockLocations(getClientMachine(), \n                                        src, offset, length);\n  }",
                "code_after_change": "  public LocatedBlocks getBlockLocations(String src, \n                                          long offset, \n                                          long length) \n      throws IOException {\n    checkNNStartup();\n    metrics.incrGetBlockLocations();\n    return namesystem.getBlockLocations(getClientMachine(), \n                                        src, offset, length);\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.createSymlink": {
                "code_before_change": "  public void createSymlink(String target, String link, FsPermission dirPerms,\n      boolean createParent) throws IOException {\n    metrics.incrCreateSymlinkOps();\n    /* We enforce the MAX_PATH_LENGTH limit even though a symlink target \n     * URI may refer to a non-HDFS file system. \n     */\n    if (!checkPathLength(link)) {\n      throw new IOException(\"Symlink path exceeds \" + MAX_PATH_LENGTH +\n                            \" character limit\");\n                            \n    }\n    if (\"\".equals(target)) {\n      throw new IOException(\"Invalid symlink target\");\n    }\n    final UserGroupInformation ugi = UserGroupInformation.getCurrentUser();\n    namesystem.createSymlink(target, link,\n      new PermissionStatus(ugi.getShortUserName(), null, dirPerms), createParent);\n  }",
                "code_after_change": "  public void createSymlink(String target, String link, FsPermission dirPerms,\n      boolean createParent) throws IOException {\n    checkNNStartup();\n    CacheEntry cacheEntry = RetryCache.waitForCompletion(retryCache);\n    if (cacheEntry != null && cacheEntry.isSuccess()) {\n      return; // Return previous response\n    }\n\n    /* We enforce the MAX_PATH_LENGTH limit even though a symlink target\n     * URI may refer to a non-HDFS file system. \n     */\n    if (!checkPathLength(link)) {\n      throw new IOException(\"Symlink path exceeds \" + MAX_PATH_LENGTH +\n                            \" character limit\");\n                            \n    }\n\n    final UserGroupInformation ugi = getRemoteUser();\n\n    boolean success = false;\n    try {\n      PermissionStatus perm = new PermissionStatus(ugi.getShortUserName(),\n          null, dirPerms);\n      namesystem.createSymlink(target, link, perm, createParent,\n          cacheEntry != null);\n      success = true;\n    } finally {\n      RetryCache.setState(cacheEntry, success);\n    }\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollingUpgrade": {
                "code_before_change": [],
                "code_after_change": "  public RollingUpgradeInfo rollingUpgrade(RollingUpgradeAction action) throws IOException {\n    checkNNStartup();\n    LOG.info(\"rollingUpgrade \" + action);\n    switch(action) {\n    case QUERY:\n      return namesystem.queryRollingUpgrade();\n    case PREPARE:\n      return namesystem.startRollingUpgrade();\n    case FINALIZE:\n      namesystem.finalizeRollingUpgrade();\n      return null;\n    default:\n      throw new UnsupportedActionException(action + \" is not yet supported.\");\n    }\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs": {
                "code_before_change": "  public boolean mkdirs(String src, FsPermission masked, boolean createParent)\n      throws IOException {\n    if(stateChangeLog.isDebugEnabled()) {\n      stateChangeLog.debug(\"*DIR* NameNode.mkdirs: \" + src);\n    }\n    if (!checkPathLength(src)) {\n      throw new IOException(\"mkdirs: Pathname too long.  Limit \" \n                            + MAX_PATH_LENGTH + \" characters, \" + MAX_PATH_DEPTH + \" levels.\");\n    }\n    return namesystem.mkdirs(src,\n        new PermissionStatus(UserGroupInformation.getCurrentUser().getShortUserName(),\n            null, masked), createParent);\n  }",
                "code_after_change": "  public boolean mkdirs(String src, FsPermission masked, boolean createParent)\n      throws IOException {\n    checkNNStartup();\n    if(stateChangeLog.isDebugEnabled()) {\n      stateChangeLog.debug(\"*DIR* NameNode.mkdirs: \" + src);\n    }\n    if (!checkPathLength(src)) {\n      throw new IOException(\"mkdirs: Pathname too long.  Limit \" \n                            + MAX_PATH_LENGTH + \" characters, \" + MAX_PATH_DEPTH + \" levels.\");\n    }\n    return namesystem.mkdirs(src,\n        new PermissionStatus(getRemoteUser().getShortUserName(),\n            null, masked), createParent);\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.abandonBlock": {
                "code_before_change": "  public void abandonBlock(ExtendedBlock b, String src, String holder)\n      throws IOException {\n    if(stateChangeLog.isDebugEnabled()) {\n      stateChangeLog.debug(\"*BLOCK* NameNode.abandonBlock: \"\n          +b+\" of file \"+src);\n    }\n    if (!namesystem.abandonBlock(b, src, holder)) {\n      throw new IOException(\"Cannot abandon block during write to \" + src);\n    }\n  }",
                "code_after_change": "  public void abandonBlock(ExtendedBlock b, long fileId, String src,\n        String holder) throws IOException {\n    checkNNStartup();\n    if(stateChangeLog.isDebugEnabled()) {\n      stateChangeLog.debug(\"*BLOCK* NameNode.abandonBlock: \"\n          +b+\" of file \"+src);\n    }\n    if (!namesystem.abandonBlock(b, fileId, src, holder)) {\n      throw new IOException(\"Cannot abandon block during write to \" + src);\n    }\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.readOp": {
                "code_before_change": [],
                "code_after_change": "  private static FSEditLogOp readOp(EditLogInputStream elis)\n      throws IOException {\n    try {\n      return elis.readOp();\n      // we can get the below two exceptions if a segment is deleted\n      // (because we have accumulated too many edits) or (for the local journal/\n      // no-QJM case only) if a in-progress segment is finalized under us ...\n      // no need to throw an exception back to the client in this case\n    } catch (FileNotFoundException e) {\n      LOG.debug(\"Tried to read from deleted or moved edit log segment\", e);\n      return null;\n    } catch (TransferFsImage.HttpGetFailedException e) {\n      LOG.debug(\"Tried to read from deleted edit log segment\", e);\n      return null;\n    }\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.saveNamespace": {
                "code_before_change": "  public void saveNamespace() throws IOException {\n    namesystem.checkOperation(OperationCategory.UNCHECKED);\n    namesystem.saveNamespace();\n  }",
                "code_after_change": "  public void saveNamespace() throws IOException {\n    checkNNStartup();\n    CacheEntry cacheEntry = RetryCache.waitForCompletion(retryCache);\n    if (cacheEntry != null && cacheEntry.isSuccess()) {\n      return; // Return previous response\n    }\n    boolean success = false;\n    try {\n      namesystem.saveNamespace();\n      success = true;\n    } finally {\n      RetryCache.setState(cacheEntry, success);\n    }\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.sendHeartbeat": {
                "code_before_change": "  public HeartbeatResponse sendHeartbeat(DatanodeRegistration nodeReg,\n      StorageReport[] report, int xmitsInProgress, int xceiverCount,\n      int failedVolumes) throws IOException {\n    verifyRequest(nodeReg);\n    return namesystem.handleHeartbeat(nodeReg, report[0].getCapacity(),\n        report[0].getDfsUsed(), report[0].getRemaining(),\n        report[0].getBlockPoolUsed(), xceiverCount, xmitsInProgress,\n        failedVolumes);\n  }",
                "code_after_change": "  public HeartbeatResponse sendHeartbeat(DatanodeRegistration nodeReg,\n      StorageReport[] report, long dnCacheCapacity, long dnCacheUsed,\n      int xmitsInProgress, int xceiverCount,\n      int failedVolumes) throws IOException {\n    checkNNStartup();\n    verifyRequest(nodeReg);\n    return namesystem.handleHeartbeat(nodeReg, report,\n        dnCacheCapacity, dnCacheUsed, xceiverCount, xmitsInProgress,\n        failedVolumes);\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.verifyRequest": {
                "code_before_change": "  void verifyRequest(NodeRegistration nodeReg) throws IOException {\n    verifyLayoutVersion(nodeReg.getVersion());\n    if (!namesystem.getRegistrationID().equals(nodeReg.getRegistrationID())) {\n      LOG.warn(\"Invalid registrationID - expected: \"\n          + namesystem.getRegistrationID() + \" received: \"\n          + nodeReg.getRegistrationID());\n      throw new UnregisteredNodeException(nodeReg);\n    }\n  }",
                "code_after_change": "  private void verifyRequest(NodeRegistration nodeReg) throws IOException {\n    // verify registration ID\n    final String id = nodeReg.getRegistrationID();\n    final String expectedID = namesystem.getRegistrationID();\n    if (!expectedID.equals(id)) {\n      LOG.warn(\"Registration IDs mismatched: the \"\n          + nodeReg.getClass().getSimpleName() + \" ID is \" + id\n          + \" but the expected ID is \" + expectedID);\n       throw new UnregisteredNodeException(nodeReg);\n    }\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.checkPathLength": {
                "code_before_change": "  private boolean checkPathLength(String src) {\n    Path srcPath = new Path(src);\n    return (src.length() <= MAX_PATH_LENGTH &&\n            srcPath.depth() <= MAX_PATH_DEPTH);\n  }",
                "code_after_change": "  private boolean checkPathLength(String src) {\n    Path srcPath = new Path(src);\n    return (src.length() <= MAX_PATH_LENGTH &&\n            srcPath.depth() <= MAX_PATH_DEPTH);\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock": {
                "code_before_change": "  public LocatedBlock addBlock(String src,\n                               String clientName,\n                               ExtendedBlock previous,\n                               DatanodeInfo[] excludedNodes)\n      throws IOException {\n    if(stateChangeLog.isDebugEnabled()) {\n      stateChangeLog.debug(\"*BLOCK* NameNode.addBlock: file \"\n          +src+\" for \"+clientName);\n    }\n    HashMap<Node, Node> excludedNodesSet = null;\n    if (excludedNodes != null) {\n      excludedNodesSet = new HashMap<Node, Node>(excludedNodes.length);\n      for (Node node:excludedNodes) {\n        excludedNodesSet.put(node, node);\n      }\n    }\n    LocatedBlock locatedBlock = \n      namesystem.getAdditionalBlock(src, clientName, previous, excludedNodesSet);\n    if (locatedBlock != null)\n      metrics.incrAddBlockOps();\n    return locatedBlock;\n  }",
                "code_after_change": "  public LocatedBlock addBlock(String src, String clientName,\n      ExtendedBlock previous, DatanodeInfo[] excludedNodes, long fileId,\n      String[] favoredNodes)\n      throws IOException {\n    checkNNStartup();\n    if (stateChangeLog.isDebugEnabled()) {\n      stateChangeLog.debug(\"*BLOCK* NameNode.addBlock: file \" + src\n          + \" fileId=\" + fileId + \" for \" + clientName);\n    }\n    Set<Node> excludedNodesSet = null;\n    if (excludedNodes != null) {\n      excludedNodesSet = new HashSet<Node>(excludedNodes.length);\n      for (Node node : excludedNodes) {\n        excludedNodesSet.add(node);\n      }\n    }\n    List<String> favoredNodesList = (favoredNodes == null) ? null\n        : Arrays.asList(favoredNodes);\n    LocatedBlock locatedBlock = namesystem.getAdditionalBlock(src, fileId,\n        clientName, previous, excludedNodesSet, favoredNodesList);\n    if (locatedBlock != null)\n      metrics.incrAddBlockOps();\n    return locatedBlock;\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addCachePool": {
                "code_before_change": [],
                "code_after_change": "  public void addCachePool(CachePoolInfo info) throws IOException {\n    checkNNStartup();\n    CacheEntry cacheEntry = RetryCache.waitForCompletion(retryCache);\n    if (cacheEntry != null && cacheEntry.isSuccess()) {\n      return; // Return previous response\n    }\n    boolean success = false;\n    try {\n      namesystem.addCachePool(info, cacheEntry != null);\n      success = true;\n    } finally {\n      RetryCache.setState(cacheEntry, success);\n    }\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.blockReport": {
                "code_before_change": "  public DatanodeCommand blockReport(DatanodeRegistration nodeReg,\n      String poolId, StorageBlockReport[] reports) throws IOException {\n    verifyRequest(nodeReg);\n    BlockListAsLongs blist = new BlockListAsLongs(reports[0].getBlocks());\n    if(stateChangeLog.isDebugEnabled()) {\n      stateChangeLog.debug(\"*BLOCK* NameNode.blockReport: \"\n           + \"from \" + nodeReg + \" \" + blist.getNumberOfBlocks()\n           + \" blocks\");\n    }\n\n    namesystem.getBlockManager().processReport(nodeReg, poolId, blist);\n    if (nn.getFSImage().isUpgradeFinalized() && !nn.isStandbyState())\n      return new FinalizeCommand(poolId);\n    return null;\n  }",
                "code_after_change": "  public DatanodeCommand blockReport(DatanodeRegistration nodeReg,\n      String poolId, StorageBlockReport[] reports) throws IOException {\n    checkNNStartup();\n    verifyRequest(nodeReg);\n    if(blockStateChangeLog.isDebugEnabled()) {\n      blockStateChangeLog.debug(\"*BLOCK* NameNode.blockReport: \"\n           + \"from \" + nodeReg + \", reports.length=\" + reports.length);\n    }\n    final BlockManager bm = namesystem.getBlockManager(); \n    boolean noStaleStorages = false;\n    for(StorageBlockReport r : reports) {\n      final BlockListAsLongs blocks = new BlockListAsLongs(r.getBlocks());\n      //\n      // BlockManager.processReport accumulates information of prior calls\n      // for the same node and storage, so the value returned by the last\n      // call of this loop is the final updated value for noStaleStorage.\n      //\n      noStaleStorages = bm.processReport(nodeReg, r.getStorage(), blocks);\n      metrics.incrStorageBlockReportOps();\n    }\n\n    if (nn.getFSImage().isUpgradeFinalized() &&\n        !namesystem.isRollingUpgrade() &&\n        !nn.isStandbyState() &&\n        noStaleStorages) {\n      return new FinalizeCommand(poolId);\n    }\n\n    return null;\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getDatanodeStorageReport": {
                "code_before_change": [],
                "code_after_change": "  public DatanodeStorageReport[] getDatanodeStorageReport(\n      DatanodeReportType type) throws IOException {\n    checkNNStartup();\n    final DatanodeStorageReport[] reports = namesystem.getDatanodeStorageReport(type);\n    return reports;\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addCacheDirective": {
                "code_before_change": [],
                "code_after_change": "  public long addCacheDirective(\n      CacheDirectiveInfo path, EnumSet<CacheFlag> flags) throws IOException {\n    checkNNStartup();\n    CacheEntryWithPayload cacheEntry = RetryCache.waitForCompletion\n      (retryCache, null);\n    if (cacheEntry != null && cacheEntry.isSuccess()) {\n      return (Long) cacheEntry.getPayload();\n    }\n\n    boolean success = false;\n    long ret = 0;\n    try {\n      ret = namesystem.addCacheDirective(path, flags, cacheEntry != null);\n      success = true;\n    } finally {\n      RetryCache.setState(cacheEntry, success, ret);\n    }\n    return ret;\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.endCheckpoint": {
                "code_before_change": "  public void endCheckpoint(NamenodeRegistration registration,\n                            CheckpointSignature sig) throws IOException {\n    namesystem.endCheckpoint(registration, sig);\n  }",
                "code_after_change": "  public void endCheckpoint(NamenodeRegistration registration,\n                            CheckpointSignature sig) throws IOException {\n    checkNNStartup();\n    namesystem.checkSuperuserPrivilege();\n    CacheEntry cacheEntry = RetryCache.waitForCompletion(retryCache);\n    if (cacheEntry != null && cacheEntry.isSuccess()) {\n      return; // Return previous response\n    }\n    boolean success = false;\n    try {\n      namesystem.endCheckpoint(registration, sig);\n      success = true;\n    } finally {\n      RetryCache.setState(cacheEntry, success);\n    }\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.commitBlockSynchronization": {
                "code_before_change": "  public void commitBlockSynchronization(ExtendedBlock block,\n      long newgenerationstamp, long newlength,\n      boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n      String[] newtargetstorages)\n      throws IOException {\n    namesystem.commitBlockSynchronization(block, newgenerationstamp,\n        newlength, closeFile, deleteblock, newtargets, newtargetstorages);\n  }",
                "code_after_change": "  public void commitBlockSynchronization(ExtendedBlock block,\n      long newgenerationstamp, long newlength,\n      boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n      String[] newtargetstorages)\n      throws IOException {\n    checkNNStartup();\n    namesystem.commitBlockSynchronization(block, newgenerationstamp,\n        newlength, closeFile, deleteblock, newtargets, newtargetstorages);\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getAdditionalDatanode": {
                "code_before_change": "  public LocatedBlock getAdditionalDatanode(final String src, final ExtendedBlock blk,\n      final DatanodeInfo[] existings, final DatanodeInfo[] excludes,\n      final int numAdditionalNodes, final String clientName\n      ) throws IOException {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"getAdditionalDatanode: src=\" + src\n          + \", blk=\" + blk\n          + \", existings=\" + Arrays.asList(existings)\n          + \", excludes=\" + Arrays.asList(excludes)\n          + \", numAdditionalNodes=\" + numAdditionalNodes\n          + \", clientName=\" + clientName);\n    }\n\n    metrics.incrGetAdditionalDatanodeOps();\n\n    HashMap<Node, Node> excludeSet = null;\n    if (excludes != null) {\n      excludeSet = new HashMap<Node, Node>(excludes.length);\n      for (Node node : excludes) {\n        excludeSet.put(node, node);\n      }\n    }\n    return namesystem.getAdditionalDatanode(src, blk,\n        existings, excludeSet, numAdditionalNodes, clientName);\n  }",
                "code_after_change": "  public LocatedBlock getAdditionalDatanode(final String src,\n      final long fileId, final ExtendedBlock blk,\n      final DatanodeInfo[] existings, final String[] existingStorageIDs,\n      final DatanodeInfo[] excludes,\n      final int numAdditionalNodes, final String clientName\n      ) throws IOException {\n    checkNNStartup();\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"getAdditionalDatanode: src=\" + src\n          + \", fileId=\" + fileId\n          + \", blk=\" + blk\n          + \", existings=\" + Arrays.asList(existings)\n          + \", excludes=\" + Arrays.asList(excludes)\n          + \", numAdditionalNodes=\" + numAdditionalNodes\n          + \", clientName=\" + clientName);\n    }\n\n    metrics.incrGetAdditionalDatanodeOps();\n\n    Set<Node> excludeSet = null;\n    if (excludes != null) {\n      excludeSet = new HashSet<Node>(excludes.length);\n      for (Node node : excludes) {\n        excludeSet.add(node);\n      }\n    }\n    return namesystem.getAdditionalDatanode(src, fileId, blk, existings,\n        existingStorageIDs, excludeSet, numAdditionalNodes, clientName);\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.blockReceivedAndDeleted": {
                "code_before_change": "  public void blockReceivedAndDeleted(DatanodeRegistration nodeReg, String poolId,\n      StorageReceivedDeletedBlocks[] receivedAndDeletedBlocks) throws IOException {\n    verifyRequest(nodeReg);\n    if(stateChangeLog.isDebugEnabled()) {\n      stateChangeLog.debug(\"*BLOCK* NameNode.blockReceivedAndDeleted: \"\n          +\"from \"+nodeReg+\" \"+receivedAndDeletedBlocks.length\n          +\" blocks.\");\n    }\n    namesystem.getBlockManager().processIncrementalBlockReport(\n        nodeReg, poolId, receivedAndDeletedBlocks[0].getBlocks());\n  }",
                "code_after_change": "  public void blockReceivedAndDeleted(DatanodeRegistration nodeReg, String poolId,\n      StorageReceivedDeletedBlocks[] receivedAndDeletedBlocks) throws IOException {\n    checkNNStartup();\n    verifyRequest(nodeReg);\n    metrics.incrBlockReceivedAndDeletedOps();\n    if(blockStateChangeLog.isDebugEnabled()) {\n      blockStateChangeLog.debug(\"*BLOCK* NameNode.blockReceivedAndDeleted: \"\n          +\"from \"+nodeReg+\" \"+receivedAndDeletedBlocks.length\n          +\" blocks.\");\n    }\n    for(StorageReceivedDeletedBlocks r : receivedAndDeletedBlocks) {\n      namesystem.processIncrementalBlockReport(nodeReg, r);\n    }\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.startCheckpoint": {
                "code_before_change": "  public NamenodeCommand startCheckpoint(NamenodeRegistration registration)\n  throws IOException {\n    verifyRequest(registration);\n    if(!nn.isRole(NamenodeRole.NAMENODE))\n      throw new IOException(\"Only an ACTIVE node can invoke startCheckpoint.\");\n    return namesystem.startCheckpoint(registration, nn.setRegistration());\n  }",
                "code_after_change": "  public NamenodeCommand startCheckpoint(NamenodeRegistration registration)\n      throws IOException {\n    checkNNStartup();\n    namesystem.checkSuperuserPrivilege();\n    verifyRequest(registration);\n    if(!nn.isRole(NamenodeRole.NAMENODE))\n      throw new IOException(\"Only an ACTIVE node can invoke startCheckpoint.\");\n\n    CacheEntryWithPayload cacheEntry = RetryCache.waitForCompletion(retryCache,\n      null);\n    if (cacheEntry != null && cacheEntry.isSuccess()) {\n      return (NamenodeCommand) cacheEntry.getPayload();\n    }\n    NamenodeCommand ret = null;\n    try {\n      ret = namesystem.startCheckpoint(registration, nn.setRegistration());\n    } finally {\n      RetryCache.setState(cacheEntry, ret != null, ret);\n    }\n    return ret;\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.deleteSnapshot": {
                "code_before_change": [],
                "code_after_change": "  public void deleteSnapshot(String snapshotRoot, String snapshotName)\n      throws IOException {\n    checkNNStartup();\n    metrics.incrDeleteSnapshotOps();\n    CacheEntry cacheEntry = RetryCache.waitForCompletion(retryCache);\n    if (cacheEntry != null && cacheEntry.isSuccess()) {\n      return; // Return previous response\n    }\n    boolean success = false;\n    try {\n      namesystem.deleteSnapshot(snapshotRoot, snapshotName, cacheEntry != null);\n      success = true;\n    } finally {\n      RetryCache.setState(cacheEntry, success);\n    }\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.modifyCacheDirective": {
                "code_before_change": [],
                "code_after_change": "  public void modifyCacheDirective(\n      CacheDirectiveInfo directive, EnumSet<CacheFlag> flags) throws IOException {\n    checkNNStartup();\n    CacheEntry cacheEntry = RetryCache.waitForCompletion(retryCache);\n    if (cacheEntry != null && cacheEntry.isSuccess()) {\n      return;\n    }\n\n    boolean success = false;\n    try {\n      namesystem.modifyCacheDirective(directive, flags, cacheEntry != null);\n      success = true;\n    } finally {\n      RetryCache.setState(cacheEntry, success);\n    }\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.createEncryptionZone": {
                "code_before_change": [],
                "code_after_change": "  public void createEncryptionZone(String src, String keyName)\n    throws IOException {\n    checkNNStartup();\n    final CacheEntry cacheEntry = RetryCache.waitForCompletion(retryCache);\n    if (cacheEntry != null && cacheEntry.isSuccess()) {\n      return;\n    }\n    boolean success = false;\n    try {\n      namesystem.createEncryptionZone(src, keyName, cacheEntry != null);\n      success = true;\n    } finally {\n      RetryCache.setState(cacheEntry, success);\n    }\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.isStandbyState": {
                "code_before_change": "  public boolean isStandbyState() {\n    return (state.equals(STANDBY_STATE));\n  }",
                "code_after_change": "  public boolean isStandbyState() {\n    return (state.equals(STANDBY_STATE));\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.removeXAttr": {
                "code_before_change": [],
                "code_after_change": "  public void removeXAttr(String src, XAttr xAttr) throws IOException {\n    checkNNStartup();\n    CacheEntry cacheEntry = RetryCache.waitForCompletion(retryCache);\n    if (cacheEntry != null && cacheEntry.isSuccess()) {\n      return; // Return previous response\n    }\n    boolean success = false;\n    try {\n      namesystem.removeXAttr(src, xAttr, cacheEntry != null);\n      success = true;\n    } finally {\n      RetryCache.setState(cacheEntry, success);\n    }\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockKeys": {
                "code_before_change": "  public ExportedBlockKeys getBlockKeys() throws IOException {\n    return namesystem.getBlockManager().getBlockKeys();\n  }",
                "code_after_change": "  public ExportedBlockKeys getBlockKeys() throws IOException {\n    checkNNStartup();\n    namesystem.checkSuperuserPrivilege();\n    return namesystem.getBlockManager().getBlockKeys();\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlocks": {
                "code_before_change": "  public BlocksWithLocations getBlocks(DatanodeInfo datanode, long size)\n  throws IOException {\n    if(size <= 0) {\n      throw new IllegalArgumentException(\n        \"Unexpected not positive size: \"+size);\n    }\n    namesystem.checkOperation(OperationCategory.READ);\n    return namesystem.getBlockManager().getBlocks(datanode, size); \n  }",
                "code_after_change": "  public BlocksWithLocations getBlocks(DatanodeInfo datanode, long size)\n  throws IOException {\n    if(size <= 0) {\n      throw new IllegalArgumentException(\n        \"Unexpected not positive size: \"+size);\n    }\n    checkNNStartup();\n    namesystem.checkSuperuserPrivilege();\n    return namesystem.getBlockManager().getBlocks(datanode, size); \n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.setQuota": {
                "code_before_change": "  public void setQuota(String path, long namespaceQuota, long diskspaceQuota) \n      throws IOException {\n    namesystem.setQuota(path, namespaceQuota, diskspaceQuota);\n  }",
                "code_after_change": "  public void setQuota(String path, long namespaceQuota, long diskspaceQuota) \n      throws IOException {\n    checkNNStartup();\n    namesystem.setQuota(path, namespaceQuota, diskspaceQuota);\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.modifyCachePool": {
                "code_before_change": [],
                "code_after_change": "  public void modifyCachePool(CachePoolInfo info) throws IOException {\n    checkNNStartup();\n    CacheEntry cacheEntry = RetryCache.waitForCompletion(retryCache);\n    if (cacheEntry != null && cacheEntry.isSuccess()) {\n      return; // Return previous response\n    }\n    boolean success = false;\n    try {\n      namesystem.modifyCachePool(info, cacheEntry != null);\n      success = true;\n    } finally {\n      RetryCache.setState(cacheEntry, success);\n    }\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.createSnapshot": {
                "code_before_change": [],
                "code_after_change": "  public String createSnapshot(String snapshotRoot, String snapshotName)\n      throws IOException {\n    checkNNStartup();\n    if (!checkPathLength(snapshotRoot)) {\n      throw new IOException(\"createSnapshot: Pathname too long.  Limit \"\n          + MAX_PATH_LENGTH + \" characters, \" + MAX_PATH_DEPTH + \" levels.\");\n    }\n    CacheEntryWithPayload cacheEntry = RetryCache.waitForCompletion(retryCache,\n      null);\n    if (cacheEntry != null && cacheEntry.isSuccess()) {\n      return (String) cacheEntry.getPayload();\n    }\n\n    metrics.incrCreateSnapshotOps();\n    String ret = null;\n    try {\n      ret = namesystem.createSnapshot(snapshotRoot, snapshotName,\n          cacheEntry != null);\n    } finally {\n      RetryCache.setState(cacheEntry, ret != null, ret);\n    }\n    return ret;\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.removeCacheDirective": {
                "code_before_change": [],
                "code_after_change": "  public void removeCacheDirective(long id) throws IOException {\n    checkNNStartup();\n    CacheEntry cacheEntry = RetryCache.waitForCompletion(retryCache);\n    if (cacheEntry != null && cacheEntry.isSuccess()) {\n      return;\n    }\n    boolean success = false;\n    try {\n      namesystem.removeCacheDirective(id, cacheEntry != null);\n      success = true;\n    } finally {\n      RetryCache.setState(cacheEntry, success);\n    }\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.NameNode": {
                "code_before_change": [],
                "code_after_change": []
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getGroupsForUser": {
                "code_before_change": "  public String[] getGroupsForUser(String user) throws IOException {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Getting groups for user \" + user);\n    }\n    return UserGroupInformation.createRemoteUser(user).getGroupNames();\n  }",
                "code_after_change": "  public String[] getGroupsForUser(String user) throws IOException {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Getting groups for user \" + user);\n    }\n    return UserGroupInformation.createRemoteUser(user).getGroupNames();\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Correct",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the issue occurring in the method `editLogTailer.catchupDuringFailover()` within the `FSNamesystem.startActiveServices` method, which is part of the stack trace context but not the exact ground truth methods. The fix suggestion to add `checkNNStartup()` in `NameNodeRpcServer` is correct as it matches the developer's fix, which involves adding this check in multiple methods of `NameNodeRpcServer`. The problem location is partially identified as it mentions `NameNodeRpcServer` in the title, which is in the shared stack trace context but not the precise ground truth methods. There is no wrong information in the bug report as all details are relevant to the context of the bug."
        }
    },
    {
        "filename": "HDFS-11479.json",
        "code_diff": {
            "hadoop-common-project.hadoop-nfs.src.main.java.org.apache.hadoop.oncrpc.SimpleUdpServer.run": {
                "code_before_change": "  public void run() {\n    // Configure the client.\n    DatagramChannelFactory f = new NioDatagramChannelFactory(\n        Executors.newCachedThreadPool(), workerCount);\n\n    server = new ConnectionlessBootstrap(f);\n    server.setPipeline(Channels.pipeline(RpcUtil.STAGE_RPC_MESSAGE_PARSER,\n        rpcProgram, RpcUtil.STAGE_RPC_UDP_RESPONSE));\n\n    server.setOption(\"broadcast\", \"false\");\n    server.setOption(\"sendBufferSize\", SEND_BUFFER_SIZE);\n    server.setOption(\"receiveBufferSize\", RECEIVE_BUFFER_SIZE);\n\n    // Listen to the UDP port\n    ch = server.bind(new InetSocketAddress(port));\n    InetSocketAddress socketAddr = (InetSocketAddress) ch.getLocalAddress();\n    boundPort = socketAddr.getPort();\n\n    LOG.info(\"Started listening to UDP requests at port \" + boundPort + \" for \"\n        + rpcProgram + \" with workerCount \" + workerCount);\n  }",
                "code_after_change": "  public void run() {\n    // Configure the client.\n    DatagramChannelFactory f = new NioDatagramChannelFactory(\n        Executors.newCachedThreadPool(), workerCount);\n\n    server = new ConnectionlessBootstrap(f);\n    server.setPipeline(Channels.pipeline(RpcUtil.STAGE_RPC_MESSAGE_PARSER,\n        rpcProgram, RpcUtil.STAGE_RPC_UDP_RESPONSE));\n\n    server.setOption(\"broadcast\", \"false\");\n    server.setOption(\"sendBufferSize\", SEND_BUFFER_SIZE);\n    server.setOption(\"receiveBufferSize\", RECEIVE_BUFFER_SIZE);\n    server.setOption(\"reuseAddress\", true);\n\n    // Listen to the UDP port\n    ch = server.bind(new InetSocketAddress(port));\n    InetSocketAddress socketAddr = (InetSocketAddress) ch.getLocalAddress();\n    boundPort = socketAddr.getPort();\n\n    LOG.info(\"Started listening to UDP requests at port \" + boundPort + \" for \"\n        + rpcProgram + \" with workerCount \" + workerCount);\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Correct",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report precisely identifies the root cause by mentioning the need for the 're-use address' option in the SimpleUdpServer, which is directly related to the ground truth method 'SimpleUdpServer.run'. The fix suggestion is correct as it matches the developer's fix, which involves setting the 'reuseAddress' option to true. The problem location is also precise as the report directly mentions 'SimpleUdpServer', which contains the ground truth method. There is no wrong information in the bug report as all details are relevant and accurate."
        }
    },
    {
        "filename": "HDFS-8055.json",
        "code_diff": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager.sortLocatedBlocks": {
                "code_before_change": "  public void sortLocatedBlocks(final String targethost,\n      final List<LocatedBlock> locatedblocks) {\n    //sort the blocks\n    // As it is possible for the separation of node manager and datanode, \n    // here we should get node but not datanode only .\n    Node client = getDatanodeByHost(targethost);\n    if (client == null) {\n      List<String> hosts = new ArrayList<String> (1);\n      hosts.add(targethost);\n      String rName = dnsToSwitchMapping.resolve(hosts).get(0);\n      if (rName != null)\n        client = new NodeBase(rName + NodeBase.PATH_SEPARATOR_STR + targethost);\n    }\n    \n    Comparator<DatanodeInfo> comparator = avoidStaleDataNodesForRead ?\n        new DFSUtil.DecomStaleComparator(staleInterval) : \n        DFSUtil.DECOM_COMPARATOR;\n        \n    for (LocatedBlock b : locatedblocks) {\n      DatanodeInfo[] di = b.getLocations();\n      // Move decommissioned/stale datanodes to the bottom\n      Arrays.sort(di, comparator);\n      \n      int lastActiveIndex = di.length - 1;\n      while (lastActiveIndex > 0 && isInactive(di[lastActiveIndex])) {\n          --lastActiveIndex;\n      }\n      int activeLen = lastActiveIndex + 1;      \n      networktopology.sortByDistance(client, b.getLocations(), activeLen);\n      // must update cache since we modified locations array\n      b.updateCachedStorageInfo();\n    }\n  }",
                "code_after_change": "  public void sortLocatedBlocks(final String targethost,\n      final List<LocatedBlock> locatedblocks) {\n    //sort the blocks\n    // As it is possible for the separation of node manager and datanode, \n    // here we should get node but not datanode only .\n    Node client = getDatanodeByHost(targethost);\n    if (client == null) {\n      List<String> hosts = new ArrayList<String> (1);\n      hosts.add(targethost);\n      List<String> resolvedHosts = dnsToSwitchMapping.resolve(hosts);\n      if (resolvedHosts != null && !resolvedHosts.isEmpty()) {\n        String rName = resolvedHosts.get(0);\n        if (rName != null) {\n          client = new NodeBase(rName + NodeBase.PATH_SEPARATOR_STR +\n            targethost);\n        }\n      } else {\n        LOG.error(\"Node Resolution failed. Please make sure that rack \" +\n          \"awareness scripts are functional.\");\n      }\n    }\n    \n    Comparator<DatanodeInfo> comparator = avoidStaleDataNodesForRead ?\n        new DFSUtil.DecomStaleComparator(staleInterval) : \n        DFSUtil.DECOM_COMPARATOR;\n        \n    for (LocatedBlock b : locatedblocks) {\n      DatanodeInfo[] di = b.getLocations();\n      // Move decommissioned/stale datanodes to the bottom\n      Arrays.sort(di, comparator);\n      \n      int lastActiveIndex = di.length - 1;\n      while (lastActiveIndex > 0 && isInactive(di[lastActiveIndex])) {\n          --lastActiveIndex;\n      }\n      int activeLen = lastActiveIndex + 1;      \n      networktopology.sortByDistance(client, b.getLocations(), activeLen);\n      // must update cache since we modified locations array\n      b.updateCachedStorageInfo();\n    }\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report precisely identifies the root cause by mentioning the NullPointerException occurring in the method 'DatanodeManager.sortLocatedBlocks', which is the ground truth method. However, the report does not provide any fix suggestion, hence it is marked as 'Missing' for fix suggestion. The problem location is also precisely identified as it directly mentions the method where the issue occurs. There is no wrong information in the bug report as all the details are consistent with the context of the bug."
        }
    },
    {
        "filename": "HDFS-10609.json",
        "code_diff": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager.clearAllKeysForTesting": {
                "code_before_change": "  public void clearAllKeysForTesting() {\n    allKeys.clear();\n  }",
                "code_after_change": "  public void clearAllKeysForTesting() {\n    allKeys.clear();\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DFSOutputStream.addDatanode2ExistingPipeline": {
                "code_before_change": [],
                "code_after_change": "    private void addDatanode2ExistingPipeline() throws IOException {\n      if (DataTransferProtocol.LOG.isDebugEnabled()) {\n        DataTransferProtocol.LOG.debug(\"lastAckedSeqno = \" + lastAckedSeqno);\n      }\n      /*\n       * Is data transfer necessary?  We have the following cases.\n       * \n       * Case 1: Failure in Pipeline Setup\n       * - Append\n       *    + Transfer the stored replica, which may be a RBW or a finalized.\n       * - Create\n       *    + If no data, then no transfer is required.\n       *    + If there are data written, transfer RBW. This case may happens \n       *      when there are streaming failure earlier in this pipeline.\n       *\n       * Case 2: Failure in Streaming\n       * - Append/Create:\n       *    + transfer RBW\n       * \n       * Case 3: Failure in Close\n       * - Append/Create:\n       *    + no transfer, let NameNode replicates the block.\n       */\n      if (!isAppend && lastAckedSeqno < 0\n          && stage == BlockConstructionStage.PIPELINE_SETUP_CREATE) {\n        //no data have been written\n        return;\n      } else if (stage == BlockConstructionStage.PIPELINE_CLOSE\n          || stage == BlockConstructionStage.PIPELINE_CLOSE_RECOVERY) {\n        //pipeline is closing\n        return;\n      }\n\n      int tried = 0;\n      final DatanodeInfo[] original = nodes;\n      final StorageType[] originalTypes = storageTypes;\n      final String[] originalIDs = storageIDs;\n      IOException caughtException = null;\n      ArrayList<DatanodeInfo> exclude = new ArrayList<DatanodeInfo>(failed);\n      while (tried < 3) {\n        LocatedBlock lb;\n        //get a new datanode\n        lb = dfsClient.namenode.getAdditionalDatanode(\n            src, fileId, block, nodes, storageIDs,\n            exclude.toArray(new DatanodeInfo[exclude.size()]),\n            1, dfsClient.clientName);\n        // a new node was allocated by the namenode. Update nodes.\n        setPipeline(lb);\n\n        //find the new datanode\n        final int d = findNewDatanode(original);\n        //transfer replica. pick a source from the original nodes\n        final DatanodeInfo src = original[tried % original.length];\n        final DatanodeInfo[] targets = {nodes[d]};\n        final StorageType[] targetStorageTypes = {storageTypes[d]};\n\n        try {\n          transfer(src, targets, targetStorageTypes, lb.getBlockToken());\n        } catch (IOException ioe) {\n          DFSClient.LOG.warn(\"Error transferring data from \" + src + \" to \" +\n              nodes[d] + \": \" + ioe.getMessage());\n          caughtException = ioe;\n          // add the allocated node to the exclude list.\n          exclude.add(nodes[d]);\n          setPipeline(original, originalTypes, originalIDs);\n          tried++;\n          continue;\n        }\n        return; // finished successfully\n      }\n      // All retries failed\n      throw (caughtException != null) ? caughtException :\n         new IOException(\"Failed to add a node\");\n    }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataNode.getBlockScanner": {
                "code_before_change": "  public BlockScanner getBlockScanner() {\n    return blockScanner;\n  }",
                "code_after_change": "  public BlockScanner getBlockScanner() {\n    return blockScanner;\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DFSClient.newDataEncryptionKey": {
                "code_before_change": [],
                "code_after_change": "  public DataEncryptionKey newDataEncryptionKey() throws IOException {\n    if (shouldEncryptData()) {\n      synchronized (this) {\n        if (encryptionKey == null ||\n            encryptionKey.expiryDate < Time.now()) {\n          LOG.debug(\"Getting new encryption token from NN\");\n          encryptionKey = namenode.getDataEncryptionKey();\n        }\n        return encryptionKey;\n      }\n    } else {\n      return null;\n    }\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.security.token.block.BlockPoolTokenSecretManager.addBlockPool": {
                "code_before_change": "  public synchronized void addBlockPool(String bpid,\n      BlockTokenSecretManager secretMgr) {\n    map.put(bpid, secretMgr);\n  }",
                "code_after_change": "  public synchronized void addBlockPool(String bpid,\n      BlockTokenSecretManager secretMgr) {\n    map.put(bpid, secretMgr);\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DFSOutputStream.createHeartbeatPacket": {
                "code_before_change": [],
                "code_after_change": "  private DFSPacket createHeartbeatPacket() throws InterruptedIOException {\n    final byte[] buf = new byte[PacketHeader.PKT_MAX_HEADER_LEN];\n    return new DFSPacket(buf, 0, 0, DFSPacket.HEART_BEAT_SEQNO,\n                         getChecksumSize(), false);\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the issue with the handling of InvalidEncryptionKeyException during pipeline recovery, which is related to the stack trace context of the ground truth methods, particularly DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline. However, it does not precisely identify the root cause at the ground truth methods. The report lacks a specific fix suggestion, as it only mentions that the exception should be handled similarly to createBlockOutputStream() but does not provide a concrete solution. The problem location is partially identified as it mentions methods in the stack trace context but not the exact ground truth methods. There is no wrong information in the report as it accurately describes the issue and its impact."
        }
    },
    {
        "filename": "HDFS-2310.json",
        "code_diff": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNode.getProtocolSignature": {
                "code_before_change": "  public ProtocolSignature getProtocolSignature(String protocol,\n      long clientVersion, int clientMethodsHash) throws IOException {\n    return ProtocolSignature.getProtocolSignature(\n        this, protocol, clientVersion, clientMethodsHash);\n  }",
                "code_after_change": "  public ProtocolSignature getProtocolSignature(String protocol,\n      long clientVersion, int clientMethodsHash) throws IOException {\n    return ProtocolSignature.getProtocolSignature(\n        this, protocol, clientVersion, clientMethodsHash);\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Missing",
                "sub_category": ""
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the issue as related to the 'JournalProtocol' not being registered, which is mentioned in the stack trace. However, it does not precisely identify the root cause in the ground truth method 'NameNode.getProtocolSignature'. The report lacks any fix suggestion, as there is no 'Suggestions' or 'possible_fix' field, nor is there any suggestion in the 'Description'. The problem location is not precisely identified, as the report does not mention the ground truth method or any related methods outside of the stack trace context. There is no wrong information in the report; it accurately describes the error observed in the logs."
        }
    },
    {
        "filename": "HDFS-3385.json",
        "code_diff": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.recoverLeaseInternal": {
                "code_before_change": "  private void recoverLeaseInternal(INode fileInode, \n      String src, String holder, String clientMachine, boolean force)\n      throws IOException {\n    assert hasWriteLock();\n    if (fileInode != null && fileInode.isUnderConstruction()) {\n      INodeFileUnderConstruction pendingFile = (INodeFileUnderConstruction) fileInode;\n      //\n      // If the file is under construction , then it must be in our\n      // leases. Find the appropriate lease record.\n      //\n      Lease lease = leaseManager.getLease(holder);\n      //\n      // We found the lease for this file. And surprisingly the original\n      // holder is trying to recreate this file. This should never occur.\n      //\n      if (!force && lease != null) {\n        Lease leaseFile = leaseManager.getLeaseByPath(src);\n        if ((leaseFile != null && leaseFile.equals(lease)) ||\n            lease.getHolder().equals(holder)) { \n          throw new AlreadyBeingCreatedException(\n            \"failed to create file \" + src + \" for \" + holder +\n            \" on client \" + clientMachine + \n            \" because current leaseholder is trying to recreate file.\");\n        }\n      }\n      //\n      // Find the original holder.\n      //\n      lease = leaseManager.getLease(pendingFile.getClientName());\n      if (lease == null) {\n        throw new AlreadyBeingCreatedException(\n          \"failed to create file \" + src + \" for \" + holder +\n          \" on client \" + clientMachine + \n          \" because pendingCreates is non-null but no leases found.\");\n      }\n      if (force) {\n        // close now: no need to wait for soft lease expiration and \n        // close only the file src\n        LOG.info(\"recoverLease: recover lease \" + lease + \", src=\" + src +\n          \" from client \" + pendingFile.getClientName());\n        internalReleaseLease(lease, src, holder);\n      } else {\n        assert lease.getHolder().equals(pendingFile.getClientName()) :\n          \"Current lease holder \" + lease.getHolder() +\n          \" does not match file creator \" + pendingFile.getClientName();\n        //\n        // If the original holder has not renewed in the last SOFTLIMIT \n        // period, then start lease recovery.\n        //\n        if (lease.expiredSoftLimit()) {\n          LOG.info(\"startFile: recover lease \" + lease + \", src=\" + src +\n              \" from client \" + pendingFile.getClientName());\n          boolean isClosed = internalReleaseLease(lease, src, null);\n          if(!isClosed)\n            throw new RecoveryInProgressException(\n                \"Failed to close file \" + src +\n                \". Lease recovery is in progress. Try again later.\");\n        } else {\n          BlockInfoUnderConstruction lastBlock=pendingFile.getLastBlock();\n          if(lastBlock != null && lastBlock.getBlockUCState() ==\n            BlockUCState.UNDER_RECOVERY) {\n            throw new RecoveryInProgressException(\n              \"Recovery in progress, file [\" + src + \"], \" +\n              \"lease owner [\" + lease.getHolder() + \"]\");\n            } else {\n              throw new AlreadyBeingCreatedException(\n                \"Failed to create file [\" + src + \"] for [\" + holder +\n                \"] on client [\" + clientMachine +\n                \"], because this file is already being created by [\" +\n                pendingFile.getClientName() + \"] on [\" +\n                pendingFile.getClientMachine() + \"]\");\n            }\n         }\n      }\n    }\n\n  }",
                "code_after_change": "  private void recoverLeaseInternal(INode fileInode, \n      String src, String holder, String clientMachine, boolean force)\n      throws IOException {\n    assert hasWriteLock();\n    if (fileInode != null && fileInode.isUnderConstruction()) {\n      INodeFileUnderConstruction pendingFile = (INodeFileUnderConstruction) fileInode;\n      //\n      // If the file is under construction , then it must be in our\n      // leases. Find the appropriate lease record.\n      //\n      Lease lease = leaseManager.getLease(holder);\n      //\n      // We found the lease for this file. And surprisingly the original\n      // holder is trying to recreate this file. This should never occur.\n      //\n      if (!force && lease != null) {\n        Lease leaseFile = leaseManager.getLeaseByPath(src);\n        if ((leaseFile != null && leaseFile.equals(lease)) ||\n            lease.getHolder().equals(holder)) { \n          throw new AlreadyBeingCreatedException(\n            \"failed to create file \" + src + \" for \" + holder +\n            \" on client \" + clientMachine + \n            \" because current leaseholder is trying to recreate file.\");\n        }\n      }\n      //\n      // Find the original holder.\n      //\n      lease = leaseManager.getLease(pendingFile.getClientName());\n      if (lease == null) {\n        throw new AlreadyBeingCreatedException(\n          \"failed to create file \" + src + \" for \" + holder +\n          \" on client \" + clientMachine + \n          \" because pendingCreates is non-null but no leases found.\");\n      }\n      if (force) {\n        // close now: no need to wait for soft lease expiration and \n        // close only the file src\n        LOG.info(\"recoverLease: recover lease \" + lease + \", src=\" + src +\n          \" from client \" + pendingFile.getClientName());\n        internalReleaseLease(lease, src, holder);\n      } else {\n        assert lease.getHolder().equals(pendingFile.getClientName()) :\n          \"Current lease holder \" + lease.getHolder() +\n          \" does not match file creator \" + pendingFile.getClientName();\n        //\n        // If the original holder has not renewed in the last SOFTLIMIT \n        // period, then start lease recovery.\n        //\n        if (lease.expiredSoftLimit()) {\n          LOG.info(\"startFile: recover lease \" + lease + \", src=\" + src +\n              \" from client \" + pendingFile.getClientName());\n          boolean isClosed = internalReleaseLease(lease, src, null);\n          if(!isClosed)\n            throw new RecoveryInProgressException(\n                \"Failed to close file \" + src +\n                \". Lease recovery is in progress. Try again later.\");\n        } else {\n          final BlockInfo lastBlock = pendingFile.getLastBlock();\n          if (lastBlock != null\n              && lastBlock.getBlockUCState() == BlockUCState.UNDER_RECOVERY) {\n            throw new RecoveryInProgressException(\"Recovery in progress, file [\"\n                + src + \"], \" + \"lease owner [\" + lease.getHolder() + \"]\");\n          } else {\n            throw new AlreadyBeingCreatedException(\"Failed to create file [\"\n                + src + \"] for [\" + holder + \"] on client [\" + clientMachine\n                + \"], because this file is already being created by [\"\n                + pendingFile.getClientName() + \"] on [\"\n                + pendingFile.getClientMachine() + \"]\");\n          }\n        }\n      }\n    }\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the issue as a ClassCastException occurring during a file append operation, which is related to the method 'FSNamesystem.recoverLeaseInternal' as seen in the stack trace. However, it does not precisely identify the root cause within the method itself, hence it is classified as 'Partial' with 'Shared Stack Trace Context'. There is no fix suggestion provided in the bug report, so it is marked as 'Missing'. The problem location is also identified as 'Partial' with 'Shared Stack Trace Context' because the stack trace includes the ground truth method, but the report does not explicitly mention it as the problem location. There is no incorrect information in the bug report, so 'Wrong Information' is marked as 'No'."
        }
    },
    {
        "filename": "HDFS-4006.json",
        "code_diff": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.initialize": {
                "code_before_change": "  private void initialize(final Configuration conf,\n      CommandLineOpts commandLineOpts) throws IOException {\n    final InetSocketAddress infoSocAddr = getHttpAddress(conf);\n    infoBindAddress = infoSocAddr.getHostName();\n    UserGroupInformation.setConfiguration(conf);\n    if (UserGroupInformation.isSecurityEnabled()) {\n      SecurityUtil.login(conf, DFS_SECONDARY_NAMENODE_KEYTAB_FILE_KEY,\n          DFS_SECONDARY_NAMENODE_USER_NAME_KEY, infoBindAddress);\n    }\n    // initiate Java VM metrics\n    DefaultMetricsSystem.initialize(\"SecondaryNameNode\");\n    JvmMetrics.create(\"SecondaryNameNode\",\n        conf.get(DFS_METRICS_SESSION_ID_KEY), DefaultMetricsSystem.instance());\n    \n    // Create connection to the namenode.\n    shouldRun = true;\n    nameNodeAddr = NameNode.getServiceAddress(conf, true);\n\n    this.conf = conf;\n    this.namenode = NameNodeProxies.createNonHAProxy(conf, nameNodeAddr, \n        NamenodeProtocol.class, UserGroupInformation.getCurrentUser(),\n        true).getProxy();\n\n    // initialize checkpoint directories\n    fsName = getInfoServer();\n    checkpointDirs = FSImage.getCheckpointDirs(conf,\n                                  \"/tmp/hadoop/dfs/namesecondary\");\n    checkpointEditsDirs = FSImage.getCheckpointEditsDirs(conf, \n                                  \"/tmp/hadoop/dfs/namesecondary\");    \n    checkpointImage = new CheckpointStorage(conf, checkpointDirs, checkpointEditsDirs);\n    checkpointImage.recoverCreate(commandLineOpts.shouldFormat());\n    \n    namesystem = new FSNamesystem(conf, checkpointImage);\n\n    // Initialize other scheduling parameters from the configuration\n    checkpointConf = new CheckpointConf(conf);\n\n    // initialize the webserver for uploading files.\n    int tmpInfoPort = infoSocAddr.getPort();\n    infoServer = new HttpServer(\"secondary\", infoBindAddress, tmpInfoPort,\n                                tmpInfoPort == 0, conf,\n                                new AccessControlList(conf.get(DFS_ADMIN, \" \"))) {\n      {\n        if (UserGroupInformation.isSecurityEnabled()) {\n          initSpnego(conf, DFSConfigKeys.DFS_SECONDARY_NAMENODE_INTERNAL_SPNEGO_USER_NAME_KEY,\n              DFSConfigKeys.DFS_SECONDARY_NAMENODE_KEYTAB_FILE_KEY);\n        }\n      }\n    };\n    infoServer.setAttribute(\"secondary.name.node\", this);\n    infoServer.setAttribute(\"name.system.image\", checkpointImage);\n    infoServer.setAttribute(JspHelper.CURRENT_CONF, conf);\n    infoServer.addInternalServlet(\"getimage\", \"/getimage\",\n                                  GetImageServlet.class, true);\n    infoServer.start();\n\n    LOG.info(\"Web server init done\");\n\n    // The web-server port can be ephemeral... ensure we have the correct info\n    infoPort = infoServer.getPort();\n\n    conf.set(DFS_NAMENODE_SECONDARY_HTTP_ADDRESS_KEY, infoBindAddress + \":\" + infoPort);\n    LOG.info(\"Secondary Web-server up at: \" + infoBindAddress + \":\" + infoPort);\n    LOG.info(\"Checkpoint Period   :\" + checkpointConf.getPeriod() + \" secs \" +\n             \"(\" + checkpointConf.getPeriod() / 60 + \" min)\");\n    LOG.info(\"Log Size Trigger    :\" + checkpointConf.getTxnCount() + \" txns\");\n  }",
                "code_after_change": "  private void initialize(final Configuration conf,\n      CommandLineOpts commandLineOpts) throws IOException {\n    final InetSocketAddress infoSocAddr = getHttpAddress(conf);\n    infoBindAddress = infoSocAddr.getHostName();\n    UserGroupInformation.setConfiguration(conf);\n    if (UserGroupInformation.isSecurityEnabled()) {\n      SecurityUtil.login(conf, DFS_SECONDARY_NAMENODE_KEYTAB_FILE_KEY,\n          DFS_SECONDARY_NAMENODE_USER_NAME_KEY, infoBindAddress);\n    }\n    // initiate Java VM metrics\n    DefaultMetricsSystem.initialize(\"SecondaryNameNode\");\n    JvmMetrics.create(\"SecondaryNameNode\",\n        conf.get(DFS_METRICS_SESSION_ID_KEY), DefaultMetricsSystem.instance());\n    \n    // Create connection to the namenode.\n    shouldRun = true;\n    nameNodeAddr = NameNode.getServiceAddress(conf, true);\n\n    this.conf = conf;\n    this.namenode = NameNodeProxies.createNonHAProxy(conf, nameNodeAddr, \n        NamenodeProtocol.class, UserGroupInformation.getCurrentUser(),\n        true).getProxy();\n\n    // initialize checkpoint directories\n    fsName = getInfoServer();\n    checkpointDirs = FSImage.getCheckpointDirs(conf,\n                                  \"/tmp/hadoop/dfs/namesecondary\");\n    checkpointEditsDirs = FSImage.getCheckpointEditsDirs(conf, \n                                  \"/tmp/hadoop/dfs/namesecondary\");    \n    checkpointImage = new CheckpointStorage(conf, checkpointDirs, checkpointEditsDirs);\n    checkpointImage.recoverCreate(commandLineOpts.shouldFormat());\n    \n    namesystem = new FSNamesystem(conf, checkpointImage);\n\n    // Initialize other scheduling parameters from the configuration\n    checkpointConf = new CheckpointConf(conf);\n\n    // initialize the webserver for uploading files.\n    int tmpInfoPort = infoSocAddr.getPort();\n    infoServer = new HttpServer(\"secondary\", infoBindAddress, tmpInfoPort,\n                                tmpInfoPort == 0, conf,\n                                new AccessControlList(conf.get(DFS_ADMIN, \" \"))) {\n      {\n        if (UserGroupInformation.isSecurityEnabled()) {\n          initSpnego(conf, DFSConfigKeys.DFS_SECONDARY_NAMENODE_INTERNAL_SPNEGO_USER_NAME_KEY,\n              DFSConfigKeys.DFS_SECONDARY_NAMENODE_KEYTAB_FILE_KEY);\n        }\n      }\n    };\n    infoServer.setAttribute(\"secondary.name.node\", this);\n    infoServer.setAttribute(\"name.system.image\", checkpointImage);\n    infoServer.setAttribute(JspHelper.CURRENT_CONF, conf);\n    infoServer.addInternalServlet(\"getimage\", \"/getimage\",\n                                  GetImageServlet.class, true);\n    infoServer.start();\n\n    LOG.info(\"Web server init done\");\n\n    // The web-server port can be ephemeral... ensure we have the correct info\n    infoPort = infoServer.getPort();\n\n    conf.set(DFS_NAMENODE_SECONDARY_HTTP_ADDRESS_KEY, infoBindAddress + \":\" + infoPort);\n    LOG.info(\"Secondary Web-server up at: \" + infoBindAddress + \":\" + infoPort);\n    LOG.info(\"Checkpoint Period   :\" + checkpointConf.getPeriod() + \" secs \" +\n             \"(\" + checkpointConf.getPeriod() / 60 + \" min)\");\n    LOG.info(\"Log Size Trigger    :\" + checkpointConf.getTxnCount() + \" txns\");\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.main": {
                "code_before_change": "  public static void main(String[] argv) throws Exception {\n    CommandLineOpts opts = SecondaryNameNode.parseArgs(argv);\n    if (opts == null) {\n      LOG.fatal(\"Failed to parse options\");\n      terminate(1);\n    } else if (opts.shouldPrintHelp()) {\n      opts.usage();\n      System.exit(0);\n    }\n    \n    StringUtils.startupShutdownMessage(SecondaryNameNode.class, argv, LOG);\n    Configuration tconf = new HdfsConfiguration();\n    SecondaryNameNode secondary = null;\n    try {\n      secondary = new SecondaryNameNode(tconf, opts);\n    } catch (IOException ioe) {\n      LOG.fatal(\"Failed to start secondary namenode\", ioe);\n      terminate(1);\n    }\n\n    if (opts != null && opts.getCommand() != null) {\n      int ret = secondary.processStartupCommand(opts);\n      terminate(ret);\n    }\n\n    // Create a never ending deamon\n    Daemon checkpointThread = new Daemon(secondary);\n    checkpointThread.start();\n  }",
                "code_after_change": "  public static void main(String[] argv) throws Exception {\n    CommandLineOpts opts = SecondaryNameNode.parseArgs(argv);\n    if (opts == null) {\n      LOG.fatal(\"Failed to parse options\");\n      terminate(1);\n    } else if (opts.shouldPrintHelp()) {\n      opts.usage();\n      System.exit(0);\n    }\n    \n    StringUtils.startupShutdownMessage(SecondaryNameNode.class, argv, LOG);\n    Configuration tconf = new HdfsConfiguration();\n    SecondaryNameNode secondary = null;\n    try {\n      secondary = new SecondaryNameNode(tconf, opts);\n    } catch (IOException ioe) {\n      LOG.fatal(\"Failed to start secondary namenode\", ioe);\n      terminate(1);\n    }\n\n    if (opts != null && opts.getCommand() != null) {\n      int ret = secondary.processStartupCommand(opts);\n      terminate(ret);\n    }\n\n    secondary.startCheckpointThread();\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Buggy Method"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Buggy Method"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the method 'doCheckpoint' in the stack trace, which is where the error (NPE) occurred, but not where the actual fix was made. Therefore, it is classified as 'Partial' under 'Buggy Method' for both root cause and problem location identification. There is no fix suggestion provided in the bug report, so it is marked as 'Missing' for fix suggestion. All information in the bug report is relevant to the context of the bug, so there is no wrong information."
        }
    },
    {
        "filename": "HDFS-2392.json",
        "code_diff": {
            "src.hdfs.org.apache.hadoop.hdfs.HftpFileSystem.initialize": {
                "code_before_change": [],
                "code_after_change": "  public void initialize(final URI name, final Configuration conf) \n  throws IOException {\n    setConf(conf);\n    super.initialize(name, conf);\n    this.ugi = UserGroupInformation.getCurrentUser();\n    this.nnAddr = getNamenodeAddr(name);\n    this.nnSecureAddr = getNamenodeSecureAddr(name);\n    this.hftpURI = createUri(name.getScheme(), nnAddr);\n    \n    if (UserGroupInformation.isSecurityEnabled()) {\n      initDelegationToken();\n    }\n  }"
            },
            "src.hdfs.org.apache.hadoop.hdfs.HftpFileSystem.createUri": {
                "code_before_change": [],
                "code_after_change": "  private static URI createUri(String scheme, InetSocketAddress addr) {\n    URI uri = null;\n    try {\n      uri = new URI(scheme, null, addr.getHostName(), addr.getPort(), null, null, null);\n    } catch (URISyntaxException ue) {\n      throw new IllegalArgumentException(ue);\n    }\n    return uri;\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report does not precisely identify the root cause related to the ground truth methods. However, it mentions methods in the stack trace that are related to the problem, such as 'org.apache.hadoop.tools.DistCp$CopyFilesMapper.close', which is in the same stack trace context as the ground truth methods. There is no fix suggestion provided in the bug report, as there is no 'Suggestions' or 'possible_fix' field, nor is there any suggestion in the 'Description'. The problem location is partially identified as it mentions methods in the stack trace context but not the exact ground truth methods. There is no wrong information in the bug report; all information is relevant to the context of the bug."
        }
    },
    {
        "filename": "HDFS-11472.json",
        "code_diff": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.recoverRbw": {
                "code_before_change": "  public ReplicaHandler recoverRbw(\n      ExtendedBlock b, long newGS, long minBytesRcvd, long maxBytesRcvd)\n      throws IOException {\n    LOG.info(\"Recover RBW replica \" + b);\n\n    while (true) {\n      try {\n        try (AutoCloseableLock lock = datasetLock.acquire()) {\n          ReplicaInfo replicaInfo =\n              getReplicaInfo(b.getBlockPoolId(), b.getBlockId());\n          // check the replica's state\n          if (replicaInfo.getState() != ReplicaState.RBW) {\n            throw new ReplicaNotFoundException(\n                ReplicaNotFoundException.NON_RBW_REPLICA + replicaInfo);\n          }\n          ReplicaInPipeline rbw = (ReplicaInPipeline)replicaInfo;\n          if (!rbw.attemptToSetWriter(null, Thread.currentThread())) {\n            throw new MustStopExistingWriter(rbw);\n          }\n          LOG.info(\"At \" + datanode.getDisplayName() + \", Recovering \" + rbw);\n          return recoverRbwImpl(rbw, b, newGS, minBytesRcvd, maxBytesRcvd);\n        }\n      } catch (MustStopExistingWriter e) {\n        e.getReplicaInPipeline().stopWriter(\n            datanode.getDnConf().getXceiverStopTimeout());\n      }\n    }\n  }",
                "code_after_change": "  public synchronized ReplicaHandler recoverRbw(\n      ExtendedBlock b, long newGS, long minBytesRcvd, long maxBytesRcvd)\n      throws IOException {\n    LOG.info(\"Recover RBW replica \" + b);\n\n    ReplicaInfo replicaInfo = getReplicaInfo(b.getBlockPoolId(), b.getBlockId());\n    \n    // check the replica's state\n    if (replicaInfo.getState() != ReplicaState.RBW) {\n      throw new ReplicaNotFoundException(\n          ReplicaNotFoundException.NON_RBW_REPLICA + replicaInfo);\n    }\n    ReplicaBeingWritten rbw = (ReplicaBeingWritten)replicaInfo;\n    \n    LOG.info(\"Recovering \" + rbw);\n\n    // Stop the previous writer\n    rbw.stopWriter(datanode.getDnConf().getXceiverStopTimeout());\n    rbw.setWriter(Thread.currentThread());\n\n    // check generation stamp\n    long replicaGenerationStamp = rbw.getGenerationStamp();\n    if (replicaGenerationStamp < b.getGenerationStamp() ||\n        replicaGenerationStamp > newGS) {\n      throw new ReplicaNotFoundException(\n          ReplicaNotFoundException.UNEXPECTED_GS_REPLICA + b +\n          \". Expected GS range is [\" + b.getGenerationStamp() + \", \" + \n          newGS + \"].\");\n    }\n    \n    // check replica length\n    long bytesAcked = rbw.getBytesAcked();\n    long numBytes = rbw.getNumBytes();\n    if (bytesAcked < minBytesRcvd || numBytes > maxBytesRcvd){\n      throw new ReplicaNotFoundException(\"Unmatched length replica \" + \n          replicaInfo + \": BytesAcked = \" + bytesAcked + \n          \" BytesRcvd = \" + numBytes + \" are not in the range of [\" + \n          minBytesRcvd + \", \" + maxBytesRcvd + \"].\");\n    }\n\n    long bytesOnDisk = rbw.getBytesOnDisk();\n    long blockDataLength = rbw.getBlockFile().length();\n    if (bytesOnDisk != blockDataLength) {\n      LOG.info(\"Resetting bytesOnDisk to match blockDataLength (=\" +\n          blockDataLength + \") for replica \" + rbw);\n      bytesOnDisk = blockDataLength;\n      rbw.setLastChecksumAndDataLen(bytesOnDisk, null);\n    }\n\n    if (bytesOnDisk < bytesAcked) {\n      throw new ReplicaNotFoundException(\"Found fewer bytesOnDisk than \" +\n          \"bytesAcked for replica \" + rbw);\n    }\n\n    FsVolumeReference ref = rbw.getVolume().obtainReference();\n    try {\n      // Truncate the potentially corrupt portion.\n      // If the source was client and the last node in the pipeline was lost,\n      // any corrupt data written after the acked length can go unnoticed.\n      if (bytesOnDisk > bytesAcked) {\n        final File replicafile = rbw.getBlockFile();\n        truncateBlock(replicafile, rbw.getMetaFile(), bytesOnDisk, bytesAcked);\n        rbw.setNumBytes(bytesAcked);\n        rbw.setLastChecksumAndDataLen(bytesAcked, null);\n      }\n\n      // bump the replica's generation stamp to newGS\n      bumpReplicaGS(rbw, newGS);\n    } catch (IOException e) {\n      IOUtils.cleanup(null, ref);\n      throw e;\n    }\n    return new ReplicaHandler(rbw, ref);\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.initReplicaRecovery": {
                "code_before_change": "  public ReplicaRecoveryInfo initReplicaRecovery(RecoveringBlock rBlock)\n      throws IOException {\n    return initReplicaRecovery(rBlock.getBlock().getBlockPoolId(), volumeMap,\n        rBlock.getBlock().getLocalBlock(), rBlock.getNewGenerationStamp(),\n        datanode.getDnConf().getXceiverStopTimeout());\n  }",
                "code_after_change": "  public synchronized ReplicaRecoveryInfo initReplicaRecovery(\n      RecoveringBlock rBlock) throws IOException {\n    return initReplicaRecovery(rBlock.getBlock().getBlockPoolId(), volumeMap,\n        rBlock.getBlock().getLocalBlock(), rBlock.getNewGenerationStamp(),\n        datanode.getDnConf().getXceiverStopTimeout());\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Buggy Method"
            },
            "fix_suggestion": "Preventive",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Buggy Method"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the issue occurring in the 'BlockReceiver#receivePacket' method, which is where the error occurs but not where the actual fix was made, thus it is classified as 'Partial' under 'Buggy Method' for both root cause and problem location identification. The fix suggestion is 'Preventive' as it suggests improving the replica recovery code to handle the case where the on-disk size is less than the acknowledged size, which would mitigate the bug. There is no wrong information in the bug report as all details are relevant to the context of the bug."
        }
    },
    {
        "filename": "HDFS-10760.json",
        "code_diff": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataXceiver.run": {
                "code_before_change": "  public void run() {\n    int opsProcessed = 0;\n    Op op = null;\n\n    try {\n      synchronized(this) {\n        xceiver = Thread.currentThread();\n      }\n      dataXceiverServer.addPeer(peer, Thread.currentThread(), this);\n      peer.setWriteTimeout(datanode.getDnConf().socketWriteTimeout);\n      InputStream input = socketIn;\n      try {\n        IOStreamPair saslStreams = datanode.saslServer.receive(peer, socketOut,\n          socketIn, datanode.getXferAddress().getPort(),\n          datanode.getDatanodeId());\n        input = new BufferedInputStream(saslStreams.in,\n            smallBufferSize);\n        socketOut = saslStreams.out;\n      } catch (InvalidMagicNumberException imne) {\n        if (imne.isHandshake4Encryption()) {\n          LOG.info(\"Failed to read expected encryption handshake from client \" +\n              \"at \" + peer.getRemoteAddressString() + \". Perhaps the client \" +\n              \"is running an older version of Hadoop which does not support \" +\n              \"encryption\");\n        } else {\n          LOG.info(\"Failed to read expected SASL data transfer protection \" +\n              \"handshake from client at \" + peer.getRemoteAddressString() + \n              \". Perhaps the client is running an older version of Hadoop \" +\n              \"which does not support SASL data transfer protection\");\n        }\n        return;\n      }\n      \n      super.initialize(new DataInputStream(input));\n      \n      // We process requests in a loop, and stay around for a short timeout.\n      // This optimistic behaviour allows the other end to reuse connections.\n      // Setting keepalive timeout to 0 disable this behavior.\n      do {\n        updateCurrentThreadName(\"Waiting for operation #\" + (opsProcessed + 1));\n\n        try {\n          if (opsProcessed != 0) {\n            assert dnConf.socketKeepaliveTimeout > 0;\n            peer.setReadTimeout(dnConf.socketKeepaliveTimeout);\n          } else {\n            peer.setReadTimeout(dnConf.socketTimeout);\n          }\n          op = readOp();\n        } catch (InterruptedIOException ignored) {\n          // Time out while we wait for client rpc\n          break;\n        } catch (EOFException | ClosedChannelException e) {\n          // Since we optimistically expect the next op, it's quite normal to\n          // get EOF here.\n          LOG.debug(\"Cached {} closing after {} ops.  \" +\n              \"This message is usually benign.\", peer, opsProcessed);\n          break;\n        } catch (IOException err) {\n          incrDatanodeNetworkErrors();\n          throw err;\n        }\n\n        // restore normal timeout\n        if (opsProcessed != 0) {\n          peer.setReadTimeout(dnConf.socketTimeout);\n        }\n\n        opStartTime = monotonicNow();\n        processOp(op);\n        ++opsProcessed;\n      } while ((peer != null) &&\n          (!peer.isClosed() && dnConf.socketKeepaliveTimeout > 0));\n    } catch (Throwable t) {\n      String s = datanode.getDisplayName() + \":DataXceiver error processing \"\n          + ((op == null) ? \"unknown\" : op.name()) + \" operation \"\n          + \" src: \" + remoteAddress + \" dst: \" + localAddress;\n      if (op == Op.WRITE_BLOCK && t instanceof ReplicaAlreadyExistsException) {\n        // For WRITE_BLOCK, it is okay if the replica already exists since\n        // client and replication may write the same block to the same datanode\n        // at the same time.\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(s, t);\n        } else {\n          LOG.info(s + \"; \" + t);\n        }\n      } else if (op == Op.READ_BLOCK && t instanceof SocketTimeoutException) {\n        String s1 =\n            \"Likely the client has stopped reading, disconnecting it\";\n        s1 += \" (\" + s + \")\";\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(s1, t);\n        } else {\n          LOG.info(s1 + \"; \" + t);          \n        }\n      } else {\n        LOG.error(s, t);\n      }\n    } finally {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(datanode.getDisplayName() + \":Number of active connections is: \"\n            + datanode.getXceiverCount());\n      }\n      updateCurrentThreadName(\"Cleaning up\");\n      if (peer != null) {\n        dataXceiverServer.closePeer(peer);\n        IOUtils.closeStream(in);\n      }\n    }\n  }",
                "code_after_change": "  public void run() {\n    int opsProcessed = 0;\n    Op op = null;\n\n    try {\n      synchronized(this) {\n        xceiver = Thread.currentThread();\n      }\n      dataXceiverServer.addPeer(peer, Thread.currentThread(), this);\n      peer.setWriteTimeout(datanode.getDnConf().socketWriteTimeout);\n      InputStream input = socketIn;\n      try {\n        IOStreamPair saslStreams = datanode.saslServer.receive(peer, socketOut,\n          socketIn, datanode.getXferAddress().getPort(),\n          datanode.getDatanodeId());\n        input = new BufferedInputStream(saslStreams.in,\n            smallBufferSize);\n        socketOut = saslStreams.out;\n      } catch (InvalidMagicNumberException imne) {\n        if (imne.isHandshake4Encryption()) {\n          LOG.info(\"Failed to read expected encryption handshake from client \" +\n              \"at \" + peer.getRemoteAddressString() + \". Perhaps the client \" +\n              \"is running an older version of Hadoop which does not support \" +\n              \"encryption\");\n        } else {\n          LOG.info(\"Failed to read expected SASL data transfer protection \" +\n              \"handshake from client at \" + peer.getRemoteAddressString() + \n              \". Perhaps the client is running an older version of Hadoop \" +\n              \"which does not support SASL data transfer protection\");\n        }\n        return;\n      }\n      \n      super.initialize(new DataInputStream(input));\n      \n      // We process requests in a loop, and stay around for a short timeout.\n      // This optimistic behaviour allows the other end to reuse connections.\n      // Setting keepalive timeout to 0 disable this behavior.\n      do {\n        updateCurrentThreadName(\"Waiting for operation #\" + (opsProcessed + 1));\n\n        try {\n          if (opsProcessed != 0) {\n            assert dnConf.socketKeepaliveTimeout > 0;\n            peer.setReadTimeout(dnConf.socketKeepaliveTimeout);\n          } else {\n            peer.setReadTimeout(dnConf.socketTimeout);\n          }\n          op = readOp();\n        } catch (InterruptedIOException ignored) {\n          // Time out while we wait for client rpc\n          break;\n        } catch (EOFException | ClosedChannelException e) {\n          // Since we optimistically expect the next op, it's quite normal to\n          // get EOF here.\n          LOG.debug(\"Cached {} closing after {} ops.  \" +\n              \"This message is usually benign.\", peer, opsProcessed);\n          break;\n        } catch (IOException err) {\n          incrDatanodeNetworkErrors();\n          throw err;\n        }\n\n        // restore normal timeout\n        if (opsProcessed != 0) {\n          peer.setReadTimeout(dnConf.socketTimeout);\n        }\n\n        opStartTime = monotonicNow();\n        processOp(op);\n        ++opsProcessed;\n      } while ((peer != null) &&\n          (!peer.isClosed() && dnConf.socketKeepaliveTimeout > 0));\n    } catch (Throwable t) {\n      String s = datanode.getDisplayName() + \":DataXceiver error processing \"\n          + ((op == null) ? \"unknown\" : op.name()) + \" operation \"\n          + \" src: \" + remoteAddress + \" dst: \" + localAddress;\n      if (op == Op.WRITE_BLOCK && t instanceof ReplicaAlreadyExistsException) {\n        // For WRITE_BLOCK, it is okay if the replica already exists since\n        // client and replication may write the same block to the same datanode\n        // at the same time.\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(s, t);\n        } else {\n          LOG.info(s + \"; \" + t);\n        }\n      } else if (op == Op.READ_BLOCK && t instanceof SocketTimeoutException) {\n        String s1 =\n            \"Likely the client has stopped reading, disconnecting it\";\n        s1 += \" (\" + s + \")\";\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(s1, t);\n        } else {\n          LOG.info(s1 + \"; \" + t);          \n        }\n      } else if (t instanceof InvalidToken) {\n        // The InvalidToken exception has already been logged in\n        // checkAccess() method and this is not a server error.\n        if (LOG.isTraceEnabled()) {\n          LOG.trace(s, t);\n        }\n      } else {\n        LOG.error(s, t);\n      }\n    } finally {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(datanode.getDisplayName() + \":Number of active connections is: \"\n            + datanode.getXceiverCount());\n      }\n      updateCurrentThreadName(\"Cleaning up\");\n      if (peer != null) {\n        dataXceiverServer.closePeer(peer);\n        IOUtils.closeStream(in);\n      }\n    }\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Correct",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report precisely identifies the root cause by mentioning that the InvalidToken exception should not be logged as an error in the DataXceiver#run() method, which is the ground truth method. The fix suggestion is correct as it matches the developer's fix, which involves catching the InvalidToken exception in DataXceiver#run() and only logging it as a warning. The problem location is precisely identified as the report directly mentions DataXceiver#run(), which is the ground truth method. There is no wrong information in the bug report as all statements are relevant and accurate."
        }
    },
    {
        "filename": "HDFS-11608.json",
        "code_diff": {
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.DFSOutputStream.adjustChunkBoundary": {
                "code_before_change": "  protected void adjustChunkBoundary() {\n    if (getStreamer().getAppendChunk() &&\n        getStreamer().getBytesCurBlock() % bytesPerChecksum == 0) {\n      getStreamer().setAppendChunk(false);\n      resetChecksumBufSize();\n    }\n\n    if (!getStreamer().getAppendChunk()) {\n      int psize = Math.min((int)(blockSize- getStreamer().getBytesCurBlock()),\n          dfsClient.getConf().getWritePacketSize());\n      computePacketChunkSize(psize, bytesPerChecksum);\n    }\n  }",
                "code_after_change": "  protected void adjustChunkBoundary() {\n    if (getStreamer().getAppendChunk() &&\n        getStreamer().getBytesCurBlock() % bytesPerChecksum == 0) {\n      getStreamer().setAppendChunk(false);\n      resetChecksumBufSize();\n    }\n\n    if (!getStreamer().getAppendChunk()) {\n      final int psize = (int) Math\n          .min(blockSize - getStreamer().getBytesCurBlock(), writePacketSize);\n      computePacketChunkSize(psize, bytesPerChecksum);\n    }\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.DFSOutputStream.DFSOutputStream": {
                "code_before_change": [],
                "code_after_change": []
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the issue with HDFS write crashes due to large block sizes, which is related to the ground truth methods in terms of context but does not precisely identify the root cause in the ground truth methods. The stack trace mentions methods like 'DataXceiver.writeBlock' and 'PacketReceiver.doRead', which are in the same context as the ground truth methods but not the exact methods where the fix was applied. There is no fix suggestion provided in the bug report. The problem location is partially identified as it mentions methods in the stack trace that are related to the issue but not the exact ground truth methods. There is no wrong information in the bug report as it accurately describes the observed behavior and related stack traces."
        }
    },
    {
        "filename": "HDFS-12638.json",
        "code_diff": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.INode.addDeleteBlock": {
                "code_before_change": "    public void addDeleteBlock(BlockInfo toDelete) {\n      assert toDelete != null : \"toDelete is null\";\n      toDelete.delete();\n      toDeleteList.add(toDelete);\n    }",
                "code_after_change": "    public void addDeleteBlock(BlockInfo toDelete) {\n      assert toDelete != null : \"toDelete is null\";\n      toDelete.delete();\n      toDeleteList.add(toDelete);\n      // If the file is being truncated\n      // the copy-on-truncate block should also be collected for deletion\n      BlockUnderConstructionFeature uc = toDelete.getUnderConstructionFeature();\n      if(uc == null) {\n        return;\n      }\n      Block truncateBlock = uc.getTruncateBlock();\n      if(truncateBlock == null || truncateBlock.equals(toDelete)) {\n        return;\n      }\n      assert truncateBlock instanceof BlockInfo : \"should be BlockInfo\";\n      addDeleteBlock((BlockInfo) truncateBlock);\n    }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Missing",
                "sub_category": ""
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies a NullPointerException occurring in the ReplicationWork.chooseTargets method, which is part of the stack trace context shared with the ground truth method INode.addDeleteBlock. However, it does not precisely identify the root cause related to the ground truth method. The report does not provide any fix suggestions, hence 'Missing' for fix suggestion. The problem location is not precisely identified as the report does not mention the ground truth method or any related methods outside of the stack trace. There is no wrong information in the report as it accurately describes the observed error and its context."
        }
    },
    {
        "filename": "HDFS-12383.json",
        "code_diff": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.ReencryptionUpdater.run": {
                "code_before_change": "  public void run() {\n    throttleTimerAll.start();\n    while (true) {\n      try {\n        // Assuming single-threaded updater.\n        takeAndProcessTasks();\n      } catch (InterruptedException ie) {\n        LOG.warn(\"Re-encryption updater thread interrupted. Exiting.\");\n        Thread.currentThread().interrupt();\n        return;\n      } catch (IOException ioe) {\n        LOG.warn(\"Re-encryption updater thread exception.\", ioe);\n      } catch (Throwable t) {\n        LOG.error(\"Re-encryption updater thread exiting.\", t);\n        return;\n      }\n    }\n  }",
                "code_after_change": "  public void run() {\n    isRunning = true;\n    throttleTimerAll.start();\n    while (true) {\n      try {\n        // Assuming single-threaded updater.\n        takeAndProcessTasks();\n      } catch (InterruptedException ie) {\n        LOG.warn(\"Re-encryption updater thread interrupted. Exiting.\");\n        Thread.currentThread().interrupt();\n        isRunning = false;\n        return;\n      } catch (IOException | CancellationException e) {\n        LOG.warn(\"Re-encryption updater thread exception.\", e);\n      } catch (Throwable t) {\n        LOG.error(\"Re-encryption updater thread exiting.\", t);\n        isRunning = false;\n        return;\n      }\n    }\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.ReencryptionUpdater.markZoneSubmissionDone": {
                "code_before_change": "  void markZoneSubmissionDone(final long zoneId)\n      throws IOException, InterruptedException {\n    final ZoneSubmissionTracker tracker = handler.getTracker(zoneId);\n    if (tracker != null) {\n      tracker.submissionDone = true;\n    } else {\n      // Caller thinks submission is done, but no tasks submitted - meaning\n      // no files in the EZ need to be re-encrypted. Complete directly.\n      handler.addDummyTracker(zoneId);\n    }\n  }",
                "code_after_change": "  void markZoneSubmissionDone(final long zoneId)\n      throws IOException, InterruptedException {\n    final ZoneSubmissionTracker tracker = handler.getTracker(zoneId);\n    if (tracker != null) {\n      tracker.submissionDone = true;\n    } else {\n      // Caller thinks submission is done, but no tasks submitted - meaning\n      // no files in the EZ need to be re-encrypted. Complete directly.\n      handler.addDummyTracker(zoneId);\n    }\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.ReencryptionUpdater.takeAndProcessTasks": {
                "code_before_change": "  private void takeAndProcessTasks() throws Exception {\n    final Future<ReencryptionTask> completed = batchService.take();\n    throttle();\n    checkPauseForTesting();\n    ReencryptionTask task = completed.get();\n    if (completed.isCancelled()) {\n      LOG.debug(\"Skipped canceled re-encryption task for zone {}, last: {}\",\n          task.zoneId, task.lastFile);\n    }\n\n    boolean shouldRetry;\n    do {\n      dir.getFSNamesystem().writeLock();\n      try {\n        throttleTimerLocked.start();\n        processTask(task);\n        shouldRetry = false;\n      } catch (RetriableException | SafeModeException re) {\n        // Keep retrying until succeed.\n        LOG.info(\"Exception when processing re-encryption task for zone {}, \"\n                + \"retrying...\", task.zoneId, re);\n        shouldRetry = true;\n        Thread.sleep(faultRetryInterval);\n      } catch (IOException ioe) {\n        LOG.warn(\"Failure processing re-encryption task for zone {}\",\n            task.zoneId, ioe);\n        ++task.numFailures;\n        task.processed = true;\n        shouldRetry = false;\n      } finally {\n        dir.getFSNamesystem().writeUnlock(\"reencryptUpdater\");\n        throttleTimerLocked.stop();\n      }\n      // logSync regardless, to prevent edit log buffer overflow triggering\n      // logSync inside FSN writelock.\n      dir.getEditLog().logSync();\n    } while (shouldRetry);\n  }",
                "code_after_change": "  private void takeAndProcessTasks() throws Exception {\n    final Future<ReencryptionTask> completed = batchService.take();\n    throttle();\n    checkPauseForTesting();\n    ReencryptionTask task = completed.get();\n    if (completed.isCancelled()) {\n      LOG.debug(\"Skipped canceled re-encryption task for zone {}, last: {}\",\n          task.zoneId, task.lastFile);\n      return;\n    }\n\n    boolean shouldRetry;\n    do {\n      dir.getFSNamesystem().writeLock();\n      try {\n        throttleTimerLocked.start();\n        processTask(task);\n        shouldRetry = false;\n      } catch (RetriableException | SafeModeException re) {\n        // Keep retrying until succeed.\n        LOG.info(\"Exception when processing re-encryption task for zone {}, \"\n                + \"retrying...\", task.zoneId, re);\n        shouldRetry = true;\n        Thread.sleep(faultRetryInterval);\n      } catch (IOException ioe) {\n        LOG.warn(\"Failure processing re-encryption task for zone {}\",\n            task.zoneId, ioe);\n        ++task.numFailures;\n        task.processed = true;\n        shouldRetry = false;\n      } finally {\n        dir.getFSNamesystem().writeUnlock(\"reencryptUpdater\");\n        throttleTimerLocked.stop();\n      }\n      // logSync regardless, to prevent edit log buffer overflow triggering\n      // logSync inside FSN writelock.\n      dir.getEditLog().logSync();\n    } while (shouldRetry);\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.ReencryptionUpdater.pauseForTestingAfterNthCheckpoint": {
                "code_before_change": "  void pauseForTestingAfterNthCheckpoint(final long zoneId, final int count) {\n    assert pauseAfterNthCheckpoint == 0;\n    pauseAfterNthCheckpoint = count;\n    pauseZoneId = zoneId;\n  }",
                "code_after_change": "  void pauseForTestingAfterNthCheckpoint(final long zoneId, final int count) {\n    assert pauseAfterNthCheckpoint == 0;\n    pauseAfterNthCheckpoint = count;\n    pauseZoneId = zoneId;\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report accurately identifies the root cause as the re-encryption updater thread exiting due to a CancellationException, which is directly related to the ground truth method 'ReencryptionUpdater.run'. The report suggests that the updater should handle canceled tasks better, which aligns with the issue in 'takeAndProcessTasks'. However, it does not provide a specific fix suggestion, hence 'Missing' for fix suggestion. The problem location is precise as it mentions 'ReencryptionUpdater' which is part of the ground truth methods. There is no wrong information in the report as all details are relevant to the context of the bug."
        }
    },
    {
        "filename": "HDFS-5322.json",
        "code_diff": {
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.SaslRpcServer.handle": {
                "code_before_change": "    public void handle(Callback[] callbacks) throws InvalidToken,\n        UnsupportedCallbackException {\n      NameCallback nc = null;\n      PasswordCallback pc = null;\n      AuthorizeCallback ac = null;\n      for (Callback callback : callbacks) {\n        if (callback instanceof AuthorizeCallback) {\n          ac = (AuthorizeCallback) callback;\n        } else if (callback instanceof NameCallback) {\n          nc = (NameCallback) callback;\n        } else if (callback instanceof PasswordCallback) {\n          pc = (PasswordCallback) callback;\n        } else if (callback instanceof RealmCallback) {\n          continue; // realm is ignored\n        } else {\n          throw new UnsupportedCallbackException(callback,\n              \"Unrecognized SASL DIGEST-MD5 Callback\");\n        }\n      }\n      if (pc != null) {\n        TokenIdentifier tokenIdentifier = getIdentifier(nc.getDefaultName(), secretManager);\n        char[] password = getPassword(tokenIdentifier);\n        UserGroupInformation user = null;\n        user = tokenIdentifier.getUser(); // may throw exception\n        connection.attemptingUser = user;\n        \n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"SASL server DIGEST-MD5 callback: setting password \"\n              + \"for client: \" + tokenIdentifier.getUser());\n        }\n        pc.setPassword(password);\n      }\n      if (ac != null) {\n        String authid = ac.getAuthenticationID();\n        String authzid = ac.getAuthorizationID();\n        if (authid.equals(authzid)) {\n          ac.setAuthorized(true);\n        } else {\n          ac.setAuthorized(false);\n        }\n        if (ac.isAuthorized()) {\n          if (LOG.isDebugEnabled()) {\n            String username =\n              getIdentifier(authzid, secretManager).getUser().getUserName();\n            LOG.debug(\"SASL server DIGEST-MD5 callback: setting \"\n                + \"canonicalized client ID: \" + username);\n          }\n          ac.setAuthorizedID(authzid);\n        }\n      }\n    }",
                "code_after_change": "    public void handle(Callback[] callbacks) throws InvalidToken,\n        UnsupportedCallbackException, StandbyException, RetriableException,\n        IOException {\n      NameCallback nc = null;\n      PasswordCallback pc = null;\n      AuthorizeCallback ac = null;\n      for (Callback callback : callbacks) {\n        if (callback instanceof AuthorizeCallback) {\n          ac = (AuthorizeCallback) callback;\n        } else if (callback instanceof NameCallback) {\n          nc = (NameCallback) callback;\n        } else if (callback instanceof PasswordCallback) {\n          pc = (PasswordCallback) callback;\n        } else if (callback instanceof RealmCallback) {\n          continue; // realm is ignored\n        } else {\n          throw new UnsupportedCallbackException(callback,\n              \"Unrecognized SASL DIGEST-MD5 Callback\");\n        }\n      }\n      if (pc != null) {\n        TokenIdentifier tokenIdentifier = getIdentifier(nc.getDefaultName(),\n            secretManager);\n        char[] password = getPassword(tokenIdentifier);\n        UserGroupInformation user = null;\n        user = tokenIdentifier.getUser(); // may throw exception\n        connection.attemptingUser = user;\n        \n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"SASL server DIGEST-MD5 callback: setting password \"\n              + \"for client: \" + tokenIdentifier.getUser());\n        }\n        pc.setPassword(password);\n      }\n      if (ac != null) {\n        String authid = ac.getAuthenticationID();\n        String authzid = ac.getAuthorizationID();\n        if (authid.equals(authzid)) {\n          ac.setAuthorized(true);\n        } else {\n          ac.setAuthorized(false);\n        }\n        if (ac.isAuthorized()) {\n          if (LOG.isDebugEnabled()) {\n            String username =\n              getIdentifier(authzid, secretManager).getUser().getUserName();\n            LOG.debug(\"SASL server DIGEST-MD5 callback: setting \"\n                + \"canonicalized client ID: \" + username);\n          }\n          ac.setAuthorizedID(authzid);\n        }\n      }\n    }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startActiveServices": {
                "code_before_change": "  void startActiveServices() throws IOException {\n    LOG.info(\"Starting services required for active state\");\n    writeLock();\n    try {\n      FSEditLog editLog = dir.fsImage.getEditLog();\n      \n      if (!editLog.isOpenForWrite()) {\n        // During startup, we're already open for write during initialization.\n        editLog.initJournalsForWrite();\n        // May need to recover\n        editLog.recoverUnclosedStreams();\n        \n        LOG.info(\"Catching up to latest edits from old active before \" +\n            \"taking over writer role in edits logs\");\n        editLogTailer.catchupDuringFailover();\n        \n        blockManager.setPostponeBlocksFromFuture(false);\n        blockManager.getDatanodeManager().markAllDatanodesStale();\n        blockManager.clearQueues();\n        blockManager.processAllPendingDNMessages();\n        \n        if (!isInSafeMode() ||\n            (isInSafeMode() && safeMode.isPopulatingReplQueues())) {\n          LOG.info(\"Reprocessing replication and invalidation queues\");\n          blockManager.processMisReplicatedBlocks();\n        }\n        \n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"NameNode metadata after re-processing \" +\n              \"replication and invalidation queues during failover:\\n\" +\n              metaSaveAsString());\n        }\n        \n        long nextTxId = dir.fsImage.getLastAppliedTxId() + 1;\n        LOG.info(\"Will take over writing edit logs at txnid \" + \n            nextTxId);\n        editLog.setNextTxId(nextTxId);\n\n        dir.fsImage.editLog.openForWrite();\n      }\n      if (haEnabled) {\n        // Renew all of the leases before becoming active.\n        // This is because, while we were in standby mode,\n        // the leases weren't getting renewed on this NN.\n        // Give them all a fresh start here.\n        leaseManager.renewAllLeases();\n      }\n      leaseManager.startMonitor();\n      startSecretManagerIfNecessary();\n\n      //ResourceMonitor required only at ActiveNN. See HDFS-2914\n      this.nnrmthread = new Daemon(new NameNodeResourceMonitor());\n      nnrmthread.start();\n    } finally {\n      writeUnlock();\n    }\n  }",
                "code_after_change": "  void startActiveServices() throws IOException {\n    startingActiveService = true;\n    LOG.info(\"Starting services required for active state\");\n    writeLock();\n    try {\n      FSEditLog editLog = dir.fsImage.getEditLog();\n      \n      if (!editLog.isOpenForWrite()) {\n        // During startup, we're already open for write during initialization.\n        editLog.initJournalsForWrite();\n        // May need to recover\n        editLog.recoverUnclosedStreams();\n        \n        LOG.info(\"Catching up to latest edits from old active before \" +\n            \"taking over writer role in edits logs\");\n        editLogTailer.catchupDuringFailover();\n        \n        blockManager.setPostponeBlocksFromFuture(false);\n        blockManager.getDatanodeManager().markAllDatanodesStale();\n        blockManager.clearQueues();\n        blockManager.processAllPendingDNMessages();\n        \n        if (!isInSafeMode() ||\n            (isInSafeMode() && safeMode.isPopulatingReplQueues())) {\n          LOG.info(\"Reprocessing replication and invalidation queues\");\n          blockManager.processMisReplicatedBlocks();\n        }\n        \n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"NameNode metadata after re-processing \" +\n              \"replication and invalidation queues during failover:\\n\" +\n              metaSaveAsString());\n        }\n        \n        long nextTxId = dir.fsImage.getLastAppliedTxId() + 1;\n        LOG.info(\"Will take over writing edit logs at txnid \" + \n            nextTxId);\n        editLog.setNextTxId(nextTxId);\n\n        dir.fsImage.editLog.openForWrite();\n      }\n      if (haEnabled) {\n        // Renew all of the leases before becoming active.\n        // This is because, while we were in standby mode,\n        // the leases weren't getting renewed on this NN.\n        // Give them all a fresh start here.\n        leaseManager.renewAllLeases();\n      }\n      leaseManager.startMonitor();\n      startSecretManagerIfNecessary();\n\n      //ResourceMonitor required only at ActiveNN. See HDFS-2914\n      this.nnrmthread = new Daemon(new NameNodeResourceMonitor());\n      nnrmthread.start();\n    } finally {\n      writeUnlock();\n      startingActiveService = false;\n    }\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getEditLogTailer": {
                "code_before_change": "  public EditLogTailer getEditLogTailer() {\n    return editLogTailer;\n  }",
                "code_after_change": "  public EditLogTailer getEditLogTailer() {\n    return editLogTailer;\n  }"
            },
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.saslProcess": {
                "code_before_change": "    private void saslProcess(RpcSaslProto saslMessage)\n        throws WrappedRpcServerException, IOException, InterruptedException {\n      if (saslContextEstablished) {\n        throw new WrappedRpcServerException(\n            RpcErrorCodeProto.FATAL_INVALID_RPC_HEADER,\n            new SaslException(\"Negotiation is already complete\"));\n      }\n      RpcSaslProto saslResponse = null;\n      try {\n        try {\n          saslResponse = processSaslMessage(saslMessage);\n        } catch (IOException e) {\n          IOException sendToClient = e;\n          Throwable cause = e;\n          while (cause != null) {\n            if (cause instanceof InvalidToken) {\n              // FIXME: hadoop method signatures are restricting the SASL\n              // callbacks to only returning InvalidToken, but some services\n              // need to throw other exceptions (ex. NN + StandyException),\n              // so for now we'll tunnel the real exceptions via an\n              // InvalidToken's cause which normally is not set \n              if (cause.getCause() != null) {\n                cause = cause.getCause();\n              }\n              sendToClient = (IOException) cause;\n              break;\n            }\n            cause = cause.getCause();\n          }\n          rpcMetrics.incrAuthenticationFailures();\n          String clientIP = this.toString();\n          // attempting user could be null\n          AUDITLOG.warn(AUTH_FAILED_FOR + clientIP + \":\" + attemptingUser +\n            \" (\" + e.getLocalizedMessage() + \")\");\n          throw sendToClient;\n        }\n        \n        if (saslServer != null && saslServer.isComplete()) {\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"SASL server context established. Negotiated QoP is \"\n                + saslServer.getNegotiatedProperty(Sasl.QOP));\n          }\n          user = getAuthorizedUgi(saslServer.getAuthorizationID());\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"SASL server successfully authenticated client: \" + user);\n          }\n          rpcMetrics.incrAuthenticationSuccesses();\n          AUDITLOG.info(AUTH_SUCCESSFUL_FOR + user);\n          saslContextEstablished = true;\n        }\n      } catch (WrappedRpcServerException wrse) { // don't re-wrap\n        throw wrse;\n      } catch (IOException ioe) {\n        throw new WrappedRpcServerException(\n            RpcErrorCodeProto.FATAL_UNAUTHORIZED, ioe);\n      }\n      // send back response if any, may throw IOException\n      if (saslResponse != null) {\n        doSaslReply(saslResponse);\n      }\n      // do NOT enable wrapping until the last auth response is sent\n      if (saslContextEstablished) {\n        String qop = (String) saslServer.getNegotiatedProperty(Sasl.QOP);\n        // SASL wrapping is only used if the connection has a QOP, and\n        // the value is not auth.  ex. auth-int & auth-priv\n        useWrap = (qop != null && !\"auth\".equalsIgnoreCase(qop));        \n      }\n    }",
                "code_after_change": "    private void saslProcess(RpcSaslProto saslMessage)\n        throws WrappedRpcServerException, IOException, InterruptedException {\n      if (saslContextEstablished) {\n        throw new WrappedRpcServerException(\n            RpcErrorCodeProto.FATAL_INVALID_RPC_HEADER,\n            new SaslException(\"Negotiation is already complete\"));\n      }\n      RpcSaslProto saslResponse = null;\n      try {\n        try {\n          saslResponse = processSaslMessage(saslMessage);\n        } catch (IOException e) {\n          rpcMetrics.incrAuthenticationFailures();\n          // attempting user could be null\n          AUDITLOG.warn(AUTH_FAILED_FOR + this.toString() + \":\"\n              + attemptingUser + \" (\" + e.getLocalizedMessage() + \")\");\n          throw (IOException) getCauseForInvalidToken(e);\n        }\n        \n        if (saslServer != null && saslServer.isComplete()) {\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"SASL server context established. Negotiated QoP is \"\n                + saslServer.getNegotiatedProperty(Sasl.QOP));\n          }\n          user = getAuthorizedUgi(saslServer.getAuthorizationID());\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"SASL server successfully authenticated client: \" + user);\n          }\n          rpcMetrics.incrAuthenticationSuccesses();\n          AUDITLOG.info(AUTH_SUCCESSFUL_FOR + user);\n          saslContextEstablished = true;\n        }\n      } catch (WrappedRpcServerException wrse) { // don't re-wrap\n        throw wrse;\n      } catch (IOException ioe) {\n        throw new WrappedRpcServerException(\n            RpcErrorCodeProto.FATAL_UNAUTHORIZED, ioe);\n      }\n      // send back response if any, may throw IOException\n      if (saslResponse != null) {\n        doSaslReply(saslResponse);\n      }\n      // do NOT enable wrapping until the last auth response is sent\n      if (saslContextEstablished) {\n        String qop = (String) saslServer.getNegotiatedProperty(Sasl.QOP);\n        // SASL wrapping is only used if the connection has a QOP, and\n        // the value is not auth.  ex. auth-int & auth-priv\n        useWrap = (qop != null && !\"auth\".equalsIgnoreCase(qop));        \n      }\n    }"
            },
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.createPassword": {
                "code_before_change": "  protected synchronized byte[] createPassword(TokenIdent identifier) {\n    int sequenceNum;\n    long now = Time.now();\n    sequenceNum = ++delegationTokenSequenceNumber;\n    identifier.setIssueDate(now);\n    identifier.setMaxDate(now + tokenMaxLifetime);\n    identifier.setMasterKeyId(currentId);\n    identifier.setSequenceNumber(sequenceNum);\n    LOG.info(\"Creating password for identifier: \" + identifier);\n    byte[] password = createPassword(identifier.getBytes(), currentKey.getKey());\n    storeNewToken(identifier, now + tokenRenewInterval);\n    currentTokens.put(identifier, new DelegationTokenInformation(now\n        + tokenRenewInterval, password, getTrackingIdIfEnabled(identifier)));\n    return password;\n  }",
                "code_after_change": "  protected synchronized byte[] createPassword(TokenIdent identifier) {\n    int sequenceNum;\n    long now = Time.now();\n    sequenceNum = ++delegationTokenSequenceNumber;\n    identifier.setIssueDate(now);\n    identifier.setMaxDate(now + tokenMaxLifetime);\n    identifier.setMasterKeyId(currentId);\n    identifier.setSequenceNumber(sequenceNum);\n    LOG.info(\"Creating password for identifier: \" + identifier);\n    byte[] password = createPassword(identifier.getBytes(), currentKey.getKey());\n    storeNewToken(identifier, now + tokenRenewInterval);\n    currentTokens.put(identifier, new DelegationTokenInformation(now\n        + tokenRenewInterval, password, getTrackingIdIfEnabled(identifier)));\n    return password;\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenSecretManager.retrievePassword": {
                "code_before_change": "  public byte[] retrievePassword(\n      DelegationTokenIdentifier identifier) throws InvalidToken {\n    try {\n      // this check introduces inconsistency in the authentication to a\n      // HA standby NN.  non-token auths are allowed into the namespace which\n      // decides whether to throw a StandbyException.  tokens are a bit\n      // different in that a standby may be behind and thus not yet know\n      // of all tokens issued by the active NN.  the following check does\n      // not allow ANY token auth, however it should allow known tokens in\n      namesystem.checkOperation(OperationCategory.READ);\n    } catch (StandbyException se) {\n      // FIXME: this is a hack to get around changing method signatures by\n      // tunneling a non-InvalidToken exception as the cause which the\n      // RPC server will unwrap before returning to the client\n      InvalidToken wrappedStandby = new InvalidToken(\"StandbyException\");\n      wrappedStandby.initCause(se);\n      throw wrappedStandby;\n    }\n    return super.retrievePassword(identifier);\n  }",
                "code_after_change": "  public byte[] retrievePassword(\n      DelegationTokenIdentifier identifier) throws InvalidToken {\n    try {\n      // this check introduces inconsistency in the authentication to a\n      // HA standby NN.  non-token auths are allowed into the namespace which\n      // decides whether to throw a StandbyException.  tokens are a bit\n      // different in that a standby may be behind and thus not yet know\n      // of all tokens issued by the active NN.  the following check does\n      // not allow ANY token auth, however it should allow known tokens in\n      namesystem.checkOperation(OperationCategory.READ);\n    } catch (StandbyException se) {\n      // FIXME: this is a hack to get around changing method signatures by\n      // tunneling a non-InvalidToken exception as the cause which the\n      // RPC server will unwrap before returning to the client\n      InvalidToken wrappedStandby = new InvalidToken(\"StandbyException\");\n      wrappedStandby.initCause(se);\n      throw wrappedStandby;\n    }\n    return super.retrievePassword(identifier);\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.stopCommonServices": {
                "code_before_change": "  void stopCommonServices() {\n    writeLock();\n    try {\n      if (blockManager != null) blockManager.close();\n    } finally {\n      writeUnlock();\n    }\n    RetryCache.clear(retryCache);\n  }",
                "code_after_change": "  void stopCommonServices() {\n    writeLock();\n    try {\n      if (blockManager != null) blockManager.close();\n    } finally {\n      writeUnlock();\n    }\n    RetryCache.clear(retryCache);\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.logAuditEvent": {
                "code_before_change": "  private void logAuditEvent(boolean succeeded, String cmd, String src)\n      throws IOException {\n    logAuditEvent(succeeded, cmd, src, null, null);\n  }",
                "code_after_change": "  private void logAuditEvent(boolean succeeded, String cmd, String src)\n      throws IOException {\n    logAuditEvent(succeeded, cmd, src, null, null);\n  }"
            },
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.token.SecretManager.InvalidToken": {
                "code_before_change": [],
                "code_after_change": []
            },
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Server.saslReadAndProcess": {
                "code_before_change": "    private void saslReadAndProcess(DataInputStream dis) throws\n    WrappedRpcServerException, IOException, InterruptedException {\n      final RpcSaslProto saslMessage =\n          decodeProtobufFromStream(RpcSaslProto.newBuilder(), dis);\n      switch (saslMessage.getState()) {\n        case WRAP: {\n          if (!saslContextEstablished || !useWrap) {\n            throw new WrappedRpcServerException(\n                RpcErrorCodeProto.FATAL_INVALID_RPC_HEADER,\n                new SaslException(\"Server is not wrapping data\"));\n          }\n          // loops over decoded data and calls processOneRpc\n          unwrapPacketAndProcessRpcs(saslMessage.getToken().toByteArray());\n          break;\n        }\n        default:\n          saslProcess(saslMessage);\n      }\n    }",
                "code_after_change": "    private void saslReadAndProcess(DataInputStream dis) throws\n    WrappedRpcServerException, IOException, InterruptedException {\n      final RpcSaslProto saslMessage =\n          decodeProtobufFromStream(RpcSaslProto.newBuilder(), dis);\n      switch (saslMessage.getState()) {\n        case WRAP: {\n          if (!saslContextEstablished || !useWrap) {\n            throw new WrappedRpcServerException(\n                RpcErrorCodeProto.FATAL_INVALID_RPC_HEADER,\n                new SaslException(\"Server is not wrapping data\"));\n          }\n          // loops over decoded data and calls processOneRpc\n          unwrapPacketAndProcessRpcs(saslMessage.getToken().toByteArray());\n          break;\n        }\n        default:\n          saslProcess(saslMessage);\n      }\n    }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getSoftwareVersion": {
                "code_before_change": "  public String getSoftwareVersion() {\n    return VersionInfo.getVersion();\n  }",
                "code_after_change": "  public String getSoftwareVersion() {\n    return VersionInfo.getVersion();\n  }"
            },
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.security.SaslRpcServer.SaslDigestCallbackHandler": {
                "code_before_change": [],
                "code_after_change": []
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report mentions 'HDFS delegation token not found in cache errors' and includes stack traces that contain methods like 'org.apache.hadoop.security.UserGroupInformation.doAs' and 'org.apache.hadoop.ipc.Client.call'. These methods are in the shared stack trace context with the ground truth methods, but do not directly identify the root cause or problem location. There is no fix suggestion provided in the bug report. All information in the bug report is relevant to the context of the bug, so there is no wrong information."
        }
    },
    {
        "filename": "HDFS-11741.json",
        "code_diff": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager.updateKeys": {
                "code_before_change": "  public synchronized boolean updateKeys(final long updateTime) throws IOException {\n    if (updateTime > keyUpdateInterval) {\n      return updateKeys();\n    }\n    return false;\n  }",
                "code_after_change": "  public synchronized boolean updateKeys(final long updateTime) throws IOException {\n    if (updateTime > keyUpdateInterval) {\n      return updateKeys();\n    }\n    return false;\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.balancer.KeyManager.getAccessToken": {
                "code_before_change": "  public Token<BlockTokenIdentifier> getAccessToken(ExtendedBlock eb,\n      StorageType[] storageTypes) throws IOException {\n    if (!isBlockTokenEnabled) {\n      return BlockTokenSecretManager.DUMMY_TOKEN;\n    } else {\n      if (!shouldRun) {\n        throw new IOException(\n            \"Cannot get access token since BlockKeyUpdater is not running\");\n      }\n      return blockTokenSecretManager.generateToken(null, eb,\n          EnumSet.of(BlockTokenIdentifier.AccessMode.REPLACE,\n              BlockTokenIdentifier.AccessMode.COPY), storageTypes);\n    }\n  }",
                "code_after_change": "  public Token<BlockTokenIdentifier> getAccessToken(ExtendedBlock eb\n      ) throws IOException {\n    if (!isBlockTokenEnabled) {\n      return BlockTokenSecretManager.DUMMY_TOKEN;\n    } else {\n      if (!shouldRun) {\n        throw new IOException(\n            \"Cannot get access token since BlockKeyUpdater is not running\");\n      }\n      return blockTokenSecretManager.generateToken(null, eb,\n          EnumSet.of(AccessMode.REPLACE, AccessMode.COPY));\n    }\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager.exportKeys": {
                "code_before_change": "  public synchronized ExportedBlockKeys exportKeys() {\n    if (!isMaster)\n      return null;\n    if (LOG.isDebugEnabled())\n      LOG.debug(\"Exporting access keys\");\n    return new ExportedBlockKeys(true, keyUpdateInterval, tokenLifetime,\n        currentKey, allKeys.values().toArray(new BlockKey[0]));\n  }",
                "code_after_change": "  public synchronized ExportedBlockKeys exportKeys() {\n    if (!isMaster)\n      return null;\n    if (LOG.isDebugEnabled())\n      LOG.debug(\"Exporting access keys\");\n    return new ExportedBlockKeys(true, keyUpdateInterval, tokenLifetime,\n        currentKey, allKeys.values().toArray(new BlockKey[0]));\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager.generateDataEncryptionKey": {
                "code_before_change": "  public DataEncryptionKey generateDataEncryptionKey() {\n    byte[] nonce = new byte[8];\n    nonceGenerator.nextBytes(nonce);\n    BlockKey key = null;\n    synchronized (this) {\n      key = currentKey;\n    }\n    byte[] encryptionKey = createPassword(nonce, key.getKey());\n    return new DataEncryptionKey(key.getKeyId(), blockPoolId, nonce,\n        encryptionKey, Time.now() + tokenLifetime,\n        encryptionAlgorithm);\n  }",
                "code_after_change": "  public DataEncryptionKey generateDataEncryptionKey() {\n    byte[] nonce = new byte[8];\n    nonceGenerator.nextBytes(nonce);\n    BlockKey key = null;\n    synchronized (this) {\n      key = currentKey;\n    }\n    byte[] encryptionKey = createPassword(nonce, key.getKey());\n    return new DataEncryptionKey(key.getKeyId(), blockPoolId, nonce,\n        encryptionKey, timer.now() + tokenLifetime,\n        encryptionAlgorithm);\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager.generateKeys": {
                "code_before_change": "  private synchronized void generateKeys() {\n    if (!isMaster)\n      return;\n    /*\n     * Need to set estimated expiry dates for currentKey and nextKey so that if\n     * NN crashes, DN can still expire those keys. NN will stop using the newly\n     * generated currentKey after the first keyUpdateInterval, however it may\n     * still be used by DN and Balancer to generate new tokens before they get a\n     * chance to sync their keys with NN. Since we require keyUpdInterval to be\n     * long enough so that all live DN's and Balancer will sync their keys with\n     * NN at least once during the period, the estimated expiry date for\n     * currentKey is set to now() + 2 * keyUpdateInterval + tokenLifetime.\n     * Similarly, the estimated expiry date for nextKey is one keyUpdateInterval\n     * more.\n     */\n    setSerialNo(serialNo + 1);\n    currentKey = new BlockKey(serialNo, Time.now() + 2\n        * keyUpdateInterval + tokenLifetime, generateSecret());\n    setSerialNo(serialNo + 1);\n    nextKey = new BlockKey(serialNo, Time.now() + 3\n        * keyUpdateInterval + tokenLifetime, generateSecret());\n    allKeys.put(currentKey.getKeyId(), currentKey);\n    allKeys.put(nextKey.getKeyId(), nextKey);\n  }",
                "code_after_change": "  private synchronized void generateKeys() {\n    if (!isMaster)\n      return;\n    /*\n     * Need to set estimated expiry dates for currentKey and nextKey so that if\n     * NN crashes, DN can still expire those keys. NN will stop using the newly\n     * generated currentKey after the first keyUpdateInterval, however it may\n     * still be used by DN and Balancer to generate new tokens before they get a\n     * chance to sync their keys with NN. Since we require keyUpdInterval to be\n     * long enough so that all live DN's and Balancer will sync their keys with\n     * NN at least once during the period, the estimated expiry date for\n     * currentKey is set to now() + 2 * keyUpdateInterval + tokenLifetime.\n     * Similarly, the estimated expiry date for nextKey is one keyUpdateInterval\n     * more.\n     */\n    setSerialNo(serialNo + 1);\n    currentKey = new BlockKey(serialNo, timer.now() + 2\n        * keyUpdateInterval + tokenLifetime, generateSecret());\n    setSerialNo(serialNo + 1);\n    nextKey = new BlockKey(serialNo, timer.now() + 3\n        * keyUpdateInterval + tokenLifetime, generateSecret());\n    allKeys.put(currentKey.getKeyId(), currentKey);\n    allKeys.put(nextKey.getKeyId(), nextKey);\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager.createPassword": {
                "code_before_change": "  protected byte[] createPassword(BlockTokenIdentifier identifier) {\n    BlockKey key = null;\n    synchronized (this) {\n      key = currentKey;\n    }\n    if (key == null)\n      throw new IllegalStateException(\"currentKey hasn't been initialized.\");\n    identifier.setExpiryDate(Time.now() + tokenLifetime);\n    identifier.setKeyId(key.getKeyId());\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Generating block token for \" + identifier.toString());\n    }\n    return createPassword(identifier.getBytes(), key.getKey());\n  }",
                "code_after_change": "  protected byte[] createPassword(BlockTokenIdentifier identifier) {\n    BlockKey key = null;\n    synchronized (this) {\n      key = currentKey;\n    }\n    if (key == null)\n      throw new IllegalStateException(\"currentKey hasn't been initialized.\");\n    identifier.setExpiryDate(timer.now() + tokenLifetime);\n    identifier.setKeyId(key.getKeyId());\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Generating block token for \" + identifier.toString());\n    }\n    return createPassword(identifier.getBytes(), key.getKey());\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.security.token.block.BlockTokenSecretManager.BlockTokenSecretManager": {
                "code_before_change": [],
                "code_after_change": []
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Same Class or Module"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Same Class or Module"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the issue with the KeyManager not updating the DataEncryptionKey, which is related to the ground truth methods in the same module, but it does not pinpoint the exact root cause method. The report does not provide any fix suggestion, hence 'Missing' for fix suggestion. The problem location is partially identified as it mentions the KeyManager, which is in the same module as the ground truth methods. There is no wrong information in the bug report as it correctly describes the issue context."
        }
    },
    {
        "filename": "HDFS-3936.json",
        "code_diff": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlocksMap.computeCapacity": {
                "code_before_change": "  private static int computeCapacity() {\n    //VM detection\n    //See http://java.sun.com/docs/hotspot/HotSpotFAQ.html#64bit_detection\n    final String vmBit = System.getProperty(\"sun.arch.data.model\");\n\n    //2% of max memory\n    final double twoPC = Runtime.getRuntime().maxMemory()/50.0;\n\n    //compute capacity\n    final int e1 = (int)(Math.log(twoPC)/Math.log(2.0) + 0.5);\n    final int e2 = e1 - (\"32\".equals(vmBit)? 2: 3);\n    final int exponent = e2 < 0? 0: e2 > 30? 30: e2;\n    final int c = 1 << exponent;\n\n    if (LightWeightGSet.LOG.isDebugEnabled()) {\n      LightWeightGSet.LOG.debug(\"VM type       = \" + vmBit + \"-bit\");\n      LightWeightGSet.LOG.debug(\"2% max memory = \" + twoPC/(1 << 20) + \" MB\");\n      LightWeightGSet.LOG.debug(\"capacity      = 2^\" + exponent\n          + \" = \" + c + \" entries\");\n    }\n    return c;\n  }",
                "code_after_change": "  private static int computeCapacity() {\n    //VM detection\n    //See http://java.sun.com/docs/hotspot/HotSpotFAQ.html#64bit_detection\n    final String vmBit = System.getProperty(\"sun.arch.data.model\");\n\n    //2% of max memory\n    final double twoPC = Runtime.getRuntime().maxMemory()/50.0;\n\n    //compute capacity\n    final int e1 = (int)(Math.log(twoPC)/Math.log(2.0) + 0.5);\n    final int e2 = e1 - (\"32\".equals(vmBit)? 2: 3);\n    final int exponent = e2 < 0? 0: e2 > 30? 30: e2;\n    final int c = 1 << exponent;\n\n    if (LightWeightGSet.LOG.isDebugEnabled()) {\n      LightWeightGSet.LOG.debug(\"VM type       = \" + vmBit + \"-bit\");\n      LightWeightGSet.LOG.debug(\"2% max memory = \" + twoPC/(1 << 20) + \" MB\");\n      LightWeightGSet.LOG.debug(\"capacity      = 2^\" + exponent\n          + \" = \" + c + \" entries\");\n    }\n    return c;\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies a NullPointerException occurring in the BlocksMap.getBlockCollection method, which is part of the stack trace context shared with the ground truth method BlocksMap.computeCapacity. However, it does not precisely identify the root cause in the ground truth method. There is no fix suggestion provided in the bug report. The problem location is partially identified as it mentions methods in the shared stack trace context but not the exact ground truth method. There is no wrong information in the bug report as it accurately describes the issue within the context of the stack trace."
        }
    },
    {
        "filename": "HDFS-6348.json",
        "code_diff": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.main": {
                "code_before_change": "  public static void main(String[] argv) throws Exception {\n    CommandLineOpts opts = SecondaryNameNode.parseArgs(argv);\n    if (opts == null) {\n      LOG.fatal(\"Failed to parse options\");\n      terminate(1);\n    } else if (opts.shouldPrintHelp()) {\n      opts.usage();\n      System.exit(0);\n    }\n    \n    StringUtils.startupShutdownMessage(SecondaryNameNode.class, argv, LOG);\n    Configuration tconf = new HdfsConfiguration();\n    SecondaryNameNode secondary = null;\n    try {\n      secondary = new SecondaryNameNode(tconf, opts);\n    } catch (IOException ioe) {\n      LOG.fatal(\"Failed to start secondary namenode\", ioe);\n      terminate(1);\n    }\n\n    if (opts != null && opts.getCommand() != null) {\n      int ret = secondary.processStartupCommand(opts);\n      terminate(ret);\n    }\n\n    if (secondary != null) {\n      secondary.startCheckpointThread();\n      secondary.join();\n    }\n  }",
                "code_after_change": "  public static void main(String[] argv) throws Exception {\n    CommandLineOpts opts = SecondaryNameNode.parseArgs(argv);\n    if (opts == null) {\n      LOG.fatal(\"Failed to parse options\");\n      terminate(1);\n    } else if (opts.shouldPrintHelp()) {\n      opts.usage();\n      System.exit(0);\n    }\n\n    try {\n      StringUtils.startupShutdownMessage(SecondaryNameNode.class, argv, LOG);\n      Configuration tconf = new HdfsConfiguration();\n      SecondaryNameNode secondary = null;\n      secondary = new SecondaryNameNode(tconf, opts);\n\n      if (opts != null && opts.getCommand() != null) {\n        int ret = secondary.processStartupCommand(opts);\n        terminate(ret);\n      }\n\n      if (secondary != null) {\n        secondary.startCheckpointThread();\n        secondary.join();\n      }\n    } catch (Throwable e) {\n      LOG.fatal(\"Failed to start secondary namenode\", e);\n      terminate(1);\n    }\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies a RuntimeException related to the SecondaryNameNode not terminating properly, which is mentioned in the stack trace. However, it does not precisely identify the root cause in the 'main' method of SecondaryNameNode, but it does mention methods in the same stack trace context. The report does not provide any fix suggestion, hence 'Missing' for fix suggestion. The problem location is partially identified as it mentions methods in the stack trace but not the exact ground truth method. There is no wrong information in the report as all details are relevant to the context of the bug."
        }
    },
    {
        "filename": "HDFS-7996.json",
        "code_diff": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockReceiver.getStorageUuid": {
                "code_before_change": "  String getStorageUuid() {\n    return replicaInfo.getStorageUuid();\n  }",
                "code_after_change": "  String getStorageUuid() {\n    return replicaInfo.getStorageUuid();\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receiveBlock": {
                "code_before_change": "  void receiveBlock(\n      DataOutputStream mirrOut, // output to next datanode\n      DataInputStream mirrIn,   // input from next datanode\n      DataOutputStream replyOut,  // output to previous datanode\n      String mirrAddr, DataTransferThrottler throttlerArg,\n      DatanodeInfo[] downstreams,\n      boolean isReplaceBlock) throws IOException {\n\n      syncOnClose = datanode.getDnConf().syncOnClose;\n      boolean responderClosed = false;\n      mirrorOut = mirrOut;\n      mirrorAddr = mirrAddr;\n      throttler = throttlerArg;\n\n      this.replyOut = replyOut;\n      this.isReplaceBlock = isReplaceBlock;\n\n    try {\n      if (isClient && !isTransfer) {\n        responder = new Daemon(datanode.threadGroup, \n            new PacketResponder(replyOut, mirrIn, downstreams));\n        responder.start(); // start thread to processes responses\n      }\n\n      while (receivePacket() >= 0) { /* Receive until the last packet */ }\n\n      // wait for all outstanding packet responses. And then\n      // indicate responder to gracefully shutdown.\n      // Mark that responder has been closed for future processing\n      if (responder != null) {\n        ((PacketResponder)responder.getRunnable()).close();\n        responderClosed = true;\n      }\n\n      // If this write is for a replication or transfer-RBW/Finalized,\n      // then finalize block or convert temporary to RBW.\n      // For client-writes, the block is finalized in the PacketResponder.\n      if (isDatanode || isTransfer) {\n        // close the block/crc files\n        close();\n        block.setNumBytes(replicaInfo.getNumBytes());\n\n        if (stage == BlockConstructionStage.TRANSFER_RBW) {\n          // for TRANSFER_RBW, convert temporary to RBW\n          datanode.data.convertTemporaryToRbw(block);\n        } else {\n          // for isDatnode or TRANSFER_FINALIZED\n          // Finalize the block.\n          datanode.data.finalizeBlock(block);\n        }\n        datanode.metrics.incrBlocksWritten();\n      }\n\n    } catch (IOException ioe) {\n      if (datanode.isRestarting()) {\n        // Do not throw if shutting down for restart. Otherwise, it will cause\n        // premature termination of responder.\n        LOG.info(\"Shutting down for restart (\" + block + \").\");\n      } else {\n        LOG.info(\"Exception for \" + block, ioe);\n        throw ioe;\n      }\n    } finally {\n      // Clear the previous interrupt state of this thread.\n      Thread.interrupted();\n\n      // If a shutdown for restart was initiated, upstream needs to be notified.\n      // There is no need to do anything special if the responder was closed\n      // normally.\n      if (!responderClosed) { // Data transfer was not complete.\n        if (responder != null) {\n          // In case this datanode is shutting down for quick restart,\n          // send a special ack upstream.\n          if (datanode.isRestarting() && isClient && !isTransfer) {\n            File blockFile = ((ReplicaInPipeline)replicaInfo).getBlockFile();\n            File restartMeta = new File(blockFile.getParent()  + \n                File.pathSeparator + \".\" + blockFile.getName() + \".restart\");\n            if (restartMeta.exists() && !restartMeta.delete()) {\n              LOG.warn(\"Failed to delete restart meta file: \" +\n                  restartMeta.getPath());\n            }\n            try (Writer out = new OutputStreamWriter(\n                new FileOutputStream(restartMeta), \"UTF-8\")) {\n              // write out the current time.\n              out.write(Long.toString(Time.now() + restartBudget));\n              out.flush();\n            } catch (IOException ioe) {\n              // The worst case is not recovering this RBW replica. \n              // Client will fall back to regular pipeline recovery.\n            } finally {\n              IOUtils.cleanup(LOG, out);\n            }\n            try {              \n              // Even if the connection is closed after the ack packet is\n              // flushed, the client can react to the connection closure \n              // first. Insert a delay to lower the chance of client \n              // missing the OOB ack.\n              Thread.sleep(1000);\n            } catch (InterruptedException ie) {\n              // It is already going down. Ignore this.\n            }\n          }\n          responder.interrupt();\n        }\n        IOUtils.closeStream(this);\n        cleanupBlock();\n      }\n      if (responder != null) {\n        try {\n          responder.interrupt();\n          // join() on the responder should timeout a bit earlier than the\n          // configured deadline. Otherwise, the join() on this thread will\n          // likely timeout as well.\n          long joinTimeout = datanode.getDnConf().getXceiverStopTimeout();\n          joinTimeout = joinTimeout > 1  ? joinTimeout*8/10 : joinTimeout;\n          responder.join(joinTimeout);\n          if (responder.isAlive()) {\n            String msg = \"Join on responder thread \" + responder\n                + \" timed out\";\n            LOG.warn(msg + \"\\n\" + StringUtils.getStackTrace(responder));\n            throw new IOException(msg);\n          }\n        } catch (InterruptedException e) {\n          responder.interrupt();\n          // do not throw if shutting down for restart.\n          if (!datanode.isRestarting()) {\n            throw new IOException(\"Interrupted receiveBlock\");\n          }\n        }\n        responder = null;\n      }\n    }\n  }",
                "code_after_change": "  void receiveBlock(\n      DataOutputStream mirrOut, // output to next datanode\n      DataInputStream mirrIn,   // input from next datanode\n      DataOutputStream replyOut,  // output to previous datanode\n      String mirrAddr, DataTransferThrottler throttlerArg,\n      DatanodeInfo[] downstreams,\n      boolean isReplaceBlock) throws IOException {\n\n      syncOnClose = datanode.getDnConf().syncOnClose;\n      boolean responderClosed = false;\n      mirrorOut = mirrOut;\n      mirrorAddr = mirrAddr;\n      throttler = throttlerArg;\n\n      this.replyOut = replyOut;\n      this.isReplaceBlock = isReplaceBlock;\n\n    try {\n      if (isClient && !isTransfer) {\n        responder = new Daemon(datanode.threadGroup, \n            new PacketResponder(replyOut, mirrIn, downstreams));\n        responder.start(); // start thread to processes responses\n      }\n\n      while (receivePacket() >= 0) { /* Receive until the last packet */ }\n\n      // wait for all outstanding packet responses. And then\n      // indicate responder to gracefully shutdown.\n      // Mark that responder has been closed for future processing\n      if (responder != null) {\n        ((PacketResponder)responder.getRunnable()).close();\n        responderClosed = true;\n      }\n\n      // If this write is for a replication or transfer-RBW/Finalized,\n      // then finalize block or convert temporary to RBW.\n      // For client-writes, the block is finalized in the PacketResponder.\n      if (isDatanode || isTransfer) {\n        // Hold a volume reference to finalize block.\n        try (ReplicaHandler handler = claimReplicaHandler()) {\n          // close the block/crc files\n          close();\n          block.setNumBytes(replicaInfo.getNumBytes());\n\n          if (stage == BlockConstructionStage.TRANSFER_RBW) {\n            // for TRANSFER_RBW, convert temporary to RBW\n            datanode.data.convertTemporaryToRbw(block);\n          } else {\n            // for isDatnode or TRANSFER_FINALIZED\n            // Finalize the block.\n            datanode.data.finalizeBlock(block);\n          }\n        }\n        datanode.metrics.incrBlocksWritten();\n      }\n\n    } catch (IOException ioe) {\n      if (datanode.isRestarting()) {\n        // Do not throw if shutting down for restart. Otherwise, it will cause\n        // premature termination of responder.\n        LOG.info(\"Shutting down for restart (\" + block + \").\");\n      } else {\n        LOG.info(\"Exception for \" + block, ioe);\n        throw ioe;\n      }\n    } finally {\n      // Clear the previous interrupt state of this thread.\n      Thread.interrupted();\n\n      // If a shutdown for restart was initiated, upstream needs to be notified.\n      // There is no need to do anything special if the responder was closed\n      // normally.\n      if (!responderClosed) { // Data transfer was not complete.\n        if (responder != null) {\n          // In case this datanode is shutting down for quick restart,\n          // send a special ack upstream.\n          if (datanode.isRestarting() && isClient && !isTransfer) {\n            File blockFile = ((ReplicaInPipeline)replicaInfo).getBlockFile();\n            File restartMeta = new File(blockFile.getParent()  + \n                File.pathSeparator + \".\" + blockFile.getName() + \".restart\");\n            if (restartMeta.exists() && !restartMeta.delete()) {\n              LOG.warn(\"Failed to delete restart meta file: \" +\n                  restartMeta.getPath());\n            }\n            try (Writer out = new OutputStreamWriter(\n                new FileOutputStream(restartMeta), \"UTF-8\")) {\n              // write out the current time.\n              out.write(Long.toString(Time.now() + restartBudget));\n              out.flush();\n            } catch (IOException ioe) {\n              // The worst case is not recovering this RBW replica. \n              // Client will fall back to regular pipeline recovery.\n            } finally {\n              IOUtils.cleanup(LOG, out);\n            }\n            try {              \n              // Even if the connection is closed after the ack packet is\n              // flushed, the client can react to the connection closure \n              // first. Insert a delay to lower the chance of client \n              // missing the OOB ack.\n              Thread.sleep(1000);\n            } catch (InterruptedException ie) {\n              // It is already going down. Ignore this.\n            }\n          }\n          responder.interrupt();\n        }\n        IOUtils.closeStream(this);\n        cleanupBlock();\n      }\n      if (responder != null) {\n        try {\n          responder.interrupt();\n          // join() on the responder should timeout a bit earlier than the\n          // configured deadline. Otherwise, the join() on this thread will\n          // likely timeout as well.\n          long joinTimeout = datanode.getDnConf().getXceiverStopTimeout();\n          joinTimeout = joinTimeout > 1  ? joinTimeout*8/10 : joinTimeout;\n          responder.join(joinTimeout);\n          if (responder.isAlive()) {\n            String msg = \"Join on responder thread \" + responder\n                + \" timed out\";\n            LOG.warn(msg + \"\\n\" + StringUtils.getStackTrace(responder));\n            throw new IOException(msg);\n          }\n        } catch (InterruptedException e) {\n          responder.interrupt();\n          // do not throw if shutting down for restart.\n          if (!datanode.isRestarting()) {\n            throw new IOException(\"Interrupted receiveBlock\");\n          }\n        }\n        responder = null;\n      }\n    }\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockReceiver.computePartialChunkCrc": {
                "code_before_change": "  private Checksum computePartialChunkCrc(long blkoff, long ckoff)\n      throws IOException {\n\n    // find offset of the beginning of partial chunk.\n    //\n    int sizePartialChunk = (int) (blkoff % bytesPerChecksum);\n    blkoff = blkoff - sizePartialChunk;\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"computePartialChunkCrc for \" + block\n          + \": sizePartialChunk=\" + sizePartialChunk\n          + \", block offset=\" + blkoff\n          + \", metafile offset=\" + ckoff);\n    }\n\n    // create an input stream from the block file\n    // and read in partial crc chunk into temporary buffer\n    //\n    byte[] buf = new byte[sizePartialChunk];\n    byte[] crcbuf = new byte[checksumSize];\n    try (ReplicaInputStreams instr =\n        datanode.data.getTmpInputStreams(block, blkoff, ckoff)) {\n      IOUtils.readFully(instr.getDataIn(), buf, 0, sizePartialChunk);\n\n      // open meta file and read in crc value computer earlier\n      IOUtils.readFully(instr.getChecksumIn(), crcbuf, 0, crcbuf.length);\n    }\n\n    // compute crc of partial chunk from data read in the block file.\n    final Checksum partialCrc = DataChecksum.newDataChecksum(\n        diskChecksum.getChecksumType(), diskChecksum.getBytesPerChecksum());\n    partialCrc.update(buf, 0, sizePartialChunk);\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Read in partial CRC chunk from disk for \" + block);\n    }\n\n    // paranoia! verify that the pre-computed crc matches what we\n    // recalculated just now\n    if (partialCrc.getValue() != checksum2long(crcbuf)) {\n      String msg = \"Partial CRC \" + partialCrc.getValue() +\n                   \" does not match value computed the \" +\n                   \" last time file was closed \" +\n                   checksum2long(crcbuf);\n      throw new IOException(msg);\n    }\n    return partialCrc;\n  }",
                "code_after_change": "  private Checksum computePartialChunkCrc(long blkoff, long ckoff)\n      throws IOException {\n\n    // find offset of the beginning of partial chunk.\n    //\n    int sizePartialChunk = (int) (blkoff % bytesPerChecksum);\n    blkoff = blkoff - sizePartialChunk;\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"computePartialChunkCrc for \" + block\n          + \": sizePartialChunk=\" + sizePartialChunk\n          + \", block offset=\" + blkoff\n          + \", metafile offset=\" + ckoff);\n    }\n\n    // create an input stream from the block file\n    // and read in partial crc chunk into temporary buffer\n    //\n    byte[] buf = new byte[sizePartialChunk];\n    byte[] crcbuf = new byte[checksumSize];\n    try (ReplicaInputStreams instr =\n        datanode.data.getTmpInputStreams(block, blkoff, ckoff)) {\n      IOUtils.readFully(instr.getDataIn(), buf, 0, sizePartialChunk);\n\n      // open meta file and read in crc value computer earlier\n      IOUtils.readFully(instr.getChecksumIn(), crcbuf, 0, crcbuf.length);\n    }\n\n    // compute crc of partial chunk from data read in the block file.\n    final Checksum partialCrc = DataChecksum.newDataChecksum(\n        diskChecksum.getChecksumType(), diskChecksum.getBytesPerChecksum());\n    partialCrc.update(buf, 0, sizePartialChunk);\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Read in partial CRC chunk from disk for \" + block);\n    }\n\n    // paranoia! verify that the pre-computed crc matches what we\n    // recalculated just now\n    if (partialCrc.getValue() != checksum2long(crcbuf)) {\n      String msg = \"Partial CRC \" + partialCrc.getValue() +\n                   \" does not match value computed the \" +\n                   \" last time file was closed \" +\n                   checksum2long(crcbuf);\n      throw new IOException(msg);\n    }\n    return partialCrc;\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockReceiver.run": {
                "code_before_change": "    public void run() {\n      boolean lastPacketInBlock = false;\n      final long startTime = ClientTraceLog.isInfoEnabled() ? System.nanoTime() : 0;\n      while (isRunning() && !lastPacketInBlock) {\n        long totalAckTimeNanos = 0;\n        boolean isInterrupted = false;\n        try {\n          Packet pkt = null;\n          long expected = -2;\n          PipelineAck ack = new PipelineAck();\n          long seqno = PipelineAck.UNKOWN_SEQNO;\n          long ackRecvNanoTime = 0;\n          try {\n            if (type != PacketResponderType.LAST_IN_PIPELINE && !mirrorError) {\n              // read an ack from downstream datanode\n              ack.readFields(downstreamIn);\n              ackRecvNanoTime = System.nanoTime();\n              if (LOG.isDebugEnabled()) {\n                LOG.debug(myString + \" got \" + ack);\n              }\n              // Process an OOB ACK.\n              Status oobStatus = ack.getOOBStatus();\n              if (oobStatus != null) {\n                LOG.info(\"Relaying an out of band ack of type \" + oobStatus);\n                sendAckUpstream(ack, PipelineAck.UNKOWN_SEQNO, 0L, 0L,\n                    PipelineAck.combineHeader(datanode.getECN(),\n                      Status.SUCCESS));\n                continue;\n              }\n              seqno = ack.getSeqno();\n            }\n            if (seqno != PipelineAck.UNKOWN_SEQNO\n                || type == PacketResponderType.LAST_IN_PIPELINE) {\n              pkt = waitForAckHead(seqno);\n              if (!isRunning()) {\n                break;\n              }\n              expected = pkt.seqno;\n              if (type == PacketResponderType.HAS_DOWNSTREAM_IN_PIPELINE\n                  && seqno != expected) {\n                throw new IOException(myString + \"seqno: expected=\" + expected\n                    + \", received=\" + seqno);\n              }\n              if (type == PacketResponderType.HAS_DOWNSTREAM_IN_PIPELINE) {\n                // The total ack time includes the ack times of downstream\n                // nodes.\n                // The value is 0 if this responder doesn't have a downstream\n                // DN in the pipeline.\n                totalAckTimeNanos = ackRecvNanoTime - pkt.ackEnqueueNanoTime;\n                // Report the elapsed time from ack send to ack receive minus\n                // the downstream ack time.\n                long ackTimeNanos = totalAckTimeNanos\n                    - ack.getDownstreamAckTimeNanos();\n                if (ackTimeNanos < 0) {\n                  if (LOG.isDebugEnabled()) {\n                    LOG.debug(\"Calculated invalid ack time: \" + ackTimeNanos\n                        + \"ns.\");\n                  }\n                } else {\n                  datanode.metrics.addPacketAckRoundTripTimeNanos(ackTimeNanos);\n                }\n              }\n              lastPacketInBlock = pkt.lastPacketInBlock;\n            }\n          } catch (InterruptedException ine) {\n            isInterrupted = true;\n          } catch (IOException ioe) {\n            if (Thread.interrupted()) {\n              isInterrupted = true;\n            } else {\n              // continue to run even if can not read from mirror\n              // notify client of the error\n              // and wait for the client to shut down the pipeline\n              mirrorError = true;\n              LOG.info(myString, ioe);\n            }\n          }\n\n          if (Thread.interrupted() || isInterrupted) {\n            /*\n             * The receiver thread cancelled this thread. We could also check\n             * any other status updates from the receiver thread (e.g. if it is\n             * ok to write to replyOut). It is prudent to not send any more\n             * status back to the client because this datanode has a problem.\n             * The upstream datanode will detect that this datanode is bad, and\n             * rightly so.\n             *\n             * The receiver thread can also interrupt this thread for sending\n             * an out-of-band response upstream.\n             */\n            LOG.info(myString + \": Thread is interrupted.\");\n            running = false;\n            continue;\n          }\n\n          if (lastPacketInBlock) {\n            // Finalize the block and close the block file\n            finalizeBlock(startTime);\n          }\n\n          Status myStatus = pkt != null ? pkt.ackStatus : Status.SUCCESS;\n          sendAckUpstream(ack, expected, totalAckTimeNanos,\n            (pkt != null ? pkt.offsetInBlock : 0),\n            PipelineAck.combineHeader(datanode.getECN(), myStatus));\n          if (pkt != null) {\n            // remove the packet from the ack queue\n            removeAckHead();\n          }\n        } catch (IOException e) {\n          LOG.warn(\"IOException in BlockReceiver.run(): \", e);\n          if (running) {\n            datanode.checkDiskErrorAsync();\n            LOG.info(myString, e);\n            running = false;\n            if (!Thread.interrupted()) { // failure not caused by interruption\n              receiverThread.interrupt();\n            }\n          }\n        } catch (Throwable e) {\n          if (running) {\n            LOG.info(myString, e);\n            running = false;\n            receiverThread.interrupt();\n          }\n        }\n      }\n      LOG.info(myString + \" terminating\");\n    }",
                "code_after_change": "    public void run() {\n      boolean lastPacketInBlock = false;\n      final long startTime = ClientTraceLog.isInfoEnabled() ? System.nanoTime() : 0;\n      while (isRunning() && !lastPacketInBlock) {\n        long totalAckTimeNanos = 0;\n        boolean isInterrupted = false;\n        try {\n          Packet pkt = null;\n          long expected = -2;\n          PipelineAck ack = new PipelineAck();\n          long seqno = PipelineAck.UNKOWN_SEQNO;\n          long ackRecvNanoTime = 0;\n          try {\n            if (type != PacketResponderType.LAST_IN_PIPELINE && !mirrorError) {\n              // read an ack from downstream datanode\n              ack.readFields(downstreamIn);\n              ackRecvNanoTime = System.nanoTime();\n              if (LOG.isDebugEnabled()) {\n                LOG.debug(myString + \" got \" + ack);\n              }\n              // Process an OOB ACK.\n              Status oobStatus = ack.getOOBStatus();\n              if (oobStatus != null) {\n                LOG.info(\"Relaying an out of band ack of type \" + oobStatus);\n                sendAckUpstream(ack, PipelineAck.UNKOWN_SEQNO, 0L, 0L,\n                    PipelineAck.combineHeader(datanode.getECN(),\n                      Status.SUCCESS));\n                continue;\n              }\n              seqno = ack.getSeqno();\n            }\n            if (seqno != PipelineAck.UNKOWN_SEQNO\n                || type == PacketResponderType.LAST_IN_PIPELINE) {\n              pkt = waitForAckHead(seqno);\n              if (!isRunning()) {\n                break;\n              }\n              expected = pkt.seqno;\n              if (type == PacketResponderType.HAS_DOWNSTREAM_IN_PIPELINE\n                  && seqno != expected) {\n                throw new IOException(myString + \"seqno: expected=\" + expected\n                    + \", received=\" + seqno);\n              }\n              if (type == PacketResponderType.HAS_DOWNSTREAM_IN_PIPELINE) {\n                // The total ack time includes the ack times of downstream\n                // nodes.\n                // The value is 0 if this responder doesn't have a downstream\n                // DN in the pipeline.\n                totalAckTimeNanos = ackRecvNanoTime - pkt.ackEnqueueNanoTime;\n                // Report the elapsed time from ack send to ack receive minus\n                // the downstream ack time.\n                long ackTimeNanos = totalAckTimeNanos\n                    - ack.getDownstreamAckTimeNanos();\n                if (ackTimeNanos < 0) {\n                  if (LOG.isDebugEnabled()) {\n                    LOG.debug(\"Calculated invalid ack time: \" + ackTimeNanos\n                        + \"ns.\");\n                  }\n                } else {\n                  datanode.metrics.addPacketAckRoundTripTimeNanos(ackTimeNanos);\n                }\n              }\n              lastPacketInBlock = pkt.lastPacketInBlock;\n            }\n          } catch (InterruptedException ine) {\n            isInterrupted = true;\n          } catch (IOException ioe) {\n            if (Thread.interrupted()) {\n              isInterrupted = true;\n            } else {\n              // continue to run even if can not read from mirror\n              // notify client of the error\n              // and wait for the client to shut down the pipeline\n              mirrorError = true;\n              LOG.info(myString, ioe);\n            }\n          }\n\n          if (Thread.interrupted() || isInterrupted) {\n            /*\n             * The receiver thread cancelled this thread. We could also check\n             * any other status updates from the receiver thread (e.g. if it is\n             * ok to write to replyOut). It is prudent to not send any more\n             * status back to the client because this datanode has a problem.\n             * The upstream datanode will detect that this datanode is bad, and\n             * rightly so.\n             *\n             * The receiver thread can also interrupt this thread for sending\n             * an out-of-band response upstream.\n             */\n            LOG.info(myString + \": Thread is interrupted.\");\n            running = false;\n            continue;\n          }\n\n          if (lastPacketInBlock) {\n            // Finalize the block and close the block file\n            finalizeBlock(startTime);\n          }\n\n          Status myStatus = pkt != null ? pkt.ackStatus : Status.SUCCESS;\n          sendAckUpstream(ack, expected, totalAckTimeNanos,\n            (pkt != null ? pkt.offsetInBlock : 0),\n            PipelineAck.combineHeader(datanode.getECN(), myStatus));\n          if (pkt != null) {\n            // remove the packet from the ack queue\n            removeAckHead();\n          }\n        } catch (IOException e) {\n          LOG.warn(\"IOException in BlockReceiver.run(): \", e);\n          if (running) {\n            datanode.checkDiskErrorAsync();\n            LOG.info(myString, e);\n            running = false;\n            if (!Thread.interrupted()) { // failure not caused by interruption\n              receiverThread.interrupt();\n            }\n          }\n        } catch (Throwable e) {\n          if (running) {\n            LOG.info(myString, e);\n            running = false;\n            receiverThread.interrupt();\n          }\n        }\n      }\n      LOG.info(myString + \" terminating\");\n    }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Direct Caller/Callee"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Direct Caller/Callee"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the issue occurring in the 'PacketResponder#finalizeBlock()' method, which is a direct caller of the 'BlockReceiver.run' method, one of the ground truth methods. However, it does not precisely identify the root cause in the ground truth methods themselves. The report does not provide a fix suggestion, as it only describes the problem without suggesting a solution. The problem location is partially identified as it points to a method directly related to the ground truth methods. There is no wrong information in the report; all details are relevant to the context of the bug."
        }
    },
    {
        "filename": "HDFS-4302.json",
        "code_diff": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords": {
                "code_before_change": "  long loadEditRecords(EditLogInputStream in, boolean closeOnExit,\n                      long expectedStartingTxId, MetaRecoveryContext recovery)\n      throws IOException {\n    FSDirectory fsDir = fsNamesys.dir;\n\n    EnumMap<FSEditLogOpCodes, Holder<Integer>> opCounts =\n      new EnumMap<FSEditLogOpCodes, Holder<Integer>>(FSEditLogOpCodes.class);\n\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"Acquiring write lock to replay edit log\");\n    }\n\n    fsNamesys.writeLock();\n    fsDir.writeLock();\n\n    long recentOpcodeOffsets[] = new long[4];\n    Arrays.fill(recentOpcodeOffsets, -1);\n    \n    long expectedTxId = expectedStartingTxId;\n    long numEdits = 0;\n    long lastTxId = in.getLastTxId();\n    long numTxns = (lastTxId - expectedStartingTxId) + 1;\n    long lastLogTime = now();\n\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"edit log length: \" + in.length() + \", start txid: \"\n          + expectedStartingTxId + \", last txid: \" + lastTxId);\n    }\n    try {\n      while (true) {\n        try {\n          FSEditLogOp op;\n          try {\n            op = in.readOp();\n            if (op == null) {\n              break;\n            }\n          } catch (Throwable e) {\n            // Handle a problem with our input\n            check203UpgradeFailure(in.getVersion(), e);\n            String errorMessage =\n              formatEditLogReplayError(in, recentOpcodeOffsets, expectedTxId);\n            FSImage.LOG.error(errorMessage, e);\n            if (recovery == null) {\n               // We will only try to skip over problematic opcodes when in\n               // recovery mode.\n              throw new EditLogInputException(errorMessage, e, numEdits);\n            }\n            MetaRecoveryContext.editLogLoaderPrompt(\n                \"We failed to read txId \" + expectedTxId,\n                recovery, \"skipping the bad section in the log\");\n            in.resync();\n            continue;\n          }\n          recentOpcodeOffsets[(int)(numEdits % recentOpcodeOffsets.length)] =\n            in.getPosition();\n          if (op.hasTransactionId()) {\n            if (op.getTransactionId() > expectedTxId) { \n              MetaRecoveryContext.editLogLoaderPrompt(\"There appears \" +\n                  \"to be a gap in the edit log.  We expected txid \" +\n                  expectedTxId + \", but got txid \" +\n                  op.getTransactionId() + \".\", recovery, \"ignoring missing \" +\n                  \" transaction IDs\");\n            } else if (op.getTransactionId() < expectedTxId) { \n              MetaRecoveryContext.editLogLoaderPrompt(\"There appears \" +\n                  \"to be an out-of-order edit in the edit log.  We \" +\n                  \"expected txid \" + expectedTxId + \", but got txid \" +\n                  op.getTransactionId() + \".\", recovery,\n                  \"skipping the out-of-order edit\");\n              continue;\n            }\n          }\n          try {\n            applyEditLogOp(op, fsDir, in.getVersion());\n          } catch (Throwable e) {\n            LOG.error(\"Encountered exception on operation \" + op, e);\n            MetaRecoveryContext.editLogLoaderPrompt(\"Failed to \" +\n             \"apply edit log operation \" + op + \": error \" +\n             e.getMessage(), recovery, \"applying edits\");\n          }\n          // Now that the operation has been successfully decoded and\n          // applied, update our bookkeeping.\n          incrOpCount(op.opCode, opCounts);\n          if (op.hasTransactionId()) {\n            lastAppliedTxId = op.getTransactionId();\n            expectedTxId = lastAppliedTxId + 1;\n          } else {\n            expectedTxId = lastAppliedTxId = expectedStartingTxId;\n          }\n          // log progress\n          if (op.hasTransactionId()) {\n            long now = now();\n            if (now - lastLogTime > REPLAY_TRANSACTION_LOG_INTERVAL) {\n              long deltaTxId = lastAppliedTxId - expectedStartingTxId + 1;\n              int percent = Math.round((float) deltaTxId / numTxns * 100);\n              LOG.info(\"replaying edit log: \" + deltaTxId + \"/\" + numTxns\n                  + \" transactions completed. (\" + percent + \"%)\");\n              lastLogTime = now;\n            }\n          }\n          numEdits++;\n        } catch (MetaRecoveryContext.RequestStopException e) {\n          MetaRecoveryContext.LOG.warn(\"Stopped reading edit log at \" +\n              in.getPosition() + \"/\"  + in.length());\n          break;\n        }\n      }\n    } finally {\n      if(closeOnExit) {\n        in.close();\n      }\n      fsDir.writeUnlock();\n      fsNamesys.writeUnlock();\n\n      if (LOG.isTraceEnabled()) {\n        LOG.trace(\"replaying edit log finished\");\n      }\n\n      if (FSImage.LOG.isDebugEnabled()) {\n        dumpOpCounts(opCounts);\n      }\n    }\n    return numEdits;\n  }",
                "code_after_change": "  long loadEditRecords(EditLogInputStream in, boolean closeOnExit,\n                      long expectedStartingTxId, MetaRecoveryContext recovery)\n      throws IOException {\n    FSDirectory fsDir = fsNamesys.dir;\n\n    EnumMap<FSEditLogOpCodes, Holder<Integer>> opCounts =\n      new EnumMap<FSEditLogOpCodes, Holder<Integer>>(FSEditLogOpCodes.class);\n\n    if (LOG.isTraceEnabled()) {\n      LOG.trace(\"Acquiring write lock to replay edit log\");\n    }\n\n    fsNamesys.writeLock();\n    fsDir.writeLock();\n\n    long recentOpcodeOffsets[] = new long[4];\n    Arrays.fill(recentOpcodeOffsets, -1);\n    \n    long expectedTxId = expectedStartingTxId;\n    long numEdits = 0;\n    long lastTxId = in.getLastTxId();\n    long numTxns = (lastTxId - expectedStartingTxId) + 1;\n    long lastLogTime = now();\n    long lastInodeId = fsNamesys.getLastInodeId();\n    \n    try {\n      while (true) {\n        try {\n          FSEditLogOp op;\n          try {\n            op = in.readOp();\n            if (op == null) {\n              break;\n            }\n          } catch (Throwable e) {\n            // Handle a problem with our input\n            check203UpgradeFailure(in.getVersion(), e);\n            String errorMessage =\n              formatEditLogReplayError(in, recentOpcodeOffsets, expectedTxId);\n            FSImage.LOG.error(errorMessage, e);\n            if (recovery == null) {\n               // We will only try to skip over problematic opcodes when in\n               // recovery mode.\n              throw new EditLogInputException(errorMessage, e, numEdits);\n            }\n            MetaRecoveryContext.editLogLoaderPrompt(\n                \"We failed to read txId \" + expectedTxId,\n                recovery, \"skipping the bad section in the log\");\n            in.resync();\n            continue;\n          }\n          recentOpcodeOffsets[(int)(numEdits % recentOpcodeOffsets.length)] =\n            in.getPosition();\n          if (op.hasTransactionId()) {\n            if (op.getTransactionId() > expectedTxId) { \n              MetaRecoveryContext.editLogLoaderPrompt(\"There appears \" +\n                  \"to be a gap in the edit log.  We expected txid \" +\n                  expectedTxId + \", but got txid \" +\n                  op.getTransactionId() + \".\", recovery, \"ignoring missing \" +\n                  \" transaction IDs\");\n            } else if (op.getTransactionId() < expectedTxId) { \n              MetaRecoveryContext.editLogLoaderPrompt(\"There appears \" +\n                  \"to be an out-of-order edit in the edit log.  We \" +\n                  \"expected txid \" + expectedTxId + \", but got txid \" +\n                  op.getTransactionId() + \".\", recovery,\n                  \"skipping the out-of-order edit\");\n              continue;\n            }\n          }\n          try {\n            long inodeId = applyEditLogOp(op, fsDir, in.getVersion());\n            if (lastInodeId < inodeId) {\n              lastInodeId = inodeId;\n            }\n          } catch (Throwable e) {\n            LOG.error(\"Encountered exception on operation \" + op, e);\n            MetaRecoveryContext.editLogLoaderPrompt(\"Failed to \" +\n             \"apply edit log operation \" + op + \": error \" +\n             e.getMessage(), recovery, \"applying edits\");\n          }\n          // Now that the operation has been successfully decoded and\n          // applied, update our bookkeeping.\n          incrOpCount(op.opCode, opCounts);\n          if (op.hasTransactionId()) {\n            lastAppliedTxId = op.getTransactionId();\n            expectedTxId = lastAppliedTxId + 1;\n          } else {\n            expectedTxId = lastAppliedTxId = expectedStartingTxId;\n          }\n          // log progress\n          if (op.hasTransactionId()) {\n            long now = now();\n            if (now - lastLogTime > REPLAY_TRANSACTION_LOG_INTERVAL) {\n              long deltaTxId = lastAppliedTxId - expectedStartingTxId + 1;\n              int percent = Math.round((float) deltaTxId / numTxns * 100);\n              LOG.info(\"replaying edit log: \" + deltaTxId + \"/\" + numTxns\n                  + \" transactions completed. (\" + percent + \"%)\");\n              lastLogTime = now;\n            }\n          }\n          numEdits++;\n        } catch (MetaRecoveryContext.RequestStopException e) {\n          MetaRecoveryContext.LOG.warn(\"Stopped reading edit log at \" +\n              in.getPosition() + \"/\"  + in.length());\n          break;\n        }\n      }\n    } finally {\n      fsNamesys.resetLastInodeId(lastInodeId);\n      if(closeOnExit) {\n        in.close();\n      }\n      fsDir.writeUnlock();\n      fsNamesys.writeUnlock();\n\n      if (LOG.isTraceEnabled()) {\n        LOG.trace(\"replaying edit log finished\");\n      }\n\n      if (FSImage.LOG.isDebugEnabled()) {\n        dumpOpCounts(opCounts);\n      }\n    }\n    return numEdits;\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Buggy Method"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Buggy Method"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the issue occurring in the `EditLogFileInputStream.length()` method, which is where the error manifests, but not where the actual fix was made (`FSEditLogLoader.loadEditRecords`). Therefore, it is classified as 'Partial' under 'Buggy Method' for both root cause and problem location identification. The report does not provide any fix suggestion, so it is marked as 'Missing' for fix suggestion. There is no incorrect information in the bug report, so 'Wrong Information' is marked as 'No'."
        }
    },
    {
        "filename": "HDFS-11849.json",
        "code_diff": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.qjournal.server.JournalNode.reportErrorOnFile": {
                "code_before_change": "    public void reportErrorOnFile(File f) {\n      LOG.fatal(\"Error reported on file \" + f + \"... exiting\",\n          new Exception());\n      stop(1);\n    }",
                "code_after_change": "    public void reportErrorOnFile(File f) {\n      LOG.fatal(\"Error reported on file \" + f + \"... exiting\",\n          new Exception());\n      stop(1);\n    }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Same Class or Module"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Same Class or Module"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the issue with the JournalNode startup failure due to a Kerberos login problem, which is related to the same class as the ground truth method 'reportErrorOnFile'. However, it does not precisely identify the root cause at the method level. The report does not provide any fix suggestion, hence 'Missing' for fix suggestion. The problem location is partially identified as it mentions the JournalNode class but not the specific method 'reportErrorOnFile'. There is no wrong information in the bug report as it accurately describes the context of the error."
        }
    },
    {
        "filename": "HDFS-4841.json",
        "code_diff": {
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.fs.FileSystem.getInternal": {
                "code_before_change": "    private FileSystem getInternal(URI uri, Configuration conf, Key key) throws IOException{\n      FileSystem fs;\n      synchronized (this) {\n        fs = map.get(key);\n      }\n      if (fs != null) {\n        return fs;\n      }\n\n      fs = createFileSystem(uri, conf);\n      synchronized (this) { // refetch the lock again\n        FileSystem oldfs = map.get(key);\n        if (oldfs != null) { // a file system is created while lock is releasing\n          fs.close(); // close the new file system\n          return oldfs;  // return the old file system\n        }\n        \n        // now insert the new file system into the map\n        if (map.isEmpty() ) {\n          ShutdownHookManager.get().addShutdownHook(clientFinalizer, SHUTDOWN_HOOK_PRIORITY);\n        }\n        fs.key = key;\n        map.put(key, fs);\n        if (conf.getBoolean(\"fs.automatic.close\", true)) {\n          toAutoClose.add(key);\n        }\n        return fs;\n      }\n    }",
                "code_after_change": "    private FileSystem getInternal(URI uri, Configuration conf, Key key) throws IOException{\n      FileSystem fs;\n      synchronized (this) {\n        fs = map.get(key);\n      }\n      if (fs != null) {\n        return fs;\n      }\n\n      fs = createFileSystem(uri, conf);\n      synchronized (this) { // refetch the lock again\n        FileSystem oldfs = map.get(key);\n        if (oldfs != null) { // a file system is created while lock is releasing\n          fs.close(); // close the new file system\n          return oldfs;  // return the old file system\n        }\n        \n        // now insert the new file system into the map\n        if (map.isEmpty()\n                && !ShutdownHookManager.get().isShutdownInProgress()) {\n          ShutdownHookManager.get().addShutdownHook(clientFinalizer, SHUTDOWN_HOOK_PRIORITY);\n        }\n        fs.key = key;\n        map.put(key, fs);\n        if (conf.getBoolean(\"fs.automatic.close\", true)) {\n          toAutoClose.add(key);\n        }\n        return fs;\n      }\n    }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the issue with the 'ClientFinalizer' shutdown hook failing due to a shutdown in progress, which is related to the stack trace context where the ground truth method 'getInternal' is mentioned. However, it does not precisely identify the root cause at the ground truth method. The report does not provide any fix suggestion, hence 'Missing' for fix suggestion. The problem location is partially identified as it mentions methods in the same stack trace context as the ground truth method. There is no wrong information in the bug report as all details are relevant to the issue described."
        }
    },
    {
        "filename": "HDFS-3384.json",
        "code_diff": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.DataStreamer.run": {
                "code_before_change": [],
                "code_after_change": "  public void run() {\n    long lastPacket = Time.monotonicNow();\n    TraceScope scope = NullScope.INSTANCE;\n    while (!streamerClosed && dfsClient.clientRunning) {\n      // if the Responder encountered an error, shutdown Responder\n      if (hasError && response != null) {\n        try {\n          response.close();\n          response.join();\n          response = null;\n        } catch (InterruptedException  e) {\n          LOG.warn(\"Caught exception\", e);\n        }\n      }\n\n      DFSPacket one;\n      try {\n        // process datanode IO errors if any\n        boolean doSleep = false;\n        if (hasError && (errorIndex >= 0 || restartingNodeIndex.get() >= 0)) {\n          doSleep = processDatanodeError();\n        }\n\n        final int halfSocketTimeout = dfsClient.getConf().getSocketTimeout()/2; \n        synchronized (dataQueue) {\n          // wait for a packet to be sent.\n          long now = Time.monotonicNow();\n          while ((!streamerClosed && !hasError && dfsClient.clientRunning\n              && dataQueue.size() == 0 &&\n              (stage != BlockConstructionStage.DATA_STREAMING ||\n                  stage == BlockConstructionStage.DATA_STREAMING &&\n                      now - lastPacket < halfSocketTimeout)) || doSleep ) {\n            long timeout = halfSocketTimeout - (now-lastPacket);\n            timeout = timeout <= 0 ? 1000 : timeout;\n            timeout = (stage == BlockConstructionStage.DATA_STREAMING)?\n                timeout : 1000;\n            try {\n              dataQueue.wait(timeout);\n            } catch (InterruptedException  e) {\n              LOG.warn(\"Caught exception\", e);\n            }\n            doSleep = false;\n            now = Time.monotonicNow();\n          }\n          if (streamerClosed || hasError || !dfsClient.clientRunning) {\n            continue;\n          }\n          // get packet to be sent.\n          if (dataQueue.isEmpty()) {\n            one = createHeartbeatPacket();\n            assert one != null;\n          } else {\n            try {\n              backOffIfNecessary();\n            } catch (InterruptedException e) {\n              LOG.warn(\"Caught exception\", e);\n            }\n            one = dataQueue.getFirst(); // regular data packet\n            long parents[] = one.getTraceParents();\n            if (parents.length > 0) {\n              scope = Trace.startSpan(\"dataStreamer\", new TraceInfo(0, parents[0]));\n              // TODO: use setParents API once it's available from HTrace 3.2\n              // scope = Trace.startSpan(\"dataStreamer\", Sampler.ALWAYS);\n              // scope.getSpan().setParents(parents);\n            }\n          }\n        }\n\n        // get new block from namenode.\n        if (stage == BlockConstructionStage.PIPELINE_SETUP_CREATE) {\n          if(LOG.isDebugEnabled()) {\n            LOG.debug(\"Allocating new block\");\n          }\n          setPipeline(nextBlockOutputStream());\n          initDataStreaming();\n        } else if (stage == BlockConstructionStage.PIPELINE_SETUP_APPEND) {\n          if(LOG.isDebugEnabled()) {\n            LOG.debug(\"Append to block \" + block);\n          }\n          setupPipelineForAppendOrRecovery();\n          if (true == streamerClosed) {\n            continue;\n          }\n          initDataStreaming();\n        }\n\n        long lastByteOffsetInBlock = one.getLastByteOffsetBlock();\n        if (lastByteOffsetInBlock > stat.getBlockSize()) {\n          throw new IOException(\"BlockSize \" + stat.getBlockSize() +\n              \" is smaller than data size. \" +\n              \" Offset of packet in block \" +\n              lastByteOffsetInBlock +\n              \" Aborting file \" + src);\n        }\n\n        if (one.isLastPacketInBlock()) {\n          // wait for all data packets have been successfully acked\n          synchronized (dataQueue) {\n            while (!streamerClosed && !hasError &&\n                ackQueue.size() != 0 && dfsClient.clientRunning) {\n              try {\n                // wait for acks to arrive from datanodes\n                dataQueue.wait(1000);\n              } catch (InterruptedException  e) {\n                LOG.warn(\"Caught exception\", e);\n              }\n            }\n          }\n          if (streamerClosed || hasError || !dfsClient.clientRunning) {\n            continue;\n          }\n          stage = BlockConstructionStage.PIPELINE_CLOSE;\n        }\n\n        // send the packet\n        Span span = null;\n        synchronized (dataQueue) {\n          // move packet from dataQueue to ackQueue\n          if (!one.isHeartbeatPacket()) {\n            span = scope.detach();\n            one.setTraceSpan(span);\n            dataQueue.removeFirst();\n            ackQueue.addLast(one);\n            dataQueue.notifyAll();\n          }\n        }\n\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"DataStreamer block \" + block +\n              \" sending packet \" + one);\n        }\n\n        // write out data to remote datanode\n        TraceScope writeScope = Trace.startSpan(\"writeTo\", span);\n        try {\n          one.writeTo(blockStream);\n          blockStream.flush();\n        } catch (IOException e) {\n          // HDFS-3398 treat primary DN is down since client is unable to\n          // write to primary DN. If a failed or restarting node has already\n          // been recorded by the responder, the following call will have no\n          // effect. Pipeline recovery can handle only one node error at a\n          // time. If the primary node fails again during the recovery, it\n          // will be taken out then.\n          tryMarkPrimaryDatanodeFailed();\n          throw e;\n        } finally {\n          writeScope.close();\n        }\n        lastPacket = Time.monotonicNow();\n\n        // update bytesSent\n        long tmpBytesSent = one.getLastByteOffsetBlock();\n        if (bytesSent < tmpBytesSent) {\n          bytesSent = tmpBytesSent;\n        }\n\n        if (streamerClosed || hasError || !dfsClient.clientRunning) {\n          continue;\n        }\n\n        // Is this block full?\n        if (one.isLastPacketInBlock()) {\n          // wait for the close packet has been acked\n          synchronized (dataQueue) {\n            while (!streamerClosed && !hasError &&\n                ackQueue.size() != 0 && dfsClient.clientRunning) {\n              dataQueue.wait(1000);// wait for acks to arrive from datanodes\n            }\n          }\n          if (streamerClosed || hasError || !dfsClient.clientRunning) {\n            continue;\n          }\n\n          endBlock();\n        }\n        if (progress != null) { progress.progress(); }\n\n        // This is used by unit test to trigger race conditions.\n        if (artificialSlowdown != 0 && dfsClient.clientRunning) {\n          Thread.sleep(artificialSlowdown);\n        }\n      } catch (Throwable e) {\n        // Log warning if there was a real error.\n        if (restartingNodeIndex.get() == -1) {\n          // Since their messages are descriptive enough, do not always\n          // log a verbose stack-trace WARN for quota exceptions.\n          if (e instanceof QuotaExceededException) {\n            LOG.debug(\"DataStreamer Quota Exception\", e);\n          } else {\n            LOG.warn(\"DataStreamer Exception\", e);\n          }\n        }\n        lastException.set(e);\n        assert !(e instanceof NullPointerException);\n        hasError = true;\n        if (errorIndex == -1 && restartingNodeIndex.get() == -1) {\n          // Not a datanode issue\n          streamerClosed = true;\n        }\n      } finally {\n        scope.close();\n      }\n    }\n    closeInternal();\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the method 'setupPipelineForAppendOrRecovery' and 'run' in the stack trace, which are in the same stack trace context as the ground truth method 'DataStreamer.run'. However, it does not precisely identify the root cause or suggest a fix. The report lacks a specific fix suggestion, and no incorrect information is present."
        }
    },
    {
        "filename": "HDFS-5657.json",
        "code_diff": {
            "hadoop-hdfs-project.hadoop-hdfs-nfs.src.main.java.org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx.offerNextToWrite": {
                "code_before_change": "  private synchronized WriteCtx offerNextToWrite() {\n    if (pendingWrites.isEmpty()) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"The asyn write task has no pending writes, fileId: \"\n            + latestAttr.getFileId());\n      }\n      // process pending commit again to handle this race: a commit is added\n      // to pendingCommits map just after the last doSingleWrite returns.\n      // There is no pending write and the commit should be handled by the\n      // last doSingleWrite. Due to the race, the commit is left along and\n      // can't be processed until cleanup. Therefore, we should do another\n      // processCommits to fix the race issue.\n      processCommits(nextOffset.get()); // nextOffset has same value as\n                                        // flushedOffset\n      this.asyncStatus = false;\n      return null;\n    } \n    \n      Entry<OffsetRange, WriteCtx> lastEntry = pendingWrites.lastEntry();\n      OffsetRange range = lastEntry.getKey();\n      WriteCtx toWrite = lastEntry.getValue();\n      \n      if (LOG.isTraceEnabled()) {\n        LOG.trace(\"range.getMin()=\" + range.getMin() + \" nextOffset=\"\n            + nextOffset);\n      }\n      \n      long offset = nextOffset.get();\n      if (range.getMin() > offset) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"The next sequencial write has not arrived yet\");\n        }\n        processCommits(nextOffset.get()); // handle race\n        this.asyncStatus = false;\n      } else if (range.getMin() < offset && range.getMax() > offset) {\n        // shouldn't happen since we do sync for overlapped concurrent writers\n        LOG.warn(\"Got a overlapping write (\" + range.getMin() + \",\"\n            + range.getMax() + \"), nextOffset=\" + offset\n            + \". Silently drop it now\");\n        pendingWrites.remove(range);\n        processCommits(nextOffset.get()); // handle race\n      } else {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Remove write(\" + range.getMin() + \"-\" + range.getMax()\n              + \") from the list\");\n        }\n        // after writing, remove the WriteCtx from cache \n        pendingWrites.remove(range);\n        // update nextOffset\n        nextOffset.addAndGet(toWrite.getCount());\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Change nextOffset to \" + nextOffset.get());\n        }\n        return toWrite;\n      }\n    \n    return null;\n  }",
                "code_after_change": "  private synchronized WriteCtx offerNextToWrite() {\n    if (pendingWrites.isEmpty()) {\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"The asyn write task has no pending writes, fileId: \"\n            + latestAttr.getFileId());\n      }\n      // process pending commit again to handle this race: a commit is added\n      // to pendingCommits map just after the last doSingleWrite returns.\n      // There is no pending write and the commit should be handled by the\n      // last doSingleWrite. Due to the race, the commit is left along and\n      // can't be processed until cleanup. Therefore, we should do another\n      // processCommits to fix the race issue.\n      processCommits(nextOffset.get()); // nextOffset has same value as\n                                        // flushedOffset\n      this.asyncStatus = false;\n      return null;\n    } \n    \n      Entry<OffsetRange, WriteCtx> lastEntry = pendingWrites.lastEntry();\n      OffsetRange range = lastEntry.getKey();\n      WriteCtx toWrite = lastEntry.getValue();\n      \n      if (LOG.isTraceEnabled()) {\n        LOG.trace(\"range.getMin()=\" + range.getMin() + \" nextOffset=\"\n            + nextOffset);\n      }\n      \n      long offset = nextOffset.get();\n      if (range.getMin() > offset) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"The next sequencial write has not arrived yet\");\n        }\n        processCommits(nextOffset.get()); // handle race\n        this.asyncStatus = false;\n      } else if (range.getMin() < offset && range.getMax() > offset) {\n        // shouldn't happen since we do sync for overlapped concurrent writers\n        LOG.warn(\"Got a overlapping write (\" + range.getMin() + \",\"\n            + range.getMax() + \"), nextOffset=\" + offset\n            + \". Silently drop it now\");\n        pendingWrites.remove(range);\n        processCommits(nextOffset.get()); // handle race\n      } else {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Remove write(\" + range.getMin() + \"-\" + range.getMax()\n              + \") from the list\");\n        }\n        // after writing, remove the WriteCtx from cache \n        pendingWrites.remove(range);\n        // update nextOffset\n        nextOffset.addAndGet(toWrite.getCount());\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Change nextOffset to \" + nextOffset.get());\n        }\n        return toWrite;\n      }\n    \n    return null;\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs-nfs.src.main.java.org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx.executeWriteBack": {
                "code_before_change": "  void executeWriteBack() {\n    Preconditions.checkState(asyncStatus,\n        \"The openFileCtx has false async status\");\n    try {\n      while (activeState) {\n        WriteCtx toWrite = offerNextToWrite();\n        if (toWrite != null) {\n          // Do the write\n          doSingleWrite(toWrite);\n          updateLastAccessTime();\n        } else {\n          break;\n        }\n      }\n      \n      if (!activeState && LOG.isDebugEnabled()) {\n        LOG.debug(\"The openFileCtx is not active anymore, fileId: \"\n            + latestAttr.getFileId());\n      }\n    } finally {\n      // make sure we reset asyncStatus to false\n      asyncStatus = false;\n    }\n  }",
                "code_after_change": "  void executeWriteBack() {\n    Preconditions.checkState(asyncStatus,\n        \"openFileCtx has false asyncStatus, fileId:\" + latestAttr.getFileid());\n    final long startOffset = asyncWriteBackStartOffset;  \n    try {\n      while (activeState) {\n        // asyncStatus could be changed to false in offerNextToWrite()\n        WriteCtx toWrite = offerNextToWrite();\n        if (toWrite != null) {\n          // Do the write\n          doSingleWrite(toWrite);\n          updateLastAccessTime();\n        } else {\n          break;\n        }\n      }\n      \n      if (!activeState && LOG.isDebugEnabled()) {\n        LOG.debug(\"The openFileCtx is not active anymore, fileId: \"\n            + latestAttr.getFileId());\n      }\n    } finally {\n      // Make sure to reset asyncStatus to false unless a race happens\n      synchronized (this) {\n        if (startOffset == asyncWriteBackStartOffset) {\n          asyncStatus = false;\n        } else {\n          LOG.info(\"Another asyn task is already started before this one\"\n              + \" is finalized. fileId:\" + latestAttr.getFileid()\n              + \" asyncStatus:\" + asyncStatus + \" original startOffset:\"\n              + startOffset + \" new startOffset:\" + asyncWriteBackStartOffset\n              + \". Won't change asyncStatus here.\");\n        }\n      }\n    }\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs-nfs.src.main.java.org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx.updateNonSequentialWriteInMemory": {
                "code_before_change": "  private long updateNonSequentialWriteInMemory(long count) {\n    long newValue = nonSequentialWriteInMemory.addAndGet(count);\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Update nonSequentialWriteInMemory by \" + count + \" new value:\"\n          + newValue);\n    }\n\n    Preconditions.checkState(newValue >= 0,\n        \"nonSequentialWriteInMemory is negative after update with count \"\n            + count);\n    return newValue;\n  }",
                "code_after_change": "  private long updateNonSequentialWriteInMemory(long count) {\n    long newValue = nonSequentialWriteInMemory.addAndGet(count);\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"Update nonSequentialWriteInMemory by \" + count + \" new value:\"\n          + newValue);\n    }\n\n    Preconditions.checkState(newValue >= 0,\n        \"nonSequentialWriteInMemory is negative after update with count \"\n            + count);\n    return newValue;\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs-nfs.src.main.java.org.apache.hadoop.hdfs.nfs.nfs3.OpenFileCtx.checkAndStartWrite": {
                "code_before_change": "  private synchronized boolean checkAndStartWrite(\n      AsyncDataService asyncDataService, WriteCtx writeCtx) {\n    \n    if (writeCtx.getOffset() == nextOffset.get()) {\n      if (!asyncStatus) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Trigger the write back task. Current nextOffset: \"\n              + nextOffset.get());\n        }\n        asyncStatus = true;\n        asyncDataService.execute(new AsyncDataService.WriteBackTask(this));\n      } else {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"The write back thread is working.\");\n        }\n      }\n      return true;\n    } else {\n      return false;\n    }\n  }",
                "code_after_change": "  private synchronized boolean checkAndStartWrite(\n      AsyncDataService asyncDataService, WriteCtx writeCtx) {\n    \n    if (writeCtx.getOffset() == nextOffset.get()) {\n      if (!asyncStatus) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Trigger the write back task. Current nextOffset: \"\n              + nextOffset.get());\n        }\n        asyncStatus = true;\n        asyncWriteBackStartOffset = writeCtx.getOffset();\n        asyncDataService.execute(new AsyncDataService.WriteBackTask(this));\n      } else {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"The write back thread is working.\");\n        }\n      }\n      return true;\n    } else {\n      return false;\n    }\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies a race condition issue in the NFS gateway, which is related to the methods in the stack trace, particularly 'OpenFileCtx.executeWriteBack' and 'OpenFileCtx.offerNextToWrite'. These methods are part of the ground truth methods, but the report does not precisely identify the root cause at the exact method level. The report does not provide any fix suggestion, hence 'Missing' for fix suggestion. The problem location is partially identified as it mentions methods in the stack trace context, but not precisely the ground truth methods. There is no wrong information in the bug report as it accurately describes the issue context."
        }
    },
    {
        "filename": "HDFS-11827.json",
        "code_diff": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.chooseRandom": {
                "code_before_change": "  protected DatanodeStorageInfo chooseRandom(String scope,\n      Set<Node> excludedNodes,\n      long blocksize,\n      int maxNodesPerRack,\n      List<DatanodeStorageInfo> results,\n      boolean avoidStaleNodes,\n      EnumMap<StorageType, Integer> storageTypes)\n          throws NotEnoughReplicasException {\n    return chooseRandom(1, scope, excludedNodes, blocksize, maxNodesPerRack,\n        results, avoidStaleNodes, storageTypes);\n  }",
                "code_after_change": "  protected DatanodeStorageInfo chooseRandom(String scope,\n      Set<Node> excludedNodes,\n      long blocksize,\n      int maxNodesPerRack,\n      List<DatanodeStorageInfo> results,\n      boolean avoidStaleNodes,\n      EnumMap<StorageType, Integer> storageTypes)\n          throws NotEnoughReplicasException {\n    return chooseRandom(1, scope, excludedNodes, blocksize, maxNodesPerRack,\n        results, avoidStaleNodes, storageTypes);\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report precisely identifies the root cause as a missing NPE check in the 'chooseRandom' method of the 'BlockPlacementPolicyDefault' class, which matches the ground truth method. There is no fix suggestion provided in the bug report, hence it is marked as 'Missing'. The problem location is also precisely identified as the 'chooseRandom' method, which is part of the ground truth. There is no wrong information in the bug report as all the details are consistent with the context of the bug."
        }
    },
    {
        "filename": "HDFS-6804.json",
        "code_diff": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataNodeFaultInjector.failPipeline": {
                "code_before_change": [],
                "code_after_change": "  public void failPipeline(ReplicaInPipelineInterface replicaInfo,\n      String mirrorAddr) throws IOException { }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report describes a checksum mismatch issue and provides a stack trace that includes methods related to the problem, such as 'BlockReceiver.receivePacket'. However, it does not precisely identify the root cause in the 'DataNodeFaultInjector.failPipeline' method, which is the ground truth method. The report lacks any fix suggestion, as there is no mention of how to resolve the issue. The problem location is partially identified through the stack trace, which shares context with the ground truth method. There is no incorrect information in the report; it accurately describes the observed behavior and logs."
        }
    },
    {
        "filename": "HDFS-5843.json",
        "code_diff": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataXceiver.blockChecksum": {
                "code_before_change": "  public void blockChecksum(final ExtendedBlock block,\n      final Token<BlockTokenIdentifier> blockToken) throws IOException {\n    final DataOutputStream out = new DataOutputStream(\n        getOutputStream());\n    checkAccess(out, true, block, blockToken,\n        Op.BLOCK_CHECKSUM, BlockTokenSecretManager.AccessMode.READ);\n    updateCurrentThreadName(\"Reading metadata for block \" + block);\n    final LengthInputStream metadataIn = \n      datanode.data.getMetaDataInputStream(block);\n    final DataInputStream checksumIn = new DataInputStream(new BufferedInputStream(\n        metadataIn, HdfsConstants.IO_FILE_BUFFER_SIZE));\n\n    updateCurrentThreadName(\"Getting checksum for block \" + block);\n    try {\n      //read metadata file\n      final BlockMetadataHeader header = BlockMetadataHeader.readHeader(checksumIn);\n      final DataChecksum checksum = header.getChecksum(); \n      final int bytesPerCRC = checksum.getBytesPerChecksum();\n      final long crcPerBlock = (metadataIn.getLength()\n          - BlockMetadataHeader.getHeaderSize())/checksum.getChecksumSize();\n      \n      //compute block checksum\n      final MD5Hash md5 = MD5Hash.digest(checksumIn);\n\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"block=\" + block + \", bytesPerCRC=\" + bytesPerCRC\n            + \", crcPerBlock=\" + crcPerBlock + \", md5=\" + md5);\n      }\n\n      //write reply\n      BlockOpResponseProto.newBuilder()\n        .setStatus(SUCCESS)\n        .setChecksumResponse(OpBlockChecksumResponseProto.newBuilder()             \n          .setBytesPerCrc(bytesPerCRC)\n          .setCrcPerBlock(crcPerBlock)\n          .setMd5(ByteString.copyFrom(md5.getDigest()))\n          .setCrcType(PBHelper.convert(checksum.getChecksumType()))\n          )\n        .build()\n        .writeDelimitedTo(out);\n      out.flush();\n    } finally {\n      IOUtils.closeStream(out);\n      IOUtils.closeStream(checksumIn);\n      IOUtils.closeStream(metadataIn);\n    }\n\n    //update metrics\n    datanode.metrics.addBlockChecksumOp(elapsed());\n  }",
                "code_after_change": "  public void blockChecksum(final ExtendedBlock block,\n      final Token<BlockTokenIdentifier> blockToken) throws IOException {\n    final DataOutputStream out = new DataOutputStream(\n        getOutputStream());\n    checkAccess(out, true, block, blockToken,\n        Op.BLOCK_CHECKSUM, BlockTokenSecretManager.AccessMode.READ);\n    updateCurrentThreadName(\"Reading metadata for block \" + block);\n    final LengthInputStream metadataIn = \n      datanode.data.getMetaDataInputStream(block);\n    final DataInputStream checksumIn = new DataInputStream(new BufferedInputStream(\n        metadataIn, HdfsConstants.IO_FILE_BUFFER_SIZE));\n\n    updateCurrentThreadName(\"Getting checksum for block \" + block);\n    try {\n      //read metadata file\n      final BlockMetadataHeader header = BlockMetadataHeader.readHeader(checksumIn);\n      final DataChecksum checksum = header.getChecksum(); \n      final int bytesPerCRC = checksum.getBytesPerChecksum();\n      final long crcPerBlock = checksum.getChecksumSize() > 0 \n              ? (metadataIn.getLength() - BlockMetadataHeader.getHeaderSize())/checksum.getChecksumSize()\n              : 0;\n      \n      //compute block checksum\n      final MD5Hash md5 = MD5Hash.digest(checksumIn);\n\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(\"block=\" + block + \", bytesPerCRC=\" + bytesPerCRC\n            + \", crcPerBlock=\" + crcPerBlock + \", md5=\" + md5);\n      }\n\n      //write reply\n      BlockOpResponseProto.newBuilder()\n        .setStatus(SUCCESS)\n        .setChecksumResponse(OpBlockChecksumResponseProto.newBuilder()             \n          .setBytesPerCrc(bytesPerCRC)\n          .setCrcPerBlock(crcPerBlock)\n          .setMd5(ByteString.copyFrom(md5.getDigest()))\n          .setCrcType(PBHelper.convert(checksum.getChecksumType()))\n          )\n        .build()\n        .writeDelimitedTo(out);\n      out.flush();\n    } finally {\n      IOUtils.closeStream(out);\n      IOUtils.closeStream(checksumIn);\n      IOUtils.closeStream(metadataIn);\n    }\n\n    //update metrics\n    datanode.metrics.addBlockChecksumOp(elapsed());\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report accurately identifies the root cause related to the ground truth method 'DataXceiver.blockChecksum', as it mentions the arithmetic error due to 'crcPerBlock' leading to a division by zero. However, there is no fix suggestion provided in the bug report. The problem location is partially identified as the report mentions 'DFSClient.getFileChecksum', which is in the shared stack trace context with the ground truth method. There is no wrong information in the bug report as all details are relevant to the context of the bug."
        }
    },
    {
        "filename": "HDFS-8070.json",
        "code_diff": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.Receiver.opRequestShortCircuitFds": {
                "code_before_change": "  private void opRequestShortCircuitFds(DataInputStream in) throws IOException {\n    final OpRequestShortCircuitAccessProto proto =\n      OpRequestShortCircuitAccessProto.parseFrom(vintPrefixed(in));\n    SlotId slotId = (proto.hasSlotId()) ? \n        PBHelper.convert(proto.getSlotId()) : null;\n    TraceScope traceScope = continueTraceSpan(proto.getHeader(),\n        proto.getClass().getSimpleName());\n    try {\n      requestShortCircuitFds(PBHelper.convert(proto.getHeader().getBlock()),\n          PBHelper.convert(proto.getHeader().getToken()),\n          slotId, proto.getMaxVersion(), true);\n    } finally {\n      if (traceScope != null) traceScope.close();\n    }\n  }",
                "code_after_change": "  private void opRequestShortCircuitFds(DataInputStream in) throws IOException {\n    final OpRequestShortCircuitAccessProto proto =\n      OpRequestShortCircuitAccessProto.parseFrom(vintPrefixed(in));\n    SlotId slotId = (proto.hasSlotId()) ? \n        PBHelper.convert(proto.getSlotId()) : null;\n    TraceScope traceScope = continueTraceSpan(proto.getHeader(),\n        proto.getClass().getSimpleName());\n    try {\n      requestShortCircuitFds(PBHelper.convert(proto.getHeader().getBlock()),\n          PBHelper.convert(proto.getHeader().getToken()),\n          slotId, proto.getMaxVersion(),\n          proto.getSupportsReceiptVerification());\n    } finally {\n      if (traceScope != null) traceScope.close();\n    }\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.BlockReaderFactory.requestFileDescriptors": {
                "code_before_change": "  private ShortCircuitReplicaInfo requestFileDescriptors(DomainPeer peer,\n          Slot slot) throws IOException {\n    ShortCircuitCache cache = clientContext.getShortCircuitCache();\n    final DataOutputStream out =\n        new DataOutputStream(new BufferedOutputStream(peer.getOutputStream()));\n    SlotId slotId = slot == null ? null : slot.getSlotId();\n    new Sender(out).requestShortCircuitFds(block, token, slotId, 1, true);\n    DataInputStream in = new DataInputStream(peer.getInputStream());\n    BlockOpResponseProto resp = BlockOpResponseProto.parseFrom(\n        PBHelper.vintPrefixed(in));\n    DomainSocket sock = peer.getDomainSocket();\n    failureInjector.injectRequestFileDescriptorsFailure();\n    switch (resp.getStatus()) {\n    case SUCCESS:\n      byte buf[] = new byte[1];\n      FileInputStream fis[] = new FileInputStream[2];\n      sock.recvFileInputStreams(fis, buf, 0, buf.length);\n      ShortCircuitReplica replica = null;\n      try {\n        ExtendedBlockId key =\n            new ExtendedBlockId(block.getBlockId(), block.getBlockPoolId());\n        if (buf[0] == USE_RECEIPT_VERIFICATION.getNumber()) {\n          LOG.trace(\"Sending receipt verification byte for slot \" + slot);\n          sock.getOutputStream().write(0);\n        }\n        replica = new ShortCircuitReplica(key, fis[0], fis[1], cache,\n            Time.monotonicNow(), slot);\n        return new ShortCircuitReplicaInfo(replica);\n      } catch (IOException e) {\n        // This indicates an error reading from disk, or a format error.  Since\n        // it's not a socket communication problem, we return null rather than\n        // throwing an exception.\n        LOG.warn(this + \": error creating ShortCircuitReplica.\", e);\n        return null;\n      } finally {\n        if (replica == null) {\n          IOUtils.cleanup(DFSClient.LOG, fis[0], fis[1]);\n        }\n      }\n    case ERROR_UNSUPPORTED:\n      if (!resp.hasShortCircuitAccessVersion()) {\n        LOG.warn(\"short-circuit read access is disabled for \" +\n            \"DataNode \" + datanode + \".  reason: \" + resp.getMessage());\n        clientContext.getDomainSocketFactory()\n            .disableShortCircuitForPath(pathInfo.getPath());\n      } else {\n        LOG.warn(\"short-circuit read access for the file \" +\n            fileName + \" is disabled for DataNode \" + datanode +\n            \".  reason: \" + resp.getMessage());\n      }\n      return null;\n    case ERROR_ACCESS_TOKEN:\n      String msg = \"access control error while \" +\n          \"attempting to set up short-circuit access to \" +\n          fileName + resp.getMessage();\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(this + \":\" + msg);\n      }\n      return new ShortCircuitReplicaInfo(new InvalidToken(msg));\n    default:\n      LOG.warn(this + \": unknown response code \" + resp.getStatus() +\n          \" while attempting to set up short-circuit access. \" +\n          resp.getMessage());\n      clientContext.getDomainSocketFactory()\n          .disableShortCircuitForPath(pathInfo.getPath());\n      return null;\n    }\n  }",
                "code_after_change": "  private ShortCircuitReplicaInfo requestFileDescriptors(DomainPeer peer,\n          Slot slot) throws IOException {\n    ShortCircuitCache cache = clientContext.getShortCircuitCache();\n    final DataOutputStream out =\n        new DataOutputStream(new BufferedOutputStream(peer.getOutputStream()));\n    SlotId slotId = slot == null ? null : slot.getSlotId();\n    new Sender(out).requestShortCircuitFds(block, token, slotId, 1,\n        failureInjector.getSupportsReceiptVerification());\n    DataInputStream in = new DataInputStream(peer.getInputStream());\n    BlockOpResponseProto resp = BlockOpResponseProto.parseFrom(\n        PBHelper.vintPrefixed(in));\n    DomainSocket sock = peer.getDomainSocket();\n    failureInjector.injectRequestFileDescriptorsFailure();\n    switch (resp.getStatus()) {\n    case SUCCESS:\n      byte buf[] = new byte[1];\n      FileInputStream fis[] = new FileInputStream[2];\n      sock.recvFileInputStreams(fis, buf, 0, buf.length);\n      ShortCircuitReplica replica = null;\n      try {\n        ExtendedBlockId key =\n            new ExtendedBlockId(block.getBlockId(), block.getBlockPoolId());\n        if (buf[0] == USE_RECEIPT_VERIFICATION.getNumber()) {\n          LOG.trace(\"Sending receipt verification byte for slot \" + slot);\n          sock.getOutputStream().write(0);\n        }\n        replica = new ShortCircuitReplica(key, fis[0], fis[1], cache,\n            Time.monotonicNow(), slot);\n        return new ShortCircuitReplicaInfo(replica);\n      } catch (IOException e) {\n        // This indicates an error reading from disk, or a format error.  Since\n        // it's not a socket communication problem, we return null rather than\n        // throwing an exception.\n        LOG.warn(this + \": error creating ShortCircuitReplica.\", e);\n        return null;\n      } finally {\n        if (replica == null) {\n          IOUtils.cleanup(DFSClient.LOG, fis[0], fis[1]);\n        }\n      }\n    case ERROR_UNSUPPORTED:\n      if (!resp.hasShortCircuitAccessVersion()) {\n        LOG.warn(\"short-circuit read access is disabled for \" +\n            \"DataNode \" + datanode + \".  reason: \" + resp.getMessage());\n        clientContext.getDomainSocketFactory()\n            .disableShortCircuitForPath(pathInfo.getPath());\n      } else {\n        LOG.warn(\"short-circuit read access for the file \" +\n            fileName + \" is disabled for DataNode \" + datanode +\n            \".  reason: \" + resp.getMessage());\n      }\n      return null;\n    case ERROR_ACCESS_TOKEN:\n      String msg = \"access control error while \" +\n          \"attempting to set up short-circuit access to \" +\n          fileName + resp.getMessage();\n      if (LOG.isDebugEnabled()) {\n        LOG.debug(this + \":\" + msg);\n      }\n      return new ShortCircuitReplicaInfo(new InvalidToken(msg));\n    default:\n      LOG.warn(this + \": unknown response code \" + resp.getStatus() +\n          \" while attempting to set up short-circuit access. \" +\n          resp.getMessage());\n      clientContext.getDomainSocketFactory()\n          .disableShortCircuitForPath(pathInfo.getPath());\n      return null;\n    }\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report mentions issues related to short-circuit shared memory and exceptions in the stack trace, which are related to the ground truth methods. However, it does not precisely identify the root cause in the ground truth methods but is in the shared stack trace context. There is no explicit fix suggestion provided in the bug report. The problem location is partially identified as it mentions methods in the shared stack trace context but not the exact ground truth methods. There is no wrong information as the report discusses relevant issues and exceptions."
        }
    },
    {
        "filename": "HDFS-12339.json",
        "code_diff": {
            "hadoop-hdfs-project.hadoop-hdfs-nfs.src.main.java.org.apache.hadoop.hdfs.nfs.nfs3.Nfs3.startServiceInternal": {
                "code_before_change": "  public void startServiceInternal(boolean register) throws IOException {\n    mountd.start(register); // Start mountd\n    start(register);\n  }",
                "code_after_change": "  public void startServiceInternal(boolean register) throws IOException {\n    mountd.start(register); // Start mountd\n    start(register);\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs-nfs.src.main.java.org.apache.hadoop.hdfs.nfs.nfs3.PrivilegedNfsGatewayStarter.init": {
                "code_before_change": "  public void init(DaemonContext context) throws Exception {\n    System.err.println(\"Initializing privileged NFS client socket...\");\n    NfsConfiguration conf = new NfsConfiguration();\n    int clientPort = conf.getInt(NfsConfigKeys.DFS_NFS_REGISTRATION_PORT_KEY,\n        NfsConfigKeys.DFS_NFS_REGISTRATION_PORT_DEFAULT);\n    if (clientPort < 1 || clientPort > 1023) {\n      throw new RuntimeException(\"Must start privileged NFS server with '\" +\n          NfsConfigKeys.DFS_NFS_REGISTRATION_PORT_KEY + \"' configured to a \" +\n          \"privileged port.\");\n    }\n\n    try {\n      InetSocketAddress socketAddress =\n                new InetSocketAddress(\"localhost\", clientPort);\n      registrationSocket = new DatagramSocket(null);\n      registrationSocket.setReuseAddress(true);\n      registrationSocket.bind(socketAddress);\n    } catch (SocketException e) {\n      LOG.error(\"Init failed for port=\" + clientPort, e);\n      throw e;\n    }\n    args = context.getArguments();\n  }",
                "code_after_change": "  public void init(DaemonContext context) throws Exception {\n    System.err.println(\"Initializing privileged NFS client socket...\");\n    NfsConfiguration conf = new NfsConfiguration();\n    int clientPort = conf.getInt(NfsConfigKeys.DFS_NFS_REGISTRATION_PORT_KEY,\n        NfsConfigKeys.DFS_NFS_REGISTRATION_PORT_DEFAULT);\n    if (clientPort < 1 || clientPort > 1023) {\n      throw new RuntimeException(\"Must start privileged NFS server with '\" +\n          NfsConfigKeys.DFS_NFS_REGISTRATION_PORT_KEY + \"' configured to a \" +\n          \"privileged port.\");\n    }\n\n    try {\n      InetSocketAddress socketAddress =\n                new InetSocketAddress(\"localhost\", clientPort);\n      registrationSocket = new DatagramSocket(null);\n      registrationSocket.setReuseAddress(true);\n      registrationSocket.bind(socketAddress);\n    } catch (SocketException e) {\n      LOG.error(\"Init failed for port=\" + clientPort, e);\n      throw e;\n    }\n    args = context.getArguments();\n  }"
            },
            "hadoop-common-project.hadoop-nfs.src.main.java.org.apache.hadoop.mount.MountdBase.start": {
                "code_before_change": "  public void start(boolean register) {\n    startUDPServer();\n    startTCPServer();\n    if (register) {\n      ShutdownHookManager.get().addShutdownHook(new Unregister(),\n          SHUTDOWN_HOOK_PRIORITY);\n      try {\n        rpcProgram.register(PortmapMapping.TRANSPORT_UDP, udpBoundPort);\n        rpcProgram.register(PortmapMapping.TRANSPORT_TCP, tcpBoundPort);\n      } catch (Throwable e) {\n        LOG.error(\"Failed to register the MOUNT service.\", e);\n        terminate(1, e);\n      }\n    }\n  }",
                "code_after_change": "  public void start(boolean register) {\n    startUDPServer();\n    startTCPServer();\n    if (register) {\n      ShutdownHookManager.get().addShutdownHook(new Unregister(),\n          SHUTDOWN_HOOK_PRIORITY);\n      try {\n        rpcProgram.register(PortmapMapping.TRANSPORT_UDP, udpBoundPort);\n        rpcProgram.register(PortmapMapping.TRANSPORT_TCP, tcpBoundPort);\n      } catch (Throwable e) {\n        LOG.error(\"Failed to register the MOUNT service.\", e);\n        terminate(1, e);\n      }\n    }\n  }"
            },
            "hadoop-common-project.hadoop-nfs.src.main.java.org.apache.hadoop.nfs.nfs3.Nfs3Base.stop": {
                "code_before_change": [],
                "code_after_change": "  public void stop() {\n    if (nfsBoundPort > 0) {\n      rpcProgram.unregister(PortmapMapping.TRANSPORT_TCP, nfsBoundPort);\n      nfsBoundPort = 0;\n    }\n    rpcProgram.stopDaemons();\n  }"
            },
            "hadoop-common-project.hadoop-nfs.src.main.java.org.apache.hadoop.mount.MountdBase.stop": {
                "code_before_change": [],
                "code_after_change": "  public void stop() {\n    if (udpBoundPort > 0) {\n      rpcProgram.unregister(PortmapMapping.TRANSPORT_UDP, udpBoundPort);\n      udpBoundPort = 0;\n    }\n    if (tcpBoundPort > 0) {\n      rpcProgram.unregister(PortmapMapping.TRANSPORT_TCP, tcpBoundPort);\n      tcpBoundPort = 0;\n    }\n  }"
            },
            "hadoop-common-project.hadoop-nfs.src.main.java.org.apache.hadoop.nfs.nfs3.Nfs3Base.startTCPServer": {
                "code_before_change": "  private void startTCPServer() {\n    SimpleTcpServer tcpServer = new SimpleTcpServer(rpcProgram.getPort(),\n        rpcProgram, 0);\n    rpcProgram.startDaemons();\n    try {\n      tcpServer.run();\n    } catch (Throwable e) {\n      LOG.error(\"Failed to start the TCP server.\", e);\n      if (tcpServer.getBoundPort() > 0) {\n        rpcProgram.unregister(PortmapMapping.TRANSPORT_TCP,\n            tcpServer.getBoundPort());\n      }\n      tcpServer.shutdown();\n      terminate(1, e);\n    }\n    nfsBoundPort = tcpServer.getBoundPort();\n  }",
                "code_after_change": "  private void startTCPServer() {\n    SimpleTcpServer tcpServer = new SimpleTcpServer(rpcProgram.getPort(),\n        rpcProgram, 0);\n    rpcProgram.startDaemons();\n    try {\n      tcpServer.run();\n    } catch (Throwable e) {\n      LOG.error(\"Failed to start the TCP server.\", e);\n      if (tcpServer.getBoundPort() > 0) {\n        rpcProgram.unregister(PortmapMapping.TRANSPORT_TCP,\n            tcpServer.getBoundPort());\n      }\n      tcpServer.shutdown();\n      terminate(1, e);\n    }\n    nfsBoundPort = tcpServer.getBoundPort();\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs-nfs.src.main.java.org.apache.hadoop.hdfs.nfs.nfs3.Nfs3.startService": {
                "code_before_change": "  static void startService(String[] args,\n      DatagramSocket registrationSocket) throws IOException {\n    StringUtils.startupShutdownMessage(Nfs3.class, args, LOG);\n    NfsConfiguration conf = new NfsConfiguration();\n    boolean allowInsecurePorts = conf.getBoolean(\n        NfsConfigKeys.DFS_NFS_PORT_MONITORING_DISABLED_KEY,\n        NfsConfigKeys.DFS_NFS_PORT_MONITORING_DISABLED_DEFAULT);\n    final Nfs3 nfsServer = new Nfs3(conf, registrationSocket,\n        allowInsecurePorts);\n    nfsServer.startServiceInternal(true);\n  }",
                "code_after_change": "  static Nfs3 startService(String[] args,\n      DatagramSocket registrationSocket) throws IOException {\n    StringUtils.startupShutdownMessage(Nfs3.class, args, LOG);\n    NfsConfiguration conf = new NfsConfiguration();\n    boolean allowInsecurePorts = conf.getBoolean(\n        NfsConfigKeys.DFS_NFS_PORT_MONITORING_DISABLED_KEY,\n        NfsConfigKeys.DFS_NFS_PORT_MONITORING_DISABLED_DEFAULT);\n    final Nfs3 nfsServer = new Nfs3(conf, registrationSocket,\n        allowInsecurePorts);\n    nfsServer.startServiceInternal(true);\n    return nfsServer;\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report mentions an unregistration failure and a socket exception in the stack trace, which is related to the ground truth methods as they involve starting and stopping services that would require registration and unregistration. However, it does not precisely identify the root cause in the ground truth methods. The report does not provide any fix suggestion, hence 'Missing' for fix suggestion. The problem location is partially identified as it mentions methods in the stack trace that are related to the ground truth methods, but not the exact methods. There is no wrong information as the report accurately describes the error context."
        }
    },
    {
        "filename": "HDFS-6520.json",
        "code_diff": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NamenodeFsck.copyBlock": {
                "code_before_change": "  private void copyBlock(DFSClient dfs, LocatedBlock lblock,\n                         OutputStream fos) throws Exception {\n    int failures = 0;\n    InetSocketAddress targetAddr = null;\n    TreeSet<DatanodeInfo> deadNodes = new TreeSet<DatanodeInfo>();\n    BlockReader blockReader = null; \n    ExtendedBlock block = lblock.getBlock(); \n\n    while (blockReader == null) {\n      DatanodeInfo chosenNode;\n      \n      try {\n        chosenNode = bestNode(dfs, lblock.getLocations(), deadNodes);\n        targetAddr = NetUtils.createSocketAddr(chosenNode.getXferAddr());\n      }  catch (IOException ie) {\n        if (failures >= DFSConfigKeys.DFS_CLIENT_MAX_BLOCK_ACQUIRE_FAILURES_DEFAULT) {\n          throw new IOException(\"Could not obtain block \" + lblock, ie);\n        }\n        LOG.info(\"Could not obtain block from any node:  \" + ie);\n        try {\n          Thread.sleep(10000);\n        }  catch (InterruptedException iex) {\n        }\n        deadNodes.clear();\n        failures++;\n        continue;\n      }\n      try {\n        String file = BlockReaderFactory.getFileName(targetAddr,\n            block.getBlockPoolId(), block.getBlockId());\n        blockReader = new BlockReaderFactory(dfs.getConf()).\n            setFileName(file).\n            setBlock(block).\n            setBlockToken(lblock.getBlockToken()).\n            setStartOffset(0).\n            setLength(-1).\n            setVerifyChecksum(true).\n            setClientName(\"fsck\").\n            setDatanodeInfo(chosenNode).\n            setInetSocketAddress(targetAddr).\n            setCachingStrategy(CachingStrategy.newDropBehind()).\n            setClientCacheContext(dfs.getClientContext()).\n            setConfiguration(namenode.conf).\n            setRemotePeerFactory(new RemotePeerFactory() {\n              @Override\n              public Peer newConnectedPeer(InetSocketAddress addr)\n                  throws IOException {\n                Peer peer = null;\n                Socket s = NetUtils.getDefaultSocketFactory(conf).createSocket();\n                try {\n                  s.connect(addr, HdfsServerConstants.READ_TIMEOUT);\n                  s.setSoTimeout(HdfsServerConstants.READ_TIMEOUT);\n                  peer = TcpPeerServer.peerFromSocketAndKey(s, namenode.getRpcServer().\n                        getDataEncryptionKey());\n                } finally {\n                  if (peer == null) {\n                    IOUtils.closeQuietly(s);\n                  }\n                }\n                return peer;\n              }\n            }).\n            build();\n      }  catch (IOException ex) {\n        // Put chosen node into dead list, continue\n        LOG.info(\"Failed to connect to \" + targetAddr + \":\" + ex);\n        deadNodes.add(chosenNode);\n      }\n    }\n    byte[] buf = new byte[1024];\n    int cnt = 0;\n    boolean success = true;\n    long bytesRead = 0;\n    try {\n      while ((cnt = blockReader.read(buf, 0, buf.length)) > 0) {\n        fos.write(buf, 0, cnt);\n        bytesRead += cnt;\n      }\n      if ( bytesRead != block.getNumBytes() ) {\n        throw new IOException(\"Recorded block size is \" + block.getNumBytes() + \n                              \", but datanode returned \" +bytesRead+\" bytes\");\n      }\n    } catch (Exception e) {\n      LOG.error(\"Error reading block\", e);\n      success = false;\n    } finally {\n      blockReader.close();\n    }\n    if (!success) {\n      throw new Exception(\"Could not copy block data for \" + lblock.getBlock());\n    }\n  }",
                "code_after_change": "  private void copyBlock(final DFSClient dfs, LocatedBlock lblock,\n                         OutputStream fos) throws Exception {\n    int failures = 0;\n    InetSocketAddress targetAddr = null;\n    TreeSet<DatanodeInfo> deadNodes = new TreeSet<DatanodeInfo>();\n    BlockReader blockReader = null;\n    ExtendedBlock block = lblock.getBlock();\n\n    while (blockReader == null) {\n      DatanodeInfo chosenNode;\n\n      try {\n        chosenNode = bestNode(dfs, lblock.getLocations(), deadNodes);\n        targetAddr = NetUtils.createSocketAddr(chosenNode.getXferAddr());\n      }  catch (IOException ie) {\n        if (failures >= HdfsClientConfigKeys.DFS_CLIENT_MAX_BLOCK_ACQUIRE_FAILURES_DEFAULT) {\n          throw new IOException(\"Could not obtain block \" + lblock, ie);\n        }\n        LOG.info(\"Could not obtain block from any node:  \" + ie);\n        try {\n          Thread.sleep(10000);\n        }  catch (InterruptedException iex) {\n        }\n        deadNodes.clear();\n        failures++;\n        continue;\n      }\n      try {\n        String file = BlockReaderFactory.getFileName(targetAddr,\n            block.getBlockPoolId(), block.getBlockId());\n        blockReader = new BlockReaderFactory(dfs.getConf()).\n            setFileName(file).\n            setBlock(block).\n            setBlockToken(lblock.getBlockToken()).\n            setStartOffset(0).\n            setLength(block.getNumBytes()).\n            setVerifyChecksum(true).\n            setClientName(\"fsck\").\n            setDatanodeInfo(chosenNode).\n            setInetSocketAddress(targetAddr).\n            setCachingStrategy(CachingStrategy.newDropBehind()).\n            setClientCacheContext(dfs.getClientContext()).\n            setConfiguration(namenode.getConf()).\n            setTracer(tracer).\n            setRemotePeerFactory(new RemotePeerFactory() {\n              @Override\n              public Peer newConnectedPeer(InetSocketAddress addr,\n                  Token<BlockTokenIdentifier> blockToken, DatanodeID datanodeId)\n                  throws IOException {\n                Peer peer = null;\n                Socket s = NetUtils.getDefaultSocketFactory(conf).createSocket();\n                try {\n                  s.connect(addr, HdfsConstants.READ_TIMEOUT);\n                  s.setSoTimeout(HdfsConstants.READ_TIMEOUT);\n                  peer = DFSUtilClient.peerFromSocketAndKey(\n                        dfs.getSaslDataTransferClient(), s, NamenodeFsck.this,\n                        blockToken, datanodeId, HdfsConstants.READ_TIMEOUT);\n                } finally {\n                  if (peer == null) {\n                    IOUtils.closeQuietly(s);\n                  }\n                }\n                return peer;\n              }\n            }).\n            build();\n      }  catch (IOException ex) {\n        // Put chosen node into dead list, continue\n        LOG.info(\"Failed to connect to \" + targetAddr + \":\" + ex);\n        deadNodes.add(chosenNode);\n      }\n    }\n    byte[] buf = new byte[1024];\n    int cnt = 0;\n    boolean success = true;\n    long bytesRead = 0;\n    try {\n      while ((cnt = blockReader.read(buf, 0, buf.length)) > 0) {\n        fos.write(buf, 0, cnt);\n        bytesRead += cnt;\n      }\n      if ( bytesRead != block.getNumBytes() ) {\n        throw new IOException(\"Recorded block size is \" + block.getNumBytes() +\n                              \", but datanode returned \" +bytesRead+\" bytes\");\n      }\n    } catch (Exception e) {\n      LOG.error(\"Error reading block\", e);\n      success = false;\n    } finally {\n      blockReader.close();\n    }\n    if (!success) {\n      throw new Exception(\"Could not copy block data for \" + lblock.getBlock());\n    }\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.BlockReaderFactory.setFailureInjectorForTesting": {
                "code_before_change": [],
                "code_after_change": "  public static void setFailureInjectorForTesting(FailureInjector injector) {\n    failureInjector = injector;\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.BlockReaderFactory.getSupportsReceiptVerification": {
                "code_before_change": [],
                "code_after_change": "    public boolean getSupportsReceiptVerification() {\n      return true;\n    }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the issue occurring during the 'fsck -move' operation and provides a stack trace that includes the method 'NamenodeFsck.copyBlock', which is a ground truth method. However, it does not explicitly identify this method as the root cause, hence it is classified as 'Partial' with 'Shared Stack Trace Context'. There is no fix suggestion provided in the bug report, so it is marked as 'Missing'. The problem location is also identified through the stack trace, but not directly mentioned, so it is 'Partial' with 'Shared Stack Trace Context'. There is no incorrect information in the bug report, so 'Wrong Information' is 'No'."
        }
    },
    {
        "filename": "HDFS-10715.json",
        "code_diff": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.AvailableSpaceBlockPlacementPolicy.chooseDataNode": {
                "code_before_change": "  protected DatanodeDescriptor chooseDataNode(final String scope,\n      final Collection<Node> excludedNode) {\n    DatanodeDescriptor a =\n        (DatanodeDescriptor) clusterMap.chooseRandom(scope, excludedNode);\n    DatanodeDescriptor b =\n        (DatanodeDescriptor) clusterMap.chooseRandom(scope, excludedNode);\n    int ret = compareDataNode(a, b);\n    if (ret == 0) {\n      return a;\n    } else if (ret < 0) {\n      return (RAND.nextInt(100) < balancedPreference) ? a : b;\n    } else {\n      return (RAND.nextInt(100) < balancedPreference) ? b : a;\n    }\n  }",
                "code_after_change": "  protected DatanodeDescriptor chooseDataNode(final String scope,\n      final Collection<Node> excludedNode) {\n    DatanodeDescriptor a =\n        (DatanodeDescriptor) clusterMap.chooseRandom(scope, excludedNode);\n    DatanodeDescriptor b =\n        (DatanodeDescriptor) clusterMap.chooseRandom(scope, excludedNode);\n    if (a != null && b != null){\n      int ret = compareDataNode(a, b);\n      if (ret == 0) {\n        return a;\n      } else if (ret < 0) {\n        return (RAND.nextInt(100) < balancedPreference) ? a : b;\n      } else {\n        return (RAND.nextInt(100) < balancedPreference) ? b : a;\n      }\n    } else {\n      return a == null ? b : a;\n    }\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Correct",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report precisely identifies the root cause in the method 'chooseDataNode', which is the ground truth method. The report correctly suggests that 'clusterMap.chooseRandom' may return null, which matches the developer's fix of adding null checks for 'a' and 'b'. The problem location is also precisely identified as 'chooseDataNode', which is the ground truth method. There is no wrong information in the bug report as all details are consistent with the context of the bug."
        }
    },
    {
        "filename": "HDFS-3332.json",
        "code_diff": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BPServiceActor.scheduleBlockReport": {
                "code_before_change": "  void scheduleBlockReport(long delay) {\n    if (delay > 0) { // send BR after random delay\n      lastBlockReport = System.currentTimeMillis()\n      - ( dnConf.blockReportInterval - DFSUtil.getRandom().nextInt((int)(delay)));\n    } else { // send at next heartbeat\n      lastBlockReport = lastHeartbeat - dnConf.blockReportInterval;\n    }\n    resetBlockReportTime = true; // reset future BRs for randomness\n  }",
                "code_after_change": "  void scheduleBlockReport(long delay) {\n    if (delay > 0) { // send BR after random delay\n      lastBlockReport = System.currentTimeMillis()\n      - ( dnConf.blockReportInterval - DFSUtil.getRandom().nextInt((int)(delay)));\n    } else { // send at next heartbeat\n      lastBlockReport = lastHeartbeat - dnConf.blockReportInterval;\n    }\n    resetBlockReportTime = true; // reset future BRs for randomness\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Direct Caller/Callee"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Direct Caller/Callee"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the method 'BPServiceActor.reportBadBlocks' in the stack trace, which is a direct caller of the ground truth method 'BPServiceActor.scheduleBlockReport'. Therefore, the root cause identification is partial with a sub-category of Direct Caller/Callee. The report does not provide any fix suggestion, so it is marked as missing. The problem location identification is also partial with the same sub-category because it points to a method directly related to the ground truth method. There is no wrong information in the bug report as all the details are relevant to the context of the bug."
        }
    },
    {
        "filename": "HDFS-6130.json",
        "code_diff": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImageFormat.loadFullNameINodes": {
                "code_before_change": "  private void loadFullNameINodes(long numFiles, DataInput in, Counter counter)\n      throws IOException {\n    byte[][] pathComponents;\n    byte[][] parentPath = {{}};      \n    FSDirectory fsDir = namesystem.dir;\n    INodeDirectory parentINode = fsDir.rootDir;\n    for (long i = 0; i < numFiles; i++) {\n      pathComponents = FSImageSerialization.readPathComponents(in);\n      final INode newNode = loadINode(\n          pathComponents[pathComponents.length-1], false, in, counter);\n\n      if (isRoot(pathComponents)) { // it is the root\n        // update the root's attributes\n        updateRootAttr(newNode.asDirectory());\n        continue;\n      }\n      // check if the new inode belongs to the same parent\n      if(!isParent(pathComponents, parentPath)) {\n        parentINode = getParentINodeDirectory(pathComponents);\n        parentPath = getParent(pathComponents);\n      }\n\n      // add new inode\n      addToParent(parentINode, newNode);\n    }\n  }",
                "code_after_change": "  private void loadFullNameINodes(long numFiles, DataInput in, Counter counter)\n      throws IOException {\n    byte[][] pathComponents;\n    byte[][] parentPath = {{}};      \n    FSDirectory fsDir = namesystem.dir;\n    INodeDirectory parentINode = fsDir.rootDir;\n    for (long i = 0; i < numFiles; i++) {\n      pathComponents = FSImageSerialization.readPathComponents(in);\n      final INode newNode = loadINode(\n          pathComponents[pathComponents.length-1], false, in, counter);\n\n      if (isRoot(pathComponents)) { // it is the root\n        // update the root's attributes\n        updateRootAttr(newNode.asDirectory());\n        continue;\n      }\n\n      namesystem.dir.addToInodeMap(newNode);\n      // check if the new inode belongs to the same parent\n      if(!isParent(pathComponents, parentPath)) {\n        parentINode = getParentINodeDirectory(pathComponents);\n        parentPath = getParent(pathComponents);\n      }\n\n      // add new inode\n      addToParent(parentINode, newNode);\n    }\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "1. **Root Cause Identification**: The bug report mentions a NullPointerException (NPE) occurring in the stack trace, specifically in methods like `FSDirectory.isReservedName` and `FSImageFormatPBINode.Loader.addToParent`, which are in the same stack trace as the ground truth method `FSImageFormat.loadFullNameINodes`. However, it does not precisely identify the root cause in the ground truth method.\n\n2. **Fix Suggestion**: The bug report does not provide any suggestions for fixing the bug. There is no `Suggestions` or `possible_fix` field, nor is there any mention of a fix in the `Description` field.\n\n3. **Problem Location Identification**: The bug report includes a stack trace that mentions methods related to the problem, such as `FSDirectory.isReservedName` and `FSImageFormatPBINode.Loader.addToParent`, which are in the same stack trace as the ground truth method. However, it does not precisely identify the ground truth method itself.\n\n4. **Wrong Information**: The bug report does not contain any incorrect or unrelated information. All the information provided is relevant to the context of the bug."
        }
    },
    {
        "filename": "HDFS-2827.json",
        "code_diff": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.unprotectedChangeLease": {
                "code_before_change": "  void unprotectedChangeLease(String src, String dst, HdfsFileStatus dinfo) {\n    String overwrite;\n    String replaceBy;\n    assert hasWriteLock();\n\n    boolean destinationExisted = true;\n    if (dinfo == null) {\n      destinationExisted = false;\n    }\n\n    if (destinationExisted && dinfo.isDir()) {\n      Path spath = new Path(src);\n      overwrite = spath.getParent().toString() + Path.SEPARATOR;\n      replaceBy = dst + Path.SEPARATOR;\n    } else {\n      overwrite = src;\n      replaceBy = dst;\n    }\n\n    leaseManager.changeLease(src, dst, overwrite, replaceBy);\n  }",
                "code_after_change": "  void unprotectedChangeLease(String src, String dst, HdfsFileStatus dinfo) {\n    String overwrite;\n    String replaceBy;\n    assert hasWriteLock();\n\n    boolean destinationExisted = true;\n    if (dinfo == null) {\n      destinationExisted = false;\n    }\n\n    if (destinationExisted && dinfo.isDir()) {\n      Path spath = new Path(src);\n      Path parent = spath.getParent();\n      if (isRoot(parent)) {\n        overwrite = parent.toString();\n      } else {\n        overwrite = parent.toString() + Path.SEPARATOR;\n      }\n      replaceBy = dst + Path.SEPARATOR;\n    } else {\n      overwrite = src;\n      replaceBy = dst;\n    }\n\n    leaseManager.changeLease(src, dst, overwrite, replaceBy);\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the issue occurring during a rename operation and mentions the stack trace, which includes 'FSNamesystem.saveFilesUnderConstruction'. This method is in the same stack trace as the ground truth method 'FSNamesystem.unprotectedChangeLease', hence 'Shared Stack Trace Context'. However, it does not precisely identify the root cause or the exact method where the fix was applied. There is no fix suggestion provided in the bug report, hence 'Missing' for fix suggestion. The problem location is also identified in the shared stack trace context but not precisely at the ground truth method. There is no incorrect information in the bug report, so 'No' for wrong information."
        }
    },
    {
        "filename": "HDFS-11056.json",
        "code_diff": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetUtil.parseGenerationStamp": {
                "code_before_change": "  static long parseGenerationStamp(File blockFile, File metaFile\n      ) throws IOException {\n    final String metaname = metaFile.getName();\n    final String gs = metaname.substring(blockFile.getName().length() + 1,\n        metaname.length() - Block.METADATA_EXTENSION.length());\n    try {\n      return Long.parseLong(gs);\n    } catch(NumberFormatException nfe) {\n      throw new IOException(\"Failed to parse generation stamp: blockFile=\"\n          + blockFile + \", metaFile=\" + metaFile, nfe);\n    }\n  }",
                "code_after_change": "  static long parseGenerationStamp(File blockFile, File metaFile\n      ) throws IOException {\n    final String metaname = metaFile.getName();\n    final String gs = metaname.substring(blockFile.getName().length() + 1,\n        metaname.length() - Block.METADATA_EXTENSION.length());\n    try {\n      return Long.parseLong(gs);\n    } catch(NumberFormatException nfe) {\n      throw new IOException(\"Failed to parse generation stamp: blockFile=\"\n          + blockFile + \", metaFile=\" + metaFile, nfe);\n    }\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.updateReplicaUnderRecovery": {
                "code_before_change": "  public Replica updateReplicaUnderRecovery(\n                                    final ExtendedBlock oldBlock,\n                                    final long recoveryId,\n                                    final long newBlockId,\n                                    final long newlength) throws IOException {\n    try (AutoCloseableLock lock = datasetLock.acquire()) {\n      //get replica\n      final String bpid = oldBlock.getBlockPoolId();\n      final ReplicaInfo replica = volumeMap.get(bpid, oldBlock.getBlockId());\n      LOG.info(\"updateReplica: \" + oldBlock\n          + \", recoveryId=\" + recoveryId\n          + \", length=\" + newlength\n          + \", replica=\" + replica);\n\n      //check replica\n      if (replica == null) {\n        throw new ReplicaNotFoundException(oldBlock);\n      }\n\n      //check replica state\n      if (replica.getState() != ReplicaState.RUR) {\n        throw new IOException(\"replica.getState() != \" + ReplicaState.RUR\n            + \", replica=\" + replica);\n      }\n\n      //check replica's byte on disk\n      if (replica.getBytesOnDisk() != oldBlock.getNumBytes()) {\n        throw new IOException(\"THIS IS NOT SUPPOSED TO HAPPEN:\"\n            + \" replica.getBytesOnDisk() != block.getNumBytes(), block=\"\n            + oldBlock + \", replica=\" + replica);\n      }\n\n      //check replica files before update\n      checkReplicaFiles(replica);\n\n      //update replica\n      final ReplicaInfo finalized = updateReplicaUnderRecovery(oldBlock\n          .getBlockPoolId(), replica, recoveryId,\n          newBlockId, newlength);\n\n      boolean copyTruncate = newBlockId != oldBlock.getBlockId();\n      if (!copyTruncate) {\n        assert finalized.getBlockId() == oldBlock.getBlockId()\n            && finalized.getGenerationStamp() == recoveryId\n            && finalized.getNumBytes() == newlength\n            : \"Replica information mismatched: oldBlock=\" + oldBlock\n            + \", recoveryId=\" + recoveryId + \", newlength=\" + newlength\n            + \", newBlockId=\" + newBlockId + \", finalized=\" + finalized;\n      } else {\n        assert finalized.getBlockId() == oldBlock.getBlockId()\n            && finalized.getGenerationStamp() == oldBlock.getGenerationStamp()\n            && finalized.getNumBytes() == oldBlock.getNumBytes()\n            : \"Finalized and old information mismatched: oldBlock=\" + oldBlock\n            + \", genStamp=\" + oldBlock.getGenerationStamp()\n            + \", len=\" + oldBlock.getNumBytes()\n            + \", finalized=\" + finalized;\n      }\n      //check replica files after update\n      checkReplicaFiles(finalized);\n\n      return finalized;\n    }\n  }",
                "code_after_change": "  public synchronized String updateReplicaUnderRecovery(\n                                    final ExtendedBlock oldBlock,\n                                    final long recoveryId,\n                                    final long newBlockId,\n                                    final long newlength) throws IOException {\n    //get replica\n    final String bpid = oldBlock.getBlockPoolId();\n    final ReplicaInfo replica = volumeMap.get(bpid, oldBlock.getBlockId());\n    LOG.info(\"updateReplica: \" + oldBlock\n                 + \", recoveryId=\" + recoveryId\n                 + \", length=\" + newlength\n                 + \", replica=\" + replica);\n\n    //check replica\n    if (replica == null) {\n      throw new ReplicaNotFoundException(oldBlock);\n    }\n\n    //check replica state\n    if (replica.getState() != ReplicaState.RUR) {\n      throw new IOException(\"replica.getState() != \" + ReplicaState.RUR\n          + \", replica=\" + replica);\n    }\n\n    //check replica's byte on disk\n    if (replica.getBytesOnDisk() != oldBlock.getNumBytes()) {\n      throw new IOException(\"THIS IS NOT SUPPOSED TO HAPPEN:\"\n          + \" replica.getBytesOnDisk() != block.getNumBytes(), block=\"\n          + oldBlock + \", replica=\" + replica);\n    }\n\n    //check replica files before update\n    checkReplicaFiles(replica);\n\n    //update replica\n    final FinalizedReplica finalized = updateReplicaUnderRecovery(oldBlock\n        .getBlockPoolId(), (ReplicaUnderRecovery) replica, recoveryId,\n        newBlockId, newlength);\n\n    boolean copyTruncate = newBlockId != oldBlock.getBlockId();\n    if(!copyTruncate) {\n      assert finalized.getBlockId() == oldBlock.getBlockId()\n          && finalized.getGenerationStamp() == recoveryId\n          && finalized.getNumBytes() == newlength\n          : \"Replica information mismatched: oldBlock=\" + oldBlock\n              + \", recoveryId=\" + recoveryId + \", newlength=\" + newlength\n              + \", newBlockId=\" + newBlockId + \", finalized=\" + finalized;\n    } else {\n      assert finalized.getBlockId() == oldBlock.getBlockId()\n          && finalized.getGenerationStamp() == oldBlock.getGenerationStamp()\n          && finalized.getNumBytes() == oldBlock.getNumBytes()\n          : \"Finalized and old information mismatched: oldBlock=\" + oldBlock\n              + \", genStamp=\" + oldBlock.getGenerationStamp()\n              + \", len=\" + oldBlock.getNumBytes()\n              + \", finalized=\" + finalized;\n    }\n\n    //check replica files after update\n    checkReplicaFiles(finalized);\n\n    //return storage ID\n    return getVolume(new ExtendedBlock(bpid, finalized)).getStorageID();\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.convertTemporaryToRbw": {
                "code_before_change": "  public ReplicaInPipeline convertTemporaryToRbw(\n      final ExtendedBlock b) throws IOException {\n\n    try (AutoCloseableLock lock = datasetLock.acquire()) {\n      final long blockId = b.getBlockId();\n      final long expectedGs = b.getGenerationStamp();\n      final long visible = b.getNumBytes();\n      LOG.info(\"Convert \" + b + \" from Temporary to RBW, visible length=\"\n          + visible);\n\n      final ReplicaInfo temp;\n      {\n        // get replica\n        final ReplicaInfo r = volumeMap.get(b.getBlockPoolId(), blockId);\n        if (r == null) {\n          throw new ReplicaNotFoundException(\n              ReplicaNotFoundException.NON_EXISTENT_REPLICA + b);\n        }\n        // check the replica's state\n        if (r.getState() != ReplicaState.TEMPORARY) {\n          throw new ReplicaAlreadyExistsException(\n              \"r.getState() != ReplicaState.TEMPORARY, r=\" + r);\n        }\n        temp = r;\n      }\n      // check generation stamp\n      if (temp.getGenerationStamp() != expectedGs) {\n        throw new ReplicaAlreadyExistsException(\n            \"temp.getGenerationStamp() != expectedGs = \" + expectedGs\n                + \", temp=\" + temp);\n      }\n\n      // TODO: check writer?\n      // set writer to the current thread\n      // temp.setWriter(Thread.currentThread());\n\n      // check length\n      final long numBytes = temp.getNumBytes();\n      if (numBytes < visible) {\n        throw new IOException(numBytes + \" = numBytes < visible = \"\n            + visible + \", temp=\" + temp);\n      }\n      // check volume\n      final FsVolumeImpl v = (FsVolumeImpl) temp.getVolume();\n      if (v == null) {\n        throw new IOException(\"r.getVolume() = null, temp=\" + temp);\n      }\n\n      final ReplicaInPipeline rbw = v.convertTemporaryToRbw(b, temp);\n\n      if(rbw.getState() != ReplicaState.RBW) {\n        throw new IOException(\"Expected replica state: \" + ReplicaState.RBW\n            + \" obtained \" + rbw.getState() + \" for converting block \"\n            + b.getBlockId());\n      }\n      // overwrite the RBW in the volume map\n      volumeMap.add(b.getBlockPoolId(), rbw.getReplicaInfo());\n      return rbw;\n    }\n  }",
                "code_after_change": "  public synchronized ReplicaInPipeline convertTemporaryToRbw(\n      final ExtendedBlock b) throws IOException {\n    final long blockId = b.getBlockId();\n    final long expectedGs = b.getGenerationStamp();\n    final long visible = b.getNumBytes();\n    LOG.info(\"Convert \" + b + \" from Temporary to RBW, visible length=\"\n        + visible);\n\n    final ReplicaInPipeline temp;\n    {\n      // get replica\n      final ReplicaInfo r = volumeMap.get(b.getBlockPoolId(), blockId);\n      if (r == null) {\n        throw new ReplicaNotFoundException(\n            ReplicaNotFoundException.NON_EXISTENT_REPLICA + b);\n      }\n      // check the replica's state\n      if (r.getState() != ReplicaState.TEMPORARY) {\n        throw new ReplicaAlreadyExistsException(\n            \"r.getState() != ReplicaState.TEMPORARY, r=\" + r);\n      }\n      temp = (ReplicaInPipeline)r;\n    }\n    // check generation stamp\n    if (temp.getGenerationStamp() != expectedGs) {\n      throw new ReplicaAlreadyExistsException(\n          \"temp.getGenerationStamp() != expectedGs = \" + expectedGs\n          + \", temp=\" + temp);\n    }\n\n    // TODO: check writer?\n    // set writer to the current thread\n    // temp.setWriter(Thread.currentThread());\n\n    // check length\n    final long numBytes = temp.getNumBytes();\n    if (numBytes < visible) {\n      throw new IOException(numBytes + \" = numBytes < visible = \"\n          + visible + \", temp=\" + temp);\n    }\n    // check volume\n    final FsVolumeImpl v = (FsVolumeImpl)temp.getVolume();\n    if (v == null) {\n      throw new IOException(\"r.getVolume() = null, temp=\"  + temp);\n    }\n    \n    // move block files to the rbw directory\n    BlockPoolSlice bpslice = v.getBlockPoolSlice(b.getBlockPoolId());\n    final File dest = moveBlockFiles(b.getLocalBlock(), temp.getBlockFile(), \n        bpslice.getRbwDir());\n    // create RBW\n    final ReplicaBeingWritten rbw = new ReplicaBeingWritten(\n        blockId, numBytes, expectedGs,\n        v, dest.getParentFile(), Thread.currentThread(), 0);\n    rbw.setBytesAcked(visible);\n\n    // load last checksum and datalen\n    final File destMeta = FsDatasetUtil.getMetaFile(dest,\n        b.getGenerationStamp());\n    byte[] lastChunkChecksum = loadLastPartialChunkChecksum(dest, destMeta);\n    rbw.setLastChecksumAndDataLen(numBytes, lastChunkChecksum);\n    // overwrite the RBW in the volume map\n    volumeMap.add(b.getBlockPoolId(), rbw);\n    return rbw;\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.moveBlockAcrossStorage": {
                "code_before_change": "  public ReplicaInfo moveBlockAcrossStorage(ExtendedBlock block,\n      StorageType targetStorageType) throws IOException {\n    ReplicaInfo replicaInfo = getReplicaInfo(block);\n    if (replicaInfo.getState() != ReplicaState.FINALIZED) {\n      throw new ReplicaNotFoundException(\n          ReplicaNotFoundException.UNFINALIZED_REPLICA + block);\n    }\n    if (replicaInfo.getNumBytes() != block.getNumBytes()) {\n      throw new IOException(\"Corrupted replica \" + replicaInfo\n          + \" with a length of \" + replicaInfo.getNumBytes()\n          + \" expected length is \" + block.getNumBytes());\n    }\n    if (replicaInfo.getVolume().getStorageType() == targetStorageType) {\n      throw new ReplicaAlreadyExistsException(\"Replica \" + replicaInfo\n          + \" already exists on storage \" + targetStorageType);\n    }\n\n    if (replicaInfo.isOnTransientStorage()) {\n      // Block movement from RAM_DISK will be done by LazyPersist mechanism\n      throw new IOException(\"Replica \" + replicaInfo\n          + \" cannot be moved from storageType : \"\n          + replicaInfo.getVolume().getStorageType());\n    }\n\n    FsVolumeReference volumeRef = null;\n    try (AutoCloseableLock lock = datasetLock.acquire()) {\n      volumeRef = volumes.getNextVolume(targetStorageType, block.getNumBytes());\n    }\n    try {\n      moveBlock(block, replicaInfo, volumeRef);\n    } finally {\n      if (volumeRef != null) {\n        volumeRef.close();\n      }\n    }\n\n    // Replace the old block if any to reschedule the scanning.\n    return replicaInfo;\n  }",
                "code_after_change": "  public ReplicaInfo moveBlockAcrossStorage(ExtendedBlock block,\n      StorageType targetStorageType) throws IOException {\n    ReplicaInfo replicaInfo = getReplicaInfo(block);\n    if (replicaInfo.getState() != ReplicaState.FINALIZED) {\n      throw new ReplicaNotFoundException(\n          ReplicaNotFoundException.UNFINALIZED_REPLICA + block);\n    }\n    if (replicaInfo.getNumBytes() != block.getNumBytes()) {\n      throw new IOException(\"Corrupted replica \" + replicaInfo\n          + \" with a length of \" + replicaInfo.getNumBytes()\n          + \" expected length is \" + block.getNumBytes());\n    }\n    if (replicaInfo.getVolume().getStorageType() == targetStorageType) {\n      throw new ReplicaAlreadyExistsException(\"Replica \" + replicaInfo\n          + \" already exists on storage \" + targetStorageType);\n    }\n\n    if (replicaInfo.isOnTransientStorage()) {\n      // Block movement from RAM_DISK will be done by LazyPersist mechanism\n      throw new IOException(\"Replica \" + replicaInfo\n          + \" cannot be moved from storageType : \"\n          + replicaInfo.getVolume().getStorageType());\n    }\n\n    FsVolumeReference volumeRef = null;\n    synchronized (this) {\n      volumeRef = volumes.getNextVolume(targetStorageType, block.getNumBytes());\n    }\n    try {\n      File oldBlockFile = replicaInfo.getBlockFile();\n      File oldMetaFile = replicaInfo.getMetaFile();\n      FsVolumeImpl targetVolume = (FsVolumeImpl) volumeRef.getVolume();\n      // Copy files to temp dir first\n      File[] blockFiles = copyBlockFiles(block.getBlockId(),\n          block.getGenerationStamp(), oldMetaFile, oldBlockFile,\n          targetVolume.getTmpDir(block.getBlockPoolId()),\n          replicaInfo.isOnTransientStorage());\n\n      ReplicaInfo newReplicaInfo = new ReplicaInPipeline(\n          replicaInfo.getBlockId(), replicaInfo.getGenerationStamp(),\n          targetVolume, blockFiles[0].getParentFile(), 0);\n      newReplicaInfo.setNumBytes(blockFiles[1].length());\n      // Finalize the copied files\n      newReplicaInfo = finalizeReplica(block.getBlockPoolId(), newReplicaInfo);\n\n      removeOldReplica(replicaInfo, newReplicaInfo, oldBlockFile, oldMetaFile,\n          oldBlockFile.length(), oldMetaFile.length(), block.getBlockPoolId());\n    } finally {\n      if (volumeRef != null) {\n        volumeRef.close();\n      }\n    }\n\n    // Replace the old block if any to reschedule the scanning.\n    return replicaInfo;\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.append": {
                "code_before_change": "  public ReplicaHandler append(ExtendedBlock b,\n      long newGS, long expectedBlockLen) throws IOException {\n    try (AutoCloseableLock lock = datasetLock.acquire()) {\n      // If the block was successfully finalized because all packets\n      // were successfully processed at the Datanode but the ack for\n      // some of the packets were not received by the client. The client\n      // re-opens the connection and retries sending those packets.\n      // The other reason is that an \"append\" is occurring to this block.\n\n      // check the validity of the parameter\n      if (newGS < b.getGenerationStamp()) {\n        throw new IOException(\"The new generation stamp \" + newGS +\n            \" should be greater than the replica \" + b + \"'s generation stamp\");\n      }\n      ReplicaInfo replicaInfo = getReplicaInfo(b);\n      LOG.info(\"Appending to \" + replicaInfo);\n      if (replicaInfo.getState() != ReplicaState.FINALIZED) {\n        throw new ReplicaNotFoundException(\n            ReplicaNotFoundException.UNFINALIZED_REPLICA + b);\n      }\n      if (replicaInfo.getNumBytes() != expectedBlockLen) {\n        throw new IOException(\"Corrupted replica \" + replicaInfo +\n            \" with a length of \" + replicaInfo.getNumBytes() +\n            \" expected length is \" + expectedBlockLen);\n      }\n\n      FsVolumeReference ref = replicaInfo.getVolume().obtainReference();\n      ReplicaInPipeline replica = null;\n      try {\n        replica = append(b.getBlockPoolId(), replicaInfo, newGS,\n            b.getNumBytes());\n      } catch (IOException e) {\n        IOUtils.cleanup(null, ref);\n        throw e;\n      }\n      return new ReplicaHandler(replica, ref);\n    }\n  }",
                "code_after_change": "  public synchronized ReplicaHandler append(ExtendedBlock b,\n      long newGS, long expectedBlockLen) throws IOException {\n    // If the block was successfully finalized because all packets\n    // were successfully processed at the Datanode but the ack for\n    // some of the packets were not received by the client. The client \n    // re-opens the connection and retries sending those packets.\n    // The other reason is that an \"append\" is occurring to this block.\n    \n    // check the validity of the parameter\n    if (newGS < b.getGenerationStamp()) {\n      throw new IOException(\"The new generation stamp \" + newGS + \n          \" should be greater than the replica \" + b + \"'s generation stamp\");\n    }\n    ReplicaInfo replicaInfo = getReplicaInfo(b);\n    LOG.info(\"Appending to \" + replicaInfo);\n    if (replicaInfo.getState() != ReplicaState.FINALIZED) {\n      throw new ReplicaNotFoundException(\n          ReplicaNotFoundException.UNFINALIZED_REPLICA + b);\n    }\n    if (replicaInfo.getNumBytes() != expectedBlockLen) {\n      throw new IOException(\"Corrupted replica \" + replicaInfo + \n          \" with a length of \" + replicaInfo.getNumBytes() + \n          \" expected length is \" + expectedBlockLen);\n    }\n\n    FsVolumeReference ref = replicaInfo.getVolume().obtainReference();\n    ReplicaBeingWritten replica = null;\n    try {\n      replica = append(b.getBlockPoolId(), (FinalizedReplica)replicaInfo, newGS,\n          b.getNumBytes());\n    } catch (IOException e) {\n      IOUtils.cleanup(null, ref);\n      throw e;\n    }\n    return new ReplicaHandler(replica, ref);\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report describes a checksum error occurring during concurrent append and read operations, which is related to the methods in the stack trace, such as `FsDatasetImpl.getBlockInputStream` and `BlockSender.sendBlock`. These methods are in the same stack trace context as the ground truth methods, but the report does not precisely identify the root cause in the ground truth methods. There is no fix suggestion provided in the bug report. The problem location is partially identified as it mentions methods in the shared stack trace context. There is no wrong information in the bug report as it accurately describes the issue and its symptoms."
        }
    },
    {
        "filename": "HDFS-6825.json",
        "code_diff": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.nextBlockId": {
                "code_before_change": "  private long nextBlockId() throws IOException {\n    assert hasWriteLock();\n    checkNameNodeSafeMode(\"Cannot get next block ID\");\n    final long blockId = blockIdGenerator.nextValue();\n    getEditLog().logAllocateBlockId(blockId);\n    // NB: callers sync the log\n    return blockId;\n  }",
                "code_after_change": "  private long nextBlockId() throws IOException {\n    assert hasWriteLock();\n    checkNameNodeSafeMode(\"Cannot get next block ID\");\n    final long blockId = blockIdGenerator.nextValue();\n    getEditLog().logAllocateBlockId(blockId);\n    // NB: callers sync the log\n    return blockId;\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.commitBlockSynchronization": {
                "code_before_change": "  void commitBlockSynchronization(ExtendedBlock lastblock,\n      long newgenerationstamp, long newlength,\n      boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n      String[] newtargetstorages)\n      throws IOException, UnresolvedLinkException {\n    LOG.info(\"commitBlockSynchronization(lastblock=\" + lastblock\n             + \", newgenerationstamp=\" + newgenerationstamp\n             + \", newlength=\" + newlength\n             + \", newtargets=\" + Arrays.asList(newtargets)\n             + \", closeFile=\" + closeFile\n             + \", deleteBlock=\" + deleteblock\n             + \")\");\n    checkOperation(OperationCategory.WRITE);\n    String src = \"\";\n    waitForLoadingFSImage();\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      // If a DN tries to commit to the standby, the recovery will\n      // fail, and the next retry will succeed on the new NN.\n  \n      checkNameNodeSafeMode(\n          \"Cannot commitBlockSynchronization while in safe mode\");\n      final BlockInfo storedBlock = getStoredBlock(\n          ExtendedBlock.getLocalBlock(lastblock));\n      if (storedBlock == null) {\n        if (deleteblock) {\n          // This may be a retry attempt so ignore the failure\n          // to locate the block.\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Block (=\" + lastblock + \") not found\");\n          }\n          return;\n        } else {\n          throw new IOException(\"Block (=\" + lastblock + \") not found\");\n        }\n      }\n      INodeFile iFile = ((INode)storedBlock.getBlockCollection()).asFile();\n      if (!iFile.isUnderConstruction() || storedBlock.isComplete()) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Unexpected block (=\" + lastblock\n                    + \") since the file (=\" + iFile.getLocalName()\n                    + \") is not under construction\");\n        }\n        return;\n      }\n\n      long recoveryId =\n        ((BlockInfoUnderConstruction)storedBlock).getBlockRecoveryId();\n      if(recoveryId != newgenerationstamp) {\n        throw new IOException(\"The recovery id \" + newgenerationstamp\n                              + \" does not match current recovery id \"\n                              + recoveryId + \" for block \" + lastblock); \n      }\n\n      if (deleteblock) {\n        Block blockToDel = ExtendedBlock.getLocalBlock(lastblock);\n        boolean remove = iFile.removeLastBlock(blockToDel);\n        if (remove) {\n          blockManager.removeBlockFromMap(storedBlock);\n        }\n      }\n      else {\n        // update last block\n        storedBlock.setGenerationStamp(newgenerationstamp);\n        storedBlock.setNumBytes(newlength);\n\n        // find the DatanodeDescriptor objects\n        // There should be no locations in the blockManager till now because the\n        // file is underConstruction\n        ArrayList<DatanodeDescriptor> trimmedTargets =\n            new ArrayList<DatanodeDescriptor>(newtargets.length);\n        ArrayList<String> trimmedStorages =\n            new ArrayList<String>(newtargets.length);\n        if (newtargets.length > 0) {\n          for (int i = 0; i < newtargets.length; ++i) {\n            // try to get targetNode\n            DatanodeDescriptor targetNode =\n                blockManager.getDatanodeManager().getDatanode(newtargets[i]);\n            if (targetNode != null) {\n              trimmedTargets.add(targetNode);\n              trimmedStorages.add(newtargetstorages[i]);\n            } else if (LOG.isDebugEnabled()) {\n              LOG.debug(\"DatanodeDescriptor (=\" + newtargets[i] + \") not found\");\n            }\n          }\n        }\n        if ((closeFile) && !trimmedTargets.isEmpty()) {\n          // the file is getting closed. Insert block locations into blockManager.\n          // Otherwise fsck will report these blocks as MISSING, especially if the\n          // blocksReceived from Datanodes take a long time to arrive.\n          for (int i = 0; i < trimmedTargets.size(); i++) {\n            DatanodeStorageInfo storageInfo =\n                trimmedTargets.get(i).getStorageInfo(trimmedStorages.get(i));\n            if (storageInfo != null) {\n              storageInfo.addBlock(storedBlock);\n            }\n          }\n        }\n\n        // add pipeline locations into the INodeUnderConstruction\n        DatanodeStorageInfo[] trimmedStorageInfos =\n            blockManager.getDatanodeManager().getDatanodeStorageInfos(\n                trimmedTargets.toArray(new DatanodeID[trimmedTargets.size()]),\n                trimmedStorages.toArray(new String[trimmedStorages.size()]));\n        iFile.setLastBlock(storedBlock, trimmedStorageInfos);\n      }\n\n      if (closeFile) {\n        src = closeFileCommitBlocks(iFile, storedBlock);\n      } else {\n        // If this commit does not want to close the file, persist blocks\n        src = iFile.getFullPathName();\n        persistBlocks(src, iFile, false);\n      }\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n    if (closeFile) {\n      LOG.info(\"commitBlockSynchronization(newblock=\" + lastblock\n          + \", file=\" + src\n          + \", newgenerationstamp=\" + newgenerationstamp\n          + \", newlength=\" + newlength\n          + \", newtargets=\" + Arrays.asList(newtargets) + \") successful\");\n    } else {\n      LOG.info(\"commitBlockSynchronization(\" + lastblock + \") successful\");\n    }\n  }",
                "code_after_change": "  void commitBlockSynchronization(ExtendedBlock lastblock,\n      long newgenerationstamp, long newlength,\n      boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n      String[] newtargetstorages)\n      throws IOException, UnresolvedLinkException {\n    LOG.info(\"commitBlockSynchronization(lastblock=\" + lastblock\n             + \", newgenerationstamp=\" + newgenerationstamp\n             + \", newlength=\" + newlength\n             + \", newtargets=\" + Arrays.asList(newtargets)\n             + \", closeFile=\" + closeFile\n             + \", deleteBlock=\" + deleteblock\n             + \")\");\n    checkOperation(OperationCategory.WRITE);\n    String src = \"\";\n    waitForLoadingFSImage();\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      // If a DN tries to commit to the standby, the recovery will\n      // fail, and the next retry will succeed on the new NN.\n  \n      checkNameNodeSafeMode(\n          \"Cannot commitBlockSynchronization while in safe mode\");\n      final BlockInfo storedBlock = getStoredBlock(\n          ExtendedBlock.getLocalBlock(lastblock));\n      if (storedBlock == null) {\n        if (deleteblock) {\n          // This may be a retry attempt so ignore the failure\n          // to locate the block.\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Block (=\" + lastblock + \") not found\");\n          }\n          return;\n        } else {\n          throw new IOException(\"Block (=\" + lastblock + \") not found\");\n        }\n      }\n      //\n      // The implementation of delete operation (see @deleteInternal method)\n      // first removes the file paths from namespace, and delays the removal\n      // of blocks to later time for better performance. When\n      // commitBlockSynchronization (this method) is called in between, the\n      // blockCollection of storedBlock could have been assigned to null by\n      // the delete operation, throw IOException here instead of NPE; if the\n      // file path is already removed from namespace by the delete operation,\n      // throw FileNotFoundException here, so not to proceed to the end of\n      // this method to add a CloseOp to the edit log for an already deleted\n      // file (See HDFS-6825).\n      //\n      BlockCollection blockCollection = storedBlock.getBlockCollection();\n      if (blockCollection == null) {\n        throw new IOException(\"The blockCollection of \" + storedBlock\n            + \" is null, likely because the file owning this block was\"\n            + \" deleted and the block removal is delayed\");\n      }\n      INodeFile iFile = ((INode)blockCollection).asFile();\n      if (isFileDeleted(iFile)) {\n        throw new FileNotFoundException(\"File not found: \"\n            + iFile.getFullPathName() + \", likely due to delayed block\"\n            + \" removal\");\n      }\n      if (!iFile.isUnderConstruction() || storedBlock.isComplete()) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Unexpected block (=\" + lastblock\n                    + \") since the file (=\" + iFile.getLocalName()\n                    + \") is not under construction\");\n        }\n        return;\n      }\n\n      long recoveryId =\n        ((BlockInfoUnderConstruction)storedBlock).getBlockRecoveryId();\n      if(recoveryId != newgenerationstamp) {\n        throw new IOException(\"The recovery id \" + newgenerationstamp\n                              + \" does not match current recovery id \"\n                              + recoveryId + \" for block \" + lastblock); \n      }\n\n      if (deleteblock) {\n        Block blockToDel = ExtendedBlock.getLocalBlock(lastblock);\n        boolean remove = iFile.removeLastBlock(blockToDel);\n        if (remove) {\n          blockManager.removeBlockFromMap(storedBlock);\n        }\n      }\n      else {\n        // update last block\n        storedBlock.setGenerationStamp(newgenerationstamp);\n        storedBlock.setNumBytes(newlength);\n\n        // find the DatanodeDescriptor objects\n        // There should be no locations in the blockManager till now because the\n        // file is underConstruction\n        ArrayList<DatanodeDescriptor> trimmedTargets =\n            new ArrayList<DatanodeDescriptor>(newtargets.length);\n        ArrayList<String> trimmedStorages =\n            new ArrayList<String>(newtargets.length);\n        if (newtargets.length > 0) {\n          for (int i = 0; i < newtargets.length; ++i) {\n            // try to get targetNode\n            DatanodeDescriptor targetNode =\n                blockManager.getDatanodeManager().getDatanode(newtargets[i]);\n            if (targetNode != null) {\n              trimmedTargets.add(targetNode);\n              trimmedStorages.add(newtargetstorages[i]);\n            } else if (LOG.isDebugEnabled()) {\n              LOG.debug(\"DatanodeDescriptor (=\" + newtargets[i] + \") not found\");\n            }\n          }\n        }\n        if ((closeFile) && !trimmedTargets.isEmpty()) {\n          // the file is getting closed. Insert block locations into blockManager.\n          // Otherwise fsck will report these blocks as MISSING, especially if the\n          // blocksReceived from Datanodes take a long time to arrive.\n          for (int i = 0; i < trimmedTargets.size(); i++) {\n            DatanodeStorageInfo storageInfo =\n                trimmedTargets.get(i).getStorageInfo(trimmedStorages.get(i));\n            if (storageInfo != null) {\n              storageInfo.addBlock(storedBlock);\n            }\n          }\n        }\n\n        // add pipeline locations into the INodeUnderConstruction\n        DatanodeStorageInfo[] trimmedStorageInfos =\n            blockManager.getDatanodeManager().getDatanodeStorageInfos(\n                trimmedTargets.toArray(new DatanodeID[trimmedTargets.size()]),\n                trimmedStorages.toArray(new String[trimmedStorages.size()]));\n        iFile.setLastBlock(storedBlock, trimmedStorageInfos);\n      }\n\n      if (closeFile) {\n        src = closeFileCommitBlocks(iFile, storedBlock);\n      } else {\n        // If this commit does not want to close the file, persist blocks\n        src = iFile.getFullPathName();\n        persistBlocks(src, iFile, false);\n      }\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n    if (closeFile) {\n      LOG.info(\"commitBlockSynchronization(newblock=\" + lastblock\n          + \", file=\" + src\n          + \", newgenerationstamp=\" + newgenerationstamp\n          + \", newlength=\" + newlength\n          + \", newtargets=\" + Arrays.asList(newtargets) + \") successful\");\n    } else {\n      LOG.info(\"commitBlockSynchronization(\" + lastblock + \") successful\");\n    }\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.INodeDirectory.addDirectoryWithQuotaFeature": {
                "code_before_change": "  DirectoryWithQuotaFeature addDirectoryWithQuotaFeature(\n      long nsQuota, long dsQuota) {\n    Preconditions.checkState(!isWithQuota(), \"Directory is already with quota\");\n    final DirectoryWithQuotaFeature quota = new DirectoryWithQuotaFeature(\n        nsQuota, dsQuota);\n    addFeature(quota);\n    return quota;\n  }",
                "code_after_change": "  DirectoryWithQuotaFeature addDirectoryWithQuotaFeature(\n      long nsQuota, long dsQuota) {\n    Preconditions.checkState(!isWithQuota(), \"Directory is already with quota\");\n    final DirectoryWithQuotaFeature quota = new DirectoryWithQuotaFeature(\n        nsQuota, dsQuota);\n    addFeature(quota);\n    return quota;\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoUnderConstruction.appendUCParts": {
                "code_before_change": "  private void appendUCParts(StringBuilder sb) {\n    sb.append(\"{blockUCState=\").append(blockUCState)\n      .append(\", primaryNodeIndex=\").append(primaryNodeIndex)\n      .append(\", replicas=[\");\n    Iterator<ReplicaUnderConstruction> iter = replicas.iterator();\n    if (iter.hasNext()) {\n      iter.next().appendStringTo(sb);\n      while (iter.hasNext()) {\n        sb.append(\", \");\n        iter.next().appendStringTo(sb);\n      }\n    }\n    sb.append(\"]}\");\n  }",
                "code_after_change": "  private void appendUCParts(StringBuilder sb) {\n    sb.append(\"{blockUCState=\").append(blockUCState)\n      .append(\", primaryNodeIndex=\").append(primaryNodeIndex)\n      .append(\", replicas=[\");\n    if (replicas != null) {\n      Iterator<ReplicaUnderConstruction> iter = replicas.iterator();\n      if (iter.hasNext()) {\n        iter.next().appendStringTo(sb);\n        while (iter.hasNext()) {\n          sb.append(\", \");\n          iter.next().appendStringTo(sb);\n        }\n      }\n    }\n    sb.append(\"]}\");\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the issue occurring in the 'commitBlockSynchronization' method, which is part of the stack trace and is a ground truth method. However, it does not precisely identify the root cause related to the specific changes made in the ground truth methods. The report lacks a fix suggestion, as there is no explicit suggestion or alternative fix provided. The problem location is partially identified as it mentions 'commitBlockSynchronization' in the stack trace, which is related to the ground truth methods. There is no wrong information in the report as all the details provided are relevant to the context of the bug."
        }
    },
    {
        "filename": "HDFS-5710.json",
        "code_diff": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirectory.getFullPathINodes": {
                "code_before_change": "  private static INode[] getFullPathINodes(INode inode) {\n    return getRelativePathINodes(inode, null);\n  }",
                "code_after_change": "  private static INode[] getFullPathINodes(INode inode) {\n    return getRelativePathINodes(inode, null);\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Direct Caller/Callee"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Direct Caller/Callee"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the method `FSDirectory#getFullPathName` as the location where the issue occurs, which is a direct caller of the ground truth method `FSDirectory#getFullPathINodes`. This makes the root cause identification and problem location identification partial with a sub-category of Direct Caller/Callee. The report does not provide any fix suggestion, hence it is marked as missing. There is no incorrect information in the bug report, so the wrong information criterion is marked as No."
        }
    },
    {
        "filename": "HDFS-3555.json",
        "code_diff": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockSender.sendPacket": {
                "code_before_change": "  private int sendPacket(ByteBuffer pkt, int maxChunks, OutputStream out,\n      boolean transferTo, DataTransferThrottler throttler) throws IOException {\n    int dataLen = (int) Math.min(endOffset - offset,\n                             (chunkSize * (long) maxChunks));\n    \n    int numChunks = numberOfChunks(dataLen); // Number of chunks be sent in the packet\n    int checksumDataLen = numChunks * checksumSize;\n    int packetLen = dataLen + checksumDataLen + 4;\n    boolean lastDataPacket = offset + dataLen == endOffset && dataLen > 0;\n\n    writePacketHeader(pkt, dataLen, packetLen);\n\n    int checksumOff = pkt.position();\n    byte[] buf = pkt.array();\n    \n    if (checksumSize > 0 && checksumIn != null) {\n      readChecksum(buf, checksumOff, checksumDataLen);\n\n      // write in progress that we need to use to get last checksum\n      if (lastDataPacket && lastChunkChecksum != null) {\n        int start = checksumOff + checksumDataLen - checksumSize;\n        byte[] updatedChecksum = lastChunkChecksum.getChecksum();\n        \n        if (updatedChecksum != null) {\n          System.arraycopy(updatedChecksum, 0, buf, start, checksumSize);\n        }\n      }\n    }\n    \n    int dataOff = checksumOff + checksumDataLen;\n    if (!transferTo) { // normal transfer\n      IOUtils.readFully(blockIn, buf, dataOff, dataLen);\n\n      if (verifyChecksum) {\n        verifyChecksum(buf, dataOff, dataLen, numChunks, checksumOff);\n      }\n    }\n    \n    try {\n      if (transferTo) {\n        SocketOutputStream sockOut = (SocketOutputStream)out;\n        sockOut.write(buf, 0, dataOff); // First write checksum\n        \n        // no need to flush. since we know out is not a buffered stream. \n        sockOut.transferToFully(((FileInputStream)blockIn).getChannel(), \n                                blockInPosition, dataLen);\n        blockInPosition += dataLen;\n      } else { \n        // normal transfer\n        out.write(buf, 0, dataOff + dataLen);\n      }\n    } catch (IOException e) {\n      /* Exception while writing to the client. Connection closure from\n       * the other end is mostly the case and we do not care much about\n       * it. But other things can go wrong, especially in transferTo(),\n       * which we do not want to ignore.\n       *\n       * The message parsing below should not be considered as a good\n       * coding example. NEVER do it to drive a program logic. NEVER.\n       * It was done here because the NIO throws an IOException for EPIPE.\n       */\n      String ioem = e.getMessage();\n      if (!ioem.startsWith(\"Broken pipe\") && !ioem.startsWith(\"Connection reset\")) {\n        LOG.error(\"BlockSender.sendChunks() exception: \", e);\n      }\n      throw ioeToSocketException(e);\n    }\n\n    if (throttler != null) { // rebalancing so throttle\n      throttler.throttle(packetLen);\n    }\n\n    return dataLen;\n  }",
                "code_after_change": "  private int sendPacket(ByteBuffer pkt, int maxChunks, OutputStream out,\n      boolean transferTo, DataTransferThrottler throttler) throws IOException {\n    int dataLen = (int) Math.min(endOffset - offset,\n                             (chunkSize * (long) maxChunks));\n    \n    int numChunks = numberOfChunks(dataLen); // Number of chunks be sent in the packet\n    int checksumDataLen = numChunks * checksumSize;\n    int packetLen = dataLen + checksumDataLen + 4;\n    boolean lastDataPacket = offset + dataLen == endOffset && dataLen > 0;\n\n    writePacketHeader(pkt, dataLen, packetLen);\n\n    int checksumOff = pkt.position();\n    byte[] buf = pkt.array();\n    \n    if (checksumSize > 0 && checksumIn != null) {\n      readChecksum(buf, checksumOff, checksumDataLen);\n\n      // write in progress that we need to use to get last checksum\n      if (lastDataPacket && lastChunkChecksum != null) {\n        int start = checksumOff + checksumDataLen - checksumSize;\n        byte[] updatedChecksum = lastChunkChecksum.getChecksum();\n        \n        if (updatedChecksum != null) {\n          System.arraycopy(updatedChecksum, 0, buf, start, checksumSize);\n        }\n      }\n    }\n    \n    int dataOff = checksumOff + checksumDataLen;\n    if (!transferTo) { // normal transfer\n      IOUtils.readFully(blockIn, buf, dataOff, dataLen);\n\n      if (verifyChecksum) {\n        verifyChecksum(buf, dataOff, dataLen, numChunks, checksumOff);\n      }\n    }\n    \n    try {\n      if (transferTo) {\n        SocketOutputStream sockOut = (SocketOutputStream)out;\n        sockOut.write(buf, 0, dataOff); // First write checksum\n        \n        // no need to flush since we know out is not a buffered stream\n        FileChannel fileCh = ((FileInputStream)blockIn).getChannel();\n        sockOut.transferToFully(fileCh, blockInPosition, dataLen, \n            datanode.metrics.getSendDataPacketBlockedOnNetworkNanos(),\n            datanode.metrics.getSendDataPacketTransferNanos());\n        blockInPosition += dataLen;\n      } else { \n        // normal transfer\n        out.write(buf, 0, dataOff + dataLen);\n      }\n    } catch (IOException e) {\n      if (e instanceof SocketTimeoutException) {\n        /*\n         * writing to client timed out.  This happens if the client reads\n         * part of a block and then decides not to read the rest (but leaves\n         * the socket open).\n         */\n          LOG.info(\"BlockSender.sendChunks() exception: \", e);\n      } else {\n        /* Exception while writing to the client. Connection closure from\n         * the other end is mostly the case and we do not care much about\n         * it. But other things can go wrong, especially in transferTo(),\n         * which we do not want to ignore.\n         *\n         * The message parsing below should not be considered as a good\n         * coding example. NEVER do it to drive a program logic. NEVER.\n         * It was done here because the NIO throws an IOException for EPIPE.\n         */\n        String ioem = e.getMessage();\n        if (!ioem.startsWith(\"Broken pipe\") && !ioem.startsWith(\"Connection reset\")) {\n          LOG.error(\"BlockSender.sendChunks() exception: \", e);\n        }\n      }\n      throw ioeToSocketException(e);\n    }\n\n    if (throttler != null) { // rebalancing so throttle\n      throttler.throttle(packetLen);\n    }\n\n    return dataLen;\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the issue of logging a SocketTimeoutException at an inappropriate log level, which is related to the ground truth method 'BlockSender.sendPacket' as it deals with sending data and handling exceptions. However, it does not precisely identify the root cause in the ground truth method but mentions methods in the same stack trace context. The report does not provide any fix suggestion, hence 'Missing' for fix suggestion. The problem location is partially identified as it mentions methods in the stack trace but not the exact ground truth method. There is no wrong information in the bug report as it correctly describes the issue with the log level."
        }
    },
    {
        "filename": "HDFS-12363.json",
        "code_diff": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.scanAndCompactStorages": {
                "code_before_change": "    private void scanAndCompactStorages() throws InterruptedException {\n      ArrayList<String> datanodesAndStorages = new ArrayList<>();\n      for (DatanodeDescriptor node\n          : datanodeManager.getDatanodeListForReport(DatanodeReportType.ALL)) {\n        for (DatanodeStorageInfo storage : node.getStorageInfos()) {\n          try {\n            namesystem.readLock();\n            double ratio = storage.treeSetFillRatio();\n            if (ratio < storageInfoDefragmentRatio) {\n              datanodesAndStorages.add(node.getDatanodeUuid());\n              datanodesAndStorages.add(storage.getStorageID());\n            }\n            LOG.info(\"StorageInfo TreeSet fill ratio {} : {}{}\",\n                     storage.getStorageID(), ratio,\n                     (ratio < storageInfoDefragmentRatio)\n                     ? \" (queued for defragmentation)\" : \"\");\n          } finally {\n            namesystem.readUnlock();\n          }\n        }\n      }\n      if (!datanodesAndStorages.isEmpty()) {\n        for (int i = 0; i < datanodesAndStorages.size(); i += 2) {\n          namesystem.writeLock();\n          try {\n            DatanodeStorageInfo storage = datanodeManager.\n                getDatanode(datanodesAndStorages.get(i)).\n                getStorageInfo(datanodesAndStorages.get(i + 1));\n            if (storage != null) {\n              boolean aborted =\n                  !storage.treeSetCompact(storageInfoDefragmentTimeout);\n              if (aborted) {\n                // Compaction timed out, reset iterator to continue with\n                // the same storage next iteration.\n                i -= 2;\n              }\n              LOG.info(\"StorageInfo TreeSet defragmented {} : {}{}\",\n                       storage.getStorageID(), storage.treeSetFillRatio(),\n                       aborted ? \" (aborted)\" : \"\");\n            }\n          } finally {\n            namesystem.writeUnlock();\n          }\n          // Wait between each iteration\n          Thread.sleep(1000);\n        }\n      }\n    }",
                "code_after_change": "    private void scanAndCompactStorages() throws InterruptedException {\n      ArrayList<String> datanodesAndStorages = new ArrayList<>();\n      for (DatanodeDescriptor node\n          : datanodeManager.getDatanodeListForReport(DatanodeReportType.ALL)) {\n        for (DatanodeStorageInfo storage : node.getStorageInfos()) {\n          try {\n            namesystem.readLock();\n            double ratio = storage.treeSetFillRatio();\n            if (ratio < storageInfoDefragmentRatio) {\n              datanodesAndStorages.add(node.getDatanodeUuid());\n              datanodesAndStorages.add(storage.getStorageID());\n            }\n            LOG.info(\"StorageInfo TreeSet fill ratio {} : {}{}\",\n                     storage.getStorageID(), ratio,\n                     (ratio < storageInfoDefragmentRatio)\n                     ? \" (queued for defragmentation)\" : \"\");\n          } finally {\n            namesystem.readUnlock();\n          }\n        }\n      }\n      if (!datanodesAndStorages.isEmpty()) {\n        for (int i = 0; i < datanodesAndStorages.size(); i += 2) {\n          namesystem.writeLock();\n          try {\n            final DatanodeDescriptor dn = datanodeManager.\n                getDatanode(datanodesAndStorages.get(i));\n            if (dn == null) {\n              continue;\n            }\n            final DatanodeStorageInfo storage = dn.\n                getStorageInfo(datanodesAndStorages.get(i + 1));\n            if (storage != null) {\n              boolean aborted =\n                  !storage.treeSetCompact(storageInfoDefragmentTimeout);\n              if (aborted) {\n                // Compaction timed out, reset iterator to continue with\n                // the same storage next iteration.\n                i -= 2;\n              }\n              LOG.info(\"StorageInfo TreeSet defragmented {} : {}{}\",\n                       storage.getStorageID(), storage.treeSetFillRatio(),\n                       aborted ? \" (aborted)\" : \"\");\n            }\n          } finally {\n            namesystem.writeUnlock();\n          }\n          // Wait between each iteration\n          Thread.sleep(1000);\n        }\n      }\n    }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockManager$StorageInfoDefragmenter.scanAndCompactStorages": {
                "code_before_change": [],
                "code_after_change": []
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report precisely identifies the root cause by mentioning the method 'BlockManager$StorageInfoDefragmenter.scanAndCompactStorages', which is part of the ground truth methods. However, there is no fix suggestion provided in the bug report, as there is no 'Suggestions' or 'possible_fix' field, nor any suggestion in the 'Description'. The problem location is also precisely identified as the method 'BlockManager$StorageInfoDefragmenter.scanAndCompactStorages' is mentioned in the 'Title' and 'Description'. There is no wrong information in the bug report as all the information is relevant to the context of the bug."
        }
    },
    {
        "filename": "HDFS-7916.json",
        "code_diff": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.ReportBadBlockAction.reportTo": {
                "code_before_change": "  public void reportTo(DatanodeProtocolClientSideTranslatorPB bpNamenode, \n    DatanodeRegistration bpRegistration) throws BPServiceActorActionException {\n    if (bpRegistration == null) {\n      return;\n    }\n    DatanodeInfo[] dnArr = { new DatanodeInfo(bpRegistration) };\n    String[] uuids = { storageUuid };\n    StorageType[] types = { storageType };\n    LocatedBlock[] locatedBlock = { new LocatedBlock(block,\n        dnArr, uuids, types) };\n\n    try {\n      bpNamenode.reportBadBlocks(locatedBlock);  \n    } catch (IOException e){\n      throw new BPServiceActorActionException(\"Failed to report bad block \"\n          + block + \" to namenode: \");\n    }\n  }",
                "code_after_change": "  public void reportTo(DatanodeProtocolClientSideTranslatorPB bpNamenode, \n    DatanodeRegistration bpRegistration) throws BPServiceActorActionException {\n    if (bpRegistration == null) {\n      return;\n    }\n    DatanodeInfo[] dnArr = { new DatanodeInfo(bpRegistration) };\n    String[] uuids = { storageUuid };\n    StorageType[] types = { storageType };\n    LocatedBlock[] locatedBlock = { new LocatedBlock(block,\n        dnArr, uuids, types) };\n\n    try {\n      bpNamenode.reportBadBlocks(locatedBlock);\n    } catch (RemoteException re) {\n      DataNode.LOG.info(\"reportBadBlock encountered RemoteException for \"\n          + \"block:  \" + block , re);\n    } catch (IOException e) {\n      throw new BPServiceActorActionException(\"Failed to report bad block \"\n          + block + \" to namenode: \");\n    }\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.ErrorReportAction.reportTo": {
                "code_before_change": "  public void reportTo(DatanodeProtocolClientSideTranslatorPB bpNamenode, \n    DatanodeRegistration bpRegistration) throws BPServiceActorActionException {\n    try {\n      bpNamenode.errorReport(bpRegistration, errorCode, errorMessage);\n    } catch(IOException e) {\n      throw new BPServiceActorActionException(\"Error reporting \"\n          + \"an error to namenode: \");\n    }\n  }",
                "code_after_change": "  public void reportTo(DatanodeProtocolClientSideTranslatorPB bpNamenode, \n    DatanodeRegistration bpRegistration) throws BPServiceActorActionException {\n    try {\n      bpNamenode.errorReport(bpRegistration, errorCode, errorMessage);\n    } catch (RemoteException re) {\n      DataNode.LOG.info(\"trySendErrorReport encountered RemoteException  \"\n          + \"errorMessage: \" + errorMessage + \"  errorCode: \" + errorCode, re);\n    } catch(IOException e) {\n      throw new BPServiceActorActionException(\"Error reporting \"\n          + \"an error to namenode: \");\n    }\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report precisely identifies the root cause by mentioning the method 'reportTo' in 'ReportBadBlockAction', which is a ground truth method. However, there is no fix suggestion provided in the bug report. The problem location identification is partial because the report mentions 'BPServiceActor.processQueueMessages' and 'BPServiceActor.offerService' in the stack trace, which are in the shared stack trace context with the ground truth method. There is no wrong information in the bug report as all the details are relevant to the context of the bug."
        }
    },
    {
        "filename": "HDFS-11164.json",
        "code_diff": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.balancer.Dispatcher.waitForMoveCompletion": {
                "code_before_change": "  public static boolean waitForMoveCompletion(\n      Iterable<? extends StorageGroup> targets) {\n    boolean hasFailure = false;\n    for(;;) {\n      boolean empty = true;\n      for (StorageGroup t : targets) {\n        if (!t.getDDatanode().isPendingQEmpty()) {\n          empty = false;\n          break;\n        } else {\n          hasFailure |= t.getDDatanode().hasFailure;\n        }\n      }\n      if (empty) {\n        return hasFailure; // all pending queues are empty\n      }\n      try {\n        Thread.sleep(1000);\n      } catch (InterruptedException ignored) {\n      }\n    }\n  }",
                "code_after_change": "  public static boolean waitForMoveCompletion(\n      Iterable<? extends StorageGroup> targets) {\n    boolean hasFailure = false;\n    for(;;) {\n      boolean empty = true;\n      for (StorageGroup t : targets) {\n        if (!t.getDDatanode().isPendingQEmpty()) {\n          empty = false;\n          break;\n        } else {\n          hasFailure |= t.getDDatanode().hasFailure;\n        }\n      }\n      if (empty) {\n        return hasFailure; // all pending queues are empty\n      }\n      try {\n        Thread.sleep(1000);\n      } catch (InterruptedException ignored) {\n      }\n    }\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.mover.Mover.emptySet": {
                "code_before_change": [],
                "code_after_change": []
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.balancer.Dispatcher.receiveResponse": {
                "code_before_change": "    private void receiveResponse(DataInputStream in) throws IOException {\n      long startTime = Time.monotonicNow();\n      BlockOpResponseProto response =\n          BlockOpResponseProto.parseFrom(vintPrefixed(in));\n      while (response.getStatus() == Status.IN_PROGRESS) {\n        // read intermediate responses\n        response = BlockOpResponseProto.parseFrom(vintPrefixed(in));\n        // Stop waiting for slow block moves. Even if it stops waiting,\n        // the actual move may continue.\n        if (stopWaitingForResponse(startTime)) {\n          throw new IOException(\"Block move timed out\");\n        }\n      }\n      String logInfo = \"reportedBlock move is failed\";\n      DataTransferProtoUtil.checkBlockOpStatus(response, logInfo);\n    }",
                "code_after_change": "    private void receiveResponse(DataInputStream in) throws IOException {\n      long startTime = Time.monotonicNow();\n      BlockOpResponseProto response =\n          BlockOpResponseProto.parseFrom(vintPrefixed(in));\n      while (response.getStatus() == Status.IN_PROGRESS) {\n        // read intermediate responses\n        response = BlockOpResponseProto.parseFrom(vintPrefixed(in));\n        // Stop waiting for slow block moves. Even if it stops waiting,\n        // the actual move may continue.\n        if (stopWaitingForResponse(startTime)) {\n          throw new IOException(\"Block move timed out\");\n        }\n      }\n      String logInfo = \"reportedBlock move is failed\";\n      DataTransferProtoUtil.checkBlockOpStatus(response, logInfo, true);\n    }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.mover.Mover.processNamespace": {
                "code_before_change": "    private Result processNamespace() throws IOException {\n      getSnapshottableDirs();\n      Result result = new Result();\n      for (Path target : targetPaths) {\n        processPath(target.toUri().getPath(), result);\n      }\n      // wait for pending move to finish and retry the failed migration\n      boolean hasFailed = Dispatcher.waitForMoveCompletion(storages.targets\n          .values());\n      boolean hasSuccess = Dispatcher.checkForSuccess(storages.targets\n          .values());\n      if (hasFailed && !hasSuccess) {\n        if (retryCount.get() == retryMaxAttempts) {\n          result.setRetryFailed();\n          LOG.error(\"Failed to move some block's after \"\n              + retryMaxAttempts + \" retries.\");\n          return result;\n        } else {\n          retryCount.incrementAndGet();\n        }\n      } else {\n        // Reset retry count if no failure.\n        retryCount.set(0);\n      }\n      result.updateHasRemaining(hasFailed);\n      return result;\n    }",
                "code_after_change": "    private Result processNamespace() throws IOException {\n      getSnapshottableDirs();\n      Result result = new Result();\n      for (Path target : targetPaths) {\n        processPath(target.toUri().getPath(), result);\n      }\n      // wait for pending move to finish and retry the failed migration\n      boolean hasFailed = Dispatcher.waitForMoveCompletion(storages.targets\n          .values());\n      Dispatcher.checkForBlockPinningFailures(excludedPinnedBlocks,\n          storages.targets.values());\n      boolean hasSuccess = Dispatcher.checkForSuccess(storages.targets\n          .values());\n      if (hasFailed && !hasSuccess) {\n        if (retryCount.get() == retryMaxAttempts) {\n          result.setRetryFailed();\n          LOG.error(\"Failed to move some block's after \"\n              + retryMaxAttempts + \" retries.\");\n          return result;\n        } else {\n          retryCount.incrementAndGet();\n        }\n      } else {\n        // Reset retry count if no failure.\n        retryCount.set(0);\n      }\n      result.updateHasRemaining(hasFailed);\n      return result;\n    }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataXceiver.replaceBlock": {
                "code_before_change": "  public void replaceBlock(final ExtendedBlock block,\n      final StorageType storageType, \n      final Token<BlockTokenIdentifier> blockToken,\n      final String delHint,\n      final DatanodeInfo proxySource) throws IOException {\n    updateCurrentThreadName(\"Replacing block \" + block + \" from \" + delHint);\n    DataOutputStream replyOut = new DataOutputStream(getOutputStream());\n    checkAccess(replyOut, true, block, blockToken,\n        Op.REPLACE_BLOCK, BlockTokenIdentifier.AccessMode.REPLACE);\n\n    if (!dataXceiverServer.balanceThrottler.acquire()) { // not able to start\n      String msg = \"Not able to receive block \" + block.getBlockId() +\n          \" from \" + peer.getRemoteAddressString() + \" because threads \" +\n          \"quota is exceeded.\";\n      LOG.warn(msg);\n      sendResponse(ERROR, msg);\n      return;\n    }\n\n    Socket proxySock = null;\n    DataOutputStream proxyOut = null;\n    Status opStatus = SUCCESS;\n    String errMsg = null;\n    DataInputStream proxyReply = null;\n    boolean IoeDuringCopyBlockOperation = false;\n    try {\n      // Move the block to different storage in the same datanode\n      if (proxySource.equals(datanode.getDatanodeId())) {\n        ReplicaInfo oldReplica = datanode.data.moveBlockAcrossStorage(block,\n            storageType);\n        if (oldReplica != null) {\n          LOG.info(\"Moved \" + block + \" from StorageType \"\n              + oldReplica.getVolume().getStorageType() + \" to \" + storageType);\n        }\n      } else {\n        block.setNumBytes(dataXceiverServer.estimateBlockSize);\n        // get the output stream to the proxy\n        final String dnAddr = proxySource.getXferAddr(connectToDnViaHostname);\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Connecting to datanode \" + dnAddr);\n        }\n        InetSocketAddress proxyAddr = NetUtils.createSocketAddr(dnAddr);\n        proxySock = datanode.newSocket();\n        NetUtils.connect(proxySock, proxyAddr, dnConf.socketTimeout);\n        proxySock.setTcpNoDelay(dnConf.getDataTransferServerTcpNoDelay());\n        proxySock.setSoTimeout(dnConf.socketTimeout);\n        proxySock.setKeepAlive(true);\n\n        OutputStream unbufProxyOut = NetUtils.getOutputStream(proxySock,\n            dnConf.socketWriteTimeout);\n        InputStream unbufProxyIn = NetUtils.getInputStream(proxySock);\n        DataEncryptionKeyFactory keyFactory =\n            datanode.getDataEncryptionKeyFactoryForBlock(block);\n        IOStreamPair saslStreams = datanode.saslClient.socketSend(proxySock,\n            unbufProxyOut, unbufProxyIn, keyFactory, blockToken, proxySource);\n        unbufProxyOut = saslStreams.out;\n        unbufProxyIn = saslStreams.in;\n        \n        proxyOut = new DataOutputStream(new BufferedOutputStream(unbufProxyOut,\n            smallBufferSize));\n        proxyReply = new DataInputStream(new BufferedInputStream(unbufProxyIn,\n            ioFileBufferSize));\n        \n        /* send request to the proxy */\n        IoeDuringCopyBlockOperation = true;\n        new Sender(proxyOut).copyBlock(block, blockToken);\n        IoeDuringCopyBlockOperation = false;\n        \n        // receive the response from the proxy\n        \n        BlockOpResponseProto copyResponse = BlockOpResponseProto.parseFrom(\n            PBHelperClient.vintPrefixed(proxyReply));\n\n        String logInfo = \"copy block \" + block + \" from \"\n            + proxySock.getRemoteSocketAddress();\n        DataTransferProtoUtil.checkBlockOpStatus(copyResponse, logInfo);\n\n        // get checksum info about the block we're copying\n        ReadOpChecksumInfoProto checksumInfo = copyResponse.getReadOpChecksumInfo();\n        DataChecksum remoteChecksum = DataTransferProtoUtil.fromProto(\n            checksumInfo.getChecksum());\n        // open a block receiver and check if the block does not exist\n        setCurrentBlockReceiver(getBlockReceiver(block, storageType,\n            proxyReply, proxySock.getRemoteSocketAddress().toString(),\n            proxySock.getLocalSocketAddress().toString(),\n            null, 0, 0, 0, \"\", null, datanode, remoteChecksum,\n            CachingStrategy.newDropBehind(), false, false));\n        \n        // receive a block\n        blockReceiver.receiveBlock(null, null, replyOut, null, \n            dataXceiverServer.balanceThrottler, null, true);\n        \n        // notify name node\n        final Replica r = blockReceiver.getReplica();\n        datanode.notifyNamenodeReceivedBlock(\n            block, delHint, r.getStorageUuid(), r.isOnTransientStorage());\n        \n        LOG.info(\"Moved \" + block + \" from \" + peer.getRemoteAddressString()\n            + \", delHint=\" + delHint);\n      }\n    } catch (IOException ioe) {\n      opStatus = ERROR;\n      errMsg = \"opReplaceBlock \" + block + \" received exception \" + ioe; \n      LOG.info(errMsg);\n      if (!IoeDuringCopyBlockOperation) {\n        // Don't double count IO errors\n        incrDatanodeNetworkErrors();\n      }\n      throw ioe;\n    } finally {\n      // receive the last byte that indicates the proxy released its thread resource\n      if (opStatus == SUCCESS && proxyReply != null) {\n        try {\n          proxyReply.readChar();\n        } catch (IOException ignored) {\n        }\n      }\n      \n      // now release the thread resource\n      dataXceiverServer.balanceThrottler.release();\n      \n      // send response back\n      try {\n        sendResponse(opStatus, errMsg);\n      } catch (IOException ioe) {\n        LOG.warn(\"Error writing reply back to \" + peer.getRemoteAddressString());\n        incrDatanodeNetworkErrors();\n      }\n      IOUtils.closeStream(proxyOut);\n      IOUtils.closeStream(blockReceiver);\n      IOUtils.closeStream(proxyReply);\n      IOUtils.closeStream(replyOut);\n    }\n\n    //update metrics\n    datanode.metrics.addReplaceBlockOp(elapsed());\n  }",
                "code_after_change": "  public void replaceBlock(final ExtendedBlock block,\n      final StorageType storageType, \n      final Token<BlockTokenIdentifier> blockToken,\n      final String delHint,\n      final DatanodeInfo proxySource) throws IOException {\n    updateCurrentThreadName(\"Replacing block \" + block + \" from \" + delHint);\n    DataOutputStream replyOut = new DataOutputStream(getOutputStream());\n    checkAccess(replyOut, true, block, blockToken,\n        Op.REPLACE_BLOCK, BlockTokenIdentifier.AccessMode.REPLACE);\n\n    if (!dataXceiverServer.balanceThrottler.acquire()) { // not able to start\n      String msg = \"Not able to receive block \" + block.getBlockId() +\n          \" from \" + peer.getRemoteAddressString() + \" because threads \" +\n          \"quota is exceeded.\";\n      LOG.warn(msg);\n      sendResponse(ERROR, msg);\n      return;\n    }\n\n    Socket proxySock = null;\n    DataOutputStream proxyOut = null;\n    Status opStatus = SUCCESS;\n    String errMsg = null;\n    DataInputStream proxyReply = null;\n    boolean IoeDuringCopyBlockOperation = false;\n    try {\n      // Move the block to different storage in the same datanode\n      if (proxySource.equals(datanode.getDatanodeId())) {\n        ReplicaInfo oldReplica = datanode.data.moveBlockAcrossStorage(block,\n            storageType);\n        if (oldReplica != null) {\n          LOG.info(\"Moved \" + block + \" from StorageType \"\n              + oldReplica.getVolume().getStorageType() + \" to \" + storageType);\n        }\n      } else {\n        block.setNumBytes(dataXceiverServer.estimateBlockSize);\n        // get the output stream to the proxy\n        final String dnAddr = proxySource.getXferAddr(connectToDnViaHostname);\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Connecting to datanode \" + dnAddr);\n        }\n        InetSocketAddress proxyAddr = NetUtils.createSocketAddr(dnAddr);\n        proxySock = datanode.newSocket();\n        NetUtils.connect(proxySock, proxyAddr, dnConf.socketTimeout);\n        proxySock.setTcpNoDelay(dnConf.getDataTransferServerTcpNoDelay());\n        proxySock.setSoTimeout(dnConf.socketTimeout);\n        proxySock.setKeepAlive(true);\n\n        OutputStream unbufProxyOut = NetUtils.getOutputStream(proxySock,\n            dnConf.socketWriteTimeout);\n        InputStream unbufProxyIn = NetUtils.getInputStream(proxySock);\n        DataEncryptionKeyFactory keyFactory =\n            datanode.getDataEncryptionKeyFactoryForBlock(block);\n        IOStreamPair saslStreams = datanode.saslClient.socketSend(proxySock,\n            unbufProxyOut, unbufProxyIn, keyFactory, blockToken, proxySource);\n        unbufProxyOut = saslStreams.out;\n        unbufProxyIn = saslStreams.in;\n        \n        proxyOut = new DataOutputStream(new BufferedOutputStream(unbufProxyOut,\n            smallBufferSize));\n        proxyReply = new DataInputStream(new BufferedInputStream(unbufProxyIn,\n            ioFileBufferSize));\n        \n        /* send request to the proxy */\n        IoeDuringCopyBlockOperation = true;\n        new Sender(proxyOut).copyBlock(block, blockToken);\n        IoeDuringCopyBlockOperation = false;\n        \n        // receive the response from the proxy\n        \n        BlockOpResponseProto copyResponse = BlockOpResponseProto.parseFrom(\n            PBHelperClient.vintPrefixed(proxyReply));\n\n        String logInfo = \"copy block \" + block + \" from \"\n            + proxySock.getRemoteSocketAddress();\n        DataTransferProtoUtil.checkBlockOpStatus(copyResponse, logInfo, true);\n\n        // get checksum info about the block we're copying\n        ReadOpChecksumInfoProto checksumInfo = copyResponse.getReadOpChecksumInfo();\n        DataChecksum remoteChecksum = DataTransferProtoUtil.fromProto(\n            checksumInfo.getChecksum());\n        // open a block receiver and check if the block does not exist\n        setCurrentBlockReceiver(getBlockReceiver(block, storageType,\n            proxyReply, proxySock.getRemoteSocketAddress().toString(),\n            proxySock.getLocalSocketAddress().toString(),\n            null, 0, 0, 0, \"\", null, datanode, remoteChecksum,\n            CachingStrategy.newDropBehind(), false, false));\n        \n        // receive a block\n        blockReceiver.receiveBlock(null, null, replyOut, null, \n            dataXceiverServer.balanceThrottler, null, true);\n        \n        // notify name node\n        final Replica r = blockReceiver.getReplica();\n        datanode.notifyNamenodeReceivedBlock(\n            block, delHint, r.getStorageUuid(), r.isOnTransientStorage());\n        \n        LOG.info(\"Moved \" + block + \" from \" + peer.getRemoteAddressString()\n            + \", delHint=\" + delHint);\n      }\n    } catch (IOException ioe) {\n      opStatus = ERROR;\n      if (ioe instanceof BlockPinningException) {\n        opStatus = Status.ERROR_BLOCK_PINNED;\n      }\n      errMsg = \"opReplaceBlock \" + block + \" received exception \" + ioe; \n      LOG.info(errMsg);\n      if (!IoeDuringCopyBlockOperation) {\n        // Don't double count IO errors\n        incrDatanodeNetworkErrors();\n      }\n      throw ioe;\n    } finally {\n      // receive the last byte that indicates the proxy released its thread resource\n      if (opStatus == SUCCESS && proxyReply != null) {\n        try {\n          proxyReply.readChar();\n        } catch (IOException ignored) {\n        }\n      }\n      \n      // now release the thread resource\n      dataXceiverServer.balanceThrottler.release();\n      \n      // send response back\n      try {\n        sendResponse(opStatus, errMsg);\n      } catch (IOException ioe) {\n        LOG.warn(\"Error writing reply back to \" + peer.getRemoteAddressString());\n        incrDatanodeNetworkErrors();\n      }\n      IOUtils.closeStream(proxyOut);\n      IOUtils.closeStream(blockReceiver);\n      IOUtils.closeStream(proxyReply);\n      IOUtils.closeStream(replyOut);\n    }\n\n    //update metrics\n    datanode.metrics.addReplaceBlockOp(elapsed());\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.balancer.Dispatcher.dispatch": {
                "code_before_change": "    private void dispatch() {\n      LOG.info(\"Start moving \" + this);\n      assert !(reportedBlock instanceof DBlockStriped);\n\n      Socket sock = new Socket();\n      DataOutputStream out = null;\n      DataInputStream in = null;\n      try {\n        sock.connect(\n            NetUtils.createSocketAddr(target.getDatanodeInfo().\n                getXferAddr(Dispatcher.this.connectToDnViaHostname)),\n                HdfsConstants.READ_TIMEOUT);\n\n        // Set read timeout so that it doesn't hang forever against\n        // unresponsive nodes. Datanode normally sends IN_PROGRESS response\n        // twice within the client read timeout period (every 30 seconds by\n        // default). Here, we make it give up after 5 minutes of no response.\n        sock.setSoTimeout(HdfsConstants.READ_TIMEOUT * 5);\n        sock.setKeepAlive(true);\n\n        OutputStream unbufOut = sock.getOutputStream();\n        InputStream unbufIn = sock.getInputStream();\n        ExtendedBlock eb = new ExtendedBlock(nnc.getBlockpoolID(),\n            reportedBlock.getBlock());\n        final KeyManager km = nnc.getKeyManager(); \n        Token<BlockTokenIdentifier> accessToken = km.getAccessToken(eb);\n        IOStreamPair saslStreams = saslClient.socketSend(sock, unbufOut,\n            unbufIn, km, accessToken, target.getDatanodeInfo());\n        unbufOut = saslStreams.out;\n        unbufIn = saslStreams.in;\n        out = new DataOutputStream(new BufferedOutputStream(unbufOut,\n            ioFileBufferSize));\n        in = new DataInputStream(new BufferedInputStream(unbufIn,\n            ioFileBufferSize));\n\n        sendRequest(out, eb, accessToken);\n        receiveResponse(in);\n        nnc.getBytesMoved().addAndGet(reportedBlock.getNumBytes());\n        target.getDDatanode().setHasSuccess();\n        LOG.info(\"Successfully moved \" + this);\n      } catch (IOException e) {\n        LOG.warn(\"Failed to move \" + this, e);\n        target.getDDatanode().setHasFailure();\n        // Proxy or target may have some issues, delay before using these nodes\n        // further in order to avoid a potential storm of \"threads quota\n        // exceeded\" warnings when the dispatcher gets out of sync with work\n        // going on in datanodes.\n        proxySource.activateDelay(delayAfterErrors);\n        target.getDDatanode().activateDelay(delayAfterErrors);\n      } finally {\n        IOUtils.closeStream(out);\n        IOUtils.closeStream(in);\n        IOUtils.closeSocket(sock);\n\n        proxySource.removePendingBlock(this);\n        target.getDDatanode().removePendingBlock(this);\n\n        synchronized (this) {\n          reset();\n        }\n        synchronized (Dispatcher.this) {\n          Dispatcher.this.notifyAll();\n        }\n      }\n    }",
                "code_after_change": "    private void dispatch() {\n      LOG.info(\"Start moving \" + this);\n      assert !(reportedBlock instanceof DBlockStriped);\n\n      Socket sock = new Socket();\n      DataOutputStream out = null;\n      DataInputStream in = null;\n      try {\n        sock.connect(\n            NetUtils.createSocketAddr(target.getDatanodeInfo().\n                getXferAddr(Dispatcher.this.connectToDnViaHostname)),\n                HdfsConstants.READ_TIMEOUT);\n\n        // Set read timeout so that it doesn't hang forever against\n        // unresponsive nodes. Datanode normally sends IN_PROGRESS response\n        // twice within the client read timeout period (every 30 seconds by\n        // default). Here, we make it give up after 5 minutes of no response.\n        sock.setSoTimeout(HdfsConstants.READ_TIMEOUT * 5);\n        sock.setKeepAlive(true);\n\n        OutputStream unbufOut = sock.getOutputStream();\n        InputStream unbufIn = sock.getInputStream();\n        ExtendedBlock eb = new ExtendedBlock(nnc.getBlockpoolID(),\n            reportedBlock.getBlock());\n        final KeyManager km = nnc.getKeyManager(); \n        Token<BlockTokenIdentifier> accessToken = km.getAccessToken(eb);\n        IOStreamPair saslStreams = saslClient.socketSend(sock, unbufOut,\n            unbufIn, km, accessToken, target.getDatanodeInfo());\n        unbufOut = saslStreams.out;\n        unbufIn = saslStreams.in;\n        out = new DataOutputStream(new BufferedOutputStream(unbufOut,\n            ioFileBufferSize));\n        in = new DataInputStream(new BufferedInputStream(unbufIn,\n            ioFileBufferSize));\n\n        sendRequest(out, eb, accessToken);\n        receiveResponse(in);\n        nnc.getBytesMoved().addAndGet(reportedBlock.getNumBytes());\n        target.getDDatanode().setHasSuccess();\n        LOG.info(\"Successfully moved \" + this);\n      } catch (IOException e) {\n        LOG.warn(\"Failed to move \" + this, e);\n        target.getDDatanode().setHasFailure();\n        // Check that the failure is due to block pinning errors.\n        if (e instanceof BlockPinningException) {\n          // Pinned block can't be moved. Add this block into failure list.\n          // Later in the next iteration mover will exclude these blocks from\n          // pending moves.\n          target.getDDatanode().addBlockPinningFailures(this);\n          return;\n        }\n\n        // Proxy or target may have some issues, delay before using these nodes\n        // further in order to avoid a potential storm of \"threads quota\n        // exceeded\" warnings when the dispatcher gets out of sync with work\n        // going on in datanodes.\n        proxySource.activateDelay(delayAfterErrors);\n        target.getDDatanode().activateDelay(delayAfterErrors);\n      } finally {\n        IOUtils.closeStream(out);\n        IOUtils.closeStream(in);\n        IOUtils.closeSocket(sock);\n\n        proxySource.removePendingBlock(this);\n        target.getDDatanode().removePendingBlock(this);\n\n        synchronized (this) {\n          reset();\n        }\n        synchronized (Dispatcher.this) {\n          Dispatcher.this.notifyAll();\n        }\n      }\n    }"
            },
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.DataTransferProtoUtil.fromProto": {
                "code_before_change": "  static BlockConstructionStage fromProto(\n      OpWriteBlockProto.BlockConstructionStage stage) {\n    return BlockConstructionStage.valueOf(stage.name());\n  }",
                "code_after_change": "  static BlockConstructionStage fromProto(\n      OpWriteBlockProto.BlockConstructionStage stage) {\n    return BlockConstructionStage.valueOf(stage.name());\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.balancer.Dispatcher.setHasFailure": {
                "code_before_change": "    void setHasFailure() {\n      this.hasFailure = true;\n    }",
                "code_after_change": "    void setHasFailure() {\n      this.hasFailure = true;\n    }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.balancer.Dispatcher.equals": {
                "code_before_change": "      public boolean equals(Object obj) {\n        if (this == obj) {\n          return true;\n        } else if (obj == null || !(obj instanceof StorageGroup)) {\n          return false;\n        } else {\n          final StorageGroup that = (StorageGroup) obj;\n          return this.getStorageType() == that.getStorageType()\n              && this.getDatanodeInfo().equals(that.getDatanodeInfo());\n        }\n      }",
                "code_after_change": "      public boolean equals(Object obj) {\n        if (this == obj) {\n          return true;\n        } else if (obj == null || !(obj instanceof StorageGroup)) {\n          return false;\n        } else {\n          final StorageGroup that = (StorageGroup) obj;\n          return this.getStorageType() == that.getStorageType()\n              && this.getDatanodeInfo().equals(that.getDatanodeInfo());\n        }\n      }"
            },
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.protocol.datatransfer.DataTransferProtoUtil.checkBlockOpStatus": {
                "code_before_change": "  public static void checkBlockOpStatus(\n          BlockOpResponseProto response,\n          String logInfo) throws IOException {\n    if (response.getStatus() != Status.SUCCESS) {\n      if (response.getStatus() == Status.ERROR_ACCESS_TOKEN) {\n        throw new InvalidBlockTokenException(\n          \"Got access token error\"\n          + \", status message \" + response.getMessage()\n          + \", \" + logInfo\n        );\n      } else {\n        throw new IOException(\n          \"Got error\"\n          + \", status=\" + response.getStatus().name()\n          + \", status message \" + response.getMessage()\n          + \", \" + logInfo\n        );\n      }\n    }\n  }",
                "code_after_change": "  public static void checkBlockOpStatus(\n          BlockOpResponseProto response,\n          String logInfo) throws IOException {\n    checkBlockOpStatus(response, logInfo, false);\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.balancer.Dispatcher.PendingMove": {
                "code_before_change": [],
                "code_after_change": []
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.mover.Mover.run": {
                "code_before_change": "  private ExitStatus run() {\n    try {\n      init();\n      return new Processor().processNamespace().getExitStatus();\n    } catch (IllegalArgumentException e) {\n      System.out.println(e + \".  Exiting ...\");\n      return ExitStatus.ILLEGAL_ARGUMENTS;\n    } catch (IOException e) {\n      System.out.println(e + \".  Exiting ...\");\n      return ExitStatus.IO_EXCEPTION;\n    } finally {\n      dispatcher.shutdownNow();\n    }\n  }",
                "code_after_change": "  private ExitStatus run() {\n    try {\n      init();\n      return new Processor().processNamespace().getExitStatus();\n    } catch (IllegalArgumentException e) {\n      System.out.println(e + \".  Exiting ...\");\n      return ExitStatus.ILLEGAL_ARGUMENTS;\n    } catch (IOException e) {\n      System.out.println(e + \".  Exiting ...\");\n      return ExitStatus.IO_EXCEPTION;\n    } finally {\n      dispatcher.shutdownNow();\n    }\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.mover.Mover.getTargetStorages": {
                "code_before_change": "    private List<StorageGroup> getTargetStorages(StorageType t) {\n      return targetStorageTypeMap.get(t);\n    }",
                "code_after_change": "    private List<StorageGroup> getTargetStorages(StorageType t) {\n      return targetStorageTypeMap.get(t);\n    }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataXceiver.copyBlock": {
                "code_before_change": "  public void copyBlock(final ExtendedBlock block,\n      final Token<BlockTokenIdentifier> blockToken) throws IOException {\n    updateCurrentThreadName(\"Copying block \" + block);\n    DataOutputStream reply = getBufferedOutputStream();\n    checkAccess(reply, true, block, blockToken,\n        Op.COPY_BLOCK, BlockTokenIdentifier.AccessMode.COPY);\n\n    if (datanode.data.getPinning(block)) {\n      String msg = \"Not able to copy block \" + block.getBlockId() + \" \" +\n          \"to \" + peer.getRemoteAddressString() + \" because it's pinned \";\n      LOG.info(msg);\n      sendResponse(ERROR, msg);\n      return;\n    }\n    \n    if (!dataXceiverServer.balanceThrottler.acquire()) { // not able to start\n      String msg = \"Not able to copy block \" + block.getBlockId() + \" \" +\n          \"to \" + peer.getRemoteAddressString() + \" because threads \" +\n          \"quota is exceeded.\";\n      LOG.info(msg);\n      sendResponse(ERROR, msg);\n      return;\n    }\n\n    BlockSender blockSender = null;\n    boolean isOpSuccess = true;\n\n    try {\n      // check if the block exists or not\n      blockSender = new BlockSender(block, 0, -1, false, false, true, datanode, \n          null, CachingStrategy.newDropBehind());\n\n      OutputStream baseStream = getOutputStream();\n\n      // send status first\n      writeSuccessWithChecksumInfo(blockSender, reply);\n\n      long beginRead = Time.monotonicNow();\n      // send block content to the target\n      long read = blockSender.sendBlock(reply, baseStream,\n                                        dataXceiverServer.balanceThrottler);\n      long duration = Time.monotonicNow() - beginRead;\n      datanode.metrics.incrBytesRead((int) read);\n      datanode.metrics.incrBlocksRead();\n      datanode.metrics.incrTotalReadTime(duration);\n      \n      LOG.info(\"Copied \" + block + \" to \" + peer.getRemoteAddressString());\n    } catch (IOException ioe) {\n      isOpSuccess = false;\n      LOG.info(\"opCopyBlock \" + block + \" received exception \" + ioe);\n      incrDatanodeNetworkErrors();\n      throw ioe;\n    } finally {\n      dataXceiverServer.balanceThrottler.release();\n      if (isOpSuccess) {\n        try {\n          // send one last byte to indicate that the resource is cleaned.\n          reply.writeChar('d');\n        } catch (IOException ignored) {\n        }\n      }\n      IOUtils.closeStream(reply);\n      IOUtils.closeStream(blockSender);\n    }\n\n    //update metrics    \n    datanode.metrics.addCopyBlockOp(elapsed());\n  }",
                "code_after_change": "  public void copyBlock(final ExtendedBlock block,\n      final Token<BlockTokenIdentifier> blockToken) throws IOException {\n    updateCurrentThreadName(\"Copying block \" + block);\n    DataOutputStream reply = getBufferedOutputStream();\n    checkAccess(reply, true, block, blockToken,\n        Op.COPY_BLOCK, BlockTokenIdentifier.AccessMode.COPY);\n\n    if (datanode.data.getPinning(block)) {\n      String msg = \"Not able to copy block \" + block.getBlockId() + \" \" +\n          \"to \" + peer.getRemoteAddressString() + \" because it's pinned \";\n      LOG.info(msg);\n      sendResponse(Status.ERROR_BLOCK_PINNED, msg);\n      return;\n    }\n    \n    if (!dataXceiverServer.balanceThrottler.acquire()) { // not able to start\n      String msg = \"Not able to copy block \" + block.getBlockId() + \" \" +\n          \"to \" + peer.getRemoteAddressString() + \" because threads \" +\n          \"quota is exceeded.\";\n      LOG.info(msg);\n      sendResponse(ERROR, msg);\n      return;\n    }\n\n    BlockSender blockSender = null;\n    boolean isOpSuccess = true;\n\n    try {\n      // check if the block exists or not\n      blockSender = new BlockSender(block, 0, -1, false, false, true, datanode, \n          null, CachingStrategy.newDropBehind());\n\n      OutputStream baseStream = getOutputStream();\n\n      // send status first\n      writeSuccessWithChecksumInfo(blockSender, reply);\n\n      long beginRead = Time.monotonicNow();\n      // send block content to the target\n      long read = blockSender.sendBlock(reply, baseStream,\n                                        dataXceiverServer.balanceThrottler);\n      long duration = Time.monotonicNow() - beginRead;\n      datanode.metrics.incrBytesRead((int) read);\n      datanode.metrics.incrBlocksRead();\n      datanode.metrics.incrTotalReadTime(duration);\n      \n      LOG.info(\"Copied \" + block + \" to \" + peer.getRemoteAddressString());\n    } catch (IOException ioe) {\n      isOpSuccess = false;\n      LOG.info(\"opCopyBlock \" + block + \" received exception \" + ioe);\n      incrDatanodeNetworkErrors();\n      throw ioe;\n    } finally {\n      dataXceiverServer.balanceThrottler.release();\n      if (isOpSuccess) {\n        try {\n          // send one last byte to indicate that the resource is cleaned.\n          reply.writeChar('d');\n        } catch (IOException ignored) {\n        }\n      }\n      IOUtils.closeStream(reply);\n      IOUtils.closeStream(blockSender);\n    }\n\n    //update metrics    \n    datanode.metrics.addCopyBlockOp(elapsed());\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.mover.Mover.scheduleMoveReplica": {
                "code_before_change": "    boolean scheduleMoveReplica(DBlock db, MLocation ml,\n                                List<StorageType> targetTypes) {\n      final Source source = storages.getSource(ml);\n      return source == null ? false : scheduleMoveReplica(db, source,\n          targetTypes);\n    }",
                "code_after_change": "    boolean scheduleMoveReplica(DBlock db, MLocation ml,\n                                List<StorageType> targetTypes) {\n      final Source source = storages.getSource(ml);\n      return source == null ? false : scheduleMoveReplica(db, source,\n          targetTypes);\n    }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the issue of unnecessary retries due to block pinning, which is related to the methods in the stack trace, such as `Dispatcher.receiveResponse` and `DataTransferProtoUtil.checkBlockOpStatus`. However, it does not pinpoint the exact root cause in the ground truth methods. The report does not provide a specific fix suggestion, hence 'Missing' for fix suggestion. The problem location is partially identified as it mentions methods in the stack trace context but not the exact ground truth methods. There is no incorrect information in the bug report."
        }
    },
    {
        "filename": "HDFS-5291.json",
        "code_diff": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.cancelDelegationToken": {
                "code_before_change": "  void cancelDelegationToken(Token<DelegationTokenIdentifier> token)\n      throws IOException {\n    checkOperation(OperationCategory.WRITE);\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n\n      if (isInSafeMode()) {\n        throw new SafeModeException(\"Cannot cancel delegation token\", safeMode);\n      }\n      String canceller = getRemoteUser().getUserName();\n      DelegationTokenIdentifier id = dtSecretManager\n        .cancelToken(token, canceller);\n      getEditLog().logCancelDelegationToken(id);\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n  }",
                "code_after_change": "  void cancelDelegationToken(Token<DelegationTokenIdentifier> token)\n      throws IOException {\n    checkOperation(OperationCategory.WRITE);\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n\n      checkNameNodeSafeMode(\"Cannot cancel delegation token\");\n      String canceller = getRemoteUser().getUserName();\n      DelegationTokenIdentifier id = dtSecretManager\n        .cancelToken(token, canceller);\n      getEditLog().logCancelDelegationToken(id);\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.fsync": {
                "code_before_change": "  void fsync(String src, String clientName, long lastBlockLength) \n      throws IOException, UnresolvedLinkException {\n    NameNode.stateChangeLog.info(\"BLOCK* fsync: \" + src + \" for \" + clientName);\n    checkOperation(OperationCategory.WRITE);\n    byte[][] pathComponents = FSDirectory.getPathComponentsForReservedPath(src);\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      if (isInSafeMode()) {\n        throw new SafeModeException(\"Cannot fsync file \" + src, safeMode);\n      }\n      src = FSDirectory.resolvePath(src, pathComponents, dir);\n      INodeFileUnderConstruction pendingFile  = checkLease(src, clientName);\n      if (lastBlockLength > 0) {\n        pendingFile.updateLengthOfLastBlock(lastBlockLength);\n      }\n      dir.persistBlocks(src, pendingFile, false);\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n  }",
                "code_after_change": "  void fsync(String src, String clientName, long lastBlockLength) \n      throws IOException, UnresolvedLinkException {\n    NameNode.stateChangeLog.info(\"BLOCK* fsync: \" + src + \" for \" + clientName);\n    checkOperation(OperationCategory.WRITE);\n    byte[][] pathComponents = FSDirectory.getPathComponentsForReservedPath(src);\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      checkNameNodeSafeMode(\"Cannot fsync file \" + src);\n      src = FSDirectory.resolvePath(src, pathComponents, dir);\n      INodeFileUnderConstruction pendingFile  = checkLease(src, clientName);\n      if (lastBlockLength > 0) {\n        pendingFile.updateLengthOfLastBlock(lastBlockLength);\n      }\n      dir.persistBlocks(src, pendingFile, false);\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.appendFileInt": {
                "code_before_change": "  private LocatedBlock appendFileInt(String src, String holder,\n      String clientMachine, boolean logRetryCache)\n      throws AccessControlException, SafeModeException,\n      FileAlreadyExistsException, FileNotFoundException,\n      ParentNotDirectoryException, IOException {\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* NameSystem.appendFile: src=\" + src\n          + \", holder=\" + holder\n          + \", clientMachine=\" + clientMachine);\n    }\n    boolean skipSync = false;\n    if (!supportAppends) {\n      throw new UnsupportedOperationException(\n          \"Append is not enabled on this NameNode. Use the \" +\n          DFS_SUPPORT_APPEND_KEY + \" configuration option to enable it.\");\n    }\n\n    LocatedBlock lb = null;\n    FSPermissionChecker pc = getPermissionChecker();\n    checkOperation(OperationCategory.WRITE);\n    byte[][] pathComponents = FSDirectory.getPathComponentsForReservedPath(src);\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      if (isInSafeMode()) {\n        throw new SafeModeException(\"Cannot append to file\" + src, safeMode);\n      }\n      src = FSDirectory.resolvePath(src, pathComponents, dir);\n      lb = appendFileInternal(pc, src, holder, clientMachine, logRetryCache);\n    } catch (StandbyException se) {\n      skipSync = true;\n      throw se;\n    } finally {\n      writeUnlock();\n      // There might be transactions logged while trying to recover the lease.\n      // They need to be sync'ed even when an exception was thrown.\n      if (!skipSync) {\n        getEditLog().logSync();\n      }\n    }\n    if (lb != null) {\n      if (NameNode.stateChangeLog.isDebugEnabled()) {\n        NameNode.stateChangeLog.debug(\"DIR* NameSystem.appendFile: file \"\n            +src+\" for \"+holder+\" at \"+clientMachine\n            +\" block \" + lb.getBlock()\n            +\" block size \" + lb.getBlock().getNumBytes());\n      }\n    }\n    logAuditEvent(true, \"append\", src);\n    return lb;\n  }",
                "code_after_change": "  private LocatedBlock appendFileInt(String src, String holder,\n      String clientMachine, boolean logRetryCache)\n      throws AccessControlException, SafeModeException,\n      FileAlreadyExistsException, FileNotFoundException,\n      ParentNotDirectoryException, IOException {\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* NameSystem.appendFile: src=\" + src\n          + \", holder=\" + holder\n          + \", clientMachine=\" + clientMachine);\n    }\n    boolean skipSync = false;\n    if (!supportAppends) {\n      throw new UnsupportedOperationException(\n          \"Append is not enabled on this NameNode. Use the \" +\n          DFS_SUPPORT_APPEND_KEY + \" configuration option to enable it.\");\n    }\n\n    LocatedBlock lb = null;\n    FSPermissionChecker pc = getPermissionChecker();\n    checkOperation(OperationCategory.WRITE);\n    byte[][] pathComponents = FSDirectory.getPathComponentsForReservedPath(src);\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      checkNameNodeSafeMode(\"Cannot append to file\" + src);\n      src = FSDirectory.resolvePath(src, pathComponents, dir);\n      lb = appendFileInternal(pc, src, holder, clientMachine, logRetryCache);\n    } catch (StandbyException se) {\n      skipSync = true;\n      throw se;\n    } finally {\n      writeUnlock();\n      // There might be transactions logged while trying to recover the lease.\n      // They need to be sync'ed even when an exception was thrown.\n      if (!skipSync) {\n        getEditLog().logSync();\n      }\n    }\n    if (lb != null) {\n      if (NameNode.stateChangeLog.isDebugEnabled()) {\n        NameNode.stateChangeLog.debug(\"DIR* NameSystem.appendFile: file \"\n            +src+\" for \"+holder+\" at \"+clientMachine\n            +\" block \" + lb.getBlock()\n            +\" block size \" + lb.getBlock().getNumBytes());\n      }\n    }\n    logAuditEvent(true, \"append\", src);\n    return lb;\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.endCheckpoint": {
                "code_before_change": "  void endCheckpoint(NamenodeRegistration registration,\n                            CheckpointSignature sig) throws IOException {\n    CacheEntry cacheEntry = RetryCache.waitForCompletion(retryCache);\n    if (cacheEntry != null && cacheEntry.isSuccess()) {\n      return; // Return previous response\n    }\n    checkOperation(OperationCategory.CHECKPOINT);\n    boolean success = false;\n    readLock();\n    try {\n      checkOperation(OperationCategory.CHECKPOINT);\n\n      if (isInSafeMode()) {\n        throw new SafeModeException(\"Checkpoint not ended\", safeMode);\n      }\n      LOG.info(\"End checkpoint for \" + registration.getAddress());\n      getFSImage().endCheckpoint(sig);\n      success = true;\n    } finally {\n      readUnlock();\n      RetryCache.setState(cacheEntry, success);\n    }\n  }",
                "code_after_change": "  void endCheckpoint(NamenodeRegistration registration,\n                            CheckpointSignature sig) throws IOException {\n    checkOperation(OperationCategory.CHECKPOINT);\n    CacheEntry cacheEntry = RetryCache.waitForCompletion(retryCache);\n    if (cacheEntry != null && cacheEntry.isSuccess()) {\n      return; // Return previous response\n    }\n    boolean success = false;\n    readLock();\n    try {\n      checkOperation(OperationCategory.CHECKPOINT);\n\n      checkNameNodeSafeMode(\"Checkpoint not ended\");\n      LOG.info(\"End checkpoint for \" + registration.getAddress());\n      getFSImage().endCheckpoint(sig);\n      success = true;\n    } finally {\n      readUnlock();\n      RetryCache.setState(cacheEntry, success);\n    }\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getLastAllocatedBlockId": {
                "code_before_change": "  long getLastAllocatedBlockId() {\n    return blockIdGenerator.getCurrentValue();\n  }",
                "code_after_change": "  long getLastAllocatedBlockId() {\n    return blockIdGenerator.getCurrentValue();\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.setTimesInt": {
                "code_before_change": "  private void setTimesInt(String src, long mtime, long atime) \n    throws IOException, UnresolvedLinkException {\n    HdfsFileStatus resultingStat = null;\n    FSPermissionChecker pc = getPermissionChecker();\n    checkOperation(OperationCategory.WRITE);\n    byte[][] pathComponents = FSDirectory.getPathComponentsForReservedPath(src);\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      if (isInSafeMode()) {\n        throw new SafeModeException(\"Cannot set times \" + src, safeMode);\n      }\n      src = FSDirectory.resolvePath(src, pathComponents, dir);\n\n      // Write access is required to set access and modification times\n      if (isPermissionEnabled) {\n        checkPathAccess(pc, src, FsAction.WRITE);\n      }\n      final INodesInPath iip = dir.getINodesInPath4Write(src);\n      final INode inode = iip.getLastINode();\n      if (inode != null) {\n        dir.setTimes(src, inode, mtime, atime, true, iip.getLatestSnapshot());\n        resultingStat = getAuditFileInfo(src, false);\n      } else {\n        throw new FileNotFoundException(\"File/Directory \" + src + \" does not exist.\");\n      }\n    } finally {\n      writeUnlock();\n    }\n    logAuditEvent(true, \"setTimes\", src, null, resultingStat);\n  }",
                "code_after_change": "  private void setTimesInt(String src, long mtime, long atime) \n    throws IOException, UnresolvedLinkException {\n    HdfsFileStatus resultingStat = null;\n    FSPermissionChecker pc = getPermissionChecker();\n    checkOperation(OperationCategory.WRITE);\n    byte[][] pathComponents = FSDirectory.getPathComponentsForReservedPath(src);\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      checkNameNodeSafeMode(\"Cannot set times \" + src);\n      src = FSDirectory.resolvePath(src, pathComponents, dir);\n\n      // Write access is required to set access and modification times\n      if (isPermissionEnabled) {\n        checkPathAccess(pc, src, FsAction.WRITE);\n      }\n      final INodesInPath iip = dir.getINodesInPath4Write(src);\n      final INode inode = iip.getLastINode();\n      if (inode != null) {\n        dir.setTimes(src, inode, mtime, atime, true, iip.getLatestSnapshot());\n        resultingStat = getAuditFileInfo(src, false);\n      } else {\n        throw new FileNotFoundException(\"File/Directory \" + src + \" does not exist.\");\n      }\n    } finally {\n      writeUnlock();\n    }\n    logAuditEvent(true, \"setTimes\", src, null, resultingStat);\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInt": {
                "code_before_change": "  private HdfsFileStatus startFileInt(String src, PermissionStatus permissions,\n      String holder, String clientMachine, EnumSet<CreateFlag> flag,\n      boolean createParent, short replication, long blockSize,\n      boolean logRetryCache) throws AccessControlException, SafeModeException,\n      FileAlreadyExistsException, UnresolvedLinkException,\n      FileNotFoundException, ParentNotDirectoryException, IOException {\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* NameSystem.startFile: src=\" + src\n          + \", holder=\" + holder\n          + \", clientMachine=\" + clientMachine\n          + \", createParent=\" + createParent\n          + \", replication=\" + replication\n          + \", createFlag=\" + flag.toString());\n    }\n    if (!DFSUtil.isValidName(src)) {\n      throw new InvalidPathException(src);\n    }\n    blockManager.verifyReplication(src, replication, clientMachine);\n\n    boolean skipSync = false;\n    HdfsFileStatus stat = null;\n    FSPermissionChecker pc = getPermissionChecker();\n    checkOperation(OperationCategory.WRITE);\n    if (blockSize < minBlockSize) {\n      throw new IOException(\"Specified block size is less than configured\" +\n          \" minimum value (\" + DFSConfigKeys.DFS_NAMENODE_MIN_BLOCK_SIZE_KEY\n          + \"): \" + blockSize + \" < \" + minBlockSize);\n    }\n    byte[][] pathComponents = FSDirectory.getPathComponentsForReservedPath(src);\n    boolean create = flag.contains(CreateFlag.CREATE);\n    boolean overwrite = flag.contains(CreateFlag.OVERWRITE);\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      if (isInSafeMode()) {\n        throw new SafeModeException(\"Cannot create file\" + src, safeMode);\n      }\n      src = FSDirectory.resolvePath(src, pathComponents, dir);\n      startFileInternal(pc, src, permissions, holder, clientMachine, create,\n          overwrite, createParent, replication, blockSize, logRetryCache);\n      stat = dir.getFileInfo(src, false);\n    } catch (StandbyException se) {\n      skipSync = true;\n      throw se;\n    } finally {\n      writeUnlock();\n      // There might be transactions logged while trying to recover the lease.\n      // They need to be sync'ed even when an exception was thrown.\n      if (!skipSync) {\n        getEditLog().logSync();\n      }\n    } \n\n    logAuditEvent(true, \"create\", src, null, stat);\n    return stat;\n  }",
                "code_after_change": "  private HdfsFileStatus startFileInt(String src, PermissionStatus permissions,\n      String holder, String clientMachine, EnumSet<CreateFlag> flag,\n      boolean createParent, short replication, long blockSize,\n      boolean logRetryCache) throws AccessControlException, SafeModeException,\n      FileAlreadyExistsException, UnresolvedLinkException,\n      FileNotFoundException, ParentNotDirectoryException, IOException {\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* NameSystem.startFile: src=\" + src\n          + \", holder=\" + holder\n          + \", clientMachine=\" + clientMachine\n          + \", createParent=\" + createParent\n          + \", replication=\" + replication\n          + \", createFlag=\" + flag.toString());\n    }\n    if (!DFSUtil.isValidName(src)) {\n      throw new InvalidPathException(src);\n    }\n    blockManager.verifyReplication(src, replication, clientMachine);\n\n    boolean skipSync = false;\n    HdfsFileStatus stat = null;\n    FSPermissionChecker pc = getPermissionChecker();\n    checkOperation(OperationCategory.WRITE);\n    if (blockSize < minBlockSize) {\n      throw new IOException(\"Specified block size is less than configured\" +\n          \" minimum value (\" + DFSConfigKeys.DFS_NAMENODE_MIN_BLOCK_SIZE_KEY\n          + \"): \" + blockSize + \" < \" + minBlockSize);\n    }\n    byte[][] pathComponents = FSDirectory.getPathComponentsForReservedPath(src);\n    boolean create = flag.contains(CreateFlag.CREATE);\n    boolean overwrite = flag.contains(CreateFlag.OVERWRITE);\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      checkNameNodeSafeMode(\"Cannot create file\" + src);\n      src = FSDirectory.resolvePath(src, pathComponents, dir);\n      startFileInternal(pc, src, permissions, holder, clientMachine, create,\n          overwrite, createParent, replication, blockSize, logRetryCache);\n      stat = dir.getFileInfo(src, false);\n    } catch (StandbyException se) {\n      skipSync = true;\n      throw se;\n    } finally {\n      writeUnlock();\n      // There might be transactions logged while trying to recover the lease.\n      // They need to be sync'ed even when an exception was thrown.\n      if (!skipSync) {\n        getEditLog().logSync();\n      }\n    } \n\n    logAuditEvent(true, \"create\", src, null, stat);\n    return stat;\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.setReplicationInt": {
                "code_before_change": "  private boolean setReplicationInt(String src, final short replication)\n      throws IOException {\n    blockManager.verifyReplication(src, replication, null);\n    final boolean isFile;\n    FSPermissionChecker pc = getPermissionChecker();\n    checkOperation(OperationCategory.WRITE);\n    byte[][] pathComponents = FSDirectory.getPathComponentsForReservedPath(src);\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      if (isInSafeMode()) {\n        throw new SafeModeException(\"Cannot set replication for \" + src, safeMode);\n      }\n      src = FSDirectory.resolvePath(src, pathComponents, dir);\n      if (isPermissionEnabled) {\n        checkPathAccess(pc, src, FsAction.WRITE);\n      }\n\n      final short[] blockRepls = new short[2]; // 0: old, 1: new\n      final Block[] blocks = dir.setReplication(src, replication, blockRepls);\n      isFile = blocks != null;\n      if (isFile) {\n        blockManager.setReplication(blockRepls[0], blockRepls[1], src, blocks);\n      }\n    } finally {\n      writeUnlock();\n    }\n\n    getEditLog().logSync();\n    if (isFile) {\n      logAuditEvent(true, \"setReplication\", src);\n    }\n    return isFile;\n  }",
                "code_after_change": "  private boolean setReplicationInt(String src, final short replication)\n      throws IOException {\n    blockManager.verifyReplication(src, replication, null);\n    final boolean isFile;\n    FSPermissionChecker pc = getPermissionChecker();\n    checkOperation(OperationCategory.WRITE);\n    byte[][] pathComponents = FSDirectory.getPathComponentsForReservedPath(src);\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      checkNameNodeSafeMode(\"Cannot set replication for \" + src);\n      src = FSDirectory.resolvePath(src, pathComponents, dir);\n      if (isPermissionEnabled) {\n        checkPathAccess(pc, src, FsAction.WRITE);\n      }\n\n      final short[] blockRepls = new short[2]; // 0: old, 1: new\n      final Block[] blocks = dir.setReplication(src, replication, blockRepls);\n      isFile = blocks != null;\n      if (isFile) {\n        blockManager.setReplication(blockRepls[0], blockRepls[1], src, blocks);\n      }\n    } finally {\n      writeUnlock();\n    }\n\n    getEditLog().logSync();\n    if (isFile) {\n      logAuditEvent(true, \"setReplication\", src);\n    }\n    return isFile;\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.analyzeFileState": {
                "code_before_change": "  INodesInPath analyzeFileState(String src,\n                                long fileId,\n                                String clientName,\n                                ExtendedBlock previous,\n                                LocatedBlock[] onRetryBlock)\n          throws IOException  {\n    assert hasReadOrWriteLock();\n\n    checkBlock(previous);\n    onRetryBlock[0] = null;\n    checkOperation(OperationCategory.WRITE);\n    if (isInSafeMode()) {\n      throw new SafeModeException(\"Cannot add block to \" + src, safeMode);\n    }\n\n    // have we exceeded the configured limit of fs objects.\n    checkFsObjectLimit();\n\n    Block previousBlock = ExtendedBlock.getLocalBlock(previous);\n    final INodesInPath iip = dir.getINodesInPath4Write(src);\n    final INodeFileUnderConstruction pendingFile\n        = checkLease(src, fileId, clientName, iip.getLastINode());\n    BlockInfo lastBlockInFile = pendingFile.getLastBlock();\n    if (!Block.matchingIdAndGenStamp(previousBlock, lastBlockInFile)) {\n      // The block that the client claims is the current last block\n      // doesn't match up with what we think is the last block. There are\n      // four possibilities:\n      // 1) This is the first block allocation of an append() pipeline\n      //    which started appending exactly at a block boundary.\n      //    In this case, the client isn't passed the previous block,\n      //    so it makes the allocateBlock() call with previous=null.\n      //    We can distinguish this since the last block of the file\n      //    will be exactly a full block.\n      // 2) This is a retry from a client that missed the response of a\n      //    prior getAdditionalBlock() call, perhaps because of a network\n      //    timeout, or because of an HA failover. In that case, we know\n      //    by the fact that the client is re-issuing the RPC that it\n      //    never began to write to the old block. Hence it is safe to\n      //    to return the existing block.\n      // 3) This is an entirely bogus request/bug -- we should error out\n      //    rather than potentially appending a new block with an empty\n      //    one in the middle, etc\n      // 4) This is a retry from a client that timed out while\n      //    the prior getAdditionalBlock() is still being processed,\n      //    currently working on chooseTarget(). \n      //    There are no means to distinguish between the first and \n      //    the second attempts in Part I, because the first one hasn't\n      //    changed the namesystem state yet.\n      //    We run this analysis again in Part II where case 4 is impossible.\n\n      BlockInfo penultimateBlock = pendingFile.getPenultimateBlock();\n      if (previous == null &&\n          lastBlockInFile != null &&\n          lastBlockInFile.getNumBytes() == pendingFile.getPreferredBlockSize() &&\n          lastBlockInFile.isComplete()) {\n        // Case 1\n        if (NameNode.stateChangeLog.isDebugEnabled()) {\n           NameNode.stateChangeLog.debug(\n               \"BLOCK* NameSystem.allocateBlock: handling block allocation\" +\n               \" writing to a file with a complete previous block: src=\" +\n               src + \" lastBlock=\" + lastBlockInFile);\n        }\n      } else if (Block.matchingIdAndGenStamp(penultimateBlock, previousBlock)) {\n        if (lastBlockInFile.getNumBytes() != 0) {\n          throw new IOException(\n              \"Request looked like a retry to allocate block \" +\n              lastBlockInFile + \" but it already contains \" +\n              lastBlockInFile.getNumBytes() + \" bytes\");\n        }\n\n        // Case 2\n        // Return the last block.\n        NameNode.stateChangeLog.info(\"BLOCK* allocateBlock: \" +\n            \"caught retry for allocation of a new block in \" +\n            src + \". Returning previously allocated block \" + lastBlockInFile);\n        long offset = pendingFile.computeFileSize();\n        onRetryBlock[0] = makeLocatedBlock(lastBlockInFile,\n            ((BlockInfoUnderConstruction)lastBlockInFile).getExpectedLocations(),\n            offset);\n        return iip;\n      } else {\n        // Case 3\n        throw new IOException(\"Cannot allocate block in \" + src + \": \" +\n            \"passed 'previous' block \" + previous + \" does not match actual \" +\n            \"last block in file \" + lastBlockInFile);\n      }\n    }\n\n    // Check if the penultimate block is minimally replicated\n    if (!checkFileProgress(pendingFile, false)) {\n      throw new NotReplicatedYetException(\"Not replicated yet: \" + src);\n    }\n    return iip;\n  }",
                "code_after_change": "  INodesInPath analyzeFileState(String src,\n                                long fileId,\n                                String clientName,\n                                ExtendedBlock previous,\n                                LocatedBlock[] onRetryBlock)\n          throws IOException  {\n    assert hasReadOrWriteLock();\n\n    checkBlock(previous);\n    onRetryBlock[0] = null;\n    checkOperation(OperationCategory.WRITE);\n    checkNameNodeSafeMode(\"Cannot add block to \" + src);\n\n    // have we exceeded the configured limit of fs objects.\n    checkFsObjectLimit();\n\n    Block previousBlock = ExtendedBlock.getLocalBlock(previous);\n    final INodesInPath iip = dir.getINodesInPath4Write(src);\n    final INodeFileUnderConstruction pendingFile\n        = checkLease(src, fileId, clientName, iip.getLastINode());\n    BlockInfo lastBlockInFile = pendingFile.getLastBlock();\n    if (!Block.matchingIdAndGenStamp(previousBlock, lastBlockInFile)) {\n      // The block that the client claims is the current last block\n      // doesn't match up with what we think is the last block. There are\n      // four possibilities:\n      // 1) This is the first block allocation of an append() pipeline\n      //    which started appending exactly at a block boundary.\n      //    In this case, the client isn't passed the previous block,\n      //    so it makes the allocateBlock() call with previous=null.\n      //    We can distinguish this since the last block of the file\n      //    will be exactly a full block.\n      // 2) This is a retry from a client that missed the response of a\n      //    prior getAdditionalBlock() call, perhaps because of a network\n      //    timeout, or because of an HA failover. In that case, we know\n      //    by the fact that the client is re-issuing the RPC that it\n      //    never began to write to the old block. Hence it is safe to\n      //    to return the existing block.\n      // 3) This is an entirely bogus request/bug -- we should error out\n      //    rather than potentially appending a new block with an empty\n      //    one in the middle, etc\n      // 4) This is a retry from a client that timed out while\n      //    the prior getAdditionalBlock() is still being processed,\n      //    currently working on chooseTarget(). \n      //    There are no means to distinguish between the first and \n      //    the second attempts in Part I, because the first one hasn't\n      //    changed the namesystem state yet.\n      //    We run this analysis again in Part II where case 4 is impossible.\n\n      BlockInfo penultimateBlock = pendingFile.getPenultimateBlock();\n      if (previous == null &&\n          lastBlockInFile != null &&\n          lastBlockInFile.getNumBytes() == pendingFile.getPreferredBlockSize() &&\n          lastBlockInFile.isComplete()) {\n        // Case 1\n        if (NameNode.stateChangeLog.isDebugEnabled()) {\n           NameNode.stateChangeLog.debug(\n               \"BLOCK* NameSystem.allocateBlock: handling block allocation\" +\n               \" writing to a file with a complete previous block: src=\" +\n               src + \" lastBlock=\" + lastBlockInFile);\n        }\n      } else if (Block.matchingIdAndGenStamp(penultimateBlock, previousBlock)) {\n        if (lastBlockInFile.getNumBytes() != 0) {\n          throw new IOException(\n              \"Request looked like a retry to allocate block \" +\n              lastBlockInFile + \" but it already contains \" +\n              lastBlockInFile.getNumBytes() + \" bytes\");\n        }\n\n        // Case 2\n        // Return the last block.\n        NameNode.stateChangeLog.info(\"BLOCK* allocateBlock: \" +\n            \"caught retry for allocation of a new block in \" +\n            src + \". Returning previously allocated block \" + lastBlockInFile);\n        long offset = pendingFile.computeFileSize();\n        onRetryBlock[0] = makeLocatedBlock(lastBlockInFile,\n            ((BlockInfoUnderConstruction)lastBlockInFile).getExpectedLocations(),\n            offset);\n        return iip;\n      } else {\n        // Case 3\n        throw new IOException(\"Cannot allocate block in \" + src + \": \" +\n            \"passed 'previous' block \" + previous + \" does not match actual \" +\n            \"last block in file \" + lastBlockInFile);\n      }\n    }\n\n    // Check if the penultimate block is minimally replicated\n    if (!checkFileProgress(pendingFile, false)) {\n      throw new NotReplicatedYetException(\"Not replicated yet: \" + src);\n    }\n    return iip;\n  }"
            },
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.io.retry.RetryPolicies.isWrappedStandbyException": {
                "code_before_change": "  private static boolean isWrappedStandbyException(Exception e) {\n    if (!(e instanceof RemoteException)) {\n      return false;\n    }\n    Exception unwrapped = ((RemoteException)e).unwrapRemoteException(\n        StandbyException.class);\n    return unwrapped instanceof StandbyException;\n  }",
                "code_after_change": "  private static boolean isWrappedStandbyException(Exception e) {\n    if (!(e instanceof RemoteException)) {\n      return false;\n    }\n    Exception unwrapped = ((RemoteException)e).unwrapRemoteException(\n        StandbyException.class);\n    return unwrapped instanceof StandbyException;\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startCheckpoint": {
                "code_before_change": "  NamenodeCommand startCheckpoint(NamenodeRegistration backupNode,\n      NamenodeRegistration activeNamenode) throws IOException {\n    checkOperation(OperationCategory.CHECKPOINT);\n    CacheEntryWithPayload cacheEntry = RetryCache.waitForCompletion(retryCache,\n        null);\n    if (cacheEntry != null && cacheEntry.isSuccess()) {\n      return (NamenodeCommand) cacheEntry.getPayload();\n    }\n    writeLock();\n    NamenodeCommand cmd = null;\n    try {\n      checkOperation(OperationCategory.CHECKPOINT);\n\n      if (isInSafeMode()) {\n        throw new SafeModeException(\"Checkpoint not started\", safeMode);\n      }\n      LOG.info(\"Start checkpoint for \" + backupNode.getAddress());\n      cmd = getFSImage().startCheckpoint(backupNode, activeNamenode);\n      getEditLog().logSync();\n      return cmd;\n    } finally {\n      writeUnlock();\n      RetryCache.setState(cacheEntry, cmd != null, cmd);\n    }\n  }",
                "code_after_change": "  NamenodeCommand startCheckpoint(NamenodeRegistration backupNode,\n      NamenodeRegistration activeNamenode) throws IOException {\n    checkOperation(OperationCategory.CHECKPOINT);\n    CacheEntryWithPayload cacheEntry = RetryCache.waitForCompletion(retryCache,\n        null);\n    if (cacheEntry != null && cacheEntry.isSuccess()) {\n      return (NamenodeCommand) cacheEntry.getPayload();\n    }\n    writeLock();\n    NamenodeCommand cmd = null;\n    try {\n      checkOperation(OperationCategory.CHECKPOINT);\n\n      checkNameNodeSafeMode(\"Checkpoint not started\");\n      LOG.info(\"Start checkpoint for \" + backupNode.getAddress());\n      cmd = getFSImage().startCheckpoint(backupNode, activeNamenode);\n      getEditLog().logSync();\n      return cmd;\n    } finally {\n      writeUnlock();\n      RetryCache.setState(cacheEntry, cmd != null, cmd);\n    }\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.renameToInt": {
                "code_before_change": "  private boolean renameToInt(String src, String dst, boolean logRetryCache) \n    throws IOException, UnresolvedLinkException {\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* NameSystem.renameTo: \" + src +\n          \" to \" + dst);\n    }\n    if (!DFSUtil.isValidName(dst)) {\n      throw new IOException(\"Invalid name: \" + dst);\n    }\n    FSPermissionChecker pc = getPermissionChecker();\n    checkOperation(OperationCategory.WRITE);\n    byte[][] srcComponents = FSDirectory.getPathComponentsForReservedPath(src);\n    byte[][] dstComponents = FSDirectory.getPathComponentsForReservedPath(dst);\n    boolean status = false;\n    HdfsFileStatus resultingStat = null;\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      if (isInSafeMode()) {\n        throw new SafeModeException(\"Cannot rename \" + src, safeMode);\n      }\n      src = FSDirectory.resolvePath(src, srcComponents, dir);\n      dst = FSDirectory.resolvePath(dst, dstComponents, dir);\n      checkOperation(OperationCategory.WRITE);\n      status = renameToInternal(pc, src, dst, logRetryCache);\n      if (status) {\n        resultingStat = getAuditFileInfo(dst, false);\n      }\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n    if (status) {\n      logAuditEvent(true, \"rename\", src, dst, resultingStat);\n    }\n    return status;\n  }",
                "code_after_change": "  private boolean renameToInt(String src, String dst, boolean logRetryCache) \n    throws IOException, UnresolvedLinkException {\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* NameSystem.renameTo: \" + src +\n          \" to \" + dst);\n    }\n    if (!DFSUtil.isValidName(dst)) {\n      throw new IOException(\"Invalid name: \" + dst);\n    }\n    FSPermissionChecker pc = getPermissionChecker();\n    checkOperation(OperationCategory.WRITE);\n    byte[][] srcComponents = FSDirectory.getPathComponentsForReservedPath(src);\n    byte[][] dstComponents = FSDirectory.getPathComponentsForReservedPath(dst);\n    boolean status = false;\n    HdfsFileStatus resultingStat = null;\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      checkNameNodeSafeMode(\"Cannot rename \" + src);\n      src = FSDirectory.resolvePath(src, srcComponents, dir);\n      dst = FSDirectory.resolvePath(dst, dstComponents, dir);\n      checkOperation(OperationCategory.WRITE);\n      status = renameToInternal(pc, src, dst, logRetryCache);\n      if (status) {\n        resultingStat = getAuditFileInfo(dst, false);\n      }\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n    if (status) {\n      logAuditEvent(true, \"rename\", src, dst, resultingStat);\n    }\n    return status;\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.setQuota": {
                "code_before_change": "  void setQuota(String path, long nsQuota, long dsQuota) \n      throws IOException, UnresolvedLinkException {\n    checkSuperuserPrivilege();\n    checkOperation(OperationCategory.WRITE);\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      if (isInSafeMode()) {\n        throw new SafeModeException(\"Cannot set quota on \" + path, safeMode);\n      }\n      dir.setQuota(path, nsQuota, dsQuota);\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n  }",
                "code_after_change": "  void setQuota(String path, long nsQuota, long dsQuota) \n      throws IOException, UnresolvedLinkException {\n    checkSuperuserPrivilege();\n    checkOperation(OperationCategory.WRITE);\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      checkNameNodeSafeMode(\"Cannot set quota on \" + path);\n      dir.setQuota(path, nsQuota, dsQuota);\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirsInt": {
                "code_before_change": "  private boolean mkdirsInt(String src, PermissionStatus permissions,\n      boolean createParent) throws IOException, UnresolvedLinkException {\n    if(NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* NameSystem.mkdirs: \" + src);\n    }\n    if (!DFSUtil.isValidName(src)) {\n      throw new InvalidPathException(src);\n    }\n    FSPermissionChecker pc = getPermissionChecker();\n    checkOperation(OperationCategory.WRITE);\n    byte[][] pathComponents = FSDirectory.getPathComponentsForReservedPath(src);\n    HdfsFileStatus resultingStat = null;\n    boolean status = false;\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);   \n      if (isInSafeMode()) {\n        throw new SafeModeException(\"Cannot create directory \" + src, safeMode);\n      }\n      src = FSDirectory.resolvePath(src, pathComponents, dir);\n      status = mkdirsInternal(pc, src, permissions, createParent);\n      if (status) {\n        resultingStat = dir.getFileInfo(src, false);\n      }\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n    if (status) {\n      logAuditEvent(true, \"mkdirs\", src, null, resultingStat);\n    }\n    return status;\n  }",
                "code_after_change": "  private boolean mkdirsInt(String src, PermissionStatus permissions,\n      boolean createParent) throws IOException, UnresolvedLinkException {\n    if(NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* NameSystem.mkdirs: \" + src);\n    }\n    if (!DFSUtil.isValidName(src)) {\n      throw new InvalidPathException(src);\n    }\n    FSPermissionChecker pc = getPermissionChecker();\n    checkOperation(OperationCategory.WRITE);\n    byte[][] pathComponents = FSDirectory.getPathComponentsForReservedPath(src);\n    HdfsFileStatus resultingStat = null;\n    boolean status = false;\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);   \n      checkNameNodeSafeMode(\"Cannot create directory \" + src);\n      src = FSDirectory.resolvePath(src, pathComponents, dir);\n      status = mkdirsInternal(pc, src, permissions, createParent);\n      if (status) {\n        resultingStat = dir.getFileInfo(src, false);\n      }\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n    if (status) {\n      logAuditEvent(true, \"mkdirs\", src, null, resultingStat);\n    }\n    return status;\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.createSymlinkInt": {
                "code_before_change": "  private void createSymlinkInt(String target, String link,\n      PermissionStatus dirPerms, boolean createParent, boolean logRetryCache) \n      throws IOException, UnresolvedLinkException {\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* NameSystem.createSymlink: target=\"\n          + target + \" link=\" + link);\n    }\n    HdfsFileStatus resultingStat = null;\n    FSPermissionChecker pc = getPermissionChecker();\n    checkOperation(OperationCategory.WRITE);\n    byte[][] pathComponents = FSDirectory.getPathComponentsForReservedPath(link);\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      if (isInSafeMode()) {\n        throw new SafeModeException(\"Cannot create symlink \" + link, safeMode);\n      }\n      link = FSDirectory.resolvePath(link, pathComponents, dir);\n      if (!createParent) {\n        verifyParentDir(link);\n      }\n      if (!dir.isValidToCreate(link)) {\n        throw new IOException(\"failed to create link \" + link \n            +\" either because the filename is invalid or the file exists\");\n      }\n      if (isPermissionEnabled) {\n        checkAncestorAccess(pc, link, FsAction.WRITE);\n      }\n      // validate that we have enough inodes.\n      checkFsObjectLimit();\n\n      // add symbolic link to namespace\n      dir.addSymlink(link, target, dirPerms, createParent, logRetryCache);\n      resultingStat = getAuditFileInfo(link, false);\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n    logAuditEvent(true, \"createSymlink\", link, target, resultingStat);\n  }",
                "code_after_change": "  private void createSymlinkInt(String target, String link,\n      PermissionStatus dirPerms, boolean createParent, boolean logRetryCache) \n      throws IOException, UnresolvedLinkException {\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* NameSystem.createSymlink: target=\"\n          + target + \" link=\" + link);\n    }\n    HdfsFileStatus resultingStat = null;\n    FSPermissionChecker pc = getPermissionChecker();\n    checkOperation(OperationCategory.WRITE);\n    byte[][] pathComponents = FSDirectory.getPathComponentsForReservedPath(link);\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      checkNameNodeSafeMode(\"Cannot create symlink \" + link);\n      link = FSDirectory.resolvePath(link, pathComponents, dir);\n      if (!createParent) {\n        verifyParentDir(link);\n      }\n      if (!dir.isValidToCreate(link)) {\n        throw new IOException(\"failed to create link \" + link \n            +\" either because the filename is invalid or the file exists\");\n      }\n      if (isPermissionEnabled) {\n        checkAncestorAccess(pc, link, FsAction.WRITE);\n      }\n      // validate that we have enough inodes.\n      checkFsObjectLimit();\n\n      // add symbolic link to namespace\n      dir.addSymlink(link, target, dirPerms, createParent, logRetryCache);\n      resultingStat = getAuditFileInfo(link, false);\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n    logAuditEvent(true, \"createSymlink\", link, target, resultingStat);\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.updatePipeline": {
                "code_before_change": "  void updatePipeline(String clientName, ExtendedBlock oldBlock, \n      ExtendedBlock newBlock, DatanodeID[] newNodes)\n      throws IOException {\n    CacheEntry cacheEntry = RetryCache.waitForCompletion(retryCache);\n    if (cacheEntry != null && cacheEntry.isSuccess()) {\n      return; // Return previous response\n    }\n    checkOperation(OperationCategory.WRITE);\n    LOG.info(\"updatePipeline(block=\" + oldBlock\n             + \", newGenerationStamp=\" + newBlock.getGenerationStamp()\n             + \", newLength=\" + newBlock.getNumBytes()\n             + \", newNodes=\" + Arrays.asList(newNodes)\n             + \", clientName=\" + clientName\n             + \")\");\n    writeLock();\n    boolean success = false;\n    try {\n      checkOperation(OperationCategory.WRITE);\n      if (isInSafeMode()) {\n        throw new SafeModeException(\"Pipeline not updated\", safeMode);\n      }\n      assert newBlock.getBlockId()==oldBlock.getBlockId() : newBlock + \" and \"\n        + oldBlock + \" has different block identifier\";\n      updatePipelineInternal(clientName, oldBlock, newBlock, newNodes,\n          cacheEntry != null);\n      success = true;\n    } finally {\n      writeUnlock();\n      RetryCache.setState(cacheEntry, success);\n    }\n    getEditLog().logSync();\n    LOG.info(\"updatePipeline(\" + oldBlock + \") successfully to \" + newBlock);\n  }",
                "code_after_change": "  void updatePipeline(String clientName, ExtendedBlock oldBlock, \n      ExtendedBlock newBlock, DatanodeID[] newNodes)\n      throws IOException {\n    checkOperation(OperationCategory.WRITE);\n    CacheEntry cacheEntry = RetryCache.waitForCompletion(retryCache);\n    if (cacheEntry != null && cacheEntry.isSuccess()) {\n      return; // Return previous response\n    }\n    LOG.info(\"updatePipeline(block=\" + oldBlock\n             + \", newGenerationStamp=\" + newBlock.getGenerationStamp()\n             + \", newLength=\" + newBlock.getNumBytes()\n             + \", newNodes=\" + Arrays.asList(newNodes)\n             + \", clientName=\" + clientName\n             + \")\");\n    writeLock();\n    boolean success = false;\n    try {\n      checkOperation(OperationCategory.WRITE);\n      checkNameNodeSafeMode(\"Pipeline not updated\");\n      assert newBlock.getBlockId()==oldBlock.getBlockId() : newBlock + \" and \"\n        + oldBlock + \" has different block identifier\";\n      updatePipelineInternal(clientName, oldBlock, newBlock, newNodes,\n          cacheEntry != null);\n      success = true;\n    } finally {\n      writeUnlock();\n      RetryCache.setState(cacheEntry, success);\n    }\n    getEditLog().logSync();\n    LOG.info(\"updatePipeline(\" + oldBlock + \") successfully to \" + newBlock);\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.concatInt": {
                "code_before_change": "  private void concatInt(String target, String [] srcs, \n      boolean logRetryCache) throws IOException, UnresolvedLinkException {\n    // verify args\n    if(target.isEmpty()) {\n      throw new IllegalArgumentException(\"Target file name is empty\");\n    }\n    if(srcs == null || srcs.length == 0) {\n      throw new IllegalArgumentException(\"No sources given\");\n    }\n    \n    // We require all files be in the same directory\n    String trgParent = \n      target.substring(0, target.lastIndexOf(Path.SEPARATOR_CHAR));\n    for (String s : srcs) {\n      String srcParent = s.substring(0, s.lastIndexOf(Path.SEPARATOR_CHAR));\n      if (!srcParent.equals(trgParent)) {\n        throw new IllegalArgumentException(\n           \"Sources and target are not in the same directory\");\n      }\n    }\n\n    HdfsFileStatus resultingStat = null;\n    FSPermissionChecker pc = getPermissionChecker();\n    checkOperation(OperationCategory.WRITE);\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      if (isInSafeMode()) {\n        throw new SafeModeException(\"Cannot concat \" + target, safeMode);\n      }\n      concatInternal(pc, target, srcs, logRetryCache);\n      resultingStat = getAuditFileInfo(target, false);\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n    logAuditEvent(true, \"concat\", Arrays.toString(srcs), target, resultingStat);\n  }",
                "code_after_change": "  private void concatInt(String target, String [] srcs, \n      boolean logRetryCache) throws IOException, UnresolvedLinkException {\n    // verify args\n    if(target.isEmpty()) {\n      throw new IllegalArgumentException(\"Target file name is empty\");\n    }\n    if(srcs == null || srcs.length == 0) {\n      throw new IllegalArgumentException(\"No sources given\");\n    }\n    \n    // We require all files be in the same directory\n    String trgParent = \n      target.substring(0, target.lastIndexOf(Path.SEPARATOR_CHAR));\n    for (String s : srcs) {\n      String srcParent = s.substring(0, s.lastIndexOf(Path.SEPARATOR_CHAR));\n      if (!srcParent.equals(trgParent)) {\n        throw new IllegalArgumentException(\n           \"Sources and target are not in the same directory\");\n      }\n    }\n\n    HdfsFileStatus resultingStat = null;\n    FSPermissionChecker pc = getPermissionChecker();\n    checkOperation(OperationCategory.WRITE);\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      checkNameNodeSafeMode(\"Cannot concat \" + target);\n      concatInternal(pc, target, srcs, logRetryCache);\n      resultingStat = getAuditFileInfo(target, false);\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n    logAuditEvent(true, \"concat\", Arrays.toString(srcs), target, resultingStat);\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.renewDelegationToken": {
                "code_before_change": "  long renewDelegationToken(Token<DelegationTokenIdentifier> token)\n      throws InvalidToken, IOException {\n    long expiryTime;\n    checkOperation(OperationCategory.WRITE);\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n\n      if (isInSafeMode()) {\n        throw new SafeModeException(\"Cannot renew delegation token\", safeMode);\n      }\n      if (!isAllowedDelegationTokenOp()) {\n        throw new IOException(\n            \"Delegation Token can be renewed only with kerberos or web authentication\");\n      }\n      String renewer = getRemoteUser().getShortUserName();\n      expiryTime = dtSecretManager.renewToken(token, renewer);\n      DelegationTokenIdentifier id = new DelegationTokenIdentifier();\n      ByteArrayInputStream buf = new ByteArrayInputStream(token.getIdentifier());\n      DataInputStream in = new DataInputStream(buf);\n      id.readFields(in);\n      getEditLog().logRenewDelegationToken(id, expiryTime);\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n    return expiryTime;\n  }",
                "code_after_change": "  long renewDelegationToken(Token<DelegationTokenIdentifier> token)\n      throws InvalidToken, IOException {\n    long expiryTime;\n    checkOperation(OperationCategory.WRITE);\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n\n      checkNameNodeSafeMode(\"Cannot renew delegation token\");\n      if (!isAllowedDelegationTokenOp()) {\n        throw new IOException(\n            \"Delegation Token can be renewed only with kerberos or web authentication\");\n      }\n      String renewer = getRemoteUser().getShortUserName();\n      expiryTime = dtSecretManager.renewToken(token, renewer);\n      DelegationTokenIdentifier id = new DelegationTokenIdentifier();\n      ByteArrayInputStream buf = new ByteArrayInputStream(token.getIdentifier());\n      DataInputStream in = new DataInputStream(buf);\n      id.readFields(in);\n      getEditLog().logRenewDelegationToken(id, expiryTime);\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n    return expiryTime;\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.allowSnapshot": {
                "code_before_change": "  void allowSnapshot(String path) throws SafeModeException, IOException {\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      if (isInSafeMode()) {\n        throw new SafeModeException(\"Cannot allow snapshot for \" + path,\n            safeMode);\n      }\n      checkSuperuserPrivilege();\n\n      dir.writeLock();\n      try {\n        snapshotManager.setSnapshottable(path, true);\n      } finally {\n        dir.writeUnlock();\n      }\n      getEditLog().logAllowSnapshot(path);\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n\n    if (auditLog.isInfoEnabled() && isExternalInvocation()) {\n      logAuditEvent(true, \"allowSnapshot\", path, null, null);\n    }\n  }",
                "code_after_change": "  void allowSnapshot(String path) throws SafeModeException, IOException {\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      checkNameNodeSafeMode(\"Cannot allow snapshot for \" + path);\n      checkSuperuserPrivilege();\n\n      dir.writeLock();\n      try {\n        snapshotManager.setSnapshottable(path, true);\n      } finally {\n        dir.writeUnlock();\n      }\n      getEditLog().logAllowSnapshot(path);\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n\n    if (auditLog.isInfoEnabled() && isExternalInvocation()) {\n      logAuditEvent(true, \"allowSnapshot\", path, null, null);\n    }\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkOperation": {
                "code_before_change": "  public void checkOperation(OperationCategory op) throws StandbyException {\n    if (haContext != null) {\n      // null in some unit tests\n      haContext.checkOperation(op);\n    }\n  }",
                "code_after_change": "  public void checkOperation(OperationCategory op) throws StandbyException {\n    if (haContext != null) {\n      // null in some unit tests\n      haContext.checkOperation(op);\n    }\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.isLegacyBlock": {
                "code_before_change": "  boolean isLegacyBlock(Block block) {\n    return block.getGenerationStamp() < getGenerationStampV1Limit();\n  }",
                "code_after_change": "  boolean isLegacyBlock(Block block) {\n    return block.getGenerationStamp() < getGenerationStampV1Limit();\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.recoverLease": {
                "code_before_change": "  boolean recoverLease(String src, String holder, String clientMachine)\n      throws IOException {\n    if (!DFSUtil.isValidName(src)) {\n      throw new IOException(\"Invalid file name: \" + src);\n    }\n  \n    boolean skipSync = false;\n    FSPermissionChecker pc = getPermissionChecker();\n    checkOperation(OperationCategory.WRITE);\n    byte[][] pathComponents = FSDirectory.getPathComponentsForReservedPath(src);\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      if (isInSafeMode()) {\n        throw new SafeModeException(\n            \"Cannot recover the lease of \" + src, safeMode);\n      }\n      src = FSDirectory.resolvePath(src, pathComponents, dir);\n      final INodeFile inode = INodeFile.valueOf(dir.getINode(src), src);\n      if (!inode.isUnderConstruction()) {\n        return true;\n      }\n      if (isPermissionEnabled) {\n        checkPathAccess(pc, src, FsAction.WRITE);\n      }\n  \n      recoverLeaseInternal(inode, src, holder, clientMachine, true);\n    } catch (StandbyException se) {\n      skipSync = true;\n      throw se;\n    } finally {\n      writeUnlock();\n      // There might be transactions logged while trying to recover the lease.\n      // They need to be sync'ed even when an exception was thrown.\n      if (!skipSync) {\n        getEditLog().logSync();\n      }\n    }\n    return false;\n  }",
                "code_after_change": "  boolean recoverLease(String src, String holder, String clientMachine)\n      throws IOException {\n    if (!DFSUtil.isValidName(src)) {\n      throw new IOException(\"Invalid file name: \" + src);\n    }\n  \n    boolean skipSync = false;\n    FSPermissionChecker pc = getPermissionChecker();\n    checkOperation(OperationCategory.WRITE);\n    byte[][] pathComponents = FSDirectory.getPathComponentsForReservedPath(src);\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      checkNameNodeSafeMode(\"Cannot recover the lease of \" + src);\n      src = FSDirectory.resolvePath(src, pathComponents, dir);\n      final INodeFile inode = INodeFile.valueOf(dir.getINode(src), src);\n      if (!inode.isUnderConstruction()) {\n        return true;\n      }\n      if (isPermissionEnabled) {\n        checkPathAccess(pc, src, FsAction.WRITE);\n      }\n  \n      recoverLeaseInternal(inode, src, holder, clientMachine, true);\n    } catch (StandbyException se) {\n      skipSync = true;\n      throw se;\n    } finally {\n      writeUnlock();\n      // There might be transactions logged while trying to recover the lease.\n      // They need to be sync'ed even when an exception was thrown.\n      if (!skipSync) {\n        getEditLog().logSync();\n      }\n    }\n    return false;\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getDelegationToken": {
                "code_before_change": "  Token<DelegationTokenIdentifier> getDelegationToken(Text renewer)\n      throws IOException {\n    Token<DelegationTokenIdentifier> token;\n    checkOperation(OperationCategory.WRITE);\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      if (isInSafeMode()) {\n        throw new SafeModeException(\"Cannot issue delegation token\", safeMode);\n      }\n      if (!isAllowedDelegationTokenOp()) {\n        throw new IOException(\n          \"Delegation Token can be issued only with kerberos or web authentication\");\n      }\n      if (dtSecretManager == null || !dtSecretManager.isRunning()) {\n        LOG.warn(\"trying to get DT with no secret manager running\");\n        return null;\n      }\n\n      UserGroupInformation ugi = getRemoteUser();\n      String user = ugi.getUserName();\n      Text owner = new Text(user);\n      Text realUser = null;\n      if (ugi.getRealUser() != null) {\n        realUser = new Text(ugi.getRealUser().getUserName());\n      }\n      DelegationTokenIdentifier dtId = new DelegationTokenIdentifier(owner,\n        renewer, realUser);\n      token = new Token<DelegationTokenIdentifier>(\n        dtId, dtSecretManager);\n      long expiryTime = dtSecretManager.getTokenExpiryTime(dtId);\n      getEditLog().logGetDelegationToken(dtId, expiryTime);\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n    return token;\n  }",
                "code_after_change": "  Token<DelegationTokenIdentifier> getDelegationToken(Text renewer)\n      throws IOException {\n    Token<DelegationTokenIdentifier> token;\n    checkOperation(OperationCategory.WRITE);\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      checkNameNodeSafeMode(\"Cannot issue delegation token\");\n      if (!isAllowedDelegationTokenOp()) {\n        throw new IOException(\n          \"Delegation Token can be issued only with kerberos or web authentication\");\n      }\n      if (dtSecretManager == null || !dtSecretManager.isRunning()) {\n        LOG.warn(\"trying to get DT with no secret manager running\");\n        return null;\n      }\n\n      UserGroupInformation ugi = getRemoteUser();\n      String user = ugi.getUserName();\n      Text owner = new Text(user);\n      Text realUser = null;\n      if (ugi.getRealUser() != null) {\n        realUser = new Text(ugi.getRealUser().getUserName());\n      }\n      DelegationTokenIdentifier dtId = new DelegationTokenIdentifier(owner,\n        renewer, realUser);\n      token = new Token<DelegationTokenIdentifier>(\n        dtId, dtSecretManager);\n      long expiryTime = dtSecretManager.getTokenExpiryTime(dtId);\n      getEditLog().logGetDelegationToken(dtId, expiryTime);\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n    return token;\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.deleteSnapshot": {
                "code_before_change": "  void deleteSnapshot(String snapshotRoot, String snapshotName)\n      throws SafeModeException, IOException {\n    final FSPermissionChecker pc = getPermissionChecker();\n    CacheEntry cacheEntry = RetryCache.waitForCompletion(retryCache);\n    if (cacheEntry != null && cacheEntry.isSuccess()) {\n      return; // Return previous response\n    }\n    boolean success = false;\n    checkOperation(OperationCategory.WRITE);\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      if (isInSafeMode()) {\n        throw new SafeModeException(\n            \"Cannot delete snapshot for \" + snapshotRoot, safeMode);\n      }\n      checkOwner(pc, snapshotRoot);\n\n      BlocksMapUpdateInfo collectedBlocks = new BlocksMapUpdateInfo();\n      List<INode> removedINodes = new ChunkedArrayList<INode>();\n      dir.writeLock();\n      try {\n        snapshotManager.deleteSnapshot(snapshotRoot, snapshotName,\n            collectedBlocks, removedINodes);\n        dir.removeFromInodeMap(removedINodes);\n      } finally {\n        dir.writeUnlock();\n      }\n      removedINodes.clear();\n      this.removeBlocks(collectedBlocks);\n      collectedBlocks.clear();\n      getEditLog().logDeleteSnapshot(snapshotRoot, snapshotName,\n          cacheEntry != null);\n      success = true;\n    } finally {\n      writeUnlock();\n      RetryCache.setState(cacheEntry, success);\n    }\n    getEditLog().logSync();\n    \n    if (auditLog.isInfoEnabled() && isExternalInvocation()) {\n      String rootPath = Snapshot.getSnapshotPath(snapshotRoot, snapshotName);\n      logAuditEvent(true, \"deleteSnapshot\", rootPath, null, null);\n    }\n  }",
                "code_after_change": "  void deleteSnapshot(String snapshotRoot, String snapshotName)\n      throws SafeModeException, IOException {\n    checkOperation(OperationCategory.WRITE);\n    final FSPermissionChecker pc = getPermissionChecker();\n    \n    CacheEntry cacheEntry = RetryCache.waitForCompletion(retryCache);\n    if (cacheEntry != null && cacheEntry.isSuccess()) {\n      return; // Return previous response\n    }\n    boolean success = false;\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      checkNameNodeSafeMode(\"Cannot delete snapshot for \" + snapshotRoot);\n      if (isPermissionEnabled) {\n        checkOwner(pc, snapshotRoot);\n      }\n\n      BlocksMapUpdateInfo collectedBlocks = new BlocksMapUpdateInfo();\n      List<INode> removedINodes = new ChunkedArrayList<INode>();\n      dir.writeLock();\n      try {\n        snapshotManager.deleteSnapshot(snapshotRoot, snapshotName,\n            collectedBlocks, removedINodes);\n        dir.removeFromInodeMap(removedINodes);\n      } finally {\n        dir.writeUnlock();\n      }\n      removedINodes.clear();\n      this.removeBlocks(collectedBlocks);\n      collectedBlocks.clear();\n      getEditLog().logDeleteSnapshot(snapshotRoot, snapshotName,\n          cacheEntry != null);\n      success = true;\n    } finally {\n      writeUnlock();\n      RetryCache.setState(cacheEntry, success);\n    }\n    getEditLog().logSync();\n    \n    if (auditLog.isInfoEnabled() && isExternalInvocation()) {\n      String rootPath = Snapshot.getSnapshotPath(snapshotRoot, snapshotName);\n      logAuditEvent(true, \"deleteSnapshot\", rootPath, null, null);\n    }\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.renameSnapshot": {
                "code_before_change": "  void renameSnapshot(String path, String snapshotOldName,\n      String snapshotNewName) throws SafeModeException, IOException {\n    CacheEntry cacheEntry = RetryCache.waitForCompletion(retryCache);\n    if (cacheEntry != null && cacheEntry.isSuccess()) {\n      return; // Return previous response\n    }\n    final FSPermissionChecker pc = getPermissionChecker();\n    writeLock();\n    boolean success = false;\n    try {\n      checkOperation(OperationCategory.WRITE);\n      if (isInSafeMode()) {\n        throw new SafeModeException(\"Cannot rename snapshot for \" + path,\n            safeMode);\n      }\n      if (isPermissionEnabled) {\n        checkOwner(pc, path);\n      }\n      dir.verifySnapshotName(snapshotNewName, path);\n      \n      snapshotManager.renameSnapshot(path, snapshotOldName, snapshotNewName);\n      getEditLog().logRenameSnapshot(path, snapshotOldName, snapshotNewName,\n          cacheEntry != null);\n      success = true;\n    } finally {\n      writeUnlock();\n      RetryCache.setState(cacheEntry, success);\n    }\n    getEditLog().logSync();\n    \n    if (auditLog.isInfoEnabled() && isExternalInvocation()) {\n      String oldSnapshotRoot = Snapshot.getSnapshotPath(path, snapshotOldName);\n      String newSnapshotRoot = Snapshot.getSnapshotPath(path, snapshotNewName);\n      logAuditEvent(true, \"renameSnapshot\", oldSnapshotRoot, newSnapshotRoot, null);\n    }\n  }",
                "code_after_change": "  void renameSnapshot(String path, String snapshotOldName,\n      String snapshotNewName) throws SafeModeException, IOException {\n    checkOperation(OperationCategory.WRITE);\n    final FSPermissionChecker pc = getPermissionChecker();\n    CacheEntry cacheEntry = RetryCache.waitForCompletion(retryCache);\n    if (cacheEntry != null && cacheEntry.isSuccess()) {\n      return; // Return previous response\n    }\n    writeLock();\n    boolean success = false;\n    try {\n      checkOperation(OperationCategory.WRITE);\n      checkNameNodeSafeMode(\"Cannot rename snapshot for \" + path);\n      if (isPermissionEnabled) {\n        checkOwner(pc, path);\n      }\n      dir.verifySnapshotName(snapshotNewName, path);\n      \n      snapshotManager.renameSnapshot(path, snapshotOldName, snapshotNewName);\n      getEditLog().logRenameSnapshot(path, snapshotOldName, snapshotNewName,\n          cacheEntry != null);\n      success = true;\n    } finally {\n      writeUnlock();\n      RetryCache.setState(cacheEntry, success);\n    }\n    getEditLog().logSync();\n    \n    if (auditLog.isInfoEnabled() && isExternalInvocation()) {\n      String oldSnapshotRoot = Snapshot.getSnapshotPath(path, snapshotOldName);\n      String newSnapshotRoot = Snapshot.getSnapshotPath(path, snapshotNewName);\n      logAuditEvent(true, \"renameSnapshot\", oldSnapshotRoot, newSnapshotRoot, null);\n    }\n  }"
            },
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.io.retry.RetryPolicies.FailoverOnNetworkExceptionRetry": {
                "code_before_change": [],
                "code_after_change": []
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.commitBlockSynchronization": {
                "code_before_change": "  void commitBlockSynchronization(ExtendedBlock lastblock,\n      long newgenerationstamp, long newlength,\n      boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n      String[] newtargetstorages)\n      throws IOException, UnresolvedLinkException {\n    LOG.info(\"commitBlockSynchronization(lastblock=\" + lastblock\n             + \", newgenerationstamp=\" + newgenerationstamp\n             + \", newlength=\" + newlength\n             + \", newtargets=\" + Arrays.asList(newtargets)\n             + \", closeFile=\" + closeFile\n             + \", deleteBlock=\" + deleteblock\n             + \")\");\n    checkOperation(OperationCategory.WRITE);\n    String src = \"\";\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      // If a DN tries to commit to the standby, the recovery will\n      // fail, and the next retry will succeed on the new NN.\n  \n      if (isInSafeMode()) {\n        throw new SafeModeException(\n          \"Cannot commitBlockSynchronization while in safe mode\",\n          safeMode);\n      }\n      final BlockInfo storedBlock = getStoredBlock(\n          ExtendedBlock.getLocalBlock(lastblock));\n      if (storedBlock == null) {\n        if (deleteblock) {\n          // This may be a retry attempt so ignore the failure\n          // to locate the block.\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Block (=\" + lastblock + \") not found\");\n          }\n          return;\n        } else {\n          throw new IOException(\"Block (=\" + lastblock + \") not found\");\n        }\n      }\n      INodeFile iFile = ((INode)storedBlock.getBlockCollection()).asFile();\n      if (!iFile.isUnderConstruction() || storedBlock.isComplete()) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Unexpected block (=\" + lastblock\n                    + \") since the file (=\" + iFile.getLocalName()\n                    + \") is not under construction\");\n        }\n        return;\n      }\n\n      long recoveryId =\n        ((BlockInfoUnderConstruction)storedBlock).getBlockRecoveryId();\n      if(recoveryId != newgenerationstamp) {\n        throw new IOException(\"The recovery id \" + newgenerationstamp\n                              + \" does not match current recovery id \"\n                              + recoveryId + \" for block \" + lastblock); \n      }\n\n      INodeFileUnderConstruction pendingFile = (INodeFileUnderConstruction)iFile;\n\n      if (deleteblock) {\n        Block blockToDel = ExtendedBlock.getLocalBlock(lastblock);\n        boolean remove = pendingFile.removeLastBlock(blockToDel);\n        if (remove) {\n          blockManager.removeBlockFromMap(storedBlock);\n        }\n      }\n      else {\n        // update last block\n        storedBlock.setGenerationStamp(newgenerationstamp);\n        storedBlock.setNumBytes(newlength);\n\n        // find the DatanodeDescriptor objects\n        // There should be no locations in the blockManager till now because the\n        // file is underConstruction\n        List<DatanodeDescriptor> targetList =\n            new ArrayList<DatanodeDescriptor>(newtargets.length);\n        if (newtargets.length > 0) {\n          for (DatanodeID newtarget : newtargets) {\n            // try to get targetNode\n            DatanodeDescriptor targetNode =\n                blockManager.getDatanodeManager().getDatanode(newtarget);\n            if (targetNode != null)\n              targetList.add(targetNode);\n            else if (LOG.isDebugEnabled()) {\n              LOG.debug(\"DatanodeDescriptor (=\" + newtarget + \") not found\");\n            }\n          }\n        }\n        if ((closeFile) && !targetList.isEmpty()) {\n          // the file is getting closed. Insert block locations into blockManager.\n          // Otherwise fsck will report these blocks as MISSING, especially if the\n          // blocksReceived from Datanodes take a long time to arrive.\n          for (DatanodeDescriptor targetNode : targetList) {\n            targetNode.addBlock(storedBlock);\n          }\n        }\n        // add pipeline locations into the INodeUnderConstruction\n        DatanodeDescriptor[] targetArray =\n            new DatanodeDescriptor[targetList.size()];\n        pendingFile.setLastBlock(storedBlock, targetList.toArray(targetArray));\n      }\n\n      if (closeFile) {\n        src = closeFileCommitBlocks(pendingFile, storedBlock);\n      } else {\n        // If this commit does not want to close the file, persist blocks\n        src = persistBlocks(pendingFile, false);\n      }\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n    if (closeFile) {\n      LOG.info(\"commitBlockSynchronization(newblock=\" + lastblock\n          + \", file=\" + src\n          + \", newgenerationstamp=\" + newgenerationstamp\n          + \", newlength=\" + newlength\n          + \", newtargets=\" + Arrays.asList(newtargets) + \") successful\");\n    } else {\n      LOG.info(\"commitBlockSynchronization(\" + lastblock + \") successful\");\n    }\n  }",
                "code_after_change": "  void commitBlockSynchronization(ExtendedBlock lastblock,\n      long newgenerationstamp, long newlength,\n      boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n      String[] newtargetstorages)\n      throws IOException, UnresolvedLinkException {\n    LOG.info(\"commitBlockSynchronization(lastblock=\" + lastblock\n             + \", newgenerationstamp=\" + newgenerationstamp\n             + \", newlength=\" + newlength\n             + \", newtargets=\" + Arrays.asList(newtargets)\n             + \", closeFile=\" + closeFile\n             + \", deleteBlock=\" + deleteblock\n             + \")\");\n    checkOperation(OperationCategory.WRITE);\n    String src = \"\";\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      // If a DN tries to commit to the standby, the recovery will\n      // fail, and the next retry will succeed on the new NN.\n  \n      checkNameNodeSafeMode(\n          \"Cannot commitBlockSynchronization while in safe mode\");\n      final BlockInfo storedBlock = getStoredBlock(\n          ExtendedBlock.getLocalBlock(lastblock));\n      if (storedBlock == null) {\n        if (deleteblock) {\n          // This may be a retry attempt so ignore the failure\n          // to locate the block.\n          if (LOG.isDebugEnabled()) {\n            LOG.debug(\"Block (=\" + lastblock + \") not found\");\n          }\n          return;\n        } else {\n          throw new IOException(\"Block (=\" + lastblock + \") not found\");\n        }\n      }\n      INodeFile iFile = ((INode)storedBlock.getBlockCollection()).asFile();\n      if (!iFile.isUnderConstruction() || storedBlock.isComplete()) {\n        if (LOG.isDebugEnabled()) {\n          LOG.debug(\"Unexpected block (=\" + lastblock\n                    + \") since the file (=\" + iFile.getLocalName()\n                    + \") is not under construction\");\n        }\n        return;\n      }\n\n      long recoveryId =\n        ((BlockInfoUnderConstruction)storedBlock).getBlockRecoveryId();\n      if(recoveryId != newgenerationstamp) {\n        throw new IOException(\"The recovery id \" + newgenerationstamp\n                              + \" does not match current recovery id \"\n                              + recoveryId + \" for block \" + lastblock); \n      }\n\n      INodeFileUnderConstruction pendingFile = (INodeFileUnderConstruction)iFile;\n\n      if (deleteblock) {\n        Block blockToDel = ExtendedBlock.getLocalBlock(lastblock);\n        boolean remove = pendingFile.removeLastBlock(blockToDel);\n        if (remove) {\n          blockManager.removeBlockFromMap(storedBlock);\n        }\n      }\n      else {\n        // update last block\n        storedBlock.setGenerationStamp(newgenerationstamp);\n        storedBlock.setNumBytes(newlength);\n\n        // find the DatanodeDescriptor objects\n        // There should be no locations in the blockManager till now because the\n        // file is underConstruction\n        List<DatanodeDescriptor> targetList =\n            new ArrayList<DatanodeDescriptor>(newtargets.length);\n        if (newtargets.length > 0) {\n          for (DatanodeID newtarget : newtargets) {\n            // try to get targetNode\n            DatanodeDescriptor targetNode =\n                blockManager.getDatanodeManager().getDatanode(newtarget);\n            if (targetNode != null)\n              targetList.add(targetNode);\n            else if (LOG.isDebugEnabled()) {\n              LOG.debug(\"DatanodeDescriptor (=\" + newtarget + \") not found\");\n            }\n          }\n        }\n        if ((closeFile) && !targetList.isEmpty()) {\n          // the file is getting closed. Insert block locations into blockManager.\n          // Otherwise fsck will report these blocks as MISSING, especially if the\n          // blocksReceived from Datanodes take a long time to arrive.\n          for (DatanodeDescriptor targetNode : targetList) {\n            targetNode.addBlock(storedBlock);\n          }\n        }\n        // add pipeline locations into the INodeUnderConstruction\n        DatanodeDescriptor[] targetArray =\n            new DatanodeDescriptor[targetList.size()];\n        pendingFile.setLastBlock(storedBlock, targetList.toArray(targetArray));\n      }\n\n      if (closeFile) {\n        src = closeFileCommitBlocks(pendingFile, storedBlock);\n      } else {\n        // If this commit does not want to close the file, persist blocks\n        src = persistBlocks(pendingFile, false);\n      }\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n    if (closeFile) {\n      LOG.info(\"commitBlockSynchronization(newblock=\" + lastblock\n          + \", file=\" + src\n          + \", newgenerationstamp=\" + newgenerationstamp\n          + \", newlength=\" + newlength\n          + \", newtargets=\" + Arrays.asList(newtargets) + \") successful\");\n    } else {\n      LOG.info(\"commitBlockSynchronization(\" + lastblock + \") successful\");\n    }\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.disallowSnapshot": {
                "code_before_change": "  void disallowSnapshot(String path) throws SafeModeException, IOException {\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      if (isInSafeMode()) {\n        throw new SafeModeException(\"Cannot disallow snapshot for \" + path,\n            safeMode);\n      }\n      checkSuperuserPrivilege();\n\n      dir.writeLock();\n      try {\n        snapshotManager.resetSnapshottable(path);\n      } finally {\n        dir.writeUnlock();\n      }\n      getEditLog().logDisallowSnapshot(path);\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n    \n    if (auditLog.isInfoEnabled() && isExternalInvocation()) {\n      logAuditEvent(true, \"disallowSnapshot\", path, null, null);\n    }\n  }",
                "code_after_change": "  void disallowSnapshot(String path) throws SafeModeException, IOException {\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      checkNameNodeSafeMode(\"Cannot disallow snapshot for \" + path);\n      checkSuperuserPrivilege();\n\n      dir.writeLock();\n      try {\n        snapshotManager.resetSnapshottable(path);\n      } finally {\n        dir.writeUnlock();\n      }\n      getEditLog().logDisallowSnapshot(path);\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n    \n    if (auditLog.isInfoEnabled() && isExternalInvocation()) {\n      logAuditEvent(true, \"disallowSnapshot\", path, null, null);\n    }\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.completeFile": {
                "code_before_change": "  boolean completeFile(String src, String holder,\n                       ExtendedBlock last, long fileId)\n    throws SafeModeException, UnresolvedLinkException, IOException {\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* NameSystem.completeFile: \" +\n          src + \" for \" + holder);\n    }\n    checkBlock(last);\n    boolean success = false;\n    checkOperation(OperationCategory.WRITE);\n    byte[][] pathComponents = FSDirectory.getPathComponentsForReservedPath(src);\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      if (isInSafeMode()) {\n        throw new SafeModeException(\"Cannot complete file \" + src, safeMode);\n      }\n      src = FSDirectory.resolvePath(src, pathComponents, dir);\n      success = completeFileInternal(src, holder,\n        ExtendedBlock.getLocalBlock(last), fileId);\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n    NameNode.stateChangeLog.info(\"DIR* completeFile: \" + src + \" is closed by \"\n        + holder);\n    return success;\n  }",
                "code_after_change": "  boolean completeFile(String src, String holder,\n                       ExtendedBlock last, long fileId)\n    throws SafeModeException, UnresolvedLinkException, IOException {\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* NameSystem.completeFile: \" +\n          src + \" for \" + holder);\n    }\n    checkBlock(last);\n    boolean success = false;\n    checkOperation(OperationCategory.WRITE);\n    byte[][] pathComponents = FSDirectory.getPathComponentsForReservedPath(src);\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      checkNameNodeSafeMode(\"Cannot complete file \" + src);\n      src = FSDirectory.resolvePath(src, pathComponents, dir);\n      success = completeFileInternal(src, holder,\n        ExtendedBlock.getLocalBlock(last), fileId);\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n    NameNode.stateChangeLog.info(\"DIR* completeFile: \" + src + \" is closed by \"\n        + holder);\n    return success;\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog": {
                "code_before_change": "  CheckpointSignature rollEditLog() throws IOException {\n    checkSuperuserPrivilege();\n    checkOperation(OperationCategory.JOURNAL);\n    writeLock();\n    try {\n      checkOperation(OperationCategory.JOURNAL);\n      if (isInSafeMode()) {\n        throw new SafeModeException(\"Log not rolled\", safeMode);\n      }\n      LOG.info(\"Roll Edit Log from \" + Server.getRemoteAddress());\n      return getFSImage().rollEditLog();\n    } finally {\n      writeUnlock();\n    }\n  }",
                "code_after_change": "  CheckpointSignature rollEditLog() throws IOException {\n    checkSuperuserPrivilege();\n    checkOperation(OperationCategory.JOURNAL);\n    writeLock();\n    try {\n      checkOperation(OperationCategory.JOURNAL);\n      checkNameNodeSafeMode(\"Log not rolled\");\n      LOG.info(\"Roll Edit Log from \" + Server.getRemoteAddress());\n      return getFSImage().rollEditLog();\n    } finally {\n      writeUnlock();\n    }\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsInt": {
                "code_before_change": "  private LocatedBlocks getBlockLocationsInt(String src, long offset,\n      long length, boolean doAccessTime, boolean needBlockToken,\n      boolean checkSafeMode)\n      throws FileNotFoundException, UnresolvedLinkException, IOException {\n    if (offset < 0) {\n      throw new HadoopIllegalArgumentException(\n          \"Negative offset is not supported. File: \" + src);\n    }\n    if (length < 0) {\n      throw new HadoopIllegalArgumentException(\n          \"Negative length is not supported. File: \" + src);\n    }\n    final LocatedBlocks ret = getBlockLocationsUpdateTimes(src,\n        offset, length, doAccessTime, needBlockToken);  \n    logAuditEvent(true, \"open\", src);\n    if (checkSafeMode && isInSafeMode()) {\n      for (LocatedBlock b : ret.getLocatedBlocks()) {\n        // if safemode & no block locations yet then throw safemodeException\n        if ((b.getLocations() == null) || (b.getLocations().length == 0)) {\n          throw new SafeModeException(\"Zero blocklocations for \" + src,\n              safeMode);\n        }\n      }\n    }\n    return ret;\n  }",
                "code_after_change": "  private LocatedBlocks getBlockLocationsInt(String src, long offset,\n      long length, boolean doAccessTime, boolean needBlockToken,\n      boolean checkSafeMode)\n      throws FileNotFoundException, UnresolvedLinkException, IOException {\n    if (offset < 0) {\n      throw new HadoopIllegalArgumentException(\n          \"Negative offset is not supported. File: \" + src);\n    }\n    if (length < 0) {\n      throw new HadoopIllegalArgumentException(\n          \"Negative length is not supported. File: \" + src);\n    }\n    final LocatedBlocks ret = getBlockLocationsUpdateTimes(src,\n        offset, length, doAccessTime, needBlockToken);  \n    logAuditEvent(true, \"open\", src);\n    if (checkSafeMode && isInSafeMode()) {\n      for (LocatedBlock b : ret.getLocatedBlocks()) {\n        // if safemode & no block locations yet then throw safemodeException\n        if ((b.getLocations() == null) || (b.getLocations().length == 0)) {\n          SafeModeException se = new SafeModeException(\n              \"Zero blocklocations for \" + src, safeMode);\n          if (haEnabled && haContext != null && \n              haContext.getState().getServiceState() == HAServiceState.ACTIVE) {\n            throw new RetriableException(se);\n          } else {\n            throw se;\n          }\n        }\n      }\n    }\n    return ret;\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.renameTo": {
                "code_before_change": "  boolean renameTo(String src, String dst) \n      throws IOException, UnresolvedLinkException {\n    CacheEntry cacheEntry = RetryCache.waitForCompletion(retryCache);\n    if (cacheEntry != null && cacheEntry.isSuccess()) {\n      return true; // Return previous response\n    }\n    boolean ret = false;\n    try {\n      ret = renameToInt(src, dst, cacheEntry != null);\n    } catch (AccessControlException e) {\n      logAuditEvent(false, \"rename\", src, dst, null);\n      throw e;\n    } finally {\n      RetryCache.setState(cacheEntry, ret);\n    }\n    return ret;\n  }",
                "code_after_change": "  boolean renameTo(String src, String dst) \n      throws IOException, UnresolvedLinkException {\n    CacheEntry cacheEntry = RetryCache.waitForCompletion(retryCache);\n    if (cacheEntry != null && cacheEntry.isSuccess()) {\n      return true; // Return previous response\n    }\n    boolean ret = false;\n    try {\n      ret = renameToInt(src, dst, cacheEntry != null);\n    } catch (AccessControlException e) {\n      logAuditEvent(false, \"rename\", src, dst, null);\n      throw e;\n    } finally {\n      RetryCache.setState(cacheEntry, ret);\n    }\n    return ret;\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.setBalancerBandwidth": {
                "code_before_change": "  void setBalancerBandwidth(long bandwidth) throws IOException {\n    checkOperation(OperationCategory.UNCHECKED);\n    checkSuperuserPrivilege();\n    getBlockManager().getDatanodeManager().setBalancerBandwidth(bandwidth);\n  }",
                "code_after_change": "  void setBalancerBandwidth(long bandwidth) throws IOException {\n    checkOperation(OperationCategory.UNCHECKED);\n    checkSuperuserPrivilege();\n    getBlockManager().getDatanodeManager().setBalancerBandwidth(bandwidth);\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.setPermissionInt": {
                "code_before_change": "  private void setPermissionInt(String src, FsPermission permission)\n      throws AccessControlException, FileNotFoundException, SafeModeException,\n      UnresolvedLinkException, IOException {\n    HdfsFileStatus resultingStat = null;\n    FSPermissionChecker pc = getPermissionChecker();\n    checkOperation(OperationCategory.WRITE);\n    byte[][] pathComponents = FSDirectory.getPathComponentsForReservedPath(src);\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      if (isInSafeMode()) {\n        throw new SafeModeException(\"Cannot set permission for \" + src, safeMode);\n      }\n      src = FSDirectory.resolvePath(src, pathComponents, dir);\n      checkOwner(pc, src);\n      dir.setPermission(src, permission);\n      resultingStat = getAuditFileInfo(src, false);\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n    logAuditEvent(true, \"setPermission\", src, null, resultingStat);\n  }",
                "code_after_change": "  private void setPermissionInt(String src, FsPermission permission)\n      throws AccessControlException, FileNotFoundException, SafeModeException,\n      UnresolvedLinkException, IOException {\n    HdfsFileStatus resultingStat = null;\n    FSPermissionChecker pc = getPermissionChecker();\n    checkOperation(OperationCategory.WRITE);\n    byte[][] pathComponents = FSDirectory.getPathComponentsForReservedPath(src);\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      checkNameNodeSafeMode(\"Cannot set permission for \" + src);\n      src = FSDirectory.resolvePath(src, pathComponents, dir);\n      checkOwner(pc, src);\n      dir.setPermission(src, permission);\n      resultingStat = getAuditFileInfo(src, false);\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n    logAuditEvent(true, \"setPermission\", src, null, resultingStat);\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.abandonBlock": {
                "code_before_change": "  boolean abandonBlock(ExtendedBlock b, String src, String holder)\n      throws LeaseExpiredException, FileNotFoundException,\n      UnresolvedLinkException, IOException {\n    if(NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"BLOCK* NameSystem.abandonBlock: \" + b\n          + \"of file \" + src);\n    }\n    checkOperation(OperationCategory.WRITE);\n    byte[][] pathComponents = FSDirectory.getPathComponentsForReservedPath(src);\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      if (isInSafeMode()) {\n        throw new SafeModeException(\"Cannot abandon block \" + b +\n                                    \" for fle\" + src, safeMode);\n      }\n      src = FSDirectory.resolvePath(src, pathComponents, dir);\n\n      //\n      // Remove the block from the pending creates list\n      //\n      INodeFileUnderConstruction file = checkLease(src, holder);\n      boolean removed = dir.removeBlock(src, file,\n          ExtendedBlock.getLocalBlock(b));\n      if (!removed) {\n        return true;\n      }\n      if(NameNode.stateChangeLog.isDebugEnabled()) {\n        NameNode.stateChangeLog.debug(\"BLOCK* NameSystem.abandonBlock: \"\n                                      + b + \" is removed from pendingCreates\");\n      }\n      dir.persistBlocks(src, file, false);\n    } finally {\n      writeUnlock();\n    }\n    if (persistBlocks) {\n      getEditLog().logSync();\n    }\n\n    return true;\n  }",
                "code_after_change": "  boolean abandonBlock(ExtendedBlock b, String src, String holder)\n      throws LeaseExpiredException, FileNotFoundException,\n      UnresolvedLinkException, IOException {\n    if(NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"BLOCK* NameSystem.abandonBlock: \" + b\n          + \"of file \" + src);\n    }\n    checkOperation(OperationCategory.WRITE);\n    byte[][] pathComponents = FSDirectory.getPathComponentsForReservedPath(src);\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      checkNameNodeSafeMode(\"Cannot abandon block \" + b + \" for fle\" + src);\n      src = FSDirectory.resolvePath(src, pathComponents, dir);\n\n      //\n      // Remove the block from the pending creates list\n      //\n      INodeFileUnderConstruction file = checkLease(src, holder);\n      boolean removed = dir.removeBlock(src, file,\n          ExtendedBlock.getLocalBlock(b));\n      if (!removed) {\n        return true;\n      }\n      if(NameNode.stateChangeLog.isDebugEnabled()) {\n        NameNode.stateChangeLog.debug(\"BLOCK* NameSystem.abandonBlock: \"\n                                      + b + \" is removed from pendingCreates\");\n      }\n      dir.persistBlocks(src, file, false);\n    } finally {\n      writeUnlock();\n    }\n    if (persistBlocks) {\n      getEditLog().logSync();\n    }\n\n    return true;\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.renewLease": {
                "code_before_change": "  void renewLease(String holder) throws IOException {\n    checkOperation(OperationCategory.WRITE);\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      if (isInSafeMode()) {\n        throw new SafeModeException(\"Cannot renew lease for \" + holder, safeMode);\n      }\n      leaseManager.renewLease(holder);\n    } finally {\n      writeUnlock();\n    }\n  }",
                "code_after_change": "  void renewLease(String holder) throws IOException {\n    checkOperation(OperationCategory.WRITE);\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      checkNameNodeSafeMode(\"Cannot renew lease for \" + holder);\n      leaseManager.renewLease(holder);\n    } finally {\n      writeUnlock();\n    }\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.nextBlockId": {
                "code_before_change": "  private long nextBlockId() throws SafeModeException {\n    assert hasWriteLock();\n    if (isInSafeMode()) {\n      throw new SafeModeException(\n          \"Cannot get next block ID\", safeMode);\n    }\n    final long blockId = blockIdGenerator.nextValue();\n    getEditLog().logAllocateBlockId(blockId);\n    // NB: callers sync the log\n    return blockId;\n  }",
                "code_after_change": "  private long nextBlockId() throws IOException {\n    assert hasWriteLock();\n    checkNameNodeSafeMode(\"Cannot get next block ID\");\n    final long blockId = blockIdGenerator.nextValue();\n    getEditLog().logAllocateBlockId(blockId);\n    // NB: callers sync the log\n    return blockId;\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.setOwnerInt": {
                "code_before_change": "  private void setOwnerInt(String src, String username, String group)\n      throws AccessControlException, FileNotFoundException, SafeModeException,\n      UnresolvedLinkException, IOException {\n    HdfsFileStatus resultingStat = null;\n    FSPermissionChecker pc = getPermissionChecker();\n    checkOperation(OperationCategory.WRITE);\n    byte[][] pathComponents = FSDirectory.getPathComponentsForReservedPath(src);\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      if (isInSafeMode()) {\n        throw new SafeModeException(\"Cannot set owner for \" + src, safeMode);\n      }\n      src = FSDirectory.resolvePath(src, pathComponents, dir);\n      checkOwner(pc, src);\n      if (!pc.isSuperUser()) {\n        if (username != null && !pc.getUser().equals(username)) {\n          throw new AccessControlException(\"Non-super user cannot change owner\");\n        }\n        if (group != null && !pc.containsGroup(group)) {\n          throw new AccessControlException(\"User does not belong to \" + group);\n        }\n      }\n      dir.setOwner(src, username, group);\n      resultingStat = getAuditFileInfo(src, false);\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n    logAuditEvent(true, \"setOwner\", src, null, resultingStat);\n  }",
                "code_after_change": "  private void setOwnerInt(String src, String username, String group)\n      throws AccessControlException, FileNotFoundException, SafeModeException,\n      UnresolvedLinkException, IOException {\n    HdfsFileStatus resultingStat = null;\n    FSPermissionChecker pc = getPermissionChecker();\n    checkOperation(OperationCategory.WRITE);\n    byte[][] pathComponents = FSDirectory.getPathComponentsForReservedPath(src);\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      checkNameNodeSafeMode(\"Cannot set owner for \" + src);\n      src = FSDirectory.resolvePath(src, pathComponents, dir);\n      checkOwner(pc, src);\n      if (!pc.isSuperUser()) {\n        if (username != null && !pc.getUser().equals(username)) {\n          throw new AccessControlException(\"Non-super user cannot change owner\");\n        }\n        if (group != null && !pc.containsGroup(group)) {\n          throw new AccessControlException(\"User does not belong to \" + group);\n        }\n      }\n      dir.setOwner(src, username, group);\n      resultingStat = getAuditFileInfo(src, false);\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync();\n    logAuditEvent(true, \"setOwner\", src, null, resultingStat);\n  }"
            },
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.io.retry.RetryPolicies.shouldRetry": {
                "code_before_change": "    public RetryAction shouldRetry(Exception e, int retries, int failovers,\n        boolean isIdempotentOrAtMostOnce) throws Exception {\n      return RetryAction.FAIL;\n    }",
                "code_after_change": "    public RetryAction shouldRetry(Exception e, int retries, int failovers,\n        boolean isIdempotentOrAtMostOnce) throws Exception {\n      return RetryAction.FAIL;\n    }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.saveNamespace": {
                "code_before_change": "  void saveNamespace() throws AccessControlException, IOException {\n    CacheEntry cacheEntry = RetryCache.waitForCompletion(retryCache);\n    if (cacheEntry != null && cacheEntry.isSuccess()) {\n      return; // Return previous response\n    }\n    checkSuperuserPrivilege();\n    checkOperation(OperationCategory.UNCHECKED);\n    boolean success = false;\n    readLock();\n    try {\n      checkOperation(OperationCategory.UNCHECKED);\n      if (!isInSafeMode()) {\n        throw new IOException(\"Safe mode should be turned ON \" +\n                              \"in order to create namespace image.\");\n      }\n      getFSImage().saveNamespace(this);\n      success = true;\n    } finally {\n      readUnlock();\n      RetryCache.setState(cacheEntry, success);\n    }\n    LOG.info(\"New namespace image has been created\");\n  }",
                "code_after_change": "  void saveNamespace() throws AccessControlException, IOException {\n    checkOperation(OperationCategory.UNCHECKED);\n    checkSuperuserPrivilege();\n    \n    CacheEntry cacheEntry = RetryCache.waitForCompletion(retryCache);\n    if (cacheEntry != null && cacheEntry.isSuccess()) {\n      return; // Return previous response\n    }\n    boolean success = false;\n    readLock();\n    try {\n      checkOperation(OperationCategory.UNCHECKED);\n      if (!isInSafeMode()) {\n        throw new IOException(\"Safe mode should be turned ON \"\n            + \"in order to create namespace image.\");\n      }\n      getFSImage().saveNamespace(this);\n      success = true;\n    } finally {\n      readUnlock();\n      RetryCache.setState(cacheEntry, success);\n    }\n    LOG.info(\"New namespace image has been created\");\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalDatanode": {
                "code_before_change": "  LocatedBlock getAdditionalDatanode(String src, final ExtendedBlock blk,\n      final DatanodeInfo[] existings,  final Set<Node> excludes,\n      final int numAdditionalNodes, final String clientName\n      ) throws IOException {\n    //check if the feature is enabled\n    dtpReplaceDatanodeOnFailure.checkEnabled();\n\n    final DatanodeDescriptor clientnode;\n    final long preferredblocksize;\n    final List<DatanodeDescriptor> chosen;\n    checkOperation(OperationCategory.READ);\n    byte[][] pathComponents = FSDirectory.getPathComponentsForReservedPath(src);\n    readLock();\n    try {\n      checkOperation(OperationCategory.READ);\n      //check safe mode\n      if (isInSafeMode()) {\n        throw new SafeModeException(\"Cannot add datanode; src=\" + src\n            + \", blk=\" + blk, safeMode);\n      }\n      src = FSDirectory.resolvePath(src, pathComponents, dir);\n\n      //check lease\n      final INodeFileUnderConstruction file = checkLease(src, clientName);\n      clientnode = file.getClientNode();\n      preferredblocksize = file.getPreferredBlockSize();\n\n      //find datanode descriptors\n      chosen = new ArrayList<DatanodeDescriptor>();\n      for(DatanodeInfo d : existings) {\n        final DatanodeDescriptor descriptor = blockManager.getDatanodeManager(\n            ).getDatanode(d);\n        if (descriptor != null) {\n          chosen.add(descriptor);\n        }\n      }\n    } finally {\n      readUnlock();\n    }\n\n    // choose new datanodes.\n    final DatanodeInfo[] targets = blockManager.getBlockPlacementPolicy(\n        ).chooseTarget(src, numAdditionalNodes, clientnode, chosen, true,\n        excludes, preferredblocksize);\n    final LocatedBlock lb = new LocatedBlock(blk, targets);\n    blockManager.setBlockToken(lb, AccessMode.COPY);\n    return lb;\n  }",
                "code_after_change": "  LocatedBlock getAdditionalDatanode(String src, final ExtendedBlock blk,\n      final DatanodeInfo[] existings,  final Set<Node> excludes,\n      final int numAdditionalNodes, final String clientName\n      ) throws IOException {\n    //check if the feature is enabled\n    dtpReplaceDatanodeOnFailure.checkEnabled();\n\n    final DatanodeDescriptor clientnode;\n    final long preferredblocksize;\n    final List<DatanodeDescriptor> chosen;\n    checkOperation(OperationCategory.READ);\n    byte[][] pathComponents = FSDirectory.getPathComponentsForReservedPath(src);\n    readLock();\n    try {\n      checkOperation(OperationCategory.READ);\n      //check safe mode\n      checkNameNodeSafeMode(\"Cannot add datanode; src=\" + src + \", blk=\" + blk);\n      src = FSDirectory.resolvePath(src, pathComponents, dir);\n\n      //check lease\n      final INodeFileUnderConstruction file = checkLease(src, clientName);\n      clientnode = file.getClientNode();\n      preferredblocksize = file.getPreferredBlockSize();\n\n      //find datanode descriptors\n      chosen = new ArrayList<DatanodeDescriptor>();\n      for(DatanodeInfo d : existings) {\n        final DatanodeDescriptor descriptor = blockManager.getDatanodeManager(\n            ).getDatanode(d);\n        if (descriptor != null) {\n          chosen.add(descriptor);\n        }\n      }\n    } finally {\n      readUnlock();\n    }\n\n    // choose new datanodes.\n    final DatanodeInfo[] targets = blockManager.getBlockPlacementPolicy(\n        ).chooseTarget(src, numAdditionalNodes, clientnode, chosen, true,\n        excludes, preferredblocksize);\n    final LocatedBlock lb = new LocatedBlock(blk, targets);\n    blockManager.setBlockToken(lb, AccessMode.COPY);\n    return lb;\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.createSnapshot": {
                "code_before_change": "  String createSnapshot(String snapshotRoot, String snapshotName)\n      throws SafeModeException, IOException {\n    CacheEntryWithPayload cacheEntry = RetryCache.waitForCompletion(retryCache,\n        null);\n    if (cacheEntry != null && cacheEntry.isSuccess()) {\n      return (String) cacheEntry.getPayload();\n    }\n    final FSPermissionChecker pc = getPermissionChecker();\n    writeLock();\n    String snapshotPath = null;\n    try {\n      checkOperation(OperationCategory.WRITE);\n      if (isInSafeMode()) {\n        throw new SafeModeException(\"Cannot create snapshot for \"\n            + snapshotRoot, safeMode);\n      }\n      if (isPermissionEnabled) {\n        checkOwner(pc, snapshotRoot);\n      }\n\n      if (snapshotName == null || snapshotName.isEmpty()) {\n        snapshotName = Snapshot.generateDefaultSnapshotName();\n      }\n      dir.verifySnapshotName(snapshotName, snapshotRoot);\n      dir.writeLock();\n      try {\n        snapshotPath = snapshotManager.createSnapshot(snapshotRoot, snapshotName);\n      } finally {\n        dir.writeUnlock();\n      }\n      getEditLog().logCreateSnapshot(snapshotRoot, snapshotName,\n          cacheEntry != null);\n    } finally {\n      writeUnlock();\n      RetryCache.setState(cacheEntry, snapshotPath != null, snapshotPath);\n    }\n    getEditLog().logSync();\n    \n    if (auditLog.isInfoEnabled() && isExternalInvocation()) {\n      logAuditEvent(true, \"createSnapshot\", snapshotRoot, snapshotPath, null);\n    }\n    return snapshotPath;\n  }",
                "code_after_change": "  String createSnapshot(String snapshotRoot, String snapshotName)\n      throws SafeModeException, IOException {\n    checkOperation(OperationCategory.WRITE);\n    final FSPermissionChecker pc = getPermissionChecker();\n    CacheEntryWithPayload cacheEntry = RetryCache.waitForCompletion(retryCache,\n        null);\n    if (cacheEntry != null && cacheEntry.isSuccess()) {\n      return (String) cacheEntry.getPayload();\n    }\n    writeLock();\n    String snapshotPath = null;\n    try {\n      checkOperation(OperationCategory.WRITE);\n      checkNameNodeSafeMode(\"Cannot create snapshot for \" + snapshotRoot);\n      if (isPermissionEnabled) {\n        checkOwner(pc, snapshotRoot);\n      }\n\n      if (snapshotName == null || snapshotName.isEmpty()) {\n        snapshotName = Snapshot.generateDefaultSnapshotName();\n      }\n      dir.verifySnapshotName(snapshotName, snapshotRoot);\n      dir.writeLock();\n      try {\n        snapshotPath = snapshotManager.createSnapshot(snapshotRoot, snapshotName);\n      } finally {\n        dir.writeUnlock();\n      }\n      getEditLog().logCreateSnapshot(snapshotRoot, snapshotName,\n          cacheEntry != null);\n    } finally {\n      writeUnlock();\n      RetryCache.setState(cacheEntry, snapshotPath != null, snapshotPath);\n    }\n    getEditLog().logSync();\n    \n    if (auditLog.isInfoEnabled() && isExternalInvocation()) {\n      logAuditEvent(true, \"createSnapshot\", snapshotRoot, snapshotPath, null);\n    }\n    return snapshotPath;\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.deleteInternal": {
                "code_before_change": "  private boolean deleteInternal(String src, boolean recursive,\n      boolean enforcePermission, boolean logRetryCache)\n      throws AccessControlException, SafeModeException, UnresolvedLinkException,\n             IOException {\n    BlocksMapUpdateInfo collectedBlocks = new BlocksMapUpdateInfo();\n    List<INode> removedINodes = new ChunkedArrayList<INode>();\n    FSPermissionChecker pc = getPermissionChecker();\n    checkOperation(OperationCategory.WRITE);\n    byte[][] pathComponents = FSDirectory.getPathComponentsForReservedPath(src);\n    boolean ret = false;\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      if (isInSafeMode()) {\n        throw new SafeModeException(\"Cannot delete \" + src, safeMode);\n      }\n      src = FSDirectory.resolvePath(src, pathComponents, dir);\n      if (!recursive && dir.isNonEmptyDirectory(src)) {\n        throw new IOException(src + \" is non empty\");\n      }\n      if (enforcePermission && isPermissionEnabled) {\n        checkPermission(pc, src, false, null, FsAction.WRITE, null,\n            FsAction.ALL, false);\n      }\n      // Unlink the target directory from directory tree\n      if (!dir.delete(src, collectedBlocks, removedINodes, logRetryCache)) {\n        return false;\n      }\n      ret = true;\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync(); \n    removeBlocks(collectedBlocks); // Incremental deletion of blocks\n    collectedBlocks.clear();\n    dir.writeLock();\n    try {\n      dir.removeFromInodeMap(removedINodes);\n    } finally {\n      dir.writeUnlock();\n    }\n    removedINodes.clear();\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* Namesystem.delete: \"\n        + src +\" is removed\");\n    }\n    return ret;\n  }",
                "code_after_change": "  private boolean deleteInternal(String src, boolean recursive,\n      boolean enforcePermission, boolean logRetryCache)\n      throws AccessControlException, SafeModeException, UnresolvedLinkException,\n             IOException {\n    BlocksMapUpdateInfo collectedBlocks = new BlocksMapUpdateInfo();\n    List<INode> removedINodes = new ChunkedArrayList<INode>();\n    FSPermissionChecker pc = getPermissionChecker();\n    checkOperation(OperationCategory.WRITE);\n    byte[][] pathComponents = FSDirectory.getPathComponentsForReservedPath(src);\n    boolean ret = false;\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      checkNameNodeSafeMode(\"Cannot delete \" + src);\n      src = FSDirectory.resolvePath(src, pathComponents, dir);\n      if (!recursive && dir.isNonEmptyDirectory(src)) {\n        throw new IOException(src + \" is non empty\");\n      }\n      if (enforcePermission && isPermissionEnabled) {\n        checkPermission(pc, src, false, null, FsAction.WRITE, null,\n            FsAction.ALL, false);\n      }\n      // Unlink the target directory from directory tree\n      if (!dir.delete(src, collectedBlocks, removedINodes, logRetryCache)) {\n        return false;\n      }\n      ret = true;\n    } finally {\n      writeUnlock();\n    }\n    getEditLog().logSync(); \n    removeBlocks(collectedBlocks); // Incremental deletion of blocks\n    collectedBlocks.clear();\n    dir.writeLock();\n    try {\n      dir.removeFromInodeMap(removedINodes);\n    } finally {\n      dir.writeUnlock();\n    }\n    removedINodes.clear();\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* Namesystem.delete: \"\n        + src +\" is removed\");\n    }\n    return ret;\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Preventive",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the issue of the NameNode entering SafeMode after transitioning to active state, which is related to the ground truth methods that handle SafeMode exceptions. However, it does not precisely identify the root cause in the specific methods where the fix was applied. The report suggests allowing clients to retry when HA is enabled and ANN is in SafeMode, which is a preventive measure to mitigate the issue. The problem location is partially identified as it mentions the context of SafeMode and HA, which is related to the ground truth methods but not specific to them. There is no wrong information in the bug report as it accurately describes the problem and its context."
        }
    },
    {
        "filename": "HDFS-12836.json",
        "code_diff": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager.selectInputStreams": {
                "code_before_change": "  public void selectInputStreams(Collection<EditLogInputStream> streams,\n      long fromTxnId, boolean inProgressOk) throws IOException {\n    selectInputStreams(streams, fromTxnId, inProgressOk, false);\n  }",
                "code_after_change": "  public void selectInputStreams(Collection<EditLogInputStream> streams,\n      long fromTxnId, boolean inProgressOk) throws IOException {\n    selectInputStreams(streams, fromTxnId, inProgressOk, false);\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies an issue with transaction IDs in the context of edit log tailing, which is related to the stack trace context but does not precisely identify the root cause in the ground truth method 'QuorumJournalManager.selectInputStreams'. The report does not provide any fix suggestion, hence 'Missing' for fix suggestion. The problem location is partially identified as it mentions methods in the stack trace context but not the exact ground truth method. There is no wrong information as the report accurately describes the error scenario."
        }
    },
    {
        "filename": "HDFS-8113.json",
        "code_diff": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoContiguous.BlockInfoContiguous": {
                "code_before_change": [],
                "code_after_change": []
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report precisely identifies the root cause by mentioning the NullPointerException in the BlockInfoContiguous constructor, which is the ground truth method. However, there is no fix suggestion provided in the bug report. The problem location is also precisely identified as it directly mentions the BlockInfoContiguous constructor. There is no wrong information in the bug report as all the details are relevant to the context of the bug."
        }
    },
    {
        "filename": "HDFS-10512.json",
        "code_diff": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.VolumeScanner.handle": {
                "code_before_change": "    public void handle(ExtendedBlock block, IOException e) {\n      FsVolumeSpi volume = scanner.volume;\n      if (e == null) {\n        LOG.trace(\"Successfully scanned {} on {}\", block, volume.getBasePath());\n        return;\n      }\n      // If the block does not exist anymore, then it's not an error.\n      if (!volume.getDataset().contains(block)) {\n        LOG.debug(\"Volume {}: block {} is no longer in the dataset.\",\n            volume.getBasePath(), block);\n        return;\n      }\n      // If the block exists, the exception may due to a race with write:\n      // The BlockSender got an old block path in rbw. BlockReceiver removed\n      // the rbw block from rbw to finalized but BlockSender tried to open the\n      // file before BlockReceiver updated the VolumeMap. The state of the\n      // block can be changed again now, so ignore this error here. If there\n      // is a block really deleted by mistake, DirectoryScan should catch it.\n      if (e instanceof FileNotFoundException ) {\n        LOG.info(\"Volume {}: verification failed for {} because of \" +\n                \"FileNotFoundException.  This may be due to a race with write.\",\n            volume.getBasePath(), block);\n        return;\n      }\n      LOG.warn(\"Reporting bad {} on {}\", block, volume.getBasePath());\n      try {\n        scanner.datanode.reportBadBlocks(block);\n      } catch (IOException ie) {\n        // This is bad, but not bad enough to shut down the scanner.\n        LOG.warn(\"Cannot report bad \" + block.getBlockId(), e);\n      }\n    }",
                "code_after_change": "    public void handle(ExtendedBlock block, IOException e) {\n      FsVolumeSpi volume = scanner.volume;\n      if (e == null) {\n        LOG.trace(\"Successfully scanned {} on {}\", block, volume.getBasePath());\n        return;\n      }\n      // If the block does not exist anymore, then it's not an error.\n      if (!volume.getDataset().contains(block)) {\n        LOG.debug(\"Volume {}: block {} is no longer in the dataset.\",\n            volume.getBasePath(), block);\n        return;\n      }\n      // If the block exists, the exception may due to a race with write:\n      // The BlockSender got an old block path in rbw. BlockReceiver removed\n      // the rbw block from rbw to finalized but BlockSender tried to open the\n      // file before BlockReceiver updated the VolumeMap. The state of the\n      // block can be changed again now, so ignore this error here. If there\n      // is a block really deleted by mistake, DirectoryScan should catch it.\n      if (e instanceof FileNotFoundException ) {\n        LOG.info(\"Volume {}: verification failed for {} because of \" +\n                \"FileNotFoundException.  This may be due to a race with write.\",\n            volume.getBasePath(), block);\n        return;\n      }\n      LOG.warn(\"Reporting bad {} on {}\", block, volume.getBasePath());\n      try {\n        scanner.datanode.reportBadBlocks(block, volume);\n      } catch (IOException ie) {\n        // This is bad, but not bad enough to shut down the scanner.\n        LOG.warn(\"Cannot report bad \" + block.getBlockId(), e);\n      }\n    }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.checkAndUpdate": {
                "code_before_change": "  public void checkAndUpdate(String bpid, long blockId, File diskFile,\n      File diskMetaFile, FsVolumeSpi vol) throws IOException {\n    Block corruptBlock = null;\n    ReplicaInfo memBlockInfo;\n    synchronized (this) {\n      memBlockInfo = volumeMap.get(bpid, blockId);\n      if (memBlockInfo != null && memBlockInfo.getState() != ReplicaState.FINALIZED) {\n        // Block is not finalized - ignore the difference\n        return;\n      }\n\n      final long diskGS = diskMetaFile != null && diskMetaFile.exists() ?\n          Block.getGenerationStamp(diskMetaFile.getName()) :\n            HdfsConstants.GRANDFATHER_GENERATION_STAMP;\n\n      if (diskFile == null || !diskFile.exists()) {\n        if (memBlockInfo == null) {\n          // Block file does not exist and block does not exist in memory\n          // If metadata file exists then delete it\n          if (diskMetaFile != null && diskMetaFile.exists()\n              && diskMetaFile.delete()) {\n            LOG.warn(\"Deleted a metadata file without a block \"\n                + diskMetaFile.getAbsolutePath());\n          }\n          return;\n        }\n        if (!memBlockInfo.getBlockFile().exists()) {\n          // Block is in memory and not on the disk\n          // Remove the block from volumeMap\n          volumeMap.remove(bpid, blockId);\n          if (vol.isTransientStorage()) {\n            ramDiskReplicaTracker.discardReplica(bpid, blockId, true);\n          }\n          LOG.warn(\"Removed block \" + blockId\n              + \" from memory with missing block file on the disk\");\n          // Finally remove the metadata file\n          if (diskMetaFile != null && diskMetaFile.exists()\n              && diskMetaFile.delete()) {\n            LOG.warn(\"Deleted a metadata file for the deleted block \"\n                + diskMetaFile.getAbsolutePath());\n          }\n        }\n        return;\n      }\n      /*\n       * Block file exists on the disk\n       */\n      if (memBlockInfo == null) {\n        // Block is missing in memory - add the block to volumeMap\n        ReplicaInfo diskBlockInfo = new FinalizedReplica(blockId, \n            diskFile.length(), diskGS, vol, diskFile.getParentFile());\n        volumeMap.add(bpid, diskBlockInfo);\n        if (vol.isTransientStorage()) {\n          long lockedBytesReserved =\n              cacheManager.reserve(diskBlockInfo.getNumBytes()) > 0 ?\n                  diskBlockInfo.getNumBytes() : 0;\n          ramDiskReplicaTracker.addReplica(\n              bpid, blockId, (FsVolumeImpl) vol, lockedBytesReserved);\n        }\n        LOG.warn(\"Added missing block to memory \" + diskBlockInfo);\n        return;\n      }\n      /*\n       * Block exists in volumeMap and the block file exists on the disk\n       */\n      // Compare block files\n      File memFile = memBlockInfo.getBlockFile();\n      if (memFile.exists()) {\n        if (memFile.compareTo(diskFile) != 0) {\n          if (diskMetaFile.exists()) {\n            if (memBlockInfo.getMetaFile().exists()) {\n              // We have two sets of block+meta files. Decide which one to\n              // keep.\n              ReplicaInfo diskBlockInfo = new FinalizedReplica(\n                  blockId, diskFile.length(), diskGS, vol, diskFile.getParentFile());\n              ((FsVolumeImpl) vol).getBlockPoolSlice(bpid).resolveDuplicateReplicas(\n                  memBlockInfo, diskBlockInfo, volumeMap);\n            }\n          } else {\n            if (!diskFile.delete()) {\n              LOG.warn(\"Failed to delete \" + diskFile + \". Will retry on next scan\");\n            }\n          }\n        }\n      } else {\n        // Block refers to a block file that does not exist.\n        // Update the block with the file found on the disk. Since the block\n        // file and metadata file are found as a pair on the disk, update\n        // the block based on the metadata file found on the disk\n        LOG.warn(\"Block file in volumeMap \"\n            + memFile.getAbsolutePath()\n            + \" does not exist. Updating it to the file found during scan \"\n            + diskFile.getAbsolutePath());\n        memBlockInfo.setDir(diskFile.getParentFile());\n        memFile = diskFile;\n\n        LOG.warn(\"Updating generation stamp for block \" + blockId\n            + \" from \" + memBlockInfo.getGenerationStamp() + \" to \" + diskGS);\n        memBlockInfo.setGenerationStamp(diskGS);\n      }\n\n      // Compare generation stamp\n      if (memBlockInfo.getGenerationStamp() != diskGS) {\n        File memMetaFile = FsDatasetUtil.getMetaFile(diskFile, \n            memBlockInfo.getGenerationStamp());\n        if (memMetaFile.exists()) {\n          if (memMetaFile.compareTo(diskMetaFile) != 0) {\n            LOG.warn(\"Metadata file in memory \"\n                + memMetaFile.getAbsolutePath()\n                + \" does not match file found by scan \"\n                + (diskMetaFile == null? null: diskMetaFile.getAbsolutePath()));\n          }\n        } else {\n          // Metadata file corresponding to block in memory is missing\n          // If metadata file found during the scan is on the same directory\n          // as the block file, then use the generation stamp from it\n          long gs = diskMetaFile != null && diskMetaFile.exists()\n              && diskMetaFile.getParent().equals(memFile.getParent()) ? diskGS\n              : HdfsConstants.GRANDFATHER_GENERATION_STAMP;\n\n          LOG.warn(\"Updating generation stamp for block \" + blockId\n              + \" from \" + memBlockInfo.getGenerationStamp() + \" to \" + gs);\n\n          memBlockInfo.setGenerationStamp(gs);\n        }\n      }\n\n      // Compare block size\n      if (memBlockInfo.getNumBytes() != memFile.length()) {\n        // Update the length based on the block file\n        corruptBlock = new Block(memBlockInfo);\n        LOG.warn(\"Updating size of block \" + blockId + \" from \"\n            + memBlockInfo.getNumBytes() + \" to \" + memFile.length());\n        memBlockInfo.setNumBytes(memFile.length());\n      }\n    }\n\n    // Send corrupt block report outside the lock\n    if (corruptBlock != null) {\n      LOG.warn(\"Reporting the block \" + corruptBlock\n          + \" as corrupt due to length mismatch\");\n      try {\n        datanode.reportBadBlocks(new ExtendedBlock(bpid, corruptBlock));  \n      } catch (IOException e) {\n        LOG.warn(\"Failed to repot bad block \" + corruptBlock, e);\n      }\n    }\n  }",
                "code_after_change": "  public void checkAndUpdate(String bpid, long blockId, File diskFile,\n      File diskMetaFile, FsVolumeSpi vol) throws IOException {\n    Block corruptBlock = null;\n    ReplicaInfo memBlockInfo;\n    synchronized (this) {\n      memBlockInfo = volumeMap.get(bpid, blockId);\n      if (memBlockInfo != null && memBlockInfo.getState() != ReplicaState.FINALIZED) {\n        // Block is not finalized - ignore the difference\n        return;\n      }\n\n      final long diskGS = diskMetaFile != null && diskMetaFile.exists() ?\n          Block.getGenerationStamp(diskMetaFile.getName()) :\n            HdfsConstants.GRANDFATHER_GENERATION_STAMP;\n\n      if (diskFile == null || !diskFile.exists()) {\n        if (memBlockInfo == null) {\n          // Block file does not exist and block does not exist in memory\n          // If metadata file exists then delete it\n          if (diskMetaFile != null && diskMetaFile.exists()\n              && diskMetaFile.delete()) {\n            LOG.warn(\"Deleted a metadata file without a block \"\n                + diskMetaFile.getAbsolutePath());\n          }\n          return;\n        }\n        if (!memBlockInfo.getBlockFile().exists()) {\n          // Block is in memory and not on the disk\n          // Remove the block from volumeMap\n          volumeMap.remove(bpid, blockId);\n          if (vol.isTransientStorage()) {\n            ramDiskReplicaTracker.discardReplica(bpid, blockId, true);\n          }\n          LOG.warn(\"Removed block \" + blockId\n              + \" from memory with missing block file on the disk\");\n          // Finally remove the metadata file\n          if (diskMetaFile != null && diskMetaFile.exists()\n              && diskMetaFile.delete()) {\n            LOG.warn(\"Deleted a metadata file for the deleted block \"\n                + diskMetaFile.getAbsolutePath());\n          }\n        }\n        return;\n      }\n      /*\n       * Block file exists on the disk\n       */\n      if (memBlockInfo == null) {\n        // Block is missing in memory - add the block to volumeMap\n        ReplicaInfo diskBlockInfo = new FinalizedReplica(blockId, \n            diskFile.length(), diskGS, vol, diskFile.getParentFile());\n        volumeMap.add(bpid, diskBlockInfo);\n        if (vol.isTransientStorage()) {\n          long lockedBytesReserved =\n              cacheManager.reserve(diskBlockInfo.getNumBytes()) > 0 ?\n                  diskBlockInfo.getNumBytes() : 0;\n          ramDiskReplicaTracker.addReplica(\n              bpid, blockId, (FsVolumeImpl) vol, lockedBytesReserved);\n        }\n        LOG.warn(\"Added missing block to memory \" + diskBlockInfo);\n        return;\n      }\n      /*\n       * Block exists in volumeMap and the block file exists on the disk\n       */\n      // Compare block files\n      File memFile = memBlockInfo.getBlockFile();\n      if (memFile.exists()) {\n        if (memFile.compareTo(diskFile) != 0) {\n          if (diskMetaFile.exists()) {\n            if (memBlockInfo.getMetaFile().exists()) {\n              // We have two sets of block+meta files. Decide which one to\n              // keep.\n              ReplicaInfo diskBlockInfo = new FinalizedReplica(\n                  blockId, diskFile.length(), diskGS, vol, diskFile.getParentFile());\n              ((FsVolumeImpl) vol).getBlockPoolSlice(bpid).resolveDuplicateReplicas(\n                  memBlockInfo, diskBlockInfo, volumeMap);\n            }\n          } else {\n            if (!diskFile.delete()) {\n              LOG.warn(\"Failed to delete \" + diskFile + \". Will retry on next scan\");\n            }\n          }\n        }\n      } else {\n        // Block refers to a block file that does not exist.\n        // Update the block with the file found on the disk. Since the block\n        // file and metadata file are found as a pair on the disk, update\n        // the block based on the metadata file found on the disk\n        LOG.warn(\"Block file in volumeMap \"\n            + memFile.getAbsolutePath()\n            + \" does not exist. Updating it to the file found during scan \"\n            + diskFile.getAbsolutePath());\n        memBlockInfo.setDir(diskFile.getParentFile());\n        memFile = diskFile;\n\n        LOG.warn(\"Updating generation stamp for block \" + blockId\n            + \" from \" + memBlockInfo.getGenerationStamp() + \" to \" + diskGS);\n        memBlockInfo.setGenerationStamp(diskGS);\n      }\n\n      // Compare generation stamp\n      if (memBlockInfo.getGenerationStamp() != diskGS) {\n        File memMetaFile = FsDatasetUtil.getMetaFile(diskFile, \n            memBlockInfo.getGenerationStamp());\n        if (memMetaFile.exists()) {\n          if (memMetaFile.compareTo(diskMetaFile) != 0) {\n            LOG.warn(\"Metadata file in memory \"\n                + memMetaFile.getAbsolutePath()\n                + \" does not match file found by scan \"\n                + (diskMetaFile == null? null: diskMetaFile.getAbsolutePath()));\n          }\n        } else {\n          // Metadata file corresponding to block in memory is missing\n          // If metadata file found during the scan is on the same directory\n          // as the block file, then use the generation stamp from it\n          long gs = diskMetaFile != null && diskMetaFile.exists()\n              && diskMetaFile.getParent().equals(memFile.getParent()) ? diskGS\n              : HdfsConstants.GRANDFATHER_GENERATION_STAMP;\n\n          LOG.warn(\"Updating generation stamp for block \" + blockId\n              + \" from \" + memBlockInfo.getGenerationStamp() + \" to \" + gs);\n\n          memBlockInfo.setGenerationStamp(gs);\n        }\n      }\n\n      // Compare block size\n      if (memBlockInfo.getNumBytes() != memFile.length()) {\n        // Update the length based on the block file\n        corruptBlock = new Block(memBlockInfo);\n        LOG.warn(\"Updating size of block \" + blockId + \" from \"\n            + memBlockInfo.getNumBytes() + \" to \" + memFile.length());\n        memBlockInfo.setNumBytes(memFile.length());\n      }\n    }\n\n    // Send corrupt block report outside the lock\n    if (corruptBlock != null) {\n      LOG.warn(\"Reporting the block \" + corruptBlock\n          + \" as corrupt due to length mismatch\");\n      try {\n        datanode.reportBadBlocks(new ExtendedBlock(bpid, corruptBlock),\n            memBlockInfo.getVolume());\n      } catch (IOException e) {\n        LOG.warn(\"Failed to repot bad block \" + corruptBlock, e);\n      }\n    }\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataNode.getXmitsInProgress": {
                "code_before_change": "  int getXmitsInProgress() {\n    return xmitsInProgress.get();\n  }",
                "code_after_change": "  int getXmitsInProgress() {\n    return xmitsInProgress.get();\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataNode.notifyNamenodeDeletedBlock": {
                "code_before_change": "  public void notifyNamenodeDeletedBlock(ExtendedBlock block, String storageUuid) {\n    BPOfferService bpos = blockPoolManager.get(block.getBlockPoolId());\n    if (bpos != null) {\n      bpos.notifyNamenodeDeletedBlock(block, storageUuid);\n    } else {\n      LOG.error(\"Cannot find BPOfferService for reporting block deleted for bpid=\"\n          + block.getBlockPoolId());\n    }\n  }",
                "code_after_change": "  public void notifyNamenodeDeletedBlock(ExtendedBlock block, String storageUuid) {\n    BPOfferService bpos = blockPoolManager.get(block.getBlockPoolId());\n    if (bpos != null) {\n      bpos.notifyNamenodeDeletedBlock(block, storageUuid);\n    } else {\n      LOG.error(\"Cannot find BPOfferService for reporting block deleted for bpid=\"\n          + block.getBlockPoolId());\n    }\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Buggy Method"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Buggy Method"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the method `DataNode.reportBadBlocks` as the location of the NullPointerException, which is where the error occurs but not where the fix was made. The actual fix was made in `VolumeScanner.handle`, which is a direct caller of `DataNode.reportBadBlocks`. Therefore, the root cause identification and problem location identification are classified as 'Partial' with the sub-category 'Buggy Method'. The report does not provide any fix suggestion, so it is marked as 'Missing' for fix suggestion. There is no incorrect information in the bug report, so 'Wrong Information' is marked as 'No'."
        }
    },
    {
        "filename": "HDFS-10729.json",
        "code_diff": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.addFileForEditLog": {
                "code_before_change": "  static INodeFile addFileForEditLog(\n      FSDirectory fsd, long id, INodesInPath existing, byte[] localName,\n      PermissionStatus permissions, List<AclEntry> aclEntries,\n      List<XAttr> xAttrs, short replication, long modificationTime, long atime,\n      long preferredBlockSize, boolean underConstruction, String clientName,\n      String clientMachine, byte storagePolicyId) {\n    final INodeFile newNode;\n    Preconditions.checkNotNull(existing);\n    assert fsd.hasWriteLock();\n    try {\n      // check if the file has an EC policy\n      ErasureCodingPolicy ecPolicy = FSDirErasureCodingOp.\n          getErasureCodingPolicy(fsd.getFSNamesystem(), existing);\n      if (ecPolicy != null) {\n        replication = ecPolicy.getId();\n      }\n      if (underConstruction) {\n        newNode = newINodeFile(id, permissions, modificationTime,\n            modificationTime, replication, preferredBlockSize, storagePolicyId,\n            ecPolicy != null);\n        newNode.toUnderConstruction(clientName, clientMachine);\n      } else {\n        newNode = newINodeFile(id, permissions, modificationTime, atime,\n            replication, preferredBlockSize, storagePolicyId, ecPolicy != null);\n      }\n      newNode.setLocalName(localName);\n      INodesInPath iip = fsd.addINode(existing, newNode);\n      if (iip != null) {\n        if (aclEntries != null) {\n          AclStorage.updateINodeAcl(newNode, aclEntries, CURRENT_STATE_ID);\n        }\n        if (xAttrs != null) {\n          XAttrStorage.updateINodeXAttrs(newNode, xAttrs, CURRENT_STATE_ID);\n        }\n        return newNode;\n      }\n    } catch (IOException e) {\n      if(NameNode.stateChangeLog.isDebugEnabled()) {\n        NameNode.stateChangeLog.debug(\n            \"DIR* FSDirectory.unprotectedAddFile: exception when add \"\n                + existing.getPath() + \" to the file system\", e);\n      }\n    }\n    return null;\n  }",
                "code_after_change": "  static INodeFile addFileForEditLog(\n      FSDirectory fsd, long id, INodesInPath existing, byte[] localName,\n      PermissionStatus permissions, List<AclEntry> aclEntries,\n      List<XAttr> xAttrs, short replication, long modificationTime, long atime,\n      long preferredBlockSize, boolean underConstruction, String clientName,\n      String clientMachine, byte storagePolicyId) {\n    final INodeFile newNode;\n    Preconditions.checkNotNull(existing);\n    assert fsd.hasWriteLock();\n    try {\n      // check if the file has an EC policy\n      ErasureCodingPolicy ecPolicy = FSDirErasureCodingOp.\n          getErasureCodingPolicy(fsd.getFSNamesystem(), existing);\n      if (ecPolicy != null) {\n        replication = ecPolicy.getId();\n      }\n      if (underConstruction) {\n        newNode = newINodeFile(id, permissions, modificationTime,\n            modificationTime, replication, preferredBlockSize, storagePolicyId,\n            ecPolicy != null);\n        newNode.toUnderConstruction(clientName, clientMachine);\n      } else {\n        newNode = newINodeFile(id, permissions, modificationTime, atime,\n            replication, preferredBlockSize, storagePolicyId, ecPolicy != null);\n      }\n      newNode.setLocalName(localName);\n      INodesInPath iip = fsd.addINode(existing, newNode);\n      if (iip != null) {\n        if (aclEntries != null) {\n          AclStorage.updateINodeAcl(newNode, aclEntries, CURRENT_STATE_ID);\n        }\n        if (xAttrs != null) {\n          XAttrStorage.updateINodeXAttrs(newNode, xAttrs, CURRENT_STATE_ID);\n        }\n        return newNode;\n      }\n    } catch (IOException e) {\n      NameNode.stateChangeLog.warn(\n          \"DIR* FSDirectory.unprotectedAddFile: exception when add \" + existing\n              .getPath() + \" to the file system\", e);\n      if (e instanceof FSLimitException.MaxDirectoryItemsExceededException) {\n        NameNode.stateChangeLog.warn(\"Please increase \"\n            + \"dfs.namenode.fs-limits.max-directory-items and make it \"\n            + \"consistent across all NameNodes.\");\n      }\n    }\n    return null;\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Preventive",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the issue as a NullPointerException (NPE) occurring in the FSDirectory.unprotectedAddFile method, which is part of the stack trace context shared with the ground truth method FSDirWriteFileOp.addFileForEditLog. However, it does not precisely identify the root cause in the ground truth method. The fix suggestion in the report is preventive, as it suggests increasing the directory item limit to avoid the NPE, which aligns with the developer's fix that includes a warning to increase the limit. The problem location is partially identified as it mentions FSDirectory.unprotectedAddFile, which is in the same stack trace context as the ground truth method. There is no wrong information in the report, as all details are relevant to the context of the bug."
        }
    },
    {
        "filename": "HDFS-13040.json",
        "code_diff": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getEditsFromTxid": {
                "code_before_change": "  public EventBatchList getEditsFromTxid(long txid) throws IOException {\n    checkNNStartup();\n    namesystem.checkOperation(OperationCategory.READ); // only active\n    namesystem.checkSuperuserPrivilege();\n    int maxEventsPerRPC = nn.getConf().getInt(\n        DFSConfigKeys.DFS_NAMENODE_INOTIFY_MAX_EVENTS_PER_RPC_KEY,\n        DFSConfigKeys.DFS_NAMENODE_INOTIFY_MAX_EVENTS_PER_RPC_DEFAULT);\n    FSEditLog log = namesystem.getFSImage().getEditLog();\n    long syncTxid = log.getSyncTxId();\n    // If we haven't synced anything yet, we can only read finalized\n    // segments since we can't reliably determine which txns in in-progress\n    // segments have actually been committed (e.g. written to a quorum of JNs).\n    // If we have synced txns, we can definitely read up to syncTxid since\n    // syncTxid is only updated after a transaction is committed to all\n    // journals. (In-progress segments written by old writers are already\n    // discarded for us, so if we read any in-progress segments they are\n    // guaranteed to have been written by this NameNode.)\n    boolean readInProgress = syncTxid > 0;\n\n    List<EventBatch> batches = Lists.newArrayList();\n    int totalEvents = 0;\n    long maxSeenTxid = -1;\n    long firstSeenTxid = -1;\n\n    if (syncTxid > 0 && txid > syncTxid) {\n      // we can't read past syncTxid, so there's no point in going any further\n      return new EventBatchList(batches, firstSeenTxid, maxSeenTxid, syncTxid);\n    }\n\n    Collection<EditLogInputStream> streams = null;\n    try {\n      streams = log.selectInputStreams(txid, 0, null, readInProgress);\n    } catch (IllegalStateException e) { // can happen if we have\n      // transitioned out of active and haven't yet transitioned to standby\n      // and are using QJM -- the edit log will be closed and this exception\n      // will result\n      LOG.info(\"NN is transitioning from active to standby and FSEditLog \" +\n      \"is closed -- could not read edits\");\n      return new EventBatchList(batches, firstSeenTxid, maxSeenTxid, syncTxid);\n    }\n\n    boolean breakOuter = false;\n    for (EditLogInputStream elis : streams) {\n      // our assumption in this code is the EditLogInputStreams are ordered by\n      // starting txid\n      try {\n        FSEditLogOp op = null;\n        while ((op = readOp(elis)) != null) {\n          // break out of here in the unlikely event that syncTxid is so\n          // out of date that its segment has already been deleted, so the first\n          // txid we get is greater than syncTxid\n          if (syncTxid > 0 && op.getTransactionId() > syncTxid) {\n            breakOuter = true;\n            break;\n          }\n\n          EventBatch eventBatch = InotifyFSEditLogOpTranslator.translate(op);\n          if (eventBatch != null) {\n            batches.add(eventBatch);\n            totalEvents += eventBatch.getEvents().length;\n          }\n          if (op.getTransactionId() > maxSeenTxid) {\n            maxSeenTxid = op.getTransactionId();\n          }\n          if (firstSeenTxid == -1) {\n            firstSeenTxid = op.getTransactionId();\n          }\n          if (totalEvents >= maxEventsPerRPC || (syncTxid > 0 &&\n              op.getTransactionId() == syncTxid)) {\n            // we're done\n            breakOuter = true;\n            break;\n          }\n        }\n      } finally {\n        elis.close();\n      }\n      if (breakOuter) {\n        break;\n      }\n    }\n\n    return new EventBatchList(batches, firstSeenTxid, maxSeenTxid, syncTxid);\n  }",
                "code_after_change": "  public EventBatchList getEditsFromTxid(final long txid) throws IOException {\n    checkNNStartup();\n    namesystem.checkOperation(OperationCategory.READ); // only active\n    namesystem.checkSuperuserPrivilege();\n    final int maxEventsPerRPC = nn.getConf().getInt(\n        DFSConfigKeys.DFS_NAMENODE_INOTIFY_MAX_EVENTS_PER_RPC_KEY,\n        DFSConfigKeys.DFS_NAMENODE_INOTIFY_MAX_EVENTS_PER_RPC_DEFAULT);\n    final FSEditLog log = namesystem.getFSImage().getEditLog();\n    final long syncTxid = log.getSyncTxId();\n    // If we haven't synced anything yet, we can only read finalized\n    // segments since we can't reliably determine which txns in in-progress\n    // segments have actually been committed (e.g. written to a quorum of JNs).\n    // If we have synced txns, we can definitely read up to syncTxid since\n    // syncTxid is only updated after a transaction is committed to all\n    // journals. (In-progress segments written by old writers are already\n    // discarded for us, so if we read any in-progress segments they are\n    // guaranteed to have been written by this NameNode.)\n    final boolean readInProgress = syncTxid > 0;\n\n    // doas the NN login user for the actual operations to get edits.\n    // Notably this is necessary when polling from the remote edits via https.\n    // We have validated the client is a superuser from the NN RPC, so this\n    // running as the login user here is safe.\n    EventBatchList ret = SecurityUtil.doAsLoginUser(\n        new PrivilegedExceptionAction<EventBatchList>() {\n          @Override\n          public EventBatchList run() throws IOException {\n            return getEventBatchList(syncTxid, txid, log, readInProgress,\n                maxEventsPerRPC);\n          }\n        });\n    return ret;\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getEventBatchList": {
                "code_before_change": [],
                "code_after_change": "  private EventBatchList getEventBatchList(long syncTxid, long txid,\n      FSEditLog log, boolean readInProgress, int maxEventsPerRPC)\n      throws IOException {\n    List<EventBatch> batches = Lists.newArrayList();\n    int totalEvents = 0;\n    long maxSeenTxid = -1;\n    long firstSeenTxid = -1;\n\n    if (syncTxid > 0 && txid > syncTxid) {\n      // we can't read past syncTxid, so there's no point in going any further\n      return new EventBatchList(batches, firstSeenTxid, maxSeenTxid, syncTxid);\n    }\n\n    Collection<EditLogInputStream> streams = null;\n    try {\n      streams = log.selectInputStreams(txid, 0, null, readInProgress);\n    } catch (IllegalStateException e) { // can happen if we have\n      // transitioned out of active and haven't yet transitioned to standby\n      // and are using QJM -- the edit log will be closed and this exception\n      // will result\n      LOG.info(\"NN is transitioning from active to standby and FSEditLog \" +\n          \"is closed -- could not read edits\");\n      return new EventBatchList(batches, firstSeenTxid, maxSeenTxid, syncTxid);\n    }\n\n    boolean breakOuter = false;\n    for (EditLogInputStream elis : streams) {\n      // our assumption in this code is the EditLogInputStreams are ordered by\n      // starting txid\n      try {\n        FSEditLogOp op = null;\n        while ((op = readOp(elis)) != null) {\n          // break out of here in the unlikely event that syncTxid is so\n          // out of date that its segment has already been deleted, so the first\n          // txid we get is greater than syncTxid\n          if (syncTxid > 0 && op.getTransactionId() > syncTxid) {\n            breakOuter = true;\n            break;\n          }\n\n          EventBatch eventBatch = InotifyFSEditLogOpTranslator.translate(op);\n          if (eventBatch != null) {\n            batches.add(eventBatch);\n            totalEvents += eventBatch.getEvents().length;\n          }\n          if (op.getTransactionId() > maxSeenTxid) {\n            maxSeenTxid = op.getTransactionId();\n          }\n          if (firstSeenTxid == -1) {\n            firstSeenTxid = op.getTransactionId();\n          }\n          if (totalEvents >= maxEventsPerRPC || (syncTxid > 0 &&\n              op.getTransactionId() == syncTxid)) {\n            // we're done\n            breakOuter = true;\n            break;\n          }\n        }\n      } finally {\n        elis.close();\n      }\n      if (breakOuter) {\n        break;\n      }\n    }\n\n    return new EventBatchList(batches, firstSeenTxid, maxSeenTxid, syncTxid);\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.readOp": {
                "code_before_change": "  private static FSEditLogOp readOp(EditLogInputStream elis)\n      throws IOException {\n    try {\n      return elis.readOp();\n      // we can get the below two exceptions if a segment is deleted\n      // (because we have accumulated too many edits) or (for the local journal/\n      // no-QJM case only) if a in-progress segment is finalized under us ...\n      // no need to throw an exception back to the client in this case\n    } catch (FileNotFoundException e) {\n      LOG.debug(\"Tried to read from deleted or moved edit log segment\", e);\n      return null;\n    } catch (HttpGetFailedException e) {\n      LOG.debug(\"Tried to read from deleted edit log segment\", e);\n      return null;\n    }\n  }",
                "code_after_change": "  private static FSEditLogOp readOp(EditLogInputStream elis)\n      throws IOException {\n    try {\n      return elis.readOp();\n      // we can get the below two exceptions if a segment is deleted\n      // (because we have accumulated too many edits) or (for the local journal/\n      // no-QJM case only) if a in-progress segment is finalized under us ...\n      // no need to throw an exception back to the client in this case\n    } catch (FileNotFoundException e) {\n      LOG.debug(\"Tried to read from deleted or moved edit log segment\", e);\n      return null;\n    } catch (TransferFsImage.HttpGetFailedException e) {\n      LOG.debug(\"Tried to read from deleted edit log segment\", e);\n      return null;\n    }\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Alternative Fix",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the issue related to Kerberos ticket renewal and mentions methods in the stack trace that are related to the ground truth methods, but it does not precisely identify the root cause in the ground truth methods. The fix suggestion in the report involves using a proxy user, which is an alternative approach to the developer's fix of using 'doAsLoginUser'. The problem location is partially identified as it mentions methods in the stack trace context but not the exact ground truth methods. There is no wrong information as the report accurately describes the problem and context."
        }
    },
    {
        "filename": "HDFS-3374.json",
        "code_diff": {
            "src.core.org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.run": {
                "code_before_change": [],
                "code_after_change": "    public void run() {\n      LOG.info(\"Starting expired delegation token remover thread, \"\n          + \"tokenRemoverScanInterval=\" + tokenRemoverScanInterval\n          / (60 * 1000) + \" min(s)\");\n      try {\n        while (running) {\n          long now = System.currentTimeMillis();\n          if (lastMasterKeyUpdate + keyUpdateInterval < now) {\n            synchronized (AbstractDelegationTokenSecretManager.this) {\n              if (running) {\n                try {\n                  rollMasterKey();\n                  lastMasterKeyUpdate = now;\n                } catch (IOException e) {\n                  LOG.error(\"Master key updating failed. \"\n                            + StringUtils.stringifyException(e));\n                }\n              }\n            }\n          }\n          if (lastTokenCacheCleanup + tokenRemoverScanInterval < now) {\n            synchronized (AbstractDelegationTokenSecretManager.this) {\n              if (running) {\n                removeExpiredToken();\n                lastTokenCacheCleanup = now;\n              }\n            }\n          }\n          try {\n            Thread.sleep(5000); // 5 seconds\n          } catch (InterruptedException ie) {\n            return;\n          }\n        }\n      } catch (Throwable t) {\n        LOG.error(\"ExpiredTokenRemover thread received unexpected exception. \"\n            + t);\n        Runtime.getRuntime().exit(-1);\n      }\n    }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the issue as a race condition related to the shutdown of MiniDFSCluster before the secret manager can change the key, which is related to the stack trace context but not the precise root cause in the ground truth method. The report does not provide a specific fix suggestion, hence 'Missing' for fix suggestion. The problem location is identified in the stack trace context but not precisely in the ground truth method, so it is 'Partial' with 'Shared Stack Trace Context' as the sub-category. There is no wrong information in the bug report as it accurately describes the context of the error."
        }
    },
    {
        "filename": "HDFS-2359.json",
        "code_diff": {
            "src.hdfs.org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.setMaxNumLines": {
                "code_before_change": [],
                "code_after_change": "    synchronized void setMaxNumLines(int maxNumLines) {\n      this.maxNumLines = Math.max(maxNumLines, minLineLimit);\n    }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report mentions a NullPointerException occurring in the DataBlockScanner class, which is in the same stack trace context as the ground truth method DataBlockScanner.setMaxNumLines. However, it does not precisely identify the root cause or the exact method where the fix was applied. There is no fix suggestion provided in the bug report. The problem location is partially identified as it points to a method in the same stack trace context but not the exact method. There is no wrong information in the bug report as it accurately describes the scenario and the error observed."
        }
    },
    {
        "filename": "HDFS-10986.json",
        "code_diff": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.tools.DFSAdmin.getDatanodeInfo": {
                "code_before_change": "  private int getDatanodeInfo(String[] argv, int i) throws IOException {\n    ClientDatanodeProtocol dnProxy = getDataNodeProxy(argv[i]);\n    try {\n      DatanodeLocalInfo dnInfo = dnProxy.getDatanodeInfo();\n      System.out.println(dnInfo.getDatanodeLocalReport());\n    } catch (IOException ioe) {\n      System.err.println(\"Datanode unreachable.\");\n      return -1;\n    }\n    return 0;\n  }",
                "code_after_change": "  private int getDatanodeInfo(String[] argv, int i) throws IOException {\n    ClientDatanodeProtocol dnProxy = getDataNodeProxy(argv[i]);\n    try {\n      DatanodeLocalInfo dnInfo = dnProxy.getDatanodeInfo();\n      System.out.println(dnInfo.getDatanodeLocalReport());\n    } catch (IOException ioe) {\n      throw new IOException(\"Datanode unreachable. \" + ioe, ioe);\n    }\n    return 0;\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.tools.DFSAdmin.getBalancerBandwidth": {
                "code_before_change": "  public int getBalancerBandwidth(String[] argv, int idx) throws IOException {\n    ClientDatanodeProtocol dnProxy = getDataNodeProxy(argv[idx]);\n    try {\n      long bandwidth = dnProxy.getBalancerBandwidth();\n      System.out.println(\"Balancer bandwidth is \" + bandwidth\n          + \" bytes per second.\");\n    } catch (IOException ioe) {\n      System.err.println(\"Datanode unreachable.\");\n      return -1;\n    }\n    return 0;\n  }",
                "code_after_change": "  public int getBalancerBandwidth(String[] argv, int idx) throws IOException {\n    ClientDatanodeProtocol dnProxy = getDataNodeProxy(argv[idx]);\n    try {\n      long bandwidth = dnProxy.getBalancerBandwidth();\n      System.out.println(\"Balancer bandwidth is \" + bandwidth\n          + \" bytes per second.\");\n    } catch (IOException ioe) {\n      throw new IOException(\"Datanode unreachable. \" + ioe, ioe);\n    }\n    return 0;\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.tools.DFSAdmin.evictWriters": {
                "code_before_change": "  private int evictWriters(String[] argv, int i) throws IOException {\n    final String dn = argv[i];\n    ClientDatanodeProtocol dnProxy = getDataNodeProxy(dn);\n    try {\n      dnProxy.evictWriters();\n      System.out.println(\"Requested writer eviction to datanode \" + dn);\n    } catch (IOException ioe) {\n      return -1;\n    }\n    return 0;\n  }",
                "code_after_change": "  private int evictWriters(String[] argv, int i) throws IOException {\n    final String dn = argv[i];\n    ClientDatanodeProtocol dnProxy = getDataNodeProxy(dn);\n    try {\n      dnProxy.evictWriters();\n      System.out.println(\"Requested writer eviction to datanode \" + dn);\n    } catch (IOException ioe) {\n      throw new IOException(\"Datanode unreachable. \" + ioe, ioe);\n    }\n    return 0;\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Correct",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report precisely identifies the root cause by mentioning that exceptions are being swallowed in the methods `getDatanodeInfo`, `getBalancerBandwidth`, and `evictWriters`, which are the ground truth methods. The fix suggestion is correct as it aligns with the developer's fix, which involves throwing a new IOException with a detailed message. The problem location is also precise as the report directly mentions the methods where the issue occurs. There is no wrong information in the bug report as all details are relevant and accurate."
        }
    },
    {
        "filename": "HDFS-6455.json",
        "code_diff": {
            "hadoop-hdfs-project.hadoop-hdfs-nfs.src.main.java.org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3.checkAccessPrivilege": {
                "code_before_change": "  private boolean checkAccessPrivilege(final InetAddress client,\n      final AccessPrivilege expected) {\n    AccessPrivilege access = exports.getAccessPrivilege(client);\n    if (access == AccessPrivilege.NONE) {\n      return false;\n    }\n    if (access == AccessPrivilege.READ_ONLY\n        && expected == AccessPrivilege.READ_WRITE) {\n      return false;\n    }\n    return true;\n  }",
                "code_after_change": "  private boolean checkAccessPrivilege(RpcInfo info,\n      final AccessPrivilege expected) {\n    SocketAddress remoteAddress = info.remoteAddress();\n    return checkAccessPrivilege(remoteAddress, expected);\n  }"
            },
            "hadoop-common-project.hadoop-nfs.src.main.java.org.apache.hadoop.nfs.NfsExports.getInstance": {
                "code_before_change": "  public static synchronized NfsExports getInstance(Configuration conf) {\n    if (exports == null) {\n      String matchHosts = conf.get(Nfs3Constant.EXPORTS_ALLOWED_HOSTS_KEY,\n          Nfs3Constant.EXPORTS_ALLOWED_HOSTS_KEY_DEFAULT);\n      int cacheSize = conf.getInt(Nfs3Constant.EXPORTS_CACHE_SIZE_KEY,\n          Nfs3Constant.EXPORTS_CACHE_SIZE_DEFAULT);\n      long expirationPeriodNano = conf.getLong(\n          Nfs3Constant.EXPORTS_CACHE_EXPIRYTIME_MILLIS_KEY,\n          Nfs3Constant.EXPORTS_CACHE_EXPIRYTIME_MILLIS_DEFAULT) * 1000 * 1000;\n      exports = new NfsExports(cacheSize, expirationPeriodNano, matchHosts);\n    }\n    return exports;\n  }",
                "code_after_change": "  public static synchronized NfsExports getInstance(Configuration conf) {\n    if (exports == null) {\n      String matchHosts = conf.get(\n          CommonConfigurationKeys.NFS_EXPORTS_ALLOWED_HOSTS_KEY,\n          CommonConfigurationKeys.NFS_EXPORTS_ALLOWED_HOSTS_KEY_DEFAULT);\n      int cacheSize = conf.getInt(Nfs3Constant.NFS_EXPORTS_CACHE_SIZE_KEY,\n          Nfs3Constant.NFS_EXPORTS_CACHE_SIZE_DEFAULT);\n      long expirationPeriodNano = conf.getLong(\n          Nfs3Constant.NFS_EXPORTS_CACHE_EXPIRYTIME_MILLIS_KEY,\n          Nfs3Constant.NFS_EXPORTS_CACHE_EXPIRYTIME_MILLIS_DEFAULT) * 1000 * 1000;\n      try {\n        exports = new NfsExports(cacheSize, expirationPeriodNano, matchHosts);\n      } catch (IllegalArgumentException e) {\n        LOG.error(\"Invalid NFS Exports provided: \", e);\n        return exports;\n      }\n    }\n    return exports;\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs-nfs.src.main.java.org.apache.hadoop.hdfs.nfs.mount.RpcProgramMountd.nullOp": {
                "code_before_change": "  public XDR nullOp(XDR out, int xid, InetAddress client) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"MOUNT NULLOP : \" + \" client: \" + client);\n    }\n    return RpcAcceptedReply.getAcceptInstance(xid, new VerifierNone()).write(\n        out);\n  }",
                "code_after_change": "  public XDR nullOp(XDR out, int xid, InetAddress client) {\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"MOUNT NULLOP : \" + \" client: \" + client);\n    }\n    return RpcAcceptedReply.getAcceptInstance(xid, new VerifierNone()).write(\n        out);\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs-nfs.src.main.java.org.apache.hadoop.hdfs.nfs.mount.RpcProgramMountd.handleInternal": {
                "code_before_change": "  public void handleInternal(ChannelHandlerContext ctx, RpcInfo info) {\n    RpcCall rpcCall = (RpcCall) info.header();\n    final MNTPROC mntproc = MNTPROC.fromValue(rpcCall.getProcedure());\n    int xid = rpcCall.getXid();\n    byte[] data = new byte[info.data().readableBytes()];\n    info.data().readBytes(data);\n    XDR xdr = new XDR(data);\n    XDR out = new XDR();\n    InetAddress client = ((InetSocketAddress) info.remoteAddress()).getAddress();\n\n    if (mntproc == MNTPROC.NULL) {\n      out = nullOp(out, xid, client);\n    } else if (mntproc == MNTPROC.MNT) {\n      out = mnt(xdr, out, xid, client);\n    } else if (mntproc == MNTPROC.DUMP) {\n      out = dump(out, xid, client);\n    } else if (mntproc == MNTPROC.UMNT) {      \n      out = umnt(xdr, out, xid, client);\n    } else if (mntproc == MNTPROC.UMNTALL) {\n      umntall(out, xid, client);\n    } else if (mntproc == MNTPROC.EXPORT) {\n      // Currently only support one NFS export \n      List<NfsExports> hostsMatchers = new ArrayList<NfsExports>();\n      hostsMatchers.add(hostsMatcher);\n      out = MountResponse.writeExportList(out, xid, exports, hostsMatchers);\n    } else {\n      // Invalid procedure\n      RpcAcceptedReply.getInstance(xid,\n          RpcAcceptedReply.AcceptState.PROC_UNAVAIL, new VerifierNone()).write(\n          out);\n    }  \n    ChannelBuffer buf = ChannelBuffers.wrappedBuffer(out.asReadOnlyWrap().buffer());\n    RpcResponse rsp = new RpcResponse(buf, info.remoteAddress());\n    RpcUtil.sendRpcResponse(ctx, rsp);\n  }",
                "code_after_change": "  public void handleInternal(ChannelHandlerContext ctx, RpcInfo info) {\n    RpcCall rpcCall = (RpcCall) info.header();\n    final MNTPROC mntproc = MNTPROC.fromValue(rpcCall.getProcedure());\n    int xid = rpcCall.getXid();\n    byte[] data = new byte[info.data().readableBytes()];\n    info.data().readBytes(data);\n    XDR xdr = new XDR(data);\n    XDR out = new XDR();\n    InetAddress client = ((InetSocketAddress) info.remoteAddress()).getAddress();\n\n    if (mntproc == MNTPROC.NULL) {\n      out = nullOp(out, xid, client);\n    } else if (mntproc == MNTPROC.MNT) {\n      // Only do port monitoring for MNT\n      if (!doPortMonitoring(info.remoteAddress())) {\n        out = MountResponse.writeMNTResponse(Nfs3Status.NFS3ERR_ACCES, out,\n            xid, null);\n      } else {\n        out = mnt(xdr, out, xid, client);\n      }\n    } else if (mntproc == MNTPROC.DUMP) {\n      out = dump(out, xid, client);\n    } else if (mntproc == MNTPROC.UMNT) {      \n      out = umnt(xdr, out, xid, client);\n    } else if (mntproc == MNTPROC.UMNTALL) {\n      umntall(out, xid, client);\n    } else if (mntproc == MNTPROC.EXPORT) {\n      // Currently only support one NFS export\n      List<NfsExports> hostsMatchers = new ArrayList<NfsExports>();\n      if (hostsMatcher != null) {\n        hostsMatchers.add(hostsMatcher);\n        out = MountResponse.writeExportList(out, xid, exports, hostsMatchers);\n      } else {\n        // This means there are no valid exports provided.\n        RpcAcceptedReply.getInstance(xid,\n          RpcAcceptedReply.AcceptState.PROC_UNAVAIL, new VerifierNone()).write(\n          out);\n      }\n    } else {\n      // Invalid procedure\n      RpcAcceptedReply.getInstance(xid,\n          RpcAcceptedReply.AcceptState.PROC_UNAVAIL, new VerifierNone()).write(\n          out);\n    }\n    ChannelBuffer buf = ChannelBuffers.wrappedBuffer(out.asReadOnlyWrap().buffer());\n    RpcResponse rsp = new RpcResponse(buf, info.remoteAddress());\n    RpcUtil.sendRpcResponse(ctx, rsp);\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the issue with the 'dfs.nfs.exports.allowed.hosts' property and mentions the exception in the stack trace, which includes 'NfsExports.getMatch' and 'NfsExports.getInstance'. These methods are in the shared stack trace context with the ground truth methods. However, the report does not precisely identify the root cause or problem location as it does not mention the specific ground truth methods where the fix was applied. The report lacks any fix suggestion, hence 'Missing' for fix suggestion. There is no incorrect information in the report, so 'No' for wrong information."
        }
    },
    {
        "filename": "HDFS-5185.json",
        "code_diff": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockReceiver.close": {
                "code_before_change": "  public void close() throws IOException {\n    if (packetReceiver != null) {\n      packetReceiver.close();\n    }\n    \n    IOException ioe = null;\n    if (syncOnClose && (out != null || checksumOut != null)) {\n      datanode.metrics.incrFsyncCount();      \n    }\n    long flushTotalNanos = 0;\n    boolean measuredFlushTime = false;\n    // close checksum file\n    try {\n      if (checksumOut != null) {\n        long flushStartNanos = System.nanoTime();\n        checksumOut.flush();\n        long flushEndNanos = System.nanoTime();\n        if (syncOnClose && (cout instanceof FileOutputStream)) {\n          long fsyncStartNanos = flushEndNanos;\n          ((FileOutputStream)cout).getChannel().force(true);\n          datanode.metrics.addFsyncNanos(System.nanoTime() - fsyncStartNanos);\n        }\n        flushTotalNanos += flushEndNanos - flushStartNanos;\n        measuredFlushTime = true;\n        checksumOut.close();\n        checksumOut = null;\n      }\n    } catch(IOException e) {\n      ioe = e;\n    }\n    finally {\n      IOUtils.closeStream(checksumOut);\n    }\n    // close block file\n    try {\n      if (out != null) {\n        long flushStartNanos = System.nanoTime();\n        out.flush();\n        long flushEndNanos = System.nanoTime();\n        if (syncOnClose && (out instanceof FileOutputStream)) {\n          long fsyncStartNanos = flushEndNanos;\n          ((FileOutputStream)out).getChannel().force(true);\n          datanode.metrics.addFsyncNanos(System.nanoTime() - fsyncStartNanos);\n        }\n        flushTotalNanos += flushEndNanos - flushStartNanos;\n        measuredFlushTime = true;\n        out.close();\n        out = null;\n      }\n    } catch (IOException e) {\n      ioe = e;\n    }\n    finally{\n      IOUtils.closeStream(out);\n    }\n    if (measuredFlushTime) {\n      datanode.metrics.addFlushNanos(flushTotalNanos);\n    }\n    // disk check\n    if(ioe != null) {\n      datanode.checkDiskError(ioe);\n      throw ioe;\n    }\n  }",
                "code_after_change": "  public void close() throws IOException {\n    packetReceiver.close();\n\n    IOException ioe = null;\n    if (syncOnClose && (out != null || checksumOut != null)) {\n      datanode.metrics.incrFsyncCount();      \n    }\n    long flushTotalNanos = 0;\n    boolean measuredFlushTime = false;\n    // close checksum file\n    try {\n      if (checksumOut != null) {\n        long flushStartNanos = System.nanoTime();\n        checksumOut.flush();\n        long flushEndNanos = System.nanoTime();\n        if (syncOnClose) {\n          long fsyncStartNanos = flushEndNanos;\n          streams.syncChecksumOut();\n          datanode.metrics.addFsyncNanos(System.nanoTime() - fsyncStartNanos);\n        }\n        flushTotalNanos += flushEndNanos - flushStartNanos;\n        measuredFlushTime = true;\n        checksumOut.close();\n        checksumOut = null;\n      }\n    } catch(IOException e) {\n      ioe = e;\n    }\n    finally {\n      IOUtils.closeStream(checksumOut);\n    }\n    // close block file\n    try {\n      if (out != null) {\n        long flushStartNanos = System.nanoTime();\n        out.flush();\n        long flushEndNanos = System.nanoTime();\n        if (syncOnClose) {\n          long fsyncStartNanos = flushEndNanos;\n          streams.syncDataOut();\n          datanode.metrics.addFsyncNanos(System.nanoTime() - fsyncStartNanos);\n        }\n        flushTotalNanos += flushEndNanos - flushStartNanos;\n        measuredFlushTime = true;\n        out.close();\n        out = null;\n      }\n    } catch (IOException e) {\n      ioe = e;\n    }\n    finally{\n      IOUtils.closeStream(out);\n    }\n    if (measuredFlushTime) {\n      datanode.metrics.addFlushNanos(flushTotalNanos);\n    }\n    // disk check\n    if(ioe != null) {\n      datanode.checkDiskErrorAsync();\n      throw ioe;\n    }\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataNode.shutdown": {
                "code_before_change": "  public void shutdown() {\n    if (plugins != null) {\n      for (ServicePlugin p : plugins) {\n        try {\n          p.stop();\n          LOG.info(\"Stopped plug-in \" + p);\n        } catch (Throwable t) {\n          LOG.warn(\"ServicePlugin \" + p + \" could not be stopped\", t);\n        }\n      }\n    }\n    \n    // We need to make a copy of the original blockPoolManager#offerServices to\n    // make sure blockPoolManager#shutDownAll() can still access all the \n    // BPOfferServices, since after setting DataNode#shouldRun to false the \n    // offerServices may be modified.\n    BPOfferService[] bposArray = this.blockPoolManager == null ? null\n        : this.blockPoolManager.getAllNamenodeThreads();\n    this.shouldRun = false;\n    shutdownPeriodicScanners();\n    \n    if (infoServer != null) {\n      try {\n        infoServer.stop();\n      } catch (Exception e) {\n        LOG.warn(\"Exception shutting down DataNode\", e);\n      }\n    }\n    if (ipcServer != null) {\n      ipcServer.stop();\n    }\n    if (pauseMonitor != null) {\n      pauseMonitor.stop();\n    }\n    \n    if (dataXceiverServer != null) {\n      ((DataXceiverServer) this.dataXceiverServer.getRunnable()).kill();\n      this.dataXceiverServer.interrupt();\n    }\n    if (localDataXceiverServer != null) {\n      ((DataXceiverServer) this.localDataXceiverServer.getRunnable()).kill();\n      this.localDataXceiverServer.interrupt();\n    }\n    // wait for all data receiver threads to exit\n    if (this.threadGroup != null) {\n      int sleepMs = 2;\n      while (true) {\n        this.threadGroup.interrupt();\n        LOG.info(\"Waiting for threadgroup to exit, active threads is \" +\n                 this.threadGroup.activeCount());\n        if (this.threadGroup.activeCount() == 0) {\n          break;\n        }\n        try {\n          Thread.sleep(sleepMs);\n        } catch (InterruptedException e) {}\n        sleepMs = sleepMs * 3 / 2; // exponential backoff\n        if (sleepMs > 1000) {\n          sleepMs = 1000;\n        }\n      }\n      this.threadGroup = null;\n    }\n    if (this.dataXceiverServer != null) {\n      // wait for dataXceiverServer to terminate\n      try {\n        this.dataXceiverServer.join();\n      } catch (InterruptedException ie) {\n      }\n    }\n    if (this.localDataXceiverServer != null) {\n      // wait for localDataXceiverServer to terminate\n      try {\n        this.localDataXceiverServer.join();\n      } catch (InterruptedException ie) {\n      }\n    }\n    \n    if(blockPoolManager != null) {\n      try {\n        this.blockPoolManager.shutDownAll(bposArray);\n      } catch (InterruptedException ie) {\n        LOG.warn(\"Received exception in BlockPoolManager#shutDownAll: \", ie);\n      }\n    }\n    \n    if (storage != null) {\n      try {\n        this.storage.unlockAll();\n      } catch (IOException ie) {\n        LOG.warn(\"Exception when unlocking storage: \" + ie, ie);\n      }\n    }\n    if (data != null) {\n      data.shutdown();\n    }\n    if (metrics != null) {\n      metrics.shutdown();\n    }\n  }",
                "code_after_change": "  public void shutdown() {\n    if (plugins != null) {\n      for (ServicePlugin p : plugins) {\n        try {\n          p.stop();\n          LOG.info(\"Stopped plug-in \" + p);\n        } catch (Throwable t) {\n          LOG.warn(\"ServicePlugin \" + p + \" could not be stopped\", t);\n        }\n      }\n    }\n    \n    // We need to make a copy of the original blockPoolManager#offerServices to\n    // make sure blockPoolManager#shutDownAll() can still access all the \n    // BPOfferServices, since after setting DataNode#shouldRun to false the \n    // offerServices may be modified.\n    BPOfferService[] bposArray = this.blockPoolManager == null ? null\n        : this.blockPoolManager.getAllNamenodeThreads();\n    // If shutdown is not for restart, set shouldRun to false early. \n    if (!shutdownForUpgrade) {\n      shouldRun = false;\n    }\n\n    // When shutting down for restart, DataXceiverServer is interrupted\n    // in order to avoid any further acceptance of requests, but the peers\n    // for block writes are not closed until the clients are notified.\n    if (dataXceiverServer != null) {\n      ((DataXceiverServer) this.dataXceiverServer.getRunnable()).kill();\n      this.dataXceiverServer.interrupt();\n    }\n\n    // Interrupt the checkDiskErrorThread and terminate it.\n    if(this.checkDiskErrorThread != null) {\n      this.checkDiskErrorThread.interrupt();\n    }\n    \n    // Record the time of initial notification\n    long timeNotified = Time.monotonicNow();\n\n    if (localDataXceiverServer != null) {\n      ((DataXceiverServer) this.localDataXceiverServer.getRunnable()).kill();\n      this.localDataXceiverServer.interrupt();\n    }\n\n    // Terminate directory scanner and block scanner\n    shutdownPeriodicScanners();\n\n    // Stop the web server\n    if (infoServer != null) {\n      try {\n        infoServer.stop();\n      } catch (Exception e) {\n        LOG.warn(\"Exception shutting down DataNode\", e);\n      }\n    }\n    if (pauseMonitor != null) {\n      pauseMonitor.stop();\n    }\n\n    // shouldRun is set to false here to prevent certain threads from exiting\n    // before the restart prep is done.\n    this.shouldRun = false;\n    \n    // wait for all data receiver threads to exit\n    if (this.threadGroup != null) {\n      int sleepMs = 2;\n      while (true) {\n        // When shutting down for restart, wait 2.5 seconds before forcing\n        // termination of receiver threads.\n        if (!this.shutdownForUpgrade ||\n            (this.shutdownForUpgrade && (Time.monotonicNow() - timeNotified\n                > 2500))) {\n          this.threadGroup.interrupt();\n        }\n        LOG.info(\"Waiting for threadgroup to exit, active threads is \" +\n                 this.threadGroup.activeCount());\n        if (this.threadGroup.activeCount() == 0) {\n          break;\n        }\n        try {\n          Thread.sleep(sleepMs);\n        } catch (InterruptedException e) {}\n        sleepMs = sleepMs * 3 / 2; // exponential backoff\n        if (sleepMs > 1000) {\n          sleepMs = 1000;\n        }\n      }\n      this.threadGroup = null;\n    }\n    if (this.dataXceiverServer != null) {\n      // wait for dataXceiverServer to terminate\n      try {\n        this.dataXceiverServer.join();\n      } catch (InterruptedException ie) {\n      }\n    }\n    if (this.localDataXceiverServer != null) {\n      // wait for localDataXceiverServer to terminate\n      try {\n        this.localDataXceiverServer.join();\n      } catch (InterruptedException ie) {\n      }\n    }\n   \n   // IPC server needs to be shutdown late in the process, otherwise\n   // shutdown command response won't get sent.\n   if (ipcServer != null) {\n      ipcServer.stop();\n    }\n\n    if(blockPoolManager != null) {\n      try {\n        this.blockPoolManager.shutDownAll(bposArray);\n      } catch (InterruptedException ie) {\n        LOG.warn(\"Received exception in BlockPoolManager#shutDownAll: \", ie);\n      }\n    }\n    \n    if (storage != null) {\n      try {\n        this.storage.unlockAll();\n      } catch (IOException ie) {\n        LOG.warn(\"Exception when unlocking storage: \" + ie, ie);\n      }\n    }\n    if (data != null) {\n      data.shutdown();\n    }\n    if (metrics != null) {\n      metrics.shutdown();\n    }\n    if (dataNodeInfoBeanName != null) {\n      MBeans.unregister(dataNodeInfoBeanName);\n      dataNodeInfoBeanName = null;\n    }\n    if (shortCircuitRegistry != null) shortCircuitRegistry.shutdown();\n    LOG.info(\"Shutdown complete.\");\n    synchronized(this) {\n      // it is already false, but setting it again to avoid a findbug warning.\n      this.shouldRun = false;\n      // Notify the main thread.\n      notifyAll();\n    }\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl.validateBlockFile": {
                "code_before_change": "  File validateBlockFile(String bpid, Block b) {\n    //Should we check for metadata file too?\n    final File f;\n    synchronized(this) {\n      f = getFile(bpid, b.getBlockId());\n    }\n    \n    if(f != null ) {\n      if(f.exists())\n        return f;\n   \n      // if file is not null, but doesn't exist - possibly disk failed\n      datanode.checkDiskError();\n    }\n    \n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"b=\" + b + \", f=\" + f);\n    }\n    return null;\n  }",
                "code_after_change": "  File validateBlockFile(String bpid, Block b) {\n    //Should we check for metadata file too?\n    final File f;\n    synchronized(this) {\n      f = getFile(bpid, b.getBlockId());\n    }\n    \n    if(f != null ) {\n      if(f.exists())\n        return f;\n   \n      // if file is not null, but doesn't exist - possibly disk failed\n      datanode.checkDiskErrorAsync();\n    }\n    \n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"b=\" + b + \", f=\" + f);\n    }\n    return null;\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockReceiver.run": {
                "code_before_change": "    public void run() {\n      boolean lastPacketInBlock = false;\n      final long startTime = ClientTraceLog.isInfoEnabled() ? System.nanoTime() : 0;\n      while (isRunning() && !lastPacketInBlock) {\n        long totalAckTimeNanos = 0;\n        boolean isInterrupted = false;\n        try {\n          Packet pkt = null;\n          long expected = -2;\n          PipelineAck ack = new PipelineAck();\n          long seqno = PipelineAck.UNKOWN_SEQNO;\n          long ackRecvNanoTime = 0;\n          try {\n            if (type != PacketResponderType.LAST_IN_PIPELINE && !mirrorError) {\n              // read an ack from downstream datanode\n              ack.readFields(downstreamIn);\n              ackRecvNanoTime = System.nanoTime();\n              if (LOG.isDebugEnabled()) {\n                LOG.debug(myString + \" got \" + ack);\n              }\n              seqno = ack.getSeqno();\n            }\n            if (seqno != PipelineAck.UNKOWN_SEQNO\n                || type == PacketResponderType.LAST_IN_PIPELINE) {\n              pkt = waitForAckHead(seqno);\n              if (!isRunning()) {\n                break;\n              }\n              expected = pkt.seqno;\n              if (type == PacketResponderType.HAS_DOWNSTREAM_IN_PIPELINE\n                  && seqno != expected) {\n                throw new IOException(myString + \"seqno: expected=\" + expected\n                    + \", received=\" + seqno);\n              }\n              if (type == PacketResponderType.HAS_DOWNSTREAM_IN_PIPELINE) {\n                // The total ack time includes the ack times of downstream\n                // nodes.\n                // The value is 0 if this responder doesn't have a downstream\n                // DN in the pipeline.\n                totalAckTimeNanos = ackRecvNanoTime - pkt.ackEnqueueNanoTime;\n                // Report the elapsed time from ack send to ack receive minus\n                // the downstream ack time.\n                long ackTimeNanos = totalAckTimeNanos\n                    - ack.getDownstreamAckTimeNanos();\n                if (ackTimeNanos < 0) {\n                  if (LOG.isDebugEnabled()) {\n                    LOG.debug(\"Calculated invalid ack time: \" + ackTimeNanos\n                        + \"ns.\");\n                  }\n                } else {\n                  datanode.metrics.addPacketAckRoundTripTimeNanos(ackTimeNanos);\n                }\n              }\n              lastPacketInBlock = pkt.lastPacketInBlock;\n            }\n          } catch (InterruptedException ine) {\n            isInterrupted = true;\n          } catch (IOException ioe) {\n            if (Thread.interrupted()) {\n              isInterrupted = true;\n            } else {\n              // continue to run even if can not read from mirror\n              // notify client of the error\n              // and wait for the client to shut down the pipeline\n              mirrorError = true;\n              LOG.info(myString, ioe);\n            }\n          }\n\n          if (Thread.interrupted() || isInterrupted) {\n            /*\n             * The receiver thread cancelled this thread. We could also check\n             * any other status updates from the receiver thread (e.g. if it is\n             * ok to write to replyOut). It is prudent to not send any more\n             * status back to the client because this datanode has a problem.\n             * The upstream datanode will detect that this datanode is bad, and\n             * rightly so.\n             */\n            LOG.info(myString + \": Thread is interrupted.\");\n            running = false;\n            continue;\n          }\n\n          if (lastPacketInBlock) {\n            // Finalize the block and close the block file\n            finalizeBlock(startTime);\n          }\n\n          sendAckUpstream(ack, expected, totalAckTimeNanos,\n              (pkt != null ? pkt.offsetInBlock : 0), \n              (pkt != null ? pkt.ackStatus : Status.SUCCESS));\n          if (pkt != null) {\n            // remove the packet from the ack queue\n            removeAckHead();\n          }\n        } catch (IOException e) {\n          LOG.warn(\"IOException in BlockReceiver.run(): \", e);\n          if (running) {\n            try {\n              datanode.checkDiskError(e); // may throw an exception here\n            } catch (IOException ioe) {\n              LOG.warn(\"DataNode.checkDiskError failed in run() with: \", ioe);\n            }\n            LOG.info(myString, e);\n            running = false;\n            if (!Thread.interrupted()) { // failure not caused by interruption\n              receiverThread.interrupt();\n            }\n          }\n        } catch (Throwable e) {\n          if (running) {\n            LOG.info(myString, e);\n            running = false;\n            receiverThread.interrupt();\n          }\n        }\n      }\n      LOG.info(myString + \" terminating\");\n    }",
                "code_after_change": "    public void run() {\n      boolean lastPacketInBlock = false;\n      final long startTime = ClientTraceLog.isInfoEnabled() ? System.nanoTime() : 0;\n      while (isRunning() && !lastPacketInBlock) {\n        long totalAckTimeNanos = 0;\n        boolean isInterrupted = false;\n        try {\n          Packet pkt = null;\n          long expected = -2;\n          PipelineAck ack = new PipelineAck();\n          long seqno = PipelineAck.UNKOWN_SEQNO;\n          long ackRecvNanoTime = 0;\n          try {\n            if (type != PacketResponderType.LAST_IN_PIPELINE && !mirrorError) {\n              // read an ack from downstream datanode\n              ack.readFields(downstreamIn);\n              ackRecvNanoTime = System.nanoTime();\n              if (LOG.isDebugEnabled()) {\n                LOG.debug(myString + \" got \" + ack);\n              }\n              // Process an OOB ACK.\n              Status oobStatus = ack.getOOBStatus();\n              if (oobStatus != null) {\n                LOG.info(\"Relaying an out of band ack of type \" + oobStatus);\n                sendAckUpstream(ack, PipelineAck.UNKOWN_SEQNO, 0L, 0L,\n                    Status.SUCCESS);\n                continue;\n              }\n              seqno = ack.getSeqno();\n            }\n            if (seqno != PipelineAck.UNKOWN_SEQNO\n                || type == PacketResponderType.LAST_IN_PIPELINE) {\n              pkt = waitForAckHead(seqno);\n              if (!isRunning()) {\n                break;\n              }\n              expected = pkt.seqno;\n              if (type == PacketResponderType.HAS_DOWNSTREAM_IN_PIPELINE\n                  && seqno != expected) {\n                throw new IOException(myString + \"seqno: expected=\" + expected\n                    + \", received=\" + seqno);\n              }\n              if (type == PacketResponderType.HAS_DOWNSTREAM_IN_PIPELINE) {\n                // The total ack time includes the ack times of downstream\n                // nodes.\n                // The value is 0 if this responder doesn't have a downstream\n                // DN in the pipeline.\n                totalAckTimeNanos = ackRecvNanoTime - pkt.ackEnqueueNanoTime;\n                // Report the elapsed time from ack send to ack receive minus\n                // the downstream ack time.\n                long ackTimeNanos = totalAckTimeNanos\n                    - ack.getDownstreamAckTimeNanos();\n                if (ackTimeNanos < 0) {\n                  if (LOG.isDebugEnabled()) {\n                    LOG.debug(\"Calculated invalid ack time: \" + ackTimeNanos\n                        + \"ns.\");\n                  }\n                } else {\n                  datanode.metrics.addPacketAckRoundTripTimeNanos(ackTimeNanos);\n                }\n              }\n              lastPacketInBlock = pkt.lastPacketInBlock;\n            }\n          } catch (InterruptedException ine) {\n            isInterrupted = true;\n          } catch (IOException ioe) {\n            if (Thread.interrupted()) {\n              isInterrupted = true;\n            } else {\n              // continue to run even if can not read from mirror\n              // notify client of the error\n              // and wait for the client to shut down the pipeline\n              mirrorError = true;\n              LOG.info(myString, ioe);\n            }\n          }\n\n          if (Thread.interrupted() || isInterrupted) {\n            /*\n             * The receiver thread cancelled this thread. We could also check\n             * any other status updates from the receiver thread (e.g. if it is\n             * ok to write to replyOut). It is prudent to not send any more\n             * status back to the client because this datanode has a problem.\n             * The upstream datanode will detect that this datanode is bad, and\n             * rightly so.\n             *\n             * The receiver thread can also interrupt this thread for sending\n             * an out-of-band response upstream.\n             */\n            LOG.info(myString + \": Thread is interrupted.\");\n            running = false;\n            continue;\n          }\n\n          if (lastPacketInBlock) {\n            // Finalize the block and close the block file\n            finalizeBlock(startTime);\n          }\n\n          sendAckUpstream(ack, expected, totalAckTimeNanos,\n              (pkt != null ? pkt.offsetInBlock : 0), \n              (pkt != null ? pkt.ackStatus : Status.SUCCESS));\n          if (pkt != null) {\n            // remove the packet from the ack queue\n            removeAckHead();\n          }\n        } catch (IOException e) {\n          LOG.warn(\"IOException in BlockReceiver.run(): \", e);\n          if (running) {\n            datanode.checkDiskErrorAsync();\n            LOG.info(myString, e);\n            running = false;\n            if (!Thread.interrupted()) { // failure not caused by interruption\n              receiverThread.interrupt();\n            }\n          }\n        } catch (Throwable e) {\n          if (running) {\n            LOG.info(myString, e);\n            running = false;\n            receiverThread.interrupt();\n          }\n        }\n      }\n      LOG.info(myString + \" terminating\");\n    }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataNode.run": {
                "code_before_change": "            public InterDatanodeProtocol run() throws IOException {\n              return new InterDatanodeProtocolTranslatorPB(addr, loginUgi,\n                  conf, NetUtils.getDefaultSocketFactory(conf), socketTimeout);\n            }",
                "code_after_change": "            public InterDatanodeProtocol run() throws IOException {\n              return new InterDatanodeProtocolTranslatorPB(addr, loginUgi,\n                  conf, NetUtils.getDefaultSocketFactory(conf), socketTimeout);\n            }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool": {
                "code_before_change": "  void initBlockPool(BPOfferService bpos) throws IOException {\n    NamespaceInfo nsInfo = bpos.getNamespaceInfo();\n    if (nsInfo == null) {\n      throw new IOException(\"NamespaceInfo not found: Block pool \" + bpos\n          + \" should have retrieved namespace info before initBlockPool.\");\n    }\n    \n    // Register the new block pool with the BP manager.\n    blockPoolManager.addBlockPool(bpos);\n\n    setClusterId(nsInfo.clusterID, nsInfo.getBlockPoolID());\n    \n    // In the case that this is the first block pool to connect, initialize\n    // the dataset, block scanners, etc.\n    initStorage(nsInfo);\n    initPeriodicScanners(conf);\n    \n    data.addBlockPool(nsInfo.getBlockPoolID(), conf);\n  }",
                "code_after_change": "  void initBlockPool(BPOfferService bpos) throws IOException {\n    NamespaceInfo nsInfo = bpos.getNamespaceInfo();\n    if (nsInfo == null) {\n      throw new IOException(\"NamespaceInfo not found: Block pool \" + bpos\n          + \" should have retrieved namespace info before initBlockPool.\");\n    }\n    \n    setClusterId(nsInfo.clusterID, nsInfo.getBlockPoolID());\n\n    // Register the new block pool with the BP manager.\n    blockPoolManager.addBlockPool(bpos);\n    \n    // In the case that this is the first block pool to connect, initialize\n    // the dataset, block scanners, etc.\n    initStorage(nsInfo);\n\n    // Exclude failed disks before initializing the block pools to avoid startup\n    // failures.\n    checkDiskError();\n\n    initPeriodicScanners(conf);\n    \n    data.addBlockPool(nsInfo.getBlockPoolID(), conf);\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataNode.getStorage": {
                "code_before_change": [],
                "code_after_change": "  DataStorage getStorage() {\n    return storage;\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.BlockReceiver.receivePacket": {
                "code_before_change": "  private int receivePacket() throws IOException {\n    // read the next packet\n    packetReceiver.receiveNextPacket(in);\n\n    PacketHeader header = packetReceiver.getHeader();\n    if (LOG.isDebugEnabled()){\n      LOG.debug(\"Receiving one packet for block \" + block +\n                \": \" + header);\n    }\n\n    // Sanity check the header\n    if (header.getOffsetInBlock() > replicaInfo.getNumBytes()) {\n      throw new IOException(\"Received an out-of-sequence packet for \" + block + \n          \"from \" + inAddr + \" at offset \" + header.getOffsetInBlock() +\n          \". Expecting packet starting at \" + replicaInfo.getNumBytes());\n    }\n    if (header.getDataLen() < 0) {\n      throw new IOException(\"Got wrong length during writeBlock(\" + block + \n                            \") from \" + inAddr + \" at offset \" + \n                            header.getOffsetInBlock() + \": \" +\n                            header.getDataLen()); \n    }\n\n    long offsetInBlock = header.getOffsetInBlock();\n    long seqno = header.getSeqno();\n    boolean lastPacketInBlock = header.isLastPacketInBlock();\n    int len = header.getDataLen();\n    boolean syncBlock = header.getSyncBlock();\n\n    // avoid double sync'ing on close\n    if (syncBlock && lastPacketInBlock) {\n      this.syncOnClose = false;\n    }\n\n    // update received bytes\n    long firstByteInBlock = offsetInBlock;\n    offsetInBlock += len;\n    if (replicaInfo.getNumBytes() < offsetInBlock) {\n      replicaInfo.setNumBytes(offsetInBlock);\n    }\n    \n    // put in queue for pending acks, unless sync was requested\n    if (responder != null && !syncBlock && !shouldVerifyChecksum()) {\n      ((PacketResponder) responder.getRunnable()).enqueue(seqno,\n          lastPacketInBlock, offsetInBlock, Status.SUCCESS);\n    }\n\n    //First write the packet to the mirror:\n    if (mirrorOut != null && !mirrorError) {\n      try {\n        packetReceiver.mirrorPacketTo(mirrorOut);\n        mirrorOut.flush();\n      } catch (IOException e) {\n        handleMirrorOutError(e);\n      }\n    }\n    \n    ByteBuffer dataBuf = packetReceiver.getDataSlice();\n    ByteBuffer checksumBuf = packetReceiver.getChecksumSlice();\n    \n    if (lastPacketInBlock || len == 0) {\n      if(LOG.isDebugEnabled()) {\n        LOG.debug(\"Receiving an empty packet or the end of the block \" + block);\n      }\n      // sync block if requested\n      if (syncBlock) {\n        flushOrSync(true);\n      }\n    } else {\n      int checksumLen = ((len + bytesPerChecksum - 1)/bytesPerChecksum)*\n                                                            checksumSize;\n\n      if ( checksumBuf.capacity() != checksumLen) {\n        throw new IOException(\"Length of checksums in packet \" +\n            checksumBuf.capacity() + \" does not match calculated checksum \" +\n            \"length \" + checksumLen);\n      }\n\n      if (shouldVerifyChecksum()) {\n        try {\n          verifyChunks(dataBuf, checksumBuf);\n        } catch (IOException ioe) {\n          // checksum error detected locally. there is no reason to continue.\n          if (responder != null) {\n            try {\n              ((PacketResponder) responder.getRunnable()).enqueue(seqno,\n                  lastPacketInBlock, offsetInBlock,\n                  Status.ERROR_CHECKSUM);\n              // Wait until the responder sends back the response\n              // and interrupt this thread.\n              Thread.sleep(3000);\n            } catch (InterruptedException e) { }\n          }\n          throw new IOException(\"Terminating due to a checksum error.\" + ioe);\n        }\n \n        if (needsChecksumTranslation) {\n          // overwrite the checksums in the packet buffer with the\n          // appropriate polynomial for the disk storage.\n          translateChunks(dataBuf, checksumBuf);\n        }\n      }\n      \n      // by this point, the data in the buffer uses the disk checksum\n\n      byte[] lastChunkChecksum;\n      \n      try {\n        long onDiskLen = replicaInfo.getBytesOnDisk();\n        if (onDiskLen<offsetInBlock) {\n          //finally write to the disk :\n          \n          if (onDiskLen % bytesPerChecksum != 0) { \n            // prepare to overwrite last checksum\n            adjustCrcFilePosition();\n          }\n          \n          // If this is a partial chunk, then read in pre-existing checksum\n          if (firstByteInBlock % bytesPerChecksum != 0) {\n            LOG.info(\"Packet starts at \" + firstByteInBlock +\n                     \" for \" + block +\n                     \" which is not a multiple of bytesPerChecksum \" +\n                     bytesPerChecksum);\n            long offsetInChecksum = BlockMetadataHeader.getHeaderSize() +\n                onDiskLen / bytesPerChecksum * checksumSize;\n            computePartialChunkCrc(onDiskLen, offsetInChecksum, bytesPerChecksum);\n          }\n\n          int startByteToDisk = (int)(onDiskLen-firstByteInBlock) \n              + dataBuf.arrayOffset() + dataBuf.position();\n\n          int numBytesToDisk = (int)(offsetInBlock-onDiskLen);\n          \n          // Write data to disk.\n          out.write(dataBuf.array(), startByteToDisk, numBytesToDisk);\n\n          // If this is a partial chunk, then verify that this is the only\n          // chunk in the packet. Calculate new crc for this chunk.\n          if (partialCrc != null) {\n            if (len > bytesPerChecksum) {\n              throw new IOException(\"Got wrong length during writeBlock(\" + \n                                    block + \") from \" + inAddr + \" \" +\n                                    \"A packet can have only one partial chunk.\"+\n                                    \" len = \" + len + \n                                    \" bytesPerChecksum \" + bytesPerChecksum);\n            }\n            partialCrc.update(dataBuf.array(), startByteToDisk, numBytesToDisk);\n            byte[] buf = FSOutputSummer.convertToByteStream(partialCrc, checksumSize);\n            lastChunkChecksum = Arrays.copyOfRange(\n              buf, buf.length - checksumSize, buf.length\n            );\n            checksumOut.write(buf);\n            if(LOG.isDebugEnabled()) {\n              LOG.debug(\"Writing out partial crc for data len \" + len);\n            }\n            partialCrc = null;\n          } else {\n            lastChunkChecksum = Arrays.copyOfRange(\n                checksumBuf.array(),\n                checksumBuf.arrayOffset() + checksumBuf.position() + checksumLen - checksumSize,\n                checksumBuf.arrayOffset() + checksumBuf.position() + checksumLen);\n            checksumOut.write(checksumBuf.array(),\n                checksumBuf.arrayOffset() + checksumBuf.position(),\n                checksumLen);\n          }\n          /// flush entire packet, sync if requested\n          flushOrSync(syncBlock);\n          \n          replicaInfo.setLastChecksumAndDataLen(\n            offsetInBlock, lastChunkChecksum\n          );\n\n          datanode.metrics.incrBytesWritten(len);\n\n          manageWriterOsCache(offsetInBlock);\n        }\n      } catch (IOException iex) {\n        datanode.checkDiskError(iex);\n        throw iex;\n      }\n    }\n\n    // if sync was requested, put in queue for pending acks here\n    // (after the fsync finished)\n    if (responder != null && (syncBlock || shouldVerifyChecksum())) {\n      ((PacketResponder) responder.getRunnable()).enqueue(seqno,\n          lastPacketInBlock, offsetInBlock, Status.SUCCESS);\n    }\n\n    if (throttler != null) { // throttle I/O\n      throttler.throttle(len);\n    }\n    \n    return lastPacketInBlock?-1:len;\n  }",
                "code_after_change": "  private int receivePacket() throws IOException {\n    // read the next packet\n    packetReceiver.receiveNextPacket(in);\n\n    PacketHeader header = packetReceiver.getHeader();\n    if (LOG.isDebugEnabled()){\n      LOG.debug(\"Receiving one packet for block \" + block +\n                \": \" + header);\n    }\n\n    // Sanity check the header\n    if (header.getOffsetInBlock() > replicaInfo.getNumBytes()) {\n      throw new IOException(\"Received an out-of-sequence packet for \" + block + \n          \"from \" + inAddr + \" at offset \" + header.getOffsetInBlock() +\n          \". Expecting packet starting at \" + replicaInfo.getNumBytes());\n    }\n    if (header.getDataLen() < 0) {\n      throw new IOException(\"Got wrong length during writeBlock(\" + block + \n                            \") from \" + inAddr + \" at offset \" + \n                            header.getOffsetInBlock() + \": \" +\n                            header.getDataLen()); \n    }\n\n    long offsetInBlock = header.getOffsetInBlock();\n    long seqno = header.getSeqno();\n    boolean lastPacketInBlock = header.isLastPacketInBlock();\n    int len = header.getDataLen();\n    boolean syncBlock = header.getSyncBlock();\n\n    // avoid double sync'ing on close\n    if (syncBlock && lastPacketInBlock) {\n      this.syncOnClose = false;\n    }\n\n    // update received bytes\n    long firstByteInBlock = offsetInBlock;\n    offsetInBlock += len;\n    if (replicaInfo.getNumBytes() < offsetInBlock) {\n      replicaInfo.setNumBytes(offsetInBlock);\n    }\n    \n    // put in queue for pending acks, unless sync was requested\n    if (responder != null && !syncBlock && !shouldVerifyChecksum()) {\n      ((PacketResponder) responder.getRunnable()).enqueue(seqno,\n          lastPacketInBlock, offsetInBlock, Status.SUCCESS);\n    }\n\n    //First write the packet to the mirror:\n    if (mirrorOut != null && !mirrorError) {\n      try {\n        long begin = Time.monotonicNow();\n        packetReceiver.mirrorPacketTo(mirrorOut);\n        mirrorOut.flush();\n        long duration = Time.monotonicNow() - begin;\n        if (duration > datanodeSlowLogThresholdMs) {\n          LOG.warn(\"Slow BlockReceiver write packet to mirror took \" + duration\n              + \"ms (threshold=\" + datanodeSlowLogThresholdMs + \"ms)\");\n        }\n      } catch (IOException e) {\n        handleMirrorOutError(e);\n      }\n    }\n    \n    ByteBuffer dataBuf = packetReceiver.getDataSlice();\n    ByteBuffer checksumBuf = packetReceiver.getChecksumSlice();\n    \n    if (lastPacketInBlock || len == 0) {\n      if(LOG.isDebugEnabled()) {\n        LOG.debug(\"Receiving an empty packet or the end of the block \" + block);\n      }\n      // sync block if requested\n      if (syncBlock) {\n        flushOrSync(true);\n      }\n    } else {\n      int checksumLen = ((len + bytesPerChecksum - 1)/bytesPerChecksum)*\n                                                            checksumSize;\n\n      if ( checksumBuf.capacity() != checksumLen) {\n        throw new IOException(\"Length of checksums in packet \" +\n            checksumBuf.capacity() + \" does not match calculated checksum \" +\n            \"length \" + checksumLen);\n      }\n\n      if (shouldVerifyChecksum()) {\n        try {\n          verifyChunks(dataBuf, checksumBuf);\n        } catch (IOException ioe) {\n          // checksum error detected locally. there is no reason to continue.\n          if (responder != null) {\n            try {\n              ((PacketResponder) responder.getRunnable()).enqueue(seqno,\n                  lastPacketInBlock, offsetInBlock,\n                  Status.ERROR_CHECKSUM);\n              // Wait until the responder sends back the response\n              // and interrupt this thread.\n              Thread.sleep(3000);\n            } catch (InterruptedException e) { }\n          }\n          throw new IOException(\"Terminating due to a checksum error.\" + ioe);\n        }\n \n        if (needsChecksumTranslation) {\n          // overwrite the checksums in the packet buffer with the\n          // appropriate polynomial for the disk storage.\n          translateChunks(dataBuf, checksumBuf);\n        }\n      }\n      \n      // by this point, the data in the buffer uses the disk checksum\n\n      byte[] lastChunkChecksum;\n      \n      try {\n        long onDiskLen = replicaInfo.getBytesOnDisk();\n        if (onDiskLen<offsetInBlock) {\n          //finally write to the disk :\n          \n          if (onDiskLen % bytesPerChecksum != 0) { \n            // prepare to overwrite last checksum\n            adjustCrcFilePosition();\n          }\n          \n          // If this is a partial chunk, then read in pre-existing checksum\n          if (firstByteInBlock % bytesPerChecksum != 0) {\n            LOG.info(\"Packet starts at \" + firstByteInBlock +\n                     \" for \" + block +\n                     \" which is not a multiple of bytesPerChecksum \" +\n                     bytesPerChecksum);\n            long offsetInChecksum = BlockMetadataHeader.getHeaderSize() +\n                onDiskLen / bytesPerChecksum * checksumSize;\n            computePartialChunkCrc(onDiskLen, offsetInChecksum, bytesPerChecksum);\n          }\n\n          int startByteToDisk = (int)(onDiskLen-firstByteInBlock) \n              + dataBuf.arrayOffset() + dataBuf.position();\n\n          int numBytesToDisk = (int)(offsetInBlock-onDiskLen);\n          \n          // Write data to disk.\n          long begin = Time.monotonicNow();\n          out.write(dataBuf.array(), startByteToDisk, numBytesToDisk);\n          long duration = Time.monotonicNow() - begin;\n          if (duration > datanodeSlowLogThresholdMs) {\n            LOG.warn(\"Slow BlockReceiver write data to disk cost:\" + duration\n                + \"ms (threshold=\" + datanodeSlowLogThresholdMs + \"ms)\");\n          }\n\n          // If this is a partial chunk, then verify that this is the only\n          // chunk in the packet. Calculate new crc for this chunk.\n          if (partialCrc != null) {\n            if (len > bytesPerChecksum) {\n              throw new IOException(\"Got wrong length during writeBlock(\" + \n                                    block + \") from \" + inAddr + \" \" +\n                                    \"A packet can have only one partial chunk.\"+\n                                    \" len = \" + len + \n                                    \" bytesPerChecksum \" + bytesPerChecksum);\n            }\n            partialCrc.update(dataBuf.array(), startByteToDisk, numBytesToDisk);\n            byte[] buf = FSOutputSummer.convertToByteStream(partialCrc, checksumSize);\n            lastChunkChecksum = Arrays.copyOfRange(\n              buf, buf.length - checksumSize, buf.length\n            );\n            checksumOut.write(buf);\n            if(LOG.isDebugEnabled()) {\n              LOG.debug(\"Writing out partial crc for data len \" + len);\n            }\n            partialCrc = null;\n          } else {\n            lastChunkChecksum = Arrays.copyOfRange(\n                checksumBuf.array(),\n                checksumBuf.arrayOffset() + checksumBuf.position() + checksumLen - checksumSize,\n                checksumBuf.arrayOffset() + checksumBuf.position() + checksumLen);\n            checksumOut.write(checksumBuf.array(),\n                checksumBuf.arrayOffset() + checksumBuf.position(),\n                checksumLen);\n          }\n          /// flush entire packet, sync if requested\n          flushOrSync(syncBlock);\n          \n          replicaInfo.setLastChecksumAndDataLen(\n            offsetInBlock, lastChunkChecksum\n          );\n\n          datanode.metrics.incrBytesWritten(len);\n\n          manageWriterOsCache(offsetInBlock);\n        }\n      } catch (IOException iex) {\n        datanode.checkDiskErrorAsync();\n        throw iex;\n      }\n    }\n\n    // if sync was requested, put in queue for pending acks here\n    // (after the fsync finished)\n    if (responder != null && (syncBlock || shouldVerifyChecksum())) {\n      ((PacketResponder) responder.getRunnable()).enqueue(seqno,\n          lastPacketInBlock, offsetInBlock, Status.SUCCESS);\n    }\n\n    if (throttler != null) { // throttle I/O\n      throttler.throttle(len);\n    }\n    \n    return lastPacketInBlock?-1:len;\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Direct Caller/Callee"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the issue occurring when a data directory is full, which is related to the method 'DataNode.initBlockPool' in the stack trace. This method is a direct caller of the ground truth method 'DataNode.initBlockPool', hence the root cause identification is 'Partial' with 'Direct Caller/Callee'. The report does not provide any fix suggestion, so it is marked as 'Missing'. The problem location is identified in the stack trace context, but not precisely at the ground truth methods, so it is 'Partial' with 'Shared Stack Trace Context'. There is no incorrect information in the bug report, so 'Wrong Information' is 'No'."
        }
    },
    {
        "filename": "HDFS-13164.json",
        "code_diff": {
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.DFSOutputStream.closeImpl": {
                "code_before_change": "  protected synchronized void closeImpl() throws IOException {\n    if (isClosed()) {\n      getStreamer().getLastException().check(true);\n      return;\n    }\n\n    try {\n      flushBuffer();       // flush from all upper layers\n\n      if (currentPacket != null) {\n        enqueueCurrentPacket();\n      }\n\n      if (getStreamer().getBytesCurBlock() != 0) {\n        setCurrentPacketToEmpty();\n      }\n\n      flushInternal();             // flush all data to Datanodes\n      // get last block before destroying the streamer\n      ExtendedBlock lastBlock = getStreamer().getBlock();\n\n      try (TraceScope ignored =\n               dfsClient.getTracer().newScope(\"completeFile\")) {\n        completeFile(lastBlock);\n      }\n    } catch (ClosedChannelException ignored) {\n    } finally {\n      // Failures may happen when flushing data.\n      // Streamers may keep waiting for the new block information.\n      // Thus need to force closing these threads.\n      // Don't need to call setClosed() because closeThreads(true)\n      // calls setClosed() in the finally block.\n      closeThreads(true);\n    }\n  }",
                "code_after_change": "  protected synchronized void closeImpl() throws IOException {\n    if (isClosed()) {\n      LOG.debug(\"Closing an already closed stream. [Stream:{}, streamer:{}]\",\n          closed, getStreamer().streamerClosed());\n      try {\n        getStreamer().getLastException().check(true);\n      } catch (IOException ioe) {\n        cleanupAndRethrowIOException(ioe);\n      } finally {\n        if (!closed) {\n          // If stream is not closed but streamer closed, clean up the stream.\n          // Most importantly, end the file lease.\n          closeThreads(true);\n        }\n      }\n      return;\n    }\n\n    try {\n      flushBuffer();       // flush from all upper layers\n\n      if (currentPacket != null) {\n        enqueueCurrentPacket();\n      }\n\n      if (getStreamer().getBytesCurBlock() != 0) {\n        setCurrentPacketToEmpty();\n      }\n\n      try {\n        flushInternal();             // flush all data to Datanodes\n      } catch (IOException ioe) {\n        cleanupAndRethrowIOException(ioe);\n      }\n      completeFile();\n    } catch (ClosedChannelException ignored) {\n    } finally {\n      // Failures may happen when flushing data.\n      // Streamers may keep waiting for the new block information.\n      // Thus need to force closing these threads.\n      // Don't need to call setClosed() because closeThreads(true)\n      // calls setClosed() in the finally block.\n      closeThreads(true);\n    }\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs-client.src.main.java.org.apache.hadoop.hdfs.DFSOutputStream.close": {
                "code_before_change": "  public void close() throws IOException {\n    final MultipleIOException.Builder b = new MultipleIOException.Builder();\n    synchronized (this) {\n      try (TraceScope ignored = dfsClient.newPathTraceScope(\n          \"DFSOutputStream#close\", src)) {\n        closeImpl();\n      } catch (IOException e) {\n        b.add(e);\n      }\n    }\n    final IOException ioe = b.build();\n    if (ioe != null) {\n      throw ioe;\n    }\n  }",
                "code_after_change": "  public void close() throws IOException {\n    final MultipleIOException.Builder b = new MultipleIOException.Builder();\n    synchronized (this) {\n      try (TraceScope ignored = dfsClient.newPathTraceScope(\n          \"DFSOutputStream#close\", src)) {\n        closeImpl();\n      } catch (IOException e) {\n        b.add(e);\n      }\n    }\n    final IOException ioe = b.build();\n    if (ioe != null) {\n      throw ioe;\n    }\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report precisely identifies the root cause related to the ground truth methods 'DFSOutputStream.closeImpl' and 'DFSOutputStream.close'. The description explains the issue with the 'isClosed' check and the behavior when the disk quota is exceeded, which aligns with the changes made in the ground truth methods. However, the bug report does not provide any specific fix suggestion, hence it is marked as 'Missing' for fix suggestion. The problem location is also precisely identified as it directly relates to the methods in the ground truth. There is no wrong information in the bug report as all the details are relevant to the context of the bug."
        }
    },
    {
        "filename": "HDFS-11508.json",
        "code_diff": {
            "hadoop-common-project.hadoop-nfs.src.main.java.org.apache.hadoop.portmap.Portmap.getPipeline": {
                "code_before_change": "      public ChannelPipeline getPipeline() throws Exception {\n        return Channels.pipeline(RpcUtil.constructRpcFrameDecoder(),\n            RpcUtil.STAGE_RPC_MESSAGE_PARSER, idleStateHandler, handler,\n            RpcUtil.STAGE_RPC_TCP_RESPONSE);\n      }",
                "code_after_change": "      public ChannelPipeline getPipeline() throws Exception {\n        return Channels.pipeline(RpcUtil.constructRpcFrameDecoder(),\n            RpcUtil.STAGE_RPC_MESSAGE_PARSER, idleStateHandler, handler,\n            RpcUtil.STAGE_RPC_TCP_RESPONSE);\n      }"
            },
            "hadoop-common-project.hadoop-nfs.src.main.java.org.apache.hadoop.oncrpc.SimpleTcpServer.getPipeline": {
                "code_before_change": "      public ChannelPipeline getPipeline() throws Exception {\n        return Channels.pipeline(RpcUtil.constructRpcFrameDecoder(),\n            RpcUtil.STAGE_RPC_MESSAGE_PARSER, rpcProgram,\n            RpcUtil.STAGE_RPC_TCP_RESPONSE);\n      }",
                "code_after_change": "      public ChannelPipeline getPipeline() throws Exception {\n        return Channels.pipeline(RpcUtil.constructRpcFrameDecoder(),\n            RpcUtil.STAGE_RPC_MESSAGE_PARSER, rpcProgram,\n            RpcUtil.STAGE_RPC_TCP_RESPONSE);\n      }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Preventive",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the issue of bind failure due to the socket being in the TIME_WAIT state, which is related to the methods in the stack trace but not directly to the ground truth methods. The suggestion to use the setReuseAddress option is a preventive measure that could mitigate the bind failure issue. The problem location is partially identified as it mentions methods in the stack trace context but not the exact ground truth methods. There is no wrong information as the report correctly describes the issue and provides a relevant suggestion."
        }
    },
    {
        "filename": "HDFS-2991.json",
        "code_diff": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSDirectory.addFile": {
                "code_before_change": "  INodeFileUnderConstruction addFile(String path, \n                PermissionStatus permissions,\n                short replication,\n                long preferredBlockSize,\n                String clientName,\n                String clientMachine,\n                DatanodeDescriptor clientNode,\n                long generationStamp) \n    throws FileAlreadyExistsException, QuotaExceededException,\n      UnresolvedLinkException {\n    waitForReady();\n\n    // Always do an implicit mkdirs for parent directory tree.\n    long modTime = now();\n    if (!mkdirs(new Path(path).getParent().toString(), permissions, true,\n        modTime)) {\n      return null;\n    }\n    INodeFileUnderConstruction newNode = new INodeFileUnderConstruction(\n                                 permissions,replication,\n                                 preferredBlockSize, modTime, clientName, \n                                 clientMachine, clientNode);\n    writeLock();\n    try {\n      newNode = addNode(path, newNode, UNKNOWN_DISK_SPACE);\n    } finally {\n      writeUnlock();\n    }\n    if (newNode == null) {\n      NameNode.stateChangeLog.info(\"DIR* FSDirectory.addFile: \"\n                                   +\"failed to add \"+path\n                                   +\" to the file system\");\n      return null;\n    }\n    // add create file record to log, record new generation stamp\n    fsImage.getEditLog().logOpenFile(path, newNode);\n\n    if(NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.addFile: \"\n          +path+\" is added to the file system\");\n    }\n    return newNode;\n  }",
                "code_after_change": "  INodeFileUnderConstruction addFile(String path, \n                PermissionStatus permissions,\n                short replication,\n                long preferredBlockSize,\n                String clientName,\n                String clientMachine,\n                DatanodeDescriptor clientNode,\n                long generationStamp) \n    throws FileAlreadyExistsException, QuotaExceededException,\n      UnresolvedLinkException {\n    waitForReady();\n\n    // Always do an implicit mkdirs for parent directory tree.\n    long modTime = now();\n    if (!mkdirs(new Path(path).getParent().toString(), permissions, true,\n        modTime)) {\n      return null;\n    }\n    INodeFileUnderConstruction newNode = new INodeFileUnderConstruction(\n                                 permissions,replication,\n                                 preferredBlockSize, modTime, clientName, \n                                 clientMachine, clientNode);\n    writeLock();\n    try {\n      newNode = addNode(path, newNode, UNKNOWN_DISK_SPACE);\n    } finally {\n      writeUnlock();\n    }\n    if (newNode == null) {\n      NameNode.stateChangeLog.info(\"DIR* FSDirectory.addFile: \"\n                                   +\"failed to add \"+path\n                                   +\" to the file system\");\n      return null;\n    }\n\n    if(NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* FSDirectory.addFile: \"\n          +path+\" is added to the file system\");\n    }\n    return newNode;\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords": {
                "code_before_change": "  long loadEditRecords(int logVersion, EditLogInputStream in, boolean closeOnExit,\n                      long expectedStartingTxId)\n      throws IOException, EditLogInputException {\n    FSDirectory fsDir = fsNamesys.dir;\n    long numEdits = 0;\n\n    EnumMap<FSEditLogOpCodes, Holder<Integer>> opCounts =\n      new EnumMap<FSEditLogOpCodes, Holder<Integer>>(FSEditLogOpCodes.class);\n\n    fsNamesys.writeLock();\n    fsDir.writeLock();\n\n    long recentOpcodeOffsets[] = new long[4];\n    Arrays.fill(recentOpcodeOffsets, -1);\n\n    try {\n      long txId = expectedStartingTxId - 1;\n\n      try {\n        while (true) {\n          FSEditLogOp op;\n          try {\n            if ((op = in.readOp()) == null) {\n              break;\n            }\n          } catch (IOException ioe) {\n            String errorMessage = formatEditLogReplayError(in, recentOpcodeOffsets);\n            FSImage.LOG.error(errorMessage);\n            throw new EditLogInputException(errorMessage,\n                ioe, numEdits);\n          }\n          recentOpcodeOffsets[(int)(numEdits % recentOpcodeOffsets.length)] =\n            in.getPosition();\n          if (LayoutVersion.supports(Feature.STORED_TXIDS, logVersion)) {\n            long thisTxId = op.txid;\n            if (thisTxId != txId + 1) {\n              throw new IOException(\"Expected transaction ID \" +\n                  (txId + 1) + \" but got \" + thisTxId);\n            }\n            txId = thisTxId;\n          }\n\n          incrOpCount(op.opCode, opCounts);\n          try {\n            applyEditLogOp(op, fsDir, logVersion);\n          } catch (Throwable t) {\n            // Catch Throwable because in the case of a truly corrupt edits log, any\n            // sort of error might be thrown (NumberFormat, NullPointer, EOF, etc.)\n            String errorMessage = formatEditLogReplayError(in, recentOpcodeOffsets);\n            FSImage.LOG.error(errorMessage);\n            throw new IOException(errorMessage, t);\n          }\n          numEdits++;\n        }\n      } catch (IOException ex) {\n        check203UpgradeFailure(logVersion, ex);\n      } finally {\n        if(closeOnExit)\n          in.close();\n      }\n    } finally {\n      fsDir.writeUnlock();\n      fsNamesys.writeUnlock();\n      if (FSImage.LOG.isDebugEnabled()) {\n        dumpOpCounts(opCounts);\n      }\n    }\n    return numEdits;\n  }",
                "code_after_change": "  int loadEditRecords(int logVersion, EditLogInputStream in, boolean closeOnExit,\n                      long expectedStartingTxId)\n      throws IOException {\n    FSDirectory fsDir = fsNamesys.dir;\n    int numEdits = 0;\n\n    EnumMap<FSEditLogOpCodes, Holder<Integer>> opCounts =\n      new EnumMap<FSEditLogOpCodes, Holder<Integer>>(FSEditLogOpCodes.class);\n\n    fsNamesys.writeLock();\n    fsDir.writeLock();\n\n    long recentOpcodeOffsets[] = new long[4];\n    Arrays.fill(recentOpcodeOffsets, -1);\n\n    try {\n      long txId = expectedStartingTxId - 1;\n\n      try {\n        FSEditLogOp op;\n        while ((op = in.readOp()) != null) {\n          recentOpcodeOffsets[numEdits % recentOpcodeOffsets.length] =\n            in.getPosition();\n          if (LayoutVersion.supports(Feature.STORED_TXIDS, logVersion)) {\n            long thisTxId = op.txid;\n            if (thisTxId != txId + 1) {\n              throw new IOException(\"Expected transaction ID \" +\n                  (txId + 1) + \" but got \" + thisTxId);\n            }\n            txId = thisTxId;\n          }\n\n          numEdits++;\n          incrOpCount(op.opCode, opCounts);\n          switch (op.opCode) {\n          case OP_ADD:\n          case OP_CLOSE: {\n            AddCloseOp addCloseOp = (AddCloseOp)op;\n\n            // versions > 0 support per file replication\n            // get name and replication\n            final short replication  = fsNamesys.getBlockManager(\n                ).adjustReplication(addCloseOp.replication);\n\n            long blockSize = addCloseOp.blockSize;\n            BlockInfo blocks[] = new BlockInfo[addCloseOp.blocks.length];\n            for (int i = 0; i < addCloseOp.blocks.length; i++) {\n              if(addCloseOp.opCode == FSEditLogOpCodes.OP_ADD\n                 && i == addCloseOp.blocks.length-1) {\n                blocks[i] = new BlockInfoUnderConstruction(addCloseOp.blocks[i],\n                                                           replication);\n              } else {\n                blocks[i] = new BlockInfo(addCloseOp.blocks[i], replication);\n              }\n            }\n\n            PermissionStatus permissions = fsNamesys.getUpgradePermission();\n            if (addCloseOp.permissions != null) {\n              permissions = addCloseOp.permissions;\n            }\n\n\n            // Older versions of HDFS does not store the block size in inode.\n            // If the file has more than one block, use the size of the\n            // first block as the blocksize. Otherwise use the default\n            // block size.\n            if (-8 <= logVersion && blockSize == 0) {\n              if (blocks.length > 1) {\n                blockSize = blocks[0].getNumBytes();\n              } else {\n                long first = ((blocks.length == 1)? blocks[0].getNumBytes(): 0);\n                blockSize = Math.max(fsNamesys.getDefaultBlockSize(), first);\n              }\n            }\n\n\n            // The open lease transaction re-creates a file if necessary.\n            // Delete the file if it already exists.\n            if (FSNamesystem.LOG.isDebugEnabled()) {\n              FSNamesystem.LOG.debug(op.opCode + \": \" + addCloseOp.path +\n                  \" numblocks : \" + blocks.length +\n                  \" clientHolder \" + addCloseOp.clientName +\n                  \" clientMachine \" + addCloseOp.clientMachine);\n            }\n\n            // There are four cases here:\n            // 1. OP_ADD to create a new file\n            // 2. OP_ADD to update file blocks\n            // 3. OP_ADD to open file for append\n            // 4. OP_CLOSE to close the file\n\n            // See if the file already exists\n            INodeFile oldFile = fsDir.getFileINode(addCloseOp.path);\n            if (oldFile == null) { // OP_ADD for a new file\n              assert addCloseOp.opCode == FSEditLogOpCodes.OP_ADD : \n                \"Expected opcode OP_ADD, but got \" + addCloseOp.opCode;\n              fsDir.unprotectedAddFile(\n                  addCloseOp.path, permissions, blocks, replication,\n                  addCloseOp.mtime, addCloseOp.atime, blockSize,\n                  addCloseOp.clientName, addCloseOp.clientMachine);\n            } else {\n              fsDir.updateFile(oldFile, addCloseOp.path, blocks,\n                  addCloseOp.mtime, addCloseOp.atime);\n              if(addCloseOp.opCode == FSEditLogOpCodes.OP_CLOSE) {  // OP_CLOSE\n                if (!oldFile.isUnderConstruction() &&\n                    logVersion <= LayoutVersion.BUGFIX_HDFS_2991_VERSION) {\n                  // There was a bug (HDFS-2991) in hadoop < 0.23.1 where OP_CLOSE\n                  // could show up twice in a row. But after that version, this\n                  // should be fixed, so we should treat it as an error.\n                  throw new IOException(\n                      \"File is not under construction: \" + addCloseOp.path);\n                }\n                fsNamesys.getBlockManager().completeBlock(\n                    oldFile, blocks.length-1, true);\n                \n                if (oldFile.isUnderConstruction()) {\n                  INodeFile newFile =\n                    ((INodeFileUnderConstruction)oldFile).convertToInodeFile();\n                  fsDir.replaceNode(addCloseOp.path, oldFile, newFile);\n                }\n              } else if(! oldFile.isUnderConstruction()) {  // OP_ADD for append\n                INodeFileUnderConstruction cons = new INodeFileUnderConstruction(\n                    oldFile.getLocalNameBytes(),\n                    oldFile.getReplication(), \n                    oldFile.getModificationTime(),\n                    oldFile.getPreferredBlockSize(),\n                    oldFile.getBlocks(),\n                    oldFile.getPermissionStatus(),\n                    addCloseOp.clientName,\n                    addCloseOp.clientMachine,\n                    null);\n                fsDir.replaceNode(addCloseOp.path, oldFile, cons);\n              }\n            }\n            // Update file lease\n            if(addCloseOp.opCode == FSEditLogOpCodes.OP_ADD) {\n              fsNamesys.leaseManager.addLease(addCloseOp.clientName, addCloseOp.path);\n            } else {  // Ops.OP_CLOSE\n              if (oldFile.isUnderConstruction()) {\n                fsNamesys.leaseManager.removeLease(\n                    ((INodeFileUnderConstruction)oldFile).getClientName(), addCloseOp.path);\n              }\n            }\n            break;\n          }\n          case OP_SET_REPLICATION: {\n            SetReplicationOp setReplicationOp = (SetReplicationOp)op;\n            short replication = fsNamesys.getBlockManager().adjustReplication(\n                setReplicationOp.replication);\n            fsDir.unprotectedSetReplication(setReplicationOp.path,\n                                            replication, null);\n            break;\n          }\n          case OP_CONCAT_DELETE: {\n            ConcatDeleteOp concatDeleteOp = (ConcatDeleteOp)op;\n            fsDir.unprotectedConcat(concatDeleteOp.trg, concatDeleteOp.srcs,\n                concatDeleteOp.timestamp);\n            break;\n          }\n          case OP_RENAME_OLD: {\n            RenameOldOp renameOp = (RenameOldOp)op;\n            HdfsFileStatus dinfo = fsDir.getFileInfo(renameOp.dst, false);\n            fsDir.unprotectedRenameTo(renameOp.src, renameOp.dst,\n                                      renameOp.timestamp);\n            fsNamesys.unprotectedChangeLease(renameOp.src, renameOp.dst, dinfo);\n            break;\n          }\n          case OP_DELETE: {\n            DeleteOp deleteOp = (DeleteOp)op;\n            fsDir.unprotectedDelete(deleteOp.path, deleteOp.timestamp);\n            break;\n          }\n          case OP_MKDIR: {\n            MkdirOp mkdirOp = (MkdirOp)op;\n            PermissionStatus permissions = fsNamesys.getUpgradePermission();\n            if (mkdirOp.permissions != null) {\n              permissions = mkdirOp.permissions;\n            }\n\n            fsDir.unprotectedMkdir(mkdirOp.path, permissions,\n                                   mkdirOp.timestamp);\n            break;\n          }\n          case OP_SET_GENSTAMP: {\n            SetGenstampOp setGenstampOp = (SetGenstampOp)op;\n            fsNamesys.setGenerationStamp(setGenstampOp.genStamp);\n            break;\n          }\n          case OP_SET_PERMISSIONS: {\n            SetPermissionsOp setPermissionsOp = (SetPermissionsOp)op;\n            fsDir.unprotectedSetPermission(setPermissionsOp.src,\n                                           setPermissionsOp.permissions);\n            break;\n          }\n          case OP_SET_OWNER: {\n            SetOwnerOp setOwnerOp = (SetOwnerOp)op;\n            fsDir.unprotectedSetOwner(setOwnerOp.src, setOwnerOp.username,\n                                      setOwnerOp.groupname);\n            break;\n          }\n          case OP_SET_NS_QUOTA: {\n            SetNSQuotaOp setNSQuotaOp = (SetNSQuotaOp)op;\n            fsDir.unprotectedSetQuota(setNSQuotaOp.src,\n                                      setNSQuotaOp.nsQuota,\n                                      HdfsConstants.QUOTA_DONT_SET);\n            break;\n          }\n          case OP_CLEAR_NS_QUOTA: {\n            ClearNSQuotaOp clearNSQuotaOp = (ClearNSQuotaOp)op;\n            fsDir.unprotectedSetQuota(clearNSQuotaOp.src,\n                                      HdfsConstants.QUOTA_RESET,\n                                      HdfsConstants.QUOTA_DONT_SET);\n            break;\n          }\n\n          case OP_SET_QUOTA:\n            SetQuotaOp setQuotaOp = (SetQuotaOp)op;\n            fsDir.unprotectedSetQuota(setQuotaOp.src,\n                                      setQuotaOp.nsQuota,\n                                      setQuotaOp.dsQuota);\n            break;\n\n          case OP_TIMES: {\n            TimesOp timesOp = (TimesOp)op;\n\n            fsDir.unprotectedSetTimes(timesOp.path,\n                                      timesOp.mtime,\n                                      timesOp.atime, true);\n            break;\n          }\n          case OP_SYMLINK: {\n            SymlinkOp symlinkOp = (SymlinkOp)op;\n            fsDir.unprotectedSymlink(symlinkOp.path, symlinkOp.value,\n                                     symlinkOp.mtime, symlinkOp.atime,\n                                     symlinkOp.permissionStatus);\n            break;\n          }\n          case OP_RENAME: {\n            RenameOp renameOp = (RenameOp)op;\n\n            HdfsFileStatus dinfo = fsDir.getFileInfo(renameOp.dst, false);\n            fsDir.unprotectedRenameTo(renameOp.src, renameOp.dst,\n                                      renameOp.timestamp, renameOp.options);\n            fsNamesys.unprotectedChangeLease(renameOp.src, renameOp.dst, dinfo);\n            break;\n          }\n          case OP_GET_DELEGATION_TOKEN: {\n            GetDelegationTokenOp getDelegationTokenOp\n              = (GetDelegationTokenOp)op;\n\n            fsNamesys.getDelegationTokenSecretManager()\n              .addPersistedDelegationToken(getDelegationTokenOp.token,\n                                           getDelegationTokenOp.expiryTime);\n            break;\n          }\n          case OP_RENEW_DELEGATION_TOKEN: {\n            RenewDelegationTokenOp renewDelegationTokenOp\n              = (RenewDelegationTokenOp)op;\n            fsNamesys.getDelegationTokenSecretManager()\n              .updatePersistedTokenRenewal(renewDelegationTokenOp.token,\n                                           renewDelegationTokenOp.expiryTime);\n            break;\n          }\n          case OP_CANCEL_DELEGATION_TOKEN: {\n            CancelDelegationTokenOp cancelDelegationTokenOp\n              = (CancelDelegationTokenOp)op;\n            fsNamesys.getDelegationTokenSecretManager()\n                .updatePersistedTokenCancellation(\n                    cancelDelegationTokenOp.token);\n            break;\n          }\n          case OP_UPDATE_MASTER_KEY: {\n            UpdateMasterKeyOp updateMasterKeyOp = (UpdateMasterKeyOp)op;\n            fsNamesys.getDelegationTokenSecretManager()\n              .updatePersistedMasterKey(updateMasterKeyOp.key);\n            break;\n          }\n          case OP_REASSIGN_LEASE: {\n            ReassignLeaseOp reassignLeaseOp = (ReassignLeaseOp)op;\n\n            Lease lease = fsNamesys.leaseManager.getLease(\n                reassignLeaseOp.leaseHolder);\n            INodeFileUnderConstruction pendingFile =\n                (INodeFileUnderConstruction) fsDir.getFileINode(\n                    reassignLeaseOp.path);\n            fsNamesys.reassignLeaseInternal(lease,\n                reassignLeaseOp.path, reassignLeaseOp.newHolder, pendingFile);\n            break;\n          }\n          case OP_START_LOG_SEGMENT:\n          case OP_END_LOG_SEGMENT: {\n            // no data in here currently.\n            break;\n          }\n          case OP_DATANODE_ADD:\n          case OP_DATANODE_REMOVE:\n            break;\n          default:\n            throw new IOException(\"Invalid operation read \" + op.opCode);\n          }\n        }\n\n      } catch (IOException ex) {\n        check203UpgradeFailure(logVersion, ex);\n      } finally {\n        if(closeOnExit)\n          in.close();\n      }\n    } catch (Throwable t) {\n      // Catch Throwable because in the case of a truly corrupt edits log, any\n      // sort of error might be thrown (NumberFormat, NullPointer, EOF, etc.)\n      StringBuilder sb = new StringBuilder();\n      sb.append(\"Error replaying edit log at offset \" + in.getPosition());\n      if (recentOpcodeOffsets[0] != -1) {\n        Arrays.sort(recentOpcodeOffsets);\n        sb.append(\"\\nRecent opcode offsets:\");\n        for (long offset : recentOpcodeOffsets) {\n          if (offset != -1) {\n            sb.append(' ').append(offset);\n          }\n        }\n      }\n      String errorMessage = sb.toString();\n      FSImage.LOG.error(errorMessage);\n      throw new IOException(errorMessage, t);\n    } finally {\n      fsDir.writeUnlock();\n      fsNamesys.writeUnlock();\n    }\n    if (FSImage.LOG.isDebugEnabled()) {\n      dumpOpCounts(opCounts);\n    }\n    return numEdits;\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal": {
                "code_before_change": "  private LocatedBlock startFileInternal(String src,\n      PermissionStatus permissions, String holder, String clientMachine,\n      EnumSet<CreateFlag> flag, boolean createParent, short replication,\n      long blockSize) throws SafeModeException, FileAlreadyExistsException,\n      AccessControlException, UnresolvedLinkException, FileNotFoundException,\n      ParentNotDirectoryException, IOException {\n    assert hasWriteLock();\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* NameSystem.startFile: src=\" + src\n          + \", holder=\" + holder\n          + \", clientMachine=\" + clientMachine\n          + \", createParent=\" + createParent\n          + \", replication=\" + replication\n          + \", createFlag=\" + flag.toString());\n    }\n    if (isInSafeMode()) {\n      throw new SafeModeException(\"Cannot create file\" + src, safeMode);\n    }\n    if (!DFSUtil.isValidName(src)) {\n      throw new InvalidPathException(src);\n    }\n\n    // Verify that the destination does not exist as a directory already.\n    boolean pathExists = dir.exists(src);\n    if (pathExists && dir.isDir(src)) {\n      throw new FileAlreadyExistsException(\"Cannot create file \" + src\n          + \"; already exists as a directory.\");\n    }\n\n    boolean overwrite = flag.contains(CreateFlag.OVERWRITE);\n    boolean append = flag.contains(CreateFlag.APPEND);\n    if (isPermissionEnabled) {\n      if (append || (overwrite && pathExists)) {\n        checkPathAccess(src, FsAction.WRITE);\n      } else {\n        checkAncestorAccess(src, FsAction.WRITE);\n      }\n    }\n\n    if (!createParent) {\n      verifyParentDir(src);\n    }\n\n    try {\n      INode myFile = dir.getFileINode(src);\n      recoverLeaseInternal(myFile, src, holder, clientMachine, false);\n\n      try {\n        blockManager.verifyReplication(src, replication, clientMachine);\n      } catch(IOException e) {\n        throw new IOException(\"failed to create \"+e.getMessage());\n      }\n      boolean create = flag.contains(CreateFlag.CREATE);\n      if (myFile == null) {\n        if (!create) {\n          throw new FileNotFoundException(\"failed to overwrite or append to non-existent file \"\n            + src + \" on client \" + clientMachine);\n        }\n      } else {\n        // File exists - must be one of append or overwrite\n        if (overwrite) {\n          delete(src, true);\n        } else if (!append) {\n          throw new FileAlreadyExistsException(\"failed to create file \" + src\n              + \" on client \" + clientMachine\n              + \" because the file exists\");\n        }\n      }\n\n      final DatanodeDescriptor clientNode = \n          blockManager.getDatanodeManager().getDatanodeByHost(clientMachine);\n\n      if (append && myFile != null) {\n        return prepareFileForWrite(\n            src, myFile, holder, clientMachine, clientNode, true);\n      } else {\n       // Now we can add the name to the filesystem. This file has no\n       // blocks associated with it.\n       //\n       checkFsObjectLimit();\n\n        // increment global generation stamp\n        long genstamp = nextGenerationStamp();\n        INodeFileUnderConstruction newNode = dir.addFile(src, permissions,\n            replication, blockSize, holder, clientMachine, clientNode, genstamp);\n        if (newNode == null) {\n          throw new IOException(\"DIR* NameSystem.startFile: \" +\n                                \"Unable to add file to namespace.\");\n        }\n        leaseManager.addLease(newNode.getClientName(), src);\n        if (NameNode.stateChangeLog.isDebugEnabled()) {\n          NameNode.stateChangeLog.debug(\"DIR* NameSystem.startFile: \"\n                                     +\"add \"+src+\" to namespace for \"+holder);\n        }\n      }\n    } catch (IOException ie) {\n      NameNode.stateChangeLog.warn(\"DIR* NameSystem.startFile: \"\n                                   +ie.getMessage());\n      throw ie;\n    }\n    return null;\n  }",
                "code_after_change": "  private LocatedBlock startFileInternal(String src,\n      PermissionStatus permissions, String holder, String clientMachine,\n      EnumSet<CreateFlag> flag, boolean createParent, short replication,\n      long blockSize) throws SafeModeException, FileAlreadyExistsException,\n      AccessControlException, UnresolvedLinkException, FileNotFoundException,\n      ParentNotDirectoryException, IOException {\n    assert hasWriteLock();\n    if (NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\"DIR* NameSystem.startFile: src=\" + src\n          + \", holder=\" + holder\n          + \", clientMachine=\" + clientMachine\n          + \", createParent=\" + createParent\n          + \", replication=\" + replication\n          + \", createFlag=\" + flag.toString());\n    }\n    if (isInSafeMode()) {\n      throw new SafeModeException(\"Cannot create file\" + src, safeMode);\n    }\n    if (!DFSUtil.isValidName(src)) {\n      throw new InvalidPathException(src);\n    }\n\n    // Verify that the destination does not exist as a directory already.\n    boolean pathExists = dir.exists(src);\n    if (pathExists && dir.isDir(src)) {\n      throw new FileAlreadyExistsException(\"Cannot create file \" + src\n          + \"; already exists as a directory.\");\n    }\n\n    boolean overwrite = flag.contains(CreateFlag.OVERWRITE);\n    boolean append = flag.contains(CreateFlag.APPEND);\n    if (isPermissionEnabled) {\n      if (append || (overwrite && pathExists)) {\n        checkPathAccess(src, FsAction.WRITE);\n      } else {\n        checkAncestorAccess(src, FsAction.WRITE);\n      }\n    }\n\n    if (!createParent) {\n      verifyParentDir(src);\n    }\n\n    try {\n      INode myFile = dir.getFileINode(src);\n      recoverLeaseInternal(myFile, src, holder, clientMachine, false);\n\n      try {\n        blockManager.verifyReplication(src, replication, clientMachine);\n      } catch(IOException e) {\n        throw new IOException(\"failed to create \"+e.getMessage());\n      }\n      boolean create = flag.contains(CreateFlag.CREATE);\n      if (myFile == null) {\n        if (!create) {\n          throw new FileNotFoundException(\"failed to overwrite or append to non-existent file \"\n            + src + \" on client \" + clientMachine);\n        }\n      } else {\n        // File exists - must be one of append or overwrite\n        if (overwrite) {\n          delete(src, true);\n        } else if (!append) {\n          throw new FileAlreadyExistsException(\"failed to create file \" + src\n              + \" on client \" + clientMachine\n              + \" because the file exists\");\n        }\n      }\n\n      final DatanodeDescriptor clientNode = \n          blockManager.getDatanodeManager().getDatanodeByHost(clientMachine);\n\n      if (append && myFile != null) {\n        //\n        // Replace current node with a INodeUnderConstruction.\n        // Recreate in-memory lease record.\n        //\n        INodeFile node = (INodeFile) myFile;\n        INodeFileUnderConstruction cons = new INodeFileUnderConstruction(\n                                        node.getLocalNameBytes(),\n                                        node.getReplication(),\n                                        node.getModificationTime(),\n                                        node.getPreferredBlockSize(),\n                                        node.getBlocks(),\n                                        node.getPermissionStatus(),\n                                        holder,\n                                        clientMachine,\n                                        clientNode);\n        dir.replaceNode(src, node, cons);\n        leaseManager.addLease(cons.getClientName(), src);\n        \n        // convert last block to under-construction\n        LocatedBlock ret = blockManager.convertLastBlockToUnderConstruction(cons);\n\n        // add append file record to log, record lease, etc.\n        getEditLog().logOpenFile(src, cons);\n        return ret;\n      } else {\n       // Now we can add the name to the filesystem. This file has no\n       // blocks associated with it.\n       //\n       checkFsObjectLimit();\n\n        // increment global generation stamp\n        long genstamp = nextGenerationStamp();\n        INodeFileUnderConstruction newNode = dir.addFile(src, permissions,\n            replication, blockSize, holder, clientMachine, clientNode, genstamp);\n        if (newNode == null) {\n          throw new IOException(\"DIR* NameSystem.startFile: \" +\n                                \"Unable to add file to namespace.\");\n        }\n        leaseManager.addLease(newNode.getClientName(), src);\n\n        // record file record in log, record new generation stamp\n        getEditLog().logOpenFile(src, newNode);\n        if (NameNode.stateChangeLog.isDebugEnabled()) {\n          NameNode.stateChangeLog.debug(\"DIR* NameSystem.startFile: \"\n                                     +\"add \"+src+\" to namespace for \"+holder);\n        }\n      }\n    } catch (IOException ie) {\n      NameNode.stateChangeLog.warn(\"DIR* NameSystem.startFile: \"\n                                   +ie.getMessage());\n      throw ie;\n    }\n    return null;\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Precise",
                "sub_category": null
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report precisely identifies the root cause by mentioning the ClassCastException occurring in the method 'FSEditLogLoader.loadEditRecords', which is one of the ground truth methods. However, there is no fix suggestion provided in the bug report, as there is no 'Suggestions' or 'problem_location' field, nor any suggestion in the 'Description'. The problem location is also precisely identified as the method 'FSEditLogLoader.loadEditRecords' is mentioned in the stack trace, which is a ground truth method. There is no wrong information in the bug report as all the details are relevant to the context of the bug."
        }
    },
    {
        "filename": "HDFS-4404.json",
        "code_diff": {
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.closeConnection": {
                "code_before_change": "    private void closeConnection() {\n      if (socket == null) {\n        return;\n      }\n      // close the current connection\n      try {\n        socket.close();\n      } catch (IOException e) {\n        LOG.warn(\"Not able to close a socket\", e);\n      }\n      // set socket to null so that the next call to setupIOstreams\n      // can start the process of connect all over again.\n      socket = null;\n    }",
                "code_after_change": "    private void closeConnection() {\n      if (socket == null) {\n        return;\n      }\n      // close the current connection\n      try {\n        socket.close();\n      } catch (IOException e) {\n        LOG.warn(\"Not able to close a socket\", e);\n      }\n      // set socket to null so that the next call to setupIOstreams\n      // can start the process of connect all over again.\n      socket = null;\n    }"
            },
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.net.NetUtils.wrapException": {
                "code_before_change": "  public static IOException wrapException(final String destHost,\n                                          final int destPort,\n                                          final String localHost,\n                                          final int localPort,\n                                          final IOException exception) {\n    if (exception instanceof BindException) {\n      return new BindException(\n          \"Problem binding to [\"\n              + localHost\n              + \":\"\n              + localPort\n              + \"] \"\n              + exception\n              + \";\"\n              + see(\"BindException\"));\n    } else if (exception instanceof ConnectException) {\n      // connection refused; include the host:port in the error\n      return (ConnectException) new ConnectException(\n          \"Call From \"\n              + localHost\n              + \" to \"\n              + destHost\n              + \":\"\n              + destPort\n              + \" failed on connection exception: \"\n              + exception\n              + \";\"\n              + see(\"ConnectionRefused\"))\n          .initCause(exception);\n    } else if (exception instanceof UnknownHostException) {\n      return (UnknownHostException) new UnknownHostException(\n          \"Invalid host name: \"\n              + getHostDetailsAsString(destHost, destPort, localHost)\n              + exception\n              + \";\"\n              + see(\"UnknownHost\"))\n          .initCause(exception);\n    } else if (exception instanceof SocketTimeoutException) {\n      return (SocketTimeoutException) new SocketTimeoutException(\n          \"Call From \"\n              + localHost + \" to \" + destHost + \":\" + destPort\n              + \" failed on socket timeout exception: \" + exception\n              + \";\"\n              + see(\"SocketTimeout\"))\n          .initCause(exception);\n    } else if (exception instanceof NoRouteToHostException) {\n      return (NoRouteToHostException) new NoRouteToHostException(\n          \"No Route to Host from  \"\n              + localHost + \" to \" + destHost + \":\" + destPort\n              + \" failed on socket timeout exception: \" + exception\n              + \";\"\n              + see(\"NoRouteToHost\"))\n          .initCause(exception);\n    }\n    else {\n      return (IOException) new IOException(\"Failed on local exception: \"\n                                               + exception\n                                               + \"; Host Details : \"\n                                               + getHostDetailsAsString(destHost, destPort, localHost))\n          .initCause(exception);\n\n    }\n  }",
                "code_after_change": "  public static IOException wrapException(final String destHost,\n                                          final int destPort,\n                                          final String localHost,\n                                          final int localPort,\n                                          final IOException exception) {\n    if (exception instanceof BindException) {\n      return new BindException(\n          \"Problem binding to [\"\n              + localHost\n              + \":\"\n              + localPort\n              + \"] \"\n              + exception\n              + \";\"\n              + see(\"BindException\"));\n    } else if (exception instanceof ConnectException) {\n      // connection refused; include the host:port in the error\n      return wrapWithMessage(exception, \n          \"Call From \"\n              + localHost\n              + \" to \"\n              + destHost\n              + \":\"\n              + destPort\n              + \" failed on connection exception: \"\n              + exception\n              + \";\"\n              + see(\"ConnectionRefused\"));\n    } else if (exception instanceof UnknownHostException) {\n      return wrapWithMessage(exception,\n          \"Invalid host name: \"\n              + getHostDetailsAsString(destHost, destPort, localHost)\n              + exception\n              + \";\"\n              + see(\"UnknownHost\"));\n    } else if (exception instanceof SocketTimeoutException) {\n      return wrapWithMessage(exception,\n          \"Call From \"\n              + localHost + \" to \" + destHost + \":\" + destPort\n              + \" failed on socket timeout exception: \" + exception\n              + \";\"\n              + see(\"SocketTimeout\"));\n    } else if (exception instanceof NoRouteToHostException) {\n      return wrapWithMessage(exception,\n          \"No Route to Host from  \"\n              + localHost + \" to \" + destHost + \":\" + destPort\n              + \" failed on socket timeout exception: \" + exception\n              + \";\"\n              + see(\"NoRouteToHost\"));\n    }\n    else {\n      return (IOException) new IOException(\"Failed on local exception: \"\n                                               + exception\n                                               + \"; Host Details : \"\n                                               + getHostDetailsAsString(destHost, destPort, localHost))\n          .initCause(exception);\n\n    }\n  }"
            },
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.io.retry.RetryPolicies.shouldRetry": {
                "code_before_change": "    public RetryAction shouldRetry(Exception e, int retries, int failovers,\n        boolean isMethodIdempotent) throws Exception {\n      return RetryAction.FAIL;\n    }",
                "code_after_change": "    public RetryAction shouldRetry(Exception e, int retries, int failovers,\n        boolean isMethodIdempotent) throws Exception {\n      return RetryAction.FAIL;\n    }"
            },
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.net.NetUtils.connect": {
                "code_before_change": "  public static void connect(Socket socket,\n      SocketAddress address,\n      int timeout) throws IOException {\n    connect(socket, address, null, timeout);\n  }",
                "code_after_change": "  public static void connect(Socket socket,\n      SocketAddress address,\n      int timeout) throws IOException {\n    connect(socket, address, null, timeout);\n  }"
            },
            "hadoop-common-project.hadoop-common.src.main.java.org.apache.hadoop.ipc.Client.setupConnection": {
                "code_before_change": "    private synchronized void setupConnection() throws IOException {\n      short ioFailures = 0;\n      short timeoutFailures = 0;\n      while (true) {\n        try {\n          this.socket = socketFactory.createSocket();\n          this.socket.setTcpNoDelay(tcpNoDelay);\n          \n          /*\n           * Bind the socket to the host specified in the principal name of the\n           * client, to ensure Server matching address of the client connection\n           * to host name in principal passed.\n           */\n          if (UserGroupInformation.isSecurityEnabled()) {\n            KerberosInfo krbInfo = \n              remoteId.getProtocol().getAnnotation(KerberosInfo.class);\n            if (krbInfo != null && krbInfo.clientPrincipal() != null) {\n              String host = \n                SecurityUtil.getHostFromPrincipal(remoteId.getTicket().getUserName());\n              \n              // If host name is a valid local address then bind socket to it\n              InetAddress localAddr = NetUtils.getLocalInetAddress(host);\n              if (localAddr != null) {\n                this.socket.bind(new InetSocketAddress(localAddr, 0));\n              }\n            }\n          }\n          \n          // connection time out is 20s\n          NetUtils.connect(this.socket, server, 20000);\n          if (rpcTimeout > 0) {\n            pingInterval = rpcTimeout;  // rpcTimeout overwrites pingInterval\n          }\n          this.socket.setSoTimeout(pingInterval);\n          return;\n        } catch (SocketTimeoutException toe) {\n          /* Check for an address change and update the local reference.\n           * Reset the failure counter if the address was changed\n           */\n          if (updateAddress()) {\n            timeoutFailures = ioFailures = 0;\n          }\n          handleConnectionFailure(timeoutFailures++,\n              maxRetriesOnSocketTimeouts, toe);\n        } catch (IOException ie) {\n          if (updateAddress()) {\n            timeoutFailures = ioFailures = 0;\n          }\n          handleConnectionFailure(ioFailures++, ie);\n        }\n      }\n    }",
                "code_after_change": "    private synchronized void setupConnection() throws IOException {\n      short ioFailures = 0;\n      short timeoutFailures = 0;\n      while (true) {\n        try {\n          this.socket = socketFactory.createSocket();\n          this.socket.setTcpNoDelay(tcpNoDelay);\n          \n          /*\n           * Bind the socket to the host specified in the principal name of the\n           * client, to ensure Server matching address of the client connection\n           * to host name in principal passed.\n           */\n          if (UserGroupInformation.isSecurityEnabled()) {\n            KerberosInfo krbInfo = \n              remoteId.getProtocol().getAnnotation(KerberosInfo.class);\n            if (krbInfo != null && krbInfo.clientPrincipal() != null) {\n              String host = \n                SecurityUtil.getHostFromPrincipal(remoteId.getTicket().getUserName());\n              \n              // If host name is a valid local address then bind socket to it\n              InetAddress localAddr = NetUtils.getLocalInetAddress(host);\n              if (localAddr != null) {\n                this.socket.bind(new InetSocketAddress(localAddr, 0));\n              }\n            }\n          }\n          \n          NetUtils.connect(this.socket, server, connectionTimeout);\n          if (rpcTimeout > 0) {\n            pingInterval = rpcTimeout;  // rpcTimeout overwrites pingInterval\n          }\n          this.socket.setSoTimeout(pingInterval);\n          return;\n        } catch (ConnectTimeoutException toe) {\n          /* Check for an address change and update the local reference.\n           * Reset the failure counter if the address was changed\n           */\n          if (updateAddress()) {\n            timeoutFailures = ioFailures = 0;\n          }\n          handleConnectionTimeout(timeoutFailures++,\n              maxRetriesOnSocketTimeouts, toe);\n        } catch (IOException ie) {\n          if (updateAddress()) {\n            timeoutFailures = ioFailures = 0;\n          }\n          handleConnectionFailure(ioFailures++, ie);\n        }\n      }\n    }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report describes a socket timeout issue when trying to connect to a NameNode, which is related to the stack trace context shared with the ground truth methods. However, it does not precisely identify the root cause at the ground truth methods. The report lacks any fix suggestions, either in a dedicated field or within the description. The problem location is partially identified as it mentions methods in the stack trace that are related to the ground truth methods but does not directly point to the exact ground truth methods. There is no wrong information in the report; all details are relevant to the context of the bug."
        }
    },
    {
        "filename": "HDFS-12369.json",
        "code_diff": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.LeaseManager.checkLeases": {
                "code_before_change": "  synchronized boolean checkLeases() {\n    boolean needSync = false;\n    assert fsnamesystem.hasWriteLock();\n\n    long start = monotonicNow();\n\n    while(!sortedLeases.isEmpty() &&\n        sortedLeases.first().expiredHardLimit()\n        && !isMaxLockHoldToReleaseLease(start)) {\n      Lease leaseToCheck = sortedLeases.first();\n      LOG.info(leaseToCheck + \" has expired hard limit\");\n\n      final List<Long> removing = new ArrayList<>();\n      // need to create a copy of the oldest lease files, because\n      // internalReleaseLease() removes files corresponding to empty files,\n      // i.e. it needs to modify the collection being iterated over\n      // causing ConcurrentModificationException\n      Collection<Long> files = leaseToCheck.getFiles();\n      Long[] leaseINodeIds = files.toArray(new Long[files.size()]);\n      FSDirectory fsd = fsnamesystem.getFSDirectory();\n      String p = null;\n      String newHolder = getInternalLeaseHolder();\n      for(Long id : leaseINodeIds) {\n        try {\n          INodesInPath iip = INodesInPath.fromINode(fsd.getInode(id));\n          p = iip.getPath();\n          // Sanity check to make sure the path is correct\n          if (!p.startsWith(\"/\")) {\n            throw new IOException(\"Invalid path in the lease \" + p);\n          }\n          boolean completed = false;\n          try {\n            completed = fsnamesystem.internalReleaseLease(\n                leaseToCheck, p, iip, newHolder);\n          } catch (IOException e) {\n            LOG.warn(\"Cannot release the path \" + p + \" in the lease \"\n                + leaseToCheck + \". It will be retried.\", e);\n            continue;\n          }\n          if (LOG.isDebugEnabled()) {\n            if (completed) {\n              LOG.debug(\"Lease recovery for inode \" + id + \" is complete. \" +\n                            \"File closed.\");\n            } else {\n              LOG.debug(\"Started block recovery \" + p + \" lease \" + leaseToCheck);\n            }\n          }\n          // If a lease recovery happened, we need to sync later.\n          if (!needSync && !completed) {\n            needSync = true;\n          }\n        } catch (IOException e) {\n          LOG.warn(\"Removing lease with an invalid path: \" + p + \",\"\n              + leaseToCheck, e);\n          removing.add(id);\n        }\n        if (isMaxLockHoldToReleaseLease(start)) {\n          LOG.debug(\"Breaking out of checkLeases after \" +\n              fsnamesystem.getMaxLockHoldToReleaseLeaseMs() + \"ms.\");\n          break;\n        }\n      }\n\n      for(Long id : removing) {\n        removeLease(leaseToCheck, id);\n      }\n    }\n\n    return needSync;\n  }",
                "code_after_change": "  synchronized boolean checkLeases() {\n    boolean needSync = false;\n    assert fsnamesystem.hasWriteLock();\n\n    long start = monotonicNow();\n\n    while(!sortedLeases.isEmpty() &&\n        sortedLeases.first().expiredHardLimit()\n        && !isMaxLockHoldToReleaseLease(start)) {\n      Lease leaseToCheck = sortedLeases.first();\n      LOG.info(leaseToCheck + \" has expired hard limit\");\n\n      final List<Long> removing = new ArrayList<>();\n      // need to create a copy of the oldest lease files, because\n      // internalReleaseLease() removes files corresponding to empty files,\n      // i.e. it needs to modify the collection being iterated over\n      // causing ConcurrentModificationException\n      Collection<Long> files = leaseToCheck.getFiles();\n      Long[] leaseINodeIds = files.toArray(new Long[files.size()]);\n      FSDirectory fsd = fsnamesystem.getFSDirectory();\n      String p = null;\n      String newHolder = getInternalLeaseHolder();\n      for(Long id : leaseINodeIds) {\n        try {\n          INodesInPath iip = INodesInPath.fromINode(fsd.getInode(id));\n          p = iip.getPath();\n          // Sanity check to make sure the path is correct\n          if (!p.startsWith(\"/\")) {\n            throw new IOException(\"Invalid path in the lease \" + p);\n          }\n          final INodeFile lastINode = iip.getLastINode().asFile();\n          if (fsnamesystem.isFileDeleted(lastINode)) {\n            // INode referred by the lease could have been deleted.\n            removeLease(lastINode.getId());\n            continue;\n          }\n          boolean completed = false;\n          try {\n            completed = fsnamesystem.internalReleaseLease(\n                leaseToCheck, p, iip, newHolder);\n          } catch (IOException e) {\n            LOG.warn(\"Cannot release the path \" + p + \" in the lease \"\n                + leaseToCheck + \". It will be retried.\", e);\n            continue;\n          }\n          if (LOG.isDebugEnabled()) {\n            if (completed) {\n              LOG.debug(\"Lease recovery for inode \" + id + \" is complete. \" +\n                            \"File closed.\");\n            } else {\n              LOG.debug(\"Started block recovery \" + p + \" lease \" + leaseToCheck);\n            }\n          }\n          // If a lease recovery happened, we need to sync later.\n          if (!needSync && !completed) {\n            needSync = true;\n          }\n        } catch (IOException e) {\n          LOG.warn(\"Removing lease with an invalid path: \" + p + \",\"\n              + leaseToCheck, e);\n          removing.add(id);\n        }\n        if (isMaxLockHoldToReleaseLease(start)) {\n          LOG.debug(\"Breaking out of checkLeases after \" +\n              fsnamesystem.getMaxLockHoldToReleaseLeaseMs() + \"ms.\");\n          break;\n        }\n      }\n\n      for(Long id : removing) {\n        removeLease(leaseToCheck, id);\n      }\n    }\n\n    return needSync;\n  }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.nextBlockId": {
                "code_before_change": "  private long nextBlockId(BlockType blockType) throws IOException {\n    assert hasWriteLock();\n    checkNameNodeSafeMode(\"Cannot get next block ID\");\n    final long blockId = blockManager.nextBlockId(blockType);\n    getEditLog().logAllocateBlockId(blockId);\n    // NB: callers sync the log\n    return blockId;\n  }",
                "code_after_change": "  private long nextBlockId(BlockType blockType) throws IOException {\n    assert hasWriteLock();\n    checkNameNodeSafeMode(\"Cannot get next block ID\");\n    final long blockId = blockManager.nextBlockId(blockType);\n    getEditLog().logAllocateBlockId(blockId);\n    // NB: callers sync the log\n    return blockId;\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report mentions the method 'INodeFile.valueOf' which is in the stack trace and related to the ground truth method 'LeaseManager.checkLeases' through the shared context of the stack trace. However, it does not directly identify the root cause in the ground truth methods. There is no fix suggestion provided in the bug report. The problem location is partially identified as it mentions methods in the stack trace but not the exact ground truth methods. There is no wrong information in the bug report as all mentioned details are relevant to the context of the bug."
        }
    },
    {
        "filename": "HDFS-6462.json",
        "code_diff": {
            "hadoop-hdfs-project.hadoop-hdfs-nfs.src.main.java.org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3.RpcProgramNfs3": {
                "code_before_change": [],
                "code_after_change": []
            },
            "hadoop-hdfs-project.hadoop-hdfs-nfs.src.main.java.org.apache.hadoop.hdfs.nfs.nfs3.RpcProgramNfs3.fsstat": {
                "code_before_change": "  public FSSTAT3Response fsstat(XDR xdr, SecurityHandler securityHandler,\n      InetAddress client) {\n    FSSTAT3Response response = new FSSTAT3Response(Nfs3Status.NFS3_OK);\n    \n    if (!checkAccessPrivilege(client, AccessPrivilege.READ_ONLY)) {\n      response.setStatus(Nfs3Status.NFS3ERR_ACCES);\n      return response;\n    }\n    \n    DFSClient dfsClient = clientCache.getDfsClient(securityHandler.getUser());\n    if (dfsClient == null) {\n      response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);\n      return response;\n    }\n    \n    FSSTAT3Request request = null;\n    try {\n      request = new FSSTAT3Request(xdr);\n    } catch (IOException e) {\n      LOG.error(\"Invalid FSSTAT request\");\n      return new FSSTAT3Response(Nfs3Status.NFS3ERR_INVAL);\n    }\n\n    FileHandle handle = request.getHandle();\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"NFS FSSTAT fileId: \" + handle.getFileId());\n    }\n\n    try {\n      // Use superUserClient to get file system status\n      FsStatus fsStatus = superUserClient.getDiskStatus();\n      long totalBytes = fsStatus.getCapacity();\n      long freeBytes = fsStatus.getRemaining();\n      \n      Nfs3FileAttributes attrs = writeManager.getFileAttr(dfsClient, handle,\n          iug);\n      if (attrs == null) {\n        LOG.info(\"Can't get path for fileId:\" + handle.getFileId());\n        return new FSSTAT3Response(Nfs3Status.NFS3ERR_STALE);\n      }\n      \n      long maxFsObjects = config.getLong(\"dfs.max.objects\", 0);\n      if (maxFsObjects == 0) {\n        // A value of zero in HDFS indicates no limit to the number\n        // of objects that dfs supports. Using Integer.MAX_VALUE instead of\n        // Long.MAX_VALUE so 32bit client won't complain.\n        maxFsObjects = Integer.MAX_VALUE;\n      }\n      \n      return new FSSTAT3Response(Nfs3Status.NFS3_OK, attrs, totalBytes,\n          freeBytes, freeBytes, maxFsObjects, maxFsObjects, maxFsObjects, 0);\n    } catch (RemoteException r) {\n      LOG.warn(\"Exception \", r);\n      IOException io = r.unwrapRemoteException();\n      /**\n       * AuthorizationException can be thrown if the user can't be proxy'ed.\n       */\n      if (io instanceof AuthorizationException) {\n        return new FSSTAT3Response(Nfs3Status.NFS3ERR_ACCES);\n      } else {\n        return new FSSTAT3Response(Nfs3Status.NFS3ERR_IO);\n      }\n    } catch (IOException e) {\n      LOG.warn(\"Exception \", e);\n      return new FSSTAT3Response(Nfs3Status.NFS3ERR_IO);\n    }\n  }",
                "code_after_change": "  public FSSTAT3Response fsstat(XDR xdr, SecurityHandler securityHandler,\n      InetAddress client) {\n    FSSTAT3Response response = new FSSTAT3Response(Nfs3Status.NFS3_OK);\n    \n    if (!checkAccessPrivilege(client, AccessPrivilege.READ_ONLY)) {\n      response.setStatus(Nfs3Status.NFS3ERR_ACCES);\n      return response;\n    }\n    \n    DFSClient dfsClient = clientCache.getDfsClient(securityHandler.getUser());\n    if (dfsClient == null) {\n      response.setStatus(Nfs3Status.NFS3ERR_SERVERFAULT);\n      return response;\n    }\n    \n    FSSTAT3Request request = null;\n    try {\n      request = new FSSTAT3Request(xdr);\n    } catch (IOException e) {\n      LOG.error(\"Invalid FSSTAT request\");\n      return new FSSTAT3Response(Nfs3Status.NFS3ERR_INVAL);\n    }\n\n    FileHandle handle = request.getHandle();\n    if (LOG.isDebugEnabled()) {\n      LOG.debug(\"NFS FSSTAT fileId: \" + handle.getFileId());\n    }\n\n    try {\n      FsStatus fsStatus = dfsClient.getDiskStatus();\n      long totalBytes = fsStatus.getCapacity();\n      long freeBytes = fsStatus.getRemaining();\n      \n      Nfs3FileAttributes attrs = writeManager.getFileAttr(dfsClient, handle,\n          iug);\n      if (attrs == null) {\n        LOG.info(\"Can't get path for fileId:\" + handle.getFileId());\n        return new FSSTAT3Response(Nfs3Status.NFS3ERR_STALE);\n      }\n      \n      long maxFsObjects = config.getLong(\"dfs.max.objects\", 0);\n      if (maxFsObjects == 0) {\n        // A value of zero in HDFS indicates no limit to the number\n        // of objects that dfs supports. Using Integer.MAX_VALUE instead of\n        // Long.MAX_VALUE so 32bit client won't complain.\n        maxFsObjects = Integer.MAX_VALUE;\n      }\n      \n      return new FSSTAT3Response(Nfs3Status.NFS3_OK, attrs, totalBytes,\n          freeBytes, freeBytes, maxFsObjects, maxFsObjects, maxFsObjects, 0);\n    } catch (RemoteException r) {\n      LOG.warn(\"Exception \", r);\n      IOException io = r.unwrapRemoteException();\n      /**\n       * AuthorizationException can be thrown if the user can't be proxy'ed.\n       */\n      if (io instanceof AuthorizationException) {\n        return new FSSTAT3Response(Nfs3Status.NFS3ERR_ACCES);\n      } else {\n        return new FSSTAT3Response(Nfs3Status.NFS3ERR_IO);\n      }\n    } catch (IOException e) {\n      LOG.warn(\"Exception \", e);\n      return new FSSTAT3Response(Nfs3Status.NFS3ERR_IO);\n    }\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report mentions the error occurring in the NFS environment and provides a stack trace that includes the method 'RpcProgramNfs3.fsstat', which is a ground truth method. However, it does not explicitly identify the root cause within the ground truth methods, hence it is classified as 'Partial' with 'Shared Stack Trace Context'. There is no fix suggestion provided in the bug report, so it is marked as 'Missing'. The problem location is also 'Partial' with 'Shared Stack Trace Context' because the stack trace context includes the ground truth method but does not directly identify it as the problem location. There is no incorrect information in the bug report, so 'Wrong Information' is marked as 'No'."
        }
    },
    {
        "filename": "HDFS-5425.json",
        "code_diff": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSImageFormat.loadFilesUnderConstruction": {
                "code_before_change": "    private void loadFilesUnderConstruction(DataInput in,\n        boolean supportSnapshot, Counter counter) throws IOException {\n      FSDirectory fsDir = namesystem.dir;\n      int size = in.readInt();\n\n      LOG.info(\"Number of files under construction = \" + size);\n\n      for (int i = 0; i < size; i++) {\n        INodeFileUnderConstruction cons = FSImageSerialization\n            .readINodeUnderConstruction(in, namesystem, getLayoutVersion());\n        counter.increment();\n\n        // verify that file exists in namespace\n        String path = cons.getLocalName();\n        final INodesInPath iip = fsDir.getLastINodeInPath(path);\n        INodeFile oldnode = INodeFile.valueOf(iip.getINode(0), path);\n        cons.setLocalName(oldnode.getLocalNameBytes());\n        cons.setParent(oldnode.getParent());\n\n        if (oldnode instanceof INodeFileWithSnapshot) {\n          cons = new INodeFileUnderConstructionWithSnapshot(cons,\n              ((INodeFileWithSnapshot)oldnode).getDiffs());\n        }\n\n        fsDir.replaceINodeFile(path, oldnode, cons);\n        namesystem.leaseManager.addLease(cons.getClientName(), path); \n      }\n    }",
                "code_after_change": "    private void loadFilesUnderConstruction(DataInput in,\n        boolean supportSnapshot, Counter counter) throws IOException {\n      FSDirectory fsDir = namesystem.dir;\n      int size = in.readInt();\n\n      LOG.info(\"Number of files under construction = \" + size);\n\n      for (int i = 0; i < size; i++) {\n        INodeFileUnderConstruction cons = FSImageSerialization\n            .readINodeUnderConstruction(in, namesystem, getLayoutVersion());\n        counter.increment();\n\n        // verify that file exists in namespace\n        String path = cons.getLocalName();\n        final INodesInPath iip = fsDir.getLastINodeInPath(path);\n        INodeFile oldnode = INodeFile.valueOf(iip.getINode(0), path);\n        cons.setLocalName(oldnode.getLocalNameBytes());\n        INodeReference parentRef = oldnode.getParentReference();\n        if (parentRef != null) {\n          cons.setParentReference(parentRef);\n        } else {\n          cons.setParent(oldnode.getParent());\n        }\n\n        if (oldnode instanceof INodeFileWithSnapshot) {\n          cons = new INodeFileUnderConstructionWithSnapshot(cons,\n              ((INodeFileWithSnapshot)oldnode).getDiffs());\n        }\n\n        fsDir.replaceINodeFile(path, oldnode, cons);\n        namesystem.leaseManager.addLease(cons.getClientName(), path); \n      }\n    }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.snapshot.INodeDirectoryWithSnapshot.removeChild": {
                "code_before_change": "    private final boolean removeChild(ListType type, final INode child) {\n      final List<INode> list = getList(type);\n      final int i = searchIndex(type, child.getLocalNameBytes());\n      if (i >= 0 && list.get(i) == child) {\n        list.remove(i);\n        return true;\n      }\n      return false;\n    }",
                "code_after_change": "    private final boolean removeChild(ListType type, final INode child) {\n      final List<INode> list = getList(type);\n      final int i = searchIndex(type, child.getLocalNameBytes());\n      if (i >= 0 && list.get(i) == child) {\n        list.remove(i);\n        return true;\n      }\n      return false;\n    }"
            },
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock": {
                "code_before_change": "  LocatedBlock getAdditionalBlock(String src, long fileId, String clientName,\n      ExtendedBlock previous, Set<Node> excludedNodes, \n      List<String> favoredNodes)\n      throws LeaseExpiredException, NotReplicatedYetException,\n      QuotaExceededException, SafeModeException, UnresolvedLinkException,\n      IOException {\n    long blockSize;\n    int replication;\n    DatanodeDescriptor clientNode = null;\n\n    if(NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\n          \"BLOCK* NameSystem.getAdditionalBlock: file \"\n          +src+\" for \"+clientName);\n    }\n\n    // Part I. Analyze the state of the file with respect to the input data.\n    checkOperation(OperationCategory.READ);\n    byte[][] pathComponents = FSDirectory.getPathComponentsForReservedPath(src);\n    readLock();\n    try {\n      checkOperation(OperationCategory.READ);\n      src = FSDirectory.resolvePath(src, pathComponents, dir);\n      LocatedBlock[] onRetryBlock = new LocatedBlock[1];\n      final INode[] inodes = analyzeFileState(\n          src, fileId, clientName, previous, onRetryBlock).getINodes();\n      final INodeFileUnderConstruction pendingFile =\n          (INodeFileUnderConstruction) inodes[inodes.length - 1];\n\n      if(onRetryBlock[0] != null) {\n        // This is a retry. Just return the last block.\n        return onRetryBlock[0];\n      }\n      if (pendingFile.getBlocks().length >= maxBlocksPerFile) {\n        throw new IOException(\"File has reached the limit on maximum number of\"\n            + \" blocks (\" + DFSConfigKeys.DFS_NAMENODE_MAX_BLOCKS_PER_FILE_KEY\n            + \"): \" + pendingFile.getBlocks().length + \" >= \"\n            + maxBlocksPerFile);\n      }\n      blockSize = pendingFile.getPreferredBlockSize();\n      clientNode = pendingFile.getClientNode();\n      replication = pendingFile.getFileReplication();\n    } finally {\n      readUnlock();\n    }\n\n    // choose targets for the new block to be allocated.\n    final DatanodeDescriptor targets[] = getBlockManager().chooseTarget( \n        src, replication, clientNode, excludedNodes, blockSize, favoredNodes);\n\n    // Part II.\n    // Allocate a new block, add it to the INode and the BlocksMap. \n    Block newBlock = null;\n    long offset;\n    checkOperation(OperationCategory.WRITE);\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      // Run the full analysis again, since things could have changed\n      // while chooseTarget() was executing.\n      LocatedBlock[] onRetryBlock = new LocatedBlock[1];\n      INodesInPath inodesInPath =\n          analyzeFileState(src, fileId, clientName, previous, onRetryBlock);\n      INode[] inodes = inodesInPath.getINodes();\n      final INodeFileUnderConstruction pendingFile =\n          (INodeFileUnderConstruction) inodes[inodes.length - 1];\n\n      if(onRetryBlock[0] != null) {\n        // This is a retry. Just return the last block.\n        return onRetryBlock[0];\n      }\n\n      // commit the last block and complete it if it has minimum replicas\n      commitOrCompleteLastBlock(pendingFile,\n                                ExtendedBlock.getLocalBlock(previous));\n\n      // allocate new block, record block locations in INode.\n      newBlock = createNewBlock();\n      saveAllocatedBlock(src, inodesInPath, newBlock, targets);\n\n      dir.persistBlocks(src, pendingFile, false);\n      offset = pendingFile.computeFileSize();\n    } finally {\n      writeUnlock();\n    }\n    if (persistBlocks) {\n      getEditLog().logSync();\n    }\n\n    // Return located block\n    return makeLocatedBlock(newBlock, targets, offset);\n  }",
                "code_after_change": "  LocatedBlock getAdditionalBlock(String src, long fileId, String clientName,\n      ExtendedBlock previous, Set<Node> excludedNodes, \n      List<String> favoredNodes)\n      throws LeaseExpiredException, NotReplicatedYetException,\n      QuotaExceededException, SafeModeException, UnresolvedLinkException,\n      IOException {\n    long blockSize;\n    int replication;\n    DatanodeDescriptor clientNode = null;\n\n    if(NameNode.stateChangeLog.isDebugEnabled()) {\n      NameNode.stateChangeLog.debug(\n          \"BLOCK* NameSystem.getAdditionalBlock: file \"\n          +src+\" for \"+clientName);\n    }\n\n    // Part I. Analyze the state of the file with respect to the input data.\n    checkOperation(OperationCategory.READ);\n    byte[][] pathComponents = FSDirectory.getPathComponentsForReservedPath(src);\n    readLock();\n    try {\n      checkOperation(OperationCategory.READ);\n      src = FSDirectory.resolvePath(src, pathComponents, dir);\n      LocatedBlock[] onRetryBlock = new LocatedBlock[1];\n      final INode[] inodes = analyzeFileState(\n          src, fileId, clientName, previous, onRetryBlock).getINodes();\n      final INodeFileUnderConstruction pendingFile =\n          (INodeFileUnderConstruction) inodes[inodes.length - 1].asFile();\n\n      if (onRetryBlock[0] != null && onRetryBlock[0].getLocations().length > 0) {\n        // This is a retry. Just return the last block if having locations.\n        return onRetryBlock[0];\n      }\n      if (pendingFile.getBlocks().length >= maxBlocksPerFile) {\n        throw new IOException(\"File has reached the limit on maximum number of\"\n            + \" blocks (\" + DFSConfigKeys.DFS_NAMENODE_MAX_BLOCKS_PER_FILE_KEY\n            + \"): \" + pendingFile.getBlocks().length + \" >= \"\n            + maxBlocksPerFile);\n      }\n      blockSize = pendingFile.getPreferredBlockSize();\n      clientNode = pendingFile.getClientNode();\n      replication = pendingFile.getFileReplication();\n    } finally {\n      readUnlock();\n    }\n\n    // choose targets for the new block to be allocated.\n    final DatanodeDescriptor targets[] = getBlockManager().chooseTarget( \n        src, replication, clientNode, excludedNodes, blockSize, favoredNodes);\n\n    // Part II.\n    // Allocate a new block, add it to the INode and the BlocksMap. \n    Block newBlock = null;\n    long offset;\n    checkOperation(OperationCategory.WRITE);\n    writeLock();\n    try {\n      checkOperation(OperationCategory.WRITE);\n      // Run the full analysis again, since things could have changed\n      // while chooseTarget() was executing.\n      LocatedBlock[] onRetryBlock = new LocatedBlock[1];\n      INodesInPath inodesInPath =\n          analyzeFileState(src, fileId, clientName, previous, onRetryBlock);\n      INode[] inodes = inodesInPath.getINodes();\n      final INodeFileUnderConstruction pendingFile =\n          (INodeFileUnderConstruction) inodes[inodes.length - 1].asFile();\n\n      if (onRetryBlock[0] != null) {\n        if (onRetryBlock[0].getLocations().length > 0) {\n          // This is a retry. Just return the last block if having locations.\n          return onRetryBlock[0];\n        } else {\n          // add new chosen targets to already allocated block and return\n          BlockInfo lastBlockInFile = pendingFile.getLastBlock();\n          ((BlockInfoUnderConstruction) lastBlockInFile)\n              .setExpectedLocations(targets);\n          offset = pendingFile.computeFileSize();\n          return makeLocatedBlock(lastBlockInFile, targets, offset);\n        }\n      }\n\n      // commit the last block and complete it if it has minimum replicas\n      commitOrCompleteLastBlock(pendingFile,\n                                ExtendedBlock.getLocalBlock(previous));\n\n      // allocate new block, record block locations in INode.\n      newBlock = createNewBlock();\n      saveAllocatedBlock(src, inodesInPath, newBlock, targets);\n\n      dir.persistBlocks(src, pendingFile, false);\n      offset = pendingFile.computeFileSize();\n    } finally {\n      writeUnlock();\n    }\n    if (persistBlocks) {\n      getEditLog().logSync();\n    }\n\n    // Return located block\n    return makeLocatedBlock(newBlock, targets, offset);\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report mentions methods in the stack trace that are related to the ground truth methods, such as 'FSImageFormat$Loader.loadFilesUnderConstruction' and 'INodeDirectoryWithSnapshot.replaceChild', which are in the shared stack trace context with the ground truth methods. However, it does not precisely identify the root cause or problem location as it does not mention the exact ground truth methods. There is no fix suggestion provided in the bug report. All information in the bug report is relevant and related to the context of the bug, so there is no wrong information."
        }
    },
    {
        "filename": "HDFS-13145.json",
        "code_diff": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager.selectInputStreams": {
                "code_before_change": "  public void selectInputStreams(Collection<EditLogInputStream> streams,\n      long fromTxnId, boolean inProgressOk) throws IOException {\n    selectInputStreams(streams, fromTxnId, inProgressOk, false);\n  }",
                "code_after_change": "  public void selectInputStreams(Collection<EditLogInputStream> streams,\n      long fromTxnId, boolean inProgressOk) throws IOException {\n    selectInputStreams(streams, fromTxnId, inProgressOk, false);\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the issue related to the 'openForWrite' method, which is in the same stack trace as the ground truth method 'selectInputStreams'. However, it does not precisely identify the root cause in the ground truth method. The report does not provide any fix suggestion, hence 'Missing' for fix suggestion. The problem location is partially identified as it mentions a method in the shared stack trace context but not the exact ground truth method. There is no wrong information in the bug report as all details are relevant to the context of the bug."
        }
    },
    {
        "filename": "HDFS-8807.json",
        "code_diff": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.StorageLocation.parse": {
                "code_before_change": "  public static StorageLocation parse(String rawLocation)\n      throws IOException, SecurityException {\n    Matcher matcher = regex.matcher(rawLocation);\n    StorageType storageType = StorageType.DEFAULT;\n    String location = rawLocation;\n\n    if (matcher.matches()) {\n      String classString = matcher.group(1);\n      location = matcher.group(2);\n      if (!classString.isEmpty()) {\n        storageType =\n            StorageType.valueOf(StringUtils.toUpperCase(classString));\n      }\n    }\n\n    return new StorageLocation(storageType, new Path(location).toUri());\n  }",
                "code_after_change": "  public static StorageLocation parse(String rawLocation)\n      throws IOException, SecurityException {\n    Matcher matcher = regex.matcher(rawLocation);\n    StorageType storageType = StorageType.DEFAULT;\n    String location = rawLocation;\n\n    if (matcher.matches()) {\n      String classString = matcher.group(1);\n      location = matcher.group(2).trim();\n      if (!classString.isEmpty()) {\n        storageType =\n            StorageType.valueOf(StringUtils.toUpperCase(classString));\n      }\n    }\n\n    return new StorageLocation(storageType, new Path(location).toUri());\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Correct",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report identifies the issue with spaces in the configuration, which is related to the parsing error in the stack trace. The root cause is partially identified as it mentions the parsing error in the stack trace, which includes the ground truth method 'StorageLocation.parse'. The fix suggestion is correct as it aligns with the developer's fix of trimming spaces. The problem location is partially identified as the stack trace context includes the ground truth method, but it is not directly mentioned in the report. There is no wrong information in the bug report as all details are relevant to the issue."
        }
    },
    {
        "filename": "HDFS-3436.json",
        "code_diff": {
            "hadoop-hdfs-project.hadoop-hdfs.src.main.java.org.apache.hadoop.hdfs.server.datanode.DataNode.transferReplicaForPipelineRecovery": {
                "code_before_change": "  void transferReplicaForPipelineRecovery(final ExtendedBlock b,\n      final DatanodeInfo[] targets, final String client) throws IOException {\n    final long storedGS;\n    final long visible;\n    final BlockConstructionStage stage;\n\n    //get replica information\n    synchronized(data) {\n      if (data.isValidRbw(b)) {\n        stage = BlockConstructionStage.TRANSFER_RBW;\n      } else if (data.isValidBlock(b)) {\n        stage = BlockConstructionStage.TRANSFER_FINALIZED;\n      } else {\n        final String r = data.getReplicaString(b.getBlockPoolId(), b.getBlockId());\n        throw new IOException(b + \" is neither a RBW nor a Finalized, r=\" + r);\n      }\n\n      storedGS = data.getStoredBlock(b.getBlockPoolId(),\n          b.getBlockId()).getGenerationStamp();\n      if (storedGS < b.getGenerationStamp()) {\n        throw new IOException(\n            storedGS + \" = storedGS < b.getGenerationStamp(), b=\" + b);        \n      }\n      visible = data.getReplicaVisibleLength(b);\n    }\n\n    //set storedGS and visible length\n    b.setGenerationStamp(storedGS);\n    b.setNumBytes(visible);\n\n    if (targets.length > 0) {\n      new DataTransfer(targets, b, stage, client).run();\n    }\n  }",
                "code_after_change": "  void transferReplicaForPipelineRecovery(final ExtendedBlock b,\n      final DatanodeInfo[] targets, final String client) throws IOException {\n    final long storedGS;\n    final long visible;\n    final BlockConstructionStage stage;\n\n    //get replica information\n    synchronized(data) {\n      Block storedBlock = data.getStoredBlock(b.getBlockPoolId(),\n          b.getBlockId());\n      if (null == storedBlock) {\n        throw new IOException(b + \" not found in datanode.\");\n      }\n      storedGS = storedBlock.getGenerationStamp();\n      if (storedGS < b.getGenerationStamp()) {\n        throw new IOException(storedGS\n            + \" = storedGS < b.getGenerationStamp(), b=\" + b);\n      }\n      // Update the genstamp with storedGS\n      b.setGenerationStamp(storedGS);\n      if (data.isValidRbw(b)) {\n        stage = BlockConstructionStage.TRANSFER_RBW;\n      } else if (data.isValidBlock(b)) {\n        stage = BlockConstructionStage.TRANSFER_FINALIZED;\n      } else {\n        final String r = data.getReplicaString(b.getBlockPoolId(), b.getBlockId());\n        throw new IOException(b + \" is neither a RBW nor a Finalized, r=\" + r);\n      }\n      visible = data.getReplicaVisibleLength(b);\n    }\n    //set visible length\n    b.setNumBytes(visible);\n\n    if (targets.length > 0) {\n      new DataTransfer(targets, b, stage, client).run();\n    }\n  }"
            }
        },
        "llm_judgement": {
            "root_cause_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "fix_suggestion": "Missing",
            "problem_location_identification": {
                "level": "Partial",
                "sub_category": "Shared Stack Trace Context"
            },
            "wrong_information": "No",
            "explanation_of_judgement": "The bug report mentions methods like 'addDatanode2ExistingPipeline' and 'setupPipelineForAppendOrRecovery' which are in the same stack trace as the ground truth method 'transferReplicaForPipelineRecovery'. However, it does not precisely identify the root cause in the ground truth method. There is no fix suggestion provided in the bug report. The problem location is partially identified as it mentions methods in the shared stack trace context but not the exact ground truth method. There is no wrong information in the bug report as all mentioned methods are relevant to the context of the bug."
        }
    }
]